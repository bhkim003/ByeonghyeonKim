{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23977/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #     criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    slice_bucket.append(slice_concat)\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                slice_bucket.append(slice_concat)\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb 과거 하이퍼파라미터 가져와서 붙여넣기 (devices unique_name은 니가 할당해라)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],)\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.25 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "# const2 = False # True # False\n",
    "\n",
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "# if const2 == True:\n",
    "#     const2 = decay\n",
    "# else:\n",
    "#     const2 = 0.0\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"0\",\n",
    "#                 single_step = True, # True # False # DFA_on이랑 같이 가라\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "#                 BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 16, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.75,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 10000,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 5, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "#                 # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "#                 # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "#                 trace_on = True,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용\n",
    "\n",
    "#                 exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "#                 denoise_on = True, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = 0, \n",
    "\n",
    "#                 num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = True, # True # False \n",
    "\n",
    "#                 last_lif = False,\n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoid와 BN이 있어야 잘된다.\n",
    "# # average pooling  \n",
    "# # 이 낫다. \n",
    "\n",
    "# # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8sj0vb5t with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_164640-8sj0vb5t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8sj0vb5t' target=\"_blank\">super-sweep-3</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8sj0vb5t' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8sj0vb5t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = b57dfc72d05d304ce829f424a70e455b\n",
      "cache path doesn't exist\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.210812/  1.681721, val:  31.67%, val_best:  31.67%, tr:  13.07%, tr_best:  13.07%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.386555/  1.353671, val:  55.42%, val_best:  55.42%, tr:  49.54%, tr_best:  49.54%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.224881/  1.376191, val:  57.92%, val_best:  57.92%, tr:  58.94%, tr_best:  58.94%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.063203/  1.275939, val:  60.00%, val_best:  60.00%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.065158/  1.262300, val:  62.50%, val_best:  62.50%, tr:  61.49%, tr_best:  64.04%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.002423/  1.201253, val:  61.67%, val_best:  62.50%, tr:  69.46%, tr_best:  69.46%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.940286/  1.223511, val:  61.25%, val_best:  62.50%, tr:  70.28%, tr_best:  70.28%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.940461/  1.198340, val:  59.17%, val_best:  62.50%, tr:  69.05%, tr_best:  70.28%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.919078/  1.182571, val:  63.33%, val_best:  63.33%, tr:  69.25%, tr_best:  70.28%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.877852/  1.247708, val:  62.92%, val_best:  63.33%, tr:  73.85%, tr_best:  73.85%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.881022/  1.361780, val:  61.67%, val_best:  63.33%, tr:  72.93%, tr_best:  73.85%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.866890/  1.164792, val:  64.58%, val_best:  64.58%, tr:  73.24%, tr_best:  73.85%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.786168/  1.154214, val:  65.83%, val_best:  65.83%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.769624/  1.186466, val:  68.75%, val_best:  68.75%, tr:  79.98%, tr_best:  79.98%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.722701/  1.417607, val:  68.33%, val_best:  68.75%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.689308/  1.264138, val:  63.33%, val_best:  68.75%, tr:  80.80%, tr_best:  81.00%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.654901/  1.278010, val:  62.08%, val_best:  68.75%, tr:  82.12%, tr_best:  82.12%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.639970/  1.258729, val:  65.42%, val_best:  68.75%, tr:  86.82%, tr_best:  86.82%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.589919/  1.419813, val:  65.00%, val_best:  68.75%, tr:  87.95%, tr_best:  87.95%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.623234/  1.257494, val:  69.58%, val_best:  69.58%, tr:  85.80%, tr_best:  87.95%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.622719/  1.389385, val:  68.33%, val_best:  69.58%, tr:  85.90%, tr_best:  87.95%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.556096/  1.529907, val:  62.92%, val_best:  69.58%, tr:  87.95%, tr_best:  87.95%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.521368/  1.358394, val:  62.92%, val_best:  69.58%, tr:  89.79%, tr_best:  89.79%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.520175/  1.370903, val:  71.25%, val_best:  71.25%, tr:  90.81%, tr_best:  90.81%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.465218/  1.357821, val:  72.92%, val_best:  72.92%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.520684/  1.376833, val:  65.42%, val_best:  72.92%, tr:  89.38%, tr_best:  93.36%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.497071/  1.364841, val:  72.08%, val_best:  72.92%, tr:  90.19%, tr_best:  93.36%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.426883/  1.524139, val:  68.33%, val_best:  72.92%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.394330/  1.393904, val:  70.83%, val_best:  72.92%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.356238/  1.511496, val:  67.08%, val_best:  72.92%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.356547/  1.548883, val:  64.17%, val_best:  72.92%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.344267/  1.541734, val:  71.67%, val_best:  72.92%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.377733/  1.658543, val:  65.00%, val_best:  72.92%, tr:  94.59%, tr_best:  98.06%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.342921/  1.606889, val:  72.50%, val_best:  72.92%, tr:  96.94%, tr_best:  98.06%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.328112/  1.641720, val:  67.08%, val_best:  72.92%, tr:  97.75%, tr_best:  98.06%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.296126/  1.649913, val:  73.33%, val_best:  73.33%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.274631/  1.615058, val:  71.25%, val_best:  73.33%, tr:  98.26%, tr_best:  98.57%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.278728/  1.793739, val:  63.33%, val_best:  73.33%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.314043/  1.647128, val:  72.92%, val_best:  73.33%, tr:  97.34%, tr_best:  98.57%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.253166/  1.730766, val:  69.17%, val_best:  73.33%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.235799/  1.697649, val:  71.25%, val_best:  73.33%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.232454/  1.765837, val:  74.58%, val_best:  74.58%, tr:  98.77%, tr_best:  99.18%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.200667/  1.739882, val:  71.25%, val_best:  74.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.198410/  1.808813, val:  68.33%, val_best:  74.58%, tr:  99.28%, tr_best:  99.80%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.218983/  1.819561, val:  69.17%, val_best:  74.58%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.200601/  1.799066, val:  70.83%, val_best:  74.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.181203/  1.816794, val:  74.58%, val_best:  74.58%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.161995/  1.828239, val:  75.42%, val_best:  75.42%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.161031/  1.907332, val:  73.33%, val_best:  75.42%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.146101/  1.856286, val:  73.75%, val_best:  75.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.142790/  1.970911, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.147648/  1.916479, val:  73.75%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.149722/  1.970506, val:  74.58%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.128960/  1.925620, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.108222/  2.021706, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.118099/  2.030490, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.115658/  2.077209, val:  72.92%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.124004/  2.038905, val:  75.00%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.109410/  2.073846, val:  72.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.101244/  2.093413, val:  70.83%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.100152/  2.076909, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.110134/  2.089176, val:  72.92%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.098404/  2.215020, val:  70.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.095727/  2.103407, val:  75.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.092267/  2.138840, val:  75.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.083851/  2.116917, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.088618/  2.160309, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.084024/  2.214997, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.077517/  2.206853, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.066714/  2.226120, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.072256/  2.220895, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.076774/  2.253786, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.075388/  2.242424, val:  73.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.072948/  2.329711, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.066100/  2.289666, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.064934/  2.308052, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.066532/  2.287781, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.066405/  2.326518, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.062989/  2.363613, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.055204/  2.349234, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.056934/  2.382283, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.054835/  2.381593, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.050144/  2.429185, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.063122/  2.430706, val:  72.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.057895/  2.338602, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.042740/  2.479431, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.042660/  2.459172, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.050772/  2.454995, val:  73.33%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.053046/  2.475026, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.044934/  2.510246, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.041122/  2.498242, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.035333/  2.469730, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.037683/  2.534963, val:  73.33%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.040039/  2.510855, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.040805/  2.540192, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.033972/  2.552210, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.036334/  2.548234, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.030993/  2.583926, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.033398/  2.530824, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.031649/  2.593419, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834c87df659f4e4f987b8a458f52cdbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▄▄▆▅▆▅▅█▇██▇▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▅▆▆▇▆▇▆▇▇▆▆▇▆▇▇▇█▇▇▇█▇▇█▇█▇█▇███▇█▇█▇</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▆▆▇▇▇▇▇████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▅▆▆▇▆▇▆▇▇▆▆▇▆▇▇▇█▇▇▇█▇▇█▇█▇█▇███▇█▇█▇</td></tr><tr><td>val_loss</td><td>▄▂▂▁▁▁▂▂▂▃▂▂▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03165</td></tr><tr><td>val_acc_best</td><td>0.78333</td></tr><tr><td>val_acc_now</td><td>0.76667</td></tr><tr><td>val_loss</td><td>2.59342</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">super-sweep-3</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8sj0vb5t' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8sj0vb5t</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_164640-8sj0vb5t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3fepn5o9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_165459-3fepn5o9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3fepn5o9' target=\"_blank\">tough-sweep-5</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3fepn5o9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3fepn5o9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.324377/  2.253040, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.09%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.620876/  1.434216, val:  55.83%, val_best:  55.83%, tr:  43.11%, tr_best:  43.11%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.395223/  1.387492, val:  60.00%, val_best:  60.00%, tr:  57.41%, tr_best:  57.41%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.225031/  1.483529, val:  55.00%, val_best:  60.00%, tr:  61.59%, tr_best:  61.59%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.169817/  1.421775, val:  61.67%, val_best:  61.67%, tr:  63.23%, tr_best:  63.23%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.066954/  1.304021, val:  63.75%, val_best:  63.75%, tr:  66.80%, tr_best:  66.80%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.064376/  1.238216, val:  65.42%, val_best:  65.42%, tr:  68.95%, tr_best:  68.95%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.024880/  1.199529, val:  69.17%, val_best:  69.17%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.029123/  1.336443, val:  65.42%, val_best:  69.17%, tr:  72.01%, tr_best:  72.01%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.941154/  1.283742, val:  62.50%, val_best:  69.17%, tr:  76.71%, tr_best:  76.71%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.947766/  1.218557, val:  71.67%, val_best:  71.67%, tr:  74.46%, tr_best:  76.71%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.831730/  1.175322, val:  78.33%, val_best:  78.33%, tr:  82.43%, tr_best:  82.43%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.732076/  1.132564, val:  81.25%, val_best:  81.25%, tr:  85.19%, tr_best:  85.19%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.699328/  1.109289, val:  83.75%, val_best:  83.75%, tr:  87.74%, tr_best:  87.74%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.709120/  1.330841, val:  76.67%, val_best:  83.75%, tr:  86.62%, tr_best:  87.74%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.615329/  1.213431, val:  80.42%, val_best:  83.75%, tr:  89.89%, tr_best:  89.89%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.583712/  1.172938, val:  82.08%, val_best:  83.75%, tr:  91.93%, tr_best:  91.93%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.522593/  1.172161, val:  84.17%, val_best:  84.17%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.492227/  1.255606, val:  78.75%, val_best:  84.17%, tr:  95.51%, tr_best:  95.51%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.458635/  1.185620, val:  83.75%, val_best:  84.17%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.436657/  1.216015, val:  86.67%, val_best:  86.67%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.408795/  1.418315, val:  75.83%, val_best:  86.67%, tr:  96.53%, tr_best:  96.73%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.397339/  1.363236, val:  79.17%, val_best:  86.67%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.392192/  1.274249, val:  84.58%, val_best:  86.67%, tr:  96.53%, tr_best:  97.85%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.342104/  1.305457, val:  84.58%, val_best:  86.67%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.327823/  1.339410, val:  85.42%, val_best:  86.67%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.348489/  1.340654, val:  86.25%, val_best:  86.67%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.303730/  1.472875, val:  82.08%, val_best:  86.67%, tr:  98.26%, tr_best:  98.37%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.286895/  1.302079, val:  87.92%, val_best:  87.92%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.256725/  1.526289, val:  81.67%, val_best:  87.92%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.263467/  1.343612, val:  86.25%, val_best:  87.92%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.231242/  1.439291, val:  86.67%, val_best:  87.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.277236/  1.455598, val:  83.75%, val_best:  87.92%, tr:  98.26%, tr_best:  99.69%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.243337/  1.475184, val:  86.25%, val_best:  87.92%, tr:  99.18%, tr_best:  99.69%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.228707/  1.511039, val:  85.83%, val_best:  87.92%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.205928/  1.488270, val:  87.08%, val_best:  87.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.184888/  1.487229, val:  86.25%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.176397/  1.569968, val:  86.25%, val_best:  87.92%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.172465/  1.525931, val:  87.92%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.159983/  1.586069, val:  87.92%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.158487/  1.559112, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.155101/  1.608311, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.129379/  1.628664, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.138231/  1.640852, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.128470/  1.701276, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.123701/  1.678072, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.128427/  1.750961, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.121205/  1.691087, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.117829/  1.722983, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.104483/  1.782358, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.102234/  1.809709, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.102463/  1.793564, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.103370/  1.924231, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.088605/  1.887205, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.091713/  1.841688, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.084755/  1.893186, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.080027/  1.923437, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.075570/  1.865280, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.066207/  1.890004, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.078241/  1.963507, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.066644/  1.937706, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.070238/  1.974469, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.068976/  2.043910, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.066484/  2.017676, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.059064/  2.008111, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.060678/  2.038437, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.056949/  2.116035, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.057309/  2.131035, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.051204/  2.128208, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.049041/  2.085106, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.041603/  2.106980, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.047903/  2.167952, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.049546/  2.164062, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.049787/  2.154206, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.047796/  2.187084, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.048630/  2.162773, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.041851/  2.198905, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.040721/  2.220918, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.041849/  2.187904, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.038748/  2.198490, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.036521/  2.291597, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.037336/  2.265679, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.035793/  2.274508, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.035333/  2.312144, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.038348/  2.310215, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.039090/  2.357795, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.029031/  2.316517, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.027537/  2.350415, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.030840/  2.355761, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.026686/  2.383904, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.024622/  2.396797, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.027923/  2.433868, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.028283/  2.424008, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.021742/  2.396441, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.025743/  2.462657, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.022960/  2.496866, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.026696/  2.494616, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.022717/  2.501087, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.022810/  2.476700, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.023298/  2.457374, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d666ac3e3b46f2bd759af8b4e0619b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▃▃▇▇▇██████▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▆▇▇▇▇▇██▇▇██████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇▇▇███████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▆▇▇▇▇▇██▇▇██████████████████████████</td></tr><tr><td>val_loss</td><td>▇▂▂▁▂▁▂▁▁▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0233</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>2.45737</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-sweep-5</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3fepn5o9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3fepn5o9</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_165459-3fepn5o9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tt6huood with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_170034-tt6huood</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tt6huood' target=\"_blank\">exalted-sweep-7</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tt6huood' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tt6huood</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 8ba471014b61e50904ecff33d030e937\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.035592/  1.580963, val:  47.92%, val_best:  47.92%, tr:  24.62%, tr_best:  24.62%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.507693/  1.466623, val:  57.92%, val_best:  57.92%, tr:  48.83%, tr_best:  48.83%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.472715/  1.552243, val:  55.42%, val_best:  57.92%, tr:  53.83%, tr_best:  53.83%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.249971/  1.489647, val:  53.33%, val_best:  57.92%, tr:  60.57%, tr_best:  60.57%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.133631/  1.382339, val:  59.58%, val_best:  59.58%, tr:  62.31%, tr_best:  62.31%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.081048/  1.517339, val:  52.08%, val_best:  59.58%, tr:  67.11%, tr_best:  67.11%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.104153/  1.358119, val:  60.83%, val_best:  60.83%, tr:  66.39%, tr_best:  67.11%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.104998/  1.600646, val:  56.67%, val_best:  60.83%, tr:  68.85%, tr_best:  68.85%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.156435/  1.468281, val:  64.58%, val_best:  64.58%, tr:  68.74%, tr_best:  68.85%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.097169/  1.456214, val:  59.58%, val_best:  64.58%, tr:  68.34%, tr_best:  68.85%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.042390/  1.293977, val:  64.17%, val_best:  64.58%, tr:  70.89%, tr_best:  70.89%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.959784/  1.328928, val:  65.83%, val_best:  65.83%, tr:  73.03%, tr_best:  73.03%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.915014/  1.238535, val:  65.00%, val_best:  65.83%, tr:  72.63%, tr_best:  73.03%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.925886/  1.440569, val:  57.92%, val_best:  65.83%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.931944/  1.627750, val:  61.67%, val_best:  65.83%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.748272/  1.539011, val:  62.92%, val_best:  65.83%, tr:  78.86%, tr_best:  78.86%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.790984/  1.429748, val:  65.42%, val_best:  65.83%, tr:  78.65%, tr_best:  78.86%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.733015/  1.459944, val:  64.17%, val_best:  65.83%, tr:  83.04%, tr_best:  83.04%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.679227/  1.546365, val:  68.75%, val_best:  68.75%, tr:  84.47%, tr_best:  84.47%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.651026/  1.488698, val:  64.58%, val_best:  68.75%, tr:  85.80%, tr_best:  85.80%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.796947/  1.434194, val:  70.42%, val_best:  70.42%, tr:  83.45%, tr_best:  85.80%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.571238/  1.839175, val:  66.67%, val_best:  70.42%, tr:  87.95%, tr_best:  87.95%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.529673/  1.542075, val:  69.58%, val_best:  70.42%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.483220/  1.669167, val:  67.08%, val_best:  70.42%, tr:  91.73%, tr_best:  91.73%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.435203/  1.541310, val:  73.33%, val_best:  73.33%, tr:  94.59%, tr_best:  94.59%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.370709/  1.676523, val:  65.00%, val_best:  73.33%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.373787/  1.682778, val:  68.75%, val_best:  73.33%, tr:  96.12%, tr_best:  96.83%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.329681/  1.690518, val:  73.33%, val_best:  73.33%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.329053/  1.616161, val:  77.92%, val_best:  77.92%, tr:  97.14%, tr_best:  98.06%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.293315/  1.801849, val:  69.17%, val_best:  77.92%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.284227/  1.777750, val:  71.25%, val_best:  77.92%, tr:  97.96%, tr_best:  98.37%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.270596/  1.765301, val:  76.67%, val_best:  77.92%, tr:  98.26%, tr_best:  98.37%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.287493/  1.893185, val:  65.83%, val_best:  77.92%, tr:  97.55%, tr_best:  98.37%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.287292/  1.836478, val:  75.00%, val_best:  77.92%, tr:  97.75%, tr_best:  98.37%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.230640/  1.772598, val:  77.08%, val_best:  77.92%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.189669/  1.889271, val:  75.83%, val_best:  77.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.171821/  1.871721, val:  76.67%, val_best:  77.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.175313/  2.020895, val:  73.75%, val_best:  77.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.161380/  1.950887, val:  78.75%, val_best:  78.75%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.136374/  2.132795, val:  70.83%, val_best:  78.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.136474/  2.126020, val:  73.33%, val_best:  78.75%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.123623/  2.095488, val:  81.25%, val_best:  81.25%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.095931/  2.092487, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.087311/  2.242603, val:  72.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.091711/  2.155024, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.070759/  2.209466, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.069842/  2.282062, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.065690/  2.277789, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.064365/  2.368451, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.054050/  2.348161, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.050477/  2.396267, val:  76.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.052847/  2.377842, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.052765/  2.505138, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.048272/  2.445440, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.037763/  2.483969, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.034197/  2.503530, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.035505/  2.513982, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.033834/  2.507800, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.032155/  2.542345, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.030229/  2.650914, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.037819/  2.591983, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.032307/  2.632077, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.027821/  2.611564, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.025664/  2.669932, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.020138/  2.701665, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.017722/  2.704563, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.018454/  2.738942, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.020943/  2.754907, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.020699/  2.743416, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.014644/  2.744193, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.013925/  2.782266, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.013341/  2.784113, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.014974/  2.815134, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.014637/  2.863376, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.014899/  2.839279, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.011046/  2.845820, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.010504/  2.882282, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.009059/  2.851941, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.009322/  2.886022, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.009082/  2.876637, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.010420/  2.944585, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.010343/  2.968212, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.009920/  2.913336, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.009822/  2.953783, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.011606/  2.992542, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.010293/  3.044957, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.008098/  3.001204, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.010518/  3.003223, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.010434/  3.068053, val:  76.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.013115/  3.088032, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.007313/  3.058678, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.009401/  3.068687, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.006816/  3.100790, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.007906/  3.069182, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.006290/  3.133502, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.005406/  3.116328, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.006168/  3.127232, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.006102/  3.120913, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.005640/  3.169939, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.006301/  3.153309, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce9c6f0030347f88ac7b9dfc48b134f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▄▅▂▆▇▇▆████▇██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▃▃▃▅▄▄▄▅▆▅▅▅▇▆▆▇▇██▇█████▇████████▇███</td></tr><tr><td>tr_acc</td><td>▁▄▅▅▅▅▆▆▇▇▇█████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▅▅▄▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▃▄▄▅▅▅▅▆▆▆▇▇▇▇▇███████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▃▃▃▅▄▄▄▅▆▅▅▅▇▆▆▇▇██▇█████▇████████▇███</td></tr><tr><td>val_loss</td><td>▂▂▂▂▂▁▂▂▂▃▂▃▃▃▃▄▄▄▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0063</td></tr><tr><td>val_acc_best</td><td>0.81667</td></tr><tr><td>val_acc_now</td><td>0.79583</td></tr><tr><td>val_loss</td><td>3.15331</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-sweep-7</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tt6huood' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tt6huood</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_170034-tt6huood/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b10ci39g with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6163dea00d43b18f9a752af75263f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112897077368365, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_170623-b10ci39g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b10ci39g' target=\"_blank\">feasible-sweep-9</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b10ci39g' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b10ci39g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 14688881ebd6a7fed8e9c9f512432e6a\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.231682/  1.725753, val:  33.75%, val_best:  33.75%, tr:  14.20%, tr_best:  14.20%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.502246/  1.526380, val:  56.25%, val_best:  56.25%, tr:  51.38%, tr_best:  51.38%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.485144/  1.639171, val:  57.08%, val_best:  57.08%, tr:  57.10%, tr_best:  57.10%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.264549/  1.770136, val:  52.92%, val_best:  57.08%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.248263/  1.324783, val:  62.50%, val_best:  62.50%, tr:  62.21%, tr_best:  62.72%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.102589/  1.560552, val:  56.25%, val_best:  62.50%, tr:  68.13%, tr_best:  68.13%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.127105/  1.391897, val:  62.08%, val_best:  62.50%, tr:  67.82%, tr_best:  68.13%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.157054/  1.464881, val:  60.42%, val_best:  62.50%, tr:  67.82%, tr_best:  68.13%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.123250/  1.456251, val:  70.00%, val_best:  70.00%, tr:  70.17%, tr_best:  70.17%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.170959/  1.314588, val:  65.42%, val_best:  70.00%, tr:  71.71%, tr_best:  71.71%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.111409/  1.223261, val:  69.17%, val_best:  70.00%, tr:  73.75%, tr_best:  73.75%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.970058/  1.358844, val:  69.58%, val_best:  70.00%, tr:  76.10%, tr_best:  76.10%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.911124/  1.255785, val:  74.58%, val_best:  74.58%, tr:  76.10%, tr_best:  76.10%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.830981/  1.404512, val:  66.25%, val_best:  74.58%, tr:  81.61%, tr_best:  81.61%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.867591/  1.586107, val:  69.17%, val_best:  74.58%, tr:  79.88%, tr_best:  81.61%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.696896/  1.371113, val:  75.42%, val_best:  75.42%, tr:  87.44%, tr_best:  87.44%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.695881/  1.313211, val:  78.75%, val_best:  78.75%, tr:  87.44%, tr_best:  87.44%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.645288/  1.370996, val:  75.00%, val_best:  78.75%, tr:  91.22%, tr_best:  91.22%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.608394/  1.420346, val:  74.58%, val_best:  78.75%, tr:  92.85%, tr_best:  92.85%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.569320/  1.282387, val:  81.25%, val_best:  81.25%, tr:  93.46%, tr_best:  93.46%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.537541/  1.425604, val:  75.00%, val_best:  81.25%, tr:  93.97%, tr_best:  93.97%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.520091/  1.365157, val:  77.50%, val_best:  81.25%, tr:  93.16%, tr_best:  93.97%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.447665/  1.458992, val:  77.50%, val_best:  81.25%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.486769/  1.445451, val:  83.75%, val_best:  83.75%, tr:  94.48%, tr_best:  96.12%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.419895/  1.405383, val:  83.75%, val_best:  83.75%, tr:  96.63%, tr_best:  96.63%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.387360/  1.488586, val:  78.75%, val_best:  83.75%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.396446/  1.496646, val:  82.08%, val_best:  83.75%, tr:  97.24%, tr_best:  97.65%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.347656/  1.606530, val:  78.33%, val_best:  83.75%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.354080/  1.495938, val:  82.08%, val_best:  83.75%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.326816/  1.728316, val:  77.50%, val_best:  83.75%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.314989/  1.548785, val:  82.08%, val_best:  83.75%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.286499/  1.576852, val:  82.50%, val_best:  83.75%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.379415/  1.764387, val:  75.42%, val_best:  83.75%, tr:  96.42%, tr_best:  99.08%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.294208/  1.791745, val:  80.00%, val_best:  83.75%, tr:  98.57%, tr_best:  99.08%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.280195/  1.779899, val:  80.00%, val_best:  83.75%, tr:  98.77%, tr_best:  99.08%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.259849/  1.654489, val:  85.42%, val_best:  85.42%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.225250/  1.648249, val:  84.58%, val_best:  85.42%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.219818/  1.683023, val:  82.50%, val_best:  85.42%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.221132/  1.722826, val:  85.00%, val_best:  85.42%, tr:  99.18%, tr_best:  99.59%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.219920/  1.733050, val:  85.42%, val_best:  85.42%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.187905/  1.726974, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.186896/  1.821142, val:  85.83%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.171857/  1.825525, val:  83.75%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.157112/  1.833568, val:  83.75%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.155573/  1.922752, val:  80.42%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.149704/  1.890066, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.155295/  1.944295, val:  83.75%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.145255/  1.938594, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.162080/  1.923778, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.122033/  1.910164, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.120061/  1.946535, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.111476/  1.999621, val:  85.42%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.125914/  2.069286, val:  82.92%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.121611/  2.029336, val:  85.00%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.119072/  2.003530, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.099003/  2.111725, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.100748/  2.161664, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.091105/  2.142911, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.081562/  2.164029, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.083714/  2.180006, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.074662/  2.172743, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.077839/  2.196053, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.076606/  2.234782, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.071461/  2.238532, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.089436/  2.279516, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.069939/  2.289559, val:  87.08%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.066738/  2.295385, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.056468/  2.333027, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.061742/  2.374368, val:  87.08%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.050225/  2.401031, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.047095/  2.325894, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.050537/  2.386467, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.041838/  2.464985, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.048119/  2.462368, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.046133/  2.480167, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.045559/  2.485793, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.042508/  2.490080, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.045580/  2.545574, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.041018/  2.587059, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.042720/  2.553449, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.036753/  2.608487, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.037326/  2.615348, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.034386/  2.606033, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.034790/  2.609849, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.041508/  2.590512, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.029394/  2.614373, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.029366/  2.604161, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.033145/  2.642275, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.034232/  2.688861, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.033353/  2.659101, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.028558/  2.669632, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.026406/  2.739908, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.027827/  2.740283, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.026233/  2.731324, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.028431/  2.766816, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.027781/  2.768856, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.019218/  2.781971, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.018251/  2.833440, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.020688/  2.887144, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.023139/  2.857838, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee411c67ee94740bdc1b895e8883628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▅▄▃▆▇▇▇█████▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▅▄▅▆▆▆▇▇▇▇▇▆█▇█▇▇██▇██████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▅▅▆▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▅▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▅▄▅▆▆▆▇▇▇▇▇▆█▇█▇▇██▇██████████████████</td></tr><tr><td>val_loss</td><td>▃▃▁▂▁▁▂▂▁▁▂▂▃▃▃▃▃▄▄▄▄▅▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.02314</td></tr><tr><td>val_acc_best</td><td>0.89583</td></tr><tr><td>val_acc_now</td><td>0.87083</td></tr><tr><td>val_loss</td><td>2.85784</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feasible-sweep-9</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b10ci39g' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b10ci39g</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_170623-b10ci39g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kc2g11em with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_171215-kc2g11em</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kc2g11em' target=\"_blank\">easy-sweep-11</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kc2g11em' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kc2g11em</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 14688881ebd6a7fed8e9c9f512432e6a\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.325213/  2.309355, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.323420/  2.309336, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:   9.50%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.257725/  2.122288, val:  20.83%, val_best:  20.83%, tr:  13.69%, tr_best:  13.69%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.901619/  1.674979, val:  41.25%, val_best:  41.25%, tr:  31.66%, tr_best:  31.66%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.458922/  1.454160, val:  55.00%, val_best:  55.00%, tr:  49.95%, tr_best:  49.95%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.231198/  1.367196, val:  57.50%, val_best:  57.50%, tr:  60.67%, tr_best:  60.67%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.123468/  1.388977, val:  57.92%, val_best:  57.92%, tr:  67.31%, tr_best:  67.31%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.090744/  1.251736, val:  68.33%, val_best:  68.33%, tr:  64.66%, tr_best:  67.31%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.042804/  1.306539, val:  64.17%, val_best:  68.33%, tr:  67.42%, tr_best:  67.42%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.015295/  1.356825, val:  60.42%, val_best:  68.33%, tr:  70.99%, tr_best:  70.99%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.994651/  1.331126, val:  62.50%, val_best:  68.33%, tr:  72.42%, tr_best:  72.42%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.959844/  1.262506, val:  66.67%, val_best:  68.33%, tr:  72.63%, tr_best:  72.63%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.946975/  1.250174, val:  65.83%, val_best:  68.33%, tr:  76.20%, tr_best:  76.20%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.959033/  1.316757, val:  66.25%, val_best:  68.33%, tr:  73.85%, tr_best:  76.20%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.908584/  1.469307, val:  64.58%, val_best:  68.33%, tr:  77.22%, tr_best:  77.22%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.886500/  1.257038, val:  75.42%, val_best:  75.42%, tr:  78.24%, tr_best:  78.24%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.858058/  1.261414, val:  73.75%, val_best:  75.42%, tr:  83.25%, tr_best:  83.25%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.776396/  1.251712, val:  76.67%, val_best:  76.67%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.752413/  1.339310, val:  69.58%, val_best:  76.67%, tr:  87.64%, tr_best:  88.76%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.754779/  1.234095, val:  74.58%, val_best:  76.67%, tr:  87.23%, tr_best:  88.76%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.698534/  1.290398, val:  70.42%, val_best:  76.67%, tr:  91.22%, tr_best:  91.22%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.657178/  1.343645, val:  75.83%, val_best:  76.67%, tr:  92.34%, tr_best:  92.34%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.622477/  1.263137, val:  73.33%, val_best:  76.67%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.621107/  1.236583, val:  79.58%, val_best:  79.58%, tr:  93.67%, tr_best:  94.18%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.552888/  1.253900, val:  80.42%, val_best:  80.42%, tr:  95.51%, tr_best:  95.51%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.533583/  1.266387, val:  77.50%, val_best:  80.42%, tr:  95.20%, tr_best:  95.51%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.555131/  1.232987, val:  82.50%, val_best:  82.50%, tr:  94.89%, tr_best:  95.51%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.500015/  1.283973, val:  80.00%, val_best:  82.50%, tr:  95.10%, tr_best:  95.51%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.476445/  1.221905, val:  84.58%, val_best:  84.58%, tr:  96.94%, tr_best:  96.94%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.443177/  1.324456, val:  78.33%, val_best:  84.58%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.436253/  1.299680, val:  80.83%, val_best:  84.58%, tr:  97.55%, tr_best:  97.85%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.416487/  1.306945, val:  83.75%, val_best:  84.58%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.452150/  1.317830, val:  81.25%, val_best:  84.58%, tr:  95.40%, tr_best:  97.96%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.415049/  1.308843, val:  80.00%, val_best:  84.58%, tr:  97.45%, tr_best:  97.96%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.378151/  1.389603, val:  79.17%, val_best:  84.58%, tr:  97.55%, tr_best:  97.96%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.361775/  1.382216, val:  83.33%, val_best:  84.58%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.313115/  1.350943, val:  81.67%, val_best:  84.58%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.312201/  1.436656, val:  82.92%, val_best:  84.58%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.341139/  1.420843, val:  82.92%, val_best:  84.58%, tr:  98.37%, tr_best:  99.39%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.303823/  1.408807, val:  82.92%, val_best:  84.58%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.275122/  1.424196, val:  84.58%, val_best:  84.58%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.275942/  1.508841, val:  83.75%, val_best:  84.58%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.266124/  1.429088, val:  81.67%, val_best:  84.58%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.248068/  1.534346, val:  84.17%, val_best:  84.58%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.241100/  1.528304, val:  81.25%, val_best:  84.58%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.236505/  1.567416, val:  79.17%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.234601/  1.515483, val:  85.83%, val_best:  85.83%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.212999/  1.553505, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.222763/  1.561135, val:  85.83%, val_best:  85.83%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.196429/  1.564774, val:  83.33%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.186548/  1.606349, val:  83.33%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.177837/  1.635199, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.196435/  1.633104, val:  84.58%, val_best:  85.83%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.160109/  1.666535, val:  84.58%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.153279/  1.635897, val:  85.42%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.148756/  1.677055, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.143410/  1.796874, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.136822/  1.723930, val:  85.42%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.126754/  1.761441, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.126039/  1.849384, val:  80.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.126832/  1.812066, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.125630/  1.784477, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.113742/  1.829388, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.114032/  1.831561, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.105755/  1.882290, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.102055/  1.892741, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.100122/  1.909715, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.101664/  1.877176, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.092751/  1.911234, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.090008/  1.963776, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.085453/  1.948945, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.099124/  1.988902, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.088017/  2.094516, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.091853/  1.927396, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.078182/  2.022930, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.075423/  2.021489, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.070711/  2.046849, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.075020/  2.101609, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.074042/  2.095478, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.081937/  2.085367, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.064002/  2.114022, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.066530/  2.131485, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.062832/  2.119372, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.062291/  2.162821, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.061755/  2.169271, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.061586/  2.197699, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.056691/  2.189718, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.055255/  2.206592, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.056410/  2.163072, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.056890/  2.200509, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.047530/  2.237144, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.047008/  2.272897, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.053081/  2.278822, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.053720/  2.281326, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.048874/  2.316591, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.046392/  2.322711, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.052522/  2.336498, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.052780/  2.307106, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.041956/  2.341093, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.043673/  2.321198, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c43ef7debe648f199c2701ed625b5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▃▅▆▅▇▃▇█▇██▇▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▅▆▆▆▆▇▇▇▇█▇▇███▇▇█████▇████▇██████████</td></tr><tr><td>tr_acc</td><td>▁▁▄▅▆▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▅▆▆▆▆▇▇▇▇█████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▅▆▆▆▆▇▇▇▇█▇▇███▇▇█████▇████▇██████████</td></tr><tr><td>val_loss</td><td>█▇▂▁▂▁▃▁▁▂▁▁▂▂▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▇▆▇▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.04367</td></tr><tr><td>val_acc_best</td><td>0.87917</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>2.3212</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">easy-sweep-11</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kc2g11em' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kc2g11em</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_171215-kc2g11em/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wnwzce5t with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_171848-wnwzce5t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wnwzce5t' target=\"_blank\">good-sweep-13</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wnwzce5t' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wnwzce5t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 14688881ebd6a7fed8e9c9f512432e6a\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 37.486259/ 47.360313, val:  30.00%, val_best:  30.00%, tr:  26.97%, tr_best:  26.97%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 29.989681/ 22.198614, val:  36.67%, val_best:  36.67%, tr:  35.55%, tr_best:  35.55%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 21.643822/ 20.473492, val:  24.58%, val_best:  36.67%, tr:  41.06%, tr_best:  41.06%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 18.301365/ 17.366859, val:  47.50%, val_best:  47.50%, tr:  44.64%, tr_best:  44.64%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 20.244917/ 11.775909, val:  49.17%, val_best:  49.17%, tr:  48.31%, tr_best:  48.31%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 16.090427/ 26.801262, val:  47.50%, val_best:  49.17%, tr:  49.34%, tr_best:  49.34%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 20.685627/ 33.645592, val:  30.00%, val_best:  49.17%, tr:  49.74%, tr_best:  49.74%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 18.305767/ 20.743132, val:  43.33%, val_best:  49.17%, tr:  52.81%, tr_best:  52.81%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 16.264608/ 18.416811, val:  45.00%, val_best:  49.17%, tr:  53.22%, tr_best:  53.22%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 14.925664/ 21.844955, val:  50.83%, val_best:  50.83%, tr:  57.81%, tr_best:  57.81%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 18.188309/ 16.622047, val:  52.08%, val_best:  52.08%, tr:  51.99%, tr_best:  57.81%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 16.705622/ 29.579218, val:  46.67%, val_best:  52.08%, tr:  56.38%, tr_best:  57.81%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 13.926635/ 17.334406, val:  41.67%, val_best:  52.08%, tr:  61.70%, tr_best:  61.70%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 16.987127/ 24.934757, val:  41.25%, val_best:  52.08%, tr:  54.44%, tr_best:  61.70%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 17.228737/ 20.964857, val:  55.83%, val_best:  55.83%, tr:  60.78%, tr_best:  61.70%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss: 14.446385/ 13.778874, val:  58.75%, val_best:  58.75%, tr:  64.76%, tr_best:  64.76%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 14.213074/ 18.672085, val:  50.42%, val_best:  58.75%, tr:  62.21%, tr_best:  64.76%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss: 12.566867/ 12.875074, val:  60.42%, val_best:  60.42%, tr:  60.98%, tr_best:  64.76%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  9.691567/ 16.861668, val:  61.25%, val_best:  61.25%, tr:  68.03%, tr_best:  68.03%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  9.381811/ 22.196638, val:  47.50%, val_best:  61.25%, tr:  69.56%, tr_best:  69.56%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 12.961784/ 26.327480, val:  52.92%, val_best:  61.25%, tr:  67.11%, tr_best:  69.56%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss: 13.641105/ 13.093030, val:  60.83%, val_best:  61.25%, tr:  67.72%, tr_best:  69.56%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss: 11.005843/ 17.672409, val:  60.00%, val_best:  61.25%, tr:  69.15%, tr_best:  69.56%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  9.854492/ 15.451037, val:  61.25%, val_best:  61.25%, tr:  71.30%, tr_best:  71.30%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  7.155700/ 10.185705, val:  74.58%, val_best:  74.58%, tr:  77.02%, tr_best:  77.02%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  6.320178/ 15.098889, val:  57.92%, val_best:  74.58%, tr:  79.16%, tr_best:  79.16%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  7.749578/ 13.499935, val:  58.33%, val_best:  74.58%, tr:  76.30%, tr_best:  79.16%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  6.723845/ 13.793012, val:  60.42%, val_best:  74.58%, tr:  78.35%, tr_best:  79.16%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  8.480736/ 15.880428, val:  65.42%, val_best:  74.58%, tr:  80.59%, tr_best:  80.59%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  6.050873/ 12.464484, val:  63.75%, val_best:  74.58%, tr:  82.64%, tr_best:  82.64%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  6.449759/ 14.766911, val:  63.33%, val_best:  74.58%, tr:  81.31%, tr_best:  82.64%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  4.914611/ 12.338664, val:  67.08%, val_best:  74.58%, tr:  84.27%, tr_best:  84.27%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  4.874726/ 17.718662, val:  62.50%, val_best:  74.58%, tr:  86.93%, tr_best:  86.93%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  4.434342/ 11.840718, val:  70.83%, val_best:  74.58%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  7.725766/ 14.681103, val:  63.33%, val_best:  74.58%, tr:  78.24%, tr_best:  88.76%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  4.987824/ 11.821396, val:  75.00%, val_best:  75.00%, tr:  89.07%, tr_best:  89.07%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  2.979253/ 11.459023, val:  77.08%, val_best:  77.08%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  4.430218/ 17.176769, val:  64.17%, val_best:  77.08%, tr:  87.95%, tr_best:  94.28%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  4.937554/ 14.361374, val:  68.75%, val_best:  77.08%, tr:  90.91%, tr_best:  94.28%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  3.099702/ 11.076162, val:  79.58%, val_best:  79.58%, tr:  92.85%, tr_best:  94.28%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  2.135026/ 13.120961, val:  71.67%, val_best:  79.58%, tr:  95.81%, tr_best:  95.81%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  2.579241/ 10.508093, val:  80.42%, val_best:  80.42%, tr:  95.61%, tr_best:  95.81%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  4.308782/ 12.783544, val:  75.83%, val_best:  80.42%, tr:  90.09%, tr_best:  95.81%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  2.690229/ 13.678204, val:  69.58%, val_best:  80.42%, tr:  95.10%, tr_best:  95.81%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  2.305845/ 11.762297, val:  72.92%, val_best:  80.42%, tr:  95.40%, tr_best:  95.81%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  2.320054/ 12.838637, val:  72.92%, val_best:  80.42%, tr:  94.99%, tr_best:  95.81%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  3.702557/ 14.571051, val:  76.67%, val_best:  80.42%, tr:  92.13%, tr_best:  95.81%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  2.261293/ 12.362100, val:  80.00%, val_best:  80.42%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  1.913882/ 12.265964, val:  75.83%, val_best:  80.42%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  1.570438/ 12.260824, val:  73.75%, val_best:  80.42%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  2.198191/ 14.372191, val:  72.92%, val_best:  80.42%, tr:  95.91%, tr_best:  97.24%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  1.268910/ 11.046528, val:  81.25%, val_best:  81.25%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  1.765951/ 11.931877, val:  77.92%, val_best:  81.25%, tr:  97.24%, tr_best:  99.18%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  1.101592/ 11.367316, val:  77.92%, val_best:  81.25%, tr:  99.08%, tr_best:  99.18%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  1.095461/ 11.477208, val:  77.50%, val_best:  81.25%, tr:  98.88%, tr_best:  99.18%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.882902/ 10.893617, val:  77.08%, val_best:  81.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  1.359778/ 15.104761, val:  67.92%, val_best:  81.25%, tr:  97.04%, tr_best:  99.28%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  1.582675/ 11.392377, val:  80.83%, val_best:  81.25%, tr:  97.34%, tr_best:  99.28%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.865049/ 11.504857, val:  80.00%, val_best:  81.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.684509/ 11.301982, val:  80.42%, val_best:  81.25%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.541119/ 10.974893, val:  78.33%, val_best:  81.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  1.745633/ 11.276907, val:  79.17%, val_best:  81.25%, tr:  96.42%, tr_best:  99.59%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.731071/ 11.559541, val:  80.42%, val_best:  81.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.574709/ 11.165276, val:  80.83%, val_best:  81.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.463653/ 11.213685, val:  79.58%, val_best:  81.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.407403/ 11.738565, val:  78.75%, val_best:  81.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.432679/ 11.309032, val:  84.17%, val_best:  84.17%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.377989/ 10.819386, val:  82.92%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.339944/ 10.643229, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.317580/ 12.373881, val:  72.92%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.283208/ 10.373969, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.487229/ 10.815941, val:  82.92%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.304920/ 11.104212, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.262750/ 10.703748, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.228214/ 11.012471, val:  79.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.386904/ 10.798985, val:  79.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.310668/ 11.388371, val:  80.83%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.353023/ 11.255624, val:  80.42%, val_best:  84.17%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.215948/ 11.156787, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.218547/ 11.037035, val:  84.17%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.229962/ 10.969033, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.261332/ 10.903632, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.239169/ 11.074115, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.312213/ 11.551715, val:  80.83%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.178613/ 10.754430, val:  82.08%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.157501/ 11.019547, val:  81.67%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.155105/ 11.193538, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.189278/ 11.020918, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.117914/ 10.638303, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.100979/ 11.240775, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.146773/ 11.696136, val:  77.50%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.117451/ 10.870626, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.092828/ 10.823297, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.087439/ 10.966693, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.096435/ 10.766436, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.073077/ 11.283202, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.084175/ 11.452435, val:  79.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.131256/ 10.881020, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.092538/ 10.573771, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.100266/ 10.890233, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4676d99695e4d609c9f07c900075e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▂▄▂▃▆▃▅▇▇▇▇▅▇▇███▇████████████████████</td></tr><tr><td>summary_val_acc</td><td>▂▁▄▃▄▃▅▅▄▅▇▅▆▅▇▆▇▇▇█▇▇▇██▇▇███▇█████████</td></tr><tr><td>tr_acc</td><td>▁▂▃▃▄▄▄▄▅▅▆▆▆▇▇▇▇▇██████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▅▄▄▄▄▃▃▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▃▃▄▄▄▅▅▅▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>val_acc_now</td><td>▂▁▄▃▄▃▅▅▄▅▇▅▆▅▇▆▇▇▇█▇▇▇██▇▇███▇█████████</td></tr><tr><td>val_loss</td><td>█▃▁▃▃▂▃▂▃▂▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.10027</td></tr><tr><td>val_acc_best</td><td>0.84167</td></tr><tr><td>val_acc_now</td><td>0.80417</td></tr><tr><td>val_loss</td><td>10.89023</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">good-sweep-13</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wnwzce5t' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wnwzce5t</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_171848-wnwzce5t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 326jkxi9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_172438-326jkxi9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/326jkxi9' target=\"_blank\">trim-sweep-15</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/326jkxi9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/326jkxi9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 4, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = d133da00785cbf60fdc9e42f05c56a11\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.302859/  3.412908, val:  42.92%, val_best:  42.92%, tr:  33.71%, tr_best:  33.71%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.934278/  3.550709, val:  42.08%, val_best:  42.92%, tr:  48.21%, tr_best:  48.21%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  3.343939/  3.358360, val:  57.50%, val_best:  57.50%, tr:  52.81%, tr_best:  52.81%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.544603/  3.477968, val:  51.67%, val_best:  57.50%, tr:  61.90%, tr_best:  61.90%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.331729/  3.028629, val:  60.00%, val_best:  60.00%, tr:  59.86%, tr_best:  61.90%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.052053/  3.349042, val:  55.42%, val_best:  60.00%, tr:  63.02%, tr_best:  63.02%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.830607/  3.327863, val:  57.50%, val_best:  60.00%, tr:  65.68%, tr_best:  65.68%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.619918/  2.919942, val:  58.33%, val_best:  60.00%, tr:  67.42%, tr_best:  67.42%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.588235/  1.925941, val:  66.25%, val_best:  66.25%, tr:  69.87%, tr_best:  69.87%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.260744/  2.449797, val:  58.33%, val_best:  66.25%, tr:  67.11%, tr_best:  69.87%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.348828/  2.947439, val:  57.92%, val_best:  66.25%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.724347/  2.334833, val:  65.83%, val_best:  66.25%, tr:  73.14%, tr_best:  74.26%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.882436/  1.905767, val:  67.92%, val_best:  67.92%, tr:  72.52%, tr_best:  74.26%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.370643/  3.209620, val:  59.17%, val_best:  67.92%, tr:  75.49%, tr_best:  75.49%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.830113/  3.202546, val:  63.75%, val_best:  67.92%, tr:  76.00%, tr_best:  76.00%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  1.408875/  3.661679, val:  59.58%, val_best:  67.92%, tr:  79.16%, tr_best:  79.16%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  1.342881/  2.258662, val:  72.50%, val_best:  72.50%, tr:  80.80%, tr_best:  80.80%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.966214/  2.606873, val:  63.33%, val_best:  72.50%, tr:  86.21%, tr_best:  86.21%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.872597/  2.577256, val:  67.50%, val_best:  72.50%, tr:  85.70%, tr_best:  86.21%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.805932/  2.364895, val:  72.50%, val_best:  72.50%, tr:  89.68%, tr_best:  89.68%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  1.472451/  2.927283, val:  66.67%, val_best:  72.50%, tr:  82.94%, tr_best:  89.68%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  1.046630/  4.166499, val:  65.00%, val_best:  72.50%, tr:  86.31%, tr_best:  89.68%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.699598/  2.427138, val:  74.17%, val_best:  74.17%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.604698/  3.431576, val:  69.17%, val_best:  74.17%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.656644/  2.974936, val:  70.83%, val_best:  74.17%, tr:  93.97%, tr_best:  95.40%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.492734/  3.007691, val:  72.08%, val_best:  74.17%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.662961/  3.696569, val:  67.50%, val_best:  74.17%, tr:  93.67%, tr_best:  95.91%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.573945/  3.263856, val:  76.25%, val_best:  76.25%, tr:  94.48%, tr_best:  95.91%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.423524/  2.967980, val:  76.67%, val_best:  76.67%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.465054/  3.812960, val:  66.25%, val_best:  76.67%, tr:  96.63%, tr_best:  98.06%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.472600/  3.284055, val:  75.83%, val_best:  76.67%, tr:  96.73%, tr_best:  98.06%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.357979/  3.648817, val:  70.00%, val_best:  76.67%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.409212/  3.710295, val:  70.42%, val_best:  76.67%, tr:  97.24%, tr_best:  98.16%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.307308/  4.099652, val:  70.42%, val_best:  76.67%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.361041/  3.897605, val:  75.00%, val_best:  76.67%, tr:  98.88%, tr_best:  99.18%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.237145/  3.958374, val:  75.83%, val_best:  76.67%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.349545/  3.905884, val:  72.50%, val_best:  76.67%, tr:  98.16%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.248054/  4.246842, val:  72.08%, val_best:  76.67%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.248379/  4.042306, val:  75.83%, val_best:  76.67%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.263299/  4.115408, val:  74.58%, val_best:  76.67%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.188741/  4.087319, val:  76.25%, val_best:  76.67%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.196066/  3.986067, val:  77.50%, val_best:  77.50%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.119505/  4.307032, val:  76.67%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.133587/  4.681011, val:  73.75%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.146191/  4.480570, val:  75.42%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.093713/  4.298302, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.109351/  4.382546, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.078002/  4.431213, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.072809/  4.641692, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.087212/  4.726448, val:  76.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.088290/  4.715944, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.090198/  4.821241, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.069093/  5.003589, val:  74.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.077544/  5.093524, val:  75.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.079651/  5.011217, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.113688/  4.876976, val:  77.50%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.076426/  5.003674, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.063422/  4.936464, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.062893/  5.012346, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.057428/  5.091724, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.054230/  5.204537, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.046686/  5.326766, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.044977/  5.346367, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.044098/  5.415892, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.044390/  5.253315, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.045089/  5.368917, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.038295/  5.517083, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.045035/  5.300550, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.040472/  5.400847, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.034541/  5.559175, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.025244/  5.523454, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.031973/  5.546297, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.032010/  5.698973, val:  75.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.032101/  5.569327, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.023666/  5.516685, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.025433/  5.605686, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.023522/  5.654090, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.034045/  5.673500, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.024267/  5.795061, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.032075/  5.893211, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.029690/  5.723267, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.032634/  5.657419, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.036624/  5.722355, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.035356/  5.921021, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.022343/  5.703871, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.030453/  5.925690, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.029735/  5.718921, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.024645/  5.833827, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.033694/  5.928879, val:  76.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.027119/  5.884610, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.041863/  5.970672, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.024756/  5.891173, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.018066/  5.962744, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.037129/  5.864381, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.025937/  5.992049, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.017603/  5.866859, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.019779/  5.993594, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.021962/  5.816669, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.025253/  6.060416, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.022367/  5.926571, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745491e1d9224c20ac3b701ba33f6737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▅▁▅▂▅▅▅▆██▇████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▄▄▄▆▅▅▆▅▆▅▅▆▇▆▇▇▇█▇▇▇█▇▇▇██▇██████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▅▅▅▇▇▇▇▇████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▆█▆▄▆▅▅▃▃▃▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▄▄▄▆▅▅▆▅▆▅▅▆▇▆▇▇▇█▇▇▇█▇▇▇██▇██████████</td></tr><tr><td>val_loss</td><td>▄▄▃▃▂▁▃▂▂▅▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█▇█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.02237</td></tr><tr><td>val_acc_best</td><td>0.8125</td></tr><tr><td>val_acc_now</td><td>0.79583</td></tr><tr><td>val_loss</td><td>5.92657</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trim-sweep-15</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/326jkxi9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/326jkxi9</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_172438-326jkxi9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xhqko1yz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_173045-xhqko1yz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xhqko1yz' target=\"_blank\">kind-sweep-17</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xhqko1yz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xhqko1yz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  9.653196/ 10.097254, val:  36.67%, val_best:  36.67%, tr:  27.78%, tr_best:  27.78%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 13.500366/ 19.029167, val:  40.42%, val_best:  40.42%, tr:  39.22%, tr_best:  39.22%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 13.251966/ 18.831238, val:  38.75%, val_best:  40.42%, tr:  43.82%, tr_best:  43.82%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 10.096898/ 12.330011, val:  37.50%, val_best:  40.42%, tr:  54.44%, tr_best:  54.44%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 12.844937/ 10.707050, val:  57.50%, val_best:  57.50%, tr:  48.42%, tr_best:  54.44%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 11.747345/ 10.752083, val:  45.42%, val_best:  57.50%, tr:  52.60%, tr_best:  54.44%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 10.804217/ 10.838710, val:  52.08%, val_best:  57.50%, tr:  58.02%, tr_best:  58.02%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  9.687930/ 10.745437, val:  45.83%, val_best:  57.50%, tr:  58.84%, tr_best:  58.84%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  6.616214/  8.737102, val:  55.83%, val_best:  57.50%, tr:  61.29%, tr_best:  61.29%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  6.854677/ 13.564594, val:  51.25%, val_best:  57.50%, tr:  59.24%, tr_best:  61.29%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 11.667123/ 11.574658, val:  56.25%, val_best:  57.50%, tr:  57.20%, tr_best:  61.29%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  6.688242/ 16.621876, val:  50.83%, val_best:  57.50%, tr:  65.07%, tr_best:  65.07%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  9.298225/  7.598619, val:  55.83%, val_best:  57.50%, tr:  62.21%, tr_best:  65.07%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  7.955818/ 11.789598, val:  55.00%, val_best:  57.50%, tr:  63.13%, tr_best:  65.07%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  7.568067/ 23.520906, val:  55.00%, val_best:  57.50%, tr:  67.11%, tr_best:  67.11%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  9.104874/  8.831749, val:  57.92%, val_best:  57.92%, tr:  67.72%, tr_best:  67.72%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  8.505612/ 10.450143, val:  61.67%, val_best:  61.67%, tr:  66.09%, tr_best:  67.72%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  8.737367/ 14.046819, val:  61.67%, val_best:  61.67%, tr:  67.01%, tr_best:  67.72%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  7.883191/ 11.304525, val:  57.08%, val_best:  61.67%, tr:  67.93%, tr_best:  67.93%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  6.109890/ 13.016354, val:  55.42%, val_best:  61.67%, tr:  72.73%, tr_best:  72.73%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  8.012065/ 14.626973, val:  55.42%, val_best:  61.67%, tr:  72.01%, tr_best:  72.73%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  9.176218/ 21.189840, val:  47.92%, val_best:  61.67%, tr:  73.03%, tr_best:  73.03%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  6.848104/ 10.231169, val:  62.08%, val_best:  62.08%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  5.939760/ 12.651459, val:  63.75%, val_best:  63.75%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  3.773758/  9.083807, val:  68.33%, val_best:  68.33%, tr:  86.41%, tr_best:  86.41%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  3.540644/ 11.908066, val:  60.00%, val_best:  68.33%, tr:  85.09%, tr_best:  86.41%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  5.072011/ 13.631254, val:  62.92%, val_best:  68.33%, tr:  79.88%, tr_best:  86.41%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  4.181870/  9.727742, val:  67.92%, val_best:  68.33%, tr:  85.19%, tr_best:  86.41%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  4.943829/ 10.343627, val:  67.92%, val_best:  68.33%, tr:  83.04%, tr_best:  86.41%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  2.948871/ 10.418182, val:  64.17%, val_best:  68.33%, tr:  91.01%, tr_best:  91.01%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  3.744893/  9.243084, val:  72.92%, val_best:  72.92%, tr:  88.05%, tr_best:  91.01%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  2.997657/ 10.432480, val:  72.50%, val_best:  72.92%, tr:  91.01%, tr_best:  91.01%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  3.736182/ 11.376549, val:  67.92%, val_best:  72.92%, tr:  89.48%, tr_best:  91.01%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  3.951106/ 11.072464, val:  74.17%, val_best:  74.17%, tr:  89.79%, tr_best:  91.01%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  3.729919/ 11.875252, val:  72.08%, val_best:  74.17%, tr:  90.70%, tr_best:  91.01%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  2.215051/ 10.706600, val:  72.92%, val_best:  74.17%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  1.899392/ 10.593225, val:  75.83%, val_best:  75.83%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  3.073712/ 11.597697, val:  73.33%, val_best:  75.83%, tr:  91.32%, tr_best:  96.42%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  2.649398/ 12.288821, val:  70.42%, val_best:  75.83%, tr:  93.77%, tr_best:  96.42%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  2.252382/  9.992141, val:  81.25%, val_best:  81.25%, tr:  95.81%, tr_best:  96.42%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  1.967542/ 12.077719, val:  72.50%, val_best:  81.25%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  2.279192/ 11.524372, val:  76.67%, val_best:  81.25%, tr:  95.71%, tr_best:  96.42%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  1.650298/ 11.542340, val:  79.17%, val_best:  81.25%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  1.558964/ 12.879250, val:  73.33%, val_best:  81.25%, tr:  97.55%, tr_best:  97.75%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  1.675619/ 11.835859, val:  77.08%, val_best:  81.25%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  1.485490/ 10.884000, val:  80.42%, val_best:  81.25%, tr:  97.55%, tr_best:  98.06%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  1.100401/ 11.107017, val:  78.75%, val_best:  81.25%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  1.046814/ 10.986785, val:  77.50%, val_best:  81.25%, tr:  98.57%, tr_best:  99.08%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  1.374043/ 11.403273, val:  77.50%, val_best:  81.25%, tr:  98.37%, tr_best:  99.08%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  1.300975/ 11.769787, val:  78.75%, val_best:  81.25%, tr:  98.77%, tr_best:  99.08%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  1.033007/ 11.468337, val:  78.75%, val_best:  81.25%, tr:  98.88%, tr_best:  99.08%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  1.276465/ 12.208409, val:  78.75%, val_best:  81.25%, tr:  98.06%, tr_best:  99.08%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  1.284381/ 12.564207, val:  76.25%, val_best:  81.25%, tr:  98.37%, tr_best:  99.08%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  1.246612/ 11.975701, val:  78.75%, val_best:  81.25%, tr:  98.77%, tr_best:  99.08%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.813868/ 12.128924, val:  77.50%, val_best:  81.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.766216/ 11.463889, val:  83.33%, val_best:  83.33%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.606509/ 12.916757, val:  77.08%, val_best:  83.33%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.699726/ 11.420275, val:  78.75%, val_best:  83.33%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.616189/ 12.316587, val:  77.08%, val_best:  83.33%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.479696/ 12.225976, val:  82.08%, val_best:  83.33%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.544689/ 11.693621, val:  81.25%, val_best:  83.33%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  1.034104/ 12.515895, val:  84.17%, val_best:  84.17%, tr:  97.55%, tr_best:  99.90%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.685849/ 13.535407, val:  77.08%, val_best:  84.17%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.411147/ 12.401801, val:  81.67%, val_best:  84.17%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.555963/ 11.869786, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.451362/ 12.969174, val:  79.58%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.386187/ 12.750044, val:  80.83%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.311403/ 12.225190, val:  84.58%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.344457/ 12.533840, val:  82.50%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.299074/ 13.248019, val:  78.75%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.274051/ 12.727768, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.271420/ 12.869834, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.318471/ 12.970343, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.287950/ 13.301082, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.298091/ 13.387002, val:  84.17%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.208035/ 13.383734, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.237871/ 14.123357, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.179967/ 14.033495, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.225121/ 13.369085, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.271864/ 12.641840, val:  83.75%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.228743/ 13.915012, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.233050/ 13.337573, val:  83.33%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.172063/ 13.554894, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.176350/ 13.873741, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.162547/ 13.897169, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.161062/ 13.402309, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.115616/ 13.941490, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.163825/ 13.222014, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.160323/ 13.763910, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.204032/ 14.357250, val:  80.00%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.192405/ 14.752494, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.166992/ 13.900298, val:  80.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.088670/ 13.641619, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.118950/ 13.667482, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.377345/ 13.623072, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.175663/ 14.405427, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.144682/ 15.308968, val:  79.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.227335/ 14.638730, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.141051/ 14.152225, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.115073/ 14.341102, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dc9db548034c5d874b29457277d690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▃▅▂▄▅▅▄▆▇▆█▇▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▄▂▃▄▄▅▄▃▆▅▅▆▆▆█▇▇▇▇▇▇▇██▇████▇████▇███</td></tr><tr><td>tr_acc</td><td>▁▃▃▄▄▄▅▅▅▅▇▆▇▇█▇████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▆██▆▅▆▅▆▄▆▃▄▃▃▂▃▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▄▂▃▄▄▅▄▃▆▅▅▆▆▆█▇▇▇▇▇▇▇██▇████▇████▇███</td></tr><tr><td>val_loss</td><td>▂▆▂▂▄▁█▄▃▇▂▄▂▃▂▃▂▃▃▂▃▃▃▃▃▃▃▃▃▃▄▄▃▄▄▃▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.11507</td></tr><tr><td>val_acc_best</td><td>0.85833</td></tr><tr><td>val_acc_now</td><td>0.81667</td></tr><tr><td>val_loss</td><td>14.3411</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">kind-sweep-17</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xhqko1yz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xhqko1yz</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_173045-xhqko1yz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: obuy3izw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_173711-obuy3izw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/obuy3izw' target=\"_blank\">deft-sweep-19</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/obuy3izw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/obuy3izw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = b57dfc72d05d304ce829f424a70e455b\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.325213/  2.309355, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.311454/  2.228657, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  10.32%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.739595/  1.473186, val:  55.83%, val_best:  55.83%, tr:  38.30%, tr_best:  38.30%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.247396/  1.397947, val:  58.33%, val_best:  58.33%, tr:  61.59%, tr_best:  61.59%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.141421/  1.240973, val:  65.42%, val_best:  65.42%, tr:  62.41%, tr_best:  62.41%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.054635/  1.236996, val:  62.92%, val_best:  65.42%, tr:  66.39%, tr_best:  66.39%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.999431/  1.272384, val:  60.42%, val_best:  65.42%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.019566/  1.231767, val:  57.08%, val_best:  65.42%, tr:  65.27%, tr_best:  69.36%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.949083/  1.212868, val:  61.25%, val_best:  65.42%, tr:  70.79%, tr_best:  70.79%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.903196/  1.313779, val:  61.25%, val_best:  65.42%, tr:  73.44%, tr_best:  73.44%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.930804/  1.301757, val:  63.33%, val_best:  65.42%, tr:  71.60%, tr_best:  73.44%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.897585/  1.207894, val:  65.00%, val_best:  65.42%, tr:  72.32%, tr_best:  73.44%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.849210/  1.177624, val:  72.50%, val_best:  72.50%, tr:  75.59%, tr_best:  75.59%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.839344/  1.203533, val:  69.17%, val_best:  72.50%, tr:  77.83%, tr_best:  77.83%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.799645/  1.377332, val:  67.50%, val_best:  72.50%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.770307/  1.226705, val:  67.92%, val_best:  72.50%, tr:  81.51%, tr_best:  81.51%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.746914/  1.229360, val:  65.83%, val_best:  72.50%, tr:  82.64%, tr_best:  82.64%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.706755/  1.229360, val:  75.00%, val_best:  75.00%, tr:  87.33%, tr_best:  87.33%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.706907/  1.301111, val:  70.83%, val_best:  75.00%, tr:  85.60%, tr_best:  87.33%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.659280/  1.306475, val:  67.08%, val_best:  75.00%, tr:  87.95%, tr_best:  87.95%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.641333/  1.282955, val:  68.33%, val_best:  75.00%, tr:  88.05%, tr_best:  88.05%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.588022/  1.401309, val:  72.50%, val_best:  75.00%, tr:  91.01%, tr_best:  91.01%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.552848/  1.355279, val:  69.17%, val_best:  75.00%, tr:  93.46%, tr_best:  93.46%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.555915/  1.276842, val:  74.58%, val_best:  75.00%, tr:  93.05%, tr_best:  93.46%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.481431/  1.360800, val:  73.33%, val_best:  75.00%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.464557/  1.322540, val:  73.33%, val_best:  75.00%, tr:  96.32%, tr_best:  97.55%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.501920/  1.322080, val:  75.42%, val_best:  75.42%, tr:  93.77%, tr_best:  97.55%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.442563/  1.371041, val:  76.25%, val_best:  76.25%, tr:  96.32%, tr_best:  97.55%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.406191/  1.361892, val:  80.42%, val_best:  80.42%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.358996/  1.420671, val:  77.92%, val_best:  80.42%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.336345/  1.462152, val:  75.42%, val_best:  80.42%, tr:  98.47%, tr_best:  98.98%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.349780/  1.469366, val:  77.50%, val_best:  80.42%, tr:  98.47%, tr_best:  98.98%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.380411/  1.515996, val:  73.75%, val_best:  80.42%, tr:  96.42%, tr_best:  98.98%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.330683/  1.561583, val:  73.33%, val_best:  80.42%, tr:  98.26%, tr_best:  98.98%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.299966/  1.580926, val:  74.17%, val_best:  80.42%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.267480/  1.586757, val:  76.25%, val_best:  80.42%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.258263/  1.613869, val:  77.92%, val_best:  80.42%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.234935/  1.707664, val:  72.08%, val_best:  80.42%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.246771/  1.635147, val:  79.58%, val_best:  80.42%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.223261/  1.686643, val:  77.08%, val_best:  80.42%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.207724/  1.678595, val:  78.75%, val_best:  80.42%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.209221/  1.737066, val:  78.75%, val_best:  80.42%, tr:  99.08%, tr_best:  99.80%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.164777/  1.756897, val:  77.08%, val_best:  80.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.167575/  1.806901, val:  75.83%, val_best:  80.42%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.166767/  1.851166, val:  75.42%, val_best:  80.42%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.149593/  1.841962, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.155190/  1.882112, val:  77.08%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.146981/  1.893307, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.137213/  1.886092, val:  79.17%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.107688/  1.946027, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.110227/  1.986068, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.111725/  1.969123, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.114189/  1.997915, val:  78.75%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.093170/  2.049012, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.085269/  2.073539, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.092411/  2.083252, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.087624/  2.171304, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.082302/  2.118170, val:  77.08%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.070869/  2.206164, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.070637/  2.224032, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.070462/  2.201317, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.065300/  2.223187, val:  75.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.063555/  2.267654, val:  75.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.061092/  2.249075, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.057068/  2.288331, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.058538/  2.285604, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.060048/  2.269938, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.052063/  2.313932, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.049474/  2.334063, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.041292/  2.401662, val:  75.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.043128/  2.409034, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.047287/  2.363981, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.036639/  2.412328, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.040258/  2.424603, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.041799/  2.477913, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.039315/  2.434457, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.041491/  2.507374, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.040074/  2.491115, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.047334/  2.485661, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.034388/  2.516891, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.030014/  2.526033, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.027812/  2.569015, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.029412/  2.557855, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.022586/  2.593298, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.023568/  2.566650, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.025188/  2.595925, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.025336/  2.673548, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.029145/  2.678900, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.021430/  2.654994, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.019397/  2.694521, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.017163/  2.683451, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.023512/  2.767524, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.024020/  2.744335, val:  77.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.030614/  2.733993, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.021289/  2.732451, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.021431/  2.736974, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.020220/  2.751581, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.019886/  2.722641, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.018557/  2.747385, val:  77.08%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.016868/  2.799143, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be0d9b86a3d4f6d90e0a6cdb6c4e550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄▅▅▅▇▃████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▆▇▇▇▇▇▇▇█▇▇▇██▇██████▇██████████████</td></tr><tr><td>tr_acc</td><td>▁▃▅▅▆▆▇▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▆▇▇▇▇▇▇▇█▇▇▇██▇██████▇██████████████</td></tr><tr><td>val_loss</td><td>▆▂▁▁▂▁▂▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.01687</td></tr><tr><td>val_acc_best</td><td>0.81667</td></tr><tr><td>val_acc_now</td><td>0.77917</td></tr><tr><td>val_loss</td><td>2.79914</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deft-sweep-19</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/obuy3izw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/obuy3izw</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_173711-obuy3izw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9f2mjbet with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5915dc100b8c4556b3a9b422c04aa94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112857677249444, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_174317-9f2mjbet</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9f2mjbet' target=\"_blank\">wandering-sweep-21</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9f2mjbet' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9f2mjbet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  2.522145/  2.624366, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  10.21%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  2.493285/  2.401646, val:  10.00%, val_best:  10.00%, tr:   7.76%, tr_best:  10.21%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  2.471026/  2.441473, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  10.21%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  2.438038/  2.464993, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.21%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  2.435990/  2.474837, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.21%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  2.405337/  2.524804, val:  19.17%, val_best:  19.17%, tr:  14.50%, tr_best:  14.50%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  2.350922/  2.048293, val:  26.67%, val_best:  26.67%, tr:  27.07%, tr_best:  27.07%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  2.031246/  2.180416, val:  38.33%, val_best:  38.33%, tr:  30.13%, tr_best:  30.13%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  1.805076/  1.702990, val:  42.50%, val_best:  42.50%, tr:  41.98%, tr_best:  41.98%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  1.683137/  1.814878, val:  42.08%, val_best:  42.50%, tr:  46.27%, tr_best:  46.27%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  1.654888/  1.818674, val:  44.17%, val_best:  44.17%, tr:  44.84%, tr_best:  46.27%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  1.722285/  1.805489, val:  43.75%, val_best:  44.17%, tr:  45.56%, tr_best:  46.27%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  1.617972/  1.705088, val:  43.75%, val_best:  44.17%, tr:  46.07%, tr_best:  46.27%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  1.676149/  2.381062, val:  40.00%, val_best:  44.17%, tr:  44.74%, tr_best:  46.27%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  1.831298/  2.466507, val:  47.08%, val_best:  47.08%, tr:  52.60%, tr_best:  52.60%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  1.750834/  2.314839, val:  46.25%, val_best:  47.08%, tr:  52.50%, tr_best:  52.60%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  1.773410/  1.892347, val:  44.58%, val_best:  47.08%, tr:  52.71%, tr_best:  52.71%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  1.810614/  2.256142, val:  45.83%, val_best:  47.08%, tr:  49.74%, tr_best:  52.71%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  1.823092/  1.741067, val:  55.42%, val_best:  55.42%, tr:  51.69%, tr_best:  52.71%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  1.580151/  1.994727, val:  42.50%, val_best:  55.42%, tr:  58.73%, tr_best:  58.73%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  2.019428/  2.104275, val:  40.00%, val_best:  55.42%, tr:  52.91%, tr_best:  58.73%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  1.769196/  1.790228, val:  52.50%, val_best:  55.42%, tr:  56.89%, tr_best:  58.73%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  1.651305/  2.146793, val:  45.83%, val_best:  55.42%, tr:  60.37%, tr_best:  60.37%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  1.755105/  2.079033, val:  52.92%, val_best:  55.42%, tr:  54.75%, tr_best:  60.37%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  1.604272/  2.041589, val:  53.75%, val_best:  55.42%, tr:  62.82%, tr_best:  62.82%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  1.965481/  2.659870, val:  55.00%, val_best:  55.42%, tr:  58.32%, tr_best:  62.82%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  1.652635/  2.540297, val:  47.92%, val_best:  55.42%, tr:  65.99%, tr_best:  65.99%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  1.532820/  1.747498, val:  52.08%, val_best:  55.42%, tr:  63.33%, tr_best:  65.99%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  1.548967/  2.005390, val:  51.67%, val_best:  55.42%, tr:  65.58%, tr_best:  65.99%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  1.357023/  2.090022, val:  50.00%, val_best:  55.42%, tr:  65.27%, tr_best:  65.99%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  1.591093/  1.818678, val:  49.17%, val_best:  55.42%, tr:  64.56%, tr_best:  65.99%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  1.398354/  2.887241, val:  44.17%, val_best:  55.42%, tr:  66.39%, tr_best:  66.39%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  1.948865/  2.971230, val:  54.58%, val_best:  55.42%, tr:  62.41%, tr_best:  66.39%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  1.587932/  1.780280, val:  58.75%, val_best:  58.75%, tr:  63.43%, tr_best:  66.39%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  1.716771/  2.338485, val:  58.33%, val_best:  58.75%, tr:  62.10%, tr_best:  66.39%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  1.504334/  1.989291, val:  59.17%, val_best:  59.17%, tr:  66.50%, tr_best:  66.50%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  1.393039/  2.175965, val:  59.17%, val_best:  59.17%, tr:  69.46%, tr_best:  69.46%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  1.899648/  3.061460, val:  48.75%, val_best:  59.17%, tr:  66.50%, tr_best:  69.46%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  1.400572/  1.903320, val:  51.67%, val_best:  59.17%, tr:  68.85%, tr_best:  69.46%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  1.387575/  1.770079, val:  60.00%, val_best:  60.00%, tr:  66.39%, tr_best:  69.46%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  1.173629/  2.399335, val:  55.42%, val_best:  60.00%, tr:  72.52%, tr_best:  72.52%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  1.325052/  1.733581, val:  62.92%, val_best:  62.92%, tr:  72.83%, tr_best:  72.83%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  1.259946/  2.337015, val:  46.67%, val_best:  62.92%, tr:  70.28%, tr_best:  72.83%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  1.329895/  1.909890, val:  60.83%, val_best:  62.92%, tr:  69.77%, tr_best:  72.83%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  1.366193/  2.047247, val:  58.33%, val_best:  62.92%, tr:  71.09%, tr_best:  72.83%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  1.326777/  1.952389, val:  66.67%, val_best:  66.67%, tr:  73.44%, tr_best:  73.44%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  1.536085/  2.259460, val:  55.00%, val_best:  66.67%, tr:  69.66%, tr_best:  73.44%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  1.363365/  2.370815, val:  55.42%, val_best:  66.67%, tr:  73.03%, tr_best:  73.44%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  2.067288/  2.019950, val:  61.25%, val_best:  66.67%, tr:  65.78%, tr_best:  73.44%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  1.262508/  1.770099, val:  62.92%, val_best:  66.67%, tr:  73.95%, tr_best:  73.95%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  1.135287/  2.259312, val:  59.17%, val_best:  66.67%, tr:  74.97%, tr_best:  74.97%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  1.130038/  2.025950, val:  56.25%, val_best:  66.67%, tr:  78.55%, tr_best:  78.55%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  1.499805/  2.358684, val:  62.92%, val_best:  66.67%, tr:  71.81%, tr_best:  78.55%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  1.167776/  2.012892, val:  57.50%, val_best:  66.67%, tr:  76.40%, tr_best:  78.55%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  1.157225/  1.764589, val:  63.33%, val_best:  66.67%, tr:  76.20%, tr_best:  78.55%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  1.111546/  2.061407, val:  47.08%, val_best:  66.67%, tr:  77.73%, tr_best:  78.55%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  1.535544/  2.444960, val:  57.08%, val_best:  66.67%, tr:  70.99%, tr_best:  78.55%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  1.643603/  2.775775, val:  47.08%, val_best:  66.67%, tr:  71.40%, tr_best:  78.55%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  1.313424/  2.216262, val:  58.33%, val_best:  66.67%, tr:  75.49%, tr_best:  78.55%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  1.422314/  2.748458, val:  57.50%, val_best:  66.67%, tr:  73.75%, tr_best:  78.55%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  1.252817/  2.254047, val:  53.75%, val_best:  66.67%, tr:  77.22%, tr_best:  78.55%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  1.268001/  2.223155, val:  65.83%, val_best:  66.67%, tr:  75.89%, tr_best:  78.55%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  1.255988/  2.132947, val:  55.83%, val_best:  66.67%, tr:  75.18%, tr_best:  78.55%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  1.299906/  2.148556, val:  65.42%, val_best:  66.67%, tr:  77.02%, tr_best:  78.55%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  1.305173/  2.300456, val:  56.25%, val_best:  66.67%, tr:  77.02%, tr_best:  78.55%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  1.476834/  2.066457, val:  69.58%, val_best:  69.58%, tr:  77.02%, tr_best:  78.55%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  1.239510/  2.052438, val:  64.17%, val_best:  69.58%, tr:  78.96%, tr_best:  78.96%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  1.659282/  2.820800, val:  55.00%, val_best:  69.58%, tr:  72.11%, tr_best:  78.96%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  1.370266/  2.082202, val:  62.50%, val_best:  69.58%, tr:  77.83%, tr_best:  78.96%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  1.114063/  2.164150, val:  61.25%, val_best:  69.58%, tr:  79.88%, tr_best:  79.88%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  1.273173/  2.829848, val:  65.83%, val_best:  69.58%, tr:  81.41%, tr_best:  81.41%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  1.235030/  2.143695, val:  62.92%, val_best:  69.58%, tr:  82.33%, tr_best:  82.33%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  1.253505/  2.040303, val:  71.25%, val_best:  71.25%, tr:  77.53%, tr_best:  82.33%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  1.080754/  2.306112, val:  61.25%, val_best:  71.25%, tr:  82.64%, tr_best:  82.64%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.938502/  2.183710, val:  57.08%, val_best:  71.25%, tr:  86.41%, tr_best:  86.41%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.976005/  2.264148, val:  62.92%, val_best:  71.25%, tr:  85.39%, tr_best:  86.41%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  1.073269/  2.170212, val:  59.17%, val_best:  71.25%, tr:  83.55%, tr_best:  86.41%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  1.248705/  3.013535, val:  52.50%, val_best:  71.25%, tr:  79.26%, tr_best:  86.41%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  1.577183/  2.600541, val:  59.58%, val_best:  71.25%, tr:  78.45%, tr_best:  86.41%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  1.195257/  2.682153, val:  57.50%, val_best:  71.25%, tr:  81.31%, tr_best:  86.41%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  1.050668/  2.141177, val:  62.50%, val_best:  71.25%, tr:  84.98%, tr_best:  86.41%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.911185/  2.091372, val:  63.75%, val_best:  71.25%, tr:  86.82%, tr_best:  86.82%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.945738/  2.375965, val:  53.75%, val_best:  71.25%, tr:  86.72%, tr_best:  86.82%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.955315/  2.204332, val:  67.50%, val_best:  71.25%, tr:  86.11%, tr_best:  86.82%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.918726/  1.773073, val:  79.17%, val_best:  79.17%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  1.167544/  2.247759, val:  65.00%, val_best:  79.17%, tr:  82.84%, tr_best:  88.56%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.977090/  2.035621, val:  65.00%, val_best:  79.17%, tr:  84.88%, tr_best:  88.56%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.773367/  2.220483, val:  61.67%, val_best:  79.17%, tr:  89.68%, tr_best:  89.68%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.807557/  1.866566, val:  65.42%, val_best:  79.17%, tr:  90.50%, tr_best:  90.50%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.922030/  2.174705, val:  65.83%, val_best:  79.17%, tr:  86.41%, tr_best:  90.50%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.888329/  2.081197, val:  67.50%, val_best:  79.17%, tr:  87.23%, tr_best:  90.50%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.857310/  2.621269, val:  47.50%, val_best:  79.17%, tr:  87.64%, tr_best:  90.50%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.831761/  2.406146, val:  54.58%, val_best:  79.17%, tr:  87.64%, tr_best:  90.50%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  1.049443/  2.191012, val:  70.83%, val_best:  79.17%, tr:  84.07%, tr_best:  90.50%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.887936/  2.175632, val:  68.33%, val_best:  79.17%, tr:  87.84%, tr_best:  90.50%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.933590/  1.960833, val:  71.25%, val_best:  79.17%, tr:  85.90%, tr_best:  90.50%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.982836/  2.507138, val:  57.08%, val_best:  79.17%, tr:  87.54%, tr_best:  90.50%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  1.069621/  2.295491, val:  65.42%, val_best:  79.17%, tr:  86.01%, tr_best:  90.50%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.927086/  2.198703, val:  57.08%, val_best:  79.17%, tr:  87.33%, tr_best:  90.50%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.892053/  2.208448, val:  69.58%, val_best:  79.17%, tr:  88.15%, tr_best:  90.50%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2306f9e0c2254e0f85cbb392a1217a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▁▃▃▂▄▅▅▅▅▆▇▆▇▅▅▆▆▅▇▇▅▇▆▅▅▇▇▇▇▇▆▇█▇▇▇▅▇</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▄▄▄▅▅▄▅▅▅▅▆▆▅▆▅▆▆▆▆▆▅▆▇▇▆▇▇▆▅▆▅█▆▇▆▇▇</td></tr><tr><td>tr_acc</td><td>▁▁▁▃▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▆▇▆▇▇▇▆▇▇█▇▇███████</td></tr><tr><td>tr_epoch_loss</td><td>███▆▅▄▅▅▄▅▄▅▃▆▄▆▃▃▃▃▃▄▃▄▄▃▄▅▃▃▂▃▃▂▂▁▂▁▂▂</td></tr><tr><td>val_acc_best</td><td>▁▁▁▄▄▄▅▅▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>val_acc_now</td><td>▁▁▁▄▄▄▅▅▄▅▅▅▅▆▆▅▆▅▆▆▆▆▆▅▆▇▇▆▇▇▆▅▆▅█▆▇▆▇▇</td></tr><tr><td>val_loss</td><td>▆▅▅▃▂▁▅▄▂▁▃▅▃█▂█▁▄▃▄▁▄▁▇▆▄▃▇▇▃▄█▆▄▁▄▃▅▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.88151</td></tr><tr><td>tr_epoch_loss</td><td>0.89205</td></tr><tr><td>val_acc_best</td><td>0.79167</td></tr><tr><td>val_acc_now</td><td>0.69583</td></tr><tr><td>val_loss</td><td>2.20845</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wandering-sweep-21</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9f2mjbet' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9f2mjbet</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_174317-9f2mjbet/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5zf2ra6j with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_174837-5zf2ra6j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5zf2ra6j' target=\"_blank\">royal-sweep-23</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5zf2ra6j' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5zf2ra6j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.118630/  2.260622, val:  45.83%, val_best:  45.83%, tr:  25.64%, tr_best:  25.64%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.835387/  2.070286, val:  52.50%, val_best:  52.50%, tr:  51.69%, tr_best:  51.69%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.110072/  1.690472, val:  57.50%, val_best:  57.50%, tr:  52.91%, tr_best:  52.91%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.450750/  2.454147, val:  55.83%, val_best:  57.50%, tr:  63.13%, tr_best:  63.13%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.784493/  2.040477, val:  62.50%, val_best:  62.50%, tr:  59.24%, tr_best:  63.13%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.605146/  2.117920, val:  51.25%, val_best:  62.50%, tr:  66.60%, tr_best:  66.60%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.359972/  1.568491, val:  65.42%, val_best:  65.42%, tr:  67.31%, tr_best:  67.31%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.703038/  2.199175, val:  55.42%, val_best:  65.42%, tr:  66.19%, tr_best:  67.31%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.326315/  1.882868, val:  60.00%, val_best:  65.42%, tr:  69.15%, tr_best:  69.15%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.719675/  2.055604, val:  60.83%, val_best:  65.42%, tr:  67.11%, tr_best:  69.15%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.395628/  1.353487, val:  69.17%, val_best:  69.17%, tr:  72.42%, tr_best:  72.42%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.276975/  1.795898, val:  66.67%, val_best:  69.17%, tr:  76.61%, tr_best:  76.61%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.294215/  1.533216, val:  76.25%, val_best:  76.25%, tr:  74.36%, tr_best:  76.61%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.895641/  1.847525, val:  62.08%, val_best:  76.25%, tr:  83.55%, tr_best:  83.55%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.048416/  2.280606, val:  69.58%, val_best:  76.25%, tr:  80.59%, tr_best:  83.55%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.801196/  1.673741, val:  76.25%, val_best:  76.25%, tr:  87.74%, tr_best:  87.74%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.744822/  1.491428, val:  80.83%, val_best:  80.83%, tr:  90.19%, tr_best:  90.19%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.722692/  1.662414, val:  78.75%, val_best:  80.83%, tr:  90.19%, tr_best:  90.19%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.603775/  1.834073, val:  78.75%, val_best:  80.83%, tr:  93.16%, tr_best:  93.16%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.656492/  1.740478, val:  75.42%, val_best:  80.83%, tr:  90.91%, tr_best:  93.16%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.565633/  2.085495, val:  67.50%, val_best:  80.83%, tr:  92.85%, tr_best:  93.16%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.637279/  1.814546, val:  82.08%, val_best:  82.08%, tr:  92.24%, tr_best:  93.16%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.492107/  1.897689, val:  78.33%, val_best:  82.08%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.556372/  2.232617, val:  77.92%, val_best:  82.08%, tr:  93.77%, tr_best:  95.91%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.431352/  1.832140, val:  83.33%, val_best:  83.33%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.380542/  1.847352, val:  80.42%, val_best:  83.33%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.368542/  2.065606, val:  77.92%, val_best:  83.33%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.346245/  1.968446, val:  81.67%, val_best:  83.33%, tr:  97.75%, tr_best:  98.26%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.268882/  1.869810, val:  83.75%, val_best:  83.75%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.233925/  2.127951, val:  80.42%, val_best:  83.75%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.222281/  2.047245, val:  81.67%, val_best:  83.75%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.219963/  2.166744, val:  81.25%, val_best:  83.75%, tr:  99.39%, tr_best:  99.59%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.387197/  2.113506, val:  84.17%, val_best:  84.17%, tr:  96.22%, tr_best:  99.59%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.286503/  2.501651, val:  78.75%, val_best:  84.17%, tr:  98.47%, tr_best:  99.59%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.231310/  2.275326, val:  81.25%, val_best:  84.17%, tr:  99.28%, tr_best:  99.59%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.183276/  2.146365, val:  84.58%, val_best:  84.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.185402/  2.122126, val:  85.83%, val_best:  85.83%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.158497/  2.249607, val:  81.67%, val_best:  85.83%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.170619/  2.206182, val:  85.00%, val_best:  85.83%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.185316/  2.298602, val:  85.00%, val_best:  85.83%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.118102/  2.256176, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.130020/  2.401497, val:  84.58%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.125578/  2.323215, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.120794/  2.474031, val:  84.58%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.117246/  2.424778, val:  84.58%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.105547/  2.464129, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.093672/  2.592875, val:  83.75%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.083675/  2.462355, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.091911/  2.638187, val:  82.08%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.076571/  2.568453, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.065982/  2.570876, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.064255/  2.613099, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.075544/  2.643060, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.060117/  2.642920, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.055099/  2.613288, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.045379/  2.736614, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.046131/  2.807177, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.040971/  2.823308, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.046735/  2.755610, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.038606/  2.833162, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.042423/  2.793387, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.045111/  2.868954, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.043766/  2.935458, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.039138/  2.941228, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.046062/  2.904928, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.035126/  2.936387, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.030522/  2.981071, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.026349/  2.990866, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.023638/  3.068464, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.026186/  3.020876, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.021367/  3.034455, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.025542/  3.033890, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.024869/  3.078874, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.023677/  3.168297, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.023436/  3.110564, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.021014/  3.126444, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.021796/  3.215864, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.021240/  3.175078, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.021605/  3.126977, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.024834/  3.165802, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.023670/  3.280050, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.019662/  3.217168, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.017422/  3.254967, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.026119/  3.272835, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.017190/  3.148884, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.018827/  3.280258, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.018452/  3.241825, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.018758/  3.326508, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.021228/  3.316478, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.021075/  3.259895, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.023188/  3.368752, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.024026/  3.306506, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.018627/  3.346339, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.021713/  3.286166, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.019516/  3.284299, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.015707/  3.308300, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.014047/  3.367149, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.011097/  3.395116, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.013036/  3.342820, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.014194/  3.396143, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9121b9395ab4968af92b36fcc0e32c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▅▂▆▇▇█▇██████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▃▄▆▅▇▆▇▇▆▇██▇████████████████████▇██▇</td></tr><tr><td>tr_acc</td><td>▁▄▄▅▅▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▇▇▇▅▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▄▆▆▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▃▄▆▅▇▆▇▇▆▇██▇████████████████████▇██▇</td></tr><tr><td>val_loss</td><td>▄▂▃▄▃▁▄▁▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.01419</td></tr><tr><td>val_acc_best</td><td>0.875</td></tr><tr><td>val_acc_now</td><td>0.84583</td></tr><tr><td>val_loss</td><td>3.39614</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">royal-sweep-23</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5zf2ra6j' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5zf2ra6j</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_174837-5zf2ra6j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k5etho08 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_175439-k5etho08</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k5etho08' target=\"_blank\">fancy-sweep-25</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k5etho08' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k5etho08</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 14688881ebd6a7fed8e9c9f512432e6a\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 15.226648/ 20.444435, val:  36.67%, val_best:  36.67%, tr:  26.15%, tr_best:  26.15%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 17.933933/ 17.818573, val:  53.75%, val_best:  53.75%, tr:  40.55%, tr_best:  40.55%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 18.836060/ 22.720171, val:  40.42%, val_best:  53.75%, tr:  44.02%, tr_best:  44.02%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 13.965033/ 23.528444, val:  34.17%, val_best:  53.75%, tr:  49.85%, tr_best:  49.85%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 16.599781/ 11.429966, val:  42.92%, val_best:  53.75%, tr:  46.78%, tr_best:  49.85%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 11.560188/ 13.498363, val:  53.33%, val_best:  53.75%, tr:  53.42%, tr_best:  53.42%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 12.102043/ 14.170801, val:  55.00%, val_best:  55.00%, tr:  55.06%, tr_best:  55.06%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 14.782983/ 20.552420, val:  45.42%, val_best:  55.00%, tr:  50.36%, tr_best:  55.06%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  8.848624/ 11.316558, val:  55.42%, val_best:  55.42%, tr:  61.49%, tr_best:  61.49%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 11.066537/ 17.670807, val:  42.50%, val_best:  55.42%, tr:  58.02%, tr_best:  61.49%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 11.327337/ 11.860748, val:  55.83%, val_best:  55.83%, tr:  56.59%, tr_best:  61.49%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  7.137342/ 16.591690, val:  44.58%, val_best:  55.83%, tr:  64.15%, tr_best:  64.15%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 10.414684/ 12.231481, val:  51.25%, val_best:  55.83%, tr:  58.12%, tr_best:  64.15%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 11.280575/  9.839038, val:  55.00%, val_best:  55.83%, tr:  60.98%, tr_best:  64.15%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  9.097907/ 13.355634, val:  63.33%, val_best:  63.33%, tr:  62.21%, tr_best:  64.15%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  8.656774/ 11.428218, val:  50.83%, val_best:  63.33%, tr:  68.85%, tr_best:  68.85%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  8.020761/ 12.148771, val:  62.92%, val_best:  63.33%, tr:  66.70%, tr_best:  68.85%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  8.690521/ 13.053494, val:  48.33%, val_best:  63.33%, tr:  66.29%, tr_best:  68.85%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  9.028649/ 11.849286, val:  65.00%, val_best:  65.00%, tr:  67.42%, tr_best:  68.85%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  6.207046/ 13.926487, val:  46.25%, val_best:  65.00%, tr:  73.65%, tr_best:  73.65%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  9.007674/ 18.372406, val:  48.33%, val_best:  65.00%, tr:  67.31%, tr_best:  73.65%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  8.358117/ 10.647127, val:  69.17%, val_best:  69.17%, tr:  73.34%, tr_best:  73.65%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  7.220549/  9.947544, val:  64.58%, val_best:  69.17%, tr:  73.75%, tr_best:  73.75%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  6.478809/ 11.039140, val:  58.33%, val_best:  69.17%, tr:  79.78%, tr_best:  79.78%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  4.375608/  8.573229, val:  74.17%, val_best:  74.17%, tr:  83.25%, tr_best:  83.25%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  4.025969/ 11.622276, val:  52.50%, val_best:  74.17%, tr:  84.17%, tr_best:  84.17%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  4.212702/  8.983692, val:  69.17%, val_best:  74.17%, tr:  81.31%, tr_best:  84.17%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  2.434236/  6.755210, val:  75.00%, val_best:  75.00%, tr:  92.75%, tr_best:  92.75%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  5.988896/  9.411622, val:  67.92%, val_best:  75.00%, tr:  77.22%, tr_best:  92.75%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  3.306749/ 10.133925, val:  65.42%, val_best:  75.00%, tr:  88.15%, tr_best:  92.75%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  3.123227/  7.170031, val:  74.58%, val_best:  75.00%, tr:  89.58%, tr_best:  92.75%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  2.095148/  6.960184, val:  76.67%, val_best:  76.67%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  2.128088/  7.217098, val:  74.17%, val_best:  76.67%, tr:  92.24%, tr_best:  94.28%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  2.016231/  6.334766, val:  80.42%, val_best:  80.42%, tr:  93.97%, tr_best:  94.28%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  4.232775/  9.956888, val:  75.00%, val_best:  80.42%, tr:  85.19%, tr_best:  94.28%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  2.384630/  9.051965, val:  75.00%, val_best:  80.42%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  1.454750/  7.824630, val:  80.83%, val_best:  80.83%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  2.399838/ 10.406935, val:  73.33%, val_best:  80.83%, tr:  92.44%, tr_best:  97.96%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  2.226431/ 10.465952, val:  68.75%, val_best:  80.83%, tr:  94.59%, tr_best:  97.96%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  1.385219/  7.201745, val:  86.25%, val_best:  86.25%, tr:  97.45%, tr_best:  97.96%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  1.338525/  9.768265, val:  72.92%, val_best:  86.25%, tr:  97.24%, tr_best:  97.96%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  1.485784/  7.216957, val:  83.75%, val_best:  86.25%, tr:  96.63%, tr_best:  97.96%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  1.375165/  9.300796, val:  65.83%, val_best:  86.25%, tr:  96.32%, tr_best:  97.96%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  1.020028/  8.597672, val:  77.50%, val_best:  86.25%, tr:  97.85%, tr_best:  97.96%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.985473/  8.126592, val:  78.33%, val_best:  86.25%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  1.255985/  8.608353, val:  82.08%, val_best:  86.25%, tr:  96.12%, tr_best:  98.16%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.846977/  7.894583, val:  82.92%, val_best:  86.25%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.629057/  8.113746, val:  77.50%, val_best:  86.25%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.820310/  8.003426, val:  80.42%, val_best:  86.25%, tr:  98.57%, tr_best:  99.39%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.664010/  8.270447, val:  76.67%, val_best:  86.25%, tr:  98.88%, tr_best:  99.39%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  1.371297/  9.136230, val:  77.50%, val_best:  86.25%, tr:  96.53%, tr_best:  99.39%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.592410/  8.380710, val:  79.58%, val_best:  86.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.677704/  8.534517, val:  78.75%, val_best:  86.25%, tr:  98.47%, tr_best:  99.49%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.537818/  8.340693, val:  80.83%, val_best:  86.25%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.500414/  8.466639, val:  78.75%, val_best:  86.25%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.628192/  8.066092, val:  81.25%, val_best:  86.25%, tr:  98.77%, tr_best:  99.49%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.597950/  9.481099, val:  73.33%, val_best:  86.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.442401/  8.189306, val:  83.75%, val_best:  86.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.290462/  8.071167, val:  83.33%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.315315/  8.280093, val:  81.25%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.228063/  8.028838, val:  83.75%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.278271/  8.459566, val:  80.83%, val_best:  86.25%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.255276/  8.421781, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.278104/  7.802329, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.290094/  8.758030, val:  79.17%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.197754/  8.228492, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.180155/  8.254547, val:  81.67%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.106943/  8.203403, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.171544/  8.416033, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.103944/  8.478582, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.115847/  8.314570, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.105918/  7.767016, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.114430/  8.455602, val:  78.33%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.138385/  8.785267, val:  82.08%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.099329/  8.300303, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.066140/  7.955417, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.098647/  8.565562, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.080003/  7.706041, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.061060/  7.923676, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.043273/  7.880740, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.045163/  8.042602, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.062237/  7.977731, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.055489/  8.064859, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.064134/  8.344214, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.080605/  8.003790, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.068687/  8.041193, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.036669/  8.135741, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.042768/  8.049862, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.067747/  8.600947, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.099511/  8.053300, val:  85.00%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.048067/  8.233769, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.046329/  7.979647, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.035093/  8.126648, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.028448/  7.933216, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.031914/  8.139364, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.033528/  7.971307, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.025047/  7.892262, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.019069/  8.054222, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.034204/  8.083963, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.054533/  8.146461, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b049f509808245538306dbbaaafd10dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▂▃▄▁▅▆▄▅▇▆█▇▇▇▇██████▇███▇█████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▂▂▂▃▅▃▂▆▆▆▅▆▆▆█▅▇▇▇▇▇█▇▇█▇█▇██████████</td></tr><tr><td>tr_acc</td><td>▁▃▃▃▄▄▄▅▆▅▆▆▇▇▇▇████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▇█▇▆▅▅▄▄▃▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▂▂▂▃▅▃▂▆▆▆▅▆▆▆█▅▇▇▇▇▇█▇▇█▇█▇██████████</td></tr><tr><td>val_loss</td><td>▇█▃▇▆▃▄▄▄▃▂▂▂▁▂▂▁▂▁▁▁▂▂▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.05453</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>8.14646</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fancy-sweep-25</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k5etho08' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k5etho08</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_175439-k5etho08/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9d406k3u with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_180110-9d406k3u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9d406k3u' target=\"_blank\">winter-sweep-27</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9d406k3u' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9d406k3u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.180306/  3.109862, val:  47.08%, val_best:  47.08%, tr:  34.12%, tr_best:  34.12%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.404258/  2.682070, val:  48.33%, val_best:  48.33%, tr:  50.15%, tr_best:  50.15%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.344430/  2.368267, val:  46.25%, val_best:  48.33%, tr:  54.44%, tr_best:  54.44%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.851046/  2.515410, val:  54.58%, val_best:  54.58%, tr:  58.43%, tr_best:  58.43%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.860854/  2.073923, val:  64.17%, val_best:  64.17%, tr:  59.96%, tr_best:  59.96%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.759564/  2.188919, val:  55.83%, val_best:  64.17%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.370183/  2.355979, val:  56.67%, val_best:  64.17%, tr:  64.76%, tr_best:  64.76%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.715569/  2.823292, val:  50.42%, val_best:  64.17%, tr:  64.76%, tr_best:  64.76%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.503568/  2.013240, val:  56.25%, val_best:  64.17%, tr:  68.85%, tr_best:  68.85%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.934530/  2.213398, val:  62.92%, val_best:  64.17%, tr:  67.01%, tr_best:  68.85%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.558768/  1.413133, val:  67.50%, val_best:  67.50%, tr:  70.28%, tr_best:  70.28%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.066058/  2.458646, val:  46.67%, val_best:  67.50%, tr:  77.22%, tr_best:  77.22%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.298559/  1.467130, val:  73.33%, val_best:  73.33%, tr:  75.18%, tr_best:  77.22%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.972873/  2.165203, val:  62.92%, val_best:  73.33%, tr:  79.67%, tr_best:  79.67%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.200149/  1.982875, val:  69.58%, val_best:  73.33%, tr:  79.78%, tr_best:  79.78%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.802450/  1.536897, val:  71.25%, val_best:  73.33%, tr:  86.93%, tr_best:  86.93%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.823546/  1.454556, val:  77.08%, val_best:  77.08%, tr:  86.41%, tr_best:  86.93%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.757926/  1.711388, val:  67.08%, val_best:  77.08%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.700954/  1.799004, val:  72.08%, val_best:  77.08%, tr:  89.07%, tr_best:  89.07%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.644608/  1.439909, val:  79.17%, val_best:  79.17%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.577826/  2.292265, val:  65.42%, val_best:  79.17%, tr:  93.26%, tr_best:  93.26%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.708447/  1.550486, val:  81.25%, val_best:  81.25%, tr:  89.68%, tr_best:  93.26%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.486245/  1.556058, val:  80.00%, val_best:  81.25%, tr:  95.81%, tr_best:  95.81%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.490332/  1.983357, val:  75.00%, val_best:  81.25%, tr:  94.28%, tr_best:  95.81%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.460276/  1.737914, val:  76.25%, val_best:  81.25%, tr:  95.81%, tr_best:  95.81%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.375411/  1.559327, val:  82.92%, val_best:  82.92%, tr:  96.94%, tr_best:  96.94%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.377022/  1.986580, val:  72.50%, val_best:  82.92%, tr:  96.73%, tr_best:  96.94%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.329040/  1.666800, val:  81.67%, val_best:  82.92%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.339145/  1.696546, val:  83.75%, val_best:  83.75%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.280990/  1.802836, val:  78.75%, val_best:  83.75%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.288869/  1.760805, val:  80.83%, val_best:  83.75%, tr:  98.26%, tr_best:  98.88%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.230711/  1.916432, val:  79.58%, val_best:  83.75%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.359063/  1.928337, val:  77.08%, val_best:  83.75%, tr:  96.22%, tr_best:  99.28%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.281925/  2.151936, val:  78.75%, val_best:  83.75%, tr:  98.06%, tr_best:  99.28%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.246013/  1.893017, val:  79.17%, val_best:  83.75%, tr:  98.67%, tr_best:  99.28%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.196211/  1.785084, val:  85.42%, val_best:  85.42%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.178562/  1.798805, val:  82.92%, val_best:  85.42%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.183324/  1.935727, val:  82.08%, val_best:  85.42%, tr:  99.18%, tr_best:  99.59%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.195692/  1.999523, val:  78.33%, val_best:  85.42%, tr:  98.98%, tr_best:  99.59%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.216530/  1.856399, val:  85.00%, val_best:  85.42%, tr:  98.47%, tr_best:  99.59%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.144560/  1.822028, val:  87.08%, val_best:  87.08%, tr:  99.39%, tr_best:  99.59%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.124669/  1.889706, val:  84.17%, val_best:  87.08%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.120514/  1.971884, val:  83.75%, val_best:  87.08%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.106368/  2.077151, val:  82.08%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.100385/  1.975525, val:  84.17%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.096471/  2.049598, val:  83.33%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.088536/  2.079374, val:  85.00%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.073679/  1.972196, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.081272/  2.094374, val:  84.17%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.067147/  2.007720, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.068009/  2.122048, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.053742/  2.160354, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.061943/  2.164534, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.047735/  2.179672, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.053324/  2.168907, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.047885/  2.134603, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.044538/  2.276337, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.046827/  2.233861, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.035365/  2.194367, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.038916/  2.273872, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.037737/  2.292843, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.037391/  2.276153, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.032792/  2.388975, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.028515/  2.325812, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.027256/  2.307356, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.022973/  2.317866, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.025621/  2.392636, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.030058/  2.405973, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.021834/  2.397989, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.022536/  2.437896, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.024148/  2.415765, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.021669/  2.421646, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.021165/  2.437736, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.020556/  2.426752, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.021725/  2.484731, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.017507/  2.478528, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.017787/  2.532258, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.020686/  2.540888, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.020957/  2.533482, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.016392/  2.476222, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.017709/  2.605326, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.012024/  2.549072, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.010375/  2.589752, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.010645/  2.579187, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.010312/  2.563969, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.010640/  2.586890, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.009303/  2.598451, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.010791/  2.657985, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.010881/  2.649001, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.009107/  2.621606, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.008565/  2.657223, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.010358/  2.631876, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.008052/  2.667332, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.007376/  2.626499, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.007462/  2.657503, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.006648/  2.686467, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.008310/  2.689091, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.007152/  2.677456, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.007584/  2.674196, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.009602/  2.687304, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5335f760bbb49f99d9e3f07eadab0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▃▄▁▆▆▇▇█▇█▇▆▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▄▂▄▆▅▅▇▇▆▅▆▆█▇█▇▇██▇███████▇██████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▄▄▅▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▇▆▇▅▅▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▄▄▄▆▆▆▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▄▂▄▆▅▅▇▇▆▅▆▆█▇█▇▇██▇███████▇██████████</td></tr><tr><td>val_loss</td><td>█▅▄▇▄▁▃▂▁▁▂▃▃▃▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▅▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0096</td></tr><tr><td>val_acc_best</td><td>0.87917</td></tr><tr><td>val_acc_now</td><td>0.86667</td></tr><tr><td>val_loss</td><td>2.6873</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-sweep-27</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9d406k3u' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9d406k3u</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_180110-9d406k3u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6wz0jdin with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_180634-6wz0jdin</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6wz0jdin' target=\"_blank\">woven-sweep-29</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6wz0jdin' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6wz0jdin</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.104773/  2.053864, val:  51.25%, val_best:  51.25%, tr:  25.64%, tr_best:  25.64%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.736731/  2.230134, val:  47.08%, val_best:  51.25%, tr:  51.58%, tr_best:  51.58%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.002020/  2.312824, val:  55.00%, val_best:  55.00%, tr:  53.52%, tr_best:  53.52%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.386668/  2.083073, val:  54.58%, val_best:  55.00%, tr:  62.21%, tr_best:  62.21%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.492894/  1.717239, val:  62.50%, val_best:  62.50%, tr:  62.00%, tr_best:  62.21%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.332471/  1.792221, val:  52.08%, val_best:  62.50%, tr:  66.19%, tr_best:  66.19%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.253417/  1.668003, val:  58.75%, val_best:  62.50%, tr:  66.70%, tr_best:  66.70%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.379990/  1.904721, val:  55.83%, val_best:  62.50%, tr:  66.60%, tr_best:  66.70%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.114341/  1.520037, val:  62.92%, val_best:  62.92%, tr:  71.50%, tr_best:  71.50%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.328233/  1.671175, val:  64.17%, val_best:  64.17%, tr:  70.28%, tr_best:  71.50%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.106117/  1.183723, val:  75.42%, val_best:  75.42%, tr:  76.40%, tr_best:  76.40%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.013809/  1.402590, val:  68.33%, val_best:  75.42%, tr:  79.98%, tr_best:  79.98%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.884639/  1.200734, val:  80.00%, val_best:  80.00%, tr:  81.61%, tr_best:  81.61%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.759325/  1.538968, val:  64.58%, val_best:  80.00%, tr:  84.68%, tr_best:  84.68%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.800134/  1.751752, val:  72.08%, val_best:  80.00%, tr:  84.37%, tr_best:  84.68%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.619543/  1.405319, val:  76.25%, val_best:  80.00%, tr:  90.91%, tr_best:  90.91%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.616553/  1.301849, val:  81.25%, val_best:  81.25%, tr:  91.83%, tr_best:  91.83%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.576681/  1.414729, val:  80.42%, val_best:  81.25%, tr:  92.13%, tr_best:  92.13%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.513442/  1.567237, val:  79.58%, val_best:  81.25%, tr:  93.26%, tr_best:  93.26%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.550343/  1.398663, val:  79.17%, val_best:  81.25%, tr:  92.44%, tr_best:  93.26%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.467534/  1.500435, val:  79.58%, val_best:  81.25%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.428368/  1.443980, val:  80.00%, val_best:  81.25%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.370064/  1.460920, val:  80.83%, val_best:  81.25%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.394364/  1.583383, val:  83.33%, val_best:  83.33%, tr:  96.73%, tr_best:  97.45%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.342306/  1.475315, val:  83.33%, val_best:  83.33%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.302261/  1.444253, val:  84.17%, val_best:  84.17%, tr:  98.16%, tr_best:  98.37%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.296030/  1.541821, val:  83.75%, val_best:  84.17%, tr:  98.26%, tr_best:  98.37%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.251411/  1.514106, val:  84.58%, val_best:  84.58%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.238531/  1.498726, val:  84.58%, val_best:  84.58%, tr:  98.88%, tr_best:  99.18%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.220157/  1.713269, val:  80.42%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.208232/  1.658647, val:  83.33%, val_best:  84.58%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.211206/  1.738350, val:  81.67%, val_best:  84.58%, tr:  99.08%, tr_best:  99.80%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.346038/  1.796575, val:  80.42%, val_best:  84.58%, tr:  95.40%, tr_best:  99.80%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.241203/  1.924471, val:  82.50%, val_best:  84.58%, tr:  98.67%, tr_best:  99.80%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.204174/  1.868414, val:  80.42%, val_best:  84.58%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.176322/  1.680200, val:  85.42%, val_best:  85.42%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.146512/  1.680401, val:  87.92%, val_best:  87.92%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.141638/  1.708621, val:  84.17%, val_best:  87.92%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.134553/  1.713400, val:  87.50%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.133765/  1.792580, val:  86.25%, val_best:  87.92%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.108239/  1.740479, val:  87.92%, val_best:  87.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.099300/  1.829724, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.086268/  1.877101, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.089166/  1.896975, val:  86.25%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.091804/  1.941333, val:  86.25%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.077121/  1.929662, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.080588/  2.038290, val:  86.25%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.070399/  1.957218, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.078008/  2.001965, val:  86.25%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.076631/  2.071931, val:  84.58%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.059938/  2.049185, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.060356/  2.051642, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.055531/  2.078741, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.046517/  2.106687, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.044986/  2.094197, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.043671/  2.167619, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.041534/  2.220044, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.040190/  2.167526, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.039275/  2.148806, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.033740/  2.202559, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.035669/  2.164396, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.035455/  2.316478, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.036456/  2.323181, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.031262/  2.297507, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.032603/  2.269046, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.031631/  2.278704, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.029771/  2.353118, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.029575/  2.410303, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.028272/  2.415325, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.024165/  2.388385, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.020233/  2.402709, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.018546/  2.430946, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.017650/  2.420208, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.017938/  2.427801, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.020078/  2.434747, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.018854/  2.490302, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.018590/  2.467730, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.020713/  2.578107, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.035787/  2.582886, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.028671/  2.472462, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.021747/  2.518608, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.017619/  2.543981, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.017152/  2.566588, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.015547/  2.578778, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.014840/  2.582377, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.011610/  2.605446, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.012661/  2.579825, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.014981/  2.635815, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.010984/  2.581137, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.012800/  2.583062, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.010331/  2.577166, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.010233/  2.617981, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.012681/  2.630287, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.012702/  2.621710, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.011133/  2.638278, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.010843/  2.687402, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.009640/  2.692718, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.010447/  2.677902, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.011239/  2.690830, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.011328/  2.663903, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96daea17f31d45e38065ae1505502067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▅▆▂▆▇▇█████▇▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▃▂▃▆▅▇▆▆▇▇▇▇▇▇█▇██▇████▇██████████████</td></tr><tr><td>tr_acc</td><td>▁▄▄▅▅▆▇▇▇███████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▆▆▅▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▃▃▃▆▆▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▃▂▃▆▅▇▆▆▇▇▇▇▇▇█▇██▇████▇██████████████</td></tr><tr><td>val_loss</td><td>▅▆▃▄▃▁▄▂▂▂▂▃▃▄▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.01133</td></tr><tr><td>val_acc_best</td><td>0.8875</td></tr><tr><td>val_acc_now</td><td>0.87917</td></tr><tr><td>val_loss</td><td>2.6639</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">woven-sweep-29</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6wz0jdin' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6wz0jdin</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_180634-6wz0jdin/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: soxpup84 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_181148-soxpup84</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/soxpup84' target=\"_blank\">amber-sweep-31</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/soxpup84' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/soxpup84</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.346501/  3.242660, val:  43.75%, val_best:  43.75%, tr:  32.48%, tr_best:  32.48%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.374607/  2.670249, val:  45.42%, val_best:  45.42%, tr:  50.15%, tr_best:  50.15%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.381784/  1.867242, val:  57.50%, val_best:  57.50%, tr:  52.81%, tr_best:  52.81%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.689109/  2.437865, val:  53.33%, val_best:  57.50%, tr:  60.06%, tr_best:  60.06%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.952168/  2.135632, val:  64.58%, val_best:  64.58%, tr:  58.73%, tr_best:  60.06%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.726396/  2.004179, val:  51.67%, val_best:  64.58%, tr:  64.35%, tr_best:  64.35%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.311001/  2.109897, val:  58.33%, val_best:  64.58%, tr:  65.17%, tr_best:  65.17%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.365419/  2.121929, val:  53.75%, val_best:  64.58%, tr:  67.01%, tr_best:  67.01%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.388196/  2.330249, val:  50.42%, val_best:  64.58%, tr:  69.77%, tr_best:  69.77%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.960661/  2.235411, val:  58.75%, val_best:  64.58%, tr:  64.96%, tr_best:  69.77%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.456701/  1.299310, val:  75.00%, val_best:  75.00%, tr:  70.38%, tr_best:  70.38%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.197675/  2.221593, val:  55.00%, val_best:  75.00%, tr:  75.89%, tr_best:  75.89%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.130680/  1.397265, val:  73.33%, val_best:  75.00%, tr:  78.75%, tr_best:  78.75%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.969784/  2.111191, val:  60.00%, val_best:  75.00%, tr:  80.08%, tr_best:  80.08%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.049656/  2.063470, val:  69.17%, val_best:  75.00%, tr:  81.10%, tr_best:  81.10%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.825941/  1.517174, val:  75.83%, val_best:  75.83%, tr:  86.62%, tr_best:  86.62%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.803262/  1.367126, val:  82.50%, val_best:  82.50%, tr:  87.23%, tr_best:  87.23%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.734936/  1.639259, val:  74.17%, val_best:  82.50%, tr:  88.66%, tr_best:  88.66%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.635151/  1.734465, val:  73.75%, val_best:  82.50%, tr:  90.70%, tr_best:  90.70%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.641089/  1.390999, val:  81.67%, val_best:  82.50%, tr:  90.91%, tr_best:  90.91%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.511917/  1.918303, val:  68.33%, val_best:  82.50%, tr:  94.79%, tr_best:  94.79%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.711000/  1.658874, val:  75.83%, val_best:  82.50%, tr:  89.68%, tr_best:  94.79%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.449941/  1.646599, val:  76.67%, val_best:  82.50%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.604072/  2.031619, val:  73.75%, val_best:  82.50%, tr:  93.05%, tr_best:  95.91%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.445463/  1.685179, val:  82.08%, val_best:  82.50%, tr:  95.71%, tr_best:  95.91%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.375447/  1.544618, val:  84.17%, val_best:  84.17%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.370055/  1.823231, val:  78.33%, val_best:  84.17%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.360221/  1.763766, val:  75.42%, val_best:  84.17%, tr:  96.83%, tr_best:  97.24%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.355839/  1.600279, val:  83.75%, val_best:  84.17%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.295994/  1.831193, val:  78.75%, val_best:  84.17%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.299781/  1.819648, val:  80.00%, val_best:  84.17%, tr:  97.65%, tr_best:  98.88%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.242351/  1.857226, val:  81.25%, val_best:  84.17%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.306857/  1.793297, val:  78.75%, val_best:  84.17%, tr:  96.94%, tr_best:  98.98%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.299564/  2.026981, val:  83.33%, val_best:  84.17%, tr:  97.55%, tr_best:  98.98%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.252769/  2.035954, val:  78.33%, val_best:  84.17%, tr:  98.16%, tr_best:  98.98%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.193765/  1.786005, val:  83.75%, val_best:  84.17%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.190888/  1.799066, val:  87.50%, val_best:  87.50%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.187712/  1.873253, val:  85.83%, val_best:  87.50%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.198012/  1.944767, val:  81.25%, val_best:  87.50%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.189821/  1.904447, val:  85.83%, val_best:  87.50%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.130437/  1.926876, val:  83.75%, val_best:  87.50%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.124272/  1.955826, val:  82.92%, val_best:  87.50%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.099760/  1.944760, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.108759/  2.032933, val:  83.75%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.100795/  2.070785, val:  84.58%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.099366/  2.119002, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.088621/  2.131737, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.094728/  2.058121, val:  85.83%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.101432/  2.176071, val:  82.08%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.085945/  2.123490, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.072813/  2.196092, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.066919/  2.153964, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.080101/  2.236741, val:  82.08%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.062995/  2.239816, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.057680/  2.167378, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.050486/  2.205993, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.063124/  2.317795, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.044502/  2.257132, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.044441/  2.317189, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.042186/  2.340905, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.037537/  2.324151, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.040125/  2.429977, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.047992/  2.450475, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.036151/  2.460742, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.029153/  2.455425, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.032344/  2.505168, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.034759/  2.528324, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.025418/  2.438418, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.022545/  2.466334, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.020918/  2.521189, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.025386/  2.499655, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.025000/  2.570765, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.020944/  2.588822, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.025656/  2.563958, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.021384/  2.594712, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.018450/  2.612645, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.018775/  2.672585, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.020750/  2.636120, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.019531/  2.723828, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.021903/  2.653610, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.025046/  2.755788, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.018080/  2.680630, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.013840/  2.751987, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.026025/  2.772925, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.016073/  2.714725, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.014650/  2.785127, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.012545/  2.792268, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.014285/  2.757935, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.012799/  2.818671, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.012111/  2.828362, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.010717/  2.860513, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.014864/  2.877208, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.011702/  2.858006, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.009866/  2.828037, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.012317/  2.853198, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.011295/  2.898334, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.010292/  2.903925, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.009218/  2.911864, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.010190/  2.914926, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.009021/  2.886387, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc25ed9abc7d44a9a172b54e0dfa92f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▅▅▅▇▇▇▇▇█▇███████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▃▃▆▅▆▇▆▇▆▇▇▇███▇██▇███▇██████████▇███</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▄▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▇▅▇▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▄▆▆▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▃▃▆▅▆▇▆▇▆▇▇▇███▇██▇███▇██████████▇███</td></tr><tr><td>val_loss</td><td>█▃▄▄▄▁▄▂▁▂▂▃▃▃▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00902</td></tr><tr><td>val_acc_best</td><td>0.87917</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>2.88639</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-sweep-31</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/soxpup84' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/soxpup84</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_181148-soxpup84/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b9qrtfex with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_181711-b9qrtfex</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b9qrtfex' target=\"_blank\">dark-sweep-32</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b9qrtfex' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b9qrtfex</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 3, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 45e076f350e4b27d8ff87367a19d69df\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.042902/  1.875667, val:  53.75%, val_best:  53.75%, tr:  26.86%, tr_best:  26.86%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.686517/  1.905883, val:  50.00%, val_best:  53.75%, tr:  51.99%, tr_best:  51.99%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.881715/  2.162548, val:  53.75%, val_best:  53.75%, tr:  52.30%, tr_best:  52.30%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.325727/  1.763718, val:  55.42%, val_best:  55.42%, tr:  61.90%, tr_best:  61.90%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.361691/  1.519893, val:  64.58%, val_best:  64.58%, tr:  60.67%, tr_best:  61.90%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.349310/  1.650209, val:  53.33%, val_best:  64.58%, tr:  65.68%, tr_best:  65.68%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.342869/  1.633036, val:  58.75%, val_best:  64.58%, tr:  66.70%, tr_best:  66.70%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.303275/  1.886946, val:  56.25%, val_best:  64.58%, tr:  65.47%, tr_best:  66.70%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.131439/  1.258338, val:  66.67%, val_best:  66.67%, tr:  69.56%, tr_best:  69.56%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.380687/  1.374383, val:  71.67%, val_best:  71.67%, tr:  66.91%, tr_best:  69.56%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.229097/  1.231174, val:  69.17%, val_best:  71.67%, tr:  73.65%, tr_best:  73.65%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.072358/  1.538264, val:  69.58%, val_best:  71.67%, tr:  76.71%, tr_best:  76.71%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.026814/  1.283684, val:  70.83%, val_best:  71.67%, tr:  76.20%, tr_best:  76.71%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.775724/  1.652000, val:  59.17%, val_best:  71.67%, tr:  83.45%, tr_best:  83.45%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.879234/  1.688299, val:  67.08%, val_best:  71.67%, tr:  82.02%, tr_best:  83.45%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.643295/  1.349849, val:  77.92%, val_best:  77.92%, tr:  88.97%, tr_best:  88.97%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.604496/  1.423240, val:  77.50%, val_best:  77.92%, tr:  90.40%, tr_best:  90.40%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.604043/  1.480436, val:  71.25%, val_best:  77.92%, tr:  90.19%, tr_best:  90.40%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.533713/  1.540790, val:  75.42%, val_best:  77.92%, tr:  92.03%, tr_best:  92.03%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.459705/  1.335802, val:  82.08%, val_best:  82.08%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.427922/  1.631130, val:  72.50%, val_best:  82.08%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.446122/  1.519686, val:  76.67%, val_best:  82.08%, tr:  94.69%, tr_best:  95.61%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.352903/  1.598144, val:  77.08%, val_best:  82.08%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.332917/  1.698383, val:  78.33%, val_best:  82.08%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.290585/  1.532222, val:  80.42%, val_best:  82.08%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.236655/  1.568963, val:  80.42%, val_best:  82.08%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.239467/  1.654415, val:  76.25%, val_best:  82.08%, tr:  98.88%, tr_best:  99.28%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.218349/  1.788047, val:  78.33%, val_best:  82.08%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.204546/  1.674202, val:  82.92%, val_best:  82.92%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.168840/  1.780976, val:  80.42%, val_best:  82.92%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.155491/  1.845855, val:  81.67%, val_best:  82.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.163881/  1.789073, val:  81.25%, val_best:  82.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.235438/  1.884437, val:  79.58%, val_best:  82.92%, tr:  97.65%, tr_best:  99.69%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.167119/  1.903436, val:  78.75%, val_best:  82.92%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.142463/  1.899348, val:  82.50%, val_best:  82.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.104836/  1.932644, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.098242/  1.878446, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.088981/  1.955158, val:  82.50%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.088058/  1.972402, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.075841/  2.028373, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.071691/  2.065442, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.068879/  2.137785, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.061080/  2.122046, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.062745/  2.184366, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.055302/  2.209200, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.049956/  2.188888, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.048807/  2.214116, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.046593/  2.268787, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.043813/  2.219160, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.037221/  2.283121, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.033372/  2.298536, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.034690/  2.393664, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.033074/  2.359427, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.026477/  2.360284, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.028681/  2.399699, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.030131/  2.472425, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.027876/  2.463827, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.022905/  2.459348, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.018002/  2.492216, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.017557/  2.491765, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.021186/  2.533647, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.021289/  2.543438, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.017912/  2.589255, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.015169/  2.568563, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.015605/  2.552984, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.014116/  2.589525, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.015648/  2.662147, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.017942/  2.662287, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.016924/  2.637585, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.015094/  2.669524, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.015010/  2.688549, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.018817/  2.678953, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.013697/  2.705042, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.015193/  2.697368, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.014260/  2.727188, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.010446/  2.765613, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.009505/  2.749542, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.009537/  2.755304, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.009692/  2.767267, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.008283/  2.754461, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.008728/  2.755758, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.009428/  2.781890, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.011511/  2.809343, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.008812/  2.877133, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.007532/  2.797549, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.008721/  2.847627, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.007984/  2.824492, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.007021/  2.838515, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.007166/  2.828802, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.007021/  2.872946, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.006322/  2.854782, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.006368/  2.856447, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.005631/  2.897530, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.005952/  2.856000, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.005255/  2.887731, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.005871/  2.910931, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.005867/  2.909821, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.005036/  2.915831, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.004272/  2.871330, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.004719/  2.909983, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f65b52b5a24d6e9fc07f010507a65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▁▄▅▄▆█▇████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▃▂▅▅▄▅▇▆▇▆▇▇▇▇▇███▇███▇███████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▅▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▆▅▆▅▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▃▃▅▅▅▆▇▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▃▂▅▅▄▅▇▆▇▆▇▇▇▇▇███▇███▇███████████████</td></tr><tr><td>val_loss</td><td>▄▅▂▄▁▁▃▂▁▂▂▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00472</td></tr><tr><td>val_acc_best</td><td>0.8625</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>2.90998</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-sweep-32</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b9qrtfex' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b9qrtfex</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_181711-b9qrtfex/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 34ojxkah with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_182229-34ojxkah</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/34ojxkah' target=\"_blank\">wild-sweep-34</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/34ojxkah' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/34ojxkah</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.273947/  1.806684, val:  38.75%, val_best:  38.75%, tr:  11.85%, tr_best:  11.85%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.467162/  1.493605, val:  56.67%, val_best:  56.67%, tr:  51.17%, tr_best:  51.17%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.414511/  1.398171, val:  60.00%, val_best:  60.00%, tr:  59.35%, tr_best:  59.35%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.186510/  1.541223, val:  55.83%, val_best:  60.00%, tr:  63.23%, tr_best:  63.23%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.153467/  1.295481, val:  64.17%, val_best:  64.17%, tr:  64.45%, tr_best:  64.45%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.031361/  1.293204, val:  63.33%, val_best:  64.17%, tr:  69.97%, tr_best:  69.97%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.008588/  1.242948, val:  67.08%, val_best:  67.08%, tr:  72.42%, tr_best:  72.42%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.015885/  1.201563, val:  67.50%, val_best:  67.50%, tr:  70.79%, tr_best:  72.42%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.020224/  1.387571, val:  65.83%, val_best:  67.50%, tr:  72.32%, tr_best:  72.42%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.001309/  1.272785, val:  65.00%, val_best:  67.50%, tr:  76.20%, tr_best:  76.20%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.951465/  1.211743, val:  73.75%, val_best:  73.75%, tr:  76.92%, tr_best:  76.92%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.835739/  1.245916, val:  72.92%, val_best:  73.75%, tr:  81.10%, tr_best:  81.10%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.733790/  1.141488, val:  80.42%, val_best:  80.42%, tr:  84.88%, tr_best:  84.88%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.665142/  1.261687, val:  75.42%, val_best:  80.42%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.699121/  1.439272, val:  72.50%, val_best:  80.42%, tr:  86.31%, tr_best:  88.56%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.598499/  1.261880, val:  80.00%, val_best:  80.42%, tr:  90.40%, tr_best:  90.40%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.553042/  1.230566, val:  81.67%, val_best:  81.67%, tr:  93.05%, tr_best:  93.05%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.493212/  1.289647, val:  82.92%, val_best:  82.92%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.464381/  1.355630, val:  80.00%, val_best:  82.92%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.449112/  1.254447, val:  85.42%, val_best:  85.42%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.403897/  1.358711, val:  83.33%, val_best:  85.42%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.384816/  1.560365, val:  75.00%, val_best:  85.42%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.375485/  1.468501, val:  80.00%, val_best:  85.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.361212/  1.429856, val:  85.00%, val_best:  85.42%, tr:  97.34%, tr_best:  98.16%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.315505/  1.445826, val:  87.08%, val_best:  87.08%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.300145/  1.533623, val:  82.92%, val_best:  87.08%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.333502/  1.481758, val:  87.08%, val_best:  87.08%, tr:  98.16%, tr_best:  99.08%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.286715/  1.653544, val:  82.08%, val_best:  87.08%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.250731/  1.563979, val:  84.17%, val_best:  87.08%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.243850/  1.721405, val:  81.67%, val_best:  87.08%, tr:  99.18%, tr_best:  99.49%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.254699/  1.615845, val:  82.50%, val_best:  87.08%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.228258/  1.617258, val:  85.83%, val_best:  87.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.249783/  1.674787, val:  84.17%, val_best:  87.08%, tr:  99.08%, tr_best:  99.69%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.233627/  1.773798, val:  85.00%, val_best:  87.08%, tr:  98.98%, tr_best:  99.69%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.223143/  1.771026, val:  83.75%, val_best:  87.08%, tr:  98.98%, tr_best:  99.69%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.184101/  1.789760, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.168181/  1.768310, val:  86.67%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.153751/  1.860675, val:  84.17%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.158554/  1.917441, val:  85.42%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.145000/  1.864692, val:  85.83%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.148895/  1.943918, val:  85.83%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.128606/  2.077660, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.119058/  2.053119, val:  85.00%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.120838/  2.083186, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.109278/  2.177881, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.107237/  2.173741, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.104866/  2.176369, val:  84.58%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.105877/  2.183968, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.108488/  2.174741, val:  85.42%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.090391/  2.200366, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.092251/  2.303808, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.082537/  2.246820, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.083139/  2.334017, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.073361/  2.272535, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.066655/  2.320934, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.065531/  2.384599, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.064556/  2.410279, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.075207/  2.395222, val:  87.08%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.066382/  2.405537, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.059498/  2.429092, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.060228/  2.450358, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.062553/  2.486080, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.047971/  2.570749, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.057822/  2.533655, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.066974/  2.560343, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.051271/  2.557676, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.048683/  2.658024, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.046336/  2.610463, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.048905/  2.696764, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.043923/  2.700198, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.036699/  2.708548, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.038077/  2.725419, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.038306/  2.828159, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.039544/  2.794822, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.036852/  2.820452, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.031594/  2.837725, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.032175/  2.784154, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.028440/  2.869951, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.034239/  2.885410, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.038207/  2.827848, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.031707/  2.902892, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.028680/  2.879750, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.030149/  2.875530, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.033261/  2.909179, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.031495/  2.909156, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.029456/  3.041204, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.025927/  2.992855, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.027847/  2.995333, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.024644/  3.002822, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.026175/  3.026831, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.020306/  2.989691, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.023579/  3.063802, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.020496/  3.073936, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.020464/  3.083223, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.017948/  3.093217, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.018874/  3.151083, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.023280/  3.227766, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.019676/  3.139271, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.018667/  3.144496, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.025040/  3.126840, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c83db76d256451cb74d2d9221591046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▂▁▇█▇█▇██████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▅▅▅▇▆▇█▆██▇█████████████████████▇█████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▅▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▅▅▅▇▇▇████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▅▅▅▇▆▇█▆██▇█████████████████████▇█████</td></tr><tr><td>val_loss</td><td>▃▂▂▁▁▁▂▂▁▂▂▂▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.02504</td></tr><tr><td>val_acc_best</td><td>0.875</td></tr><tr><td>val_acc_now</td><td>0.85833</td></tr><tr><td>val_loss</td><td>3.12684</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wild-sweep-34</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/34ojxkah' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/34ojxkah</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_182229-34ojxkah/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cgri9w2t with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_182848-cgri9w2t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cgri9w2t' target=\"_blank\">chocolate-sweep-36</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cgri9w2t' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cgri9w2t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.152548/  3.073499, val:  47.50%, val_best:  47.50%, tr:  34.53%, tr_best:  34.53%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.240457/  2.547173, val:  47.08%, val_best:  47.50%, tr:  50.77%, tr_best:  50.77%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.153813/  1.968828, val:  55.83%, val_best:  55.83%, tr:  53.63%, tr_best:  53.63%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.660513/  2.304572, val:  57.08%, val_best:  57.08%, tr:  61.18%, tr_best:  61.18%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.695080/  1.962772, val:  64.58%, val_best:  64.58%, tr:  60.06%, tr_best:  61.18%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.553203/  2.001303, val:  51.67%, val_best:  64.58%, tr:  65.17%, tr_best:  65.17%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.443939/  2.009194, val:  63.33%, val_best:  64.58%, tr:  66.50%, tr_best:  66.50%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.592281/  2.498713, val:  51.67%, val_best:  64.58%, tr:  64.76%, tr_best:  66.50%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.254118/  1.930240, val:  57.92%, val_best:  64.58%, tr:  68.74%, tr_best:  68.74%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.799594/  1.936630, val:  63.75%, val_best:  64.58%, tr:  64.56%, tr_best:  68.74%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.407118/  1.242829, val:  78.75%, val_best:  78.75%, tr:  72.52%, tr_best:  72.52%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.196859/  2.150645, val:  53.33%, val_best:  78.75%, tr:  77.43%, tr_best:  77.43%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.204107/  1.306872, val:  78.33%, val_best:  78.75%, tr:  75.79%, tr_best:  77.43%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.830541/  1.582382, val:  69.17%, val_best:  78.75%, tr:  83.66%, tr_best:  83.66%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.902851/  2.360973, val:  65.83%, val_best:  78.75%, tr:  82.33%, tr_best:  83.66%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.742752/  1.455090, val:  72.92%, val_best:  78.75%, tr:  88.36%, tr_best:  88.36%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.673158/  1.456721, val:  77.50%, val_best:  78.75%, tr:  89.58%, tr_best:  89.58%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.648338/  1.473088, val:  77.08%, val_best:  78.75%, tr:  90.30%, tr_best:  90.30%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.604641/  1.649629, val:  74.17%, val_best:  78.75%, tr:  90.60%, tr_best:  90.60%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.577986/  1.471582, val:  79.17%, val_best:  79.17%, tr:  92.65%, tr_best:  92.65%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.478634/  1.839701, val:  68.75%, val_best:  79.17%, tr:  94.79%, tr_best:  94.79%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.556510/  1.552612, val:  78.75%, val_best:  79.17%, tr:  93.36%, tr_best:  94.79%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.408114/  1.571874, val:  77.50%, val_best:  79.17%, tr:  96.94%, tr_best:  96.94%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.424281/  1.719301, val:  75.83%, val_best:  79.17%, tr:  95.81%, tr_best:  96.94%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.419968/  1.693676, val:  78.75%, val_best:  79.17%, tr:  96.12%, tr_best:  96.94%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.371737/  1.546476, val:  81.67%, val_best:  81.67%, tr:  96.73%, tr_best:  96.94%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.304549/  1.684820, val:  77.92%, val_best:  81.67%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.310414/  1.691448, val:  78.33%, val_best:  81.67%, tr:  98.06%, tr_best:  98.26%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.279869/  1.659322, val:  79.58%, val_best:  81.67%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.258146/  1.787200, val:  77.50%, val_best:  81.67%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.222679/  1.695933, val:  80.42%, val_best:  81.67%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.207516/  1.831214, val:  80.00%, val_best:  81.67%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.352801/  1.856094, val:  79.58%, val_best:  81.67%, tr:  96.32%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.240959/  2.128573, val:  76.67%, val_best:  81.67%, tr:  98.26%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.226928/  1.953241, val:  78.75%, val_best:  81.67%, tr:  99.18%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.184252/  1.747886, val:  84.17%, val_best:  84.17%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.171507/  1.791772, val:  82.50%, val_best:  84.17%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.185853/  1.855959, val:  82.50%, val_best:  84.17%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.177670/  1.805220, val:  85.00%, val_best:  85.00%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.158532/  1.866389, val:  82.50%, val_best:  85.00%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.128483/  1.796116, val:  86.67%, val_best:  86.67%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.131740/  1.912002, val:  84.17%, val_best:  86.67%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.117360/  1.923029, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.131228/  1.933625, val:  83.33%, val_best:  86.67%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.107382/  2.036159, val:  82.92%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.091251/  1.961059, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.083765/  2.091711, val:  83.75%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.082007/  1.996443, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.101690/  2.073716, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.075526/  2.041998, val:  84.58%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.087400/  2.045304, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.068138/  2.052741, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.070102/  2.109738, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.063319/  2.181075, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.061458/  2.133056, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.050253/  2.059708, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.058110/  2.289216, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.057994/  2.183121, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.052283/  2.197763, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.050197/  2.255981, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.043825/  2.199901, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.038693/  2.266906, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.038923/  2.307765, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.038554/  2.332430, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.033886/  2.344378, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.034982/  2.366226, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.033395/  2.330647, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.030407/  2.419352, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.028784/  2.356424, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.022947/  2.475996, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.021670/  2.428758, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.022305/  2.409975, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.019604/  2.514165, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.023690/  2.435361, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.021129/  2.500155, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.017551/  2.498926, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.020084/  2.576136, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.019599/  2.579852, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.017567/  2.622230, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.020043/  2.566349, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.019356/  2.636802, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.017912/  2.634646, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.014242/  2.629376, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.014528/  2.658245, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.017273/  2.645397, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.014544/  2.695688, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.014543/  2.671787, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.015452/  2.690514, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.013118/  2.705887, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.011164/  2.711298, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.011120/  2.734122, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.012045/  2.728614, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.010153/  2.725231, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.011147/  2.744192, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.010666/  2.751169, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.010193/  2.771048, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.009824/  2.732724, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.008748/  2.804528, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.009496/  2.760743, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.010333/  2.765178, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2231b60728544c3a1c1e62741e60cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▅▅▂▅▇▇████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▄▂▄▆▄▆▇▆▆▆▆▇▇▇▇▇▇█▇█████▇██▇██████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▄▄▅▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▇▆▇▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▄▄▄▆▆▆▆▆▆▇▇▇▇▇▇███████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▄▂▄▆▄▆▇▆▆▆▆▇▇▇▇▇▇█▇█████▇██▇██████████</td></tr><tr><td>val_loss</td><td>█▄▄▆▃▁▅▂▂▂▃▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.01033</td></tr><tr><td>val_acc_best</td><td>0.8875</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>2.76518</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chocolate-sweep-36</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cgri9w2t' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cgri9w2t</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_182848-cgri9w2t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jkmm64l9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_183404-jkmm64l9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jkmm64l9' target=\"_blank\">solar-sweep-38</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jkmm64l9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jkmm64l9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.324377/  2.253040, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.09%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.620876/  1.434216, val:  55.83%, val_best:  55.83%, tr:  43.11%, tr_best:  43.11%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.395223/  1.387492, val:  60.00%, val_best:  60.00%, tr:  57.41%, tr_best:  57.41%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.225031/  1.483529, val:  55.00%, val_best:  60.00%, tr:  61.59%, tr_best:  61.59%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.169817/  1.421775, val:  61.67%, val_best:  61.67%, tr:  63.23%, tr_best:  63.23%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.066954/  1.304021, val:  63.75%, val_best:  63.75%, tr:  66.80%, tr_best:  66.80%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.064376/  1.238216, val:  65.42%, val_best:  65.42%, tr:  68.95%, tr_best:  68.95%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.024880/  1.199529, val:  69.17%, val_best:  69.17%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.029123/  1.336443, val:  65.42%, val_best:  69.17%, tr:  72.01%, tr_best:  72.01%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.941154/  1.283742, val:  62.50%, val_best:  69.17%, tr:  76.71%, tr_best:  76.71%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.947766/  1.218557, val:  71.67%, val_best:  71.67%, tr:  74.46%, tr_best:  76.71%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.831730/  1.175322, val:  78.33%, val_best:  78.33%, tr:  82.43%, tr_best:  82.43%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.732076/  1.132564, val:  81.25%, val_best:  81.25%, tr:  85.19%, tr_best:  85.19%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.699328/  1.109289, val:  83.75%, val_best:  83.75%, tr:  87.74%, tr_best:  87.74%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.709120/  1.330841, val:  76.67%, val_best:  83.75%, tr:  86.62%, tr_best:  87.74%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.615329/  1.213431, val:  80.42%, val_best:  83.75%, tr:  89.89%, tr_best:  89.89%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.583712/  1.172938, val:  82.08%, val_best:  83.75%, tr:  91.93%, tr_best:  91.93%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.522593/  1.172161, val:  84.17%, val_best:  84.17%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.492227/  1.255606, val:  78.75%, val_best:  84.17%, tr:  95.51%, tr_best:  95.51%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.458635/  1.185620, val:  83.75%, val_best:  84.17%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.436657/  1.216015, val:  86.67%, val_best:  86.67%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.408795/  1.418315, val:  75.83%, val_best:  86.67%, tr:  96.53%, tr_best:  96.73%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.397339/  1.363236, val:  79.17%, val_best:  86.67%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.392192/  1.274249, val:  84.58%, val_best:  86.67%, tr:  96.53%, tr_best:  97.85%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.342104/  1.305457, val:  84.58%, val_best:  86.67%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.327823/  1.339410, val:  85.42%, val_best:  86.67%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.348489/  1.340654, val:  86.25%, val_best:  86.67%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.303730/  1.472875, val:  82.08%, val_best:  86.67%, tr:  98.26%, tr_best:  98.37%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.286895/  1.302079, val:  87.92%, val_best:  87.92%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.256725/  1.526289, val:  81.67%, val_best:  87.92%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.263467/  1.343612, val:  86.25%, val_best:  87.92%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.231242/  1.439291, val:  86.67%, val_best:  87.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.277236/  1.455598, val:  83.75%, val_best:  87.92%, tr:  98.26%, tr_best:  99.69%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.243337/  1.475184, val:  86.25%, val_best:  87.92%, tr:  99.18%, tr_best:  99.69%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.228707/  1.511039, val:  85.83%, val_best:  87.92%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.205928/  1.488270, val:  87.08%, val_best:  87.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.184888/  1.487229, val:  86.25%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.176397/  1.569968, val:  86.25%, val_best:  87.92%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.172465/  1.525931, val:  87.92%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.159983/  1.586069, val:  87.92%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.158487/  1.559112, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.155101/  1.608311, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.129379/  1.628664, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.138231/  1.640852, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.128470/  1.701276, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.123701/  1.678072, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.128427/  1.750961, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.121205/  1.691087, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.117829/  1.722983, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.104483/  1.782358, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.102234/  1.809709, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.102463/  1.793564, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.103370/  1.924231, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.088605/  1.887205, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.091713/  1.841688, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.084755/  1.893186, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.080027/  1.923437, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.075570/  1.865280, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.066207/  1.890004, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.078241/  1.963507, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.066644/  1.937706, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.070238/  1.974469, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.068976/  2.043910, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.066484/  2.017676, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.059064/  2.008111, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.060678/  2.038437, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.056949/  2.116035, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.057309/  2.131035, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.051204/  2.128208, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.049041/  2.085106, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.041603/  2.106980, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.047903/  2.167952, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.049546/  2.164062, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.049787/  2.154206, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.047796/  2.187084, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.048630/  2.162773, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.041851/  2.198905, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.040721/  2.220918, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.041849/  2.187904, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.038748/  2.198490, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.036521/  2.291597, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.037336/  2.265679, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.035793/  2.274508, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.035333/  2.312144, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.038348/  2.310215, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.039090/  2.357795, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.029031/  2.316517, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.027537/  2.350415, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.030840/  2.355761, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.026686/  2.383904, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.024622/  2.396797, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.027923/  2.433868, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.028283/  2.424008, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.021742/  2.396441, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.025743/  2.462657, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.022960/  2.496866, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.026696/  2.494616, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.022717/  2.501087, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.022810/  2.476700, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.023298/  2.457374, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfa0f88c66f4391bfb996ef5a705d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▃▃▇▇▇██████▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▆▇▇▇▇▇██▇▇██████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇▇▇███████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▆▇▇▇▇▇██▇▇██████████████████████████</td></tr><tr><td>val_loss</td><td>▇▂▂▁▂▁▂▁▁▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0233</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>2.45737</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-sweep-38</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jkmm64l9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jkmm64l9</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_183404-jkmm64l9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l4gf2k94 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_183857-l4gf2k94</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l4gf2k94' target=\"_blank\">devout-sweep-40</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l4gf2k94' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l4gf2k94</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.324714/  2.286268, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.692343/  1.440458, val:  57.92%, val_best:  57.92%, tr:  40.04%, tr_best:  40.04%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.407705/  1.385422, val:  54.58%, val_best:  57.92%, tr:  57.41%, tr_best:  57.41%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.250945/  1.495150, val:  54.58%, val_best:  57.92%, tr:  62.10%, tr_best:  62.10%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.170930/  1.371023, val:  62.50%, val_best:  62.50%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.085329/  1.281871, val:  65.42%, val_best:  65.42%, tr:  67.52%, tr_best:  67.52%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.092702/  1.231467, val:  65.00%, val_best:  65.42%, tr:  68.03%, tr_best:  68.03%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.048118/  1.240324, val:  63.75%, val_best:  65.42%, tr:  67.82%, tr_best:  68.03%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.029154/  1.328223, val:  63.33%, val_best:  65.42%, tr:  71.40%, tr_best:  71.40%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.951323/  1.290054, val:  65.83%, val_best:  65.83%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.959766/  1.244060, val:  67.92%, val_best:  67.92%, tr:  75.89%, tr_best:  76.30%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.816679/  1.202070, val:  73.75%, val_best:  73.75%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.747890/  1.141234, val:  81.25%, val_best:  81.25%, tr:  84.98%, tr_best:  84.98%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.703539/  1.185826, val:  82.92%, val_best:  82.92%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.706075/  1.391682, val:  73.75%, val_best:  82.92%, tr:  86.72%, tr_best:  88.56%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.604286/  1.224734, val:  80.83%, val_best:  82.92%, tr:  90.09%, tr_best:  90.09%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.561919/  1.172217, val:  82.92%, val_best:  82.92%, tr:  93.87%, tr_best:  93.87%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.524343/  1.229921, val:  81.67%, val_best:  82.92%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.487522/  1.335530, val:  77.08%, val_best:  82.92%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.479926/  1.187144, val:  83.33%, val_best:  83.33%, tr:  94.99%, tr_best:  95.10%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.454232/  1.284937, val:  83.75%, val_best:  83.75%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.411124/  1.321166, val:  80.83%, val_best:  83.75%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.401416/  1.398801, val:  80.42%, val_best:  83.75%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.409850/  1.301508, val:  85.42%, val_best:  85.42%, tr:  96.12%, tr_best:  97.04%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.360577/  1.349709, val:  82.92%, val_best:  85.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.360081/  1.373224, val:  81.67%, val_best:  85.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.361630/  1.309298, val:  87.92%, val_best:  87.92%, tr:  97.65%, tr_best:  98.16%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.317644/  1.488184, val:  83.33%, val_best:  87.92%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.294707/  1.370536, val:  87.92%, val_best:  87.92%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.279281/  1.545154, val:  82.50%, val_best:  87.92%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.291486/  1.466846, val:  86.25%, val_best:  87.92%, tr:  98.67%, tr_best:  98.88%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.242385/  1.425911, val:  86.67%, val_best:  87.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.281249/  1.508531, val:  85.00%, val_best:  87.92%, tr:  98.57%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.262332/  1.540805, val:  84.17%, val_best:  87.92%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.239263/  1.657626, val:  81.67%, val_best:  87.92%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.222287/  1.524557, val:  85.42%, val_best:  87.92%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.192569/  1.567645, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.187307/  1.559691, val:  86.25%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.187297/  1.532413, val:  87.50%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.185414/  1.662082, val:  86.25%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.180150/  1.578279, val:  87.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.158662/  1.649303, val:  86.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.158380/  1.604576, val:  86.67%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.139144/  1.646242, val:  88.33%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.147587/  1.736517, val:  87.50%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.135313/  1.709815, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.136648/  1.758928, val:  87.08%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.127842/  1.726192, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.144416/  1.766882, val:  86.67%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.118283/  1.811773, val:  86.67%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.115775/  1.887517, val:  87.08%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.104303/  1.855918, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.111773/  1.860125, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.093209/  1.883933, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.086004/  1.906306, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.092298/  1.886673, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.088149/  1.965053, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.090941/  1.955923, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.079389/  1.952051, val:  88.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.082049/  2.054672, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.076718/  1.986453, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.071572/  2.013733, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.068816/  2.052522, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.072312/  2.065197, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.067766/  2.081539, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.059748/  2.099961, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.057278/  2.171205, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.067525/  2.196870, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.070476/  2.157996, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.052847/  2.168622, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.048771/  2.175929, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.056615/  2.214395, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.060089/  2.297812, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.051547/  2.244307, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.054256/  2.262815, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.048683/  2.273809, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.051884/  2.321156, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.053528/  2.329639, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.063437/  2.308314, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.066046/  2.336577, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.049934/  2.301535, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.041843/  2.348136, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.047969/  2.361818, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.045226/  2.312370, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.044027/  2.361408, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.043382/  2.397276, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.042749/  2.363960, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.038159/  2.341981, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.035929/  2.393583, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.040976/  2.445968, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.034906/  2.408180, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.032521/  2.446563, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.031469/  2.427554, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.029490/  2.477440, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.031134/  2.455287, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.027424/  2.534578, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.025009/  2.504771, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.028157/  2.479767, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.026861/  2.498552, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.032697/  2.532003, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8df2f4ecf84bcdbf5d835fe583db54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▄▃▇▇▆▇▇▇█████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▆▇▇▇▇▇▇█▇███████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇▇▇███████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▆▇▇▇▇▇▇█▇███████████████████████████</td></tr><tr><td>val_loss</td><td>▇▂▂▁▂▁▂▁▁▂▂▂▃▃▃▃▄▃▄▄▄▅▅▅▆▅▆▆▆▇▇▇▇▇▇▇█▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0327</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>2.532</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devout-sweep-40</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l4gf2k94' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l4gf2k94</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_183857-l4gf2k94/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6e2eodrt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_184405-6e2eodrt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6e2eodrt' target=\"_blank\">lilac-sweep-42</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6e2eodrt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6e2eodrt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.324377/  2.253040, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.09%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.620876/  1.434216, val:  55.83%, val_best:  55.83%, tr:  43.11%, tr_best:  43.11%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.395223/  1.387492, val:  60.00%, val_best:  60.00%, tr:  57.41%, tr_best:  57.41%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.225031/  1.483529, val:  55.00%, val_best:  60.00%, tr:  61.59%, tr_best:  61.59%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.169817/  1.421775, val:  61.67%, val_best:  61.67%, tr:  63.23%, tr_best:  63.23%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.066954/  1.304021, val:  63.75%, val_best:  63.75%, tr:  66.80%, tr_best:  66.80%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.064376/  1.238216, val:  65.42%, val_best:  65.42%, tr:  68.95%, tr_best:  68.95%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.024880/  1.199529, val:  69.17%, val_best:  69.17%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.029123/  1.336443, val:  65.42%, val_best:  69.17%, tr:  72.01%, tr_best:  72.01%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.941154/  1.283742, val:  62.50%, val_best:  69.17%, tr:  76.71%, tr_best:  76.71%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.947766/  1.218557, val:  71.67%, val_best:  71.67%, tr:  74.46%, tr_best:  76.71%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.831730/  1.175322, val:  78.33%, val_best:  78.33%, tr:  82.43%, tr_best:  82.43%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.732076/  1.132564, val:  81.25%, val_best:  81.25%, tr:  85.19%, tr_best:  85.19%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.699328/  1.109289, val:  83.75%, val_best:  83.75%, tr:  87.74%, tr_best:  87.74%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.709120/  1.330841, val:  76.67%, val_best:  83.75%, tr:  86.62%, tr_best:  87.74%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.615329/  1.213431, val:  80.42%, val_best:  83.75%, tr:  89.89%, tr_best:  89.89%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.583712/  1.172938, val:  82.08%, val_best:  83.75%, tr:  91.93%, tr_best:  91.93%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.522593/  1.172161, val:  84.17%, val_best:  84.17%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.492227/  1.255606, val:  78.75%, val_best:  84.17%, tr:  95.51%, tr_best:  95.51%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.458635/  1.185620, val:  83.75%, val_best:  84.17%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.436657/  1.216015, val:  86.67%, val_best:  86.67%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.408795/  1.418315, val:  75.83%, val_best:  86.67%, tr:  96.53%, tr_best:  96.73%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.397339/  1.363236, val:  79.17%, val_best:  86.67%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.392192/  1.274249, val:  84.58%, val_best:  86.67%, tr:  96.53%, tr_best:  97.85%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.342104/  1.305457, val:  84.58%, val_best:  86.67%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.327823/  1.339410, val:  85.42%, val_best:  86.67%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.348489/  1.340654, val:  86.25%, val_best:  86.67%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.303730/  1.472875, val:  82.08%, val_best:  86.67%, tr:  98.26%, tr_best:  98.37%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.286895/  1.302079, val:  87.92%, val_best:  87.92%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.256725/  1.526289, val:  81.67%, val_best:  87.92%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.263467/  1.343612, val:  86.25%, val_best:  87.92%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.231242/  1.439291, val:  86.67%, val_best:  87.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.277236/  1.455598, val:  83.75%, val_best:  87.92%, tr:  98.26%, tr_best:  99.69%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.243337/  1.475184, val:  86.25%, val_best:  87.92%, tr:  99.18%, tr_best:  99.69%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.228707/  1.511039, val:  85.83%, val_best:  87.92%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.205928/  1.488270, val:  87.08%, val_best:  87.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.184888/  1.487229, val:  86.25%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.176397/  1.569968, val:  86.25%, val_best:  87.92%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.172465/  1.525931, val:  87.92%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.159983/  1.586069, val:  87.92%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.158487/  1.559112, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.155101/  1.608311, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.129379/  1.628664, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.138231/  1.640852, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.128470/  1.701276, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.123701/  1.678072, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.128427/  1.750961, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.121205/  1.691087, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.117829/  1.722983, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.104483/  1.782358, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.102234/  1.809709, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.102463/  1.793564, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.103370/  1.924231, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.088605/  1.887205, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.091713/  1.841688, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.084755/  1.893186, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.080027/  1.923437, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.075570/  1.865280, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.066207/  1.890004, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.078241/  1.963507, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.066644/  1.937706, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.070238/  1.974469, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.068976/  2.043910, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.066484/  2.017676, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.059064/  2.008111, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.060678/  2.038437, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.056949/  2.116035, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.057309/  2.131035, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.051204/  2.128208, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.049041/  2.085106, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.041603/  2.106980, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.047903/  2.167952, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.049546/  2.164062, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.049787/  2.154206, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.047796/  2.187084, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.048630/  2.162773, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.041851/  2.198905, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.040721/  2.220918, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.041849/  2.187904, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.038748/  2.198490, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.036521/  2.291597, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.037336/  2.265679, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.035793/  2.274508, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.035333/  2.312144, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.038348/  2.310215, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.039090/  2.357795, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.029031/  2.316517, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.027537/  2.350415, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.030840/  2.355761, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.026686/  2.383904, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.024622/  2.396797, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.027923/  2.433868, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.028283/  2.424008, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.021742/  2.396441, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.025743/  2.462657, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.022960/  2.496866, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.026696/  2.494616, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.022717/  2.501087, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.022810/  2.476700, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.023298/  2.457374, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500e782a2d5e4cb9bfb3d6b52a639bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▃▃▇▇▇██████▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▆▇▇▇▇▇██▇▇██████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇▇▇███████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▆▇▇▇▇▇██▇▇██████████████████████████</td></tr><tr><td>val_loss</td><td>▇▂▂▁▂▁▂▁▁▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0233</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>2.45737</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lilac-sweep-42</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6e2eodrt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6e2eodrt</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_184405-6e2eodrt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0du8tgqb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_184918-0du8tgqb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0du8tgqb' target=\"_blank\">lemon-sweep-44</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0du8tgqb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0du8tgqb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.324714/  2.286268, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.692343/  1.440458, val:  57.92%, val_best:  57.92%, tr:  40.04%, tr_best:  40.04%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.407705/  1.385422, val:  54.58%, val_best:  57.92%, tr:  57.41%, tr_best:  57.41%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.250945/  1.495150, val:  54.58%, val_best:  57.92%, tr:  62.10%, tr_best:  62.10%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.170930/  1.371023, val:  62.50%, val_best:  62.50%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.085329/  1.281871, val:  65.42%, val_best:  65.42%, tr:  67.52%, tr_best:  67.52%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.092702/  1.231467, val:  65.00%, val_best:  65.42%, tr:  68.03%, tr_best:  68.03%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.048118/  1.240324, val:  63.75%, val_best:  65.42%, tr:  67.82%, tr_best:  68.03%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.029154/  1.328223, val:  63.33%, val_best:  65.42%, tr:  71.40%, tr_best:  71.40%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.951323/  1.290054, val:  65.83%, val_best:  65.83%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.959766/  1.244060, val:  67.92%, val_best:  67.92%, tr:  75.89%, tr_best:  76.30%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.816679/  1.202070, val:  73.75%, val_best:  73.75%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.747890/  1.141234, val:  81.25%, val_best:  81.25%, tr:  84.98%, tr_best:  84.98%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.703539/  1.185826, val:  82.92%, val_best:  82.92%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.706075/  1.391682, val:  73.75%, val_best:  82.92%, tr:  86.72%, tr_best:  88.56%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.604286/  1.224734, val:  80.83%, val_best:  82.92%, tr:  90.09%, tr_best:  90.09%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.561919/  1.172217, val:  82.92%, val_best:  82.92%, tr:  93.87%, tr_best:  93.87%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.524343/  1.229921, val:  81.67%, val_best:  82.92%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.487522/  1.335530, val:  77.08%, val_best:  82.92%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.479926/  1.187144, val:  83.33%, val_best:  83.33%, tr:  94.99%, tr_best:  95.10%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.454232/  1.284937, val:  83.75%, val_best:  83.75%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.411124/  1.321166, val:  80.83%, val_best:  83.75%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.401416/  1.398801, val:  80.42%, val_best:  83.75%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.409850/  1.301508, val:  85.42%, val_best:  85.42%, tr:  96.12%, tr_best:  97.04%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.360577/  1.349709, val:  82.92%, val_best:  85.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.360081/  1.373224, val:  81.67%, val_best:  85.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.361630/  1.309298, val:  87.92%, val_best:  87.92%, tr:  97.65%, tr_best:  98.16%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.317644/  1.488184, val:  83.33%, val_best:  87.92%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.294707/  1.370536, val:  87.92%, val_best:  87.92%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.279281/  1.545154, val:  82.50%, val_best:  87.92%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.291486/  1.466846, val:  86.25%, val_best:  87.92%, tr:  98.67%, tr_best:  98.88%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.242385/  1.425911, val:  86.67%, val_best:  87.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.281249/  1.508531, val:  85.00%, val_best:  87.92%, tr:  98.57%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.262332/  1.540805, val:  84.17%, val_best:  87.92%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.239263/  1.657626, val:  81.67%, val_best:  87.92%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.222287/  1.524557, val:  85.42%, val_best:  87.92%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.192569/  1.567645, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.187307/  1.559691, val:  86.25%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.187297/  1.532413, val:  87.50%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.185414/  1.662082, val:  86.25%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.180150/  1.578279, val:  87.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.158662/  1.649303, val:  86.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.158380/  1.604576, val:  86.67%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.139144/  1.646242, val:  88.33%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.147587/  1.736517, val:  87.50%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.135313/  1.709815, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.136648/  1.758928, val:  87.08%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.127842/  1.726192, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.144416/  1.766882, val:  86.67%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.118283/  1.811773, val:  86.67%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.115775/  1.887517, val:  87.08%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.104303/  1.855918, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.111773/  1.860125, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.093209/  1.883933, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.086004/  1.906306, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.092298/  1.886673, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.088149/  1.965053, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.090941/  1.955923, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.079389/  1.952051, val:  88.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.082049/  2.054672, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.076718/  1.986453, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.071572/  2.013733, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.068816/  2.052522, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.072312/  2.065197, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.067766/  2.081539, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.059748/  2.099961, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.057278/  2.171205, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.067525/  2.196870, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.070476/  2.157996, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.052847/  2.168622, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.048771/  2.175929, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.056615/  2.214395, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.060089/  2.297812, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.051547/  2.244307, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.054256/  2.262815, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.048683/  2.273809, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.051884/  2.321156, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.053528/  2.329639, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.063437/  2.308314, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.066046/  2.336577, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.049934/  2.301535, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.041843/  2.348136, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.047969/  2.361818, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.045226/  2.312370, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.044027/  2.361408, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.043382/  2.397276, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.042749/  2.363960, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.038159/  2.341981, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.035929/  2.393583, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.040976/  2.445968, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.034906/  2.408180, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.032521/  2.446563, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.031469/  2.427554, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.029490/  2.477440, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.031134/  2.455287, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.027424/  2.534578, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.025009/  2.504771, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.028157/  2.479767, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.026861/  2.498552, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.032697/  2.532003, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99726800f5c4eb3819df608a782c1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▄▃▇▇▆▇▇▇█████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▆▇▇▇▇▇▇█▇███████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇▇▇███████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▆▇▇▇▇▇▇█▇███████████████████████████</td></tr><tr><td>val_loss</td><td>▇▂▂▁▂▁▂▁▁▂▂▂▃▃▃▃▄▃▄▄▄▅▅▅▆▅▆▆▆▇▇▇▇▇▇▇█▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0327</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>2.532</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lemon-sweep-44</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0du8tgqb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0du8tgqb</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_184918-0du8tgqb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nygi9dq5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_185457-nygi9dq5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nygi9dq5' target=\"_blank\">soft-sweep-46</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nygi9dq5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nygi9dq5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 14688881ebd6a7fed8e9c9f512432e6a\n",
      "cache path doesn't exist\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.316240/  2.125607, val:  18.33%, val_best:  18.33%, tr:   9.70%, tr_best:   9.70%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.546646/  1.418840, val:  58.75%, val_best:  58.75%, tr:  44.33%, tr_best:  44.33%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.381500/  1.400138, val:  59.58%, val_best:  59.58%, tr:  58.63%, tr_best:  58.63%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.173909/  1.404554, val:  56.67%, val_best:  59.58%, tr:  63.94%, tr_best:  63.94%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.124534/  1.356403, val:  62.50%, val_best:  62.50%, tr:  64.35%, tr_best:  64.35%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.051286/  1.236503, val:  67.50%, val_best:  67.50%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.037094/  1.170779, val:  66.67%, val_best:  67.50%, tr:  70.17%, tr_best:  70.17%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.997175/  1.183554, val:  66.67%, val_best:  67.50%, tr:  68.64%, tr_best:  70.17%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.023323/  1.225868, val:  65.00%, val_best:  67.50%, tr:  71.50%, tr_best:  71.50%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.933768/  1.280784, val:  62.08%, val_best:  67.50%, tr:  75.69%, tr_best:  75.69%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.940181/  1.306782, val:  64.58%, val_best:  67.50%, tr:  76.20%, tr_best:  76.20%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.829949/  1.180190, val:  76.25%, val_best:  76.25%, tr:  82.23%, tr_best:  82.23%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.740263/  1.131214, val:  79.17%, val_best:  79.17%, tr:  85.19%, tr_best:  85.19%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.684018/  1.143260, val:  80.00%, val_best:  80.00%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.672925/  1.325401, val:  75.42%, val_best:  80.00%, tr:  87.03%, tr_best:  88.56%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.585873/  1.257601, val:  77.92%, val_best:  80.00%, tr:  90.40%, tr_best:  90.40%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.564520/  1.210406, val:  80.00%, val_best:  80.00%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.494402/  1.201871, val:  84.17%, val_best:  84.17%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.471593/  1.300224, val:  82.50%, val_best:  84.17%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.451750/  1.222694, val:  82.50%, val_best:  84.17%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.426315/  1.275714, val:  82.50%, val_best:  84.17%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.380085/  1.425778, val:  76.67%, val_best:  84.17%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.367111/  1.394547, val:  80.83%, val_best:  84.17%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.370724/  1.380912, val:  82.92%, val_best:  84.17%, tr:  97.45%, tr_best:  98.16%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.332597/  1.380815, val:  84.58%, val_best:  84.58%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.302187/  1.441498, val:  81.67%, val_best:  84.58%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.317697/  1.356555, val:  86.25%, val_best:  86.25%, tr:  98.47%, tr_best:  98.77%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.273859/  1.495213, val:  81.67%, val_best:  86.25%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.253703/  1.448968, val:  85.83%, val_best:  86.25%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.236709/  1.641042, val:  78.33%, val_best:  86.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.225136/  1.514135, val:  83.75%, val_best:  86.25%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.223559/  1.496704, val:  85.83%, val_best:  86.25%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.259963/  1.608806, val:  83.33%, val_best:  86.25%, tr:  98.26%, tr_best:  99.69%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.206038/  1.692605, val:  82.08%, val_best:  86.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.202284/  1.635619, val:  82.08%, val_best:  86.25%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.185878/  1.658356, val:  85.42%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.159791/  1.645553, val:  85.00%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.157265/  1.661087, val:  86.25%, val_best:  86.25%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.170139/  1.661088, val:  85.42%, val_best:  86.25%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.155820/  1.782553, val:  85.00%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.147634/  1.698657, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.136861/  1.797621, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.124640/  1.828688, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.116239/  1.842546, val:  82.92%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.122308/  1.855725, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.108312/  1.882175, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.108462/  1.911103, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.104315/  1.825837, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.118211/  1.842535, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.088793/  1.956562, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.093233/  1.973756, val:  84.17%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.078771/  1.990438, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.085553/  2.006944, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.075360/  2.024589, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.073751/  2.013012, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.069909/  2.050614, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.068853/  2.124649, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.064439/  2.106136, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.055649/  2.108382, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.059596/  2.150069, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.057730/  2.108778, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.060451/  2.158963, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.055261/  2.245874, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.055772/  2.199081, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.054624/  2.207465, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.046854/  2.222312, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.047702/  2.307370, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.044096/  2.284204, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.041958/  2.326537, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.038553/  2.355612, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.034253/  2.366022, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.041444/  2.366341, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.036214/  2.380904, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.040645/  2.390178, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.038895/  2.364125, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.030032/  2.391727, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.030197/  2.446134, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.031980/  2.466250, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.032447/  2.520515, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.026572/  2.512702, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.029584/  2.523398, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.028893/  2.544950, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.025529/  2.550704, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.024420/  2.574587, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.025926/  2.549877, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.026807/  2.536113, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.035752/  2.577284, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.027753/  2.588641, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.026196/  2.600521, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.024263/  2.661175, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.023956/  2.632542, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.021234/  2.666324, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.019646/  2.633343, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.017766/  2.643821, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.017557/  2.683625, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.020999/  2.732625, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.018786/  2.719090, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.016035/  2.709617, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.018288/  2.678016, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.021557/  2.763285, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fff8cc5a3424ae0a3931a5313fd0bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▆▅▅▃▇▇▆████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▅▆▅▇▇█▇▇██▇███████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▅▆▅▇▇█▇▇██▇███████████████████████████</td></tr><tr><td>val_loss</td><td>▅▂▂▁▂▁▂▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.02156</td></tr><tr><td>val_acc_best</td><td>0.875</td></tr><tr><td>val_acc_now</td><td>0.83333</td></tr><tr><td>val_loss</td><td>2.76329</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soft-sweep-46</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nygi9dq5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nygi9dq5</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_185457-nygi9dq5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eixbqhi7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_190353-eixbqhi7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/eixbqhi7' target=\"_blank\">clean-sweep-48</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/eixbqhi7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/eixbqhi7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  6.197907/ 10.012270, val:  28.33%, val_best:  28.33%, tr:  22.47%, tr_best:  22.47%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 10.453242/ 14.490256, val:  26.25%, val_best:  28.33%, tr:  38.71%, tr_best:  38.71%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 12.234682/  9.811734, val:  45.42%, val_best:  45.42%, tr:  39.63%, tr_best:  39.63%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  7.766311/ 12.004843, val:  42.08%, val_best:  45.42%, tr:  50.15%, tr_best:  50.15%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  7.351192/  7.354522, val:  39.58%, val_best:  45.42%, tr:  53.83%, tr_best:  53.83%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  6.593798/ 12.149588, val:  47.08%, val_best:  47.08%, tr:  54.75%, tr_best:  54.75%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  6.869013/  5.547773, val:  55.00%, val_best:  55.00%, tr:  51.07%, tr_best:  54.75%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  6.002259/ 10.690344, val:  45.42%, val_best:  55.00%, tr:  53.93%, tr_best:  54.75%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  4.921889/  5.808577, val:  52.50%, val_best:  55.00%, tr:  59.45%, tr_best:  59.45%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  5.226600/  8.040792, val:  45.83%, val_best:  55.00%, tr:  61.18%, tr_best:  61.18%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  8.742639/ 10.141313, val:  51.67%, val_best:  55.00%, tr:  55.26%, tr_best:  61.18%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  7.610842/ 11.524578, val:  50.83%, val_best:  55.00%, tr:  59.55%, tr_best:  61.18%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  7.973781/  6.436391, val:  54.58%, val_best:  55.00%, tr:  59.86%, tr_best:  61.18%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  4.850891/  9.370337, val:  49.17%, val_best:  55.00%, tr:  64.25%, tr_best:  64.25%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  5.294646/  8.405915, val:  58.33%, val_best:  58.33%, tr:  67.11%, tr_best:  67.11%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  5.783001/  7.625000, val:  57.08%, val_best:  58.33%, tr:  67.21%, tr_best:  67.21%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  6.079196/  7.044969, val:  64.58%, val_best:  64.58%, tr:  67.01%, tr_best:  67.21%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  5.004401/  6.858696, val:  59.58%, val_best:  64.58%, tr:  70.58%, tr_best:  70.58%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  3.507849/  7.267656, val:  62.08%, val_best:  64.58%, tr:  77.22%, tr_best:  77.22%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  2.769278/  4.755749, val:  60.00%, val_best:  64.58%, tr:  82.23%, tr_best:  82.23%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  2.681305/  7.542711, val:  47.50%, val_best:  64.58%, tr:  81.21%, tr_best:  82.23%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  3.155735/  5.203655, val:  71.67%, val_best:  71.67%, tr:  78.45%, tr_best:  82.23%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  3.181603/  6.026490, val:  73.75%, val_best:  73.75%, tr:  80.08%, tr_best:  82.23%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  3.814948/  6.548019, val:  68.75%, val_best:  73.75%, tr:  83.66%, tr_best:  83.66%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  2.380105/  4.843581, val:  65.42%, val_best:  73.75%, tr:  84.78%, tr_best:  84.78%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  2.170409/  5.373145, val:  67.92%, val_best:  73.75%, tr:  84.88%, tr_best:  84.88%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  1.905097/  5.604249, val:  67.92%, val_best:  73.75%, tr:  87.44%, tr_best:  87.44%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  1.625875/  4.304320, val:  75.00%, val_best:  75.00%, tr:  89.68%, tr_best:  89.68%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  2.542506/  5.660724, val:  65.00%, val_best:  75.00%, tr:  86.01%, tr_best:  89.68%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  1.801299/  5.085083, val:  74.17%, val_best:  75.00%, tr:  88.36%, tr_best:  89.68%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  1.521657/  4.739160, val:  78.33%, val_best:  78.33%, tr:  90.70%, tr_best:  90.70%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  1.182837/  4.602853, val:  78.75%, val_best:  78.75%, tr:  93.05%, tr_best:  93.05%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  1.155861/  4.279298, val:  79.58%, val_best:  79.58%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  1.053911/  4.546279, val:  75.00%, val_best:  79.58%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  1.401965/  6.095192, val:  71.67%, val_best:  79.58%, tr:  91.42%, tr_best:  94.99%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.992079/  4.488101, val:  75.42%, val_best:  79.58%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.798873/  4.111193, val:  75.42%, val_best:  79.58%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  1.404537/  4.926178, val:  79.17%, val_best:  79.58%, tr:  91.42%, tr_best:  96.83%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  1.356179/  7.054396, val:  65.83%, val_best:  79.58%, tr:  93.36%, tr_best:  96.83%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.885388/  5.191797, val:  77.50%, val_best:  79.58%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.744331/  4.687033, val:  78.75%, val_best:  79.58%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.591200/  4.504378, val:  81.25%, val_best:  81.25%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.798706/  4.779676, val:  82.92%, val_best:  82.92%, tr:  95.91%, tr_best:  98.77%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.561604/  5.007193, val:  72.50%, val_best:  82.92%, tr:  98.47%, tr_best:  98.77%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.419592/  4.643741, val:  77.08%, val_best:  82.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.350482/  3.908427, val:  83.75%, val_best:  83.75%, tr:  99.28%, tr_best:  99.69%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.336222/  3.979895, val:  81.67%, val_best:  83.75%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.311208/  3.884775, val:  82.50%, val_best:  83.75%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.429584/  3.904986, val:  84.17%, val_best:  84.17%, tr:  98.26%, tr_best:  99.69%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.296117/  4.062298, val:  84.58%, val_best:  84.58%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.264469/  3.915855, val:  80.00%, val_best:  84.58%, tr:  99.28%, tr_best:  99.69%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.306946/  4.066405, val:  80.00%, val_best:  84.58%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.485396/  4.493538, val:  79.58%, val_best:  84.58%, tr:  97.04%, tr_best:  99.69%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.230915/  4.666381, val:  80.00%, val_best:  84.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.226608/  4.440581, val:  81.67%, val_best:  84.58%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.212162/  4.379386, val:  80.00%, val_best:  84.58%, tr:  99.18%, tr_best:  99.90%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.208648/  4.541428, val:  82.08%, val_best:  84.58%, tr:  99.39%, tr_best:  99.90%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.186598/  4.099732, val:  84.17%, val_best:  84.58%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.144200/  4.159714, val:  84.58%, val_best:  84.58%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.142987/  3.982818, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.122386/  4.038805, val:  85.00%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.099609/  4.268592, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.117463/  4.182202, val:  85.42%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.108245/  4.157699, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.121400/  4.397201, val:  82.50%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.114954/  4.089480, val:  84.58%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.092288/  4.450796, val:  81.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.073501/  4.210776, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.080536/  4.136793, val:  85.00%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.082080/  4.271053, val:  82.92%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.066686/  4.199856, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.057471/  4.359191, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.035677/  4.246780, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.060584/  4.648414, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.058065/  4.323585, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.051445/  4.624811, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.039253/  4.543352, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.049270/  4.655951, val:  82.08%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.038617/  4.352482, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.044346/  4.353676, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.047817/  4.342651, val:  85.42%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.040594/  4.445801, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.048057/  4.448680, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.048059/  4.618350, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.042233/  4.571725, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.036220/  4.560075, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.022757/  4.465161, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.023280/  4.560931, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.016282/  4.534508, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.022180/  4.601068, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.034328/  4.589185, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.018554/  4.567242, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.021512/  4.612657, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.013362/  4.549961, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.032554/  4.528603, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.047350/  4.581904, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.024802/  4.616391, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.022029/  4.695134, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.016959/  4.737477, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.033603/  4.707391, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5bb26d53864e319097a89abc73f66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▃▄▃▆▅▆▆▇▇███▇▇███▇████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▂▃▃▄▅▅▅▆▅▆▆▇▇▇▇█▇▇█▇▇████████▇█████▇██</td></tr><tr><td>tr_acc</td><td>▁▃▄▄▄▄▅▅▆▆▇▇▇██▇████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▅█▅▄▄▆▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇▇███████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▂▃▃▄▅▅▅▆▅▆▆▇▇▇▇█▇▇█▇▇████████▇█████▇██</td></tr><tr><td>val_loss</td><td>▇▇▅█▅▄▆▄▂▂▂▃▂▁▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▂▂▁▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0336</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.82083</td></tr><tr><td>val_loss</td><td>4.70739</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-sweep-48</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/eixbqhi7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/eixbqhi7</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_190353-eixbqhi7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fl34nbhx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_190856-fl34nbhx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fl34nbhx' target=\"_blank\">comic-sweep-50</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fl34nbhx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fl34nbhx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.324714/  2.286268, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.692343/  1.440458, val:  57.92%, val_best:  57.92%, tr:  40.04%, tr_best:  40.04%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.407705/  1.385422, val:  54.58%, val_best:  57.92%, tr:  57.41%, tr_best:  57.41%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.250945/  1.495150, val:  54.58%, val_best:  57.92%, tr:  62.10%, tr_best:  62.10%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.170930/  1.371023, val:  62.50%, val_best:  62.50%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.085329/  1.281871, val:  65.42%, val_best:  65.42%, tr:  67.52%, tr_best:  67.52%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.092702/  1.231467, val:  65.00%, val_best:  65.42%, tr:  68.03%, tr_best:  68.03%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.048118/  1.240324, val:  63.75%, val_best:  65.42%, tr:  67.82%, tr_best:  68.03%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.029154/  1.328223, val:  63.33%, val_best:  65.42%, tr:  71.40%, tr_best:  71.40%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.951323/  1.290054, val:  65.83%, val_best:  65.83%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.959766/  1.244060, val:  67.92%, val_best:  67.92%, tr:  75.89%, tr_best:  76.30%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.816679/  1.202070, val:  73.75%, val_best:  73.75%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.747890/  1.141234, val:  81.25%, val_best:  81.25%, tr:  84.98%, tr_best:  84.98%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.703539/  1.185826, val:  82.92%, val_best:  82.92%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.706075/  1.391682, val:  73.75%, val_best:  82.92%, tr:  86.72%, tr_best:  88.56%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.604286/  1.224734, val:  80.83%, val_best:  82.92%, tr:  90.09%, tr_best:  90.09%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.561919/  1.172217, val:  82.92%, val_best:  82.92%, tr:  93.87%, tr_best:  93.87%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.524343/  1.229921, val:  81.67%, val_best:  82.92%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.487522/  1.335530, val:  77.08%, val_best:  82.92%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.479926/  1.187144, val:  83.33%, val_best:  83.33%, tr:  94.99%, tr_best:  95.10%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.454232/  1.284937, val:  83.75%, val_best:  83.75%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.411124/  1.321166, val:  80.83%, val_best:  83.75%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.401416/  1.398801, val:  80.42%, val_best:  83.75%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.409850/  1.301508, val:  85.42%, val_best:  85.42%, tr:  96.12%, tr_best:  97.04%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.360577/  1.349709, val:  82.92%, val_best:  85.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.360081/  1.373224, val:  81.67%, val_best:  85.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.361630/  1.309298, val:  87.92%, val_best:  87.92%, tr:  97.65%, tr_best:  98.16%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.317644/  1.488184, val:  83.33%, val_best:  87.92%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.294707/  1.370536, val:  87.92%, val_best:  87.92%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.279281/  1.545154, val:  82.50%, val_best:  87.92%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.291486/  1.466846, val:  86.25%, val_best:  87.92%, tr:  98.67%, tr_best:  98.88%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.242385/  1.425911, val:  86.67%, val_best:  87.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.281249/  1.508531, val:  85.00%, val_best:  87.92%, tr:  98.57%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.262332/  1.540805, val:  84.17%, val_best:  87.92%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.239263/  1.657626, val:  81.67%, val_best:  87.92%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.222287/  1.524557, val:  85.42%, val_best:  87.92%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.192569/  1.567645, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.187307/  1.559691, val:  86.25%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.187297/  1.532413, val:  87.50%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.185414/  1.662082, val:  86.25%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.180150/  1.578279, val:  87.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.158662/  1.649303, val:  86.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.158380/  1.604576, val:  86.67%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.139144/  1.646242, val:  88.33%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.147587/  1.736517, val:  87.50%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.135313/  1.709815, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.136648/  1.758928, val:  87.08%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.127842/  1.726192, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.144416/  1.766882, val:  86.67%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.118283/  1.811773, val:  86.67%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.115775/  1.887517, val:  87.08%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.104303/  1.855918, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.111773/  1.860125, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.093209/  1.883933, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.086004/  1.906306, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.092298/  1.886673, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.088149/  1.965053, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.090941/  1.955923, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.079389/  1.952051, val:  88.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.082049/  2.054672, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.076718/  1.986453, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.071572/  2.013733, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.068816/  2.052522, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.072312/  2.065197, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.067766/  2.081539, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.059748/  2.099961, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.057278/  2.171205, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.067525/  2.196870, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.070476/  2.157996, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.052847/  2.168622, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.048771/  2.175929, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.056615/  2.214395, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.060089/  2.297812, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.051547/  2.244307, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.054256/  2.262815, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.048683/  2.273809, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.051884/  2.321156, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.053528/  2.329639, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.063437/  2.308314, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.066046/  2.336577, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.049934/  2.301535, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.041843/  2.348136, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.047969/  2.361818, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.045226/  2.312370, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.044027/  2.361408, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.043382/  2.397276, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.042749/  2.363960, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.038159/  2.341981, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.035929/  2.393583, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.040976/  2.445968, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.034906/  2.408180, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.032521/  2.446563, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.031469/  2.427554, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.029490/  2.477440, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.031134/  2.455287, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.027424/  2.534578, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.025009/  2.504771, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.028157/  2.479767, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.026861/  2.498552, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.032697/  2.532003, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9581ec103674d20a407e3ac2543cb26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▄▃▇▇▆▇▇▇█████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▆▇▇▇▇▇▇█▇███████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇▇▇███████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▆▇▇▇▇▇▇█▇███████████████████████████</td></tr><tr><td>val_loss</td><td>▇▂▂▁▂▁▂▁▁▂▂▂▃▃▃▃▄▃▄▄▄▅▅▅▆▅▆▆▆▇▇▇▇▇▇▇█▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0327</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>2.532</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comic-sweep-50</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fl34nbhx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fl34nbhx</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_190856-fl34nbhx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6arfv7vj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f53cae1b174b8d96a3a5f7c421861d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113292542803618, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_191441-6arfv7vj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6arfv7vj' target=\"_blank\">playful-sweep-52</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6arfv7vj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6arfv7vj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.324714/  2.286268, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.692343/  1.440458, val:  57.92%, val_best:  57.92%, tr:  40.04%, tr_best:  40.04%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.407705/  1.385422, val:  54.58%, val_best:  57.92%, tr:  57.41%, tr_best:  57.41%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.250945/  1.495150, val:  54.58%, val_best:  57.92%, tr:  62.10%, tr_best:  62.10%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.170930/  1.371023, val:  62.50%, val_best:  62.50%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.085329/  1.281871, val:  65.42%, val_best:  65.42%, tr:  67.52%, tr_best:  67.52%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.092702/  1.231467, val:  65.00%, val_best:  65.42%, tr:  68.03%, tr_best:  68.03%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.048118/  1.240324, val:  63.75%, val_best:  65.42%, tr:  67.82%, tr_best:  68.03%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.029154/  1.328223, val:  63.33%, val_best:  65.42%, tr:  71.40%, tr_best:  71.40%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.951323/  1.290054, val:  65.83%, val_best:  65.83%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.959766/  1.244060, val:  67.92%, val_best:  67.92%, tr:  75.89%, tr_best:  76.30%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.816679/  1.202070, val:  73.75%, val_best:  73.75%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.747890/  1.141234, val:  81.25%, val_best:  81.25%, tr:  84.98%, tr_best:  84.98%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.703539/  1.185826, val:  82.92%, val_best:  82.92%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.706075/  1.391682, val:  73.75%, val_best:  82.92%, tr:  86.72%, tr_best:  88.56%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.604286/  1.224734, val:  80.83%, val_best:  82.92%, tr:  90.09%, tr_best:  90.09%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.561919/  1.172217, val:  82.92%, val_best:  82.92%, tr:  93.87%, tr_best:  93.87%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.524343/  1.229921, val:  81.67%, val_best:  82.92%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.487522/  1.335530, val:  77.08%, val_best:  82.92%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.479926/  1.187144, val:  83.33%, val_best:  83.33%, tr:  94.99%, tr_best:  95.10%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.454232/  1.284937, val:  83.75%, val_best:  83.75%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.411124/  1.321166, val:  80.83%, val_best:  83.75%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.401416/  1.398801, val:  80.42%, val_best:  83.75%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.409850/  1.301508, val:  85.42%, val_best:  85.42%, tr:  96.12%, tr_best:  97.04%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.360577/  1.349709, val:  82.92%, val_best:  85.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.360081/  1.373224, val:  81.67%, val_best:  85.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.361630/  1.309298, val:  87.92%, val_best:  87.92%, tr:  97.65%, tr_best:  98.16%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.317644/  1.488184, val:  83.33%, val_best:  87.92%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.294707/  1.370536, val:  87.92%, val_best:  87.92%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.279281/  1.545154, val:  82.50%, val_best:  87.92%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.291486/  1.466846, val:  86.25%, val_best:  87.92%, tr:  98.67%, tr_best:  98.88%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.242385/  1.425911, val:  86.67%, val_best:  87.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.281249/  1.508531, val:  85.00%, val_best:  87.92%, tr:  98.57%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.262332/  1.540805, val:  84.17%, val_best:  87.92%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.239263/  1.657626, val:  81.67%, val_best:  87.92%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.222287/  1.524557, val:  85.42%, val_best:  87.92%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.192569/  1.567645, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.187307/  1.559691, val:  86.25%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.187297/  1.532413, val:  87.50%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.185414/  1.662082, val:  86.25%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.180150/  1.578279, val:  87.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.158662/  1.649303, val:  86.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.158380/  1.604576, val:  86.67%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.139144/  1.646242, val:  88.33%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.147587/  1.736517, val:  87.50%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.135313/  1.709815, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.136648/  1.758928, val:  87.08%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.127842/  1.726192, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.144416/  1.766882, val:  86.67%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.118283/  1.811773, val:  86.67%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.115775/  1.887517, val:  87.08%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.104303/  1.855918, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.111773/  1.860125, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.093209/  1.883933, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.086004/  1.906306, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.092298/  1.886673, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.088149/  1.965053, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.090941/  1.955923, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.079389/  1.952051, val:  88.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.082049/  2.054672, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.076718/  1.986453, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.071572/  2.013733, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.068816/  2.052522, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.072312/  2.065197, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.067766/  2.081539, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.059748/  2.099961, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.057278/  2.171205, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.067525/  2.196870, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.070476/  2.157996, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.052847/  2.168622, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.048771/  2.175929, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.056615/  2.214395, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.060089/  2.297812, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.051547/  2.244307, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.054256/  2.262815, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.048683/  2.273809, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.051884/  2.321156, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.053528/  2.329639, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.063437/  2.308314, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.066046/  2.336577, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.049934/  2.301535, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.041843/  2.348136, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.047969/  2.361818, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.045226/  2.312370, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.044027/  2.361408, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.043382/  2.397276, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.042749/  2.363960, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.038159/  2.341981, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.035929/  2.393583, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.040976/  2.445968, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.034906/  2.408180, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.032521/  2.446563, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.031469/  2.427554, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.029490/  2.477440, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.031134/  2.455287, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.027424/  2.534578, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.025009/  2.504771, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.028157/  2.479767, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.026861/  2.498552, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.032697/  2.532003, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a176da70db444a18a82a7d2f65265fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▄▃▇▇▆▇▇▇█████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▆▇▇▇▇▇▇█▇███████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇▇▇███████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▆▇▇▇▇▇▇█▇███████████████████████████</td></tr><tr><td>val_loss</td><td>▇▂▂▁▂▁▂▁▁▂▂▂▃▃▃▃▄▃▄▄▄▅▅▅▆▅▆▆▆▇▇▇▇▇▇▇█▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0327</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>2.532</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-sweep-52</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6arfv7vj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6arfv7vj</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_191441-6arfv7vj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fpvlw4kv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_191949-fpvlw4kv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fpvlw4kv' target=\"_blank\">efficient-sweep-54</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fpvlw4kv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fpvlw4kv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.324714/  2.286268, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.692343/  1.440458, val:  57.92%, val_best:  57.92%, tr:  40.04%, tr_best:  40.04%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.407705/  1.385422, val:  54.58%, val_best:  57.92%, tr:  57.41%, tr_best:  57.41%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.250945/  1.495150, val:  54.58%, val_best:  57.92%, tr:  62.10%, tr_best:  62.10%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.170930/  1.371023, val:  62.50%, val_best:  62.50%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.085329/  1.281871, val:  65.42%, val_best:  65.42%, tr:  67.52%, tr_best:  67.52%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.092702/  1.231467, val:  65.00%, val_best:  65.42%, tr:  68.03%, tr_best:  68.03%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.048118/  1.240324, val:  63.75%, val_best:  65.42%, tr:  67.82%, tr_best:  68.03%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.029154/  1.328223, val:  63.33%, val_best:  65.42%, tr:  71.40%, tr_best:  71.40%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.951323/  1.290054, val:  65.83%, val_best:  65.83%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.959766/  1.244060, val:  67.92%, val_best:  67.92%, tr:  75.89%, tr_best:  76.30%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.816679/  1.202070, val:  73.75%, val_best:  73.75%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.747890/  1.141234, val:  81.25%, val_best:  81.25%, tr:  84.98%, tr_best:  84.98%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.703539/  1.185826, val:  82.92%, val_best:  82.92%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.706075/  1.391682, val:  73.75%, val_best:  82.92%, tr:  86.72%, tr_best:  88.56%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.604286/  1.224734, val:  80.83%, val_best:  82.92%, tr:  90.09%, tr_best:  90.09%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.561919/  1.172217, val:  82.92%, val_best:  82.92%, tr:  93.87%, tr_best:  93.87%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.524343/  1.229921, val:  81.67%, val_best:  82.92%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.487522/  1.335530, val:  77.08%, val_best:  82.92%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.479926/  1.187144, val:  83.33%, val_best:  83.33%, tr:  94.99%, tr_best:  95.10%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.454232/  1.284937, val:  83.75%, val_best:  83.75%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.411124/  1.321166, val:  80.83%, val_best:  83.75%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.401416/  1.398801, val:  80.42%, val_best:  83.75%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.409850/  1.301508, val:  85.42%, val_best:  85.42%, tr:  96.12%, tr_best:  97.04%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.360577/  1.349709, val:  82.92%, val_best:  85.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.360081/  1.373224, val:  81.67%, val_best:  85.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.361630/  1.309298, val:  87.92%, val_best:  87.92%, tr:  97.65%, tr_best:  98.16%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.317644/  1.488184, val:  83.33%, val_best:  87.92%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.294707/  1.370536, val:  87.92%, val_best:  87.92%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.279281/  1.545154, val:  82.50%, val_best:  87.92%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.291486/  1.466846, val:  86.25%, val_best:  87.92%, tr:  98.67%, tr_best:  98.88%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.242385/  1.425911, val:  86.67%, val_best:  87.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.281249/  1.508531, val:  85.00%, val_best:  87.92%, tr:  98.57%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.262332/  1.540805, val:  84.17%, val_best:  87.92%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.239263/  1.657626, val:  81.67%, val_best:  87.92%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.222287/  1.524557, val:  85.42%, val_best:  87.92%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.192569/  1.567645, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.187307/  1.559691, val:  86.25%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.187297/  1.532413, val:  87.50%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.185414/  1.662082, val:  86.25%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.180150/  1.578279, val:  87.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.158662/  1.649303, val:  86.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.158380/  1.604576, val:  86.67%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.139144/  1.646242, val:  88.33%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.147587/  1.736517, val:  87.50%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.135313/  1.709815, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.136648/  1.758928, val:  87.08%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.127842/  1.726192, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.144416/  1.766882, val:  86.67%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.118283/  1.811773, val:  86.67%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.115775/  1.887517, val:  87.08%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.104303/  1.855918, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.111773/  1.860125, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.093209/  1.883933, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.086004/  1.906306, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.092298/  1.886673, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.088149/  1.965053, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.090941/  1.955923, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.079389/  1.952051, val:  88.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.082049/  2.054672, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.076718/  1.986453, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.071572/  2.013733, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.068816/  2.052522, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.072312/  2.065197, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.067766/  2.081539, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.059748/  2.099961, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.057278/  2.171205, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.067525/  2.196870, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.070476/  2.157996, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.052847/  2.168622, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.048771/  2.175929, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.056615/  2.214395, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.060089/  2.297812, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.051547/  2.244307, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.054256/  2.262815, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.048683/  2.273809, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.051884/  2.321156, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.053528/  2.329639, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.063437/  2.308314, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.066046/  2.336577, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.049934/  2.301535, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.041843/  2.348136, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.047969/  2.361818, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.045226/  2.312370, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.044027/  2.361408, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.043382/  2.397276, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.042749/  2.363960, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.038159/  2.341981, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.035929/  2.393583, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.040976/  2.445968, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.034906/  2.408180, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.032521/  2.446563, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.031469/  2.427554, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.029490/  2.477440, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.031134/  2.455287, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.027424/  2.534578, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.025009/  2.504771, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.028157/  2.479767, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.026861/  2.498552, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.032697/  2.532003, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e50c259d7d4138a0327caa2365efa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▄▃▇▇▆▇▇▇█████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▆▇▇▇▇▇▇█▇███████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇▇▇███████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▆▇▇▇▇▇▇█▇███████████████████████████</td></tr><tr><td>val_loss</td><td>▇▂▂▁▂▁▂▁▁▂▂▂▃▃▃▃▄▃▄▄▄▅▅▅▆▅▆▆▆▇▇▇▇▇▇▇█▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0327</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>2.532</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-sweep-54</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fpvlw4kv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fpvlw4kv</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_191949-fpvlw4kv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2n7w679s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_192457-2n7w679s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2n7w679s' target=\"_blank\">astral-sweep-56</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2n7w679s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2n7w679s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.324714/  2.286268, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.692343/  1.440458, val:  57.92%, val_best:  57.92%, tr:  40.04%, tr_best:  40.04%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.407705/  1.385422, val:  54.58%, val_best:  57.92%, tr:  57.41%, tr_best:  57.41%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.250945/  1.495150, val:  54.58%, val_best:  57.92%, tr:  62.10%, tr_best:  62.10%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.170930/  1.371023, val:  62.50%, val_best:  62.50%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.085329/  1.281871, val:  65.42%, val_best:  65.42%, tr:  67.52%, tr_best:  67.52%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.092702/  1.231467, val:  65.00%, val_best:  65.42%, tr:  68.03%, tr_best:  68.03%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.048118/  1.240324, val:  63.75%, val_best:  65.42%, tr:  67.82%, tr_best:  68.03%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.029154/  1.328223, val:  63.33%, val_best:  65.42%, tr:  71.40%, tr_best:  71.40%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.951323/  1.290054, val:  65.83%, val_best:  65.83%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.959766/  1.244060, val:  67.92%, val_best:  67.92%, tr:  75.89%, tr_best:  76.30%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.816679/  1.202070, val:  73.75%, val_best:  73.75%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.747890/  1.141234, val:  81.25%, val_best:  81.25%, tr:  84.98%, tr_best:  84.98%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.703539/  1.185826, val:  82.92%, val_best:  82.92%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.706075/  1.391682, val:  73.75%, val_best:  82.92%, tr:  86.72%, tr_best:  88.56%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.604286/  1.224734, val:  80.83%, val_best:  82.92%, tr:  90.09%, tr_best:  90.09%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.561919/  1.172217, val:  82.92%, val_best:  82.92%, tr:  93.87%, tr_best:  93.87%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.524343/  1.229921, val:  81.67%, val_best:  82.92%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.487522/  1.335530, val:  77.08%, val_best:  82.92%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.479926/  1.187144, val:  83.33%, val_best:  83.33%, tr:  94.99%, tr_best:  95.10%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.454232/  1.284937, val:  83.75%, val_best:  83.75%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.411124/  1.321166, val:  80.83%, val_best:  83.75%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.401416/  1.398801, val:  80.42%, val_best:  83.75%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.409850/  1.301508, val:  85.42%, val_best:  85.42%, tr:  96.12%, tr_best:  97.04%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.360577/  1.349709, val:  82.92%, val_best:  85.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.360081/  1.373224, val:  81.67%, val_best:  85.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.361630/  1.309298, val:  87.92%, val_best:  87.92%, tr:  97.65%, tr_best:  98.16%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.317644/  1.488184, val:  83.33%, val_best:  87.92%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.294707/  1.370536, val:  87.92%, val_best:  87.92%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.279281/  1.545154, val:  82.50%, val_best:  87.92%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.291486/  1.466846, val:  86.25%, val_best:  87.92%, tr:  98.67%, tr_best:  98.88%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.242385/  1.425911, val:  86.67%, val_best:  87.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.281249/  1.508531, val:  85.00%, val_best:  87.92%, tr:  98.57%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.262332/  1.540805, val:  84.17%, val_best:  87.92%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.239263/  1.657626, val:  81.67%, val_best:  87.92%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.222287/  1.524557, val:  85.42%, val_best:  87.92%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.192569/  1.567645, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.187307/  1.559691, val:  86.25%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.187297/  1.532413, val:  87.50%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.185414/  1.662082, val:  86.25%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.180150/  1.578279, val:  87.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.158662/  1.649303, val:  86.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.158380/  1.604576, val:  86.67%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.139144/  1.646242, val:  88.33%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.147587/  1.736517, val:  87.50%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.135313/  1.709815, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.136648/  1.758928, val:  87.08%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.127842/  1.726192, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.144416/  1.766882, val:  86.67%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.118283/  1.811773, val:  86.67%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.115775/  1.887517, val:  87.08%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.104303/  1.855918, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.111773/  1.860125, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.093209/  1.883933, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.086004/  1.906306, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.092298/  1.886673, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.088149/  1.965053, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.090941/  1.955923, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.079389/  1.952051, val:  88.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.082049/  2.054672, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.076718/  1.986453, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.071572/  2.013733, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.068816/  2.052522, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.072312/  2.065197, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.067766/  2.081539, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.059748/  2.099961, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.057278/  2.171205, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.067525/  2.196870, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.070476/  2.157996, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.052847/  2.168622, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.048771/  2.175929, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.056615/  2.214395, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.060089/  2.297812, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.051547/  2.244307, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.054256/  2.262815, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.048683/  2.273809, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.051884/  2.321156, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.053528/  2.329639, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.063437/  2.308314, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.066046/  2.336577, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.049934/  2.301535, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.041843/  2.348136, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.047969/  2.361818, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.045226/  2.312370, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.044027/  2.361408, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.043382/  2.397276, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.042749/  2.363960, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.038159/  2.341981, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.035929/  2.393583, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.040976/  2.445968, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.034906/  2.408180, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.032521/  2.446563, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.031469/  2.427554, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.029490/  2.477440, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.031134/  2.455287, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.027424/  2.534578, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.025009/  2.504771, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.028157/  2.479767, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.026861/  2.498552, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.032697/  2.532003, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c869857936d44c2884a39187bf916ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▄▃▇▇▆▇▇▇█████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▆▇▇▇▇▇▇█▇███████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇▇▇███████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▆▇▇▇▇▇▇█▇███████████████████████████</td></tr><tr><td>val_loss</td><td>▇▂▂▁▂▁▂▁▁▂▂▂▃▃▃▃▄▃▄▄▄▅▅▅▆▅▆▆▆▇▇▇▇▇▇▇█▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0327</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>2.532</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">astral-sweep-56</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2n7w679s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2n7w679s</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_192457-2n7w679s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oq9gae14 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_193014-oq9gae14</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oq9gae14' target=\"_blank\">polished-sweep-58</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oq9gae14' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oq9gae14</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.5, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.5, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 32.034203/ 26.668982, val:  39.58%, val_best:  39.58%, tr:  24.82%, tr_best:  24.82%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 20.268265/ 26.774166, val:  38.33%, val_best:  39.58%, tr:  39.94%, tr_best:  39.94%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 18.931946/ 24.005432, val:  42.50%, val_best:  42.50%, tr:  46.37%, tr_best:  46.37%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 23.274698/ 24.207968, val:  40.00%, val_best:  42.50%, tr:  45.05%, tr_best:  46.37%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 19.455961/ 23.155205, val:  39.58%, val_best:  42.50%, tr:  50.66%, tr_best:  50.66%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 18.278419/ 25.063850, val:  53.33%, val_best:  53.33%, tr:  50.77%, tr_best:  50.77%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 14.740679/ 20.673677, val:  53.33%, val_best:  53.33%, tr:  54.95%, tr_best:  54.95%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 23.926901/ 18.569147, val:  47.92%, val_best:  53.33%, tr:  50.87%, tr_best:  54.95%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 16.110846/ 20.753477, val:  47.92%, val_best:  53.33%, tr:  59.55%, tr_best:  59.55%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 18.050659/ 29.864584, val:  36.25%, val_best:  53.33%, tr:  57.30%, tr_best:  59.55%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 21.503334/ 17.467577, val:  61.25%, val_best:  61.25%, tr:  56.38%, tr_best:  59.55%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 13.774194/ 19.872665, val:  48.33%, val_best:  61.25%, tr:  63.84%, tr_best:  63.84%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 13.632082/ 11.009504, val:  57.92%, val_best:  61.25%, tr:  63.02%, tr_best:  63.84%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 11.740593/ 23.384182, val:  46.25%, val_best:  61.25%, tr:  63.33%, tr_best:  63.84%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 12.388375/ 14.629869, val:  62.08%, val_best:  62.08%, tr:  64.86%, tr_best:  64.86%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  8.304804/ 22.861294, val:  47.08%, val_best:  62.08%, tr:  69.66%, tr_best:  69.66%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 14.423633/  9.755350, val:  69.17%, val_best:  69.17%, tr:  67.52%, tr_best:  69.66%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss: 10.916103/ 17.034357, val:  59.58%, val_best:  69.17%, tr:  68.44%, tr_best:  69.66%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss: 11.284723/ 17.283527, val:  51.25%, val_best:  69.17%, tr:  67.72%, tr_best:  69.66%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  9.228921/ 15.041286, val:  55.83%, val_best:  69.17%, tr:  70.89%, tr_best:  70.89%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 11.935574/ 19.331774, val:  60.00%, val_best:  69.17%, tr:  72.11%, tr_best:  72.11%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  9.705071/ 14.757591, val:  74.17%, val_best:  74.17%, tr:  75.89%, tr_best:  75.89%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  7.443206/ 16.142220, val:  57.92%, val_best:  74.17%, tr:  80.90%, tr_best:  80.90%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  9.360426/ 14.486137, val:  72.08%, val_best:  74.17%, tr:  77.22%, tr_best:  80.90%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  4.568964/ 12.811187, val:  72.08%, val_best:  74.17%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  4.008320/ 10.730420, val:  70.83%, val_best:  74.17%, tr:  89.07%, tr_best:  89.07%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  5.936326/ 15.593969, val:  60.83%, val_best:  74.17%, tr:  83.45%, tr_best:  89.07%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  4.709736/ 16.808680, val:  60.42%, val_best:  74.17%, tr:  87.44%, tr_best:  89.07%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  6.552180/ 12.197251, val:  74.58%, val_best:  74.58%, tr:  84.07%, tr_best:  89.07%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  3.721158/ 12.923842, val:  68.33%, val_best:  74.58%, tr:  92.65%, tr_best:  92.65%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  5.155349/ 14.426227, val:  74.17%, val_best:  74.58%, tr:  86.31%, tr_best:  92.65%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  3.783756/ 12.404920, val:  77.92%, val_best:  77.92%, tr:  92.65%, tr_best:  92.65%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  4.138615/ 13.070007, val:  75.00%, val_best:  77.92%, tr:  91.73%, tr_best:  92.65%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  2.891200/ 10.953104, val:  80.42%, val_best:  80.42%, tr:  95.30%, tr_best:  95.30%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  2.809086/ 11.061913, val:  79.17%, val_best:  80.42%, tr:  94.48%, tr_best:  95.30%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  2.048766/ 10.035550, val:  82.92%, val_best:  82.92%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  1.847430/ 10.733336, val:  82.50%, val_best:  82.92%, tr:  97.14%, tr_best:  97.24%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  1.979152/ 12.092752, val:  77.50%, val_best:  82.92%, tr:  96.12%, tr_best:  97.24%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  1.976498/ 12.854142, val:  76.67%, val_best:  82.92%, tr:  96.12%, tr_best:  97.24%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  1.534738/ 11.617446, val:  79.58%, val_best:  82.92%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  1.217125/  9.670904, val:  83.75%, val_best:  83.75%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  1.078621/ 11.090146, val:  79.58%, val_best:  83.75%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  1.537969/ 12.161247, val:  79.17%, val_best:  83.75%, tr:  96.94%, tr_best:  98.98%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  1.393319/ 13.566648, val:  74.17%, val_best:  83.75%, tr:  97.65%, tr_best:  98.98%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  1.513227/ 12.881515, val:  76.25%, val_best:  83.75%, tr:  97.65%, tr_best:  98.98%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  1.102108/ 12.042429, val:  81.25%, val_best:  83.75%, tr:  98.77%, tr_best:  98.98%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.910999/ 11.962527, val:  82.92%, val_best:  83.75%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.761435/ 10.532009, val:  83.75%, val_best:  83.75%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.733529/ 11.425110, val:  84.17%, val_best:  84.17%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.692407/ 11.521693, val:  82.92%, val_best:  84.17%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.565821/ 11.867971, val:  83.75%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.577807/ 11.405119, val:  84.17%, val_best:  84.17%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.529271/ 11.132936, val:  82.92%, val_best:  84.17%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.387885/ 11.462857, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.550816/ 11.883949, val:  83.33%, val_best:  84.17%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.429181/ 11.842879, val:  84.58%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.404116/ 11.969156, val:  81.25%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.345411/ 11.896661, val:  83.75%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.360201/ 11.934253, val:  85.83%, val_best:  85.83%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.380242/ 11.984423, val:  82.08%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.268402/ 11.927772, val:  81.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.296782/ 11.163910, val:  87.08%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.204471/ 11.688086, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.183045/ 11.576455, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.191700/ 12.206753, val:  82.08%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.158700/ 12.227183, val:  80.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.283194/ 11.732118, val:  82.50%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.258340/ 12.478513, val:  77.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.236238/ 12.455875, val:  81.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.181754/ 12.055372, val:  84.17%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.119193/ 11.555779, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.105077/ 12.117676, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.117159/ 12.151821, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.107038/ 11.889531, val:  85.00%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.101425/ 11.989759, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.108369/ 11.898555, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.084182/ 12.801980, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.077911/ 12.032183, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.075836/ 12.332909, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.052677/ 12.697571, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.105167/ 13.182324, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.086281/ 12.588728, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.057360/ 13.044386, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.084874/ 13.052512, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.171768/ 13.096096, val:  82.50%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.080695/ 13.136653, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.069926/ 12.540607, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.066707/ 12.774597, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.057334/ 13.012659, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.115583/ 12.520209, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.176291/ 13.001637, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.061358/ 12.893509, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.061321/ 12.625847, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.056669/ 13.014943, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.047437/ 12.822203, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.038845/ 12.776999, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.059178/ 13.500020, val:  79.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.038824/ 12.809677, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.057360/ 12.960200, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.071562/ 12.180201, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b3134d04944cc3a4ec6fe19c90bcb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄▄▃▅▅▃▂▇▇▇▇▇▇███▇▇████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▁▃▁▄▅▄▄▆▆▄▅▆▇▇▇▇▇█▇▇▇█▇█▇▇████▇▇▇███▇▇</td></tr><tr><td>tr_acc</td><td>▁▃▃▃▄▅▅▅▅▆▇▆▇▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▅▆▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▃▃▄▄▅▅▆▆▆▆▇▇▇▇███████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▁▃▁▄▅▄▄▆▆▄▅▆▇▇▇▇▇█▇▇▇█▇█▇▇████▇▇▇███▇▇</td></tr><tr><td>val_loss</td><td>▇▆▆▄█▁▃▃▃▃▂▃▂▂▁▂▂▂▂▁▂▁▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.07156</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.87083</td></tr><tr><td>val_loss</td><td>12.1802</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polished-sweep-58</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oq9gae14' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oq9gae14</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_193014-oq9gae14/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3y78xcbk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_193611-3y78xcbk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3y78xcbk' target=\"_blank\">vivid-sweep-60</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3y78xcbk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3y78xcbk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.325213/  2.309355, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.323420/  2.309336, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:   9.50%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.290745/  2.183865, val:  15.83%, val_best:  15.83%, tr:  11.64%, tr_best:  11.64%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.038579/  1.930964, val:  28.33%, val_best:  28.33%, tr:  21.45%, tr_best:  21.45%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.751258/  1.757687, val:  45.42%, val_best:  45.42%, tr:  41.98%, tr_best:  41.98%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.467292/  1.541066, val:  56.67%, val_best:  56.67%, tr:  55.26%, tr_best:  55.26%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.292819/  1.463983, val:  58.33%, val_best:  58.33%, tr:  63.02%, tr_best:  63.02%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.191316/  1.310786, val:  66.25%, val_best:  66.25%, tr:  60.67%, tr_best:  63.02%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.118217/  1.381130, val:  63.33%, val_best:  66.25%, tr:  67.01%, tr_best:  67.01%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.090238/  1.357372, val:  60.00%, val_best:  66.25%, tr:  69.97%, tr_best:  69.97%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.077696/  1.339415, val:  63.33%, val_best:  66.25%, tr:  66.91%, tr_best:  69.97%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.042638/  1.315298, val:  67.50%, val_best:  67.50%, tr:  69.56%, tr_best:  69.97%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.021591/  1.267219, val:  66.25%, val_best:  67.50%, tr:  72.93%, tr_best:  72.93%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.991357/  1.309041, val:  66.25%, val_best:  67.50%, tr:  72.22%, tr_best:  72.93%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.974443/  1.383952, val:  64.58%, val_best:  67.50%, tr:  75.28%, tr_best:  75.28%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.932868/  1.236217, val:  73.33%, val_best:  73.33%, tr:  75.08%, tr_best:  75.28%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.879629/  1.241795, val:  74.17%, val_best:  74.17%, tr:  81.31%, tr_best:  81.31%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.822820/  1.234459, val:  75.00%, val_best:  75.00%, tr:  86.21%, tr_best:  86.21%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.787923/  1.356278, val:  70.83%, val_best:  75.00%, tr:  84.88%, tr_best:  86.21%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.764003/  1.274469, val:  72.50%, val_best:  75.00%, tr:  85.60%, tr_best:  86.21%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.750060/  1.359515, val:  69.58%, val_best:  75.00%, tr:  85.80%, tr_best:  86.21%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.690590/  1.335848, val:  70.42%, val_best:  75.00%, tr:  89.68%, tr_best:  89.68%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.674804/  1.353800, val:  70.00%, val_best:  75.00%, tr:  92.03%, tr_best:  92.03%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.678268/  1.254296, val:  80.00%, val_best:  80.00%, tr:  90.40%, tr_best:  92.03%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.593653/  1.282786, val:  79.58%, val_best:  80.00%, tr:  94.79%, tr_best:  94.79%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.589402/  1.273674, val:  78.33%, val_best:  80.00%, tr:  94.18%, tr_best:  94.79%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.596184/  1.258723, val:  84.58%, val_best:  84.58%, tr:  93.56%, tr_best:  94.79%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.543833/  1.341927, val:  75.42%, val_best:  84.58%, tr:  94.08%, tr_best:  94.79%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.536264/  1.247039, val:  84.17%, val_best:  84.58%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.499192/  1.413270, val:  75.83%, val_best:  84.58%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.492557/  1.329232, val:  80.42%, val_best:  84.58%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.457069/  1.324560, val:  83.75%, val_best:  84.58%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.482454/  1.307022, val:  82.92%, val_best:  84.58%, tr:  95.51%, tr_best:  97.04%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.466972/  1.324482, val:  84.17%, val_best:  84.58%, tr:  96.53%, tr_best:  97.04%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.436571/  1.375254, val:  78.33%, val_best:  84.58%, tr:  96.73%, tr_best:  97.04%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.428035/  1.355064, val:  83.75%, val_best:  84.58%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.401326/  1.320549, val:  84.58%, val_best:  84.58%, tr:  97.45%, tr_best:  97.85%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.384241/  1.397443, val:  82.92%, val_best:  84.58%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.387975/  1.365901, val:  86.67%, val_best:  86.67%, tr:  98.16%, tr_best:  98.47%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.357199/  1.419261, val:  83.75%, val_best:  86.67%, tr:  98.26%, tr_best:  98.47%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.368700/  1.346059, val:  87.08%, val_best:  87.08%, tr:  97.75%, tr_best:  98.47%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.350706/  1.446784, val:  80.00%, val_best:  87.08%, tr:  97.96%, tr_best:  98.47%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.314539/  1.379327, val:  87.08%, val_best:  87.08%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.291533/  1.444554, val:  83.75%, val_best:  87.08%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.291223/  1.470895, val:  83.75%, val_best:  87.08%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.275483/  1.461703, val:  82.50%, val_best:  87.08%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.280608/  1.515837, val:  82.50%, val_best:  87.08%, tr:  99.08%, tr_best:  99.59%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.273781/  1.445600, val:  87.08%, val_best:  87.08%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.269875/  1.494322, val:  84.58%, val_best:  87.08%, tr:  99.39%, tr_best:  99.59%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.265195/  1.472672, val:  87.08%, val_best:  87.08%, tr:  98.88%, tr_best:  99.59%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.239865/  1.499624, val:  85.42%, val_best:  87.08%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.224577/  1.520685, val:  85.83%, val_best:  87.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.245564/  1.507180, val:  83.75%, val_best:  87.08%, tr:  99.28%, tr_best:  99.69%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.220963/  1.542994, val:  85.42%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.211197/  1.508017, val:  87.92%, val_best:  87.92%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.189874/  1.593661, val:  85.00%, val_best:  87.92%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.198342/  1.678787, val:  79.58%, val_best:  87.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.203404/  1.620555, val:  84.58%, val_best:  87.92%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.183294/  1.611250, val:  85.42%, val_best:  87.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.185624/  1.661091, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.171330/  1.616718, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.174923/  1.665880, val:  84.17%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.175139/  1.684671, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.161814/  1.642358, val:  85.42%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.147080/  1.690942, val:  87.08%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.139360/  1.737248, val:  87.50%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.134375/  1.713742, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.132850/  1.709264, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.129419/  1.806738, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.125623/  1.824162, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.120226/  1.797938, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.134045/  1.786428, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.116700/  1.870142, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.119061/  1.885970, val:  83.75%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.113637/  1.849379, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.109814/  1.845462, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.108254/  1.931945, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.108523/  1.925836, val:  86.25%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.115007/  1.962681, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.110432/  1.912355, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.106023/  1.956882, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.096873/  1.949191, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.095869/  1.976108, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.101724/  1.923836, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.096466/  1.930867, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.090655/  1.976351, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.096932/  1.972757, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.096662/  2.000497, val:  84.58%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.089029/  2.009511, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.101248/  2.036299, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.090401/  2.011409, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.068947/  2.049213, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.069378/  2.062595, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.069638/  2.050807, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.069854/  2.083913, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.066589/  2.118361, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.066422/  2.131360, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.060648/  2.132639, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.075012/  2.144089, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.064389/  2.109080, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23210af5c6d46339488a2e05c5806ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▂▅▆▆▇▃▇████▇██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▄▆▅▆▆▇▇▆▇█▇███████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▁▄▅▆▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▆▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▄▆▆▆▆▇▇▇▇█████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▄▆▅▆▆▇▇▆▇█▇███████████████████████████</td></tr><tr><td>val_loss</td><td>█▇▄▁▂▁▂▁▁▂▁▁▂▁▂▂▂▂▃▂▃▃▃▄▄▄▄▄▅▅▅▆▅▆▆▆▆▆▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.06439</td></tr><tr><td>val_acc_best</td><td>0.87917</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>2.10908</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-sweep-60</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3y78xcbk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3y78xcbk</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_193611-3y78xcbk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z47h8vte with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_194239-z47h8vte</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z47h8vte' target=\"_blank\">worldly-sweep-62</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z47h8vte' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z47h8vte</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.159312/  1.795517, val:  40.42%, val_best:  40.42%, tr:  18.39%, tr_best:  18.39%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.560156/  1.944167, val:  56.67%, val_best:  56.67%, tr:  51.89%, tr_best:  51.89%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.658199/  1.961934, val:  58.33%, val_best:  58.33%, tr:  58.73%, tr_best:  58.73%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.465286/  2.107206, val:  53.75%, val_best:  58.33%, tr:  61.80%, tr_best:  61.80%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.494406/  1.538834, val:  63.33%, val_best:  63.33%, tr:  60.37%, tr_best:  61.80%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.387645/  1.805302, val:  57.50%, val_best:  63.33%, tr:  66.29%, tr_best:  66.29%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.386201/  1.576143, val:  62.08%, val_best:  63.33%, tr:  64.76%, tr_best:  66.29%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.467983/  1.885385, val:  62.92%, val_best:  63.33%, tr:  65.07%, tr_best:  66.29%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.424846/  1.671918, val:  61.25%, val_best:  63.33%, tr:  68.95%, tr_best:  68.95%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.527591/  2.032501, val:  57.92%, val_best:  63.33%, tr:  69.66%, tr_best:  69.66%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.372352/  1.859257, val:  59.17%, val_best:  63.33%, tr:  69.36%, tr_best:  69.66%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.413089/  2.011880, val:  62.92%, val_best:  63.33%, tr:  69.77%, tr_best:  69.77%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.406455/  1.740317, val:  63.33%, val_best:  63.33%, tr:  70.99%, tr_best:  70.99%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.296229/  2.470377, val:  59.17%, val_best:  63.33%, tr:  72.11%, tr_best:  72.11%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.508568/  2.056084, val:  64.58%, val_best:  64.58%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  1.258255/  2.419678, val:  60.00%, val_best:  64.58%, tr:  73.34%, tr_best:  74.26%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  1.348752/  2.280183, val:  63.75%, val_best:  64.58%, tr:  74.87%, tr_best:  74.87%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  1.294090/  2.242330, val:  62.92%, val_best:  64.58%, tr:  77.53%, tr_best:  77.53%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  1.246767/  2.674494, val:  64.58%, val_best:  64.58%, tr:  78.24%, tr_best:  78.24%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  1.194442/  2.574567, val:  62.08%, val_best:  64.58%, tr:  81.41%, tr_best:  81.41%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  1.483205/  2.780212, val:  62.92%, val_best:  64.58%, tr:  79.37%, tr_best:  81.41%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  1.293315/  3.222157, val:  62.50%, val_best:  64.58%, tr:  81.61%, tr_best:  81.61%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  1.168842/  2.656445, val:  66.25%, val_best:  66.25%, tr:  84.68%, tr_best:  84.68%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  1.156965/  2.616183, val:  68.75%, val_best:  68.75%, tr:  86.93%, tr_best:  86.93%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  1.104745/  2.680727, val:  66.67%, val_best:  68.75%, tr:  88.97%, tr_best:  88.97%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  1.143443/  2.686563, val:  76.25%, val_best:  76.25%, tr:  86.41%, tr_best:  88.97%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  1.190784/  2.776185, val:  69.58%, val_best:  76.25%, tr:  86.72%, tr_best:  88.97%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  1.092661/  3.062845, val:  68.75%, val_best:  76.25%, tr:  90.60%, tr_best:  90.60%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  1.109580/  2.851045, val:  75.00%, val_best:  76.25%, tr:  91.52%, tr_best:  91.52%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  1.020862/  3.573902, val:  64.58%, val_best:  76.25%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  1.051472/  3.368395, val:  73.33%, val_best:  76.25%, tr:  93.67%, tr_best:  94.28%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.981321/  3.225939, val:  76.67%, val_best:  76.67%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  1.096238/  3.689135, val:  69.17%, val_best:  76.67%, tr:  92.24%, tr_best:  94.99%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  1.059094/  3.675357, val:  74.17%, val_best:  76.67%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  1.090912/  3.597582, val:  77.92%, val_best:  77.92%, tr:  94.08%, tr_best:  95.40%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  1.009458/  3.798447, val:  80.00%, val_best:  80.00%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.924800/  3.676867, val:  77.92%, val_best:  80.00%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.977245/  4.020737, val:  71.67%, val_best:  80.00%, tr:  96.53%, tr_best:  97.45%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.990297/  4.101701, val:  80.42%, val_best:  80.42%, tr:  97.14%, tr_best:  97.45%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  1.025122/  4.647239, val:  78.33%, val_best:  80.42%, tr:  95.91%, tr_best:  97.45%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  1.036195/  4.222980, val:  77.92%, val_best:  80.42%, tr:  96.42%, tr_best:  97.45%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.928891/  4.637033, val:  75.42%, val_best:  80.42%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.903497/  4.543616, val:  76.67%, val_best:  80.42%, tr:  98.37%, tr_best:  98.47%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.883833/  4.914578, val:  75.83%, val_best:  80.42%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.965847/  4.902924, val:  76.67%, val_best:  80.42%, tr:  98.37%, tr_best:  98.67%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.880992/  5.047016, val:  73.33%, val_best:  80.42%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.935733/  5.061924, val:  80.42%, val_best:  80.42%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.877204/  5.231517, val:  78.75%, val_best:  80.42%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.863070/  5.274710, val:  76.67%, val_best:  80.42%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.821527/  5.291379, val:  79.17%, val_best:  80.42%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.759630/  5.858034, val:  74.58%, val_best:  80.42%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.859186/  5.646239, val:  80.42%, val_best:  80.42%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.848070/  5.363313, val:  82.08%, val_best:  82.08%, tr:  98.98%, tr_best:  99.80%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.744821/  5.793860, val:  77.92%, val_best:  82.08%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.738686/  6.154476, val:  78.33%, val_best:  82.08%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.718742/  6.082612, val:  80.00%, val_best:  82.08%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.725792/  6.698820, val:  75.42%, val_best:  82.08%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.760468/  6.431979, val:  80.83%, val_best:  82.08%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.757902/  6.499949, val:  79.17%, val_best:  82.08%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.733986/  6.667911, val:  77.92%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.722658/  6.663065, val:  82.08%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.726392/  6.890910, val:  79.17%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.662725/  6.897679, val:  78.33%, val_best:  82.08%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.647343/  7.202251, val:  78.75%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.668964/  7.352109, val:  78.75%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.625886/  7.714131, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.590812/  7.562564, val:  77.08%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.641083/  7.974915, val:  76.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.684285/  7.671410, val:  77.50%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.589110/  7.857258, val:  79.17%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.626082/  8.108481, val:  79.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.612302/  8.085065, val:  79.58%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.584260/  8.624919, val:  75.42%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.631883/  8.567555, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.621118/  8.844125, val:  77.92%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.582171/  8.788284, val:  78.33%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.535618/  9.015253, val:  77.92%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.561966/  8.862292, val:  77.50%, val_best:  82.08%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.548566/  9.278880, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.526600/  8.963496, val:  77.92%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.515036/  9.235877, val:  79.58%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.551341/  9.254315, val:  79.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.510874/  9.401281, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.591299/  9.457226, val:  76.67%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.581596/  9.361839, val:  76.67%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.489244/  9.782494, val:  79.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.526503/  9.833311, val:  79.58%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.503846/  9.990246, val:  80.00%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.511437/ 10.187467, val:  77.50%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.516782/ 10.131238, val:  80.42%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.480763/ 10.098191, val:  79.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.478369/ 10.016171, val:  80.83%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.461200/ 10.594305, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.423309/ 10.459151, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.407575/ 10.693992, val:  77.92%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.431602/ 10.886838, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.457802/ 10.985521, val:  77.50%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.368301/ 10.924375, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.376873/ 11.177995, val:  77.92%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.483989/ 11.166943, val:  79.58%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7460dc00c1430e9f1e09e13b74322f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄▃▂▃▇▅▇▆▇██▅▇█▇███████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▅▅▄▅▅▅▅▅▅▆▅▆█▆▇▇▇▇██▇█▇██▇█▇▇▇▇█▇██▇▇█</td></tr><tr><td>tr_acc</td><td>▁▄▅▅▅▆▆▆▆▆▇▇█▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▅▆▅▅▅▄▅▄▄▄▄▄▃▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▅▅▅▅▅▅▅▅▆▇▇▇██████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▅▅▄▅▅▅▅▅▅▆▅▆█▆▇▇▇▇██▇█▇██▇█▇▇▇▇█▇██▇▇█</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>0.48399</td></tr><tr><td>val_acc_best</td><td>0.82083</td></tr><tr><td>val_acc_now</td><td>0.79583</td></tr><tr><td>val_loss</td><td>11.16694</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worldly-sweep-62</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z47h8vte' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z47h8vte</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_194239-z47h8vte/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 67nc4ml7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_194856-67nc4ml7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/67nc4ml7' target=\"_blank\">comfy-sweep-64</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/67nc4ml7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/67nc4ml7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.325213/  2.309355, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.323420/  2.309336, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:   9.50%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.290745/  2.183865, val:  15.83%, val_best:  15.83%, tr:  11.64%, tr_best:  11.64%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.038579/  1.930964, val:  28.33%, val_best:  28.33%, tr:  21.45%, tr_best:  21.45%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.751258/  1.757687, val:  45.42%, val_best:  45.42%, tr:  41.98%, tr_best:  41.98%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.467292/  1.541066, val:  56.67%, val_best:  56.67%, tr:  55.26%, tr_best:  55.26%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.292819/  1.463983, val:  58.33%, val_best:  58.33%, tr:  63.02%, tr_best:  63.02%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.191316/  1.310786, val:  66.25%, val_best:  66.25%, tr:  60.67%, tr_best:  63.02%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.118217/  1.381130, val:  63.33%, val_best:  66.25%, tr:  67.01%, tr_best:  67.01%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.090238/  1.357372, val:  60.00%, val_best:  66.25%, tr:  69.97%, tr_best:  69.97%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.077696/  1.339415, val:  63.33%, val_best:  66.25%, tr:  66.91%, tr_best:  69.97%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.042638/  1.315298, val:  67.50%, val_best:  67.50%, tr:  69.56%, tr_best:  69.97%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.021591/  1.267219, val:  66.25%, val_best:  67.50%, tr:  72.93%, tr_best:  72.93%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.991357/  1.309041, val:  66.25%, val_best:  67.50%, tr:  72.22%, tr_best:  72.93%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.974443/  1.383952, val:  64.58%, val_best:  67.50%, tr:  75.28%, tr_best:  75.28%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.932868/  1.236217, val:  73.33%, val_best:  73.33%, tr:  75.08%, tr_best:  75.28%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.879629/  1.241795, val:  74.17%, val_best:  74.17%, tr:  81.31%, tr_best:  81.31%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.822820/  1.234459, val:  75.00%, val_best:  75.00%, tr:  86.21%, tr_best:  86.21%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.787923/  1.356278, val:  70.83%, val_best:  75.00%, tr:  84.88%, tr_best:  86.21%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.764003/  1.274469, val:  72.50%, val_best:  75.00%, tr:  85.60%, tr_best:  86.21%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.750060/  1.359515, val:  69.58%, val_best:  75.00%, tr:  85.80%, tr_best:  86.21%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.690590/  1.335848, val:  70.42%, val_best:  75.00%, tr:  89.68%, tr_best:  89.68%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.674804/  1.353800, val:  70.00%, val_best:  75.00%, tr:  92.03%, tr_best:  92.03%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.678268/  1.254296, val:  80.00%, val_best:  80.00%, tr:  90.40%, tr_best:  92.03%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.593653/  1.282786, val:  79.58%, val_best:  80.00%, tr:  94.79%, tr_best:  94.79%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.589402/  1.273674, val:  78.33%, val_best:  80.00%, tr:  94.18%, tr_best:  94.79%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.596184/  1.258723, val:  84.58%, val_best:  84.58%, tr:  93.56%, tr_best:  94.79%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.543833/  1.341927, val:  75.42%, val_best:  84.58%, tr:  94.08%, tr_best:  94.79%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.536264/  1.247039, val:  84.17%, val_best:  84.58%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.499192/  1.413270, val:  75.83%, val_best:  84.58%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.492557/  1.329232, val:  80.42%, val_best:  84.58%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.457069/  1.324560, val:  83.75%, val_best:  84.58%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.482454/  1.307022, val:  82.92%, val_best:  84.58%, tr:  95.51%, tr_best:  97.04%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.466972/  1.324482, val:  84.17%, val_best:  84.58%, tr:  96.53%, tr_best:  97.04%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.436571/  1.375254, val:  78.33%, val_best:  84.58%, tr:  96.73%, tr_best:  97.04%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.428035/  1.355064, val:  83.75%, val_best:  84.58%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.401326/  1.320549, val:  84.58%, val_best:  84.58%, tr:  97.45%, tr_best:  97.85%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.384241/  1.397443, val:  82.92%, val_best:  84.58%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.387975/  1.365901, val:  86.67%, val_best:  86.67%, tr:  98.16%, tr_best:  98.47%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.357199/  1.419261, val:  83.75%, val_best:  86.67%, tr:  98.26%, tr_best:  98.47%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.368700/  1.346059, val:  87.08%, val_best:  87.08%, tr:  97.75%, tr_best:  98.47%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.350706/  1.446784, val:  80.00%, val_best:  87.08%, tr:  97.96%, tr_best:  98.47%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.314539/  1.379327, val:  87.08%, val_best:  87.08%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.291533/  1.444554, val:  83.75%, val_best:  87.08%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.291223/  1.470895, val:  83.75%, val_best:  87.08%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.275483/  1.461703, val:  82.50%, val_best:  87.08%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.280608/  1.515837, val:  82.50%, val_best:  87.08%, tr:  99.08%, tr_best:  99.59%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.273781/  1.445600, val:  87.08%, val_best:  87.08%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.269875/  1.494322, val:  84.58%, val_best:  87.08%, tr:  99.39%, tr_best:  99.59%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.265195/  1.472672, val:  87.08%, val_best:  87.08%, tr:  98.88%, tr_best:  99.59%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.239865/  1.499624, val:  85.42%, val_best:  87.08%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.224577/  1.520685, val:  85.83%, val_best:  87.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.245564/  1.507180, val:  83.75%, val_best:  87.08%, tr:  99.28%, tr_best:  99.69%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.220963/  1.542994, val:  85.42%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.211197/  1.508017, val:  87.92%, val_best:  87.92%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.189874/  1.593661, val:  85.00%, val_best:  87.92%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.198342/  1.678787, val:  79.58%, val_best:  87.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.203404/  1.620555, val:  84.58%, val_best:  87.92%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.183294/  1.611250, val:  85.42%, val_best:  87.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.185624/  1.661091, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.171330/  1.616718, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.174923/  1.665880, val:  84.17%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.175139/  1.684671, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.161814/  1.642358, val:  85.42%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.147080/  1.690942, val:  87.08%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.139360/  1.737248, val:  87.50%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.134375/  1.713742, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.132850/  1.709264, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.129419/  1.806738, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.125623/  1.824162, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.120226/  1.797938, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.134045/  1.786428, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.116700/  1.870142, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.119061/  1.885970, val:  83.75%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.113637/  1.849379, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.109814/  1.845462, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.108254/  1.931945, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.108523/  1.925836, val:  86.25%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.115007/  1.962681, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.110432/  1.912355, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.106023/  1.956882, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.096873/  1.949191, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.095869/  1.976108, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.101724/  1.923836, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.096466/  1.930867, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.090655/  1.976351, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.096932/  1.972757, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.096662/  2.000497, val:  84.58%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.089029/  2.009511, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.101248/  2.036299, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.090401/  2.011409, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.068947/  2.049213, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.069378/  2.062595, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.069638/  2.050807, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.069854/  2.083913, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.066589/  2.118361, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.066422/  2.131360, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.060648/  2.132639, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.075012/  2.144089, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.064389/  2.109080, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480d04c8da1c4c8cbe8404171061da7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▂▅▆▆▇▃▇████▇██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▄▆▅▆▆▇▇▆▇█▇███████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▁▄▅▆▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▆▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▄▆▆▆▆▇▇▇▇█████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▄▆▅▆▆▇▇▆▇█▇███████████████████████████</td></tr><tr><td>val_loss</td><td>█▇▄▁▂▁▂▁▁▂▁▁▂▁▂▂▂▂▃▂▃▃▃▄▄▄▄▄▅▅▅▆▅▆▆▆▆▆▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.06439</td></tr><tr><td>val_acc_best</td><td>0.87917</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>2.10908</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comfy-sweep-64</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/67nc4ml7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/67nc4ml7</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_194856-67nc4ml7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: r4xpc651 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_195437-r4xpc651</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/r4xpc651' target=\"_blank\">lyric-sweep-66</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/r4xpc651' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/r4xpc651</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.325213/  2.309355, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.323420/  2.309336, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:   9.50%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.319913/  2.316170, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:   9.50%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.322621/  2.318868, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.50%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.323630/  2.316624, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:   9.50%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.314643/  2.312522, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:   9.50%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  2.322383/  2.317189, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  2.323500/  2.311199, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.91%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  2.318820/  2.316814, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:   9.91%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.324861/  2.308681, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  10.21%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  2.208472/  2.187671, val:  17.92%, val_best:  17.92%, tr:  14.20%, tr_best:  14.20%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  2.121798/  2.109978, val:  20.42%, val_best:  20.42%, tr:  17.36%, tr_best:  17.36%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.976671/  1.864304, val:  36.25%, val_best:  36.25%, tr:  27.58%, tr_best:  27.58%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.795804/  1.836941, val:  36.67%, val_best:  36.67%, tr:  34.53%, tr_best:  34.53%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.551320/  1.736957, val:  49.17%, val_best:  49.17%, tr:  53.63%, tr_best:  53.63%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  1.413492/  1.688288, val:  46.25%, val_best:  49.17%, tr:  57.10%, tr_best:  57.10%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  1.312584/  1.562366, val:  57.08%, val_best:  57.08%, tr:  60.78%, tr_best:  60.78%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  1.216424/  1.615082, val:  49.58%, val_best:  57.08%, tr:  66.50%, tr_best:  66.50%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  1.222788/  1.576673, val:  52.50%, val_best:  57.08%, tr:  64.96%, tr_best:  66.50%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  1.228389/  1.536946, val:  60.42%, val_best:  60.42%, tr:  69.05%, tr_best:  69.05%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  1.138154/  1.431860, val:  60.83%, val_best:  60.83%, tr:  68.54%, tr_best:  69.05%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  1.099049/  1.399023, val:  63.75%, val_best:  63.75%, tr:  71.20%, tr_best:  71.20%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  1.080852/  1.408930, val:  59.58%, val_best:  63.75%, tr:  72.52%, tr_best:  72.52%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  1.025122/  1.409244, val:  61.67%, val_best:  63.75%, tr:  68.23%, tr_best:  72.52%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.982882/  1.446404, val:  61.67%, val_best:  63.75%, tr:  71.60%, tr_best:  72.52%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.976046/  1.384677, val:  69.58%, val_best:  69.58%, tr:  73.75%, tr_best:  73.75%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  1.040079/  1.430654, val:  66.67%, val_best:  69.58%, tr:  73.14%, tr_best:  73.75%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.963729/  1.452362, val:  63.33%, val_best:  69.58%, tr:  74.46%, tr_best:  74.46%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.946867/  1.402352, val:  67.08%, val_best:  69.58%, tr:  79.47%, tr_best:  79.47%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.895935/  1.514347, val:  63.33%, val_best:  69.58%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.862157/  1.451382, val:  65.42%, val_best:  69.58%, tr:  85.80%, tr_best:  85.80%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.903925/  1.499808, val:  62.08%, val_best:  69.58%, tr:  82.02%, tr_best:  85.80%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.846678/  1.467557, val:  69.17%, val_best:  69.58%, tr:  83.45%, tr_best:  85.80%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.814791/  1.461383, val:  66.67%, val_best:  69.58%, tr:  85.90%, tr_best:  85.90%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.798607/  1.527230, val:  66.67%, val_best:  69.58%, tr:  87.33%, tr_best:  87.33%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.812389/  1.479596, val:  68.75%, val_best:  69.58%, tr:  85.09%, tr_best:  87.33%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.767664/  1.385075, val:  72.50%, val_best:  72.50%, tr:  86.11%, tr_best:  87.33%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.724971/  1.483236, val:  69.17%, val_best:  72.50%, tr:  89.17%, tr_best:  89.17%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.733382/  1.437531, val:  74.17%, val_best:  74.17%, tr:  91.01%, tr_best:  91.01%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.688538/  1.422062, val:  74.58%, val_best:  74.58%, tr:  92.85%, tr_best:  92.85%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.698985/  1.456280, val:  73.75%, val_best:  74.58%, tr:  92.03%, tr_best:  92.85%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.673666/  1.525457, val:  70.83%, val_best:  74.58%, tr:  92.75%, tr_best:  92.85%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.649917/  1.459935, val:  73.75%, val_best:  74.58%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.619328/  1.521951, val:  72.92%, val_best:  74.58%, tr:  94.38%, tr_best:  94.99%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.604384/  1.517933, val:  72.50%, val_best:  74.58%, tr:  94.38%, tr_best:  94.99%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.605292/  1.509020, val:  70.42%, val_best:  74.58%, tr:  94.89%, tr_best:  94.99%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.623768/  1.462226, val:  79.58%, val_best:  79.58%, tr:  94.18%, tr_best:  94.99%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.569137/  1.453309, val:  76.67%, val_best:  79.58%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.572975/  1.481453, val:  76.25%, val_best:  79.58%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.546451/  1.498773, val:  79.58%, val_best:  79.58%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.522192/  1.543339, val:  75.42%, val_best:  79.58%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.542358/  1.518666, val:  79.17%, val_best:  79.58%, tr:  96.12%, tr_best:  97.14%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.518871/  1.556329, val:  78.33%, val_best:  79.58%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.513420/  1.549839, val:  79.17%, val_best:  79.58%, tr:  96.94%, tr_best:  97.34%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.501034/  1.522000, val:  78.33%, val_best:  79.58%, tr:  96.94%, tr_best:  97.34%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.487480/  1.565356, val:  78.33%, val_best:  79.58%, tr:  97.14%, tr_best:  97.34%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.506582/  1.623047, val:  77.08%, val_best:  79.58%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.492745/  1.543832, val:  80.83%, val_best:  80.83%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.478176/  1.631673, val:  78.75%, val_best:  80.83%, tr:  96.83%, tr_best:  97.55%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.460101/  1.646660, val:  77.92%, val_best:  80.83%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.466423/  1.603397, val:  75.83%, val_best:  80.83%, tr:  97.75%, tr_best:  98.37%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.452139/  1.579336, val:  77.50%, val_best:  80.83%, tr:  97.75%, tr_best:  98.37%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.450018/  1.634270, val:  77.08%, val_best:  80.83%, tr:  98.16%, tr_best:  98.37%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.421534/  1.587850, val:  79.17%, val_best:  80.83%, tr:  98.26%, tr_best:  98.37%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.402356/  1.682285, val:  78.33%, val_best:  80.83%, tr:  98.26%, tr_best:  98.37%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.402225/  1.666176, val:  80.42%, val_best:  80.83%, tr:  98.26%, tr_best:  98.37%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.401271/  1.665229, val:  74.17%, val_best:  80.83%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.401389/  1.726934, val:  78.75%, val_best:  80.83%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.393037/  1.733622, val:  76.25%, val_best:  80.83%, tr:  98.67%, tr_best:  98.98%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.408390/  1.705397, val:  77.08%, val_best:  80.83%, tr:  98.77%, tr_best:  98.98%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.373292/  1.699537, val:  78.75%, val_best:  80.83%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.389172/  1.822994, val:  75.42%, val_best:  80.83%, tr:  98.77%, tr_best:  98.98%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.369446/  1.789454, val:  76.67%, val_best:  80.83%, tr:  98.47%, tr_best:  98.98%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.389441/  1.798915, val:  78.75%, val_best:  80.83%, tr:  98.26%, tr_best:  98.98%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.366066/  1.728293, val:  81.25%, val_best:  81.25%, tr:  98.67%, tr_best:  98.98%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.348117/  1.798855, val:  80.83%, val_best:  81.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.332522/  1.802980, val:  75.42%, val_best:  81.25%, tr:  98.77%, tr_best:  99.28%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.347899/  1.873869, val:  81.67%, val_best:  81.67%, tr:  98.37%, tr_best:  99.28%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.340972/  1.861162, val:  79.58%, val_best:  81.67%, tr:  98.88%, tr_best:  99.28%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.381723/  1.861543, val:  80.42%, val_best:  81.67%, tr:  98.37%, tr_best:  99.28%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.328929/  1.847665, val:  80.42%, val_best:  81.67%, tr:  99.08%, tr_best:  99.28%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.313812/  1.886071, val:  78.33%, val_best:  81.67%, tr:  98.98%, tr_best:  99.28%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.316890/  1.912201, val:  79.17%, val_best:  81.67%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.310279/  1.914948, val:  80.00%, val_best:  81.67%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.317770/  1.875150, val:  79.58%, val_best:  81.67%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.297452/  1.940356, val:  79.17%, val_best:  81.67%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.282983/  1.932724, val:  82.08%, val_best:  82.08%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.298123/  1.975711, val:  78.33%, val_best:  82.08%, tr:  98.77%, tr_best:  99.39%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.284183/  2.002624, val:  78.75%, val_best:  82.08%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.278567/  1.953500, val:  79.58%, val_best:  82.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.273031/  1.960879, val:  81.25%, val_best:  82.08%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.259736/  1.993224, val:  79.17%, val_best:  82.08%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.278381/  2.003268, val:  81.25%, val_best:  82.08%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.252438/  1.996443, val:  81.67%, val_best:  82.08%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.254129/  2.081656, val:  79.17%, val_best:  82.08%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.263733/  2.133116, val:  79.58%, val_best:  82.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.257265/  2.054211, val:  78.33%, val_best:  82.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.253585/  2.075287, val:  81.25%, val_best:  82.08%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.274856/  2.112363, val:  80.83%, val_best:  82.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.249813/  2.116178, val:  80.00%, val_best:  82.08%, tr:  99.69%, tr_best:  99.90%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c748d7278c94c9f95c3688a41dea0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▁▁▁▃▅▃▅▇▆█▇▇▇▇█████████████▇██████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▁▄▅▅▆▆▆▇▆▇▇▇▇▇▇█████████████████████</td></tr><tr><td>tr_acc</td><td>▁▁▁▁▁▂▄▅▆▆▆▆▇▇▇▇▇███████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█████▇▅▄▄▄▃▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▄▅▆▆▆▆▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▁▄▅▅▆▆▆▇▆▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_loss</td><td>█████▅▄▃▂▁▁▁▂▂▂▂▁▁▂▁▂▂▂▂▃▂▃▄▃▄▄▅▅▅▅▅▅▆▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99694</td></tr><tr><td>tr_epoch_loss</td><td>0.24981</td></tr><tr><td>val_acc_best</td><td>0.82083</td></tr><tr><td>val_acc_now</td><td>0.8</td></tr><tr><td>val_loss</td><td>2.11618</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lyric-sweep-66</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/r4xpc651' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/r4xpc651</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_195437-r4xpc651/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ui1s8uu6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_200054-ui1s8uu6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ui1s8uu6' target=\"_blank\">avid-sweep-68</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ui1s8uu6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ui1s8uu6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = b57dfc72d05d304ce829f424a70e455b\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.696908/  3.489078, val:  46.67%, val_best:  46.67%, tr:  28.70%, tr_best:  28.70%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  3.162661/  3.249409, val:  47.92%, val_best:  47.92%, tr:  43.72%, tr_best:  43.72%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  3.272729/  3.627899, val:  49.17%, val_best:  49.17%, tr:  49.95%, tr_best:  49.95%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.324117/  2.757876, val:  49.58%, val_best:  49.58%, tr:  56.18%, tr_best:  56.18%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.436976/  2.992665, val:  57.92%, val_best:  57.92%, tr:  55.77%, tr_best:  56.18%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.986612/  2.175802, val:  55.83%, val_best:  57.92%, tr:  60.06%, tr_best:  60.06%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.545569/  2.481420, val:  53.75%, val_best:  57.92%, tr:  62.00%, tr_best:  62.00%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.664356/  2.246253, val:  55.42%, val_best:  57.92%, tr:  61.70%, tr_best:  62.00%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.698868/  2.171735, val:  58.33%, val_best:  58.33%, tr:  65.37%, tr_best:  65.37%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.130222/  2.019711, val:  63.33%, val_best:  63.33%, tr:  64.96%, tr_best:  65.37%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.817940/  2.058818, val:  59.17%, val_best:  63.33%, tr:  66.19%, tr_best:  66.19%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.816280/  2.884379, val:  50.42%, val_best:  63.33%, tr:  67.93%, tr_best:  67.93%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.689556/  1.590720, val:  70.00%, val_best:  70.00%, tr:  67.11%, tr_best:  67.93%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.529055/  2.809883, val:  49.17%, val_best:  70.00%, tr:  70.99%, tr_best:  70.99%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.201899/  2.952006, val:  61.25%, val_best:  70.00%, tr:  74.87%, tr_best:  74.87%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  1.513258/  2.895706, val:  62.50%, val_best:  70.00%, tr:  75.18%, tr_best:  75.18%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  1.462562/  2.200788, val:  63.75%, val_best:  70.00%, tr:  75.79%, tr_best:  75.79%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.951316/  2.349893, val:  65.00%, val_best:  70.00%, tr:  83.86%, tr_best:  83.86%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.884467/  2.157926, val:  70.00%, val_best:  70.00%, tr:  84.88%, tr_best:  84.88%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.761186/  2.229115, val:  62.50%, val_best:  70.00%, tr:  87.03%, tr_best:  87.03%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.979254/  2.939029, val:  62.08%, val_best:  70.00%, tr:  83.04%, tr_best:  87.03%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  1.236795/  3.927707, val:  66.25%, val_best:  70.00%, tr:  84.17%, tr_best:  87.03%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.872328/  2.263818, val:  65.83%, val_best:  70.00%, tr:  86.93%, tr_best:  87.03%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.589681/  2.638794, val:  67.50%, val_best:  70.00%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.500303/  2.155075, val:  76.25%, val_best:  76.25%, tr:  95.30%, tr_best:  95.30%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.382129/  2.221344, val:  67.50%, val_best:  76.25%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.387514/  2.496841, val:  66.67%, val_best:  76.25%, tr:  95.91%, tr_best:  97.45%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.349943/  2.151318, val:  75.00%, val_best:  76.25%, tr:  96.94%, tr_best:  97.45%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.320768/  2.159264, val:  73.75%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.264792/  2.330523, val:  75.42%, val_best:  76.25%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.211730/  2.297512, val:  74.58%, val_best:  76.25%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.217749/  2.451398, val:  79.17%, val_best:  79.17%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.348004/  2.621082, val:  70.00%, val_best:  79.17%, tr:  96.53%, tr_best:  99.39%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.212597/  2.605506, val:  77.08%, val_best:  79.17%, tr:  98.77%, tr_best:  99.39%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.184171/  2.478098, val:  75.83%, val_best:  79.17%, tr:  99.28%, tr_best:  99.39%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.142437/  2.490623, val:  73.75%, val_best:  79.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.162167/  2.574179, val:  75.00%, val_best:  79.17%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.128711/  2.700969, val:  73.75%, val_best:  79.17%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.128977/  2.566335, val:  75.00%, val_best:  79.17%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.104333/  2.731931, val:  77.92%, val_best:  79.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.075622/  2.619053, val:  76.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.074029/  2.611980, val:  77.50%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.056147/  2.680071, val:  74.58%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.046345/  2.731362, val:  75.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.040698/  2.766835, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.042587/  2.807270, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.035467/  2.782182, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.033395/  2.842690, val:  75.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.035817/  2.833262, val:  75.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.033743/  2.896800, val:  76.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.026274/  2.898185, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.019679/  2.869479, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.024463/  2.864363, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.016999/  2.880959, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.014972/  2.960037, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.016173/  2.926532, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.013366/  2.936839, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.012661/  3.033126, val:  76.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.011646/  2.980570, val:  75.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.011465/  3.041076, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.010566/  3.170609, val:  73.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.008891/  2.998665, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.009885/  3.069632, val:  76.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.008596/  3.041993, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.008438/  3.082133, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.006931/  3.093774, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.005288/  3.101288, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.008366/  3.154889, val:  76.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.005823/  3.170186, val:  75.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.006996/  3.166515, val:  75.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.009044/  3.150038, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.008728/  3.144058, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.007238/  3.171418, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.007492/  3.164795, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.006478/  3.203983, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.004542/  3.177728, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.003838/  3.203172, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.003772/  3.219125, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.003980/  3.209367, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.003449/  3.231361, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.003962/  3.196990, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.003449/  3.190355, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.002764/  3.219592, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.003174/  3.259850, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.003826/  3.263699, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.002912/  3.278527, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.003258/  3.310719, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.002968/  3.300637, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.004614/  3.319118, val:  76.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.003292/  3.287816, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.002980/  3.310694, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.002670/  3.271350, val:  76.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.002345/  3.343880, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.002199/  3.330608, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.002432/  3.344183, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.001895/  3.381016, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.001628/  3.372762, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.001638/  3.390997, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.001848/  3.356387, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.001778/  3.366464, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19df1b8d2dc2407e9d48d221796d3c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▂▅▃▅▅▆▆▇████▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▃▃▅▆▄▅▄▅▇▅▇▆▇▇█▇▇▇▇██▇▇█▇▇████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▄▅▅▆▆▇▆██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▇█▆▅▆▅▄▃▃▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▃▃▅▆▆▆▆▆▇▇▇███████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▃▃▅▆▄▅▄▅▇▅▇▆▇▇█▇▇▇▇██▇▇█▇▇████████████</td></tr><tr><td>val_loss</td><td>▇▇▅▃▂▁▅▃▃█▃▄▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00178</td></tr><tr><td>val_acc_best</td><td>0.79167</td></tr><tr><td>val_acc_now</td><td>0.77083</td></tr><tr><td>val_loss</td><td>3.36646</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">avid-sweep-68</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ui1s8uu6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ui1s8uu6</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_200054-ui1s8uu6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 22hvz9tf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_200711-22hvz9tf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/22hvz9tf' target=\"_blank\">sleek-sweep-70</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/22hvz9tf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/22hvz9tf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 36.201756/ 32.991375, val:  34.58%, val_best:  34.58%, tr:  24.31%, tr_best:  24.31%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 24.605885/ 22.079184, val:  29.58%, val_best:  34.58%, tr:  37.49%, tr_best:  37.49%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 14.947743/ 22.773880, val:  30.83%, val_best:  34.58%, tr:  46.78%, tr_best:  46.78%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 17.609684/ 16.427654, val:  41.67%, val_best:  41.67%, tr:  44.94%, tr_best:  46.78%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 24.245745/ 17.146509, val:  47.92%, val_best:  47.92%, tr:  44.13%, tr_best:  46.78%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 15.635000/ 25.687798, val:  43.33%, val_best:  47.92%, tr:  50.36%, tr_best:  50.36%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 13.821600/ 12.893143, val:  57.08%, val_best:  57.08%, tr:  54.34%, tr_best:  54.34%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 13.075734/ 31.053129, val:  37.92%, val_best:  57.08%, tr:  54.03%, tr_best:  54.34%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 13.155457/ 11.922334, val:  53.33%, val_best:  57.08%, tr:  58.12%, tr_best:  58.12%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 10.403003/ 15.483898, val:  47.92%, val_best:  57.08%, tr:  61.80%, tr_best:  61.80%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 14.744044/ 21.813387, val:  39.17%, val_best:  57.08%, tr:  55.16%, tr_best:  61.80%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 13.901218/ 17.117165, val:  59.17%, val_best:  59.17%, tr:  60.27%, tr_best:  61.80%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 13.078997/ 16.245836, val:  47.08%, val_best:  59.17%, tr:  58.63%, tr_best:  61.80%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 15.097749/ 24.995878, val:  41.67%, val_best:  59.17%, tr:  56.79%, tr_best:  61.80%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 13.947495/ 11.967245, val:  60.42%, val_best:  60.42%, tr:  60.06%, tr_best:  61.80%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  8.254730/ 14.373540, val:  56.67%, val_best:  60.42%, tr:  67.62%, tr_best:  67.62%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 15.014894/ 17.404909, val:  46.25%, val_best:  60.42%, tr:  61.90%, tr_best:  67.62%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  9.623723/ 16.148029, val:  50.42%, val_best:  60.42%, tr:  66.39%, tr_best:  67.62%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss: 11.526466/ 17.210611, val:  61.67%, val_best:  61.67%, tr:  62.72%, tr_best:  67.62%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  9.547869/ 12.584109, val:  62.92%, val_best:  62.92%, tr:  70.68%, tr_best:  70.68%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 11.056820/ 21.451527, val:  40.83%, val_best:  62.92%, tr:  68.54%, tr_best:  70.68%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  9.495090/ 13.919051, val:  58.75%, val_best:  62.92%, tr:  72.22%, tr_best:  72.22%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss: 10.137225/ 14.001911, val:  67.08%, val_best:  67.08%, tr:  72.93%, tr_best:  72.93%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  7.380722/ 11.854303, val:  70.42%, val_best:  70.42%, tr:  82.02%, tr_best:  82.02%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  6.571805/ 11.502527, val:  62.50%, val_best:  70.42%, tr:  79.67%, tr_best:  82.02%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  5.313375/ 10.834995, val:  67.08%, val_best:  70.42%, tr:  83.35%, tr_best:  83.35%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  4.797289/ 10.648401, val:  70.00%, val_best:  70.42%, tr:  85.50%, tr_best:  85.50%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  4.098008/ 11.725752, val:  71.25%, val_best:  71.25%, tr:  86.21%, tr_best:  86.21%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  5.112936/  9.224881, val:  74.17%, val_best:  74.17%, tr:  85.09%, tr_best:  86.21%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  3.641864/ 14.221341, val:  58.33%, val_best:  74.17%, tr:  87.03%, tr_best:  87.03%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  4.712085/  9.810141, val:  70.00%, val_best:  74.17%, tr:  84.37%, tr_best:  87.03%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  4.075647/ 12.197219, val:  69.17%, val_best:  74.17%, tr:  85.60%, tr_best:  87.03%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  4.294271/ 10.970866, val:  69.17%, val_best:  74.17%, tr:  87.95%, tr_best:  87.95%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  3.590572/ 11.684764, val:  66.67%, val_best:  74.17%, tr:  90.09%, tr_best:  90.09%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  3.767923/ 11.772840, val:  72.08%, val_best:  74.17%, tr:  89.27%, tr_best:  90.09%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  2.313769/  8.559300, val:  81.67%, val_best:  81.67%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  2.136992/  9.684196, val:  76.67%, val_best:  81.67%, tr:  94.08%, tr_best:  95.91%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  5.386136/ 16.900230, val:  62.08%, val_best:  81.67%, tr:  82.84%, tr_best:  95.91%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  3.636644/ 13.953088, val:  66.67%, val_best:  81.67%, tr:  91.22%, tr_best:  95.91%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  2.527964/ 11.488824, val:  76.67%, val_best:  81.67%, tr:  94.59%, tr_best:  95.91%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  2.135387/ 11.564150, val:  73.33%, val_best:  81.67%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  2.036456/  9.641970, val:  82.50%, val_best:  82.50%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  3.785154/ 13.223349, val:  67.08%, val_best:  82.50%, tr:  89.68%, tr_best:  96.22%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  2.199347/ 11.301224, val:  77.08%, val_best:  82.50%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  1.888907/ 14.508349, val:  67.92%, val_best:  82.50%, tr:  97.04%, tr_best:  97.24%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  2.450217/ 10.792015, val:  81.25%, val_best:  82.50%, tr:  94.89%, tr_best:  97.24%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  3.440300/ 14.162933, val:  74.17%, val_best:  82.50%, tr:  92.24%, tr_best:  97.24%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  2.195527/ 12.475361, val:  78.33%, val_best:  82.50%, tr:  96.63%, tr_best:  97.24%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  1.822429/ 10.521896, val:  85.42%, val_best:  85.42%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  1.422051/ 10.524093, val:  82.92%, val_best:  85.42%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  1.272902/ 10.644122, val:  81.67%, val_best:  85.42%, tr:  98.06%, tr_best:  98.98%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  1.087296/ 10.477756, val:  80.00%, val_best:  85.42%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  1.101818/  9.849925, val:  80.42%, val_best:  85.42%, tr:  98.37%, tr_best:  99.28%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  1.049111/ 10.036582, val:  80.42%, val_best:  85.42%, tr:  98.26%, tr_best:  99.28%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.805120/  9.197436, val:  85.42%, val_best:  85.42%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.611547/  8.958487, val:  86.25%, val_best:  86.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.928394/ 11.672932, val:  78.33%, val_best:  86.25%, tr:  98.67%, tr_best:  99.59%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.709239/  9.501390, val:  83.75%, val_best:  86.25%, tr:  99.18%, tr_best:  99.59%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.581318/  9.534962, val:  83.33%, val_best:  86.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.531287/ 10.251207, val:  81.25%, val_best:  86.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.471429/  9.605439, val:  80.00%, val_best:  86.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.574434/  9.391102, val:  84.58%, val_best:  86.25%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.700779/  9.441993, val:  84.17%, val_best:  86.25%, tr:  99.08%, tr_best:  99.69%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.722052/  9.611314, val:  82.92%, val_best:  86.25%, tr:  98.77%, tr_best:  99.69%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.351905/  9.909754, val:  80.83%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.353611/  9.263106, val:  85.83%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.319526/  9.543324, val:  85.00%, val_best:  86.25%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.589966/  9.446359, val:  86.25%, val_best:  86.25%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.330077/  9.760253, val:  81.67%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.305873/  9.451308, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.331217/  9.518979, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.299446/ 10.608148, val:  79.17%, val_best:  86.25%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.259201/  9.733159, val:  84.17%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.225277/ 10.408459, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.195035/ 10.120318, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.225851/ 10.265079, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.207438/  9.579424, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.221100/  9.844458, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.177435/  9.793654, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.161964/  9.356747, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.223727/  9.576505, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.194392/  8.970901, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.238433/  9.819021, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.470006/ 10.316632, val:  77.08%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.393081/  9.998871, val:  84.58%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.260158/  9.464795, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.285714/ 10.049719, val:  87.08%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.207185/  9.716611, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.195221/ 10.213067, val:  81.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.207750/  9.859935, val:  85.42%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.146523/  9.835185, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.138907/  9.581526, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.107727/ 10.029278, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.079489/  9.763079, val:  81.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.109748/  9.892659, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.084424/ 10.456849, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.095149/ 10.297928, val:  81.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.113196/  9.853174, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.086160/ 10.239012, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.078041/  9.992059, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1df5faf8cd94f74a040d98466597e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▅▃▂▃▆▃▆▇▆▇▇▆▇▆███▇████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▃▂▃▃▅▃▅▄▅▆▄▆▇▅▇▆▆▇▇▇██▇███▇█▇▇█▇███▇██</td></tr><tr><td>tr_acc</td><td>▁▃▃▄▄▄▄▅▅▅▆▇▇▇█▆▇▇██████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▄▆▄▃▄▄▃▃▃▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▃▄▄▄▄▄▅▅▆▆▆▆▇▇▇▇▇▇████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▃▂▃▃▅▃▅▄▅▆▄▆▇▅▇▆▆▇▇▇██▇███▇█▇▇█▇███▇██</td></tr><tr><td>val_loss</td><td>█▅▃▇▃▃▂▃▂▃▂▂▃▂▁▃▂▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.07804</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>9.99206</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sleek-sweep-70</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/22hvz9tf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/22hvz9tf</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_200711-22hvz9tf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zc9gi31u with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_201224-zc9gi31u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zc9gi31u' target=\"_blank\">splendid-sweep-72</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zc9gi31u' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zc9gi31u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.75, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=0.75, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.137876/  1.679679, val:  48.75%, val_best:  48.75%, tr:  19.51%, tr_best:  19.51%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.490737/  1.703933, val:  55.83%, val_best:  55.83%, tr:  53.01%, tr_best:  53.01%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.483629/  1.602163, val:  60.42%, val_best:  60.42%, tr:  58.94%, tr_best:  58.94%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.275651/  1.664932, val:  55.83%, val_best:  60.42%, tr:  63.23%, tr_best:  63.23%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.157612/  1.285879, val:  67.92%, val_best:  67.92%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.098943/  1.436649, val:  59.17%, val_best:  67.92%, tr:  69.25%, tr_best:  69.25%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.068356/  1.222740, val:  65.42%, val_best:  67.92%, tr:  69.97%, tr_best:  69.97%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.039798/  1.273355, val:  64.17%, val_best:  67.92%, tr:  72.11%, tr_best:  72.11%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.936476/  1.140344, val:  73.75%, val_best:  73.75%, tr:  75.28%, tr_best:  75.28%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.952154/  1.284183, val:  68.33%, val_best:  73.75%, tr:  79.47%, tr_best:  79.47%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.871270/  1.130532, val:  80.00%, val_best:  80.00%, tr:  80.69%, tr_best:  80.69%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.762210/  1.190443, val:  77.92%, val_best:  80.00%, tr:  85.29%, tr_best:  85.29%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.663645/  1.183921, val:  79.58%, val_best:  80.00%, tr:  89.27%, tr_best:  89.27%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.612115/  1.194576, val:  80.83%, val_best:  80.83%, tr:  90.30%, tr_best:  90.30%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.626578/  1.393636, val:  77.08%, val_best:  80.83%, tr:  89.58%, tr_best:  90.30%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.482558/  1.236323, val:  82.92%, val_best:  82.92%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.461505/  1.222696, val:  82.50%, val_best:  82.92%, tr:  95.30%, tr_best:  95.30%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.415760/  1.215227, val:  82.92%, val_best:  82.92%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.384068/  1.297188, val:  83.33%, val_best:  83.33%, tr:  96.73%, tr_best:  97.14%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.332713/  1.196882, val:  87.50%, val_best:  87.50%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.315031/  1.343832, val:  82.92%, val_best:  87.50%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.296282/  1.378863, val:  80.00%, val_best:  87.50%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.276218/  1.452831, val:  82.08%, val_best:  87.50%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.303415/  1.476292, val:  84.17%, val_best:  87.50%, tr:  97.34%, tr_best:  98.77%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.241320/  1.387616, val:  85.00%, val_best:  87.50%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.210366/  1.416021, val:  85.42%, val_best:  87.50%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.230239/  1.433942, val:  86.67%, val_best:  87.50%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.192301/  1.568517, val:  85.00%, val_best:  87.50%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.162880/  1.450702, val:  87.08%, val_best:  87.50%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.149419/  1.633752, val:  84.58%, val_best:  87.50%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.155310/  1.546967, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.144235/  1.638008, val:  84.58%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.137721/  1.646481, val:  86.67%, val_best:  87.50%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.126811/  1.764357, val:  85.00%, val_best:  87.50%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.119081/  1.772485, val:  84.17%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.103006/  1.718230, val:  85.42%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.090726/  1.722343, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.077508/  1.761647, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.079330/  1.766543, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.073647/  1.800955, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.071490/  1.904099, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.061222/  1.896556, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.058379/  1.868058, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.052546/  1.947876, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.045874/  1.929909, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.042964/  1.966739, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.042977/  1.966386, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.042333/  1.964524, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.039797/  2.020093, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.033858/  2.031644, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.029590/  2.081270, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.031170/  2.088250, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.030432/  2.130100, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.031104/  2.122824, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.022891/  2.135526, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.022686/  2.154078, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.021173/  2.214451, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.020043/  2.239140, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.019276/  2.196348, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.016025/  2.259721, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.018282/  2.192841, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.016389/  2.289150, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.015824/  2.308161, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.015105/  2.273701, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.015794/  2.302027, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.015974/  2.328719, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.016624/  2.373883, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.013698/  2.335244, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.012859/  2.385859, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.012416/  2.378423, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.011772/  2.380416, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.015588/  2.378643, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.014009/  2.429999, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.017272/  2.404970, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.017296/  2.383502, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.014516/  2.420030, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.011721/  2.431133, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.012796/  2.412534, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.011399/  2.474184, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.013309/  2.470043, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.013471/  2.472512, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.010176/  2.501974, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.008915/  2.549427, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.008846/  2.536075, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.008400/  2.486349, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.008244/  2.539192, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.008908/  2.558913, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.009010/  2.581115, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.008693/  2.583814, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.009201/  2.615011, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.009391/  2.541879, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.008653/  2.561773, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.008739/  2.584105, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.012369/  2.559881, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.013593/  2.608638, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.011005/  2.633624, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.010748/  2.593541, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.008631/  2.577610, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.007974/  2.585565, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.007944/  2.605790, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca669b0b12848439ba607bac8a7d729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▆▅▂▇█▆█████▇██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▄▄▆▆▇█▇▇█▇█▇██████████████████████▇██</td></tr><tr><td>tr_acc</td><td>▁▄▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▅▇▇▇████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▄▄▆▆▇█▇▇█▇█▇██████████████████████▇██</td></tr><tr><td>val_loss</td><td>▃▃▁▁▁▁▂▁▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00794</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>2.60579</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">splendid-sweep-72</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zc9gi31u' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zc9gi31u</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_201224-zc9gi31u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xh288r3z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ed5251bd1a4e908bbad0a52357eb96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113539819295208, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_201738-xh288r3z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xh288r3z' target=\"_blank\">avid-sweep-74</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xh288r3z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xh288r3z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  3.181676/  5.413760, val:  40.83%, val_best:  40.83%, tr:  31.66%, tr_best:  31.66%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  4.473614/  4.724107, val:  40.00%, val_best:  40.83%, tr:  49.95%, tr_best:  49.95%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  4.149961/  6.316210, val:  47.08%, val_best:  47.08%, tr:  53.93%, tr_best:  53.93%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  3.354357/  5.519504, val:  49.58%, val_best:  49.58%, tr:  59.65%, tr_best:  59.65%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  4.347077/  4.345770, val:  55.83%, val_best:  55.83%, tr:  55.06%, tr_best:  59.65%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  3.652090/  4.045402, val:  59.17%, val_best:  59.17%, tr:  62.00%, tr_best:  62.00%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  2.782055/  3.337721, val:  62.92%, val_best:  62.92%, tr:  65.17%, tr_best:  65.17%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  2.628621/  4.524745, val:  48.75%, val_best:  62.92%, tr:  64.96%, tr_best:  65.17%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  3.338085/  5.244828, val:  59.17%, val_best:  62.92%, tr:  63.53%, tr_best:  65.17%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  4.414198/  3.917485, val:  67.08%, val_best:  67.08%, tr:  67.42%, tr_best:  67.42%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  3.963005/  4.906206, val:  52.92%, val_best:  67.08%, tr:  68.44%, tr_best:  68.44%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  3.480502/  4.765339, val:  50.83%, val_best:  67.08%, tr:  69.46%, tr_best:  69.46%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  3.657098/  4.235698, val:  66.67%, val_best:  67.08%, tr:  70.28%, tr_best:  70.28%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  2.580814/  5.169703, val:  60.42%, val_best:  67.08%, tr:  76.20%, tr_best:  76.20%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  2.483023/  4.374172, val:  68.75%, val_best:  68.75%, tr:  78.04%, tr_best:  78.04%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  2.838195/  7.250171, val:  55.83%, val_best:  68.75%, tr:  78.24%, tr_best:  78.24%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  2.681814/  4.354947, val:  67.92%, val_best:  68.75%, tr:  80.90%, tr_best:  80.90%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  2.165923/  5.353134, val:  61.67%, val_best:  68.75%, tr:  84.47%, tr_best:  84.47%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  1.864133/  4.577212, val:  70.83%, val_best:  70.83%, tr:  86.31%, tr_best:  86.31%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  1.888279/  4.974484, val:  62.50%, val_best:  70.83%, tr:  85.29%, tr_best:  86.31%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  2.734180/  6.348665, val:  62.08%, val_best:  70.83%, tr:  84.58%, tr_best:  86.31%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  2.703150/  5.128409, val:  73.33%, val_best:  73.33%, tr:  84.17%, tr_best:  86.31%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  1.533946/  4.498092, val:  77.50%, val_best:  77.50%, tr:  92.34%, tr_best:  92.34%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  1.542367/  5.711166, val:  73.75%, val_best:  77.50%, tr:  92.95%, tr_best:  92.95%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  1.482611/  5.174389, val:  78.33%, val_best:  78.33%, tr:  93.87%, tr_best:  93.87%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  1.322523/  4.922507, val:  78.33%, val_best:  78.33%, tr:  94.79%, tr_best:  94.79%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  1.161947/  5.574735, val:  70.83%, val_best:  78.33%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  1.230738/  5.323920, val:  75.00%, val_best:  78.33%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  1.210166/  5.421949, val:  80.00%, val_best:  80.00%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  1.076341/  5.971218, val:  73.33%, val_best:  80.00%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  1.009699/  5.589178, val:  79.17%, val_best:  80.00%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  1.001849/  5.236221, val:  80.42%, val_best:  80.42%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  1.429282/  6.605215, val:  72.50%, val_best:  80.42%, tr:  94.38%, tr_best:  97.55%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.987376/  6.556970, val:  76.25%, val_best:  80.42%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.957783/  6.354659, val:  75.42%, val_best:  80.42%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.746335/  5.148995, val:  81.67%, val_best:  81.67%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.744431/  5.583271, val:  83.33%, val_best:  83.33%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.767535/  5.923662, val:  80.83%, val_best:  83.33%, tr:  98.57%, tr_best:  98.67%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.834892/  5.813684, val:  79.58%, val_best:  83.33%, tr:  98.47%, tr_best:  98.67%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  1.075877/  5.787474, val:  83.33%, val_best:  83.33%, tr:  96.32%, tr_best:  98.67%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.564062/  5.535264, val:  84.17%, val_best:  84.17%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.607212/  6.151634, val:  82.50%, val_best:  84.17%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.513766/  6.023014, val:  82.50%, val_best:  84.17%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.500188/  6.311450, val:  77.50%, val_best:  84.17%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.537401/  6.502505, val:  80.83%, val_best:  84.17%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.516116/  6.160154, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.452000/  6.372248, val:  81.67%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.424280/  6.132617, val:  83.33%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.477295/  7.109072, val:  78.33%, val_best:  84.17%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.538360/  6.395950, val:  83.75%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.317183/  6.659986, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.363280/  6.570338, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.388742/  7.083521, val:  81.25%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.415388/  6.985443, val:  82.92%, val_best:  84.17%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.356882/  6.512261, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.324573/  6.477503, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.321549/  7.740843, val:  77.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.364274/  6.657097, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.262410/  6.988279, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.285588/  6.949323, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.266579/  7.234397, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.268381/  6.925403, val:  80.83%, val_best:  84.17%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.224938/  7.249728, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.323874/  6.901584, val:  84.17%, val_best:  84.17%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.248007/  7.144435, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.228418/  7.030468, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.203634/  7.217637, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.180057/  7.073401, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.353959/  7.367308, val:  83.33%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.183986/  7.354402, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.158297/  7.328667, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.198200/  7.313941, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.181369/  7.666428, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.169342/  7.933135, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.154967/  7.549203, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.153150/  7.640753, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.143008/  7.703470, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.107640/  7.859007, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.129045/  7.761147, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.146374/  8.106822, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.159068/  7.881142, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.194934/  7.861989, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.124637/  7.755784, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.116165/  7.921471, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.139483/  7.884028, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.119611/  8.311483, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.122395/  8.180636, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.098022/  8.341749, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.096704/  8.349123, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.130724/  8.162910, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.108664/  8.124962, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.138748/  8.213618, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.121776/  8.439926, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.096166/  8.688362, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.110372/  8.514387, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.122590/  8.601789, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.092016/  8.678542, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.143248/  8.297621, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.091285/  8.431830, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.081033/  8.302830, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2aa583a36f945ac842bcadac698de28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▅▅▂▅▆▆▇████▇▇████████████▇████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▃▂▅▅▅▄▄▆▇▆▆▆▇▇█▇▇██▇██▇▇▇█████▇██▇████</td></tr><tr><td>tr_acc</td><td>▁▃▃▄▅▅▆▆▆▆▇▇█▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▆██▅█▇▅▄▄▅▃▃▃▃▂▂▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▃▄▅▅▅▅▆▆▇▇▇▇▇█████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▃▂▅▅▅▄▄▆▇▆▆▆▇▇█▇▇██▇██▇▇▇█████▇██▇████</td></tr><tr><td>val_loss</td><td>▃▅▂▂▁▁▂▃▃▃▃▃▄▅▃▄▄▄▅▄▅▆▅▅▆▅▆▆▆▇▇▇▇▇▇█▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.08103</td></tr><tr><td>val_acc_best</td><td>0.8625</td></tr><tr><td>val_acc_now</td><td>0.85833</td></tr><tr><td>val_loss</td><td>8.30283</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">avid-sweep-74</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xh288r3z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xh288r3z</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_201738-xh288r3z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hf0g8x6v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_202333-hf0g8x6v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hf0g8x6v' target=\"_blank\">good-sweep-76</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hf0g8x6v' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hf0g8x6v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 14688881ebd6a7fed8e9c9f512432e6a\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.290931/  3.285763, val:  50.00%, val_best:  50.00%, tr:  33.20%, tr_best:  33.20%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.274539/  2.755466, val:  50.42%, val_best:  50.42%, tr:  50.05%, tr_best:  50.05%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.287030/  2.966850, val:  54.58%, val_best:  54.58%, tr:  55.16%, tr_best:  55.16%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.615644/  2.557642, val:  57.50%, val_best:  57.50%, tr:  61.49%, tr_best:  61.49%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.840318/  2.310223, val:  61.25%, val_best:  61.25%, tr:  61.80%, tr_best:  61.80%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.618941/  2.386409, val:  48.75%, val_best:  61.25%, tr:  64.86%, tr_best:  64.86%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.302867/  1.876479, val:  62.50%, val_best:  62.50%, tr:  67.52%, tr_best:  67.52%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.447661/  2.438178, val:  52.50%, val_best:  62.50%, tr:  70.48%, tr_best:  70.48%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.117707/  1.652621, val:  57.08%, val_best:  62.50%, tr:  71.40%, tr_best:  71.40%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.425327/  1.993865, val:  60.00%, val_best:  62.50%, tr:  72.93%, tr_best:  72.93%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.266749/  1.530416, val:  67.92%, val_best:  67.92%, tr:  71.20%, tr_best:  72.93%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.348685/  1.588612, val:  69.17%, val_best:  69.17%, tr:  76.10%, tr_best:  76.10%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.277582/  1.427765, val:  72.92%, val_best:  72.92%, tr:  74.87%, tr_best:  76.10%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.892180/  1.796056, val:  62.92%, val_best:  72.92%, tr:  81.31%, tr_best:  81.31%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.181950/  2.051447, val:  67.50%, val_best:  72.92%, tr:  79.78%, tr_best:  81.31%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.691565/  1.481443, val:  75.42%, val_best:  75.42%, tr:  87.74%, tr_best:  87.74%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.567692/  1.447774, val:  78.33%, val_best:  78.33%, tr:  91.01%, tr_best:  91.01%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.542160/  1.575277, val:  72.08%, val_best:  78.33%, tr:  91.11%, tr_best:  91.11%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.495442/  1.654402, val:  71.25%, val_best:  78.33%, tr:  93.05%, tr_best:  93.05%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.405518/  1.368041, val:  81.67%, val_best:  81.67%, tr:  93.87%, tr_best:  93.87%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.525344/  2.256021, val:  70.00%, val_best:  81.67%, tr:  92.65%, tr_best:  93.87%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.599085/  1.425346, val:  76.67%, val_best:  81.67%, tr:  93.67%, tr_best:  93.87%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.240384/  1.583195, val:  75.00%, val_best:  81.67%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.267091/  1.808829, val:  73.33%, val_best:  81.67%, tr:  97.55%, tr_best:  98.47%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.251622/  1.583780, val:  78.75%, val_best:  81.67%, tr:  98.26%, tr_best:  98.47%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.248398/  1.609680, val:  77.08%, val_best:  81.67%, tr:  98.37%, tr_best:  98.47%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.213865/  1.651750, val:  79.17%, val_best:  81.67%, tr:  98.26%, tr_best:  98.47%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.162810/  1.865330, val:  73.33%, val_best:  81.67%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.152849/  1.696163, val:  81.67%, val_best:  81.67%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.128147/  1.872654, val:  74.17%, val_best:  81.67%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.110455/  1.594354, val:  81.67%, val_best:  81.67%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.094999/  1.672684, val:  81.25%, val_best:  81.67%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.130897/  1.796074, val:  79.17%, val_best:  81.67%, tr:  98.77%, tr_best:  99.90%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.085448/  1.769716, val:  81.67%, val_best:  81.67%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.093158/  1.847708, val:  75.42%, val_best:  81.67%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.062533/  1.812944, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.056526/  1.750253, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.048811/  1.899442, val:  77.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.053115/  1.781471, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.041478/  1.809064, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.032921/  1.785056, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.028225/  1.872066, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.026923/  1.878423, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.022209/  1.818166, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.022545/  1.856553, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.020860/  1.920555, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.022655/  1.933250, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.025275/  1.896502, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.023301/  1.909496, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.017334/  2.006865, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [16]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.0,0.125,0.25,0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.25, 0.5, 0.75, 1.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0, 0.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [3.0,4.0,5.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        # \"synapse_trace_const2\": {\"values\": [0, 0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"net_save/save_now_net_weights_{unique_name}.pth\"]},\n",
    "        \"learning_rate\": {\"values\": [0.01,0.1]}, \n",
    "        \"epoch_num\": {\"values\": [100]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [1,2,3,4,5,6,7]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [True, False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [True,False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [True]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [0]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [True]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"2\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  unique_name_hyper,\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.lif_layer_v_decay, #wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "                        ) \n",
    "    # sigmoid와 BN이 있어야 잘된다.\n",
    "    # average pooling\n",
    "    # 이 낫다. \n",
    "    \n",
    "    # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'bwu1v6sg'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
