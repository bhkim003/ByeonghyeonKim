{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23977/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #     criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    slice_bucket.append(slice_concat)\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                slice_bucket.append(slice_concat)\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb 과거 하이퍼파라미터 가져와서 붙여넣기 (devices unique_name은 니가 할당해라)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],)\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.25 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "# const2 = False # True # False\n",
    "\n",
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "# if const2 == True:\n",
    "#     const2 = decay\n",
    "# else:\n",
    "#     const2 = 0.0\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"0\",\n",
    "#                 single_step = True, # True # False # DFA_on이랑 같이 가라\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "#                 BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 16, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.75,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 10000,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 5, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "#                 # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "#                 # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "#                 trace_on = True,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용\n",
    "\n",
    "#                 exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "#                 denoise_on = True, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = 0, \n",
    "\n",
    "#                 num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = True, # True # False \n",
    "\n",
    "#                 last_lif = False,\n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoid와 BN이 있어야 잘된다.\n",
    "# # average pooling  \n",
    "# # 이 낫다. \n",
    "\n",
    "# # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8sj0vb5t with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_164640-8sj0vb5t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8sj0vb5t' target=\"_blank\">super-sweep-3</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8sj0vb5t' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8sj0vb5t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = b57dfc72d05d304ce829f424a70e455b\n",
      "cache path doesn't exist\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.210812/  1.681721, val:  31.67%, val_best:  31.67%, tr:  13.07%, tr_best:  13.07%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.386555/  1.353671, val:  55.42%, val_best:  55.42%, tr:  49.54%, tr_best:  49.54%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.224881/  1.376191, val:  57.92%, val_best:  57.92%, tr:  58.94%, tr_best:  58.94%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.063203/  1.275939, val:  60.00%, val_best:  60.00%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.065158/  1.262300, val:  62.50%, val_best:  62.50%, tr:  61.49%, tr_best:  64.04%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.002423/  1.201253, val:  61.67%, val_best:  62.50%, tr:  69.46%, tr_best:  69.46%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.940286/  1.223511, val:  61.25%, val_best:  62.50%, tr:  70.28%, tr_best:  70.28%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.940461/  1.198340, val:  59.17%, val_best:  62.50%, tr:  69.05%, tr_best:  70.28%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.919078/  1.182571, val:  63.33%, val_best:  63.33%, tr:  69.25%, tr_best:  70.28%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.877852/  1.247708, val:  62.92%, val_best:  63.33%, tr:  73.85%, tr_best:  73.85%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.881022/  1.361780, val:  61.67%, val_best:  63.33%, tr:  72.93%, tr_best:  73.85%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.866890/  1.164792, val:  64.58%, val_best:  64.58%, tr:  73.24%, tr_best:  73.85%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.786168/  1.154214, val:  65.83%, val_best:  65.83%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.769624/  1.186466, val:  68.75%, val_best:  68.75%, tr:  79.98%, tr_best:  79.98%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.722701/  1.417607, val:  68.33%, val_best:  68.75%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.689308/  1.264138, val:  63.33%, val_best:  68.75%, tr:  80.80%, tr_best:  81.00%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.654901/  1.278010, val:  62.08%, val_best:  68.75%, tr:  82.12%, tr_best:  82.12%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.639970/  1.258729, val:  65.42%, val_best:  68.75%, tr:  86.82%, tr_best:  86.82%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.589919/  1.419813, val:  65.00%, val_best:  68.75%, tr:  87.95%, tr_best:  87.95%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.623234/  1.257494, val:  69.58%, val_best:  69.58%, tr:  85.80%, tr_best:  87.95%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.622719/  1.389385, val:  68.33%, val_best:  69.58%, tr:  85.90%, tr_best:  87.95%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.556096/  1.529907, val:  62.92%, val_best:  69.58%, tr:  87.95%, tr_best:  87.95%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.521368/  1.358394, val:  62.92%, val_best:  69.58%, tr:  89.79%, tr_best:  89.79%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.520175/  1.370903, val:  71.25%, val_best:  71.25%, tr:  90.81%, tr_best:  90.81%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.465218/  1.357821, val:  72.92%, val_best:  72.92%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.520684/  1.376833, val:  65.42%, val_best:  72.92%, tr:  89.38%, tr_best:  93.36%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.497071/  1.364841, val:  72.08%, val_best:  72.92%, tr:  90.19%, tr_best:  93.36%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.426883/  1.524139, val:  68.33%, val_best:  72.92%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.394330/  1.393904, val:  70.83%, val_best:  72.92%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.356238/  1.511496, val:  67.08%, val_best:  72.92%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.356547/  1.548883, val:  64.17%, val_best:  72.92%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.344267/  1.541734, val:  71.67%, val_best:  72.92%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.377733/  1.658543, val:  65.00%, val_best:  72.92%, tr:  94.59%, tr_best:  98.06%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.342921/  1.606889, val:  72.50%, val_best:  72.92%, tr:  96.94%, tr_best:  98.06%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.328112/  1.641720, val:  67.08%, val_best:  72.92%, tr:  97.75%, tr_best:  98.06%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.296126/  1.649913, val:  73.33%, val_best:  73.33%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.274631/  1.615058, val:  71.25%, val_best:  73.33%, tr:  98.26%, tr_best:  98.57%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.278728/  1.793739, val:  63.33%, val_best:  73.33%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.314043/  1.647128, val:  72.92%, val_best:  73.33%, tr:  97.34%, tr_best:  98.57%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.253166/  1.730766, val:  69.17%, val_best:  73.33%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.235799/  1.697649, val:  71.25%, val_best:  73.33%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.232454/  1.765837, val:  74.58%, val_best:  74.58%, tr:  98.77%, tr_best:  99.18%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.200667/  1.739882, val:  71.25%, val_best:  74.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.198410/  1.808813, val:  68.33%, val_best:  74.58%, tr:  99.28%, tr_best:  99.80%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.218983/  1.819561, val:  69.17%, val_best:  74.58%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.200601/  1.799066, val:  70.83%, val_best:  74.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.181203/  1.816794, val:  74.58%, val_best:  74.58%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.161995/  1.828239, val:  75.42%, val_best:  75.42%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.161031/  1.907332, val:  73.33%, val_best:  75.42%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.146101/  1.856286, val:  73.75%, val_best:  75.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.142790/  1.970911, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.147648/  1.916479, val:  73.75%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.149722/  1.970506, val:  74.58%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.128960/  1.925620, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.108222/  2.021706, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.118099/  2.030490, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.115658/  2.077209, val:  72.92%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.124004/  2.038905, val:  75.00%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.109410/  2.073846, val:  72.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.101244/  2.093413, val:  70.83%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.100152/  2.076909, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.110134/  2.089176, val:  72.92%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.098404/  2.215020, val:  70.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.095727/  2.103407, val:  75.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.092267/  2.138840, val:  75.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.083851/  2.116917, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.088618/  2.160309, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.084024/  2.214997, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.077517/  2.206853, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.066714/  2.226120, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.072256/  2.220895, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.076774/  2.253786, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.075388/  2.242424, val:  73.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.072948/  2.329711, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.066100/  2.289666, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.064934/  2.308052, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.066532/  2.287781, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.066405/  2.326518, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.062989/  2.363613, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.055204/  2.349234, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.056934/  2.382283, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.054835/  2.381593, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.050144/  2.429185, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.063122/  2.430706, val:  72.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.057895/  2.338602, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.042740/  2.479431, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.042660/  2.459172, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.050772/  2.454995, val:  73.33%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.053046/  2.475026, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.044934/  2.510246, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.041122/  2.498242, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.035333/  2.469730, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.037683/  2.534963, val:  73.33%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.040039/  2.510855, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.040805/  2.540192, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.033972/  2.552210, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.036334/  2.548234, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.030993/  2.583926, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.033398/  2.530824, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.031649/  2.593419, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834c87df659f4e4f987b8a458f52cdbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▄▄▆▅▆▅▅█▇██▇▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▅▆▆▇▆▇▆▇▇▆▆▇▆▇▇▇█▇▇▇█▇▇█▇█▇█▇███▇█▇█▇</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▆▆▇▇▇▇▇████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▅▆▆▇▆▇▆▇▇▆▆▇▆▇▇▇█▇▇▇█▇▇█▇█▇█▇███▇█▇█▇</td></tr><tr><td>val_loss</td><td>▄▂▂▁▁▁▂▂▂▃▂▂▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03165</td></tr><tr><td>val_acc_best</td><td>0.78333</td></tr><tr><td>val_acc_now</td><td>0.76667</td></tr><tr><td>val_loss</td><td>2.59342</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">super-sweep-3</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8sj0vb5t' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8sj0vb5t</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_164640-8sj0vb5t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3fepn5o9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_165459-3fepn5o9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3fepn5o9' target=\"_blank\">tough-sweep-5</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3fepn5o9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3fepn5o9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.324377/  2.253040, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.09%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.620876/  1.434216, val:  55.83%, val_best:  55.83%, tr:  43.11%, tr_best:  43.11%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.395223/  1.387492, val:  60.00%, val_best:  60.00%, tr:  57.41%, tr_best:  57.41%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.225031/  1.483529, val:  55.00%, val_best:  60.00%, tr:  61.59%, tr_best:  61.59%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.169817/  1.421775, val:  61.67%, val_best:  61.67%, tr:  63.23%, tr_best:  63.23%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.066954/  1.304021, val:  63.75%, val_best:  63.75%, tr:  66.80%, tr_best:  66.80%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.064376/  1.238216, val:  65.42%, val_best:  65.42%, tr:  68.95%, tr_best:  68.95%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.024880/  1.199529, val:  69.17%, val_best:  69.17%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.029123/  1.336443, val:  65.42%, val_best:  69.17%, tr:  72.01%, tr_best:  72.01%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.941154/  1.283742, val:  62.50%, val_best:  69.17%, tr:  76.71%, tr_best:  76.71%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.947766/  1.218557, val:  71.67%, val_best:  71.67%, tr:  74.46%, tr_best:  76.71%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.831730/  1.175322, val:  78.33%, val_best:  78.33%, tr:  82.43%, tr_best:  82.43%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.732076/  1.132564, val:  81.25%, val_best:  81.25%, tr:  85.19%, tr_best:  85.19%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.699328/  1.109289, val:  83.75%, val_best:  83.75%, tr:  87.74%, tr_best:  87.74%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.709120/  1.330841, val:  76.67%, val_best:  83.75%, tr:  86.62%, tr_best:  87.74%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.615329/  1.213431, val:  80.42%, val_best:  83.75%, tr:  89.89%, tr_best:  89.89%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.583712/  1.172938, val:  82.08%, val_best:  83.75%, tr:  91.93%, tr_best:  91.93%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.522593/  1.172161, val:  84.17%, val_best:  84.17%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.492227/  1.255606, val:  78.75%, val_best:  84.17%, tr:  95.51%, tr_best:  95.51%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.458635/  1.185620, val:  83.75%, val_best:  84.17%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.436657/  1.216015, val:  86.67%, val_best:  86.67%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.408795/  1.418315, val:  75.83%, val_best:  86.67%, tr:  96.53%, tr_best:  96.73%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.397339/  1.363236, val:  79.17%, val_best:  86.67%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.392192/  1.274249, val:  84.58%, val_best:  86.67%, tr:  96.53%, tr_best:  97.85%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.342104/  1.305457, val:  84.58%, val_best:  86.67%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.327823/  1.339410, val:  85.42%, val_best:  86.67%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.348489/  1.340654, val:  86.25%, val_best:  86.67%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.303730/  1.472875, val:  82.08%, val_best:  86.67%, tr:  98.26%, tr_best:  98.37%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.286895/  1.302079, val:  87.92%, val_best:  87.92%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.256725/  1.526289, val:  81.67%, val_best:  87.92%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.263467/  1.343612, val:  86.25%, val_best:  87.92%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.231242/  1.439291, val:  86.67%, val_best:  87.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.277236/  1.455598, val:  83.75%, val_best:  87.92%, tr:  98.26%, tr_best:  99.69%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.243337/  1.475184, val:  86.25%, val_best:  87.92%, tr:  99.18%, tr_best:  99.69%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.228707/  1.511039, val:  85.83%, val_best:  87.92%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.205928/  1.488270, val:  87.08%, val_best:  87.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.184888/  1.487229, val:  86.25%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.176397/  1.569968, val:  86.25%, val_best:  87.92%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.172465/  1.525931, val:  87.92%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.159983/  1.586069, val:  87.92%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.158487/  1.559112, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.155101/  1.608311, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.129379/  1.628664, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.138231/  1.640852, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.128470/  1.701276, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.123701/  1.678072, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.128427/  1.750961, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.121205/  1.691087, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.117829/  1.722983, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.104483/  1.782358, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.102234/  1.809709, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.102463/  1.793564, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.103370/  1.924231, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.088605/  1.887205, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.091713/  1.841688, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.084755/  1.893186, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.080027/  1.923437, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.075570/  1.865280, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.066207/  1.890004, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.078241/  1.963507, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.066644/  1.937706, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.070238/  1.974469, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.068976/  2.043910, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.066484/  2.017676, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.059064/  2.008111, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.060678/  2.038437, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.056949/  2.116035, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.057309/  2.131035, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.051204/  2.128208, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.049041/  2.085106, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.041603/  2.106980, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.047903/  2.167952, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.049546/  2.164062, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.049787/  2.154206, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.047796/  2.187084, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.048630/  2.162773, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.041851/  2.198905, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.040721/  2.220918, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.041849/  2.187904, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.038748/  2.198490, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.036521/  2.291597, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.037336/  2.265679, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.035793/  2.274508, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.035333/  2.312144, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.038348/  2.310215, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.039090/  2.357795, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.029031/  2.316517, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.027537/  2.350415, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.030840/  2.355761, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.026686/  2.383904, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.024622/  2.396797, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.027923/  2.433868, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.028283/  2.424008, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.021742/  2.396441, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.025743/  2.462657, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.022960/  2.496866, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.026696/  2.494616, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.022717/  2.501087, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.022810/  2.476700, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.023298/  2.457374, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d666ac3e3b46f2bd759af8b4e0619b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▃▃▇▇▇██████▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▆▇▇▇▇▇██▇▇██████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇▇▇███████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▆▇▇▇▇▇██▇▇██████████████████████████</td></tr><tr><td>val_loss</td><td>▇▂▂▁▂▁▂▁▁▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0233</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>2.45737</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-sweep-5</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3fepn5o9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3fepn5o9</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_165459-3fepn5o9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tt6huood with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_170034-tt6huood</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tt6huood' target=\"_blank\">exalted-sweep-7</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tt6huood' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tt6huood</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 8ba471014b61e50904ecff33d030e937\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.035592/  1.580963, val:  47.92%, val_best:  47.92%, tr:  24.62%, tr_best:  24.62%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.507693/  1.466623, val:  57.92%, val_best:  57.92%, tr:  48.83%, tr_best:  48.83%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.472715/  1.552243, val:  55.42%, val_best:  57.92%, tr:  53.83%, tr_best:  53.83%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.249971/  1.489647, val:  53.33%, val_best:  57.92%, tr:  60.57%, tr_best:  60.57%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.133631/  1.382339, val:  59.58%, val_best:  59.58%, tr:  62.31%, tr_best:  62.31%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.081048/  1.517339, val:  52.08%, val_best:  59.58%, tr:  67.11%, tr_best:  67.11%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.104153/  1.358119, val:  60.83%, val_best:  60.83%, tr:  66.39%, tr_best:  67.11%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.104998/  1.600646, val:  56.67%, val_best:  60.83%, tr:  68.85%, tr_best:  68.85%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.156435/  1.468281, val:  64.58%, val_best:  64.58%, tr:  68.74%, tr_best:  68.85%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.097169/  1.456214, val:  59.58%, val_best:  64.58%, tr:  68.34%, tr_best:  68.85%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.042390/  1.293977, val:  64.17%, val_best:  64.58%, tr:  70.89%, tr_best:  70.89%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.959784/  1.328928, val:  65.83%, val_best:  65.83%, tr:  73.03%, tr_best:  73.03%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.915014/  1.238535, val:  65.00%, val_best:  65.83%, tr:  72.63%, tr_best:  73.03%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.925886/  1.440569, val:  57.92%, val_best:  65.83%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.931944/  1.627750, val:  61.67%, val_best:  65.83%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.748272/  1.539011, val:  62.92%, val_best:  65.83%, tr:  78.86%, tr_best:  78.86%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.790984/  1.429748, val:  65.42%, val_best:  65.83%, tr:  78.65%, tr_best:  78.86%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.733015/  1.459944, val:  64.17%, val_best:  65.83%, tr:  83.04%, tr_best:  83.04%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.679227/  1.546365, val:  68.75%, val_best:  68.75%, tr:  84.47%, tr_best:  84.47%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.651026/  1.488698, val:  64.58%, val_best:  68.75%, tr:  85.80%, tr_best:  85.80%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.796947/  1.434194, val:  70.42%, val_best:  70.42%, tr:  83.45%, tr_best:  85.80%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.571238/  1.839175, val:  66.67%, val_best:  70.42%, tr:  87.95%, tr_best:  87.95%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.529673/  1.542075, val:  69.58%, val_best:  70.42%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.483220/  1.669167, val:  67.08%, val_best:  70.42%, tr:  91.73%, tr_best:  91.73%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.435203/  1.541310, val:  73.33%, val_best:  73.33%, tr:  94.59%, tr_best:  94.59%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.370709/  1.676523, val:  65.00%, val_best:  73.33%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.373787/  1.682778, val:  68.75%, val_best:  73.33%, tr:  96.12%, tr_best:  96.83%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.329681/  1.690518, val:  73.33%, val_best:  73.33%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.329053/  1.616161, val:  77.92%, val_best:  77.92%, tr:  97.14%, tr_best:  98.06%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.293315/  1.801849, val:  69.17%, val_best:  77.92%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.284227/  1.777750, val:  71.25%, val_best:  77.92%, tr:  97.96%, tr_best:  98.37%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.270596/  1.765301, val:  76.67%, val_best:  77.92%, tr:  98.26%, tr_best:  98.37%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.287493/  1.893185, val:  65.83%, val_best:  77.92%, tr:  97.55%, tr_best:  98.37%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.287292/  1.836478, val:  75.00%, val_best:  77.92%, tr:  97.75%, tr_best:  98.37%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.230640/  1.772598, val:  77.08%, val_best:  77.92%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.189669/  1.889271, val:  75.83%, val_best:  77.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.171821/  1.871721, val:  76.67%, val_best:  77.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.175313/  2.020895, val:  73.75%, val_best:  77.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.161380/  1.950887, val:  78.75%, val_best:  78.75%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.136374/  2.132795, val:  70.83%, val_best:  78.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.136474/  2.126020, val:  73.33%, val_best:  78.75%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.123623/  2.095488, val:  81.25%, val_best:  81.25%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.095931/  2.092487, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.087311/  2.242603, val:  72.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.091711/  2.155024, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.070759/  2.209466, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.069842/  2.282062, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.065690/  2.277789, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.064365/  2.368451, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.054050/  2.348161, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.050477/  2.396267, val:  76.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.052847/  2.377842, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.052765/  2.505138, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.048272/  2.445440, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.037763/  2.483969, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.034197/  2.503530, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.035505/  2.513982, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.033834/  2.507800, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.032155/  2.542345, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.030229/  2.650914, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.037819/  2.591983, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.032307/  2.632077, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.027821/  2.611564, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.025664/  2.669932, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.020138/  2.701665, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.017722/  2.704563, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.018454/  2.738942, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.020943/  2.754907, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.020699/  2.743416, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.014644/  2.744193, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.013925/  2.782266, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.013341/  2.784113, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.014974/  2.815134, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.014637/  2.863376, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.014899/  2.839279, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.011046/  2.845820, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.010504/  2.882282, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.009059/  2.851941, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.009322/  2.886022, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.009082/  2.876637, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.010420/  2.944585, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.010343/  2.968212, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.009920/  2.913336, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.009822/  2.953783, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.011606/  2.992542, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.010293/  3.044957, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.008098/  3.001204, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.010518/  3.003223, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.010434/  3.068053, val:  76.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.013115/  3.088032, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.007313/  3.058678, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.009401/  3.068687, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.006816/  3.100790, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.007906/  3.069182, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.006290/  3.133502, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.005406/  3.116328, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.006168/  3.127232, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.006102/  3.120913, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.005640/  3.169939, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.006301/  3.153309, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce9c6f0030347f88ac7b9dfc48b134f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▄▅▂▆▇▇▆████▇██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▃▃▃▅▄▄▄▅▆▅▅▅▇▆▆▇▇██▇█████▇████████▇███</td></tr><tr><td>tr_acc</td><td>▁▄▅▅▅▅▆▆▇▇▇█████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▅▅▄▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▃▄▄▅▅▅▅▆▆▆▇▇▇▇▇███████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▃▃▃▅▄▄▄▅▆▅▅▅▇▆▆▇▇██▇█████▇████████▇███</td></tr><tr><td>val_loss</td><td>▂▂▂▂▂▁▂▂▂▃▂▃▃▃▃▄▄▄▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0063</td></tr><tr><td>val_acc_best</td><td>0.81667</td></tr><tr><td>val_acc_now</td><td>0.79583</td></tr><tr><td>val_loss</td><td>3.15331</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-sweep-7</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tt6huood' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tt6huood</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_170034-tt6huood/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b10ci39g with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6163dea00d43b18f9a752af75263f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112897077368365, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_170623-b10ci39g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b10ci39g' target=\"_blank\">feasible-sweep-9</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b10ci39g' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b10ci39g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 14688881ebd6a7fed8e9c9f512432e6a\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.231682/  1.725753, val:  33.75%, val_best:  33.75%, tr:  14.20%, tr_best:  14.20%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.502246/  1.526380, val:  56.25%, val_best:  56.25%, tr:  51.38%, tr_best:  51.38%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.485144/  1.639171, val:  57.08%, val_best:  57.08%, tr:  57.10%, tr_best:  57.10%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.264549/  1.770136, val:  52.92%, val_best:  57.08%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.248263/  1.324783, val:  62.50%, val_best:  62.50%, tr:  62.21%, tr_best:  62.72%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.102589/  1.560552, val:  56.25%, val_best:  62.50%, tr:  68.13%, tr_best:  68.13%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.127105/  1.391897, val:  62.08%, val_best:  62.50%, tr:  67.82%, tr_best:  68.13%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.157054/  1.464881, val:  60.42%, val_best:  62.50%, tr:  67.82%, tr_best:  68.13%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.123250/  1.456251, val:  70.00%, val_best:  70.00%, tr:  70.17%, tr_best:  70.17%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.170959/  1.314588, val:  65.42%, val_best:  70.00%, tr:  71.71%, tr_best:  71.71%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.111409/  1.223261, val:  69.17%, val_best:  70.00%, tr:  73.75%, tr_best:  73.75%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.970058/  1.358844, val:  69.58%, val_best:  70.00%, tr:  76.10%, tr_best:  76.10%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.911124/  1.255785, val:  74.58%, val_best:  74.58%, tr:  76.10%, tr_best:  76.10%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.830981/  1.404512, val:  66.25%, val_best:  74.58%, tr:  81.61%, tr_best:  81.61%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.867591/  1.586107, val:  69.17%, val_best:  74.58%, tr:  79.88%, tr_best:  81.61%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.696896/  1.371113, val:  75.42%, val_best:  75.42%, tr:  87.44%, tr_best:  87.44%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.695881/  1.313211, val:  78.75%, val_best:  78.75%, tr:  87.44%, tr_best:  87.44%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.645288/  1.370996, val:  75.00%, val_best:  78.75%, tr:  91.22%, tr_best:  91.22%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.608394/  1.420346, val:  74.58%, val_best:  78.75%, tr:  92.85%, tr_best:  92.85%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.569320/  1.282387, val:  81.25%, val_best:  81.25%, tr:  93.46%, tr_best:  93.46%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.537541/  1.425604, val:  75.00%, val_best:  81.25%, tr:  93.97%, tr_best:  93.97%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.520091/  1.365157, val:  77.50%, val_best:  81.25%, tr:  93.16%, tr_best:  93.97%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.447665/  1.458992, val:  77.50%, val_best:  81.25%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.486769/  1.445451, val:  83.75%, val_best:  83.75%, tr:  94.48%, tr_best:  96.12%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.419895/  1.405383, val:  83.75%, val_best:  83.75%, tr:  96.63%, tr_best:  96.63%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.387360/  1.488586, val:  78.75%, val_best:  83.75%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.396446/  1.496646, val:  82.08%, val_best:  83.75%, tr:  97.24%, tr_best:  97.65%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.347656/  1.606530, val:  78.33%, val_best:  83.75%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.354080/  1.495938, val:  82.08%, val_best:  83.75%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.326816/  1.728316, val:  77.50%, val_best:  83.75%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.314989/  1.548785, val:  82.08%, val_best:  83.75%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.286499/  1.576852, val:  82.50%, val_best:  83.75%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.379415/  1.764387, val:  75.42%, val_best:  83.75%, tr:  96.42%, tr_best:  99.08%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.294208/  1.791745, val:  80.00%, val_best:  83.75%, tr:  98.57%, tr_best:  99.08%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.280195/  1.779899, val:  80.00%, val_best:  83.75%, tr:  98.77%, tr_best:  99.08%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.259849/  1.654489, val:  85.42%, val_best:  85.42%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.225250/  1.648249, val:  84.58%, val_best:  85.42%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.219818/  1.683023, val:  82.50%, val_best:  85.42%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.221132/  1.722826, val:  85.00%, val_best:  85.42%, tr:  99.18%, tr_best:  99.59%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.219920/  1.733050, val:  85.42%, val_best:  85.42%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.187905/  1.726974, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.186896/  1.821142, val:  85.83%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.171857/  1.825525, val:  83.75%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.157112/  1.833568, val:  83.75%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.155573/  1.922752, val:  80.42%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.149704/  1.890066, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.155295/  1.944295, val:  83.75%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.145255/  1.938594, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.162080/  1.923778, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.122033/  1.910164, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.120061/  1.946535, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.111476/  1.999621, val:  85.42%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.125914/  2.069286, val:  82.92%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.121611/  2.029336, val:  85.00%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.119072/  2.003530, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.099003/  2.111725, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.100748/  2.161664, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.091105/  2.142911, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.081562/  2.164029, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.083714/  2.180006, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.074662/  2.172743, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.077839/  2.196053, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.076606/  2.234782, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.071461/  2.238532, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.089436/  2.279516, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.069939/  2.289559, val:  87.08%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.066738/  2.295385, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.056468/  2.333027, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.061742/  2.374368, val:  87.08%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.050225/  2.401031, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.047095/  2.325894, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.050537/  2.386467, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.041838/  2.464985, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.048119/  2.462368, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.046133/  2.480167, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.045559/  2.485793, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.042508/  2.490080, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.045580/  2.545574, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.041018/  2.587059, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.042720/  2.553449, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.036753/  2.608487, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.037326/  2.615348, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.034386/  2.606033, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.034790/  2.609849, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.041508/  2.590512, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.029394/  2.614373, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.029366/  2.604161, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.033145/  2.642275, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.034232/  2.688861, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.033353/  2.659101, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.028558/  2.669632, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.026406/  2.739908, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.027827/  2.740283, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.026233/  2.731324, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.028431/  2.766816, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.027781/  2.768856, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.019218/  2.781971, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.018251/  2.833440, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.020688/  2.887144, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.023139/  2.857838, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee411c67ee94740bdc1b895e8883628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▅▄▃▆▇▇▇█████▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▅▄▅▆▆▆▇▇▇▇▇▆█▇█▇▇██▇██████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▅▅▆▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▅▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▅▄▅▆▆▆▇▇▇▇▇▆█▇█▇▇██▇██████████████████</td></tr><tr><td>val_loss</td><td>▃▃▁▂▁▁▂▂▁▁▂▂▃▃▃▃▃▄▄▄▄▅▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.02314</td></tr><tr><td>val_acc_best</td><td>0.89583</td></tr><tr><td>val_acc_now</td><td>0.87083</td></tr><tr><td>val_loss</td><td>2.85784</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feasible-sweep-9</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b10ci39g' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b10ci39g</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_170623-b10ci39g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kc2g11em with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_171215-kc2g11em</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kc2g11em' target=\"_blank\">easy-sweep-11</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kc2g11em' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kc2g11em</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 14688881ebd6a7fed8e9c9f512432e6a\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.325213/  2.309355, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.323420/  2.309336, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:   9.50%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.257725/  2.122288, val:  20.83%, val_best:  20.83%, tr:  13.69%, tr_best:  13.69%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.901619/  1.674979, val:  41.25%, val_best:  41.25%, tr:  31.66%, tr_best:  31.66%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.458922/  1.454160, val:  55.00%, val_best:  55.00%, tr:  49.95%, tr_best:  49.95%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.231198/  1.367196, val:  57.50%, val_best:  57.50%, tr:  60.67%, tr_best:  60.67%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.123468/  1.388977, val:  57.92%, val_best:  57.92%, tr:  67.31%, tr_best:  67.31%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.090744/  1.251736, val:  68.33%, val_best:  68.33%, tr:  64.66%, tr_best:  67.31%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.042804/  1.306539, val:  64.17%, val_best:  68.33%, tr:  67.42%, tr_best:  67.42%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.015295/  1.356825, val:  60.42%, val_best:  68.33%, tr:  70.99%, tr_best:  70.99%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.994651/  1.331126, val:  62.50%, val_best:  68.33%, tr:  72.42%, tr_best:  72.42%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.959844/  1.262506, val:  66.67%, val_best:  68.33%, tr:  72.63%, tr_best:  72.63%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.946975/  1.250174, val:  65.83%, val_best:  68.33%, tr:  76.20%, tr_best:  76.20%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.959033/  1.316757, val:  66.25%, val_best:  68.33%, tr:  73.85%, tr_best:  76.20%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.908584/  1.469307, val:  64.58%, val_best:  68.33%, tr:  77.22%, tr_best:  77.22%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.886500/  1.257038, val:  75.42%, val_best:  75.42%, tr:  78.24%, tr_best:  78.24%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.858058/  1.261414, val:  73.75%, val_best:  75.42%, tr:  83.25%, tr_best:  83.25%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.776396/  1.251712, val:  76.67%, val_best:  76.67%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.752413/  1.339310, val:  69.58%, val_best:  76.67%, tr:  87.64%, tr_best:  88.76%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.754779/  1.234095, val:  74.58%, val_best:  76.67%, tr:  87.23%, tr_best:  88.76%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.698534/  1.290398, val:  70.42%, val_best:  76.67%, tr:  91.22%, tr_best:  91.22%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.657178/  1.343645, val:  75.83%, val_best:  76.67%, tr:  92.34%, tr_best:  92.34%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.622477/  1.263137, val:  73.33%, val_best:  76.67%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.621107/  1.236583, val:  79.58%, val_best:  79.58%, tr:  93.67%, tr_best:  94.18%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.552888/  1.253900, val:  80.42%, val_best:  80.42%, tr:  95.51%, tr_best:  95.51%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.533583/  1.266387, val:  77.50%, val_best:  80.42%, tr:  95.20%, tr_best:  95.51%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.555131/  1.232987, val:  82.50%, val_best:  82.50%, tr:  94.89%, tr_best:  95.51%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.500015/  1.283973, val:  80.00%, val_best:  82.50%, tr:  95.10%, tr_best:  95.51%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.476445/  1.221905, val:  84.58%, val_best:  84.58%, tr:  96.94%, tr_best:  96.94%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.443177/  1.324456, val:  78.33%, val_best:  84.58%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.436253/  1.299680, val:  80.83%, val_best:  84.58%, tr:  97.55%, tr_best:  97.85%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.416487/  1.306945, val:  83.75%, val_best:  84.58%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.452150/  1.317830, val:  81.25%, val_best:  84.58%, tr:  95.40%, tr_best:  97.96%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.415049/  1.308843, val:  80.00%, val_best:  84.58%, tr:  97.45%, tr_best:  97.96%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.378151/  1.389603, val:  79.17%, val_best:  84.58%, tr:  97.55%, tr_best:  97.96%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.361775/  1.382216, val:  83.33%, val_best:  84.58%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.313115/  1.350943, val:  81.67%, val_best:  84.58%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.312201/  1.436656, val:  82.92%, val_best:  84.58%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.341139/  1.420843, val:  82.92%, val_best:  84.58%, tr:  98.37%, tr_best:  99.39%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.303823/  1.408807, val:  82.92%, val_best:  84.58%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.275122/  1.424196, val:  84.58%, val_best:  84.58%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.275942/  1.508841, val:  83.75%, val_best:  84.58%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.266124/  1.429088, val:  81.67%, val_best:  84.58%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.248068/  1.534346, val:  84.17%, val_best:  84.58%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.241100/  1.528304, val:  81.25%, val_best:  84.58%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.236505/  1.567416, val:  79.17%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.234601/  1.515483, val:  85.83%, val_best:  85.83%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.212999/  1.553505, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.222763/  1.561135, val:  85.83%, val_best:  85.83%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.196429/  1.564774, val:  83.33%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.186548/  1.606349, val:  83.33%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.177837/  1.635199, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.196435/  1.633104, val:  84.58%, val_best:  85.83%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.160109/  1.666535, val:  84.58%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.153279/  1.635897, val:  85.42%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.148756/  1.677055, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.143410/  1.796874, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.136822/  1.723930, val:  85.42%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.126754/  1.761441, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.126039/  1.849384, val:  80.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.126832/  1.812066, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.125630/  1.784477, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.113742/  1.829388, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.114032/  1.831561, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.105755/  1.882290, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.102055/  1.892741, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.100122/  1.909715, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.101664/  1.877176, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.092751/  1.911234, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.090008/  1.963776, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.085453/  1.948945, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.099124/  1.988902, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.088017/  2.094516, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.091853/  1.927396, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.078182/  2.022930, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.075423/  2.021489, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.070711/  2.046849, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.075020/  2.101609, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.074042/  2.095478, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.081937/  2.085367, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.064002/  2.114022, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.066530/  2.131485, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.062832/  2.119372, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.062291/  2.162821, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.061755/  2.169271, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.061586/  2.197699, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.056691/  2.189718, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.055255/  2.206592, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.056410/  2.163072, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.056890/  2.200509, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.047530/  2.237144, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.047008/  2.272897, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.053081/  2.278822, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.053720/  2.281326, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.048874/  2.316591, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.046392/  2.322711, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.052522/  2.336498, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.052780/  2.307106, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.041956/  2.341093, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.043673/  2.321198, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c43ef7debe648f199c2701ed625b5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▃▅▆▅▇▃▇█▇██▇▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▅▆▆▆▆▇▇▇▇█▇▇███▇▇█████▇████▇██████████</td></tr><tr><td>tr_acc</td><td>▁▁▄▅▆▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▅▆▆▆▆▇▇▇▇█████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▅▆▆▆▆▇▇▇▇█▇▇███▇▇█████▇████▇██████████</td></tr><tr><td>val_loss</td><td>█▇▂▁▂▁▃▁▁▂▁▁▂▂▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▇▆▇▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.04367</td></tr><tr><td>val_acc_best</td><td>0.87917</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>2.3212</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">easy-sweep-11</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kc2g11em' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kc2g11em</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_171215-kc2g11em/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wnwzce5t with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_171848-wnwzce5t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wnwzce5t' target=\"_blank\">good-sweep-13</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wnwzce5t' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wnwzce5t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 14688881ebd6a7fed8e9c9f512432e6a\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 37.486259/ 47.360313, val:  30.00%, val_best:  30.00%, tr:  26.97%, tr_best:  26.97%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 29.989681/ 22.198614, val:  36.67%, val_best:  36.67%, tr:  35.55%, tr_best:  35.55%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 21.643822/ 20.473492, val:  24.58%, val_best:  36.67%, tr:  41.06%, tr_best:  41.06%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 18.301365/ 17.366859, val:  47.50%, val_best:  47.50%, tr:  44.64%, tr_best:  44.64%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 20.244917/ 11.775909, val:  49.17%, val_best:  49.17%, tr:  48.31%, tr_best:  48.31%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 16.090427/ 26.801262, val:  47.50%, val_best:  49.17%, tr:  49.34%, tr_best:  49.34%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 20.685627/ 33.645592, val:  30.00%, val_best:  49.17%, tr:  49.74%, tr_best:  49.74%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 18.305767/ 20.743132, val:  43.33%, val_best:  49.17%, tr:  52.81%, tr_best:  52.81%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 16.264608/ 18.416811, val:  45.00%, val_best:  49.17%, tr:  53.22%, tr_best:  53.22%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 14.925664/ 21.844955, val:  50.83%, val_best:  50.83%, tr:  57.81%, tr_best:  57.81%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 18.188309/ 16.622047, val:  52.08%, val_best:  52.08%, tr:  51.99%, tr_best:  57.81%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 16.705622/ 29.579218, val:  46.67%, val_best:  52.08%, tr:  56.38%, tr_best:  57.81%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 13.926635/ 17.334406, val:  41.67%, val_best:  52.08%, tr:  61.70%, tr_best:  61.70%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 16.987127/ 24.934757, val:  41.25%, val_best:  52.08%, tr:  54.44%, tr_best:  61.70%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 17.228737/ 20.964857, val:  55.83%, val_best:  55.83%, tr:  60.78%, tr_best:  61.70%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss: 14.446385/ 13.778874, val:  58.75%, val_best:  58.75%, tr:  64.76%, tr_best:  64.76%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 14.213074/ 18.672085, val:  50.42%, val_best:  58.75%, tr:  62.21%, tr_best:  64.76%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss: 12.566867/ 12.875074, val:  60.42%, val_best:  60.42%, tr:  60.98%, tr_best:  64.76%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  9.691567/ 16.861668, val:  61.25%, val_best:  61.25%, tr:  68.03%, tr_best:  68.03%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  9.381811/ 22.196638, val:  47.50%, val_best:  61.25%, tr:  69.56%, tr_best:  69.56%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 12.961784/ 26.327480, val:  52.92%, val_best:  61.25%, tr:  67.11%, tr_best:  69.56%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss: 13.641105/ 13.093030, val:  60.83%, val_best:  61.25%, tr:  67.72%, tr_best:  69.56%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss: 11.005843/ 17.672409, val:  60.00%, val_best:  61.25%, tr:  69.15%, tr_best:  69.56%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  9.854492/ 15.451037, val:  61.25%, val_best:  61.25%, tr:  71.30%, tr_best:  71.30%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  7.155700/ 10.185705, val:  74.58%, val_best:  74.58%, tr:  77.02%, tr_best:  77.02%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  6.320178/ 15.098889, val:  57.92%, val_best:  74.58%, tr:  79.16%, tr_best:  79.16%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  7.749578/ 13.499935, val:  58.33%, val_best:  74.58%, tr:  76.30%, tr_best:  79.16%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  6.723845/ 13.793012, val:  60.42%, val_best:  74.58%, tr:  78.35%, tr_best:  79.16%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  8.480736/ 15.880428, val:  65.42%, val_best:  74.58%, tr:  80.59%, tr_best:  80.59%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  6.050873/ 12.464484, val:  63.75%, val_best:  74.58%, tr:  82.64%, tr_best:  82.64%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  6.449759/ 14.766911, val:  63.33%, val_best:  74.58%, tr:  81.31%, tr_best:  82.64%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  4.914611/ 12.338664, val:  67.08%, val_best:  74.58%, tr:  84.27%, tr_best:  84.27%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  4.874726/ 17.718662, val:  62.50%, val_best:  74.58%, tr:  86.93%, tr_best:  86.93%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  4.434342/ 11.840718, val:  70.83%, val_best:  74.58%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  7.725766/ 14.681103, val:  63.33%, val_best:  74.58%, tr:  78.24%, tr_best:  88.76%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  4.987824/ 11.821396, val:  75.00%, val_best:  75.00%, tr:  89.07%, tr_best:  89.07%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  2.979253/ 11.459023, val:  77.08%, val_best:  77.08%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  4.430218/ 17.176769, val:  64.17%, val_best:  77.08%, tr:  87.95%, tr_best:  94.28%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  4.937554/ 14.361374, val:  68.75%, val_best:  77.08%, tr:  90.91%, tr_best:  94.28%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  3.099702/ 11.076162, val:  79.58%, val_best:  79.58%, tr:  92.85%, tr_best:  94.28%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  2.135026/ 13.120961, val:  71.67%, val_best:  79.58%, tr:  95.81%, tr_best:  95.81%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  2.579241/ 10.508093, val:  80.42%, val_best:  80.42%, tr:  95.61%, tr_best:  95.81%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  4.308782/ 12.783544, val:  75.83%, val_best:  80.42%, tr:  90.09%, tr_best:  95.81%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  2.690229/ 13.678204, val:  69.58%, val_best:  80.42%, tr:  95.10%, tr_best:  95.81%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  2.305845/ 11.762297, val:  72.92%, val_best:  80.42%, tr:  95.40%, tr_best:  95.81%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  2.320054/ 12.838637, val:  72.92%, val_best:  80.42%, tr:  94.99%, tr_best:  95.81%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  3.702557/ 14.571051, val:  76.67%, val_best:  80.42%, tr:  92.13%, tr_best:  95.81%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  2.261293/ 12.362100, val:  80.00%, val_best:  80.42%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  1.913882/ 12.265964, val:  75.83%, val_best:  80.42%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  1.570438/ 12.260824, val:  73.75%, val_best:  80.42%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  2.198191/ 14.372191, val:  72.92%, val_best:  80.42%, tr:  95.91%, tr_best:  97.24%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  1.268910/ 11.046528, val:  81.25%, val_best:  81.25%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  1.765951/ 11.931877, val:  77.92%, val_best:  81.25%, tr:  97.24%, tr_best:  99.18%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  1.101592/ 11.367316, val:  77.92%, val_best:  81.25%, tr:  99.08%, tr_best:  99.18%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  1.095461/ 11.477208, val:  77.50%, val_best:  81.25%, tr:  98.88%, tr_best:  99.18%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.882902/ 10.893617, val:  77.08%, val_best:  81.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  1.359778/ 15.104761, val:  67.92%, val_best:  81.25%, tr:  97.04%, tr_best:  99.28%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  1.582675/ 11.392377, val:  80.83%, val_best:  81.25%, tr:  97.34%, tr_best:  99.28%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.865049/ 11.504857, val:  80.00%, val_best:  81.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.684509/ 11.301982, val:  80.42%, val_best:  81.25%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.541119/ 10.974893, val:  78.33%, val_best:  81.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  1.745633/ 11.276907, val:  79.17%, val_best:  81.25%, tr:  96.42%, tr_best:  99.59%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.731071/ 11.559541, val:  80.42%, val_best:  81.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.574709/ 11.165276, val:  80.83%, val_best:  81.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.463653/ 11.213685, val:  79.58%, val_best:  81.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.407403/ 11.738565, val:  78.75%, val_best:  81.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.432679/ 11.309032, val:  84.17%, val_best:  84.17%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.377989/ 10.819386, val:  82.92%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.339944/ 10.643229, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.317580/ 12.373881, val:  72.92%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.283208/ 10.373969, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.487229/ 10.815941, val:  82.92%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.304920/ 11.104212, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.262750/ 10.703748, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.228214/ 11.012471, val:  79.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.386904/ 10.798985, val:  79.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.310668/ 11.388371, val:  80.83%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.353023/ 11.255624, val:  80.42%, val_best:  84.17%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.215948/ 11.156787, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.218547/ 11.037035, val:  84.17%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.229962/ 10.969033, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.261332/ 10.903632, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.239169/ 11.074115, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.312213/ 11.551715, val:  80.83%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.178613/ 10.754430, val:  82.08%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.157501/ 11.019547, val:  81.67%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.155105/ 11.193538, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.189278/ 11.020918, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.117914/ 10.638303, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.100979/ 11.240775, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.146773/ 11.696136, val:  77.50%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.117451/ 10.870626, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.092828/ 10.823297, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.087439/ 10.966693, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.096435/ 10.766436, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.073077/ 11.283202, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.084175/ 11.452435, val:  79.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.131256/ 10.881020, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.092538/ 10.573771, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.100266/ 10.890233, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4676d99695e4d609c9f07c900075e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▂▄▂▃▆▃▅▇▇▇▇▅▇▇███▇████████████████████</td></tr><tr><td>summary_val_acc</td><td>▂▁▄▃▄▃▅▅▄▅▇▅▆▅▇▆▇▇▇█▇▇▇██▇▇███▇█████████</td></tr><tr><td>tr_acc</td><td>▁▂▃▃▄▄▄▄▅▅▆▆▆▇▇▇▇▇██████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▅▄▄▄▄▃▃▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▃▃▄▄▄▅▅▅▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>val_acc_now</td><td>▂▁▄▃▄▃▅▅▄▅▇▅▆▅▇▆▇▇▇█▇▇▇██▇▇███▇█████████</td></tr><tr><td>val_loss</td><td>█▃▁▃▃▂▃▂▃▂▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.10027</td></tr><tr><td>val_acc_best</td><td>0.84167</td></tr><tr><td>val_acc_now</td><td>0.80417</td></tr><tr><td>val_loss</td><td>10.89023</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">good-sweep-13</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wnwzce5t' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wnwzce5t</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_171848-wnwzce5t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 326jkxi9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_172438-326jkxi9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/326jkxi9' target=\"_blank\">trim-sweep-15</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/326jkxi9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/326jkxi9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 4, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = d133da00785cbf60fdc9e42f05c56a11\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.302859/  3.412908, val:  42.92%, val_best:  42.92%, tr:  33.71%, tr_best:  33.71%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.934278/  3.550709, val:  42.08%, val_best:  42.92%, tr:  48.21%, tr_best:  48.21%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  3.343939/  3.358360, val:  57.50%, val_best:  57.50%, tr:  52.81%, tr_best:  52.81%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.544603/  3.477968, val:  51.67%, val_best:  57.50%, tr:  61.90%, tr_best:  61.90%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.331729/  3.028629, val:  60.00%, val_best:  60.00%, tr:  59.86%, tr_best:  61.90%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.052053/  3.349042, val:  55.42%, val_best:  60.00%, tr:  63.02%, tr_best:  63.02%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.830607/  3.327863, val:  57.50%, val_best:  60.00%, tr:  65.68%, tr_best:  65.68%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.619918/  2.919942, val:  58.33%, val_best:  60.00%, tr:  67.42%, tr_best:  67.42%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.588235/  1.925941, val:  66.25%, val_best:  66.25%, tr:  69.87%, tr_best:  69.87%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.260744/  2.449797, val:  58.33%, val_best:  66.25%, tr:  67.11%, tr_best:  69.87%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.348828/  2.947439, val:  57.92%, val_best:  66.25%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.724347/  2.334833, val:  65.83%, val_best:  66.25%, tr:  73.14%, tr_best:  74.26%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.882436/  1.905767, val:  67.92%, val_best:  67.92%, tr:  72.52%, tr_best:  74.26%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.370643/  3.209620, val:  59.17%, val_best:  67.92%, tr:  75.49%, tr_best:  75.49%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.830113/  3.202546, val:  63.75%, val_best:  67.92%, tr:  76.00%, tr_best:  76.00%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  1.408875/  3.661679, val:  59.58%, val_best:  67.92%, tr:  79.16%, tr_best:  79.16%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  1.342881/  2.258662, val:  72.50%, val_best:  72.50%, tr:  80.80%, tr_best:  80.80%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.966214/  2.606873, val:  63.33%, val_best:  72.50%, tr:  86.21%, tr_best:  86.21%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.872597/  2.577256, val:  67.50%, val_best:  72.50%, tr:  85.70%, tr_best:  86.21%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.805932/  2.364895, val:  72.50%, val_best:  72.50%, tr:  89.68%, tr_best:  89.68%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  1.472451/  2.927283, val:  66.67%, val_best:  72.50%, tr:  82.94%, tr_best:  89.68%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  1.046630/  4.166499, val:  65.00%, val_best:  72.50%, tr:  86.31%, tr_best:  89.68%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.699598/  2.427138, val:  74.17%, val_best:  74.17%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.604698/  3.431576, val:  69.17%, val_best:  74.17%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.656644/  2.974936, val:  70.83%, val_best:  74.17%, tr:  93.97%, tr_best:  95.40%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.492734/  3.007691, val:  72.08%, val_best:  74.17%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.662961/  3.696569, val:  67.50%, val_best:  74.17%, tr:  93.67%, tr_best:  95.91%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.573945/  3.263856, val:  76.25%, val_best:  76.25%, tr:  94.48%, tr_best:  95.91%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.423524/  2.967980, val:  76.67%, val_best:  76.67%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.465054/  3.812960, val:  66.25%, val_best:  76.67%, tr:  96.63%, tr_best:  98.06%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.472600/  3.284055, val:  75.83%, val_best:  76.67%, tr:  96.73%, tr_best:  98.06%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.357979/  3.648817, val:  70.00%, val_best:  76.67%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.409212/  3.710295, val:  70.42%, val_best:  76.67%, tr:  97.24%, tr_best:  98.16%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.307308/  4.099652, val:  70.42%, val_best:  76.67%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.361041/  3.897605, val:  75.00%, val_best:  76.67%, tr:  98.88%, tr_best:  99.18%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.237145/  3.958374, val:  75.83%, val_best:  76.67%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.349545/  3.905884, val:  72.50%, val_best:  76.67%, tr:  98.16%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.248054/  4.246842, val:  72.08%, val_best:  76.67%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.248379/  4.042306, val:  75.83%, val_best:  76.67%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.263299/  4.115408, val:  74.58%, val_best:  76.67%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.188741/  4.087319, val:  76.25%, val_best:  76.67%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.196066/  3.986067, val:  77.50%, val_best:  77.50%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.119505/  4.307032, val:  76.67%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.133587/  4.681011, val:  73.75%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.146191/  4.480570, val:  75.42%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.093713/  4.298302, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.109351/  4.382546, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.078002/  4.431213, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.072809/  4.641692, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.087212/  4.726448, val:  76.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.088290/  4.715944, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.090198/  4.821241, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.069093/  5.003589, val:  74.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.077544/  5.093524, val:  75.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.079651/  5.011217, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.113688/  4.876976, val:  77.50%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.076426/  5.003674, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.063422/  4.936464, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.062893/  5.012346, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.057428/  5.091724, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.054230/  5.204537, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.046686/  5.326766, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.044977/  5.346367, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.044098/  5.415892, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.044390/  5.253315, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.045089/  5.368917, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.038295/  5.517083, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.045035/  5.300550, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.040472/  5.400847, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.034541/  5.559175, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.025244/  5.523454, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.031973/  5.546297, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.032010/  5.698973, val:  75.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.032101/  5.569327, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.023666/  5.516685, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.025433/  5.605686, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.023522/  5.654090, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.034045/  5.673500, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.024267/  5.795061, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.032075/  5.893211, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.029690/  5.723267, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.032634/  5.657419, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.036624/  5.722355, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.035356/  5.921021, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.022343/  5.703871, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.030453/  5.925690, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.029735/  5.718921, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.024645/  5.833827, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.033694/  5.928879, val:  76.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.027119/  5.884610, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.041863/  5.970672, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.024756/  5.891173, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.018066/  5.962744, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.037129/  5.864381, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.025937/  5.992049, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.017603/  5.866859, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.019779/  5.993594, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.021962/  5.816669, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.025253/  6.060416, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.022367/  5.926571, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745491e1d9224c20ac3b701ba33f6737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▅▁▅▂▅▅▅▆██▇████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▄▄▄▆▅▅▆▅▆▅▅▆▇▆▇▇▇█▇▇▇█▇▇▇██▇██████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▅▅▅▇▇▇▇▇████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▆█▆▄▆▅▅▃▃▃▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▄▄▄▆▅▅▆▅▆▅▅▆▇▆▇▇▇█▇▇▇█▇▇▇██▇██████████</td></tr><tr><td>val_loss</td><td>▄▄▃▃▂▁▃▂▂▅▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█▇█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.02237</td></tr><tr><td>val_acc_best</td><td>0.8125</td></tr><tr><td>val_acc_now</td><td>0.79583</td></tr><tr><td>val_loss</td><td>5.92657</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trim-sweep-15</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/326jkxi9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/326jkxi9</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_172438-326jkxi9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xhqko1yz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_173045-xhqko1yz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xhqko1yz' target=\"_blank\">kind-sweep-17</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xhqko1yz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xhqko1yz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  9.653196/ 10.097254, val:  36.67%, val_best:  36.67%, tr:  27.78%, tr_best:  27.78%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 13.500366/ 19.029167, val:  40.42%, val_best:  40.42%, tr:  39.22%, tr_best:  39.22%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 13.251966/ 18.831238, val:  38.75%, val_best:  40.42%, tr:  43.82%, tr_best:  43.82%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 10.096898/ 12.330011, val:  37.50%, val_best:  40.42%, tr:  54.44%, tr_best:  54.44%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 12.844937/ 10.707050, val:  57.50%, val_best:  57.50%, tr:  48.42%, tr_best:  54.44%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 11.747345/ 10.752083, val:  45.42%, val_best:  57.50%, tr:  52.60%, tr_best:  54.44%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 10.804217/ 10.838710, val:  52.08%, val_best:  57.50%, tr:  58.02%, tr_best:  58.02%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  9.687930/ 10.745437, val:  45.83%, val_best:  57.50%, tr:  58.84%, tr_best:  58.84%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  6.616214/  8.737102, val:  55.83%, val_best:  57.50%, tr:  61.29%, tr_best:  61.29%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  6.854677/ 13.564594, val:  51.25%, val_best:  57.50%, tr:  59.24%, tr_best:  61.29%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 11.667123/ 11.574658, val:  56.25%, val_best:  57.50%, tr:  57.20%, tr_best:  61.29%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  6.688242/ 16.621876, val:  50.83%, val_best:  57.50%, tr:  65.07%, tr_best:  65.07%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  9.298225/  7.598619, val:  55.83%, val_best:  57.50%, tr:  62.21%, tr_best:  65.07%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  7.955818/ 11.789598, val:  55.00%, val_best:  57.50%, tr:  63.13%, tr_best:  65.07%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  7.568067/ 23.520906, val:  55.00%, val_best:  57.50%, tr:  67.11%, tr_best:  67.11%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  9.104874/  8.831749, val:  57.92%, val_best:  57.92%, tr:  67.72%, tr_best:  67.72%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  8.505612/ 10.450143, val:  61.67%, val_best:  61.67%, tr:  66.09%, tr_best:  67.72%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  8.737367/ 14.046819, val:  61.67%, val_best:  61.67%, tr:  67.01%, tr_best:  67.72%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  7.883191/ 11.304525, val:  57.08%, val_best:  61.67%, tr:  67.93%, tr_best:  67.93%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  6.109890/ 13.016354, val:  55.42%, val_best:  61.67%, tr:  72.73%, tr_best:  72.73%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  8.012065/ 14.626973, val:  55.42%, val_best:  61.67%, tr:  72.01%, tr_best:  72.73%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  9.176218/ 21.189840, val:  47.92%, val_best:  61.67%, tr:  73.03%, tr_best:  73.03%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  6.848104/ 10.231169, val:  62.08%, val_best:  62.08%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  5.939760/ 12.651459, val:  63.75%, val_best:  63.75%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  3.773758/  9.083807, val:  68.33%, val_best:  68.33%, tr:  86.41%, tr_best:  86.41%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  3.540644/ 11.908066, val:  60.00%, val_best:  68.33%, tr:  85.09%, tr_best:  86.41%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  5.072011/ 13.631254, val:  62.92%, val_best:  68.33%, tr:  79.88%, tr_best:  86.41%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  4.181870/  9.727742, val:  67.92%, val_best:  68.33%, tr:  85.19%, tr_best:  86.41%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  4.943829/ 10.343627, val:  67.92%, val_best:  68.33%, tr:  83.04%, tr_best:  86.41%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  2.948871/ 10.418182, val:  64.17%, val_best:  68.33%, tr:  91.01%, tr_best:  91.01%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  3.744893/  9.243084, val:  72.92%, val_best:  72.92%, tr:  88.05%, tr_best:  91.01%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  2.997657/ 10.432480, val:  72.50%, val_best:  72.92%, tr:  91.01%, tr_best:  91.01%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  3.736182/ 11.376549, val:  67.92%, val_best:  72.92%, tr:  89.48%, tr_best:  91.01%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  3.951106/ 11.072464, val:  74.17%, val_best:  74.17%, tr:  89.79%, tr_best:  91.01%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  3.729919/ 11.875252, val:  72.08%, val_best:  74.17%, tr:  90.70%, tr_best:  91.01%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  2.215051/ 10.706600, val:  72.92%, val_best:  74.17%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  1.899392/ 10.593225, val:  75.83%, val_best:  75.83%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  3.073712/ 11.597697, val:  73.33%, val_best:  75.83%, tr:  91.32%, tr_best:  96.42%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  2.649398/ 12.288821, val:  70.42%, val_best:  75.83%, tr:  93.77%, tr_best:  96.42%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  2.252382/  9.992141, val:  81.25%, val_best:  81.25%, tr:  95.81%, tr_best:  96.42%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  1.967542/ 12.077719, val:  72.50%, val_best:  81.25%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  2.279192/ 11.524372, val:  76.67%, val_best:  81.25%, tr:  95.71%, tr_best:  96.42%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  1.650298/ 11.542340, val:  79.17%, val_best:  81.25%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  1.558964/ 12.879250, val:  73.33%, val_best:  81.25%, tr:  97.55%, tr_best:  97.75%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  1.675619/ 11.835859, val:  77.08%, val_best:  81.25%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  1.485490/ 10.884000, val:  80.42%, val_best:  81.25%, tr:  97.55%, tr_best:  98.06%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  1.100401/ 11.107017, val:  78.75%, val_best:  81.25%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  1.046814/ 10.986785, val:  77.50%, val_best:  81.25%, tr:  98.57%, tr_best:  99.08%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  1.374043/ 11.403273, val:  77.50%, val_best:  81.25%, tr:  98.37%, tr_best:  99.08%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  1.300975/ 11.769787, val:  78.75%, val_best:  81.25%, tr:  98.77%, tr_best:  99.08%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  1.033007/ 11.468337, val:  78.75%, val_best:  81.25%, tr:  98.88%, tr_best:  99.08%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  1.276465/ 12.208409, val:  78.75%, val_best:  81.25%, tr:  98.06%, tr_best:  99.08%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  1.284381/ 12.564207, val:  76.25%, val_best:  81.25%, tr:  98.37%, tr_best:  99.08%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  1.246612/ 11.975701, val:  78.75%, val_best:  81.25%, tr:  98.77%, tr_best:  99.08%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.813868/ 12.128924, val:  77.50%, val_best:  81.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.766216/ 11.463889, val:  83.33%, val_best:  83.33%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.606509/ 12.916757, val:  77.08%, val_best:  83.33%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.699726/ 11.420275, val:  78.75%, val_best:  83.33%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.616189/ 12.316587, val:  77.08%, val_best:  83.33%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.479696/ 12.225976, val:  82.08%, val_best:  83.33%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.544689/ 11.693621, val:  81.25%, val_best:  83.33%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  1.034104/ 12.515895, val:  84.17%, val_best:  84.17%, tr:  97.55%, tr_best:  99.90%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.685849/ 13.535407, val:  77.08%, val_best:  84.17%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.411147/ 12.401801, val:  81.67%, val_best:  84.17%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.555963/ 11.869786, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.451362/ 12.969174, val:  79.58%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.386187/ 12.750044, val:  80.83%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.311403/ 12.225190, val:  84.58%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.344457/ 12.533840, val:  82.50%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.299074/ 13.248019, val:  78.75%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.274051/ 12.727768, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.271420/ 12.869834, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.318471/ 12.970343, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.287950/ 13.301082, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.298091/ 13.387002, val:  84.17%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.208035/ 13.383734, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.237871/ 14.123357, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.179967/ 14.033495, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.225121/ 13.369085, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.271864/ 12.641840, val:  83.75%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.228743/ 13.915012, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.233050/ 13.337573, val:  83.33%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.172063/ 13.554894, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.176350/ 13.873741, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.162547/ 13.897169, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.161062/ 13.402309, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.115616/ 13.941490, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.163825/ 13.222014, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.160323/ 13.763910, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.204032/ 14.357250, val:  80.00%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.192405/ 14.752494, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.166992/ 13.900298, val:  80.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.088670/ 13.641619, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.118950/ 13.667482, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.377345/ 13.623072, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.175663/ 14.405427, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.144682/ 15.308968, val:  79.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.227335/ 14.638730, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.141051/ 14.152225, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.115073/ 14.341102, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dc9db548034c5d874b29457277d690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▃▅▂▄▅▅▄▆▇▆█▇▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▄▂▃▄▄▅▄▃▆▅▅▆▆▆█▇▇▇▇▇▇▇██▇████▇████▇███</td></tr><tr><td>tr_acc</td><td>▁▃▃▄▄▄▅▅▅▅▇▆▇▇█▇████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▆██▆▅▆▅▆▄▆▃▄▃▃▂▃▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▄▂▃▄▄▅▄▃▆▅▅▆▆▆█▇▇▇▇▇▇▇██▇████▇████▇███</td></tr><tr><td>val_loss</td><td>▂▆▂▂▄▁█▄▃▇▂▄▂▃▂▃▂▃▃▂▃▃▃▃▃▃▃▃▃▃▄▄▃▄▄▃▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.11507</td></tr><tr><td>val_acc_best</td><td>0.85833</td></tr><tr><td>val_acc_now</td><td>0.81667</td></tr><tr><td>val_loss</td><td>14.3411</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">kind-sweep-17</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xhqko1yz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xhqko1yz</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_173045-xhqko1yz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: obuy3izw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_173711-obuy3izw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/obuy3izw' target=\"_blank\">deft-sweep-19</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/bwu1v6sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/obuy3izw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/obuy3izw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = b57dfc72d05d304ce829f424a70e455b\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.325213/  2.309355, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.311454/  2.228657, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  10.32%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.739595/  1.473186, val:  55.83%, val_best:  55.83%, tr:  38.30%, tr_best:  38.30%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.247396/  1.397947, val:  58.33%, val_best:  58.33%, tr:  61.59%, tr_best:  61.59%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.141421/  1.240973, val:  65.42%, val_best:  65.42%, tr:  62.41%, tr_best:  62.41%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.054635/  1.236996, val:  62.92%, val_best:  65.42%, tr:  66.39%, tr_best:  66.39%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.999431/  1.272384, val:  60.42%, val_best:  65.42%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.019566/  1.231767, val:  57.08%, val_best:  65.42%, tr:  65.27%, tr_best:  69.36%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.949083/  1.212868, val:  61.25%, val_best:  65.42%, tr:  70.79%, tr_best:  70.79%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.903196/  1.313779, val:  61.25%, val_best:  65.42%, tr:  73.44%, tr_best:  73.44%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.930804/  1.301757, val:  63.33%, val_best:  65.42%, tr:  71.60%, tr_best:  73.44%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.897585/  1.207894, val:  65.00%, val_best:  65.42%, tr:  72.32%, tr_best:  73.44%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.849210/  1.177624, val:  72.50%, val_best:  72.50%, tr:  75.59%, tr_best:  75.59%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.839344/  1.203533, val:  69.17%, val_best:  72.50%, tr:  77.83%, tr_best:  77.83%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.799645/  1.377332, val:  67.50%, val_best:  72.50%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.770307/  1.226705, val:  67.92%, val_best:  72.50%, tr:  81.51%, tr_best:  81.51%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.746914/  1.229360, val:  65.83%, val_best:  72.50%, tr:  82.64%, tr_best:  82.64%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.706755/  1.229360, val:  75.00%, val_best:  75.00%, tr:  87.33%, tr_best:  87.33%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.706907/  1.301111, val:  70.83%, val_best:  75.00%, tr:  85.60%, tr_best:  87.33%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.659280/  1.306475, val:  67.08%, val_best:  75.00%, tr:  87.95%, tr_best:  87.95%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.641333/  1.282955, val:  68.33%, val_best:  75.00%, tr:  88.05%, tr_best:  88.05%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.588022/  1.401309, val:  72.50%, val_best:  75.00%, tr:  91.01%, tr_best:  91.01%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.552848/  1.355279, val:  69.17%, val_best:  75.00%, tr:  93.46%, tr_best:  93.46%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.555915/  1.276842, val:  74.58%, val_best:  75.00%, tr:  93.05%, tr_best:  93.46%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.481431/  1.360800, val:  73.33%, val_best:  75.00%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.464557/  1.322540, val:  73.33%, val_best:  75.00%, tr:  96.32%, tr_best:  97.55%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.501920/  1.322080, val:  75.42%, val_best:  75.42%, tr:  93.77%, tr_best:  97.55%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.442563/  1.371041, val:  76.25%, val_best:  76.25%, tr:  96.32%, tr_best:  97.55%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.406191/  1.361892, val:  80.42%, val_best:  80.42%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.358996/  1.420671, val:  77.92%, val_best:  80.42%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.336345/  1.462152, val:  75.42%, val_best:  80.42%, tr:  98.47%, tr_best:  98.98%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.349780/  1.469366, val:  77.50%, val_best:  80.42%, tr:  98.47%, tr_best:  98.98%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.380411/  1.515996, val:  73.75%, val_best:  80.42%, tr:  96.42%, tr_best:  98.98%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.330683/  1.561583, val:  73.33%, val_best:  80.42%, tr:  98.26%, tr_best:  98.98%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.299966/  1.580926, val:  74.17%, val_best:  80.42%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.267480/  1.586757, val:  76.25%, val_best:  80.42%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.258263/  1.613869, val:  77.92%, val_best:  80.42%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.234935/  1.707664, val:  72.08%, val_best:  80.42%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.246771/  1.635147, val:  79.58%, val_best:  80.42%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.223261/  1.686643, val:  77.08%, val_best:  80.42%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.207724/  1.678595, val:  78.75%, val_best:  80.42%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.209221/  1.737066, val:  78.75%, val_best:  80.42%, tr:  99.08%, tr_best:  99.80%\n"
     ]
    }
   ],
   "source": [
    "# sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [16]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.0,0.125,0.25,0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.25, 0.5, 0.75, 1.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0, 0.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [3.0,4.0,5.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        # \"synapse_trace_const2\": {\"values\": [0, 0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"net_save/save_now_net_weights_{unique_name}.pth\"]},\n",
    "        \"learning_rate\": {\"values\": [0.01,0.1]}, \n",
    "        \"epoch_num\": {\"values\": [100]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [1,2,3,4,5,6,7]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [True, False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [True,False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [True]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [0]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [True]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"2\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  unique_name_hyper,\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.lif_layer_v_decay, #wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "                        ) \n",
    "    # sigmoid와 BN이 있어야 잘된다.\n",
    "    # average pooling\n",
    "    # 이 낫다. \n",
    "    \n",
    "    # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'bwu1v6sg'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
