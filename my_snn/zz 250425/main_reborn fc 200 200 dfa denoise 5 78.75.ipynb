{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12689/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on:\n",
    "        assert DFA_on and single_step\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #     criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        i = 0\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                # network save\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20250425_142654-2jzyzqz8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2jzyzqz8' target=\"_blank\">balmy-bush-6791</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2jzyzqz8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2jzyzqz8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0.0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4.0, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_main.pth', 'learning_rate': 0.001, 'epoch_num': 10000, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1.0, 'bias': True, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4.0, surrogate=sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4.0, surrogate=sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (10): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4.0, surrogate=sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.287728/  2.156547, tr:  12.05%, tr_best:  12.05%, val:  25.83%, val_best:  25.83%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  1.959872/  1.950755, tr:  40.04%, tr_best:  40.04%, val:  43.75%, val_best:  43.75%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  1.860401/  1.881659, tr:  50.36%, tr_best:  50.36%, val:  51.67%, val_best:  51.67%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  1.792504/  1.885791, tr:  57.71%, tr_best:  57.71%, val:  48.33%, val_best:  51.67%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  1.763181/  1.863410, tr:  56.69%, tr_best:  57.71%, val:  49.58%, val_best:  51.67%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  1.738529/  1.821278, tr:  57.30%, tr_best:  57.71%, val:  62.08%, val_best:  62.08%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  1.724807/  1.816233, tr:  63.94%, tr_best:  63.94%, val:  60.42%, val_best:  62.08%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  1.713379/  1.809385, tr:  65.88%, tr_best:  65.88%, val:  58.75%, val_best:  62.08%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  1.699292/  1.794253, tr:  67.52%, tr_best:  67.52%, val:  61.67%, val_best:  62.08%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  1.689666/  1.787403, tr:  70.68%, tr_best:  70.68%, val:  61.67%, val_best:  62.08%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  1.683886/  1.791122, tr:  71.30%, tr_best:  71.30%, val:  62.08%, val_best:  62.08%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  1.678991/  1.791404, tr:  69.97%, tr_best:  71.30%, val:  58.75%, val_best:  62.08%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  1.678376/  1.787105, tr:  70.99%, tr_best:  71.30%, val:  60.00%, val_best:  62.08%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  1.668955/  1.782750, tr:  72.11%, tr_best:  72.11%, val:  63.33%, val_best:  63.33%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  1.661069/  1.784368, tr:  72.52%, tr_best:  72.52%, val:  63.33%, val_best:  63.33%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  1.664941/  1.781170, tr:  70.17%, tr_best:  72.52%, val:  60.42%, val_best:  63.33%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  1.655966/  1.786973, tr:  72.22%, tr_best:  72.52%, val:  58.33%, val_best:  63.33%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  1.653412/  1.781096, tr:  72.52%, tr_best:  72.52%, val:  58.33%, val_best:  63.33%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  1.651532/  1.778401, tr:  72.22%, tr_best:  72.52%, val:  59.58%, val_best:  63.33%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  1.644897/  1.780793, tr:  71.60%, tr_best:  72.52%, val:  59.17%, val_best:  63.33%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  1.645617/  1.772897, tr:  72.52%, tr_best:  72.52%, val:  60.42%, val_best:  63.33%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  1.639767/  1.769139, tr:  73.34%, tr_best:  73.34%, val:  64.17%, val_best:  64.17%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  1.637249/  1.777816, tr:  73.34%, tr_best:  73.34%, val:  58.33%, val_best:  64.17%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  1.634581/  1.789214, tr:  73.95%, tr_best:  73.95%, val:  59.17%, val_best:  64.17%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  1.632391/  1.767772, tr:  74.87%, tr_best:  74.87%, val:  63.75%, val_best:  64.17%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  1.630273/  1.771381, tr:  73.95%, tr_best:  74.87%, val:  63.33%, val_best:  64.17%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  1.626408/  1.776612, tr:  75.69%, tr_best:  75.69%, val:  60.42%, val_best:  64.17%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  1.624037/  1.786798, tr:  75.28%, tr_best:  75.69%, val:  57.92%, val_best:  64.17%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  1.625958/  1.765324, tr:  75.59%, tr_best:  75.69%, val:  63.33%, val_best:  64.17%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  1.623780/  1.773958, tr:  75.49%, tr_best:  75.69%, val:  61.25%, val_best:  64.17%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  1.620286/  1.768294, tr:  75.59%, tr_best:  75.69%, val:  63.75%, val_best:  64.17%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  1.618595/  1.774316, tr:  74.67%, tr_best:  75.69%, val:  61.67%, val_best:  64.17%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  1.615804/  1.774374, tr:  75.18%, tr_best:  75.69%, val:  63.75%, val_best:  64.17%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  1.614794/  1.776078, tr:  76.40%, tr_best:  76.40%, val:  61.25%, val_best:  64.17%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  1.612337/  1.769477, tr:  76.71%, tr_best:  76.71%, val:  63.33%, val_best:  64.17%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  1.609374/  1.782027, tr:  77.12%, tr_best:  77.12%, val:  62.08%, val_best:  64.17%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  1.607596/  1.778895, tr:  77.22%, tr_best:  77.22%, val:  62.08%, val_best:  64.17%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  1.606313/  1.775218, tr:  76.71%, tr_best:  77.22%, val:  61.67%, val_best:  64.17%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  1.606015/  1.770004, tr:  77.02%, tr_best:  77.22%, val:  65.00%, val_best:  65.00%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  1.604795/  1.769518, tr:  77.63%, tr_best:  77.63%, val:  61.67%, val_best:  65.00%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  1.600799/  1.767851, tr:  78.55%, tr_best:  78.55%, val:  64.58%, val_best:  65.00%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  1.603724/  1.768834, tr:  77.83%, tr_best:  78.55%, val:  61.67%, val_best:  65.00%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  1.597131/  1.772357, tr:  77.63%, tr_best:  78.55%, val:  63.33%, val_best:  65.00%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  1.596755/  1.774662, tr:  78.55%, tr_best:  78.55%, val:  62.92%, val_best:  65.00%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  1.597225/  1.775278, tr:  77.53%, tr_best:  78.55%, val:  64.17%, val_best:  65.00%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  1.593374/  1.766201, tr:  77.83%, tr_best:  78.55%, val:  65.00%, val_best:  65.00%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  1.592887/  1.767412, tr:  77.53%, tr_best:  78.55%, val:  64.17%, val_best:  65.00%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  1.593297/  1.770579, tr:  78.35%, tr_best:  78.55%, val:  61.25%, val_best:  65.00%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  1.592491/  1.767272, tr:  78.24%, tr_best:  78.55%, val:  62.92%, val_best:  65.00%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  1.589769/  1.772027, tr:  78.45%, tr_best:  78.55%, val:  62.50%, val_best:  65.00%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  1.588477/  1.770607, tr:  78.35%, tr_best:  78.55%, val:  61.25%, val_best:  65.00%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  1.587006/  1.773346, tr:  78.86%, tr_best:  78.86%, val:  62.50%, val_best:  65.00%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  1.587268/  1.768478, tr:  78.75%, tr_best:  78.86%, val:  62.50%, val_best:  65.00%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  1.585775/  1.768267, tr:  78.14%, tr_best:  78.86%, val:  62.08%, val_best:  65.00%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  1.586090/  1.770373, tr:  78.45%, tr_best:  78.86%, val:  63.75%, val_best:  65.00%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  1.585343/  1.772916, tr:  78.86%, tr_best:  78.86%, val:  63.33%, val_best:  65.00%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  1.581986/  1.767482, tr:  78.55%, tr_best:  78.86%, val:  64.58%, val_best:  65.00%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  1.581217/  1.766307, tr:  78.86%, tr_best:  78.86%, val:  65.83%, val_best:  65.83%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  1.578086/  1.771987, tr:  78.96%, tr_best:  78.96%, val:  63.75%, val_best:  65.83%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  1.578226/  1.772905, tr:  78.86%, tr_best:  78.96%, val:  63.75%, val_best:  65.83%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  1.572750/  1.772355, tr:  79.98%, tr_best:  79.98%, val:  62.08%, val_best:  65.83%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  1.570289/  1.782276, tr:  79.57%, tr_best:  79.98%, val:  62.08%, val_best:  65.83%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  1.568156/  1.774040, tr:  79.78%, tr_best:  79.98%, val:  62.92%, val_best:  65.83%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  1.566431/  1.770568, tr:  79.88%, tr_best:  79.98%, val:  63.75%, val_best:  65.83%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  1.567565/  1.778929, tr:  79.67%, tr_best:  79.98%, val:  63.33%, val_best:  65.83%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  1.566510/  1.775067, tr:  80.08%, tr_best:  80.08%, val:  62.92%, val_best:  65.83%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  1.564718/  1.768403, tr:  79.88%, tr_best:  80.08%, val:  63.33%, val_best:  65.83%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  1.567300/  1.770527, tr:  80.08%, tr_best:  80.08%, val:  65.00%, val_best:  65.83%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  1.562653/  1.770901, tr:  80.80%, tr_best:  80.80%, val:  63.75%, val_best:  65.83%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  1.561617/  1.774419, tr:  80.59%, tr_best:  80.80%, val:  63.33%, val_best:  65.83%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  1.561259/  1.778476, tr:  79.78%, tr_best:  80.80%, val:  63.33%, val_best:  65.83%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  1.562358/  1.780221, tr:  79.98%, tr_best:  80.80%, val:  61.25%, val_best:  65.83%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  1.558547/  1.783985, tr:  80.59%, tr_best:  80.80%, val:  62.92%, val_best:  65.83%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  1.557832/  1.770844, tr:  81.31%, tr_best:  81.31%, val:  65.83%, val_best:  65.83%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  1.557944/  1.771610, tr:  81.21%, tr_best:  81.31%, val:  64.58%, val_best:  65.83%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  1.556036/  1.772482, tr:  81.21%, tr_best:  81.31%, val:  64.17%, val_best:  65.83%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  1.554975/  1.781501, tr:  81.61%, tr_best:  81.61%, val:  60.83%, val_best:  65.83%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  1.555582/  1.774163, tr:  81.82%, tr_best:  81.82%, val:  65.00%, val_best:  65.83%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  1.554625/  1.773826, tr:  81.82%, tr_best:  81.82%, val:  64.17%, val_best:  65.83%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  1.554018/  1.771705, tr:  82.02%, tr_best:  82.02%, val:  63.33%, val_best:  65.83%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  1.551761/  1.773787, tr:  81.82%, tr_best:  82.02%, val:  65.00%, val_best:  65.83%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  1.552553/  1.769692, tr:  82.33%, tr_best:  82.33%, val:  65.83%, val_best:  65.83%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  1.550922/  1.773116, tr:  82.12%, tr_best:  82.33%, val:  63.75%, val_best:  65.83%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  1.551669/  1.773408, tr:  82.23%, tr_best:  82.33%, val:  64.58%, val_best:  65.83%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  1.549384/  1.775882, tr:  82.64%, tr_best:  82.64%, val:  63.75%, val_best:  65.83%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  1.550758/  1.774661, tr:  82.53%, tr_best:  82.64%, val:  65.42%, val_best:  65.83%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  1.548228/  1.775448, tr:  82.23%, tr_best:  82.64%, val:  65.00%, val_best:  65.83%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  1.548614/  1.772656, tr:  83.15%, tr_best:  83.15%, val:  67.08%, val_best:  67.08%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  1.546062/  1.775163, tr:  82.84%, tr_best:  83.15%, val:  65.00%, val_best:  67.08%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  1.545661/  1.776047, tr:  83.96%, tr_best:  83.96%, val:  65.42%, val_best:  67.08%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  1.547014/  1.775849, tr:  83.86%, tr_best:  83.96%, val:  65.00%, val_best:  67.08%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  1.543620/  1.774733, tr:  83.15%, tr_best:  83.96%, val:  65.83%, val_best:  67.08%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  1.543469/  1.771316, tr:  83.86%, tr_best:  83.96%, val:  66.67%, val_best:  67.08%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  1.544312/  1.771056, tr:  84.17%, tr_best:  84.17%, val:  67.08%, val_best:  67.08%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  1.541810/  1.773192, tr:  85.39%, tr_best:  85.39%, val:  67.92%, val_best:  67.92%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  1.541622/  1.776981, tr:  84.58%, tr_best:  85.39%, val:  66.25%, val_best:  67.92%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  1.541196/  1.775529, tr:  84.37%, tr_best:  85.39%, val:  67.50%, val_best:  67.92%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  1.540441/  1.775647, tr:  84.98%, tr_best:  85.39%, val:  66.25%, val_best:  67.92%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  1.540910/  1.774216, tr:  85.39%, tr_best:  85.39%, val:  70.00%, val_best:  70.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  1.539256/  1.776739, tr:  86.11%, tr_best:  86.11%, val:  66.67%, val_best:  70.00%\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  1.538229/  1.767939, tr:  86.21%, tr_best:  86.21%, val:  69.58%, val_best:  70.00%\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  1.536245/  1.771226, tr:  86.41%, tr_best:  86.41%, val:  67.92%, val_best:  70.00%\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  1.536896/  1.780917, tr:  87.03%, tr_best:  87.03%, val:  68.75%, val_best:  70.00%\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  1.537135/  1.775218, tr:  87.74%, tr_best:  87.74%, val:  68.75%, val_best:  70.00%\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  1.535661/  1.777015, tr:  88.56%, tr_best:  88.56%, val:  70.00%, val_best:  70.00%\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  1.535794/  1.773596, tr:  89.58%, tr_best:  89.58%, val:  70.42%, val_best:  70.42%\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  1.528985/  1.773506, tr:  93.05%, tr_best:  93.05%, val:  74.17%, val_best:  74.17%\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  1.524247/  1.771463, tr:  94.38%, tr_best:  94.38%, val:  72.92%, val_best:  74.17%\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  1.521773/  1.772685, tr:  94.79%, tr_best:  94.79%, val:  75.00%, val_best:  75.00%\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  1.522087/  1.775082, tr:  95.20%, tr_best:  95.20%, val:  73.33%, val_best:  75.00%\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  1.519918/  1.774596, tr:  96.63%, tr_best:  96.63%, val:  72.92%, val_best:  75.00%\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  1.518899/  1.770398, tr:  97.45%, tr_best:  97.45%, val:  74.17%, val_best:  75.00%\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  1.517581/  1.776288, tr:  97.34%, tr_best:  97.45%, val:  73.33%, val_best:  75.00%\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  1.515988/  1.776256, tr:  97.45%, tr_best:  97.45%, val:  73.33%, val_best:  75.00%\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  1.514335/  1.781390, tr:  97.04%, tr_best:  97.45%, val:  72.50%, val_best:  75.00%\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  1.511762/  1.778580, tr:  97.24%, tr_best:  97.45%, val:  73.75%, val_best:  75.00%\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  1.509888/  1.779510, tr:  97.55%, tr_best:  97.55%, val:  74.17%, val_best:  75.00%\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  1.509639/  1.778661, tr:  97.24%, tr_best:  97.55%, val:  74.17%, val_best:  75.00%\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  1.509330/  1.779005, tr:  97.65%, tr_best:  97.65%, val:  73.75%, val_best:  75.00%\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  1.509847/  1.778021, tr:  97.75%, tr_best:  97.75%, val:  73.75%, val_best:  75.00%\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  1.508068/  1.781927, tr:  97.96%, tr_best:  97.96%, val:  73.33%, val_best:  75.00%\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  1.508042/  1.778930, tr:  98.16%, tr_best:  98.16%, val:  72.50%, val_best:  75.00%\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  1.507511/  1.778439, tr:  97.75%, tr_best:  98.16%, val:  74.17%, val_best:  75.00%\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  1.506925/  1.783635, tr:  98.26%, tr_best:  98.26%, val:  72.08%, val_best:  75.00%\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  1.505443/  1.772900, tr:  98.16%, tr_best:  98.26%, val:  75.42%, val_best:  75.42%\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  1.505685/  1.781925, tr:  98.26%, tr_best:  98.26%, val:  72.08%, val_best:  75.42%\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  1.507692/  1.785120, tr:  98.57%, tr_best:  98.57%, val:  72.92%, val_best:  75.42%\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  1.505481/  1.780336, tr:  98.37%, tr_best:  98.57%, val:  73.75%, val_best:  75.42%\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  1.504161/  1.780419, tr:  98.47%, tr_best:  98.57%, val:  72.08%, val_best:  75.42%\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  1.504469/  1.782182, tr:  98.26%, tr_best:  98.57%, val:  73.33%, val_best:  75.42%\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  1.504078/  1.775330, tr:  98.47%, tr_best:  98.57%, val:  74.17%, val_best:  75.42%\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  1.502737/  1.779930, tr:  98.47%, tr_best:  98.57%, val:  72.92%, val_best:  75.42%\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  1.502093/  1.782211, tr:  98.47%, tr_best:  98.57%, val:  75.00%, val_best:  75.42%\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  1.502534/  1.777642, tr:  98.47%, tr_best:  98.57%, val:  73.75%, val_best:  75.42%\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  1.501912/  1.776356, tr:  98.67%, tr_best:  98.67%, val:  74.58%, val_best:  75.42%\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  1.502119/  1.777380, tr:  98.47%, tr_best:  98.67%, val:  73.33%, val_best:  75.42%\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  1.502321/  1.781504, tr:  98.47%, tr_best:  98.67%, val:  74.17%, val_best:  75.42%\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  1.501692/  1.779708, tr:  98.26%, tr_best:  98.67%, val:  71.25%, val_best:  75.42%\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  1.500405/  1.780565, tr:  98.67%, tr_best:  98.67%, val:  72.92%, val_best:  75.42%\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  1.499739/  1.780161, tr:  98.67%, tr_best:  98.67%, val:  72.50%, val_best:  75.42%\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  1.499448/  1.787237, tr:  98.37%, tr_best:  98.67%, val:  73.75%, val_best:  75.42%\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  1.499623/  1.783676, tr:  98.26%, tr_best:  98.67%, val:  73.75%, val_best:  75.42%\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  1.499298/  1.782046, tr:  98.37%, tr_best:  98.67%, val:  74.17%, val_best:  75.42%\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  1.499724/  1.780792, tr:  98.47%, tr_best:  98.67%, val:  74.17%, val_best:  75.42%\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  1.499735/  1.780600, tr:  98.47%, tr_best:  98.67%, val:  73.33%, val_best:  75.42%\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  1.497698/  1.783562, tr:  98.57%, tr_best:  98.67%, val:  74.58%, val_best:  75.42%\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  1.497969/  1.784501, tr:  98.77%, tr_best:  98.77%, val:  74.58%, val_best:  75.42%\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  1.498441/  1.781653, tr:  98.67%, tr_best:  98.77%, val:  72.92%, val_best:  75.42%\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  1.496939/  1.783664, tr:  98.77%, tr_best:  98.77%, val:  74.17%, val_best:  75.42%\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  1.496724/  1.784008, tr:  98.47%, tr_best:  98.77%, val:  74.17%, val_best:  75.42%\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  1.496637/  1.788260, tr:  98.77%, tr_best:  98.77%, val:  74.17%, val_best:  75.42%\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  1.495807/  1.781991, tr:  98.88%, tr_best:  98.88%, val:  72.08%, val_best:  75.42%\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  1.495842/  1.782375, tr:  98.67%, tr_best:  98.88%, val:  72.92%, val_best:  75.42%\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  1.495368/  1.782666, tr:  98.77%, tr_best:  98.88%, val:  73.75%, val_best:  75.42%\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  1.495758/  1.783834, tr:  98.67%, tr_best:  98.88%, val:  73.33%, val_best:  75.42%\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  1.494866/  1.785602, tr:  98.98%, tr_best:  98.98%, val:  73.33%, val_best:  75.42%\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  1.495266/  1.789941, tr:  98.67%, tr_best:  98.98%, val:  72.92%, val_best:  75.42%\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  1.494163/  1.788771, tr:  99.08%, tr_best:  99.08%, val:  73.33%, val_best:  75.42%\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  1.494768/  1.787087, tr:  98.98%, tr_best:  99.08%, val:  72.50%, val_best:  75.42%\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  1.493149/  1.788080, tr:  98.98%, tr_best:  99.08%, val:  73.75%, val_best:  75.42%\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  1.492240/  1.786558, tr:  98.98%, tr_best:  99.08%, val:  74.17%, val_best:  75.42%\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  1.492150/  1.788219, tr:  99.08%, tr_best:  99.08%, val:  75.00%, val_best:  75.42%\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  1.491036/  1.791970, tr:  99.08%, tr_best:  99.08%, val:  74.58%, val_best:  75.42%\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  1.489997/  1.791188, tr:  99.18%, tr_best:  99.18%, val:  75.83%, val_best:  75.83%\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  1.488869/  1.796434, tr:  99.39%, tr_best:  99.39%, val:  74.17%, val_best:  75.83%\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  1.487241/  1.791996, tr:  99.28%, tr_best:  99.39%, val:  75.83%, val_best:  75.83%\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  1.483917/  1.797011, tr:  99.49%, tr_best:  99.49%, val:  74.17%, val_best:  75.83%\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  1.483271/  1.799462, tr:  99.49%, tr_best:  99.49%, val:  75.83%, val_best:  75.83%\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  1.482342/  1.798328, tr:  99.49%, tr_best:  99.49%, val:  75.00%, val_best:  75.83%\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  1.481404/  1.796823, tr:  99.49%, tr_best:  99.49%, val:  74.17%, val_best:  75.83%\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  1.480768/  1.803914, tr:  99.49%, tr_best:  99.49%, val:  74.17%, val_best:  75.83%\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  1.481172/  1.798512, tr:  99.49%, tr_best:  99.49%, val:  75.83%, val_best:  75.83%\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  1.479898/  1.800271, tr:  99.49%, tr_best:  99.49%, val:  75.83%, val_best:  75.83%\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  1.480082/  1.800544, tr:  99.49%, tr_best:  99.49%, val:  74.58%, val_best:  75.83%\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  1.479403/  1.802711, tr:  99.49%, tr_best:  99.49%, val:  75.00%, val_best:  75.83%\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  1.479479/  1.797290, tr:  99.49%, tr_best:  99.49%, val:  76.25%, val_best:  76.25%\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  1.478950/  1.800225, tr:  99.49%, tr_best:  99.49%, val:  75.42%, val_best:  76.25%\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  1.480608/  1.796426, tr:  99.49%, tr_best:  99.49%, val:  75.42%, val_best:  76.25%\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  1.479130/  1.800877, tr:  99.39%, tr_best:  99.49%, val:  75.83%, val_best:  76.25%\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  1.478297/  1.802111, tr:  99.49%, tr_best:  99.49%, val:  75.42%, val_best:  76.25%\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  1.478100/  1.799420, tr:  99.49%, tr_best:  99.49%, val:  74.58%, val_best:  76.25%\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  1.477849/  1.803048, tr:  99.49%, tr_best:  99.49%, val:  77.50%, val_best:  77.50%\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  1.477855/  1.798276, tr:  99.39%, tr_best:  99.49%, val:  75.83%, val_best:  77.50%\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  1.476808/  1.803109, tr:  99.49%, tr_best:  99.49%, val:  75.83%, val_best:  77.50%\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  1.476780/  1.799542, tr:  99.59%, tr_best:  99.59%, val:  75.83%, val_best:  77.50%\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  1.476797/  1.803606, tr:  99.59%, tr_best:  99.59%, val:  75.42%, val_best:  77.50%\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  1.477107/  1.804560, tr:  99.59%, tr_best:  99.59%, val:  74.17%, val_best:  77.50%\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  1.476413/  1.800839, tr:  99.49%, tr_best:  99.59%, val:  75.42%, val_best:  77.50%\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  1.476331/  1.803456, tr:  99.49%, tr_best:  99.59%, val:  74.58%, val_best:  77.50%\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  1.476016/  1.802776, tr:  99.59%, tr_best:  99.59%, val:  75.83%, val_best:  77.50%\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  1.475527/  1.802533, tr:  99.69%, tr_best:  99.69%, val:  76.25%, val_best:  77.50%\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  1.475693/  1.805254, tr:  99.59%, tr_best:  99.69%, val:  75.00%, val_best:  77.50%\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  1.474822/  1.804070, tr:  99.59%, tr_best:  99.69%, val:  76.25%, val_best:  77.50%\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  1.475045/  1.801250, tr:  99.69%, tr_best:  99.69%, val:  75.00%, val_best:  77.50%\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  1.475054/  1.800925, tr:  99.59%, tr_best:  99.69%, val:  75.42%, val_best:  77.50%\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  1.474495/  1.805671, tr:  99.90%, tr_best:  99.90%, val:  75.42%, val_best:  77.50%\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  1.475129/  1.802729, tr:  99.59%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  1.473911/  1.803581, tr:  99.69%, tr_best:  99.90%, val:  74.17%, val_best:  77.50%\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  1.473777/  1.801466, tr:  99.59%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  1.473985/  1.798479, tr:  99.80%, tr_best:  99.90%, val:  75.42%, val_best:  77.50%\n",
      "epoch-200 lr=['0.0010000'], tr/val_loss:  1.473775/  1.804837, tr:  99.69%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-201 lr=['0.0010000'], tr/val_loss:  1.473976/  1.805487, tr:  99.80%, tr_best:  99.90%, val:  75.00%, val_best:  77.50%\n",
      "epoch-202 lr=['0.0010000'], tr/val_loss:  1.473540/  1.801240, tr:  99.80%, tr_best:  99.90%, val:  74.58%, val_best:  77.50%\n",
      "epoch-203 lr=['0.0010000'], tr/val_loss:  1.474240/  1.804449, tr:  99.69%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-204 lr=['0.0010000'], tr/val_loss:  1.473104/  1.801287, tr:  99.80%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-205 lr=['0.0010000'], tr/val_loss:  1.472679/  1.800594, tr:  99.80%, tr_best:  99.90%, val:  75.42%, val_best:  77.50%\n",
      "epoch-206 lr=['0.0010000'], tr/val_loss:  1.472573/  1.796606, tr:  99.80%, tr_best:  99.90%, val:  77.08%, val_best:  77.50%\n",
      "epoch-207 lr=['0.0010000'], tr/val_loss:  1.472359/  1.803670, tr:  99.80%, tr_best:  99.90%, val:  75.00%, val_best:  77.50%\n",
      "epoch-208 lr=['0.0010000'], tr/val_loss:  1.472458/  1.801268, tr:  99.80%, tr_best:  99.90%, val:  75.00%, val_best:  77.50%\n",
      "epoch-209 lr=['0.0010000'], tr/val_loss:  1.472149/  1.798726, tr:  99.80%, tr_best:  99.90%, val:  75.42%, val_best:  77.50%\n",
      "epoch-210 lr=['0.0010000'], tr/val_loss:  1.472040/  1.801282, tr:  99.80%, tr_best:  99.90%, val:  75.42%, val_best:  77.50%\n",
      "epoch-211 lr=['0.0010000'], tr/val_loss:  1.471771/  1.800063, tr:  99.80%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-212 lr=['0.0010000'], tr/val_loss:  1.472158/  1.804358, tr:  99.80%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-213 lr=['0.0010000'], tr/val_loss:  1.471897/  1.803353, tr:  99.90%, tr_best:  99.90%, val:  75.00%, val_best:  77.50%\n",
      "epoch-214 lr=['0.0010000'], tr/val_loss:  1.471592/  1.804629, tr:  99.90%, tr_best:  99.90%, val:  75.00%, val_best:  77.50%\n",
      "epoch-215 lr=['0.0010000'], tr/val_loss:  1.471424/  1.800676, tr:  99.69%, tr_best:  99.90%, val:  75.00%, val_best:  77.50%\n",
      "epoch-216 lr=['0.0010000'], tr/val_loss:  1.471492/  1.803524, tr:  99.90%, tr_best:  99.90%, val:  75.42%, val_best:  77.50%\n",
      "epoch-217 lr=['0.0010000'], tr/val_loss:  1.471362/  1.803526, tr:  99.80%, tr_best:  99.90%, val:  74.58%, val_best:  77.50%\n",
      "epoch-218 lr=['0.0010000'], tr/val_loss:  1.470753/  1.800726, tr:  99.80%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-219 lr=['0.0010000'], tr/val_loss:  1.471148/  1.804367, tr:  99.80%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-220 lr=['0.0010000'], tr/val_loss:  1.471065/  1.803668, tr:  99.90%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-221 lr=['0.0010000'], tr/val_loss:  1.470712/  1.805363, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-222 lr=['0.0010000'], tr/val_loss:  1.470602/  1.802412, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-223 lr=['0.0010000'], tr/val_loss:  1.470625/  1.802368, tr:  99.80%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-224 lr=['0.0010000'], tr/val_loss:  1.470419/  1.805183, tr:  99.90%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-225 lr=['0.0010000'], tr/val_loss:  1.471280/  1.803592, tr:  99.80%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-226 lr=['0.0010000'], tr/val_loss:  1.471099/  1.801618, tr:  99.80%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-227 lr=['0.0010000'], tr/val_loss:  1.470154/  1.801976, tr:  99.80%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-228 lr=['0.0010000'], tr/val_loss:  1.470573/  1.807705, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-229 lr=['0.0010000'], tr/val_loss:  1.470344/  1.806690, tr:  99.90%, tr_best:  99.90%, val:  75.00%, val_best:  77.50%\n",
      "epoch-230 lr=['0.0010000'], tr/val_loss:  1.470407/  1.807907, tr:  99.90%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-231 lr=['0.0010000'], tr/val_loss:  1.470008/  1.804116, tr:  99.90%, tr_best:  99.90%, val:  77.08%, val_best:  77.50%\n",
      "epoch-232 lr=['0.0010000'], tr/val_loss:  1.469888/  1.803625, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-233 lr=['0.0010000'], tr/val_loss:  1.469893/  1.805463, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-234 lr=['0.0010000'], tr/val_loss:  1.469734/  1.806041, tr:  99.90%, tr_best:  99.90%, val:  75.42%, val_best:  77.50%\n",
      "epoch-235 lr=['0.0010000'], tr/val_loss:  1.469732/  1.809345, tr:  99.90%, tr_best:  99.90%, val:  74.58%, val_best:  77.50%\n",
      "epoch-236 lr=['0.0010000'], tr/val_loss:  1.469194/  1.809290, tr:  99.90%, tr_best:  99.90%, val:  75.00%, val_best:  77.50%\n",
      "epoch-237 lr=['0.0010000'], tr/val_loss:  1.469988/  1.807573, tr:  99.90%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-238 lr=['0.0010000'], tr/val_loss:  1.469515/  1.810544, tr:  99.90%, tr_best:  99.90%, val:  75.42%, val_best:  77.50%\n",
      "epoch-239 lr=['0.0010000'], tr/val_loss:  1.469357/  1.807135, tr:  99.90%, tr_best:  99.90%, val:  75.42%, val_best:  77.50%\n",
      "epoch-240 lr=['0.0010000'], tr/val_loss:  1.469435/  1.809059, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-241 lr=['0.0010000'], tr/val_loss:  1.469290/  1.809763, tr:  99.90%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-242 lr=['0.0010000'], tr/val_loss:  1.469145/  1.810249, tr:  99.90%, tr_best:  99.90%, val:  75.00%, val_best:  77.50%\n",
      "epoch-243 lr=['0.0010000'], tr/val_loss:  1.469613/  1.809532, tr:  99.90%, tr_best:  99.90%, val:  75.42%, val_best:  77.50%\n",
      "epoch-244 lr=['0.0010000'], tr/val_loss:  1.469267/  1.807391, tr:  99.90%, tr_best:  99.90%, val:  75.00%, val_best:  77.50%\n",
      "epoch-245 lr=['0.0010000'], tr/val_loss:  1.468697/  1.810189, tr:  99.90%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-246 lr=['0.0010000'], tr/val_loss:  1.469264/  1.809830, tr:  99.90%, tr_best:  99.90%, val:  75.42%, val_best:  77.50%\n",
      "epoch-247 lr=['0.0010000'], tr/val_loss:  1.469105/  1.811269, tr:  99.90%, tr_best:  99.90%, val:  75.00%, val_best:  77.50%\n",
      "epoch-248 lr=['0.0010000'], tr/val_loss:  1.469031/  1.809262, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-249 lr=['0.0010000'], tr/val_loss:  1.468715/  1.807458, tr:  99.90%, tr_best:  99.90%, val:  75.00%, val_best:  77.50%\n",
      "epoch-250 lr=['0.0010000'], tr/val_loss:  1.468871/  1.806769, tr:  99.90%, tr_best:  99.90%, val:  77.08%, val_best:  77.50%\n",
      "epoch-251 lr=['0.0010000'], tr/val_loss:  1.468831/  1.811678, tr:  99.90%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-252 lr=['0.0010000'], tr/val_loss:  1.468453/  1.805662, tr:  99.90%, tr_best:  99.90%, val:  77.08%, val_best:  77.50%\n",
      "epoch-253 lr=['0.0010000'], tr/val_loss:  1.468357/  1.805781, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-254 lr=['0.0010000'], tr/val_loss:  1.468403/  1.804709, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-255 lr=['0.0010000'], tr/val_loss:  1.469292/  1.802666, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-256 lr=['0.0010000'], tr/val_loss:  1.468395/  1.804908, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-257 lr=['0.0010000'], tr/val_loss:  1.468436/  1.807814, tr:  99.90%, tr_best:  99.90%, val:  75.42%, val_best:  77.50%\n",
      "epoch-258 lr=['0.0010000'], tr/val_loss:  1.468776/  1.805213, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-259 lr=['0.0010000'], tr/val_loss:  1.468312/  1.809297, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-260 lr=['0.0010000'], tr/val_loss:  1.468723/  1.803252, tr:  99.90%, tr_best:  99.90%, val:  77.08%, val_best:  77.50%\n",
      "epoch-261 lr=['0.0010000'], tr/val_loss:  1.468417/  1.801781, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-262 lr=['0.0010000'], tr/val_loss:  1.468062/  1.802940, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-263 lr=['0.0010000'], tr/val_loss:  1.468644/  1.803034, tr:  99.90%, tr_best:  99.90%, val:  77.08%, val_best:  77.50%\n",
      "epoch-264 lr=['0.0010000'], tr/val_loss:  1.467854/  1.802516, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-265 lr=['0.0010000'], tr/val_loss:  1.468049/  1.804070, tr:  99.90%, tr_best:  99.90%, val:  77.08%, val_best:  77.50%\n",
      "epoch-266 lr=['0.0010000'], tr/val_loss:  1.467912/  1.803153, tr:  99.90%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-267 lr=['0.0010000'], tr/val_loss:  1.468063/  1.805163, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-268 lr=['0.0010000'], tr/val_loss:  1.467791/  1.807346, tr:  99.90%, tr_best:  99.90%, val:  77.08%, val_best:  77.50%\n",
      "epoch-269 lr=['0.0010000'], tr/val_loss:  1.467975/  1.803158, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-270 lr=['0.0010000'], tr/val_loss:  1.468256/  1.803239, tr:  99.90%, tr_best:  99.90%, val:  77.50%, val_best:  77.50%\n",
      "epoch-271 lr=['0.0010000'], tr/val_loss:  1.467651/  1.805416, tr:  99.90%, tr_best:  99.90%, val:  77.08%, val_best:  77.50%\n",
      "epoch-272 lr=['0.0010000'], tr/val_loss:  1.467686/  1.807867, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-273 lr=['0.0010000'], tr/val_loss:  1.467393/  1.807291, tr:  99.90%, tr_best:  99.90%, val:  77.50%, val_best:  77.50%\n",
      "epoch-274 lr=['0.0010000'], tr/val_loss:  1.467900/  1.809454, tr:  99.90%, tr_best:  99.90%, val:  75.42%, val_best:  77.50%\n",
      "epoch-275 lr=['0.0010000'], tr/val_loss:  1.467307/  1.808282, tr:  99.90%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-276 lr=['0.0010000'], tr/val_loss:  1.467255/  1.804657, tr:  99.90%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-277 lr=['0.0010000'], tr/val_loss:  1.467570/  1.804398, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-278 lr=['0.0010000'], tr/val_loss:  1.468123/  1.807162, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-279 lr=['0.0010000'], tr/val_loss:  1.467048/  1.808502, tr:  99.90%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-280 lr=['0.0010000'], tr/val_loss:  1.467359/  1.808132, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-281 lr=['0.0010000'], tr/val_loss:  1.466616/  1.807636, tr:  99.90%, tr_best:  99.90%, val:  75.42%, val_best:  77.50%\n",
      "epoch-282 lr=['0.0010000'], tr/val_loss:  1.466872/  1.810262, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-283 lr=['0.0010000'], tr/val_loss:  1.466893/  1.812106, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-284 lr=['0.0010000'], tr/val_loss:  1.467192/  1.811524, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-285 lr=['0.0010000'], tr/val_loss:  1.466914/  1.810528, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-286 lr=['0.0010000'], tr/val_loss:  1.466705/  1.811361, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-287 lr=['0.0010000'], tr/val_loss:  1.466681/  1.808493, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-288 lr=['0.0010000'], tr/val_loss:  1.467451/  1.808537, tr:  99.90%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-289 lr=['0.0010000'], tr/val_loss:  1.466883/  1.809750, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-290 lr=['0.0010000'], tr/val_loss:  1.466736/  1.806960, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-291 lr=['0.0010000'], tr/val_loss:  1.466827/  1.812150, tr:  99.90%, tr_best:  99.90%, val:  77.50%, val_best:  77.50%\n",
      "epoch-292 lr=['0.0010000'], tr/val_loss:  1.466895/  1.811188, tr:  99.90%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-293 lr=['0.0010000'], tr/val_loss:  1.466864/  1.806112, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-294 lr=['0.0010000'], tr/val_loss:  1.467405/  1.808185, tr:  99.90%, tr_best:  99.90%, val:  75.83%, val_best:  77.50%\n",
      "epoch-295 lr=['0.0010000'], tr/val_loss:  1.467024/  1.808700, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  77.50%\n",
      "epoch-296 lr=['0.0010000'], tr/val_loss:  1.466438/  1.809109, tr:  99.90%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-297 lr=['0.0010000'], tr/val_loss:  1.466602/  1.809722, tr:  99.90%, tr_best:  99.90%, val:  76.25%, val_best:  77.50%\n",
      "epoch-298 lr=['0.0010000'], tr/val_loss:  1.466611/  1.806265, tr:  99.90%, tr_best:  99.90%, val:  75.42%, val_best:  77.50%\n",
      "epoch-299 lr=['0.0010000'], tr/val_loss:  1.466644/  1.809260, tr:  99.90%, tr_best:  99.90%, val:  77.08%, val_best:  77.50%\n",
      "epoch-300 lr=['0.0010000'], tr/val_loss:  1.466495/  1.804925, tr:  99.90%, tr_best:  99.90%, val:  77.50%, val_best:  77.50%\n",
      "epoch-301 lr=['0.0010000'], tr/val_loss:  1.466401/  1.808805, tr:  99.90%, tr_best:  99.90%, val:  78.75%, val_best:  78.75%\n",
      "epoch-302 lr=['0.0010000'], tr/val_loss:  1.466963/  1.808644, tr:  99.90%, tr_best:  99.90%, val:  76.67%, val_best:  78.75%\n",
      "epoch-303 lr=['0.0010000'], tr/val_loss:  1.466459/  1.811164, tr:  99.90%, tr_best:  99.90%, val:  75.42%, val_best:  78.75%\n",
      "epoch-304 lr=['0.0010000'], tr/val_loss:  1.466956/  1.811921, tr:  99.90%, tr_best:  99.90%, val:  76.25%, val_best:  78.75%\n",
      "epoch-305 lr=['0.0010000'], tr/val_loss:  1.466167/  1.809460, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-306 lr=['0.0010000'], tr/val_loss:  1.466356/  1.813778, tr:  99.90%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-307 lr=['0.0010000'], tr/val_loss:  1.466455/  1.812818, tr:  99.90%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-308 lr=['0.0010000'], tr/val_loss:  1.465923/  1.811519, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-309 lr=['0.0010000'], tr/val_loss:  1.466124/  1.809827, tr:  99.90%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-310 lr=['0.0010000'], tr/val_loss:  1.466085/  1.814003, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-311 lr=['0.0010000'], tr/val_loss:  1.467991/  1.815747, tr:  99.90%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-312 lr=['0.0010000'], tr/val_loss:  1.466203/  1.813315, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-313 lr=['0.0010000'], tr/val_loss:  1.465921/  1.812366, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-314 lr=['0.0010000'], tr/val_loss:  1.465858/  1.815970, tr:  99.90%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-315 lr=['0.0010000'], tr/val_loss:  1.466195/  1.813818, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-316 lr=['0.0010000'], tr/val_loss:  1.466229/  1.815892, tr:  99.90%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-317 lr=['0.0010000'], tr/val_loss:  1.465974/  1.817273, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-318 lr=['0.0010000'], tr/val_loss:  1.466262/  1.814123, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-319 lr=['0.0010000'], tr/val_loss:  1.467409/  1.813002, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-320 lr=['0.0010000'], tr/val_loss:  1.465884/  1.815107, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-321 lr=['0.0010000'], tr/val_loss:  1.465933/  1.816614, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-322 lr=['0.0010000'], tr/val_loss:  1.465797/  1.813485, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-323 lr=['0.0010000'], tr/val_loss:  1.465885/  1.814099, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-324 lr=['0.0010000'], tr/val_loss:  1.466111/  1.814691, tr:  99.90%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-325 lr=['0.0010000'], tr/val_loss:  1.466673/  1.815409, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-326 lr=['0.0010000'], tr/val_loss:  1.465757/  1.817709, tr:  99.90%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-327 lr=['0.0010000'], tr/val_loss:  1.465666/  1.815812, tr: 100.00%, tr_best: 100.00%, val:  78.33%, val_best:  78.75%\n",
      "epoch-328 lr=['0.0010000'], tr/val_loss:  1.465671/  1.815198, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-329 lr=['0.0010000'], tr/val_loss:  1.465574/  1.811829, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-330 lr=['0.0010000'], tr/val_loss:  1.465669/  1.814020, tr: 100.00%, tr_best: 100.00%, val:  78.33%, val_best:  78.75%\n",
      "epoch-331 lr=['0.0010000'], tr/val_loss:  1.465451/  1.815004, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-332 lr=['0.0010000'], tr/val_loss:  1.466174/  1.817252, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-333 lr=['0.0010000'], tr/val_loss:  1.465891/  1.815555, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-334 lr=['0.0010000'], tr/val_loss:  1.465465/  1.814949, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-335 lr=['0.0010000'], tr/val_loss:  1.465499/  1.815190, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-336 lr=['0.0010000'], tr/val_loss:  1.465582/  1.816713, tr: 100.00%, tr_best: 100.00%, val:  78.33%, val_best:  78.75%\n",
      "epoch-337 lr=['0.0010000'], tr/val_loss:  1.465371/  1.818517, tr: 100.00%, tr_best: 100.00%, val:  78.33%, val_best:  78.75%\n",
      "epoch-338 lr=['0.0010000'], tr/val_loss:  1.465426/  1.819203, tr: 100.00%, tr_best: 100.00%, val:  78.33%, val_best:  78.75%\n",
      "epoch-339 lr=['0.0010000'], tr/val_loss:  1.465385/  1.816499, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-340 lr=['0.0010000'], tr/val_loss:  1.465327/  1.815867, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-341 lr=['0.0010000'], tr/val_loss:  1.465347/  1.819352, tr: 100.00%, tr_best: 100.00%, val:  78.33%, val_best:  78.75%\n",
      "epoch-342 lr=['0.0010000'], tr/val_loss:  1.465431/  1.822785, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-343 lr=['0.0010000'], tr/val_loss:  1.465339/  1.821436, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-344 lr=['0.0010000'], tr/val_loss:  1.465515/  1.820141, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-345 lr=['0.0010000'], tr/val_loss:  1.465670/  1.819034, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-346 lr=['0.0010000'], tr/val_loss:  1.465833/  1.820422, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-347 lr=['0.0010000'], tr/val_loss:  1.465539/  1.820933, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-348 lr=['0.0010000'], tr/val_loss:  1.465380/  1.817994, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-349 lr=['0.0010000'], tr/val_loss:  1.465426/  1.818950, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-350 lr=['0.0010000'], tr/val_loss:  1.465168/  1.816842, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-351 lr=['0.0010000'], tr/val_loss:  1.465258/  1.820522, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-352 lr=['0.0010000'], tr/val_loss:  1.465295/  1.823324, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-353 lr=['0.0010000'], tr/val_loss:  1.465751/  1.821221, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-354 lr=['0.0010000'], tr/val_loss:  1.465400/  1.819604, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-355 lr=['0.0010000'], tr/val_loss:  1.465231/  1.819474, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-356 lr=['0.0010000'], tr/val_loss:  1.465307/  1.820957, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-357 lr=['0.0010000'], tr/val_loss:  1.465323/  1.821501, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-358 lr=['0.0010000'], tr/val_loss:  1.465251/  1.819706, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-359 lr=['0.0010000'], tr/val_loss:  1.465183/  1.820122, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-360 lr=['0.0010000'], tr/val_loss:  1.464989/  1.820900, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-361 lr=['0.0010000'], tr/val_loss:  1.464960/  1.820194, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-362 lr=['0.0010000'], tr/val_loss:  1.465006/  1.821701, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-363 lr=['0.0010000'], tr/val_loss:  1.465051/  1.820246, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-364 lr=['0.0010000'], tr/val_loss:  1.465723/  1.819831, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-365 lr=['0.0010000'], tr/val_loss:  1.464863/  1.819108, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-366 lr=['0.0010000'], tr/val_loss:  1.464904/  1.820444, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-367 lr=['0.0010000'], tr/val_loss:  1.464885/  1.819913, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-368 lr=['0.0010000'], tr/val_loss:  1.464947/  1.821436, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-369 lr=['0.0010000'], tr/val_loss:  1.464892/  1.821948, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-370 lr=['0.0010000'], tr/val_loss:  1.464676/  1.820847, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-371 lr=['0.0010000'], tr/val_loss:  1.465475/  1.821455, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-372 lr=['0.0010000'], tr/val_loss:  1.465123/  1.821734, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-373 lr=['0.0010000'], tr/val_loss:  1.464720/  1.818291, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-374 lr=['0.0010000'], tr/val_loss:  1.464837/  1.822443, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-375 lr=['0.0010000'], tr/val_loss:  1.464967/  1.818236, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-376 lr=['0.0010000'], tr/val_loss:  1.465468/  1.820566, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-377 lr=['0.0010000'], tr/val_loss:  1.464648/  1.820389, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-378 lr=['0.0010000'], tr/val_loss:  1.464732/  1.819267, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-379 lr=['0.0010000'], tr/val_loss:  1.465027/  1.819263, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-380 lr=['0.0010000'], tr/val_loss:  1.464722/  1.818575, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-381 lr=['0.0010000'], tr/val_loss:  1.464485/  1.819379, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-382 lr=['0.0010000'], tr/val_loss:  1.464537/  1.818287, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-383 lr=['0.0010000'], tr/val_loss:  1.464797/  1.820044, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-384 lr=['0.0010000'], tr/val_loss:  1.464248/  1.818337, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-385 lr=['0.0010000'], tr/val_loss:  1.464446/  1.820361, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-386 lr=['0.0010000'], tr/val_loss:  1.464975/  1.822617, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-387 lr=['0.0010000'], tr/val_loss:  1.464398/  1.820336, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-388 lr=['0.0010000'], tr/val_loss:  1.464428/  1.816505, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-389 lr=['0.0010000'], tr/val_loss:  1.464900/  1.818226, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-390 lr=['0.0010000'], tr/val_loss:  1.464446/  1.824569, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-391 lr=['0.0010000'], tr/val_loss:  1.464212/  1.819024, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-392 lr=['0.0010000'], tr/val_loss:  1.464394/  1.821215, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-393 lr=['0.0010000'], tr/val_loss:  1.464268/  1.819230, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-394 lr=['0.0010000'], tr/val_loss:  1.464380/  1.816166, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-395 lr=['0.0010000'], tr/val_loss:  1.464492/  1.821116, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-396 lr=['0.0010000'], tr/val_loss:  1.464270/  1.817619, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-397 lr=['0.0010000'], tr/val_loss:  1.464246/  1.820920, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-398 lr=['0.0010000'], tr/val_loss:  1.464295/  1.817426, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-399 lr=['0.0010000'], tr/val_loss:  1.464234/  1.820029, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-400 lr=['0.0010000'], tr/val_loss:  1.464084/  1.822783, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-401 lr=['0.0010000'], tr/val_loss:  1.464555/  1.822646, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-402 lr=['0.0010000'], tr/val_loss:  1.464012/  1.819264, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-403 lr=['0.0010000'], tr/val_loss:  1.464217/  1.822661, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-404 lr=['0.0010000'], tr/val_loss:  1.464273/  1.819456, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-405 lr=['0.0010000'], tr/val_loss:  1.464287/  1.821203, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-406 lr=['0.0010000'], tr/val_loss:  1.464179/  1.821039, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-407 lr=['0.0010000'], tr/val_loss:  1.464157/  1.821013, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-408 lr=['0.0010000'], tr/val_loss:  1.464214/  1.822855, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-409 lr=['0.0010000'], tr/val_loss:  1.464083/  1.819388, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-410 lr=['0.0010000'], tr/val_loss:  1.464411/  1.818609, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-411 lr=['0.0010000'], tr/val_loss:  1.464046/  1.819294, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-412 lr=['0.0010000'], tr/val_loss:  1.464096/  1.820936, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-413 lr=['0.0010000'], tr/val_loss:  1.464026/  1.817480, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-414 lr=['0.0010000'], tr/val_loss:  1.464125/  1.817015, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-415 lr=['0.0010000'], tr/val_loss:  1.463930/  1.814515, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-416 lr=['0.0010000'], tr/val_loss:  1.463958/  1.818603, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-417 lr=['0.0010000'], tr/val_loss:  1.463887/  1.816345, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-418 lr=['0.0010000'], tr/val_loss:  1.464331/  1.818941, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-419 lr=['0.0010000'], tr/val_loss:  1.464023/  1.819368, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-420 lr=['0.0010000'], tr/val_loss:  1.463896/  1.817494, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-421 lr=['0.0010000'], tr/val_loss:  1.464366/  1.817852, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-422 lr=['0.0010000'], tr/val_loss:  1.463675/  1.818960, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-423 lr=['0.0010000'], tr/val_loss:  1.463901/  1.819743, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-424 lr=['0.0010000'], tr/val_loss:  1.463854/  1.817793, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-425 lr=['0.0010000'], tr/val_loss:  1.464015/  1.818741, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-426 lr=['0.0010000'], tr/val_loss:  1.463913/  1.818892, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-427 lr=['0.0010000'], tr/val_loss:  1.464339/  1.816603, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-428 lr=['0.0010000'], tr/val_loss:  1.463958/  1.817111, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-429 lr=['0.0010000'], tr/val_loss:  1.464341/  1.818542, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-430 lr=['0.0010000'], tr/val_loss:  1.463715/  1.820172, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-431 lr=['0.0010000'], tr/val_loss:  1.463806/  1.818729, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-432 lr=['0.0010000'], tr/val_loss:  1.463700/  1.819662, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-433 lr=['0.0010000'], tr/val_loss:  1.464001/  1.819661, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-434 lr=['0.0010000'], tr/val_loss:  1.463786/  1.815749, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-435 lr=['0.0010000'], tr/val_loss:  1.463861/  1.815406, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-436 lr=['0.0010000'], tr/val_loss:  1.463948/  1.815552, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-437 lr=['0.0010000'], tr/val_loss:  1.463927/  1.817223, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-438 lr=['0.0010000'], tr/val_loss:  1.463756/  1.818073, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-439 lr=['0.0010000'], tr/val_loss:  1.463952/  1.815158, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-440 lr=['0.0010000'], tr/val_loss:  1.464037/  1.817975, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-441 lr=['0.0010000'], tr/val_loss:  1.463759/  1.815262, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-442 lr=['0.0010000'], tr/val_loss:  1.463860/  1.818790, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-443 lr=['0.0010000'], tr/val_loss:  1.463879/  1.816296, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-444 lr=['0.0010000'], tr/val_loss:  1.463847/  1.814901, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-445 lr=['0.0010000'], tr/val_loss:  1.463849/  1.817208, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-446 lr=['0.0010000'], tr/val_loss:  1.463849/  1.813653, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-447 lr=['0.0010000'], tr/val_loss:  1.463917/  1.816447, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-448 lr=['0.0010000'], tr/val_loss:  1.463861/  1.815399, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-449 lr=['0.0010000'], tr/val_loss:  1.463948/  1.815004, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-450 lr=['0.0010000'], tr/val_loss:  1.463968/  1.814040, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-451 lr=['0.0010000'], tr/val_loss:  1.463748/  1.815240, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-452 lr=['0.0010000'], tr/val_loss:  1.463849/  1.818554, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-453 lr=['0.0010000'], tr/val_loss:  1.463780/  1.815321, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-454 lr=['0.0010000'], tr/val_loss:  1.463679/  1.814509, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-455 lr=['0.0010000'], tr/val_loss:  1.463822/  1.816967, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-456 lr=['0.0010000'], tr/val_loss:  1.463739/  1.816161, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-457 lr=['0.0010000'], tr/val_loss:  1.463835/  1.816283, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-458 lr=['0.0010000'], tr/val_loss:  1.463768/  1.818556, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-459 lr=['0.0010000'], tr/val_loss:  1.463840/  1.814185, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-460 lr=['0.0010000'], tr/val_loss:  1.463646/  1.818890, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-461 lr=['0.0010000'], tr/val_loss:  1.463754/  1.814862, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-462 lr=['0.0010000'], tr/val_loss:  1.464013/  1.815854, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-463 lr=['0.0010000'], tr/val_loss:  1.463863/  1.815124, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-464 lr=['0.0010000'], tr/val_loss:  1.463849/  1.814551, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-465 lr=['0.0010000'], tr/val_loss:  1.463620/  1.815236, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-466 lr=['0.0010000'], tr/val_loss:  1.464392/  1.814573, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-467 lr=['0.0010000'], tr/val_loss:  1.463659/  1.816443, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-468 lr=['0.0010000'], tr/val_loss:  1.463804/  1.812968, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-469 lr=['0.0010000'], tr/val_loss:  1.463764/  1.816965, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-470 lr=['0.0010000'], tr/val_loss:  1.463796/  1.816519, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-471 lr=['0.0010000'], tr/val_loss:  1.464100/  1.814712, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-472 lr=['0.0010000'], tr/val_loss:  1.463610/  1.816503, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-473 lr=['0.0010000'], tr/val_loss:  1.463682/  1.815652, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-474 lr=['0.0010000'], tr/val_loss:  1.463929/  1.814248, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-475 lr=['0.0010000'], tr/val_loss:  1.463606/  1.816888, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-476 lr=['0.0010000'], tr/val_loss:  1.463732/  1.816223, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-477 lr=['0.0010000'], tr/val_loss:  1.463531/  1.818336, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-478 lr=['0.0010000'], tr/val_loss:  1.463390/  1.816503, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-479 lr=['0.0010000'], tr/val_loss:  1.463527/  1.818427, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-480 lr=['0.0010000'], tr/val_loss:  1.463456/  1.814893, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-481 lr=['0.0010000'], tr/val_loss:  1.463421/  1.817992, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-482 lr=['0.0010000'], tr/val_loss:  1.463448/  1.818502, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-483 lr=['0.0010000'], tr/val_loss:  1.463487/  1.818385, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-484 lr=['0.0010000'], tr/val_loss:  1.463488/  1.817869, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-485 lr=['0.0010000'], tr/val_loss:  1.463416/  1.817525, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-486 lr=['0.0010000'], tr/val_loss:  1.463388/  1.818171, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-487 lr=['0.0010000'], tr/val_loss:  1.463374/  1.817135, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-488 lr=['0.0010000'], tr/val_loss:  1.463384/  1.820881, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-489 lr=['0.0010000'], tr/val_loss:  1.464354/  1.817704, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-490 lr=['0.0010000'], tr/val_loss:  1.463270/  1.819359, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-491 lr=['0.0010000'], tr/val_loss:  1.463389/  1.818625, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-492 lr=['0.0010000'], tr/val_loss:  1.463414/  1.818205, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-493 lr=['0.0010000'], tr/val_loss:  1.463517/  1.819873, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-494 lr=['0.0010000'], tr/val_loss:  1.463434/  1.820151, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-495 lr=['0.0010000'], tr/val_loss:  1.463246/  1.821638, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-496 lr=['0.0010000'], tr/val_loss:  1.463292/  1.818400, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-497 lr=['0.0010000'], tr/val_loss:  1.463462/  1.819408, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-498 lr=['0.0010000'], tr/val_loss:  1.463375/  1.817674, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-499 lr=['0.0010000'], tr/val_loss:  1.463359/  1.821958, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-500 lr=['0.0010000'], tr/val_loss:  1.463343/  1.817346, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-501 lr=['0.0010000'], tr/val_loss:  1.463789/  1.815048, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-502 lr=['0.0010000'], tr/val_loss:  1.463472/  1.818242, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-503 lr=['0.0010000'], tr/val_loss:  1.463481/  1.818570, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-504 lr=['0.0010000'], tr/val_loss:  1.463413/  1.818131, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-505 lr=['0.0010000'], tr/val_loss:  1.463189/  1.820123, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-506 lr=['0.0010000'], tr/val_loss:  1.463387/  1.816406, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-507 lr=['0.0010000'], tr/val_loss:  1.463440/  1.820240, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-508 lr=['0.0010000'], tr/val_loss:  1.463375/  1.819736, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-509 lr=['0.0010000'], tr/val_loss:  1.463358/  1.818369, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-510 lr=['0.0010000'], tr/val_loss:  1.463226/  1.819929, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-511 lr=['0.0010000'], tr/val_loss:  1.463254/  1.816250, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-512 lr=['0.0010000'], tr/val_loss:  1.463084/  1.815763, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-513 lr=['0.0010000'], tr/val_loss:  1.463402/  1.815870, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-514 lr=['0.0010000'], tr/val_loss:  1.463228/  1.816813, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-515 lr=['0.0010000'], tr/val_loss:  1.463212/  1.819290, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-516 lr=['0.0010000'], tr/val_loss:  1.463128/  1.820129, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-517 lr=['0.0010000'], tr/val_loss:  1.462985/  1.817467, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-518 lr=['0.0010000'], tr/val_loss:  1.463040/  1.816638, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-519 lr=['0.0010000'], tr/val_loss:  1.463088/  1.818085, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-520 lr=['0.0010000'], tr/val_loss:  1.463108/  1.817450, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-521 lr=['0.0010000'], tr/val_loss:  1.463074/  1.822382, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-522 lr=['0.0010000'], tr/val_loss:  1.463214/  1.819112, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-523 lr=['0.0010000'], tr/val_loss:  1.463274/  1.820028, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-524 lr=['0.0010000'], tr/val_loss:  1.463103/  1.818457, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-525 lr=['0.0010000'], tr/val_loss:  1.463175/  1.819259, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-526 lr=['0.0010000'], tr/val_loss:  1.463187/  1.820568, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-527 lr=['0.0010000'], tr/val_loss:  1.463074/  1.817064, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-528 lr=['0.0010000'], tr/val_loss:  1.463593/  1.821601, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-529 lr=['0.0010000'], tr/val_loss:  1.463083/  1.822487, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-530 lr=['0.0010000'], tr/val_loss:  1.462900/  1.823811, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-531 lr=['0.0010000'], tr/val_loss:  1.462971/  1.818614, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-532 lr=['0.0010000'], tr/val_loss:  1.462946/  1.821480, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-533 lr=['0.0010000'], tr/val_loss:  1.462987/  1.822317, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-534 lr=['0.0010000'], tr/val_loss:  1.463149/  1.820877, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-535 lr=['0.0010000'], tr/val_loss:  1.462891/  1.821217, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-536 lr=['0.0010000'], tr/val_loss:  1.462930/  1.822505, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-537 lr=['0.0010000'], tr/val_loss:  1.462845/  1.822542, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-538 lr=['0.0010000'], tr/val_loss:  1.463157/  1.822096, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-539 lr=['0.0010000'], tr/val_loss:  1.462964/  1.821162, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-540 lr=['0.0010000'], tr/val_loss:  1.462833/  1.817751, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-541 lr=['0.0010000'], tr/val_loss:  1.462831/  1.822304, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-542 lr=['0.0010000'], tr/val_loss:  1.462989/  1.819428, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-543 lr=['0.0010000'], tr/val_loss:  1.463113/  1.820118, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-544 lr=['0.0010000'], tr/val_loss:  1.462961/  1.822042, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-545 lr=['0.0010000'], tr/val_loss:  1.462944/  1.820515, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-546 lr=['0.0010000'], tr/val_loss:  1.462888/  1.821482, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-547 lr=['0.0010000'], tr/val_loss:  1.462930/  1.822204, tr: 100.00%, tr_best: 100.00%, val:  74.17%, val_best:  78.75%\n",
      "epoch-548 lr=['0.0010000'], tr/val_loss:  1.463187/  1.818538, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-549 lr=['0.0010000'], tr/val_loss:  1.462790/  1.821356, tr: 100.00%, tr_best: 100.00%, val:  73.75%, val_best:  78.75%\n",
      "epoch-550 lr=['0.0010000'], tr/val_loss:  1.462861/  1.820734, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-551 lr=['0.0010000'], tr/val_loss:  1.462904/  1.820436, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-552 lr=['0.0010000'], tr/val_loss:  1.463005/  1.821505, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-553 lr=['0.0010000'], tr/val_loss:  1.462932/  1.821579, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-554 lr=['0.0010000'], tr/val_loss:  1.463090/  1.821243, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-555 lr=['0.0010000'], tr/val_loss:  1.463019/  1.820948, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-556 lr=['0.0010000'], tr/val_loss:  1.462876/  1.819878, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-557 lr=['0.0010000'], tr/val_loss:  1.462888/  1.824186, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-558 lr=['0.0010000'], tr/val_loss:  1.462945/  1.822536, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-559 lr=['0.0010000'], tr/val_loss:  1.462888/  1.823305, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-560 lr=['0.0010000'], tr/val_loss:  1.462849/  1.823255, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-561 lr=['0.0010000'], tr/val_loss:  1.462863/  1.821907, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-562 lr=['0.0010000'], tr/val_loss:  1.462877/  1.822121, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-563 lr=['0.0010000'], tr/val_loss:  1.462790/  1.819946, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-564 lr=['0.0010000'], tr/val_loss:  1.462881/  1.822945, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-565 lr=['0.0010000'], tr/val_loss:  1.462977/  1.823485, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-566 lr=['0.0010000'], tr/val_loss:  1.462796/  1.822986, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-567 lr=['0.0010000'], tr/val_loss:  1.462934/  1.822735, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-568 lr=['0.0010000'], tr/val_loss:  1.463115/  1.824267, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-569 lr=['0.0010000'], tr/val_loss:  1.462975/  1.822894, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-570 lr=['0.0010000'], tr/val_loss:  1.462995/  1.826387, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-571 lr=['0.0010000'], tr/val_loss:  1.463090/  1.824579, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-572 lr=['0.0010000'], tr/val_loss:  1.462909/  1.826510, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-573 lr=['0.0010000'], tr/val_loss:  1.462966/  1.825457, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-574 lr=['0.0010000'], tr/val_loss:  1.462819/  1.824927, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-575 lr=['0.0010000'], tr/val_loss:  1.462821/  1.826509, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-576 lr=['0.0010000'], tr/val_loss:  1.462806/  1.823460, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-577 lr=['0.0010000'], tr/val_loss:  1.463053/  1.824428, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-578 lr=['0.0010000'], tr/val_loss:  1.462833/  1.822224, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-579 lr=['0.0010000'], tr/val_loss:  1.462964/  1.822992, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-580 lr=['0.0010000'], tr/val_loss:  1.462892/  1.822893, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-581 lr=['0.0010000'], tr/val_loss:  1.462961/  1.823306, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-582 lr=['0.0010000'], tr/val_loss:  1.462980/  1.822887, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-583 lr=['0.0010000'], tr/val_loss:  1.462874/  1.820457, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-584 lr=['0.0010000'], tr/val_loss:  1.462908/  1.820645, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-585 lr=['0.0010000'], tr/val_loss:  1.462920/  1.822100, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-586 lr=['0.0010000'], tr/val_loss:  1.463078/  1.822333, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-587 lr=['0.0010000'], tr/val_loss:  1.462922/  1.820275, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-588 lr=['0.0010000'], tr/val_loss:  1.462950/  1.821494, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-589 lr=['0.0010000'], tr/val_loss:  1.462707/  1.820966, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-590 lr=['0.0010000'], tr/val_loss:  1.462734/  1.819776, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-591 lr=['0.0010000'], tr/val_loss:  1.462805/  1.820193, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-592 lr=['0.0010000'], tr/val_loss:  1.462927/  1.819624, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-593 lr=['0.0010000'], tr/val_loss:  1.462849/  1.822037, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-594 lr=['0.0010000'], tr/val_loss:  1.462880/  1.818750, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-595 lr=['0.0010000'], tr/val_loss:  1.462823/  1.819888, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-596 lr=['0.0010000'], tr/val_loss:  1.462762/  1.821205, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-597 lr=['0.0010000'], tr/val_loss:  1.462867/  1.818767, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-598 lr=['0.0010000'], tr/val_loss:  1.462908/  1.820442, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-599 lr=['0.0010000'], tr/val_loss:  1.462895/  1.822283, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-600 lr=['0.0010000'], tr/val_loss:  1.462809/  1.820545, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-601 lr=['0.0010000'], tr/val_loss:  1.462653/  1.819717, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-602 lr=['0.0010000'], tr/val_loss:  1.462726/  1.821056, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-603 lr=['0.0010000'], tr/val_loss:  1.462766/  1.818350, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-604 lr=['0.0010000'], tr/val_loss:  1.462942/  1.818030, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-605 lr=['0.0010000'], tr/val_loss:  1.462781/  1.821882, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-606 lr=['0.0010000'], tr/val_loss:  1.462798/  1.820533, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-607 lr=['0.0010000'], tr/val_loss:  1.462866/  1.823432, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-608 lr=['0.0010000'], tr/val_loss:  1.462754/  1.820178, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-609 lr=['0.0010000'], tr/val_loss:  1.462725/  1.820387, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-610 lr=['0.0010000'], tr/val_loss:  1.462797/  1.823069, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-611 lr=['0.0010000'], tr/val_loss:  1.462740/  1.821740, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-612 lr=['0.0010000'], tr/val_loss:  1.462740/  1.819936, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-613 lr=['0.0010000'], tr/val_loss:  1.462740/  1.818446, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-614 lr=['0.0010000'], tr/val_loss:  1.462738/  1.821208, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-615 lr=['0.0010000'], tr/val_loss:  1.462740/  1.818995, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-616 lr=['0.0010000'], tr/val_loss:  1.462825/  1.818154, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-617 lr=['0.0010000'], tr/val_loss:  1.462754/  1.819568, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-618 lr=['0.0010000'], tr/val_loss:  1.462710/  1.820868, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-619 lr=['0.0010000'], tr/val_loss:  1.462724/  1.818179, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-620 lr=['0.0010000'], tr/val_loss:  1.462772/  1.821306, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-621 lr=['0.0010000'], tr/val_loss:  1.462837/  1.819382, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-622 lr=['0.0010000'], tr/val_loss:  1.462800/  1.818673, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-623 lr=['0.0010000'], tr/val_loss:  1.462667/  1.817917, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-624 lr=['0.0010000'], tr/val_loss:  1.462740/  1.820603, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-625 lr=['0.0010000'], tr/val_loss:  1.462768/  1.820099, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-626 lr=['0.0010000'], tr/val_loss:  1.462626/  1.815582, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-627 lr=['0.0010000'], tr/val_loss:  1.462768/  1.817808, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-628 lr=['0.0010000'], tr/val_loss:  1.462825/  1.819224, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-629 lr=['0.0010000'], tr/val_loss:  1.462825/  1.816590, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-630 lr=['0.0010000'], tr/val_loss:  1.462754/  1.815600, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-631 lr=['0.0010000'], tr/val_loss:  1.462695/  1.818148, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-632 lr=['0.0010000'], tr/val_loss:  1.462958/  1.819770, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-633 lr=['0.0010000'], tr/val_loss:  1.462898/  1.818696, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-634 lr=['0.0010000'], tr/val_loss:  1.462768/  1.821833, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-635 lr=['0.0010000'], tr/val_loss:  1.462908/  1.819798, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-636 lr=['0.0010000'], tr/val_loss:  1.462813/  1.818087, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-637 lr=['0.0010000'], tr/val_loss:  1.462722/  1.819848, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-638 lr=['0.0010000'], tr/val_loss:  1.462853/  1.819023, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-639 lr=['0.0010000'], tr/val_loss:  1.462853/  1.819363, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-640 lr=['0.0010000'], tr/val_loss:  1.462667/  1.818498, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-641 lr=['0.0010000'], tr/val_loss:  1.462768/  1.819946, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-642 lr=['0.0010000'], tr/val_loss:  1.463138/  1.818711, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-643 lr=['0.0010000'], tr/val_loss:  1.462768/  1.818448, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-644 lr=['0.0010000'], tr/val_loss:  1.462865/  1.819884, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-645 lr=['0.0010000'], tr/val_loss:  1.462639/  1.818328, tr: 100.00%, tr_best: 100.00%, val:  78.33%, val_best:  78.75%\n",
      "epoch-646 lr=['0.0010000'], tr/val_loss:  1.463027/  1.817341, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-647 lr=['0.0010000'], tr/val_loss:  1.462812/  1.814531, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-648 lr=['0.0010000'], tr/val_loss:  1.462737/  1.819851, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-649 lr=['0.0010000'], tr/val_loss:  1.462827/  1.817994, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-650 lr=['0.0010000'], tr/val_loss:  1.462724/  1.818987, tr: 100.00%, tr_best: 100.00%, val:  78.33%, val_best:  78.75%\n",
      "epoch-651 lr=['0.0010000'], tr/val_loss:  1.463234/  1.818839, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-652 lr=['0.0010000'], tr/val_loss:  1.462752/  1.820006, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-653 lr=['0.0010000'], tr/val_loss:  1.462738/  1.818871, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-654 lr=['0.0010000'], tr/val_loss:  1.462793/  1.820096, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-655 lr=['0.0010000'], tr/val_loss:  1.462667/  1.817821, tr: 100.00%, tr_best: 100.00%, val:  78.33%, val_best:  78.75%\n",
      "epoch-656 lr=['0.0010000'], tr/val_loss:  1.462711/  1.817879, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-657 lr=['0.0010000'], tr/val_loss:  1.462759/  1.818420, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-658 lr=['0.0010000'], tr/val_loss:  1.462693/  1.819463, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-659 lr=['0.0010000'], tr/val_loss:  1.462653/  1.818105, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-660 lr=['0.0010000'], tr/val_loss:  1.462653/  1.816289, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-661 lr=['0.0010000'], tr/val_loss:  1.462653/  1.821086, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-662 lr=['0.0010000'], tr/val_loss:  1.462713/  1.818604, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-663 lr=['0.0010000'], tr/val_loss:  1.462697/  1.817515, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-664 lr=['0.0010000'], tr/val_loss:  1.462626/  1.816506, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-665 lr=['0.0010000'], tr/val_loss:  1.462811/  1.816074, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-666 lr=['0.0010000'], tr/val_loss:  1.462713/  1.818262, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-667 lr=['0.0010000'], tr/val_loss:  1.462612/  1.820683, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-668 lr=['0.0010000'], tr/val_loss:  1.462653/  1.814637, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-669 lr=['0.0010000'], tr/val_loss:  1.462681/  1.815928, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-670 lr=['0.0010000'], tr/val_loss:  1.462626/  1.819003, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-671 lr=['0.0010000'], tr/val_loss:  1.462612/  1.816134, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-672 lr=['0.0010000'], tr/val_loss:  1.462626/  1.814238, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-673 lr=['0.0010000'], tr/val_loss:  1.462640/  1.815461, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-674 lr=['0.0010000'], tr/val_loss:  1.462724/  1.816391, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-675 lr=['0.0010000'], tr/val_loss:  1.463485/  1.817814, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-676 lr=['0.0010000'], tr/val_loss:  1.462598/  1.814748, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-677 lr=['0.0010000'], tr/val_loss:  1.462683/  1.818775, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-678 lr=['0.0010000'], tr/val_loss:  1.462626/  1.817276, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-679 lr=['0.0010000'], tr/val_loss:  1.462669/  1.817861, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-680 lr=['0.0010000'], tr/val_loss:  1.462754/  1.818335, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-681 lr=['0.0010000'], tr/val_loss:  1.462639/  1.819060, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-682 lr=['0.0010000'], tr/val_loss:  1.462738/  1.816795, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-683 lr=['0.0010000'], tr/val_loss:  1.462655/  1.817263, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-684 lr=['0.0010000'], tr/val_loss:  1.462724/  1.818866, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-685 lr=['0.0010000'], tr/val_loss:  1.462640/  1.818515, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-686 lr=['0.0010000'], tr/val_loss:  1.462669/  1.821411, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-687 lr=['0.0010000'], tr/val_loss:  1.462584/  1.817323, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-688 lr=['0.0010000'], tr/val_loss:  1.463025/  1.817165, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-689 lr=['0.0010000'], tr/val_loss:  1.462571/  1.821561, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-690 lr=['0.0010000'], tr/val_loss:  1.462683/  1.816099, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-691 lr=['0.0010000'], tr/val_loss:  1.463150/  1.819759, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-692 lr=['0.0010000'], tr/val_loss:  1.462598/  1.821409, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-693 lr=['0.0010000'], tr/val_loss:  1.462713/  1.819168, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-694 lr=['0.0010000'], tr/val_loss:  1.462759/  1.822617, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-695 lr=['0.0010000'], tr/val_loss:  1.462585/  1.820047, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-696 lr=['0.0010000'], tr/val_loss:  1.462571/  1.820568, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-697 lr=['0.0010000'], tr/val_loss:  1.462612/  1.819754, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-698 lr=['0.0010000'], tr/val_loss:  1.462557/  1.819424, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-699 lr=['0.0010000'], tr/val_loss:  1.463095/  1.820643, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-700 lr=['0.0010000'], tr/val_loss:  1.462612/  1.818758, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-701 lr=['0.0010000'], tr/val_loss:  1.462626/  1.820682, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-702 lr=['0.0010000'], tr/val_loss:  1.462598/  1.821745, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-703 lr=['0.0010000'], tr/val_loss:  1.462948/  1.820552, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-704 lr=['0.0010000'], tr/val_loss:  1.462525/  1.818920, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-705 lr=['0.0010000'], tr/val_loss:  1.462511/  1.816645, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-706 lr=['0.0010000'], tr/val_loss:  1.462584/  1.818533, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-707 lr=['0.0010000'], tr/val_loss:  1.462497/  1.816162, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-708 lr=['0.0010000'], tr/val_loss:  1.462612/  1.817403, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-709 lr=['0.0010000'], tr/val_loss:  1.462557/  1.817501, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-710 lr=['0.0010000'], tr/val_loss:  1.462456/  1.818055, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-711 lr=['0.0010000'], tr/val_loss:  1.462685/  1.817344, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-712 lr=['0.0010000'], tr/val_loss:  1.462568/  1.818909, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-713 lr=['0.0010000'], tr/val_loss:  1.462568/  1.818299, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-714 lr=['0.0010000'], tr/val_loss:  1.462557/  1.819228, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-715 lr=['0.0010000'], tr/val_loss:  1.462555/  1.822129, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-716 lr=['0.0010000'], tr/val_loss:  1.462497/  1.820261, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-717 lr=['0.0010000'], tr/val_loss:  1.462484/  1.819146, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-718 lr=['0.0010000'], tr/val_loss:  1.462470/  1.818865, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-719 lr=['0.0010000'], tr/val_loss:  1.462456/  1.818465, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-720 lr=['0.0010000'], tr/val_loss:  1.462497/  1.815783, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-721 lr=['0.0010000'], tr/val_loss:  1.462612/  1.819424, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-722 lr=['0.0010000'], tr/val_loss:  1.462543/  1.818011, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-723 lr=['0.0010000'], tr/val_loss:  1.462470/  1.819582, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-724 lr=['0.0010000'], tr/val_loss:  1.462641/  1.819130, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-725 lr=['0.0010000'], tr/val_loss:  1.462484/  1.820693, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-726 lr=['0.0010000'], tr/val_loss:  1.462557/  1.818896, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-727 lr=['0.0010000'], tr/val_loss:  1.462428/  1.821125, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-728 lr=['0.0010000'], tr/val_loss:  1.462483/  1.819520, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-729 lr=['0.0010000'], tr/val_loss:  1.462543/  1.820495, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-730 lr=['0.0010000'], tr/val_loss:  1.462456/  1.820067, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-731 lr=['0.0010000'], tr/val_loss:  1.462456/  1.821132, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-732 lr=['0.0010000'], tr/val_loss:  1.462442/  1.820478, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-733 lr=['0.0010000'], tr/val_loss:  1.462456/  1.819420, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-734 lr=['0.0010000'], tr/val_loss:  1.462470/  1.819850, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-735 lr=['0.0010000'], tr/val_loss:  1.462456/  1.821710, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-736 lr=['0.0010000'], tr/val_loss:  1.462428/  1.822764, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-737 lr=['0.0010000'], tr/val_loss:  1.462569/  1.822914, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-738 lr=['0.0010000'], tr/val_loss:  1.462442/  1.819927, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-739 lr=['0.0010000'], tr/val_loss:  1.462456/  1.824838, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-740 lr=['0.0010000'], tr/val_loss:  1.462410/  1.819099, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-741 lr=['0.0010000'], tr/val_loss:  1.462527/  1.819948, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-742 lr=['0.0010000'], tr/val_loss:  1.462499/  1.821562, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-743 lr=['0.0010000'], tr/val_loss:  1.462571/  1.821261, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-744 lr=['0.0010000'], tr/val_loss:  1.462438/  1.823681, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-745 lr=['0.0010000'], tr/val_loss:  1.462541/  1.821405, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-746 lr=['0.0010000'], tr/val_loss:  1.462383/  1.820090, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-747 lr=['0.0010000'], tr/val_loss:  1.462396/  1.819654, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-748 lr=['0.0010000'], tr/val_loss:  1.462610/  1.820686, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-749 lr=['0.0010000'], tr/val_loss:  1.462481/  1.821594, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-750 lr=['0.0010000'], tr/val_loss:  1.462585/  1.821335, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-751 lr=['0.0010000'], tr/val_loss:  1.462484/  1.821638, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-752 lr=['0.0010000'], tr/val_loss:  1.462571/  1.821136, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-753 lr=['0.0010000'], tr/val_loss:  1.462383/  1.820472, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-754 lr=['0.0010000'], tr/val_loss:  1.462582/  1.823458, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-755 lr=['0.0010000'], tr/val_loss:  1.462569/  1.822634, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-756 lr=['0.0010000'], tr/val_loss:  1.462410/  1.818461, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-757 lr=['0.0010000'], tr/val_loss:  1.462355/  1.818910, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-758 lr=['0.0010000'], tr/val_loss:  1.462555/  1.819941, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-759 lr=['0.0010000'], tr/val_loss:  1.462397/  1.821892, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-760 lr=['0.0010000'], tr/val_loss:  1.462628/  1.823421, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-761 lr=['0.0010000'], tr/val_loss:  1.462557/  1.818596, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-762 lr=['0.0010000'], tr/val_loss:  1.462456/  1.819037, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-763 lr=['0.0010000'], tr/val_loss:  1.462554/  1.820148, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-764 lr=['0.0010000'], tr/val_loss:  1.462454/  1.820279, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-765 lr=['0.0010000'], tr/val_loss:  1.462369/  1.818220, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-766 lr=['0.0010000'], tr/val_loss:  1.462555/  1.821672, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-767 lr=['0.0010000'], tr/val_loss:  1.462481/  1.820918, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-768 lr=['0.0010000'], tr/val_loss:  1.462584/  1.819871, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-769 lr=['0.0010000'], tr/val_loss:  1.462355/  1.821427, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-770 lr=['0.0010000'], tr/val_loss:  1.462396/  1.818232, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-771 lr=['0.0010000'], tr/val_loss:  1.462383/  1.820408, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-772 lr=['0.0010000'], tr/val_loss:  1.462314/  1.820178, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-773 lr=['0.0010000'], tr/val_loss:  1.462396/  1.820139, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-774 lr=['0.0010000'], tr/val_loss:  1.462778/  1.822673, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-775 lr=['0.0010000'], tr/val_loss:  1.462328/  1.821280, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-776 lr=['0.0010000'], tr/val_loss:  1.462456/  1.820450, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-777 lr=['0.0010000'], tr/val_loss:  1.462314/  1.821572, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-778 lr=['0.0010000'], tr/val_loss:  1.462300/  1.821775, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-779 lr=['0.0010000'], tr/val_loss:  1.462341/  1.820149, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-780 lr=['0.0010000'], tr/val_loss:  1.462314/  1.820809, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-781 lr=['0.0010000'], tr/val_loss:  1.462373/  1.819825, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-782 lr=['0.0010000'], tr/val_loss:  1.462355/  1.821640, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-783 lr=['0.0010000'], tr/val_loss:  1.462300/  1.822051, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-784 lr=['0.0010000'], tr/val_loss:  1.462300/  1.822173, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-785 lr=['0.0010000'], tr/val_loss:  1.462314/  1.821067, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-786 lr=['0.0010000'], tr/val_loss:  1.462300/  1.822720, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-787 lr=['0.0010000'], tr/val_loss:  1.462415/  1.821820, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-788 lr=['0.0010000'], tr/val_loss:  1.462286/  1.823327, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-789 lr=['0.0010000'], tr/val_loss:  1.462286/  1.821171, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-790 lr=['0.0010000'], tr/val_loss:  1.462330/  1.823287, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-791 lr=['0.0010000'], tr/val_loss:  1.462286/  1.825972, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-792 lr=['0.0010000'], tr/val_loss:  1.462286/  1.824846, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-793 lr=['0.0010000'], tr/val_loss:  1.462259/  1.824466, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-794 lr=['0.0010000'], tr/val_loss:  1.462245/  1.824623, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-795 lr=['0.0010000'], tr/val_loss:  1.462272/  1.823564, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-796 lr=['0.0010000'], tr/val_loss:  1.462348/  1.826446, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-797 lr=['0.0010000'], tr/val_loss:  1.462332/  1.823966, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-798 lr=['0.0010000'], tr/val_loss:  1.462314/  1.823275, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-799 lr=['0.0010000'], tr/val_loss:  1.462273/  1.823746, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-800 lr=['0.0010000'], tr/val_loss:  1.462245/  1.823297, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-801 lr=['0.0010000'], tr/val_loss:  1.462245/  1.822687, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-802 lr=['0.0010000'], tr/val_loss:  1.462286/  1.826395, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-803 lr=['0.0010000'], tr/val_loss:  1.462343/  1.825429, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-804 lr=['0.0010000'], tr/val_loss:  1.462286/  1.821764, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-805 lr=['0.0010000'], tr/val_loss:  1.462245/  1.824917, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-806 lr=['0.0010000'], tr/val_loss:  1.462359/  1.824152, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-807 lr=['0.0010000'], tr/val_loss:  1.462259/  1.826330, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-808 lr=['0.0010000'], tr/val_loss:  1.462258/  1.824011, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-809 lr=['0.0010000'], tr/val_loss:  1.462247/  1.822889, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-810 lr=['0.0010000'], tr/val_loss:  1.462346/  1.824897, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-811 lr=['0.0010000'], tr/val_loss:  1.462259/  1.823252, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-812 lr=['0.0010000'], tr/val_loss:  1.462217/  1.822519, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-813 lr=['0.0010000'], tr/val_loss:  1.462417/  1.823951, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-814 lr=['0.0010000'], tr/val_loss:  1.462332/  1.823142, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-815 lr=['0.0010000'], tr/val_loss:  1.462231/  1.822716, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-816 lr=['0.0010000'], tr/val_loss:  1.462231/  1.823094, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-817 lr=['0.0010000'], tr/val_loss:  1.462258/  1.823886, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-818 lr=['0.0010000'], tr/val_loss:  1.462318/  1.823734, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-819 lr=['0.0010000'], tr/val_loss:  1.462245/  1.823372, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-820 lr=['0.0010000'], tr/val_loss:  1.462259/  1.821470, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-821 lr=['0.0010000'], tr/val_loss:  1.462245/  1.823333, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-822 lr=['0.0010000'], tr/val_loss:  1.462357/  1.822237, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-823 lr=['0.0010000'], tr/val_loss:  1.462259/  1.822645, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-824 lr=['0.0010000'], tr/val_loss:  1.462259/  1.820402, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-825 lr=['0.0010000'], tr/val_loss:  1.462245/  1.823657, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-826 lr=['0.0010000'], tr/val_loss:  1.462231/  1.819558, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-827 lr=['0.0010000'], tr/val_loss:  1.462231/  1.820859, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-828 lr=['0.0010000'], tr/val_loss:  1.462343/  1.821904, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-829 lr=['0.0010000'], tr/val_loss:  1.462332/  1.818952, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-830 lr=['0.0010000'], tr/val_loss:  1.462272/  1.822561, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-831 lr=['0.0010000'], tr/val_loss:  1.462343/  1.822105, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-832 lr=['0.0010000'], tr/val_loss:  1.462231/  1.820308, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-833 lr=['0.0010000'], tr/val_loss:  1.462130/  1.820179, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-834 lr=['0.0010000'], tr/val_loss:  1.462245/  1.820471, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-835 lr=['0.0010000'], tr/val_loss:  1.462709/  1.820351, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-836 lr=['0.0010000'], tr/val_loss:  1.462174/  1.819695, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-837 lr=['0.0010000'], tr/val_loss:  1.462217/  1.816108, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-838 lr=['0.0010000'], tr/val_loss:  1.462261/  1.817240, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-839 lr=['0.0010000'], tr/val_loss:  1.462286/  1.817207, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-840 lr=['0.0010000'], tr/val_loss:  1.462330/  1.818347, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-841 lr=['0.0010000'], tr/val_loss:  1.462172/  1.821802, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-842 lr=['0.0010000'], tr/val_loss:  1.462201/  1.818041, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-843 lr=['0.0010000'], tr/val_loss:  1.462245/  1.820230, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-844 lr=['0.0010000'], tr/val_loss:  1.462289/  1.817347, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-845 lr=['0.0010000'], tr/val_loss:  1.462186/  1.817866, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-846 lr=['0.0010000'], tr/val_loss:  1.462288/  1.818256, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-847 lr=['0.0010000'], tr/val_loss:  1.462160/  1.818071, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-848 lr=['0.0010000'], tr/val_loss:  1.462188/  1.819296, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-849 lr=['0.0010000'], tr/val_loss:  1.462201/  1.819858, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-850 lr=['0.0010000'], tr/val_loss:  1.462245/  1.819464, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-851 lr=['0.0010000'], tr/val_loss:  1.462174/  1.820382, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-852 lr=['0.0010000'], tr/val_loss:  1.462185/  1.819602, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-853 lr=['0.0010000'], tr/val_loss:  1.462231/  1.818163, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-854 lr=['0.0010000'], tr/val_loss:  1.462229/  1.818588, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-855 lr=['0.0010000'], tr/val_loss:  1.462201/  1.819727, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-856 lr=['0.0010000'], tr/val_loss:  1.462245/  1.819565, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-857 lr=['0.0010000'], tr/val_loss:  1.462243/  1.818038, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-858 lr=['0.0010000'], tr/val_loss:  1.462245/  1.820648, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-859 lr=['0.0010000'], tr/val_loss:  1.462245/  1.820930, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-860 lr=['0.0010000'], tr/val_loss:  1.462142/  1.819176, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-861 lr=['0.0010000'], tr/val_loss:  1.462188/  1.817110, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-862 lr=['0.0010000'], tr/val_loss:  1.462128/  1.818069, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-863 lr=['0.0010000'], tr/val_loss:  1.462128/  1.815903, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-864 lr=['0.0010000'], tr/val_loss:  1.462114/  1.815962, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-865 lr=['0.0010000'], tr/val_loss:  1.462160/  1.819609, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-866 lr=['0.0010000'], tr/val_loss:  1.462114/  1.818079, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-867 lr=['0.0010000'], tr/val_loss:  1.462115/  1.818842, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-868 lr=['0.0010000'], tr/val_loss:  1.462126/  1.819837, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-869 lr=['0.0010000'], tr/val_loss:  1.462185/  1.819994, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-870 lr=['0.0010000'], tr/val_loss:  1.462160/  1.817975, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-871 lr=['0.0010000'], tr/val_loss:  1.462087/  1.820029, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-872 lr=['0.0010000'], tr/val_loss:  1.462073/  1.818307, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-873 lr=['0.0010000'], tr/val_loss:  1.462059/  1.819963, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-874 lr=['0.0010000'], tr/val_loss:  1.462160/  1.819429, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-875 lr=['0.0010000'], tr/val_loss:  1.462128/  1.819390, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-876 lr=['0.0010000'], tr/val_loss:  1.462188/  1.819668, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-877 lr=['0.0010000'], tr/val_loss:  1.462114/  1.819404, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-878 lr=['0.0010000'], tr/val_loss:  1.462073/  1.820526, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-879 lr=['0.0010000'], tr/val_loss:  1.462073/  1.821089, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-880 lr=['0.0010000'], tr/val_loss:  1.462183/  1.819090, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-881 lr=['0.0010000'], tr/val_loss:  1.462045/  1.819092, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-882 lr=['0.0010000'], tr/val_loss:  1.462073/  1.822073, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-883 lr=['0.0010000'], tr/val_loss:  1.461972/  1.819698, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-884 lr=['0.0010000'], tr/val_loss:  1.461986/  1.821605, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-885 lr=['0.0010000'], tr/val_loss:  1.462027/  1.820936, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-886 lr=['0.0010000'], tr/val_loss:  1.461972/  1.822141, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-887 lr=['0.0010000'], tr/val_loss:  1.462000/  1.819966, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-888 lr=['0.0010000'], tr/val_loss:  1.462059/  1.821170, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-889 lr=['0.0010000'], tr/val_loss:  1.462087/  1.820769, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-890 lr=['0.0010000'], tr/val_loss:  1.462000/  1.819681, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-891 lr=['0.0010000'], tr/val_loss:  1.461986/  1.820801, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-892 lr=['0.0010000'], tr/val_loss:  1.462220/  1.820148, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-893 lr=['0.0010000'], tr/val_loss:  1.462000/  1.818935, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-894 lr=['0.0010000'], tr/val_loss:  1.462000/  1.819573, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-895 lr=['0.0010000'], tr/val_loss:  1.462085/  1.819256, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-896 lr=['0.0010000'], tr/val_loss:  1.462014/  1.817486, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-897 lr=['0.0010000'], tr/val_loss:  1.462085/  1.818311, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-898 lr=['0.0010000'], tr/val_loss:  1.462200/  1.820486, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-899 lr=['0.0010000'], tr/val_loss:  1.462113/  1.821004, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-900 lr=['0.0010000'], tr/val_loss:  1.462099/  1.820602, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-901 lr=['0.0010000'], tr/val_loss:  1.462174/  1.820954, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-902 lr=['0.0010000'], tr/val_loss:  1.462172/  1.822146, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-903 lr=['0.0010000'], tr/val_loss:  1.462159/  1.818513, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-904 lr=['0.0010000'], tr/val_loss:  1.462085/  1.819798, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-905 lr=['0.0010000'], tr/val_loss:  1.462073/  1.822713, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-906 lr=['0.0010000'], tr/val_loss:  1.462012/  1.823038, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-907 lr=['0.0010000'], tr/val_loss:  1.462071/  1.820272, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-908 lr=['0.0010000'], tr/val_loss:  1.462174/  1.823188, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-909 lr=['0.0010000'], tr/val_loss:  1.462026/  1.823447, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-910 lr=['0.0010000'], tr/val_loss:  1.462158/  1.821824, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-911 lr=['0.0010000'], tr/val_loss:  1.462014/  1.822844, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-912 lr=['0.0010000'], tr/val_loss:  1.462085/  1.819947, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-913 lr=['0.0010000'], tr/val_loss:  1.462933/  1.821300, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-914 lr=['0.0010000'], tr/val_loss:  1.462040/  1.818710, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-915 lr=['0.0010000'], tr/val_loss:  1.462113/  1.819105, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-916 lr=['0.0010000'], tr/val_loss:  1.462045/  1.822677, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-917 lr=['0.0010000'], tr/val_loss:  1.462113/  1.820597, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-918 lr=['0.0010000'], tr/val_loss:  1.461984/  1.820377, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-919 lr=['0.0010000'], tr/val_loss:  1.462085/  1.822150, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-920 lr=['0.0010000'], tr/val_loss:  1.462059/  1.822140, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-921 lr=['0.0010000'], tr/val_loss:  1.462085/  1.822196, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-922 lr=['0.0010000'], tr/val_loss:  1.461945/  1.822863, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-923 lr=['0.0010000'], tr/val_loss:  1.462172/  1.820204, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-924 lr=['0.0010000'], tr/val_loss:  1.462059/  1.823080, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-925 lr=['0.0010000'], tr/val_loss:  1.462085/  1.820554, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-926 lr=['0.0010000'], tr/val_loss:  1.462232/  1.822281, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-927 lr=['0.0010000'], tr/val_loss:  1.462085/  1.822099, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-928 lr=['0.0010000'], tr/val_loss:  1.462105/  1.822546, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-929 lr=['0.0010000'], tr/val_loss:  1.462071/  1.821323, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-930 lr=['0.0010000'], tr/val_loss:  1.461945/  1.823298, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-931 lr=['0.0010000'], tr/val_loss:  1.462044/  1.823465, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-932 lr=['0.0010000'], tr/val_loss:  1.462133/  1.823574, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-933 lr=['0.0010000'], tr/val_loss:  1.462145/  1.822152, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-934 lr=['0.0010000'], tr/val_loss:  1.462091/  1.823355, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-935 lr=['0.0010000'], tr/val_loss:  1.462030/  1.822127, tr: 100.00%, tr_best: 100.00%, val:  74.17%, val_best:  78.75%\n",
      "epoch-936 lr=['0.0010000'], tr/val_loss:  1.462044/  1.823436, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-937 lr=['0.0010000'], tr/val_loss:  1.462018/  1.823486, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-938 lr=['0.0010000'], tr/val_loss:  1.462044/  1.821304, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-939 lr=['0.0010000'], tr/val_loss:  1.462058/  1.822226, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-940 lr=['0.0010000'], tr/val_loss:  1.462119/  1.823569, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-941 lr=['0.0010000'], tr/val_loss:  1.462119/  1.821094, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-942 lr=['0.0010000'], tr/val_loss:  1.462132/  1.823583, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-943 lr=['0.0010000'], tr/val_loss:  1.462044/  1.821459, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-944 lr=['0.0010000'], tr/val_loss:  1.462105/  1.824650, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-945 lr=['0.0010000'], tr/val_loss:  1.462030/  1.823745, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-946 lr=['0.0010000'], tr/val_loss:  1.462030/  1.823322, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-947 lr=['0.0010000'], tr/val_loss:  1.462105/  1.821878, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-948 lr=['0.0010000'], tr/val_loss:  1.462117/  1.825013, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-949 lr=['0.0010000'], tr/val_loss:  1.461990/  1.824599, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-950 lr=['0.0010000'], tr/val_loss:  1.462032/  1.824710, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-951 lr=['0.0010000'], tr/val_loss:  1.462105/  1.824320, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-952 lr=['0.0010000'], tr/val_loss:  1.462131/  1.823601, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-953 lr=['0.0010000'], tr/val_loss:  1.462105/  1.822613, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-954 lr=['0.0010000'], tr/val_loss:  1.462105/  1.822963, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-955 lr=['0.0010000'], tr/val_loss:  1.462105/  1.826962, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-956 lr=['0.0010000'], tr/val_loss:  1.462132/  1.825793, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-957 lr=['0.0010000'], tr/val_loss:  1.462091/  1.823924, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-958 lr=['0.0010000'], tr/val_loss:  1.462091/  1.824318, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-959 lr=['0.0010000'], tr/val_loss:  1.462131/  1.824584, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-960 lr=['0.0010000'], tr/val_loss:  1.462105/  1.823980, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-961 lr=['0.0010000'], tr/val_loss:  1.462105/  1.822458, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-962 lr=['0.0010000'], tr/val_loss:  1.462091/  1.826266, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-963 lr=['0.0010000'], tr/val_loss:  1.462103/  1.825926, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-964 lr=['0.0010000'], tr/val_loss:  1.462091/  1.825387, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-965 lr=['0.0010000'], tr/val_loss:  1.461990/  1.825155, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-966 lr=['0.0010000'], tr/val_loss:  1.462091/  1.826609, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-967 lr=['0.0010000'], tr/val_loss:  1.462105/  1.824815, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-968 lr=['0.0010000'], tr/val_loss:  1.462105/  1.823743, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-969 lr=['0.0010000'], tr/val_loss:  1.462089/  1.824157, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-970 lr=['0.0010000'], tr/val_loss:  1.462077/  1.826238, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-971 lr=['0.0010000'], tr/val_loss:  1.462091/  1.827793, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-972 lr=['0.0010000'], tr/val_loss:  1.462077/  1.824803, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-973 lr=['0.0010000'], tr/val_loss:  1.462091/  1.822283, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-974 lr=['0.0010000'], tr/val_loss:  1.462077/  1.824824, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-975 lr=['0.0010000'], tr/val_loss:  1.462151/  1.824408, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-976 lr=['0.0010000'], tr/val_loss:  1.462063/  1.825431, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-977 lr=['0.0010000'], tr/val_loss:  1.462077/  1.823091, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-978 lr=['0.0010000'], tr/val_loss:  1.462091/  1.824124, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-979 lr=['0.0010000'], tr/val_loss:  1.462089/  1.823849, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-980 lr=['0.0010000'], tr/val_loss:  1.462091/  1.825592, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-981 lr=['0.0010000'], tr/val_loss:  1.462064/  1.825567, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-982 lr=['0.0010000'], tr/val_loss:  1.462077/  1.825277, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-983 lr=['0.0010000'], tr/val_loss:  1.462542/  1.826017, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-984 lr=['0.0010000'], tr/val_loss:  1.462077/  1.825343, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-985 lr=['0.0010000'], tr/val_loss:  1.462050/  1.827941, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-986 lr=['0.0010000'], tr/val_loss:  1.462050/  1.823872, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-987 lr=['0.0010000'], tr/val_loss:  1.462050/  1.824729, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-988 lr=['0.0010000'], tr/val_loss:  1.462077/  1.825923, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-989 lr=['0.0010000'], tr/val_loss:  1.462050/  1.825851, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-990 lr=['0.0010000'], tr/val_loss:  1.462063/  1.827817, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-991 lr=['0.0010000'], tr/val_loss:  1.462063/  1.827495, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-992 lr=['0.0010000'], tr/val_loss:  1.462064/  1.827881, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-993 lr=['0.0010000'], tr/val_loss:  1.462091/  1.825187, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-994 lr=['0.0010000'], tr/val_loss:  1.462050/  1.825570, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-995 lr=['0.0010000'], tr/val_loss:  1.462063/  1.827812, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-996 lr=['0.0010000'], tr/val_loss:  1.462050/  1.826448, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-997 lr=['0.0010000'], tr/val_loss:  1.462050/  1.828753, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-998 lr=['0.0010000'], tr/val_loss:  1.462036/  1.826833, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-999 lr=['0.0010000'], tr/val_loss:  1.462036/  1.826932, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1000 lr=['0.0010000'], tr/val_loss:  1.462022/  1.824843, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1001 lr=['0.0010000'], tr/val_loss:  1.462064/  1.824959, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1002 lr=['0.0010000'], tr/val_loss:  1.462050/  1.824137, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1003 lr=['0.0010000'], tr/val_loss:  1.462036/  1.827377, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1004 lr=['0.0010000'], tr/val_loss:  1.462063/  1.826931, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1005 lr=['0.0010000'], tr/val_loss:  1.462036/  1.823824, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-1006 lr=['0.0010000'], tr/val_loss:  1.462050/  1.828756, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1007 lr=['0.0010000'], tr/val_loss:  1.462036/  1.827039, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1008 lr=['0.0010000'], tr/val_loss:  1.462063/  1.828169, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1009 lr=['0.0010000'], tr/val_loss:  1.462050/  1.827444, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1010 lr=['0.0010000'], tr/val_loss:  1.462036/  1.826625, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1011 lr=['0.0010000'], tr/val_loss:  1.462036/  1.827723, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1012 lr=['0.0010000'], tr/val_loss:  1.462050/  1.827041, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1013 lr=['0.0010000'], tr/val_loss:  1.462036/  1.828183, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1014 lr=['0.0010000'], tr/val_loss:  1.462036/  1.828240, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1015 lr=['0.0010000'], tr/val_loss:  1.462062/  1.828231, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1016 lr=['0.0010000'], tr/val_loss:  1.462036/  1.827715, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1017 lr=['0.0010000'], tr/val_loss:  1.462063/  1.830066, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1018 lr=['0.0010000'], tr/val_loss:  1.462049/  1.828985, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1019 lr=['0.0010000'], tr/val_loss:  1.462036/  1.827422, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1020 lr=['0.0010000'], tr/val_loss:  1.462022/  1.828435, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1021 lr=['0.0010000'], tr/val_loss:  1.462008/  1.828483, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1022 lr=['0.0010000'], tr/val_loss:  1.462022/  1.828966, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1023 lr=['0.0010000'], tr/val_loss:  1.462036/  1.831154, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1024 lr=['0.0010000'], tr/val_loss:  1.462022/  1.827827, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1025 lr=['0.0010000'], tr/val_loss:  1.462008/  1.829695, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1026 lr=['0.0010000'], tr/val_loss:  1.462022/  1.828226, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1027 lr=['0.0010000'], tr/val_loss:  1.462050/  1.831273, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1028 lr=['0.0010000'], tr/val_loss:  1.462063/  1.827576, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1029 lr=['0.0010000'], tr/val_loss:  1.462459/  1.829915, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1030 lr=['0.0010000'], tr/val_loss:  1.462022/  1.828108, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1031 lr=['0.0010000'], tr/val_loss:  1.462034/  1.829900, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1032 lr=['0.0010000'], tr/val_loss:  1.462123/  1.828522, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1033 lr=['0.0010000'], tr/val_loss:  1.462022/  1.829451, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1034 lr=['0.0010000'], tr/val_loss:  1.462082/  1.828848, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-1035 lr=['0.0010000'], tr/val_loss:  1.462008/  1.829740, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1036 lr=['0.0010000'], tr/val_loss:  1.462008/  1.827921, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1037 lr=['0.0010000'], tr/val_loss:  1.462095/  1.828977, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1038 lr=['0.0010000'], tr/val_loss:  1.462022/  1.827962, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1039 lr=['0.0010000'], tr/val_loss:  1.462095/  1.827662, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-1040 lr=['0.0010000'], tr/val_loss:  1.462008/  1.831033, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1041 lr=['0.0010000'], tr/val_loss:  1.462008/  1.827299, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-1042 lr=['0.0010000'], tr/val_loss:  1.462064/  1.828310, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1043 lr=['0.0010000'], tr/val_loss:  1.462123/  1.828606, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-1044 lr=['0.0010000'], tr/val_loss:  1.462109/  1.827514, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-1045 lr=['0.0010000'], tr/val_loss:  1.462123/  1.827288, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-1046 lr=['0.0010000'], tr/val_loss:  1.462095/  1.829421, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1047 lr=['0.0010000'], tr/val_loss:  1.462081/  1.830827, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1048 lr=['0.0010000'], tr/val_loss:  1.462135/  1.829507, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1049 lr=['0.0010000'], tr/val_loss:  1.462036/  1.828355, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-1050 lr=['0.0010000'], tr/val_loss:  1.462022/  1.830866, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1051 lr=['0.0010000'], tr/val_loss:  1.462022/  1.829113, tr: 100.00%, tr_best: 100.00%, val:  77.92%, val_best:  78.75%\n",
      "epoch-1052 lr=['0.0010000'], tr/val_loss:  1.462036/  1.828355, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1053 lr=['0.0010000'], tr/val_loss:  1.462022/  1.827578, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-1054 lr=['0.0010000'], tr/val_loss:  1.462006/  1.829170, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1055 lr=['0.0010000'], tr/val_loss:  1.462020/  1.829964, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1056 lr=['0.0010000'], tr/val_loss:  1.461992/  1.830945, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1057 lr=['0.0010000'], tr/val_loss:  1.462018/  1.830317, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1058 lr=['0.0010000'], tr/val_loss:  1.461992/  1.828862, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-1059 lr=['0.0010000'], tr/val_loss:  1.462020/  1.831233, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1060 lr=['0.0010000'], tr/val_loss:  1.462107/  1.832768, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1061 lr=['0.0010000'], tr/val_loss:  1.462105/  1.829836, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-1062 lr=['0.0010000'], tr/val_loss:  1.462006/  1.831397, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1063 lr=['0.0010000'], tr/val_loss:  1.462443/  1.831557, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1064 lr=['0.0010000'], tr/val_loss:  1.462034/  1.829256, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1065 lr=['0.0010000'], tr/val_loss:  1.461992/  1.830738, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1066 lr=['0.0010000'], tr/val_loss:  1.461992/  1.830214, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1067 lr=['0.0010000'], tr/val_loss:  1.462047/  1.829257, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1068 lr=['0.0010000'], tr/val_loss:  1.462034/  1.832928, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-1069 lr=['0.0010000'], tr/val_loss:  1.462047/  1.832671, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1070 lr=['0.0010000'], tr/val_loss:  1.462034/  1.832736, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1071 lr=['0.0010000'], tr/val_loss:  1.462020/  1.832999, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1072 lr=['0.0010000'], tr/val_loss:  1.462020/  1.832598, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1073 lr=['0.0010000'], tr/val_loss:  1.462034/  1.830292, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1074 lr=['0.0010000'], tr/val_loss:  1.462020/  1.828101, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1075 lr=['0.0010000'], tr/val_loss:  1.462020/  1.830240, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-1076 lr=['0.0010000'], tr/val_loss:  1.462006/  1.829727, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1077 lr=['0.0010000'], tr/val_loss:  1.462006/  1.830908, tr: 100.00%, tr_best: 100.00%, val:  77.50%, val_best:  78.75%\n",
      "epoch-1078 lr=['0.0010000'], tr/val_loss:  1.461992/  1.829361, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1079 lr=['0.0010000'], tr/val_loss:  1.462006/  1.831514, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1080 lr=['0.0010000'], tr/val_loss:  1.462006/  1.829913, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1081 lr=['0.0010000'], tr/val_loss:  1.462020/  1.829626, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1082 lr=['0.0010000'], tr/val_loss:  1.462020/  1.833640, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1083 lr=['0.0010000'], tr/val_loss:  1.461992/  1.833411, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1084 lr=['0.0010000'], tr/val_loss:  1.462018/  1.831555, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1085 lr=['0.0010000'], tr/val_loss:  1.461993/  1.830022, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1086 lr=['0.0010000'], tr/val_loss:  1.461978/  1.831667, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1087 lr=['0.0010000'], tr/val_loss:  1.462006/  1.834590, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1088 lr=['0.0010000'], tr/val_loss:  1.462032/  1.830519, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1089 lr=['0.0010000'], tr/val_loss:  1.462020/  1.830819, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1090 lr=['0.0010000'], tr/val_loss:  1.462006/  1.828304, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1091 lr=['0.0010000'], tr/val_loss:  1.462020/  1.830464, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1092 lr=['0.0010000'], tr/val_loss:  1.461992/  1.829240, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1093 lr=['0.0010000'], tr/val_loss:  1.461979/  1.830993, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1094 lr=['0.0010000'], tr/val_loss:  1.462006/  1.831171, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1095 lr=['0.0010000'], tr/val_loss:  1.462018/  1.831507, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1096 lr=['0.0010000'], tr/val_loss:  1.462006/  1.832924, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1097 lr=['0.0010000'], tr/val_loss:  1.461992/  1.831607, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1098 lr=['0.0010000'], tr/val_loss:  1.462046/  1.831320, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1099 lr=['0.0010000'], tr/val_loss:  1.461993/  1.832585, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1100 lr=['0.0010000'], tr/val_loss:  1.461965/  1.832026, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1101 lr=['0.0010000'], tr/val_loss:  1.461992/  1.832059, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1102 lr=['0.0010000'], tr/val_loss:  1.461992/  1.831786, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1103 lr=['0.0010000'], tr/val_loss:  1.461992/  1.834453, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1104 lr=['0.0010000'], tr/val_loss:  1.462006/  1.834431, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1105 lr=['0.0010000'], tr/val_loss:  1.462018/  1.832962, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1106 lr=['0.0010000'], tr/val_loss:  1.462006/  1.831370, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1107 lr=['0.0010000'], tr/val_loss:  1.462006/  1.833290, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1108 lr=['0.0010000'], tr/val_loss:  1.461992/  1.833057, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1109 lr=['0.0010000'], tr/val_loss:  1.461993/  1.830643, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1110 lr=['0.0010000'], tr/val_loss:  1.462455/  1.831884, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1111 lr=['0.0010000'], tr/val_loss:  1.461992/  1.832939, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1112 lr=['0.0010000'], tr/val_loss:  1.461979/  1.832609, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1113 lr=['0.0010000'], tr/val_loss:  1.461965/  1.833511, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1114 lr=['0.0010000'], tr/val_loss:  1.462006/  1.829079, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1115 lr=['0.0010000'], tr/val_loss:  1.461979/  1.830710, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1116 lr=['0.0010000'], tr/val_loss:  1.461992/  1.830966, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1117 lr=['0.0010000'], tr/val_loss:  1.462006/  1.832800, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1118 lr=['0.0010000'], tr/val_loss:  1.461992/  1.835270, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1119 lr=['0.0010000'], tr/val_loss:  1.462415/  1.832113, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1120 lr=['0.0010000'], tr/val_loss:  1.461992/  1.832320, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1121 lr=['0.0010000'], tr/val_loss:  1.461979/  1.832082, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1122 lr=['0.0010000'], tr/val_loss:  1.461979/  1.832747, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1123 lr=['0.0010000'], tr/val_loss:  1.461992/  1.833274, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1124 lr=['0.0010000'], tr/val_loss:  1.462006/  1.831631, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1125 lr=['0.0010000'], tr/val_loss:  1.461978/  1.832281, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1126 lr=['0.0010000'], tr/val_loss:  1.461965/  1.830455, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1127 lr=['0.0010000'], tr/val_loss:  1.461965/  1.832708, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1128 lr=['0.0010000'], tr/val_loss:  1.461978/  1.829793, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1129 lr=['0.0010000'], tr/val_loss:  1.462006/  1.834098, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1130 lr=['0.0010000'], tr/val_loss:  1.461965/  1.831874, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1131 lr=['0.0010000'], tr/val_loss:  1.461992/  1.832268, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1132 lr=['0.0010000'], tr/val_loss:  1.461992/  1.829900, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1133 lr=['0.0010000'], tr/val_loss:  1.462006/  1.831144, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1134 lr=['0.0010000'], tr/val_loss:  1.461951/  1.831003, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1135 lr=['0.0010000'], tr/val_loss:  1.461965/  1.833194, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1136 lr=['0.0010000'], tr/val_loss:  1.461951/  1.831873, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1137 lr=['0.0010000'], tr/val_loss:  1.461979/  1.831422, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1138 lr=['0.0010000'], tr/val_loss:  1.461978/  1.829193, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1139 lr=['0.0010000'], tr/val_loss:  1.461979/  1.830972, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1140 lr=['0.0010000'], tr/val_loss:  1.461951/  1.831425, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1141 lr=['0.0010000'], tr/val_loss:  1.461965/  1.831019, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1142 lr=['0.0010000'], tr/val_loss:  1.461965/  1.832027, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1143 lr=['0.0010000'], tr/val_loss:  1.461978/  1.831854, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1144 lr=['0.0010000'], tr/val_loss:  1.462006/  1.833045, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1145 lr=['0.0010000'], tr/val_loss:  1.461965/  1.834154, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1146 lr=['0.0010000'], tr/val_loss:  1.461965/  1.832523, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1147 lr=['0.0010000'], tr/val_loss:  1.461951/  1.830649, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1148 lr=['0.0010000'], tr/val_loss:  1.461965/  1.831747, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1149 lr=['0.0010000'], tr/val_loss:  1.461878/  1.830738, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1150 lr=['0.0010000'], tr/val_loss:  1.461864/  1.832915, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1151 lr=['0.0010000'], tr/val_loss:  1.461905/  1.834480, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1152 lr=['0.0010000'], tr/val_loss:  1.461878/  1.834401, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1153 lr=['0.0010000'], tr/val_loss:  1.461891/  1.833140, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1154 lr=['0.0010000'], tr/val_loss:  1.461878/  1.834056, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1155 lr=['0.0010000'], tr/val_loss:  1.461905/  1.833465, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1156 lr=['0.0010000'], tr/val_loss:  1.461905/  1.832571, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1157 lr=['0.0010000'], tr/val_loss:  1.461976/  1.834260, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1158 lr=['0.0010000'], tr/val_loss:  1.461864/  1.835034, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1159 lr=['0.0010000'], tr/val_loss:  1.461864/  1.834604, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1160 lr=['0.0010000'], tr/val_loss:  1.461892/  1.834857, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1161 lr=['0.0010000'], tr/val_loss:  1.461891/  1.836348, tr: 100.00%, tr_best: 100.00%, val:  74.17%, val_best:  78.75%\n",
      "epoch-1162 lr=['0.0010000'], tr/val_loss:  1.461905/  1.835262, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1163 lr=['0.0010000'], tr/val_loss:  1.461933/  1.833459, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1164 lr=['0.0010000'], tr/val_loss:  1.461892/  1.834590, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1165 lr=['0.0010000'], tr/val_loss:  1.461878/  1.834338, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1166 lr=['0.0010000'], tr/val_loss:  1.461791/  1.835574, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1167 lr=['0.0010000'], tr/val_loss:  1.461892/  1.833546, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1168 lr=['0.0010000'], tr/val_loss:  1.461877/  1.833701, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1169 lr=['0.0010000'], tr/val_loss:  1.461891/  1.832666, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1170 lr=['0.0010000'], tr/val_loss:  1.461791/  1.827845, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1171 lr=['0.0010000'], tr/val_loss:  1.461818/  1.831440, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1172 lr=['0.0010000'], tr/val_loss:  1.461791/  1.832809, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1173 lr=['0.0010000'], tr/val_loss:  1.461805/  1.834705, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1174 lr=['0.0010000'], tr/val_loss:  1.461777/  1.833660, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1175 lr=['0.0010000'], tr/val_loss:  1.461791/  1.833745, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1176 lr=['0.0010000'], tr/val_loss:  1.461805/  1.833852, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1177 lr=['0.0010000'], tr/val_loss:  1.461777/  1.832004, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1178 lr=['0.0010000'], tr/val_loss:  1.461791/  1.836695, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1179 lr=['0.0010000'], tr/val_loss:  1.461790/  1.834254, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1180 lr=['0.0010000'], tr/val_loss:  1.461777/  1.834860, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1181 lr=['0.0010000'], tr/val_loss:  1.461832/  1.835557, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1182 lr=['0.0010000'], tr/val_loss:  1.461791/  1.834921, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1183 lr=['0.0010000'], tr/val_loss:  1.461891/  1.833827, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1184 lr=['0.0010000'], tr/val_loss:  1.461791/  1.832541, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1185 lr=['0.0010000'], tr/val_loss:  1.461905/  1.833261, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1186 lr=['0.0010000'], tr/val_loss:  1.461805/  1.833996, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1187 lr=['0.0010000'], tr/val_loss:  1.461804/  1.831937, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1188 lr=['0.0010000'], tr/val_loss:  1.461804/  1.833574, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1189 lr=['0.0010000'], tr/val_loss:  1.461804/  1.833915, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1190 lr=['0.0010000'], tr/val_loss:  1.461791/  1.831619, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1191 lr=['0.0010000'], tr/val_loss:  1.461777/  1.833361, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1192 lr=['0.0010000'], tr/val_loss:  1.461878/  1.834877, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1193 lr=['0.0010000'], tr/val_loss:  1.461777/  1.833379, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1194 lr=['0.0010000'], tr/val_loss:  1.461791/  1.834228, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1195 lr=['0.0010000'], tr/val_loss:  1.461818/  1.833116, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1196 lr=['0.0010000'], tr/val_loss:  1.461878/  1.831807, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1197 lr=['0.0010000'], tr/val_loss:  1.461878/  1.833152, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1198 lr=['0.0010000'], tr/val_loss:  1.461791/  1.832192, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1199 lr=['0.0010000'], tr/val_loss:  1.461791/  1.831502, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1200 lr=['0.0010000'], tr/val_loss:  1.461790/  1.831156, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1201 lr=['0.0010000'], tr/val_loss:  1.461777/  1.831933, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1202 lr=['0.0010000'], tr/val_loss:  1.461777/  1.833167, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1203 lr=['0.0010000'], tr/val_loss:  1.461777/  1.832457, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1204 lr=['0.0010000'], tr/val_loss:  1.461891/  1.832925, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1205 lr=['0.0010000'], tr/val_loss:  1.461979/  1.834013, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1206 lr=['0.0010000'], tr/val_loss:  1.461791/  1.834861, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1207 lr=['0.0010000'], tr/val_loss:  1.461965/  1.834814, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1208 lr=['0.0010000'], tr/val_loss:  1.461905/  1.833611, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1209 lr=['0.0010000'], tr/val_loss:  1.461864/  1.835388, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1210 lr=['0.0010000'], tr/val_loss:  1.461965/  1.834591, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1211 lr=['0.0010000'], tr/val_loss:  1.461951/  1.835376, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1212 lr=['0.0010000'], tr/val_loss:  1.461951/  1.834882, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1213 lr=['0.0010000'], tr/val_loss:  1.461864/  1.834767, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1214 lr=['0.0010000'], tr/val_loss:  1.461878/  1.836792, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1215 lr=['0.0010000'], tr/val_loss:  1.461878/  1.834753, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1216 lr=['0.0010000'], tr/val_loss:  1.461965/  1.835581, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1217 lr=['0.0010000'], tr/val_loss:  1.461878/  1.835446, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1218 lr=['0.0010000'], tr/val_loss:  1.461951/  1.835717, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1219 lr=['0.0010000'], tr/val_loss:  1.461905/  1.835192, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1220 lr=['0.0010000'], tr/val_loss:  1.461951/  1.836592, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1221 lr=['0.0010000'], tr/val_loss:  1.461951/  1.834755, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1222 lr=['0.0010000'], tr/val_loss:  1.461878/  1.835004, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1223 lr=['0.0010000'], tr/val_loss:  1.461850/  1.835678, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1224 lr=['0.0010000'], tr/val_loss:  1.461951/  1.833742, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1225 lr=['0.0010000'], tr/val_loss:  1.461964/  1.833608, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1226 lr=['0.0010000'], tr/val_loss:  1.461878/  1.835874, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1227 lr=['0.0010000'], tr/val_loss:  1.461864/  1.835085, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1228 lr=['0.0010000'], tr/val_loss:  1.461937/  1.835908, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1229 lr=['0.0010000'], tr/val_loss:  1.461937/  1.835721, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1230 lr=['0.0010000'], tr/val_loss:  1.461951/  1.835399, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1231 lr=['0.0010000'], tr/val_loss:  1.462025/  1.835513, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1232 lr=['0.0010000'], tr/val_loss:  1.462314/  1.836977, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1233 lr=['0.0010000'], tr/val_loss:  1.461951/  1.837569, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1234 lr=['0.0010000'], tr/val_loss:  1.461965/  1.838537, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1235 lr=['0.0010000'], tr/val_loss:  1.461979/  1.836112, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1236 lr=['0.0010000'], tr/val_loss:  1.461951/  1.837563, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1237 lr=['0.0010000'], tr/val_loss:  1.461951/  1.835858, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1238 lr=['0.0010000'], tr/val_loss:  1.461937/  1.835756, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1239 lr=['0.0010000'], tr/val_loss:  1.461978/  1.837618, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1240 lr=['0.0010000'], tr/val_loss:  1.461964/  1.836982, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1241 lr=['0.0010000'], tr/val_loss:  1.461937/  1.836232, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1242 lr=['0.0010000'], tr/val_loss:  1.461951/  1.836803, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1243 lr=['0.0010000'], tr/val_loss:  1.461965/  1.836562, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1244 lr=['0.0010000'], tr/val_loss:  1.461951/  1.835269, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1245 lr=['0.0010000'], tr/val_loss:  1.461965/  1.835897, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1246 lr=['0.0010000'], tr/val_loss:  1.461850/  1.835640, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1247 lr=['0.0010000'], tr/val_loss:  1.461864/  1.835632, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1248 lr=['0.0010000'], tr/val_loss:  1.461850/  1.836559, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1249 lr=['0.0010000'], tr/val_loss:  1.461878/  1.837153, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1250 lr=['0.0010000'], tr/val_loss:  1.461937/  1.835928, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1251 lr=['0.0010000'], tr/val_loss:  1.461965/  1.836192, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1252 lr=['0.0010000'], tr/val_loss:  1.461937/  1.835977, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1253 lr=['0.0010000'], tr/val_loss:  1.461937/  1.838012, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1254 lr=['0.0010000'], tr/val_loss:  1.461951/  1.837903, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1255 lr=['0.0010000'], tr/val_loss:  1.461923/  1.836930, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1256 lr=['0.0010000'], tr/val_loss:  1.461864/  1.836113, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1257 lr=['0.0010000'], tr/val_loss:  1.461864/  1.835476, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1258 lr=['0.0010000'], tr/val_loss:  1.461951/  1.835601, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1259 lr=['0.0010000'], tr/val_loss:  1.461923/  1.837108, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1260 lr=['0.0010000'], tr/val_loss:  1.461951/  1.837859, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1261 lr=['0.0010000'], tr/val_loss:  1.461836/  1.835514, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1262 lr=['0.0010000'], tr/val_loss:  1.461951/  1.835323, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1263 lr=['0.0010000'], tr/val_loss:  1.461864/  1.838546, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1264 lr=['0.0010000'], tr/val_loss:  1.461850/  1.838437, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1265 lr=['0.0010000'], tr/val_loss:  1.461850/  1.837591, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1266 lr=['0.0010000'], tr/val_loss:  1.461937/  1.838386, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1267 lr=['0.0010000'], tr/val_loss:  1.461937/  1.835665, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1268 lr=['0.0010000'], tr/val_loss:  1.461937/  1.836147, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1269 lr=['0.0010000'], tr/val_loss:  1.461850/  1.836653, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1270 lr=['0.0010000'], tr/val_loss:  1.461836/  1.836025, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1271 lr=['0.0010000'], tr/val_loss:  1.461850/  1.835426, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1272 lr=['0.0010000'], tr/val_loss:  1.461864/  1.835157, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1273 lr=['0.0010000'], tr/val_loss:  1.461864/  1.835970, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1274 lr=['0.0010000'], tr/val_loss:  1.461836/  1.836238, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1275 lr=['0.0010000'], tr/val_loss:  1.461836/  1.835523, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1276 lr=['0.0010000'], tr/val_loss:  1.461836/  1.836675, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1277 lr=['0.0010000'], tr/val_loss:  1.461850/  1.838606, tr: 100.00%, tr_best: 100.00%, val:  74.17%, val_best:  78.75%\n",
      "epoch-1278 lr=['0.0010000'], tr/val_loss:  1.461850/  1.837033, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1279 lr=['0.0010000'], tr/val_loss:  1.461836/  1.836482, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1280 lr=['0.0010000'], tr/val_loss:  1.461923/  1.834941, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1281 lr=['0.0010000'], tr/val_loss:  1.461850/  1.838300, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1282 lr=['0.0010000'], tr/val_loss:  1.461836/  1.838029, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1283 lr=['0.0010000'], tr/val_loss:  1.461937/  1.836810, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1284 lr=['0.0010000'], tr/val_loss:  1.461850/  1.837786, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1285 lr=['0.0010000'], tr/val_loss:  1.461836/  1.838302, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1286 lr=['0.0010000'], tr/val_loss:  1.461923/  1.837333, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1287 lr=['0.0010000'], tr/val_loss:  1.461836/  1.838014, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1288 lr=['0.0010000'], tr/val_loss:  1.461836/  1.835821, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1289 lr=['0.0010000'], tr/val_loss:  1.461836/  1.838526, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1290 lr=['0.0010000'], tr/val_loss:  1.461850/  1.837818, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1291 lr=['0.0010000'], tr/val_loss:  1.461937/  1.837208, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1292 lr=['0.0010000'], tr/val_loss:  1.461836/  1.837462, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1293 lr=['0.0010000'], tr/val_loss:  1.461836/  1.837592, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1294 lr=['0.0010000'], tr/val_loss:  1.461923/  1.838681, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1295 lr=['0.0010000'], tr/val_loss:  1.461850/  1.839005, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1296 lr=['0.0010000'], tr/val_loss:  1.461923/  1.842439, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1297 lr=['0.0010000'], tr/val_loss:  1.461836/  1.838628, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1298 lr=['0.0010000'], tr/val_loss:  1.461836/  1.838828, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1299 lr=['0.0010000'], tr/val_loss:  1.461836/  1.840212, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1300 lr=['0.0010000'], tr/val_loss:  1.461937/  1.838754, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1301 lr=['0.0010000'], tr/val_loss:  1.461923/  1.840711, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1302 lr=['0.0010000'], tr/val_loss:  1.461850/  1.838773, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1303 lr=['0.0010000'], tr/val_loss:  1.461924/  1.838506, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1304 lr=['0.0010000'], tr/val_loss:  1.461923/  1.839631, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1305 lr=['0.0010000'], tr/val_loss:  1.461951/  1.840958, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1306 lr=['0.0010000'], tr/val_loss:  1.461850/  1.839141, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1307 lr=['0.0010000'], tr/val_loss:  1.461850/  1.838685, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1308 lr=['0.0010000'], tr/val_loss:  1.461923/  1.839607, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1309 lr=['0.0010000'], tr/val_loss:  1.461923/  1.840862, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1310 lr=['0.0010000'], tr/val_loss:  1.461836/  1.838865, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1311 lr=['0.0010000'], tr/val_loss:  1.461923/  1.840229, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1312 lr=['0.0010000'], tr/val_loss:  1.461923/  1.840297, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1313 lr=['0.0010000'], tr/val_loss:  1.461923/  1.837602, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1314 lr=['0.0010000'], tr/val_loss:  1.461937/  1.837219, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1315 lr=['0.0010000'], tr/val_loss:  1.461923/  1.838029, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1316 lr=['0.0010000'], tr/val_loss:  1.461923/  1.836532, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1317 lr=['0.0010000'], tr/val_loss:  1.461937/  1.838646, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1318 lr=['0.0010000'], tr/val_loss:  1.461923/  1.840353, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1319 lr=['0.0010000'], tr/val_loss:  1.461937/  1.840274, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1320 lr=['0.0010000'], tr/val_loss:  1.461937/  1.838606, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1321 lr=['0.0010000'], tr/val_loss:  1.461951/  1.836237, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1322 lr=['0.0010000'], tr/val_loss:  1.461923/  1.839251, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1323 lr=['0.0010000'], tr/val_loss:  1.461937/  1.838019, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1324 lr=['0.0010000'], tr/val_loss:  1.461836/  1.839257, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1325 lr=['0.0010000'], tr/val_loss:  1.461937/  1.838784, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1326 lr=['0.0010000'], tr/val_loss:  1.461836/  1.838679, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1327 lr=['0.0010000'], tr/val_loss:  1.461937/  1.839017, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1328 lr=['0.0010000'], tr/val_loss:  1.461937/  1.838997, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1329 lr=['0.0010000'], tr/val_loss:  1.461937/  1.839254, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1330 lr=['0.0010000'], tr/val_loss:  1.461850/  1.838364, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1331 lr=['0.0010000'], tr/val_loss:  1.461850/  1.834244, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1332 lr=['0.0010000'], tr/val_loss:  1.461864/  1.835236, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1333 lr=['0.0010000'], tr/val_loss:  1.461850/  1.840222, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1334 lr=['0.0010000'], tr/val_loss:  1.461850/  1.837502, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1335 lr=['0.0010000'], tr/val_loss:  1.461850/  1.834245, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1336 lr=['0.0010000'], tr/val_loss:  1.461864/  1.836595, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1337 lr=['0.0010000'], tr/val_loss:  1.462328/  1.837204, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1338 lr=['0.0010000'], tr/val_loss:  1.461777/  1.834750, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1339 lr=['0.0010000'], tr/val_loss:  1.461805/  1.834918, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1340 lr=['0.0010000'], tr/val_loss:  1.461777/  1.835144, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1341 lr=['0.0010000'], tr/val_loss:  1.461791/  1.834453, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1342 lr=['0.0010000'], tr/val_loss:  1.461805/  1.833603, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1343 lr=['0.0010000'], tr/val_loss:  1.461791/  1.836468, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1344 lr=['0.0010000'], tr/val_loss:  1.461804/  1.835610, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1345 lr=['0.0010000'], tr/val_loss:  1.461791/  1.835572, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1346 lr=['0.0010000'], tr/val_loss:  1.461804/  1.835810, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1347 lr=['0.0010000'], tr/val_loss:  1.461777/  1.833624, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1348 lr=['0.0010000'], tr/val_loss:  1.462214/  1.835029, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1349 lr=['0.0010000'], tr/val_loss:  1.461805/  1.837735, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1350 lr=['0.0010000'], tr/val_loss:  1.461763/  1.837038, tr: 100.00%, tr_best: 100.00%, val:  74.17%, val_best:  78.75%\n",
      "epoch-1351 lr=['0.0010000'], tr/val_loss:  1.461818/  1.835690, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1352 lr=['0.0010000'], tr/val_loss:  1.461777/  1.835389, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1353 lr=['0.0010000'], tr/val_loss:  1.461818/  1.837229, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1354 lr=['0.0010000'], tr/val_loss:  1.461791/  1.837462, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1355 lr=['0.0010000'], tr/val_loss:  1.461804/  1.837161, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1356 lr=['0.0010000'], tr/val_loss:  1.461818/  1.834734, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1357 lr=['0.0010000'], tr/val_loss:  1.461777/  1.834940, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1358 lr=['0.0010000'], tr/val_loss:  1.461791/  1.835595, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1359 lr=['0.0010000'], tr/val_loss:  1.461805/  1.835472, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1360 lr=['0.0010000'], tr/val_loss:  1.461777/  1.835998, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1361 lr=['0.0010000'], tr/val_loss:  1.461791/  1.836519, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1362 lr=['0.0010000'], tr/val_loss:  1.462214/  1.837125, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1363 lr=['0.0010000'], tr/val_loss:  1.461791/  1.835841, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1364 lr=['0.0010000'], tr/val_loss:  1.461791/  1.837214, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1365 lr=['0.0010000'], tr/val_loss:  1.461791/  1.835808, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1366 lr=['0.0010000'], tr/val_loss:  1.461791/  1.836917, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1367 lr=['0.0010000'], tr/val_loss:  1.461763/  1.836065, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1368 lr=['0.0010000'], tr/val_loss:  1.461777/  1.836074, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1369 lr=['0.0010000'], tr/val_loss:  1.461749/  1.836425, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1370 lr=['0.0010000'], tr/val_loss:  1.461777/  1.837864, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1371 lr=['0.0010000'], tr/val_loss:  1.461805/  1.836249, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1372 lr=['0.0010000'], tr/val_loss:  1.461749/  1.836924, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1373 lr=['0.0010000'], tr/val_loss:  1.461883/  1.836499, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1374 lr=['0.0010000'], tr/val_loss:  1.461791/  1.837434, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1375 lr=['0.0010000'], tr/val_loss:  1.461777/  1.837318, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1376 lr=['0.0010000'], tr/val_loss:  1.461777/  1.837509, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1377 lr=['0.0010000'], tr/val_loss:  1.461823/  1.836159, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1378 lr=['0.0010000'], tr/val_loss:  1.461791/  1.836174, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1379 lr=['0.0010000'], tr/val_loss:  1.461777/  1.836256, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1380 lr=['0.0010000'], tr/val_loss:  1.461763/  1.836800, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1381 lr=['0.0010000'], tr/val_loss:  1.461763/  1.839444, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1382 lr=['0.0010000'], tr/val_loss:  1.461763/  1.836523, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1383 lr=['0.0010000'], tr/val_loss:  1.461777/  1.837649, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1384 lr=['0.0010000'], tr/val_loss:  1.461763/  1.839459, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1385 lr=['0.0010000'], tr/val_loss:  1.461763/  1.838708, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1386 lr=['0.0010000'], tr/val_loss:  1.461763/  1.838249, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1387 lr=['0.0010000'], tr/val_loss:  1.461749/  1.838373, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1388 lr=['0.0010000'], tr/val_loss:  1.461777/  1.838696, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1389 lr=['0.0010000'], tr/val_loss:  1.461749/  1.837503, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1390 lr=['0.0010000'], tr/val_loss:  1.461763/  1.837199, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1391 lr=['0.0010000'], tr/val_loss:  1.461763/  1.837817, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1392 lr=['0.0010000'], tr/val_loss:  1.461763/  1.838232, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1393 lr=['0.0010000'], tr/val_loss:  1.461763/  1.837852, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1394 lr=['0.0010000'], tr/val_loss:  1.461749/  1.838611, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1395 lr=['0.0010000'], tr/val_loss:  1.461763/  1.838157, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1396 lr=['0.0010000'], tr/val_loss:  1.461750/  1.839249, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1397 lr=['0.0010000'], tr/val_loss:  1.461749/  1.838277, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1398 lr=['0.0010000'], tr/val_loss:  1.461749/  1.839046, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1399 lr=['0.0010000'], tr/val_loss:  1.461763/  1.839038, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1400 lr=['0.0010000'], tr/val_loss:  1.461809/  1.842170, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1401 lr=['0.0010000'], tr/val_loss:  1.461763/  1.840319, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1402 lr=['0.0010000'], tr/val_loss:  1.461749/  1.840608, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1403 lr=['0.0010000'], tr/val_loss:  1.461763/  1.839656, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1404 lr=['0.0010000'], tr/val_loss:  1.461749/  1.842333, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1405 lr=['0.0010000'], tr/val_loss:  1.461749/  1.840892, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1406 lr=['0.0010000'], tr/val_loss:  1.461750/  1.842575, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1407 lr=['0.0010000'], tr/val_loss:  1.461749/  1.841825, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1408 lr=['0.0010000'], tr/val_loss:  1.461777/  1.840925, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1409 lr=['0.0010000'], tr/val_loss:  1.461749/  1.841630, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1410 lr=['0.0010000'], tr/val_loss:  1.461749/  1.840474, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1411 lr=['0.0010000'], tr/val_loss:  1.461749/  1.841682, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1412 lr=['0.0010000'], tr/val_loss:  1.461763/  1.842079, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1413 lr=['0.0010000'], tr/val_loss:  1.461749/  1.840437, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1414 lr=['0.0010000'], tr/val_loss:  1.461749/  1.844641, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1415 lr=['0.0010000'], tr/val_loss:  1.461777/  1.841594, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1416 lr=['0.0010000'], tr/val_loss:  1.461749/  1.840160, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1417 lr=['0.0010000'], tr/val_loss:  1.461749/  1.842916, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1418 lr=['0.0010000'], tr/val_loss:  1.461749/  1.842819, tr: 100.00%, tr_best: 100.00%, val:  74.17%, val_best:  78.75%\n",
      "epoch-1419 lr=['0.0010000'], tr/val_loss:  1.461749/  1.841748, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1420 lr=['0.0010000'], tr/val_loss:  1.461749/  1.840991, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1421 lr=['0.0010000'], tr/val_loss:  1.461749/  1.841637, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1422 lr=['0.0010000'], tr/val_loss:  1.461763/  1.841847, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1423 lr=['0.0010000'], tr/val_loss:  1.461749/  1.840499, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1424 lr=['0.0010000'], tr/val_loss:  1.461749/  1.839821, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1425 lr=['0.0010000'], tr/val_loss:  1.461749/  1.841017, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1426 lr=['0.0010000'], tr/val_loss:  1.461749/  1.841462, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1427 lr=['0.0010000'], tr/val_loss:  1.461777/  1.841879, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1428 lr=['0.0010000'], tr/val_loss:  1.461749/  1.839779, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1429 lr=['0.0010000'], tr/val_loss:  1.461749/  1.840768, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1430 lr=['0.0010000'], tr/val_loss:  1.461763/  1.838131, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1431 lr=['0.0010000'], tr/val_loss:  1.461749/  1.839065, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1432 lr=['0.0010000'], tr/val_loss:  1.461749/  1.839442, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1433 lr=['0.0010000'], tr/val_loss:  1.461749/  1.840672, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1434 lr=['0.0010000'], tr/val_loss:  1.461735/  1.840181, tr: 100.00%, tr_best: 100.00%, val:  74.58%, val_best:  78.75%\n",
      "epoch-1435 lr=['0.0010000'], tr/val_loss:  1.461749/  1.838628, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1436 lr=['0.0010000'], tr/val_loss:  1.461736/  1.841196, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1437 lr=['0.0010000'], tr/val_loss:  1.461749/  1.839417, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1438 lr=['0.0010000'], tr/val_loss:  1.461763/  1.842060, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1439 lr=['0.0010000'], tr/val_loss:  1.461749/  1.841220, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1440 lr=['0.0010000'], tr/val_loss:  1.461736/  1.841573, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1441 lr=['0.0010000'], tr/val_loss:  1.461749/  1.841151, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1442 lr=['0.0010000'], tr/val_loss:  1.461736/  1.841254, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1443 lr=['0.0010000'], tr/val_loss:  1.461749/  1.838176, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1444 lr=['0.0010000'], tr/val_loss:  1.461749/  1.839891, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1445 lr=['0.0010000'], tr/val_loss:  1.461735/  1.839486, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1446 lr=['0.0010000'], tr/val_loss:  1.461749/  1.839119, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1447 lr=['0.0010000'], tr/val_loss:  1.461736/  1.839208, tr: 100.00%, tr_best: 100.00%, val:  75.00%, val_best:  78.75%\n",
      "epoch-1448 lr=['0.0010000'], tr/val_loss:  1.461735/  1.838394, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1449 lr=['0.0010000'], tr/val_loss:  1.461749/  1.839609, tr: 100.00%, tr_best: 100.00%, val:  75.42%, val_best:  78.75%\n",
      "epoch-1450 lr=['0.0010000'], tr/val_loss:  1.461749/  1.838325, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1451 lr=['0.0010000'], tr/val_loss:  1.461749/  1.838825, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1452 lr=['0.0010000'], tr/val_loss:  1.461735/  1.837904, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1453 lr=['0.0010000'], tr/val_loss:  1.461749/  1.838238, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1454 lr=['0.0010000'], tr/val_loss:  1.461749/  1.839014, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1455 lr=['0.0010000'], tr/val_loss:  1.461736/  1.838372, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1456 lr=['0.0010000'], tr/val_loss:  1.461763/  1.837704, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1457 lr=['0.0010000'], tr/val_loss:  1.461763/  1.837214, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1458 lr=['0.0010000'], tr/val_loss:  1.461736/  1.839960, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1459 lr=['0.0010000'], tr/val_loss:  1.461736/  1.838113, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1460 lr=['0.0010000'], tr/val_loss:  1.461749/  1.838744, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1461 lr=['0.0010000'], tr/val_loss:  1.461735/  1.839985, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1462 lr=['0.0010000'], tr/val_loss:  1.461749/  1.837469, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1463 lr=['0.0010000'], tr/val_loss:  1.461749/  1.839131, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1464 lr=['0.0010000'], tr/val_loss:  1.461749/  1.837904, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1465 lr=['0.0010000'], tr/val_loss:  1.461749/  1.838141, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1466 lr=['0.0010000'], tr/val_loss:  1.461749/  1.837986, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1467 lr=['0.0010000'], tr/val_loss:  1.461735/  1.837515, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1468 lr=['0.0010000'], tr/val_loss:  1.461749/  1.838112, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1469 lr=['0.0010000'], tr/val_loss:  1.462186/  1.838469, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1470 lr=['0.0010000'], tr/val_loss:  1.461763/  1.839464, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1471 lr=['0.0010000'], tr/val_loss:  1.461736/  1.838442, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1472 lr=['0.0010000'], tr/val_loss:  1.461749/  1.837235, tr: 100.00%, tr_best: 100.00%, val:  76.25%, val_best:  78.75%\n",
      "epoch-1473 lr=['0.0010000'], tr/val_loss:  1.461809/  1.839874, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n",
      "epoch-1474 lr=['0.0010000'], tr/val_loss:  1.461749/  1.839685, tr: 100.00%, tr_best: 100.00%, val:  77.08%, val_best:  78.75%\n",
      "epoch-1475 lr=['0.0010000'], tr/val_loss:  1.461749/  1.839361, tr: 100.00%, tr_best: 100.00%, val:  76.67%, val_best:  78.75%\n",
      "epoch-1476 lr=['0.0010000'], tr/val_loss:  1.461777/  1.839470, tr: 100.00%, tr_best: 100.00%, val:  75.83%, val_best:  78.75%\n"
     ]
    }
   ],
   "source": [
    "### my_snn control board (Gesture) ########################\n",
    "decay = 0.5 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "const2 = True # trace 할거면 True, 안할거면 False\n",
    "\n",
    "unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "if const2 == True:\n",
    "    const2 = decay\n",
    "else:\n",
    "    const2 = 0.0\n",
    "\n",
    "DFA_on_True__BPTT_on_False_single_step_True = True # True # False \n",
    "\n",
    "wandb.init(project= f'my_snn {unique_name}',save_code=True)\n",
    "\n",
    "my_snn_system(  devices = \"5\",\n",
    "                single_step = DFA_on_True__BPTT_on_False_single_step_True, # True # False # DFA_on이랑 같이 가라\n",
    "                unique_name = run_name,\n",
    "                my_seed = 42,\n",
    "                TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "                BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 128, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "                # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = 'DVS_GESTURE_TONIC',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "                lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "\n",
    "                synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "                synapse_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "                cfg = ['M', 'M', 200, 200], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "                # cfg = ['M', 'M', 64], \n",
    "                # cfg = [64, 124, 64, 124],\n",
    "                # cfg = ['M','M',512], \n",
    "                # cfg = [512], \n",
    "                # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                # cfg = ['M','M',512],\n",
    "                # cfg = ['M',200],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = ['M','M',200,200],\n",
    "                # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = ['M',200,200],\n",
    "                # cfg = ['M','M',1024,512,256,128,64],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = [12], #fc\n",
    "                # cfg = [12, 'M', 48, 'M', 12], \n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [],        \n",
    "                \n",
    "                net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "                learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                epoch_num = 10000,\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "                BPTT_on = not DFA_on_True__BPTT_on_False_single_step_True,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                dvs_clipping = 5, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "                # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "                dvs_duration = 100_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "                # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "                # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "                DFA_on = DFA_on_True__BPTT_on_False_single_step_True, # True # False # single_step이랑 같이 켜야 됨.\n",
    "                OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용\n",
    "\n",
    "                exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                denoise_on = True, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "                extra_train_dataset = 0, \n",
    "\n",
    "                num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "                chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "                pin_memory = True, # True # False \n",
    "\n",
    "                UDA_on = False,  # DECREPATED # uda\n",
    "                alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                bias = True, # True # False \n",
    "                ) \n",
    "\n",
    "# num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# num_workers = batch_size / num_GPU\n",
    "# num_workers = batch_size / num_CPU\n",
    "\n",
    "# sigmoid와 BN이 있어야 잘된다.\n",
    "# average pooling  \n",
    "# 이 낫다. \n",
    "\n",
    "# nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# # 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# # wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# run_name = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'random', # 'random', 'bayes'\n",
    "#     'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         \"learning_rate\": {\"values\": [0.001]}, #0.00936191669529645\n",
    "#         \"BATCH\": {\"values\": [16]},\n",
    "#         \"decay\": {\"values\": [0.25]},\n",
    "#         \"IMAGE_SIZE\": {\"values\": [128]},\n",
    "#         \"TIME\": {\"values\": [10]},\n",
    "#         \"epoch_num\": {\"values\": [200]},\n",
    "#         \"dvs_duration\": {\"values\": [25_000,50_000,100_000]},\n",
    "#         \"dvs_clipping\": {\"values\": [1,2,3,4,5]},\n",
    "#         \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "#         \"const2\": {\"values\": [False]},\n",
    "#         \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "#         \"DFA_on\": {\"values\": [False]},\n",
    "#         \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "#         \"cfg\": {\"values\": [['M','M',200,200]]},\n",
    "#         \"e_transport_swap\": {\"values\": [0]},\n",
    "#         \"e_transport_swap_tr\": {\"values\": [0]},\n",
    "#         \"drop_rate\": {\"values\": [0.0]}, # \"drop_rate\": {\"values\": [0.25,0.5,0.75]}, #\"drop_rate\": {\"min\": 0.25, \"max\": 0.75},\n",
    "#         \"exclude_class\": {\"values\": [True]},\n",
    "#         \"merge_polarities\": {\"values\": [False]},\n",
    "#         \"lif_layer_v_reset\": {\"values\": [10000]},\n",
    "#         \"lif_layer_sg_width\": {\"values\": [3.555718888923306]},\n",
    "#         \"e_transport_swap_coin\": {\"values\": [1]},\n",
    "#         \"lif_layer_v_threshold\": {\"values\": [0.720291189014991]},\n",
    "#         \"scheduler_name\": {\"values\": ['no']},  # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "#         \"denoise_on\": {\"values\": [True,False]}, \n",
    "#         \"I_wanna_sweep_at_this_epoch\": {\"values\": [-1]}, \n",
    "#         \"dvs_duration_domain\": {\"values\": [[]]}, \n",
    "#         \"dvs_relative_timestep\": {\"values\": [[False]]}, \n",
    "#         \"extra_train_dataset\": {\"values\": [0]}, \n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     unique_name = unique_name_hyper ## 이거 설정하면 새로운 경로에 모두 save\n",
    "    \n",
    "#     wandb.init(save_code = True)\n",
    "#     learning_rate  =  wandb.config.learning_rate\n",
    "#     BATCH  =  wandb.config.BATCH\n",
    "#     decay  =  wandb.config.decay\n",
    "#     IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "#     TIME  =  wandb.config.TIME\n",
    "#     epoch_num  =  wandb.config.epoch_num \n",
    "#     dvs_duration  =  wandb.config.dvs_duration\n",
    "#     dvs_clipping  =  wandb.config.dvs_clipping\n",
    "#     which_data  =  wandb.config.which_data\n",
    "#     const2  =  wandb.config.const2\n",
    "#     surrogate  =  wandb.config.surrogate\n",
    "#     DFA_on  =  wandb.config.DFA_on\n",
    "#     OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on\n",
    "#     cfg  =  wandb.config.cfg\n",
    "#     e_transport_swap  =  wandb.config.e_transport_swap\n",
    "#     e_transport_swap_tr  =  wandb.config.e_transport_swap_tr\n",
    "#     drop_rate  =  wandb.config.drop_rate\n",
    "#     exclude_class  =  wandb.config.exclude_class\n",
    "#     merge_polarities  =  wandb.config.merge_polarities\n",
    "#     lif_layer_v_reset  =  wandb.config.lif_layer_v_reset\n",
    "#     lif_layer_sg_width  =  wandb.config.lif_layer_sg_width\n",
    "#     e_transport_swap_coin  =  wandb.config.e_transport_swap_coin\n",
    "#     lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold\n",
    "#     scheduler_name  =  wandb.config.scheduler_name\n",
    "#     denoise_on  =  wandb.config.denoise_on\n",
    "#     I_wanna_sweep_at_this_epoch  =  wandb.config.I_wanna_sweep_at_this_epoch\n",
    "#     dvs_duration_domain  =  wandb.config.dvs_duration_domain\n",
    "#     dvs_relative_timestep  =  wandb.config.dvs_relative_timestep\n",
    "#     extra_train_dataset  =  wandb.config.extra_train_dataset\n",
    "#     if const2 == True:\n",
    "#         const2 = decay\n",
    "#     else:\n",
    "#         const2 = 0.0\n",
    "\n",
    "#     my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = TIME , # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "#                 BATCH = BATCH, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = IMAGE_SIZE, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "#                 #pmnist는 28로 해야 됨. 나머지는 바꿔도 돌아는 감.\n",
    "\n",
    "#                 # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "#                 which_data = which_data,\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = lif_layer_v_threshold,  # 10000이상으로 하면 NDA LIF 씀. #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = lif_layer_v_reset, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "#                 lif_layer_sg_width = lif_layer_sg_width, # # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "#                 synapse_conv_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_conv_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "#                 synapse_fc_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_fc_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "#                 # cfg = [64, 64],\n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 cfg = cfg,\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [], \n",
    "                \n",
    "#                 net_print = True, # True # False # True로 하길 추천\n",
    "#                 weight_count_print = False, # True # False\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 learning_rate = learning_rate, # default 0.001  # ottt 0.1 # nda 0.001 \n",
    "#                 epoch_num = epoch_num,\n",
    "#                 verbose_interval = 999999999, #숫자 크게 하면 꺼짐 #걍 중간중간 iter에서 끊어서 출력\n",
    "#                 validation_interval =  999999999,#999999999, #숫자 크게 하면 에포크 마지막 iter 때 val 함\n",
    "\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = surrogate, # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "#                 gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "#                 BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = scheduler_name, # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False,   # True # False \n",
    "#                 # 지원 DATASET: cifar10, mnist\n",
    "\n",
    "#                 nda_net = False,   # True # False\n",
    "\n",
    "#                 domain_il_epoch = 0, # over 0, then domain il mode on # pmnist 쓸거면 HLOP 코드보고 더 디벨롭하셈. 지금 개발 hold함.\n",
    "                \n",
    "#                 dvs_clipping = dvs_clipping, # 숫자만큼 크면 spike 아니면 걍 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "\n",
    "#                 dvs_duration = dvs_duration, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "#                 # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "\n",
    "#                 DFA_on = DFA_on, # True # False # residual은 dfa지원안함.\n",
    "#                 OTTT_input_trace_on = OTTT_input_trace_on, # True # False # 맨 처음 input에 trace 적용\n",
    "                 \n",
    "#                 e_transport_swap = e_transport_swap, # 1 이상이면 해당 숫자 에포크만큼 val_acc_best가 변화가 없으면 e_transport scheme (BP vs DFA) swap\n",
    "#                 e_transport_swap_tr = e_transport_swap_tr, # 1 이상이면 해당 숫자 에포크만큼 tr_acc_best가 변화가 없으면 e_transport scheme (BP vs DFA) swap\n",
    "#                 e_transport_swap_coin = e_transport_swap_coin, # swap할 수 있는 coin 개수\n",
    "                    \n",
    "#                 drop_rate = drop_rate,\n",
    "\n",
    "#                 exclude_class = exclude_class, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "#                 merge_polarities = merge_polarities, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "#                 denoise_on = denoise_on,\n",
    "\n",
    "#                 I_wanna_sweep_at_this_epoch = I_wanna_sweep_at_this_epoch,\n",
    "#                 dvs_duration_domain = dvs_duration_domain,\n",
    "#                 dvs_relative_timestep = dvs_relative_timestep, # True # False \n",
    "\n",
    "#                 extra_train_dataset = extra_train_dataset,\n",
    "\n",
    "#                 num_workers = 2,\n",
    "#                 chaching_on = True,\n",
    "#                 pin_memory = True, # True # False\n",
    "#                     ) \n",
    "#     # sigmoid와 BN이 있어야 잘된다.\n",
    "#     # average pooling\n",
    "#     # 이 낫다. \n",
    "    \n",
    "#     # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d53c56eaeb842088e9a19c11ceb382b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.446 MB of 0.446 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▃▆▂▅▅▄▅█▆█▄▆▆▇▇██▇▇▇▇██████████▇██████</td></tr><tr><td>summary_val_acc</td><td>▁▂▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇██</td></tr><tr><td>tr_acc</td><td>▁▂▃▄▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▆▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▃▃▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇██</td></tr><tr><td>val_loss</td><td>█▇▅▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>299</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98979</td></tr><tr><td>tr_epoch_loss</td><td>1.50874</td></tr><tr><td>val_acc_best</td><td>0.85417</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>1.7061</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunny-aardvark-6749</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j3tg799j' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j3tg799j</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250424_193044-j3tg799j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import json\n",
    "# run_name = 'main_FINAL_TEST'\n",
    "\n",
    "# unique_name = run_name\n",
    "# def pad_array_to_match_length(array1, array2):\n",
    "#     if len(array1) > len(array2):\n",
    "#         padded_array2 = np.pad(array2, (0, len(array1) - len(array2)), 'constant')\n",
    "#         return array1, padded_array2\n",
    "#     elif len(array2) > len(array1):\n",
    "#         padded_array1 = np.pad(array1, (0, len(array2) - len(array1)), 'constant')\n",
    "#         return padded_array1, array2\n",
    "#     else:\n",
    "#         return array1, array2\n",
    "# def load_hyperparameters(filename=f'result_save/hyperparameters_{unique_name}.json'):\n",
    "#     with open(filename, 'r') as f:\n",
    "#         return json.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# current_time = '20240628_110116'\n",
    "# base_name = f'{current_time}'\n",
    "# iter_acc_file_name = f'result_save/{base_name}_iter_acc_array_{unique_name}.npy'\n",
    "# val_acc_file_name = f'result_save/{base_name}_val_acc_now_array_{unique_name}.npy'\n",
    "# hyperparameters_file_name = f'result_save/{base_name}_hyperparameters_{unique_name}.json'\n",
    "\n",
    "# ### if you want to just see most recent train and val acc###########################\n",
    "# iter_acc_file_name = f'result_save/iter_acc_array_{unique_name}.npy'\n",
    "# tr_acc_file_name = f'result_save/tr_acc_array_{unique_name}.npy'\n",
    "# val_acc_file_name = f'result_save/val_acc_now_array_{unique_name}.npy'\n",
    "# hyperparameters_file_name = f'result_save/hyperparameters_{unique_name}.json'\n",
    "\n",
    "# loaded_iter_acc_array = np.load(iter_acc_file_name)*100\n",
    "# loaded_tr_acc_array = np.load(tr_acc_file_name)*100\n",
    "# loaded_val_acc_array = np.load(val_acc_file_name)*100\n",
    "# hyperparameters = load_hyperparameters(hyperparameters_file_name)\n",
    "\n",
    "# loaded_iter_acc_array, loaded_val_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_val_acc_array)\n",
    "# loaded_iter_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_tr_acc_array)\n",
    "# loaded_val_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_val_acc_array, loaded_tr_acc_array)\n",
    "\n",
    "# top_iter_acc = np.max(loaded_iter_acc_array)\n",
    "# top_tr_acc = np.max(loaded_tr_acc_array)\n",
    "# top_val_acc = np.max(loaded_val_acc_array)\n",
    "\n",
    "# which_data = hyperparameters['which_data']\n",
    "# BPTT_on = hyperparameters['BPTT_on']\n",
    "# current_epoch = hyperparameters['current epoch']\n",
    "# surrogate = hyperparameters['surrogate']\n",
    "# cfg = hyperparameters['cfg']\n",
    "# tdBN_on = hyperparameters['tdBN_on']\n",
    "# BN_on = hyperparameters['BN_on']\n",
    "\n",
    "\n",
    "# iterations = np.arange(len(loaded_iter_acc_array))\n",
    "\n",
    "# # 그래프 그리기\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(iterations, loaded_iter_acc_array, label='Iter Accuracy', color='g', alpha=0.2)\n",
    "# plt.plot(iterations, loaded_tr_acc_array, label='Training Accuracy', color='b')\n",
    "# plt.plot(iterations, loaded_val_acc_array, label='Validation Accuracy', color='r')\n",
    "\n",
    "# # # 텍스트 추가\n",
    "# # plt.text(0.05, 0.95, f'Top Training Accuracy: {100*top_iter_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='blue')\n",
    "# # plt.text(0.05, 0.90, f'Top Validation Accuracy: {100*top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='red')\n",
    "# # 텍스트 추가\n",
    "# plt.text(0.5, 0.10, f'Top Training Accuracy: {top_tr_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='blue')\n",
    "# plt.text(0.5, 0.05, f'Top Validation Accuracy: {top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='red')\n",
    "\n",
    "# plt.xlabel('Iterations')\n",
    "# plt.ylabel('Accuracy [%]')\n",
    "\n",
    "# # 그래프 제목에 하이퍼파라미터 정보 추가\n",
    "# title = f'Training and Validation Accuracy over Iterations\\n\\nData: {which_data}, BPTT: {\"On\" if BPTT_on else \"Off\"}, Current Epoch: {current_epoch}, Surrogate: {surrogate},\\nCFG: {cfg}, tdBN: {\"On\" if tdBN_on else \"Off\"}, BN: {\"On\" if BN_on else \"Off\"}'\n",
    "\n",
    "# plt.title(title)\n",
    "\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.xlim(0)  # x축을 0부터 시작\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
