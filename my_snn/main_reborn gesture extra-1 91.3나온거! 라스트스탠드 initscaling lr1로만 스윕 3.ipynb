{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12935/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    lif_layer_sg_width2 = None,\n",
    "                    lif_layer_v_threshold2 = None,\n",
    "                    learning_rate2 = None,\n",
    "                    init_scaling = None,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp,\n",
    "                    ANPI_MODE=False,\n",
    "                    lif_layer_sg_width2=lif_layer_sg_width2,\n",
    "                    lif_layer_v_threshold2=lif_layer_v_threshold2,\n",
    "                    init_scaling=init_scaling).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                # lr = group['lr']\n",
    "\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        lr = learning_rate\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        lr = learning_rate2\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        lr = 1.0\n",
    "\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 8, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[0,0],[0,0],[0,0]], \n",
    "#                 lif_layer_sg_width2 = 4.0,\n",
    "#                 lif_layer_v_threshold2 = 8,\n",
    "#                 learning_rate2 = 8,\n",
    "#                 init_scaling = [1/2,1/2,1/2],\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pdkgxgk8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_013637-pdkgxgk8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pdkgxgk8' target=\"_blank\">treasured-sweep-4</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pdkgxgk8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pdkgxgk8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251214_013646_083', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 16, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 0.5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 0.5, 'lif_layer_v_threshold2': 32, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 0.5, self.v_threshold 16\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 0.5, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=16, v_reset=10000, sg_width=0.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=0.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 41.0\n",
      "lif layer 2 self.abs_max_v: 41.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 8.0\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 51.0\n",
      "fc layer 2 self.abs_max_out: 80.0\n",
      "lif layer 2 self.abs_max_v: 94.5\n",
      "fc layer 3 self.abs_max_out: 12.0\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "lif layer 2 self.abs_max_v: 107.5\n",
      "fc layer 1 self.abs_max_out: 55.0\n",
      "lif layer 1 self.abs_max_v: 70.0\n",
      "fc layer 3 self.abs_max_out: 16.0\n",
      "fc layer 1 self.abs_max_out: 72.0\n",
      "lif layer 1 self.abs_max_v: 86.0\n",
      "fc layer 1 self.abs_max_out: 122.0\n",
      "lif layer 1 self.abs_max_v: 122.0\n",
      "fc layer 2 self.abs_max_out: 84.0\n",
      "lif layer 2 self.abs_max_v: 108.5\n",
      "fc layer 3 self.abs_max_out: 22.0\n",
      "fc layer 1 self.abs_max_out: 141.0\n",
      "lif layer 1 self.abs_max_v: 141.0\n",
      "fc layer 2 self.abs_max_out: 129.0\n",
      "lif layer 2 self.abs_max_v: 183.5\n",
      "fc layer 3 self.abs_max_out: 36.0\n",
      "fc layer 1 self.abs_max_out: 150.0\n",
      "lif layer 1 self.abs_max_v: 185.0\n",
      "lif layer 2 self.abs_max_v: 199.0\n",
      "fc layer 1 self.abs_max_out: 166.0\n",
      "lif layer 1 self.abs_max_v: 189.5\n",
      "fc layer 3 self.abs_max_out: 37.0\n",
      "fc layer 1 self.abs_max_out: 181.0\n",
      "lif layer 1 self.abs_max_v: 231.0\n",
      "fc layer 1 self.abs_max_out: 195.0\n",
      "lif layer 1 self.abs_max_v: 239.5\n",
      "fc layer 2 self.abs_max_out: 149.0\n",
      "lif layer 2 self.abs_max_v: 218.5\n",
      "fc layer 2 self.abs_max_out: 151.0\n",
      "fc layer 1 self.abs_max_out: 216.0\n",
      "fc layer 1 self.abs_max_out: 245.0\n",
      "lif layer 1 self.abs_max_v: 245.0\n",
      "fc layer 3 self.abs_max_out: 47.0\n",
      "lif layer 1 self.abs_max_v: 259.5\n",
      "fc layer 1 self.abs_max_out: 264.0\n",
      "lif layer 1 self.abs_max_v: 331.5\n",
      "fc layer 2 self.abs_max_out: 154.0\n",
      "fc layer 2 self.abs_max_out: 167.0\n",
      "fc layer 3 self.abs_max_out: 49.0\n",
      "lif layer 1 self.abs_max_v: 357.5\n",
      "fc layer 3 self.abs_max_out: 57.0\n",
      "fc layer 2 self.abs_max_out: 196.0\n",
      "fc layer 2 self.abs_max_out: 204.0\n",
      "lif layer 1 self.abs_max_v: 390.0\n",
      "fc layer 1 self.abs_max_out: 328.0\n",
      "fc layer 2 self.abs_max_out: 220.0\n",
      "lif layer 2 self.abs_max_v: 220.0\n",
      "fc layer 2 self.abs_max_out: 231.0\n",
      "lif layer 2 self.abs_max_v: 231.0\n",
      "fc layer 1 self.abs_max_out: 348.0\n",
      "lif layer 1 self.abs_max_v: 395.0\n",
      "fc layer 3 self.abs_max_out: 62.0\n",
      "fc layer 3 self.abs_max_out: 65.0\n",
      "lif layer 2 self.abs_max_v: 258.0\n",
      "fc layer 3 self.abs_max_out: 81.0\n",
      "lif layer 2 self.abs_max_v: 291.0\n",
      "lif layer 2 self.abs_max_v: 297.5\n",
      "lif layer 1 self.abs_max_v: 409.5\n",
      "fc layer 1 self.abs_max_out: 380.0\n",
      "fc layer 1 self.abs_max_out: 427.0\n",
      "lif layer 1 self.abs_max_v: 562.5\n",
      "lif layer 1 self.abs_max_v: 596.0\n",
      "fc layer 3 self.abs_max_out: 83.0\n",
      "fc layer 2 self.abs_max_out: 232.0\n",
      "fc layer 1 self.abs_max_out: 443.0\n",
      "fc layer 3 self.abs_max_out: 84.0\n",
      "fc layer 1 self.abs_max_out: 492.0\n",
      "lif layer 2 self.abs_max_v: 312.0\n",
      "fc layer 2 self.abs_max_out: 251.0\n",
      "fc layer 3 self.abs_max_out: 98.0\n",
      "lif layer 2 self.abs_max_v: 316.5\n",
      "lif layer 1 self.abs_max_v: 604.0\n",
      "fc layer 2 self.abs_max_out: 292.0\n",
      "lif layer 1 self.abs_max_v: 623.0\n",
      "lif layer 1 self.abs_max_v: 654.5\n",
      "lif layer 2 self.abs_max_v: 320.5\n",
      "fc layer 1 self.abs_max_out: 535.0\n",
      "lif layer 2 self.abs_max_v: 324.0\n",
      "lif layer 1 self.abs_max_v: 687.5\n",
      "lif layer 2 self.abs_max_v: 326.0\n",
      "lif layer 2 self.abs_max_v: 336.0\n",
      "lif layer 2 self.abs_max_v: 373.0\n",
      "lif layer 2 self.abs_max_v: 394.0\n",
      "lif layer 2 self.abs_max_v: 396.5\n",
      "lif layer 2 self.abs_max_v: 431.5\n",
      "fc layer 2 self.abs_max_out: 318.0\n",
      "fc layer 2 self.abs_max_out: 343.0\n",
      "fc layer 2 self.abs_max_out: 352.0\n",
      "lif layer 2 self.abs_max_v: 434.0\n",
      "fc layer 3 self.abs_max_out: 107.0\n",
      "fc layer 3 self.abs_max_out: 111.0\n",
      "lif layer 2 self.abs_max_v: 442.0\n",
      "lif layer 2 self.abs_max_v: 452.5\n",
      "lif layer 2 self.abs_max_v: 469.5\n",
      "lif layer 2 self.abs_max_v: 483.5\n",
      "fc layer 3 self.abs_max_out: 113.0\n",
      "fc layer 2 self.abs_max_out: 353.0\n",
      "fc layer 3 self.abs_max_out: 124.0\n",
      "lif layer 2 self.abs_max_v: 505.5\n",
      "lif layer 2 self.abs_max_v: 566.0\n",
      "lif layer 2 self.abs_max_v: 619.0\n",
      "fc layer 3 self.abs_max_out: 135.0\n",
      "fc layer 3 self.abs_max_out: 138.0\n",
      "fc layer 3 self.abs_max_out: 139.0\n",
      "fc layer 2 self.abs_max_out: 399.0\n",
      "fc layer 1 self.abs_max_out: 605.0\n",
      "lif layer 1 self.abs_max_v: 712.5\n",
      "lif layer 2 self.abs_max_v: 623.0\n",
      "lif layer 2 self.abs_max_v: 631.5\n",
      "lif layer 2 self.abs_max_v: 637.5\n",
      "lif layer 2 self.abs_max_v: 641.0\n",
      "lif layer 2 self.abs_max_v: 691.0\n",
      "lif layer 2 self.abs_max_v: 712.0\n",
      "lif layer 2 self.abs_max_v: 732.0\n",
      "lif layer 1 self.abs_max_v: 721.5\n",
      "lif layer 2 self.abs_max_v: 736.0\n",
      "fc layer 1 self.abs_max_out: 609.0\n",
      "fc layer 1 self.abs_max_out: 620.0\n",
      "fc layer 1 self.abs_max_out: 813.0\n",
      "lif layer 1 self.abs_max_v: 813.0\n",
      "fc layer 2 self.abs_max_out: 402.0\n",
      "lif layer 2 self.abs_max_v: 762.5\n",
      "lif layer 2 self.abs_max_v: 778.5\n",
      "fc layer 2 self.abs_max_out: 417.0\n",
      "fc layer 2 self.abs_max_out: 420.0\n",
      "fc layer 2 self.abs_max_out: 435.0\n",
      "fc layer 3 self.abs_max_out: 141.0\n",
      "fc layer 3 self.abs_max_out: 146.0\n",
      "fc layer 3 self.abs_max_out: 157.0\n",
      "fc layer 3 self.abs_max_out: 168.0\n",
      "lif layer 2 self.abs_max_v: 781.0\n",
      "fc layer 2 self.abs_max_out: 446.0\n",
      "lif layer 2 self.abs_max_v: 836.5\n",
      "lif layer 2 self.abs_max_v: 842.0\n",
      "lif layer 2 self.abs_max_v: 842.5\n",
      "fc layer 3 self.abs_max_out: 196.0\n",
      "fc layer 3 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 827.5\n",
      "lif layer 1 self.abs_max_v: 844.0\n",
      "fc layer 2 self.abs_max_out: 462.0\n",
      "lif layer 2 self.abs_max_v: 844.0\n",
      "lif layer 2 self.abs_max_v: 851.0\n",
      "fc layer 3 self.abs_max_out: 211.0\n",
      "fc layer 2 self.abs_max_out: 469.0\n",
      "lif layer 2 self.abs_max_v: 861.0\n",
      "lif layer 2 self.abs_max_v: 866.5\n",
      "fc layer 2 self.abs_max_out: 472.0\n",
      "lif layer 2 self.abs_max_v: 905.5\n",
      "fc layer 2 self.abs_max_out: 513.0\n",
      "fc layer 3 self.abs_max_out: 223.0\n",
      "lif layer 2 self.abs_max_v: 906.0\n",
      "lif layer 2 self.abs_max_v: 915.0\n",
      "lif layer 2 self.abs_max_v: 919.5\n",
      "lif layer 2 self.abs_max_v: 921.0\n",
      "fc layer 1 self.abs_max_out: 831.0\n",
      "lif layer 1 self.abs_max_v: 945.5\n",
      "fc layer 1 self.abs_max_out: 846.0\n",
      "lif layer 1 self.abs_max_v: 1085.0\n",
      "fc layer 1 self.abs_max_out: 879.0\n",
      "fc layer 1 self.abs_max_out: 966.0\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "lif layer 1 self.abs_max_v: 1128.5\n",
      "fc layer 1 self.abs_max_out: 1036.0\n",
      "fc layer 1 self.abs_max_out: 1065.0\n",
      "fc layer 1 self.abs_max_out: 1086.0\n",
      "fc layer 1 self.abs_max_out: 1102.0\n",
      "fc layer 1 self.abs_max_out: 1180.0\n",
      "lif layer 1 self.abs_max_v: 1180.0\n",
      "fc layer 1 self.abs_max_out: 1387.0\n",
      "lif layer 1 self.abs_max_v: 1387.0\n",
      "lif layer 1 self.abs_max_v: 1593.0\n",
      "fc layer 2 self.abs_max_out: 525.0\n",
      "fc layer 2 self.abs_max_out: 532.0\n",
      "lif layer 2 self.abs_max_v: 952.5\n",
      "lif layer 2 self.abs_max_v: 959.0\n",
      "lif layer 2 self.abs_max_v: 981.5\n",
      "lif layer 2 self.abs_max_v: 984.0\n",
      "lif layer 2 self.abs_max_v: 1008.5\n",
      "lif layer 2 self.abs_max_v: 1023.5\n",
      "lif layer 2 self.abs_max_v: 1030.5\n",
      "lif layer 2 self.abs_max_v: 1035.5\n",
      "lif layer 2 self.abs_max_v: 1041.0\n",
      "lif layer 2 self.abs_max_v: 1043.5\n",
      "lif layer 2 self.abs_max_v: 1045.0\n",
      "fc layer 2 self.abs_max_out: 547.0\n",
      "fc layer 2 self.abs_max_out: 593.0\n",
      "lif layer 2 self.abs_max_v: 1065.5\n",
      "lif layer 2 self.abs_max_v: 1075.0\n",
      "fc layer 3 self.abs_max_out: 241.0\n",
      "fc layer 3 self.abs_max_out: 242.0\n",
      "fc layer 3 self.abs_max_out: 248.0\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "fc layer 3 self.abs_max_out: 259.0\n",
      "fc layer 3 self.abs_max_out: 262.0\n",
      "fc layer 3 self.abs_max_out: 280.0\n",
      "fc layer 1 self.abs_max_out: 1452.0\n",
      "fc layer 1 self.abs_max_out: 1603.0\n",
      "lif layer 1 self.abs_max_v: 1603.0\n",
      "fc layer 1 self.abs_max_out: 1627.0\n",
      "lif layer 1 self.abs_max_v: 1627.0\n",
      "fc layer 2 self.abs_max_out: 670.0\n",
      "lif layer 1 self.abs_max_v: 1636.5\n",
      "lif layer 1 self.abs_max_v: 1795.0\n",
      "lif layer 1 self.abs_max_v: 1910.5\n",
      "fc layer 1 self.abs_max_out: 1634.0\n",
      "fc layer 1 self.abs_max_out: 1710.0\n",
      "fc layer 1 self.abs_max_out: 1731.0\n",
      "fc layer 1 self.abs_max_out: 1750.0\n",
      "fc layer 1 self.abs_max_out: 1770.0\n",
      "fc layer 1 self.abs_max_out: 1929.0\n",
      "lif layer 1 self.abs_max_v: 1929.0\n",
      "fc layer 2 self.abs_max_out: 737.0\n",
      "lif layer 1 self.abs_max_v: 2028.0\n",
      "lif layer 1 self.abs_max_v: 2040.5\n",
      "lif layer 2 self.abs_max_v: 1087.5\n",
      "lif layer 2 self.abs_max_v: 1115.0\n",
      "lif layer 2 self.abs_max_v: 1127.5\n",
      "lif layer 2 self.abs_max_v: 1136.0\n",
      "lif layer 2 self.abs_max_v: 1150.0\n",
      "fc layer 3 self.abs_max_out: 290.0\n",
      "fc layer 1 self.abs_max_out: 2059.0\n",
      "lif layer 1 self.abs_max_v: 2059.0\n",
      "fc layer 1 self.abs_max_out: 2253.0\n",
      "lif layer 1 self.abs_max_v: 2253.0\n",
      "fc layer 3 self.abs_max_out: 296.0\n",
      "fc layer 3 self.abs_max_out: 300.0\n",
      "fc layer 3 self.abs_max_out: 320.0\n",
      "fc layer 3 self.abs_max_out: 328.0\n",
      "fc layer 3 self.abs_max_out: 332.0\n",
      "lif layer 2 self.abs_max_v: 1174.5\n",
      "fc layer 2 self.abs_max_out: 741.0\n",
      "fc layer 2 self.abs_max_out: 778.0\n",
      "fc layer 1 self.abs_max_out: 2323.0\n",
      "lif layer 1 self.abs_max_v: 2323.0\n",
      "fc layer 1 self.abs_max_out: 2339.0\n",
      "lif layer 1 self.abs_max_v: 2339.0\n",
      "fc layer 1 self.abs_max_out: 2382.0\n",
      "lif layer 1 self.abs_max_v: 2382.0\n",
      "fc layer 1 self.abs_max_out: 2415.0\n",
      "lif layer 1 self.abs_max_v: 2415.0\n",
      "fc layer 1 self.abs_max_out: 2426.0\n",
      "lif layer 1 self.abs_max_v: 2426.0\n",
      "fc layer 2 self.abs_max_out: 827.0\n",
      "fc layer 2 self.abs_max_out: 828.0\n",
      "lif layer 2 self.abs_max_v: 1213.5\n",
      "lif layer 2 self.abs_max_v: 1218.0\n",
      "fc layer 1 self.abs_max_out: 2430.0\n",
      "lif layer 1 self.abs_max_v: 2430.0\n",
      "fc layer 1 self.abs_max_out: 2544.0\n",
      "lif layer 1 self.abs_max_v: 2544.0\n",
      "fc layer 1 self.abs_max_out: 2622.0\n",
      "lif layer 1 self.abs_max_v: 2622.0\n",
      "fc layer 1 self.abs_max_out: 2660.0\n",
      "lif layer 1 self.abs_max_v: 2660.0\n",
      "fc layer 1 self.abs_max_out: 2717.0\n",
      "lif layer 1 self.abs_max_v: 2717.0\n",
      "fc layer 1 self.abs_max_out: 2806.0\n",
      "lif layer 1 self.abs_max_v: 2806.0\n",
      "fc layer 1 self.abs_max_out: 2849.0\n",
      "lif layer 1 self.abs_max_v: 2849.0\n",
      "lif layer 2 self.abs_max_v: 1221.0\n",
      "fc layer 1 self.abs_max_out: 2901.0\n",
      "lif layer 1 self.abs_max_v: 2901.0\n",
      "fc layer 3 self.abs_max_out: 333.0\n",
      "fc layer 3 self.abs_max_out: 338.0\n",
      "fc layer 2 self.abs_max_out: 834.0\n",
      "fc layer 2 self.abs_max_out: 889.0\n",
      "lif layer 2 self.abs_max_v: 1224.0\n",
      "fc layer 2 self.abs_max_out: 893.0\n",
      "lif layer 2 self.abs_max_v: 1252.0\n",
      "lif layer 2 self.abs_max_v: 1270.0\n",
      "fc layer 1 self.abs_max_out: 2921.0\n",
      "lif layer 1 self.abs_max_v: 3026.0\n",
      "lif layer 1 self.abs_max_v: 3109.5\n",
      "fc layer 1 self.abs_max_out: 3011.0\n",
      "lif layer 2 self.abs_max_v: 1276.5\n",
      "lif layer 2 self.abs_max_v: 1353.0\n",
      "lif layer 2 self.abs_max_v: 1412.5\n",
      "fc layer 3 self.abs_max_out: 341.0\n",
      "fc layer 3 self.abs_max_out: 356.0\n",
      "fc layer 2 self.abs_max_out: 935.0\n",
      "fc layer 2 self.abs_max_out: 1014.0\n",
      "lif layer 2 self.abs_max_v: 1429.5\n",
      "fc layer 2 self.abs_max_out: 1015.0\n",
      "lif layer 2 self.abs_max_v: 1464.0\n",
      "lif layer 2 self.abs_max_v: 1477.0\n",
      "lif layer 2 self.abs_max_v: 1487.5\n",
      "lif layer 2 self.abs_max_v: 1493.0\n",
      "lif layer 2 self.abs_max_v: 1495.5\n",
      "lif layer 1 self.abs_max_v: 3146.0\n",
      "lif layer 1 self.abs_max_v: 3362.0\n",
      "fc layer 2 self.abs_max_out: 1025.0\n",
      "fc layer 2 self.abs_max_out: 1101.0\n",
      "fc layer 2 self.abs_max_out: 1117.0\n",
      "fc layer 1 self.abs_max_out: 3063.0\n",
      "fc layer 1 self.abs_max_out: 3067.0\n",
      "lif layer 1 self.abs_max_v: 3372.5\n",
      "fc layer 1 self.abs_max_out: 3171.0\n",
      "fc layer 1 self.abs_max_out: 3182.0\n",
      "fc layer 1 self.abs_max_out: 3391.0\n",
      "lif layer 1 self.abs_max_v: 3391.0\n",
      "fc layer 1 self.abs_max_out: 3426.0\n",
      "lif layer 1 self.abs_max_v: 3426.0\n",
      "fc layer 1 self.abs_max_out: 3501.0\n",
      "lif layer 1 self.abs_max_v: 3501.0\n",
      "fc layer 1 self.abs_max_out: 3566.0\n",
      "lif layer 1 self.abs_max_v: 3566.0\n",
      "fc layer 1 self.abs_max_out: 3597.0\n",
      "lif layer 1 self.abs_max_v: 3597.0\n",
      "lif layer 1 self.abs_max_v: 3615.0\n",
      "lif layer 1 self.abs_max_v: 3646.5\n",
      "lif layer 2 self.abs_max_v: 1526.0\n",
      "lif layer 2 self.abs_max_v: 1548.0\n",
      "fc layer 1 self.abs_max_out: 3628.0\n",
      "fc layer 2 self.abs_max_out: 1166.0\n",
      "fc layer 2 self.abs_max_out: 1201.0\n",
      "lif layer 2 self.abs_max_v: 1594.0\n",
      "lif layer 2 self.abs_max_v: 1667.0\n",
      "fc layer 1 self.abs_max_out: 3643.0\n",
      "lif layer 2 self.abs_max_v: 1674.0\n",
      "lif layer 2 self.abs_max_v: 1700.5\n",
      "lif layer 2 self.abs_max_v: 1745.5\n",
      "lif layer 2 self.abs_max_v: 1795.5\n",
      "lif layer 2 self.abs_max_v: 1837.5\n",
      "lif layer 2 self.abs_max_v: 1860.0\n",
      "fc layer 1 self.abs_max_out: 3822.0\n",
      "lif layer 1 self.abs_max_v: 3822.0\n",
      "fc layer 1 self.abs_max_out: 3854.0\n",
      "lif layer 1 self.abs_max_v: 3854.0\n",
      "fc layer 1 self.abs_max_out: 3856.0\n",
      "lif layer 1 self.abs_max_v: 3856.0\n",
      "lif layer 1 self.abs_max_v: 4067.0\n",
      "fc layer 2 self.abs_max_out: 1226.0\n",
      "lif layer 1 self.abs_max_v: 4226.0\n",
      "lif layer 2 self.abs_max_v: 1874.5\n",
      "lif layer 1 self.abs_max_v: 4243.0\n",
      "lif layer 1 self.abs_max_v: 4355.0\n",
      "lif layer 1 self.abs_max_v: 4475.5\n",
      "fc layer 2 self.abs_max_out: 1403.0\n",
      "fc layer 3 self.abs_max_out: 375.0\n",
      "lif layer 2 self.abs_max_v: 1898.0\n",
      "lif layer 2 self.abs_max_v: 1934.0\n",
      "lif layer 2 self.abs_max_v: 2001.0\n",
      "lif layer 2 self.abs_max_v: 2052.0\n",
      "lif layer 2 self.abs_max_v: 2069.5\n",
      "lif layer 1 self.abs_max_v: 4908.0\n",
      "lif layer 1 self.abs_max_v: 5072.5\n",
      "fc layer 1 self.abs_max_out: 4101.0\n",
      "lif layer 1 self.abs_max_v: 5075.5\n",
      "lif layer 1 self.abs_max_v: 5355.5\n",
      "fc layer 1 self.abs_max_out: 4271.0\n",
      "fc layer 1 self.abs_max_out: 4350.0\n",
      "fc layer 1 self.abs_max_out: 4381.0\n",
      "fc layer 1 self.abs_max_out: 4455.0\n",
      "fc layer 2 self.abs_max_out: 1413.0\n",
      "lif layer 2 self.abs_max_v: 2148.0\n",
      "lif layer 2 self.abs_max_v: 2197.0\n",
      "lif layer 2 self.abs_max_v: 2221.5\n",
      "lif layer 2 self.abs_max_v: 2234.0\n",
      "lif layer 2 self.abs_max_v: 2240.0\n",
      "lif layer 1 self.abs_max_v: 5407.5\n",
      "lif layer 1 self.abs_max_v: 5629.0\n",
      "lif layer 1 self.abs_max_v: 5829.0\n",
      "lif layer 1 self.abs_max_v: 5876.5\n",
      "lif layer 2 self.abs_max_v: 2262.5\n",
      "lif layer 1 self.abs_max_v: 6017.5\n",
      "fc layer 1 self.abs_max_out: 4483.0\n",
      "fc layer 1 self.abs_max_out: 4596.0\n",
      "lif layer 1 self.abs_max_v: 6185.5\n",
      "lif layer 1 self.abs_max_v: 6192.0\n",
      "lif layer 2 self.abs_max_v: 2268.0\n",
      "lif layer 2 self.abs_max_v: 2278.0\n",
      "lif layer 2 self.abs_max_v: 2283.0\n",
      "lif layer 2 self.abs_max_v: 2285.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 12.070957/114.119530, val:  26.25%, val_best:  26.25%, tr:  98.16%, tr_best:  98.16%, epoch time: 80.89 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3920%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1335%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1560  15.935%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1416.0\n",
      "lif layer 2 self.abs_max_v: 2323.5\n",
      "fc layer 2 self.abs_max_out: 1629.0\n",
      "fc layer 2 self.abs_max_out: 1706.0\n",
      "fc layer 2 self.abs_max_out: 1889.0\n",
      "lif layer 2 self.abs_max_v: 2330.5\n",
      "fc layer 3 self.abs_max_out: 383.0\n",
      "fc layer 3 self.abs_max_out: 391.0\n",
      "fc layer 3 self.abs_max_out: 402.0\n",
      "lif layer 2 self.abs_max_v: 2402.5\n",
      "lif layer 2 self.abs_max_v: 2470.0\n",
      "lif layer 1 self.abs_max_v: 6740.5\n",
      "lif layer 2 self.abs_max_v: 2544.0\n",
      "lif layer 2 self.abs_max_v: 2565.5\n",
      "lif layer 2 self.abs_max_v: 2596.0\n",
      "fc layer 2 self.abs_max_out: 1930.0\n",
      "fc layer 3 self.abs_max_out: 408.0\n",
      "fc layer 2 self.abs_max_out: 1987.0\n",
      "fc layer 2 self.abs_max_out: 2087.0\n",
      "fc layer 2 self.abs_max_out: 2095.0\n",
      "fc layer 2 self.abs_max_out: 2116.0\n",
      "fc layer 1 self.abs_max_out: 4806.0\n",
      "fc layer 1 self.abs_max_out: 4816.0\n",
      "fc layer 1 self.abs_max_out: 5208.0\n",
      "fc layer 1 self.abs_max_out: 5371.0\n",
      "fc layer 2 self.abs_max_out: 2124.0\n",
      "fc layer 2 self.abs_max_out: 2291.0\n",
      "lif layer 2 self.abs_max_v: 2617.5\n",
      "lif layer 2 self.abs_max_v: 2685.0\n",
      "lif layer 2 self.abs_max_v: 2718.5\n",
      "lif layer 2 self.abs_max_v: 2729.5\n",
      "fc layer 2 self.abs_max_out: 2340.0\n",
      "lif layer 1 self.abs_max_v: 6760.0\n",
      "lif layer 1 self.abs_max_v: 6888.0\n",
      "lif layer 1 self.abs_max_v: 6996.0\n",
      "fc layer 3 self.abs_max_out: 421.0\n",
      "lif layer 2 self.abs_max_v: 2780.0\n",
      "lif layer 2 self.abs_max_v: 2785.0\n",
      "lif layer 2 self.abs_max_v: 2809.5\n",
      "lif layer 2 self.abs_max_v: 2822.0\n",
      "fc layer 1 self.abs_max_out: 5519.0\n",
      "fc layer 3 self.abs_max_out: 442.0\n",
      "lif layer 1 self.abs_max_v: 7135.0\n",
      "lif layer 1 self.abs_max_v: 7628.5\n",
      "lif layer 1 self.abs_max_v: 8063.5\n",
      "lif layer 1 self.abs_max_v: 8268.5\n",
      "lif layer 1 self.abs_max_v: 8392.5\n",
      "fc layer 2 self.abs_max_out: 2397.0\n",
      "fc layer 2 self.abs_max_out: 2475.0\n",
      "fc layer 1 self.abs_max_out: 5670.0\n",
      "fc layer 1 self.abs_max_out: 5871.0\n",
      "lif layer 2 self.abs_max_v: 2839.5\n",
      "lif layer 2 self.abs_max_v: 2871.0\n",
      "lif layer 2 self.abs_max_v: 2886.5\n",
      "lif layer 2 self.abs_max_v: 2894.5\n",
      "lif layer 2 self.abs_max_v: 2898.5\n",
      "fc layer 2 self.abs_max_out: 2501.0\n",
      "fc layer 2 self.abs_max_out: 2515.0\n",
      "fc layer 1 self.abs_max_out: 5949.0\n",
      "lif layer 2 self.abs_max_v: 2910.0\n",
      "fc layer 2 self.abs_max_out: 2568.0\n",
      "fc layer 2 self.abs_max_out: 2628.0\n",
      "lif layer 2 self.abs_max_v: 2912.5\n",
      "lif layer 2 self.abs_max_v: 2915.0\n",
      "lif layer 2 self.abs_max_v: 2921.5\n",
      "lif layer 2 self.abs_max_v: 2925.0\n",
      "lif layer 2 self.abs_max_v: 2932.0\n",
      "lif layer 2 self.abs_max_v: 2950.0\n",
      "lif layer 2 self.abs_max_v: 3005.0\n",
      "lif layer 2 self.abs_max_v: 3032.5\n",
      "lif layer 1 self.abs_max_v: 9059.0\n",
      "lif layer 1 self.abs_max_v: 9760.5\n",
      "lif layer 1 self.abs_max_v: 9793.5\n",
      "lif layer 1 self.abs_max_v: 10098.0\n",
      "lif layer 2 self.abs_max_v: 3079.0\n",
      "lif layer 2 self.abs_max_v: 3127.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 13.681352/ 84.944244, val:  28.33%, val_best:  28.33%, tr:  98.67%, tr_best:  98.67%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.1539%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7472%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3069  15.674%\n",
      "lif layer 2 self.abs_max_v: 3196.0\n",
      "lif layer 2 self.abs_max_v: 3257.0\n",
      "fc layer 2 self.abs_max_out: 2721.0\n",
      "fc layer 2 self.abs_max_out: 2729.0\n",
      "fc layer 2 self.abs_max_out: 2809.0\n",
      "fc layer 2 self.abs_max_out: 2889.0\n",
      "fc layer 2 self.abs_max_out: 2915.0\n",
      "fc layer 1 self.abs_max_out: 6050.0\n",
      "fc layer 1 self.abs_max_out: 6069.0\n",
      "lif layer 2 self.abs_max_v: 3272.5\n",
      "lif layer 2 self.abs_max_v: 3298.5\n",
      "lif layer 2 self.abs_max_v: 3311.5\n",
      "lif layer 2 self.abs_max_v: 3318.0\n",
      "lif layer 2 self.abs_max_v: 3321.0\n",
      "fc layer 2 self.abs_max_out: 2932.0\n",
      "fc layer 2 self.abs_max_out: 3032.0\n",
      "fc layer 2 self.abs_max_out: 3038.0\n",
      "fc layer 2 self.abs_max_out: 3040.0\n",
      "fc layer 2 self.abs_max_out: 3252.0\n",
      "fc layer 2 self.abs_max_out: 3261.0\n",
      "lif layer 2 self.abs_max_v: 3357.0\n",
      "fc layer 2 self.abs_max_out: 3328.0\n",
      "lif layer 2 self.abs_max_v: 3362.5\n",
      "lif layer 2 self.abs_max_v: 3420.0\n",
      "lif layer 2 self.abs_max_v: 3452.0\n",
      "lif layer 2 self.abs_max_v: 3490.0\n",
      "fc layer 2 self.abs_max_out: 3428.0\n",
      "fc layer 2 self.abs_max_out: 3497.0\n",
      "lif layer 2 self.abs_max_v: 3497.0\n",
      "fc layer 2 self.abs_max_out: 3589.0\n",
      "lif layer 2 self.abs_max_v: 3589.0\n",
      "lif layer 1 self.abs_max_v: 11015.5\n",
      "fc layer 1 self.abs_max_out: 6929.0\n",
      "fc layer 2 self.abs_max_out: 3655.0\n",
      "lif layer 2 self.abs_max_v: 3655.0\n",
      "fc layer 2 self.abs_max_out: 3701.0\n",
      "lif layer 2 self.abs_max_v: 3701.0\n",
      "fc layer 2 self.abs_max_out: 3781.0\n",
      "lif layer 2 self.abs_max_v: 3781.0\n",
      "fc layer 2 self.abs_max_out: 3822.0\n",
      "lif layer 2 self.abs_max_v: 3822.0\n",
      "fc layer 2 self.abs_max_out: 3841.0\n",
      "lif layer 2 self.abs_max_v: 3841.0\n",
      "fc layer 2 self.abs_max_out: 3993.0\n",
      "lif layer 2 self.abs_max_v: 3993.0\n",
      "fc layer 3 self.abs_max_out: 455.0\n",
      "fc layer 3 self.abs_max_out: 469.0\n",
      "fc layer 3 self.abs_max_out: 470.0\n",
      "lif layer 2 self.abs_max_v: 4073.0\n",
      "lif layer 2 self.abs_max_v: 4156.5\n",
      "fc layer 3 self.abs_max_out: 511.0\n",
      "lif layer 2 self.abs_max_v: 4199.0\n",
      "lif layer 2 self.abs_max_v: 4242.5\n",
      "lif layer 2 self.abs_max_v: 4264.5\n",
      "lif layer 2 self.abs_max_v: 4275.5\n",
      "lif layer 2 self.abs_max_v: 4393.5\n",
      "lif layer 2 self.abs_max_v: 4455.0\n",
      "lif layer 2 self.abs_max_v: 4530.5\n",
      "lif layer 2 self.abs_max_v: 4552.5\n",
      "lif layer 2 self.abs_max_v: 4569.5\n",
      "lif layer 2 self.abs_max_v: 4680.0\n",
      "lif layer 2 self.abs_max_v: 4735.0\n",
      "lif layer 2 self.abs_max_v: 4762.5\n",
      "lif layer 1 self.abs_max_v: 11234.0\n",
      "lif layer 1 self.abs_max_v: 12174.0\n",
      "lif layer 1 self.abs_max_v: 12551.5\n",
      "lif layer 1 self.abs_max_v: 12621.0\n",
      "fc layer 1 self.abs_max_out: 7117.0\n",
      "lif layer 1 self.abs_max_v: 13295.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 13.419933/ 80.779671, val:  35.42%, val_best:  35.42%, tr:  98.47%, tr_best:  98.67%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.9843%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2429%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4603  15.672%\n",
      "fc layer 2 self.abs_max_out: 4002.0\n",
      "fc layer 2 self.abs_max_out: 4028.0\n",
      "lif layer 2 self.abs_max_v: 4851.5\n",
      "lif layer 2 self.abs_max_v: 4908.0\n",
      "lif layer 2 self.abs_max_v: 4936.0\n",
      "lif layer 2 self.abs_max_v: 4950.0\n",
      "lif layer 2 self.abs_max_v: 4977.5\n",
      "fc layer 1 self.abs_max_out: 7504.0\n",
      "lif layer 2 self.abs_max_v: 5134.0\n",
      "fc layer 1 self.abs_max_out: 7569.0\n",
      "fc layer 1 self.abs_max_out: 7843.0\n",
      "lif layer 1 self.abs_max_v: 14313.0\n",
      "fc layer 1 self.abs_max_out: 8257.0\n",
      "lif layer 1 self.abs_max_v: 14409.5\n",
      "fc layer 1 self.abs_max_out: 8537.0\n",
      "lif layer 1 self.abs_max_v: 15742.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 12.041945/ 84.058113, val:  27.08%, val_best:  35.42%, tr:  97.75%, tr_best:  98.67%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.4721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1971%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6155  15.718%\n",
      "lif layer 2 self.abs_max_v: 5145.5\n",
      "lif layer 2 self.abs_max_v: 5301.5\n",
      "lif layer 2 self.abs_max_v: 5395.0\n",
      "lif layer 2 self.abs_max_v: 5441.5\n",
      "lif layer 2 self.abs_max_v: 5465.0\n",
      "fc layer 2 self.abs_max_out: 4069.0\n",
      "fc layer 1 self.abs_max_out: 8613.0\n",
      "fc layer 1 self.abs_max_out: 8816.0\n",
      "lif layer 1 self.abs_max_v: 16183.5\n",
      "fc layer 2 self.abs_max_out: 4083.0\n",
      "fc layer 2 self.abs_max_out: 4096.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 11.292169/ 94.251320, val:  33.33%, val_best:  35.42%, tr:  97.85%, tr_best:  98.67%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.0851%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4924%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7682  15.694%\n",
      "fc layer 2 self.abs_max_out: 4128.0\n",
      "lif layer 2 self.abs_max_v: 5576.0\n",
      "lif layer 2 self.abs_max_v: 5672.0\n",
      "lif layer 2 self.abs_max_v: 5674.0\n",
      "lif layer 2 self.abs_max_v: 5685.0\n",
      "lif layer 2 self.abs_max_v: 5690.5\n",
      "fc layer 2 self.abs_max_out: 4157.0\n",
      "lif layer 2 self.abs_max_v: 5840.5\n",
      "lif layer 2 self.abs_max_v: 5930.5\n",
      "lif layer 2 self.abs_max_v: 5975.5\n",
      "lif layer 2 self.abs_max_v: 5998.0\n",
      "lif layer 2 self.abs_max_v: 6053.0\n",
      "fc layer 2 self.abs_max_out: 4315.0\n",
      "fc layer 1 self.abs_max_out: 9104.0\n",
      "lif layer 1 self.abs_max_v: 16660.5\n",
      "fc layer 1 self.abs_max_out: 9150.0\n",
      "lif layer 2 self.abs_max_v: 6092.5\n",
      "lif layer 2 self.abs_max_v: 6193.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 11.415428/ 62.430656, val:  33.33%, val_best:  35.42%, tr:  97.55%, tr_best:  98.67%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6743%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9154  15.584%\n",
      "fc layer 2 self.abs_max_out: 4361.0\n",
      "lif layer 2 self.abs_max_v: 6314.0\n",
      "fc layer 2 self.abs_max_out: 4431.0\n",
      "lif layer 2 self.abs_max_v: 6399.0\n",
      "lif layer 2 self.abs_max_v: 6416.5\n",
      "lif layer 2 self.abs_max_v: 6425.5\n",
      "lif layer 2 self.abs_max_v: 6445.0\n",
      "fc layer 2 self.abs_max_out: 4476.0\n",
      "lif layer 2 self.abs_max_v: 6465.0\n",
      "lif layer 2 self.abs_max_v: 6516.5\n",
      "lif layer 2 self.abs_max_v: 6535.0\n",
      "lif layer 2 self.abs_max_v: 6724.5\n",
      "lif layer 2 self.abs_max_v: 6766.5\n",
      "fc layer 2 self.abs_max_out: 4723.0\n",
      "lif layer 2 self.abs_max_v: 6851.5\n",
      "lif layer 2 self.abs_max_v: 6858.0\n",
      "lif layer 2 self.abs_max_v: 7000.5\n",
      "fc layer 1 self.abs_max_out: 9457.0\n",
      "lif layer 1 self.abs_max_v: 17217.5\n",
      "lif layer 2 self.abs_max_v: 7056.0\n",
      "lif layer 2 self.abs_max_v: 7112.0\n",
      "lif layer 2 self.abs_max_v: 7140.0\n",
      "lif layer 2 self.abs_max_v: 7154.0\n",
      "lif layer 2 self.abs_max_v: 7161.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 10.852765/ 67.280151, val:  37.92%, val_best:  37.92%, tr:  97.85%, tr_best:  98.67%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9999%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1998%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10581  15.440%\n",
      "lif layer 2 self.abs_max_v: 7195.0\n",
      "lif layer 2 self.abs_max_v: 7200.0\n",
      "lif layer 2 self.abs_max_v: 7246.0\n",
      "lif layer 2 self.abs_max_v: 7448.5\n",
      "lif layer 2 self.abs_max_v: 7561.5\n",
      "lif layer 2 self.abs_max_v: 7618.0\n",
      "lif layer 2 self.abs_max_v: 7646.0\n",
      "lif layer 2 self.abs_max_v: 7660.0\n",
      "lif layer 2 self.abs_max_v: 7729.0\n",
      "lif layer 2 self.abs_max_v: 7896.5\n",
      "lif layer 2 self.abs_max_v: 7960.5\n",
      "lif layer 2 self.abs_max_v: 8017.5\n",
      "lif layer 2 self.abs_max_v: 8112.0\n",
      "fc layer 1 self.abs_max_out: 9593.0\n",
      "lif layer 1 self.abs_max_v: 17444.5\n",
      "fc layer 1 self.abs_max_out: 9750.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss: 11.357016/ 86.035698, val:  32.08%, val_best:  37.92%, tr:  97.85%, tr_best:  98.67%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2347%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7647%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 12066  15.406%\n",
      "lif layer 2 self.abs_max_v: 8118.0\n",
      "lif layer 2 self.abs_max_v: 8140.0\n",
      "lif layer 2 self.abs_max_v: 8558.0\n",
      "lif layer 2 self.abs_max_v: 8855.0\n",
      "lif layer 2 self.abs_max_v: 9003.5\n",
      "fc layer 1 self.abs_max_out: 10129.0\n",
      "lif layer 1 self.abs_max_v: 18490.0\n",
      "fc layer 1 self.abs_max_out: 10220.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss: 11.207370/ 52.928894, val:  35.42%, val_best:  37.92%, tr:  97.96%, tr_best:  98.67%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5481%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1789%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13520  15.344%\n",
      "fc layer 1 self.abs_max_out: 10511.0\n",
      "lif layer 1 self.abs_max_v: 18985.0\n",
      "fc layer 1 self.abs_max_out: 11045.0\n",
      "lif layer 1 self.abs_max_v: 19430.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss: 11.685987/ 71.503464, val:  40.83%, val_best:  40.83%, tr:  98.88%, tr_best:  98.88%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5378%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 15051  15.374%\n",
      "fc layer 2 self.abs_max_out: 4763.0\n",
      "lif layer 2 self.abs_max_v: 9127.0\n",
      "lif layer 2 self.abs_max_v: 9214.5\n",
      "lif layer 2 self.abs_max_v: 9258.5\n",
      "lif layer 2 self.abs_max_v: 9280.5\n",
      "fc layer 2 self.abs_max_out: 4791.0\n",
      "fc layer 2 self.abs_max_out: 4828.0\n",
      "fc layer 2 self.abs_max_out: 4880.0\n",
      "lif layer 2 self.abs_max_v: 9412.0\n",
      "lif layer 1 self.abs_max_v: 19989.5\n",
      "fc layer 1 self.abs_max_out: 11323.0\n",
      "fc layer 1 self.abs_max_out: 11981.0\n",
      "lif layer 1 self.abs_max_v: 21312.5\n",
      "fc layer 2 self.abs_max_out: 5122.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss: 10.678509/ 54.902248, val:  40.00%, val_best:  40.83%, tr:  98.06%, tr_best:  98.88%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6491%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5015%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 16504  15.325%\n",
      "lif layer 2 self.abs_max_v: 9517.0\n",
      "lif layer 2 self.abs_max_v: 9637.5\n",
      "lif layer 2 self.abs_max_v: 9677.5\n",
      "lif layer 2 self.abs_max_v: 9698.0\n",
      "lif layer 2 self.abs_max_v: 9708.0\n",
      "fc layer 2 self.abs_max_out: 5661.0\n",
      "fc layer 1 self.abs_max_out: 12049.0\n",
      "lif layer 1 self.abs_max_v: 21381.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss: 11.321392/ 85.529625, val:  22.50%, val_best:  40.83%, tr:  97.85%, tr_best:  98.88%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5657%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5303%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 18035  15.352%\n",
      "lif layer 2 self.abs_max_v: 9715.5\n",
      "lif layer 2 self.abs_max_v: 9754.0\n",
      "lif layer 2 self.abs_max_v: 9773.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss: 10.465050/ 79.456497, val:  32.50%, val_best:  40.83%, tr:  98.37%, tr_best:  98.88%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8878%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1136%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 19469  15.297%\n",
      "lif layer 2 self.abs_max_v: 9849.5\n",
      "lif layer 2 self.abs_max_v: 9914.0\n",
      "lif layer 2 self.abs_max_v: 9946.0\n",
      "lif layer 2 self.abs_max_v: 9962.0\n",
      "lif layer 2 self.abs_max_v: 9970.0\n",
      "lif layer 2 self.abs_max_v: 10006.5\n",
      "lif layer 2 self.abs_max_v: 10086.5\n",
      "fc layer 2 self.abs_max_out: 5886.0\n",
      "lif layer 2 self.abs_max_v: 10169.5\n",
      "lif layer 2 self.abs_max_v: 10229.5\n",
      "lif layer 2 self.abs_max_v: 10300.0\n",
      "lif layer 2 self.abs_max_v: 10456.0\n",
      "lif layer 2 self.abs_max_v: 10506.0\n",
      "lif layer 2 self.abs_max_v: 10567.0\n",
      "lif layer 2 self.abs_max_v: 10597.5\n",
      "lif layer 2 self.abs_max_v: 10613.0\n",
      "fc layer 1 self.abs_max_out: 12133.0\n",
      "fc layer 2 self.abs_max_out: 6339.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss: 10.997869/ 78.603127, val:  31.67%, val_best:  40.83%, tr:  98.57%, tr_best:  98.88%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0136%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 20951  15.286%\n",
      "lif layer 2 self.abs_max_v: 10684.5\n",
      "lif layer 2 self.abs_max_v: 10801.5\n",
      "lif layer 2 self.abs_max_v: 10860.0\n",
      "lif layer 2 self.abs_max_v: 10889.0\n",
      "lif layer 2 self.abs_max_v: 10898.0\n",
      "lif layer 2 self.abs_max_v: 11079.0\n",
      "lif layer 2 self.abs_max_v: 11080.0\n",
      "lif layer 2 self.abs_max_v: 11161.0\n",
      "fc layer 2 self.abs_max_out: 6732.0\n",
      "fc layer 1 self.abs_max_out: 12722.0\n",
      "lif layer 1 self.abs_max_v: 22226.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss: 10.506691/104.340012, val:  18.33%, val_best:  40.83%, tr:  98.67%, tr_best:  98.88%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5676%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0284%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 22400  15.254%\n",
      "fc layer 2 self.abs_max_out: 6779.0\n",
      "lif layer 2 self.abs_max_v: 12091.5\n",
      "fc layer 1 self.abs_max_out: 12963.0\n",
      "lif layer 1 self.abs_max_v: 22501.5\n",
      "lif layer 1 self.abs_max_v: 22551.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss: 10.615871/ 59.037773, val:  30.42%, val_best:  40.83%, tr:  98.06%, tr_best:  98.88%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8792%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 23880  15.245%\n",
      "fc layer 2 self.abs_max_out: 6848.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  9.793264/ 62.978214, val:  37.08%, val_best:  40.83%, tr:  98.47%, tr_best:  98.88%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0906%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0219%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 25289  15.195%\n",
      "lif layer 2 self.abs_max_v: 12496.5\n",
      "lif layer 1 self.abs_max_v: 23140.0\n",
      "fc layer 1 self.abs_max_out: 13353.0\n",
      "lif layer 1 self.abs_max_v: 23305.5\n",
      "fc layer 2 self.abs_max_out: 7206.0\n",
      "lif layer 2 self.abs_max_v: 12592.0\n",
      "lif layer 2 self.abs_max_v: 12617.5\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss: 10.443905/ 51.158779, val:  43.33%, val_best:  43.33%, tr:  97.65%, tr_best:  98.88%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8558%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0766%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 26780  15.197%\n",
      "fc layer 2 self.abs_max_out: 7324.0\n",
      "lif layer 2 self.abs_max_v: 12839.5\n",
      "lif layer 2 self.abs_max_v: 12945.5\n",
      "lif layer 2 self.abs_max_v: 13100.0\n",
      "lif layer 2 self.abs_max_v: 13292.0\n",
      "lif layer 2 self.abs_max_v: 13478.0\n",
      "fc layer 1 self.abs_max_out: 13843.0\n",
      "lif layer 1 self.abs_max_v: 24119.5\n",
      "lif layer 1 self.abs_max_v: 24292.0\n",
      "fc layer 2 self.abs_max_out: 7486.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss: 10.347740/ 61.380829, val:  40.42%, val_best:  43.33%, tr:  97.96%, tr_best:  98.88%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6488%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9707%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 28263  15.194%\n",
      "lif layer 2 self.abs_max_v: 13615.5\n",
      "lif layer 2 self.abs_max_v: 13631.0\n",
      "lif layer 2 self.abs_max_v: 13667.0\n",
      "fc layer 1 self.abs_max_out: 14307.0\n",
      "lif layer 1 self.abs_max_v: 25014.5\n",
      "lif layer 1 self.abs_max_v: 25169.5\n",
      "fc layer 2 self.abs_max_out: 7531.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  9.391720/ 62.028988, val:  34.58%, val_best:  43.33%, tr:  97.45%, tr_best:  98.88%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9443%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 29639  15.137%\n",
      "fc layer 2 self.abs_max_out: 8258.0\n",
      "fc layer 2 self.abs_max_out: 8767.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  9.757425/ 72.419647, val:  35.83%, val_best:  43.33%, tr:  98.26%, tr_best:  98.88%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5856%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4032%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 31072  15.114%\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  9.877657/ 80.064644, val:  21.67%, val_best:  43.33%, tr:  97.24%, tr_best:  98.88%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4354%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3296%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 32573  15.124%\n",
      "lif layer 2 self.abs_max_v: 13801.0\n",
      "fc layer 1 self.abs_max_out: 14407.0\n",
      "lif layer 1 self.abs_max_v: 25286.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  9.388762/ 48.501446, val:  45.00%, val_best:  45.00%, tr:  97.96%, tr_best:  98.88%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6695%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 34007  15.103%\n",
      "lif layer 2 self.abs_max_v: 13870.0\n",
      "lif layer 2 self.abs_max_v: 13876.0\n",
      "lif layer 2 self.abs_max_v: 13908.0\n",
      "fc layer 1 self.abs_max_out: 14623.0\n",
      "lif layer 1 self.abs_max_v: 25431.5\n",
      "lif layer 1 self.abs_max_v: 25590.0\n",
      "lif layer 2 self.abs_max_v: 14018.0\n",
      "lif layer 2 self.abs_max_v: 14169.0\n",
      "lif layer 2 self.abs_max_v: 14213.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  9.601132/ 42.957718, val:  40.42%, val_best:  45.00%, tr:  98.37%, tr_best:  98.88%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8087%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4336%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 35460  15.092%\n",
      "fc layer 2 self.abs_max_out: 9216.0\n",
      "fc layer 1 self.abs_max_out: 14676.0\n",
      "lif layer 1 self.abs_max_v: 25648.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  9.197492/ 32.888035, val:  42.50%, val_best:  45.00%, tr:  98.06%, tr_best:  98.88%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5042%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8269%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 36883  15.070%\n",
      "lif layer 2 self.abs_max_v: 14234.5\n",
      "fc layer 1 self.abs_max_out: 14747.0\n",
      "lif layer 1 self.abs_max_v: 25686.5\n",
      "lif layer 1 self.abs_max_v: 25813.5\n",
      "lif layer 2 self.abs_max_v: 14345.0\n",
      "lif layer 2 self.abs_max_v: 14375.5\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  9.737103/ 61.888474, val:  36.25%, val_best:  45.00%, tr:  98.16%, tr_best:  98.88%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0785%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 38418  15.093%\n",
      "lif layer 2 self.abs_max_v: 14500.5\n",
      "fc layer 1 self.abs_max_out: 14919.0\n",
      "lif layer 1 self.abs_max_v: 26009.0\n",
      "lif layer 1 self.abs_max_v: 26198.5\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  9.861763/ 56.759117, val:  40.00%, val_best:  45.00%, tr:  98.57%, tr_best:  98.88%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4412%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1292%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 39904  15.096%\n",
      "fc layer 1 self.abs_max_out: 14945.0\n",
      "lif layer 1 self.abs_max_v: 26289.5\n",
      "lif layer 2 self.abs_max_v: 14511.5\n",
      "lif layer 2 self.abs_max_v: 14537.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss: 10.346286/ 60.229652, val:  30.00%, val_best:  45.00%, tr:  98.26%, tr_best:  98.88%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4140%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 41414  15.108%\n",
      "fc layer 1 self.abs_max_out: 15026.0\n",
      "lif layer 1 self.abs_max_v: 26420.5\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  9.689813/ 58.264866, val:  33.33%, val_best:  45.00%, tr:  98.57%, tr_best:  98.88%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9388%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 42856  15.095%\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss: 10.140724/ 76.739861, val:  27.92%, val_best:  45.00%, tr:  97.65%, tr_best:  98.88%, epoch time: 79.50 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2920%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6186%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 44346  15.099%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss: 10.178527/ 35.585072, val:  37.92%, val_best:  45.00%, tr:  98.37%, tr_best:  98.88%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7580%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7738%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 45837  15.103%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  9.785716/ 86.265694, val:  32.08%, val_best:  45.00%, tr:  97.65%, tr_best:  98.88%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8428%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6129%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 47364  15.119%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  9.596349/ 67.082809, val:  35.83%, val_best:  45.00%, tr:  97.96%, tr_best:  98.88%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5495%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 48841  15.118%\n",
      "lif layer 2 self.abs_max_v: 14681.0\n",
      "lif layer 2 self.abs_max_v: 14777.5\n",
      "fc layer 1 self.abs_max_out: 15244.0\n",
      "lif layer 1 self.abs_max_v: 26929.5\n",
      "lif layer 1 self.abs_max_v: 26930.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  9.866463/ 58.886261, val:  32.92%, val_best:  45.00%, tr:  97.65%, tr_best:  98.88%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5083%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3017%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 50289  15.108%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  9.532175/ 77.084030, val:  35.00%, val_best:  45.00%, tr:  98.57%, tr_best:  98.88%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5009%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 51757  15.105%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  9.726157/ 63.033810, val:  35.42%, val_best:  45.00%, tr:  98.47%, tr_best:  98.88%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4466%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0910%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 53243  15.107%\n",
      "fc layer 1 self.abs_max_out: 15436.0\n",
      "lif layer 1 self.abs_max_v: 27393.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  9.671290/ 46.587963, val:  37.50%, val_best:  45.00%, tr:  98.98%, tr_best:  98.98%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3648%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1793%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 54670  15.093%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  9.666173/ 56.927967, val:  40.42%, val_best:  45.00%, tr:  98.57%, tr_best:  98.98%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8889%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0444%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 56135  15.089%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss: 10.305945/ 59.824657, val:  35.00%, val_best:  45.00%, tr:  98.37%, tr_best:  98.98%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7518%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0386%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 57676  15.106%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  9.905246/ 53.669968, val:  42.50%, val_best:  45.00%, tr:  99.08%, tr_best:  99.08%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9361%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7155%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 59127  15.099%\n",
      "lif layer 2 self.abs_max_v: 14815.0\n",
      "lif layer 2 self.abs_max_v: 14953.5\n",
      "lif layer 2 self.abs_max_v: 15023.0\n",
      "lif layer 2 self.abs_max_v: 15226.5\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  9.786762/ 43.487774, val:  40.83%, val_best:  45.00%, tr:  97.96%, tr_best:  99.08%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7633%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7361%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 60591  15.095%\n",
      "lif layer 2 self.abs_max_v: 15248.5\n",
      "lif layer 2 self.abs_max_v: 15338.5\n",
      "fc layer 1 self.abs_max_out: 15599.0\n",
      "lif layer 1 self.abs_max_v: 27518.0\n",
      "lif layer 1 self.abs_max_v: 27594.5\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  9.829162/ 68.005173, val:  25.42%, val_best:  45.00%, tr:  98.77%, tr_best:  99.08%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9332%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 62091  15.101%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  9.045461/ 56.993362, val:  38.75%, val_best:  45.00%, tr:  97.96%, tr_best:  99.08%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8078%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9373%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 63502  15.085%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  9.736447/ 48.211994, val:  39.17%, val_best:  45.00%, tr:  98.37%, tr_best:  99.08%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2861%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2722%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 64992  15.088%\n",
      "fc layer 1 self.abs_max_out: 15774.0\n",
      "lif layer 1 self.abs_max_v: 27988.5\n",
      "lif layer 1 self.abs_max_v: 28058.5\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss: 10.052396/ 64.756813, val:  38.33%, val_best:  45.00%, tr:  98.57%, tr_best:  99.08%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4319%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 66507  15.096%\n",
      "fc layer 1 self.abs_max_out: 16469.0\n",
      "lif layer 1 self.abs_max_v: 29436.5\n",
      "lif layer 1 self.abs_max_v: 29443.5\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss: 10.021457/ 59.617065, val:  34.17%, val_best:  45.00%, tr:  98.06%, tr_best:  99.08%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8235%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 67950  15.089%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss: 10.074744/ 74.737686, val:  27.08%, val_best:  45.00%, tr:  97.85%, tr_best:  99.08%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1548%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 69434  15.090%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss: 10.118959/ 75.090500, val:  32.92%, val_best:  45.00%, tr:  98.16%, tr_best:  99.08%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7003%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2517%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 70901  15.088%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss: 10.199884/ 76.010963, val:  34.58%, val_best:  45.00%, tr:  98.67%, tr_best:  99.08%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7312%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6198%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 72379  15.088%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss: 10.515356/ 52.358566, val:  40.83%, val_best:  45.00%, tr:  97.45%, tr_best:  99.08%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7916%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7872%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 73827  15.082%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss: 10.323834/ 55.313412, val:  30.00%, val_best:  45.00%, tr:  98.37%, tr_best:  99.08%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9825%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2036%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 75326  15.087%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss: 10.344355/ 52.186398, val:  33.75%, val_best:  45.00%, tr:  98.47%, tr_best:  99.08%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0102%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 76810  15.088%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss: 10.612553/ 60.855782, val:  38.75%, val_best:  45.00%, tr:  98.37%, tr_best:  99.08%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8940%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 78309  15.092%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss: 11.022522/101.195946, val:  27.50%, val_best:  45.00%, tr:  98.26%, tr_best:  99.08%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5183%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 79789  15.093%\n",
      "fc layer 1 self.abs_max_out: 16818.0\n",
      "lif layer 1 self.abs_max_v: 29954.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss: 10.921270/ 63.884991, val:  40.42%, val_best:  45.00%, tr:  98.16%, tr_best:  99.08%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6247%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 81295  15.098%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss: 10.163700/ 62.971996, val:  37.08%, val_best:  45.00%, tr:  98.16%, tr_best:  99.08%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7753%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9018%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 82717  15.088%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss: 10.873916/ 89.930580, val:  25.83%, val_best:  45.00%, tr:  98.88%, tr_best:  99.08%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4446%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 84160  15.082%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss: 10.258865/ 52.038338, val:  40.42%, val_best:  45.00%, tr:  99.18%, tr_best:  99.18%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3979%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 85593  15.074%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss: 10.688383/ 71.366440, val:  33.33%, val_best:  45.00%, tr:  98.57%, tr_best:  99.18%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0693%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0444%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 87056  15.072%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss: 10.634921/ 68.907043, val:  31.25%, val_best:  45.00%, tr:  98.88%, tr_best:  99.18%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0280%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6174%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 88509  15.068%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss: 10.609273/ 55.171436, val:  35.42%, val_best:  45.00%, tr:  98.88%, tr_best:  99.18%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3240%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2192%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 89942  15.061%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss: 10.639080/106.188164, val:  29.58%, val_best:  45.00%, tr:  99.08%, tr_best:  99.18%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4967%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7297%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 91419  15.061%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss: 10.359598/ 64.226593, val:  35.42%, val_best:  45.00%, tr:  98.88%, tr_best:  99.18%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1787%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 92824  15.050%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  9.933155/ 67.572937, val:  34.58%, val_best:  45.00%, tr:  98.57%, tr_best:  99.18%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4368%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1352%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 94169  15.030%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss: 10.528988/ 65.878830, val:  39.58%, val_best:  45.00%, tr:  98.88%, tr_best:  99.18%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5306%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8567%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 95630  15.028%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  9.734693/ 57.258591, val:  33.33%, val_best:  45.00%, tr:  99.18%, tr_best:  99.18%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4741%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 96988  15.010%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss: 10.404101/ 49.545017, val:  43.75%, val_best:  45.00%, tr:  98.98%, tr_best:  99.18%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1888%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 98405  15.002%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss: 10.399497/ 58.900379, val:  40.83%, val_best:  45.00%, tr:  98.57%, tr_best:  99.18%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2237%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4529%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 99833  14.996%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss: 10.346228/ 39.215229, val:  44.17%, val_best:  45.00%, tr:  98.37%, tr_best:  99.18%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4598%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0093%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 101300  14.996%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  9.857805/ 66.550011, val:  35.00%, val_best:  45.00%, tr:  97.96%, tr_best:  99.18%, epoch time: 80.35 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4010%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 102749  14.993%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss: 10.488509/ 37.544140, val:  44.17%, val_best:  45.00%, tr:  98.37%, tr_best:  99.18%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5110%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9456%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 104226  14.995%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss: 10.283487/ 56.061440, val:  39.17%, val_best:  45.00%, tr:  98.77%, tr_best:  99.18%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6642%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7530%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 105705  14.996%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss: 10.257692/ 53.602982, val:  40.83%, val_best:  45.00%, tr:  97.96%, tr_best:  99.18%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7115%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3673%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 107138  14.991%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss: 10.166870/ 90.485069, val:  36.67%, val_best:  45.00%, tr:  98.57%, tr_best:  99.18%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7571%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8860%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 108572  14.987%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss: 10.804794/ 59.607594, val:  42.50%, val_best:  45.00%, tr:  98.06%, tr_best:  99.18%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3487%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7359%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 109988  14.980%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss: 10.610561/ 48.365658, val:  44.17%, val_best:  45.00%, tr:  99.18%, tr_best:  99.18%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4051%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9315%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 111418  14.975%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss: 10.732618/ 70.128258, val:  39.17%, val_best:  45.00%, tr:  98.67%, tr_best:  99.18%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8419%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 112872  14.973%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss: 10.417168/101.215073, val:  23.33%, val_best:  45.00%, tr:  98.26%, tr_best:  99.18%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0313%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9447%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 114291  14.967%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss: 10.258347/ 64.653305, val:  34.17%, val_best:  45.00%, tr:  98.88%, tr_best:  99.18%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3725%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 115738  14.965%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss: 10.553319/ 82.451279, val:  32.92%, val_best:  45.00%, tr:  98.47%, tr_best:  99.18%, epoch time: 80.09 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5788%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8210%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 117179  14.962%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss: 10.921036/ 63.806797, val:  36.25%, val_best:  45.00%, tr:  98.88%, tr_best:  99.18%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5772%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7995%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 118657  14.963%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss: 10.531337/ 68.542274, val:  32.08%, val_best:  45.00%, tr:  98.88%, tr_best:  99.18%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8623%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 120130  14.964%\n",
      "fc layer 2 self.abs_max_out: 9381.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss: 10.719102/ 60.227844, val:  37.08%, val_best:  45.00%, tr:  98.98%, tr_best:  99.18%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7866%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3847%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 121582  14.963%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss: 10.993790/101.357964, val:  32.08%, val_best:  45.00%, tr:  98.88%, tr_best:  99.18%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9152%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5603%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 123024  14.960%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss: 10.681723/ 68.557808, val:  34.58%, val_best:  45.00%, tr:  98.67%, tr_best:  99.18%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8424%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 124457  14.956%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss: 10.710385/ 63.478779, val:  31.67%, val_best:  45.00%, tr:  98.98%, tr_best:  99.18%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3899%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6815%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 125933  14.957%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss: 10.225230/ 57.025375, val:  34.58%, val_best:  45.00%, tr:  98.57%, tr_best:  99.18%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5205%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 127382  14.956%\n",
      "lif layer 2 self.abs_max_v: 15435.5\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss: 10.747338/ 79.585220, val:  40.42%, val_best:  45.00%, tr:  98.77%, tr_best:  99.18%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8893%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7053%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 128836  14.954%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss: 11.096078/ 67.044083, val:  40.00%, val_best:  45.00%, tr:  97.96%, tr_best:  99.18%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 130309  14.956%\n",
      "lif layer 2 self.abs_max_v: 15530.0\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss: 11.113169/ 47.970600, val:  39.17%, val_best:  45.00%, tr:  97.85%, tr_best:  99.18%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4636%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 131797  14.958%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss: 10.620713/ 57.431908, val:  40.42%, val_best:  45.00%, tr:  98.37%, tr_best:  99.18%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3595%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2806%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 133247  14.957%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss: 10.524591/ 68.682594, val:  36.25%, val_best:  45.00%, tr:  98.98%, tr_best:  99.18%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4906%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6649%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 134685  14.954%\n",
      "fc layer 2 self.abs_max_out: 9690.0\n",
      "lif layer 2 self.abs_max_v: 15636.5\n",
      "lif layer 2 self.abs_max_v: 16721.5\n",
      "lif layer 2 self.abs_max_v: 18051.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss: 10.675114/ 96.527252, val:  30.83%, val_best:  45.00%, tr:  98.57%, tr_best:  99.18%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4176%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7259%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 136168  14.956%\n",
      "fc layer 2 self.abs_max_out: 9876.0\n",
      "lif layer 2 self.abs_max_v: 18347.5\n",
      "lif layer 2 self.abs_max_v: 18508.0\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss: 10.210091/ 78.180855, val:  26.67%, val_best:  45.00%, tr:  98.37%, tr_best:  99.18%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4455%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 137602  14.953%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss: 10.202789/ 63.965691, val:  41.25%, val_best:  45.00%, tr:  98.98%, tr_best:  99.18%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2503%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8304%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 139040  14.950%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss: 10.043864/ 49.648624, val:  42.50%, val_best:  45.00%, tr:  98.98%, tr_best:  99.18%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4493%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7216%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 140474  14.947%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  9.974416/ 48.960644, val:  44.17%, val_best:  45.00%, tr:  98.26%, tr_best:  99.18%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3987%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4932%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 141915  14.944%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss: 10.292974/ 66.679848, val:  38.33%, val_best:  45.00%, tr:  98.67%, tr_best:  99.18%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8322%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4425%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 143378  14.944%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss: 10.133140/ 69.196129, val:  37.08%, val_best:  45.00%, tr:  98.98%, tr_best:  99.18%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6505%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 144788  14.939%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss: 10.135109/ 38.914482, val:  41.67%, val_best:  45.00%, tr:  98.57%, tr_best:  99.18%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3568%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5918%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 146248  14.939%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss: 10.238083/ 62.467926, val:  37.08%, val_best:  45.00%, tr:  99.18%, tr_best:  99.18%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0948%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0628%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 147667  14.934%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss: 10.983181/ 65.737984, val:  45.00%, val_best:  45.00%, tr:  98.67%, tr_best:  99.18%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7827%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 149152  14.936%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss: 10.286138/ 78.636513, val:  32.92%, val_best:  45.00%, tr:  99.28%, tr_best:  99.28%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5274%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 150591  14.934%\n",
      "fc layer 2 self.abs_max_out: 10223.0\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss: 10.810627/ 43.664974, val:  46.25%, val_best:  46.25%, tr:  98.67%, tr_best:  99.28%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4492%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4770%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 152045  14.933%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss: 10.325765/ 54.513756, val:  31.25%, val_best:  46.25%, tr:  98.16%, tr_best:  99.28%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5492%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5242%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 153453  14.928%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss: 10.701771/ 54.121128, val:  22.50%, val_best:  46.25%, tr:  98.67%, tr_best:  99.28%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3361%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2310%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 154983  14.935%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss: 10.117858/ 97.527061, val:  28.75%, val_best:  46.25%, tr:  99.08%, tr_best:  99.28%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3253%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3234%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 156494  14.939%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  9.649107/ 66.892433, val:  40.42%, val_best:  46.25%, tr:  98.98%, tr_best:  99.28%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2802%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 157963  14.940%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss: 10.111959/ 51.880600, val:  42.50%, val_best:  46.25%, tr:  98.47%, tr_best:  99.28%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7850%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3366%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 159501  14.947%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  9.733275/ 77.467712, val:  31.25%, val_best:  46.25%, tr:  99.08%, tr_best:  99.28%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3147%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3441%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 160952  14.946%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  9.932237/ 64.920609, val:  35.00%, val_best:  46.25%, tr:  98.57%, tr_best:  99.28%, epoch time: 80.08 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3370%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 162414  14.946%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss: 10.139841/ 59.689144, val:  38.75%, val_best:  46.25%, tr:  98.77%, tr_best:  99.28%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4450%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 163922  14.950%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss: 10.565346/ 46.149597, val:  49.17%, val_best:  49.17%, tr:  98.77%, tr_best:  99.28%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5726%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7486%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 165404  14.952%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss: 10.124230/ 77.847702, val:  35.42%, val_best:  49.17%, tr:  98.57%, tr_best:  99.28%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4621%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9071%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 166864  14.951%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss: 10.253449/ 63.571777, val:  30.83%, val_best:  49.17%, tr:  97.85%, tr_best:  99.28%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1574%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 168306  14.949%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss: 10.356788/ 83.476387, val:  24.58%, val_best:  49.17%, tr:  99.28%, tr_best:  99.28%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5621%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4345%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 169777  14.950%\n",
      "fc layer 2 self.abs_max_out: 10349.0\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  9.807143/ 56.344395, val:  43.33%, val_best:  49.17%, tr:  98.88%, tr_best:  99.28%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2861%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2413%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 171221  14.948%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss: 10.891999/ 61.466473, val:  49.58%, val_best:  49.58%, tr:  98.37%, tr_best:  99.28%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5181%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 172696  14.949%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss: 10.505461/ 65.435524, val:  39.58%, val_best:  49.58%, tr:  99.18%, tr_best:  99.28%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4684%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7143%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 174119  14.946%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss: 10.328904/ 60.370155, val:  42.92%, val_best:  49.58%, tr:  99.08%, tr_best:  99.28%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7586%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 175541  14.942%\n",
      "fc layer 2 self.abs_max_out: 10401.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss: 10.358818/ 58.188229, val:  52.08%, val_best:  52.08%, tr:  98.98%, tr_best:  99.28%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0343%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 176978  14.940%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss: 10.494786/ 69.839096, val:  34.17%, val_best:  52.08%, tr:  98.67%, tr_best:  99.28%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6963%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8725%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 178415  14.938%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss: 10.774324/ 64.299698, val:  38.75%, val_best:  52.08%, tr:  99.18%, tr_best:  99.28%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0959%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 179903  14.940%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss: 10.115773/ 61.587666, val:  38.75%, val_best:  52.08%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5691%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5386%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 181398  14.943%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss: 10.506771/ 52.891525, val:  45.42%, val_best:  52.08%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5013%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3751%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 182928  14.948%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss: 10.482310/ 76.210442, val:  33.33%, val_best:  52.08%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6689%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 184439  14.952%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss: 10.047641/ 65.751862, val:  31.67%, val_best:  52.08%, tr:  98.47%, tr_best:  99.39%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5187%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4889%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 185865  14.949%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss: 10.250123/ 75.857521, val:  25.00%, val_best:  52.08%, tr:  99.18%, tr_best:  99.39%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9569%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 187296  14.946%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss: 10.374671/ 62.157612, val:  37.08%, val_best:  52.08%, tr:  98.16%, tr_best:  99.39%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7089%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6248%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 188777  14.948%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  9.794395/ 72.387024, val:  34.58%, val_best:  52.08%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6570%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1180%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 190212  14.946%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss: 10.452964/112.200928, val:  29.58%, val_best:  52.08%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4856%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 191719  14.949%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss: 10.530302/ 78.939064, val:  32.50%, val_best:  52.08%, tr:  98.67%, tr_best:  99.39%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9399%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4399%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 193202  14.950%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  9.927042/ 64.569336, val:  39.17%, val_best:  52.08%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9539%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7208%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 194659  14.950%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss: 10.490773/ 54.398605, val:  38.75%, val_best:  52.08%, tr:  98.06%, tr_best:  99.39%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 196164  14.953%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss: 10.788929/ 49.052441, val:  38.75%, val_best:  52.08%, tr:  99.08%, tr_best:  99.39%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5910%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3698%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 197689  14.958%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss: 10.423053/ 57.026733, val:  39.58%, val_best:  52.08%, tr:  98.26%, tr_best:  99.39%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8714%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3027%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 199210  14.962%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss: 10.197369/ 51.433926, val:  38.75%, val_best:  52.08%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4262%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 200693  14.963%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss: 10.195454/ 59.688232, val:  35.42%, val_best:  52.08%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5519%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3609%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 202150  14.963%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss: 10.307592/ 68.102547, val:  36.67%, val_best:  52.08%, tr:  98.77%, tr_best:  99.39%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5818%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2153%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 203670  14.967%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss: 10.415998/ 85.771812, val:  39.17%, val_best:  52.08%, tr:  98.77%, tr_best:  99.39%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3884%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3261%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 205210  14.972%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss: 10.710837/ 66.687263, val:  33.75%, val_best:  52.08%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9961%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4628%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 206743  14.977%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss: 10.858114/ 66.881912, val:  39.58%, val_best:  52.08%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3988%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3129%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 208244  14.980%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss: 10.609631/ 62.968513, val:  40.00%, val_best:  52.08%, tr:  98.47%, tr_best:  99.39%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8752%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 209843  14.989%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss: 10.426015/ 78.814362, val:  30.83%, val_best:  52.08%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3042%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8168%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 211370  14.993%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss: 10.221604/ 69.511841, val:  31.67%, val_best:  52.08%, tr:  98.47%, tr_best:  99.39%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2305%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2623%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 212907  14.998%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss: 10.632792/ 75.573326, val:  34.58%, val_best:  52.08%, tr:  98.77%, tr_best:  99.39%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0005%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 214469  15.005%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss: 10.608336/ 59.475761, val:  40.42%, val_best:  52.08%, tr:  98.57%, tr_best:  99.39%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4194%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 215990  15.008%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss: 10.335338/ 78.416855, val:  32.50%, val_best:  52.08%, tr:  98.77%, tr_best:  99.39%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0202%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4292%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 217498  15.011%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss: 10.995310/ 98.389938, val:  31.25%, val_best:  52.08%, tr:  98.77%, tr_best:  99.39%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8036%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4721%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 219038  15.016%\n",
      "fc layer 2 self.abs_max_out: 10623.0\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss: 10.751737/ 59.411476, val:  39.17%, val_best:  52.08%, tr:  98.26%, tr_best:  99.39%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5326%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7232%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 220513  15.016%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss: 10.521873/ 66.628967, val:  35.00%, val_best:  52.08%, tr:  99.08%, tr_best:  99.39%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6847%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0570%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 221968  15.015%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss: 10.346202/ 42.447128, val:  46.25%, val_best:  52.08%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4151%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8082%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 223408  15.013%\n",
      "fc layer 2 self.abs_max_out: 10632.0\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss: 10.612142/ 48.847282, val:  43.33%, val_best:  52.08%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1651%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 224885  15.014%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss: 10.962763/ 67.145439, val:  39.17%, val_best:  52.08%, tr:  98.37%, tr_best:  99.39%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2098%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 226390  15.016%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss: 10.747089/ 62.035202, val:  43.75%, val_best:  52.08%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7120%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 227868  15.017%\n",
      "lif layer 2 self.abs_max_v: 18921.0\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss: 11.217363/ 71.236504, val:  33.33%, val_best:  52.08%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1200%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 229391  15.020%\n",
      "fc layer 2 self.abs_max_out: 10878.0\n",
      "fc layer 2 self.abs_max_out: 12210.0\n",
      "lif layer 2 self.abs_max_v: 20586.5\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss: 10.685551/ 55.549507, val:  43.33%, val_best:  52.08%, tr:  98.37%, tr_best:  99.39%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3266%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6616%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 230832  15.018%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss: 10.859975/ 47.859676, val:  45.00%, val_best:  52.08%, tr:  98.98%, tr_best:  99.39%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2151%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8332%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 232287  15.017%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss: 10.824907/ 98.635223, val:  27.92%, val_best:  52.08%, tr:  98.67%, tr_best:  99.39%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1491%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0541%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 233812  15.021%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss: 11.035952/ 83.901070, val:  37.50%, val_best:  52.08%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3405%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4210%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 235298  15.022%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss: 10.527109/ 69.010841, val:  39.17%, val_best:  52.08%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6227%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2132%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 236678  15.016%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss: 10.737307/ 84.251747, val:  37.92%, val_best:  52.08%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8723%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7405%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 238096  15.013%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss: 10.806588/ 58.827671, val:  45.00%, val_best:  52.08%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4223%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3359%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 239524  15.010%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss: 10.866617/ 61.559532, val:  40.00%, val_best:  52.08%, tr:  98.57%, tr_best:  99.39%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6330%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2603%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 240959  15.008%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss: 11.027192/ 66.047211, val:  45.83%, val_best:  52.08%, tr:  98.98%, tr_best:  99.39%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5841%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7795%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 242397  15.006%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss: 11.469769/ 68.620461, val:  36.25%, val_best:  52.08%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 243906  15.008%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss: 11.058787/ 88.073746, val:  39.17%, val_best:  52.08%, tr:  99.39%, tr_best:  99.39%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9770%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8803%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 245378  15.008%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss: 11.036008/ 73.125458, val:  39.17%, val_best:  52.08%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7007%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4847%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 246856  15.009%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss: 11.184628/ 72.968636, val:  35.42%, val_best:  52.08%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6703%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2384%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 248283  15.006%\n",
      "lif layer 2 self.abs_max_v: 20803.0\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss: 10.482286/ 64.445366, val:  41.67%, val_best:  52.08%, tr:  99.28%, tr_best:  99.39%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6244%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 249695  15.003%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss: 10.976125/ 68.193192, val:  37.08%, val_best:  52.08%, tr:  98.67%, tr_best:  99.39%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3911%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0393%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 251198  15.005%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss: 10.745522/ 80.573990, val:  36.25%, val_best:  52.08%, tr:  98.98%, tr_best:  99.39%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4573%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9949%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 252640  15.003%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss: 10.984205/ 51.517136, val:  40.42%, val_best:  52.08%, tr:  98.98%, tr_best:  99.39%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2201%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 254133  15.005%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  9.857965/ 67.217293, val:  30.00%, val_best:  52.08%, tr:  98.16%, tr_best:  99.39%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1118%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 255532  15.001%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss: 10.461485/ 52.963459, val:  45.83%, val_best:  52.08%, tr:  98.77%, tr_best:  99.39%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6964%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5753%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 256969  14.999%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss: 10.971260/ 52.949448, val:  35.42%, val_best:  52.08%, tr:  98.67%, tr_best:  99.39%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7374%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7839%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 258433  14.999%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss: 10.505906/ 83.110657, val:  31.25%, val_best:  52.08%, tr:  99.28%, tr_best:  99.39%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9885%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9132%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 259861  14.996%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss: 10.538591/ 66.074738, val:  38.33%, val_best:  52.08%, tr:  97.96%, tr_best:  99.39%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0660%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4370%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 261339  14.997%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss: 10.400842/ 60.691151, val:  32.92%, val_best:  52.08%, tr:  98.06%, tr_best:  99.39%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4117%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 262806  14.997%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss: 10.139066/ 60.478695, val:  35.83%, val_best:  52.08%, tr:  98.67%, tr_best:  99.39%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0084%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1916%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 264219  14.994%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss: 10.216941/ 72.784897, val:  37.92%, val_best:  52.08%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1612%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7608%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 265637  14.991%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss: 10.219103/ 65.647423, val:  32.92%, val_best:  52.08%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5824%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3467%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 267011  14.986%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss: 10.376443/ 69.510612, val:  27.92%, val_best:  52.08%, tr:  98.67%, tr_best:  99.39%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3913%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0365%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 268438  14.983%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss: 10.239904/ 83.581024, val:  27.50%, val_best:  52.08%, tr:  98.88%, tr_best:  99.39%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2551%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7622%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 269851  14.980%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss: 10.658047/ 80.214058, val:  38.75%, val_best:  52.08%, tr:  98.06%, tr_best:  99.39%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5436%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 271265  14.978%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss: 10.968431/ 76.019051, val:  37.92%, val_best:  52.08%, tr:  98.98%, tr_best:  99.39%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6844%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6526%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 272708  14.976%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss: 11.033999/ 72.407715, val:  42.92%, val_best:  52.08%, tr:  98.47%, tr_best:  99.39%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7427%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1014%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 274153  14.975%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss: 11.025760/105.191940, val:  23.33%, val_best:  52.08%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9087%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8961%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 275564  14.972%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss: 10.744359/ 75.285400, val:  42.50%, val_best:  52.08%, tr:  98.88%, tr_best:  99.39%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7758%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 276983  14.970%\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss: 10.794314/ 74.921722, val:  40.42%, val_best:  52.08%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7565%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 278406  14.967%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss: 11.252452/ 62.232819, val:  45.00%, val_best:  52.08%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8306%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8869%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 279879  14.968%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss: 11.632155/ 83.362587, val:  34.58%, val_best:  52.08%, tr:  99.18%, tr_best:  99.39%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9697%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 281365  14.969%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss: 11.098970/ 56.467514, val:  41.67%, val_best:  52.08%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6249%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0117%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 282762  14.965%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss: 11.379392/ 98.591408, val:  31.67%, val_best:  52.08%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2747%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9937%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 284244  14.966%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss: 11.057573/103.795631, val:  30.00%, val_best:  52.08%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1339%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5829%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 285642  14.963%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss: 11.301637/ 62.149456, val:  46.25%, val_best:  52.08%, tr:  99.28%, tr_best:  99.39%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3718%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 287061  14.960%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss: 11.986982/ 74.406181, val:  37.08%, val_best:  52.08%, tr:  97.96%, tr_best:  99.39%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0271%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1959%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 288552  14.962%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss: 10.971963/ 70.855743, val:  42.50%, val_best:  52.08%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9979%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 289937  14.957%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss: 11.081672/ 93.110680, val:  35.00%, val_best:  52.08%, tr:  98.77%, tr_best:  99.39%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6179%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0153%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 291352  14.955%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss: 11.127420/ 89.943718, val:  31.67%, val_best:  52.08%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9886%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4893%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ac933981154759a16575f90262d2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÅ‚ñÑ‚ñÑ</td></tr><tr><td>tr_acc</td><td>‚ñÜ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñà‚ñÜ‚ñÜ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÅ‚ñÑ‚ñÑ</td></tr><tr><td>val_loss</td><td>‚ñÖ‚ñÉ‚ñÜ‚ñÇ‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñá‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñá‚ñÜ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98876</td></tr><tr><td>tr_epoch_loss</td><td>11.12742</td></tr><tr><td>val_acc_best</td><td>0.52083</td></tr><tr><td>val_acc_now</td><td>0.31667</td></tr><tr><td>val_loss</td><td>89.94372</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">treasured-sweep-4</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pdkgxgk8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pdkgxgk8</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_013637-pdkgxgk8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bcipyx30 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_055813-bcipyx30</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bcipyx30' target=\"_blank\">glad-sweep-9</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bcipyx30' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bcipyx30</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251214_055821_388', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 16, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 0.5, 'lif_layer_v_threshold2': 128, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 4, self.v_threshold 16\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 0.5, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=16, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=0.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 41.0\n",
      "lif layer 2 self.abs_max_v: 41.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 51.0\n",
      "fc layer 2 self.abs_max_out: 80.0\n",
      "lif layer 2 self.abs_max_v: 94.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "lif layer 2 self.abs_max_v: 107.5\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "fc layer 2 self.abs_max_out: 82.0\n",
      "lif layer 2 self.abs_max_v: 121.5\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 138.0\n",
      "fc layer 2 self.abs_max_out: 87.0\n",
      "lif layer 2 self.abs_max_v: 142.5\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 210.0\n",
      "fc layer 2 self.abs_max_out: 126.0\n",
      "lif layer 2 self.abs_max_v: 194.5\n",
      "fc layer 3 self.abs_max_out: 10.0\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 270.0\n",
      "fc layer 2 self.abs_max_out: 195.0\n",
      "lif layer 2 self.abs_max_v: 292.5\n",
      "fc layer 3 self.abs_max_out: 16.0\n",
      "fc layer 1 self.abs_max_out: 225.0\n",
      "lif layer 1 self.abs_max_v: 360.0\n",
      "lif layer 2 self.abs_max_v: 326.5\n",
      "fc layer 3 self.abs_max_out: 17.0\n",
      "fc layer 1 self.abs_max_out: 234.0\n",
      "lif layer 1 self.abs_max_v: 381.0\n",
      "lif layer 2 self.abs_max_v: 338.5\n",
      "fc layer 3 self.abs_max_out: 25.0\n",
      "lif layer 2 self.abs_max_v: 345.5\n",
      "fc layer 1 self.abs_max_out: 264.0\n",
      "fc layer 2 self.abs_max_out: 229.0\n",
      "lif layer 2 self.abs_max_v: 348.0\n",
      "lif layer 2 self.abs_max_v: 355.0\n",
      "lif layer 2 self.abs_max_v: 356.0\n",
      "fc layer 3 self.abs_max_out: 26.0\n",
      "lif layer 2 self.abs_max_v: 381.0\n",
      "fc layer 1 self.abs_max_out: 315.0\n",
      "lif layer 1 self.abs_max_v: 504.0\n",
      "fc layer 2 self.abs_max_out: 232.0\n",
      "lif layer 2 self.abs_max_v: 407.5\n",
      "lif layer 1 self.abs_max_v: 514.0\n",
      "fc layer 2 self.abs_max_out: 243.0\n",
      "lif layer 2 self.abs_max_v: 437.0\n",
      "lif layer 1 self.abs_max_v: 522.0\n",
      "fc layer 1 self.abs_max_out: 424.0\n",
      "fc layer 2 self.abs_max_out: 255.0\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "lif layer 1 self.abs_max_v: 591.0\n",
      "lif layer 2 self.abs_max_v: 453.5\n",
      "fc layer 3 self.abs_max_out: 37.0\n",
      "lif layer 1 self.abs_max_v: 592.5\n",
      "lif layer 2 self.abs_max_v: 459.0\n",
      "fc layer 2 self.abs_max_out: 262.0\n",
      "fc layer 2 self.abs_max_out: 290.0\n",
      "fc layer 3 self.abs_max_out: 62.0\n",
      "fc layer 2 self.abs_max_out: 313.0\n",
      "fc layer 2 self.abs_max_out: 333.0\n",
      "fc layer 2 self.abs_max_out: 375.0\n",
      "lif layer 1 self.abs_max_v: 693.5\n",
      "lif layer 1 self.abs_max_v: 696.5\n",
      "lif layer 2 self.abs_max_v: 508.5\n",
      "lif layer 2 self.abs_max_v: 512.5\n",
      "lif layer 2 self.abs_max_v: 538.5\n",
      "lif layer 2 self.abs_max_v: 588.0\n",
      "lif layer 2 self.abs_max_v: 623.5\n",
      "fc layer 1 self.abs_max_out: 444.0\n",
      "fc layer 1 self.abs_max_out: 532.0\n",
      "lif layer 1 self.abs_max_v: 834.0\n",
      "fc layer 2 self.abs_max_out: 444.0\n",
      "lif layer 1 self.abs_max_v: 923.0\n",
      "lif layer 2 self.abs_max_v: 624.0\n",
      "lif layer 1 self.abs_max_v: 971.0\n",
      "fc layer 1 self.abs_max_out: 699.0\n",
      "fc layer 1 self.abs_max_out: 755.0\n",
      "fc layer 2 self.abs_max_out: 478.0\n",
      "fc layer 3 self.abs_max_out: 67.0\n",
      "fc layer 3 self.abs_max_out: 68.0\n",
      "lif layer 2 self.abs_max_v: 649.0\n",
      "lif layer 2 self.abs_max_v: 672.0\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "lif layer 1 self.abs_max_v: 1015.0\n",
      "fc layer 3 self.abs_max_out: 77.0\n",
      "fc layer 2 self.abs_max_out: 497.0\n",
      "fc layer 2 self.abs_max_out: 505.0\n",
      "fc layer 3 self.abs_max_out: 88.0\n",
      "fc layer 3 self.abs_max_out: 93.0\n",
      "fc layer 3 self.abs_max_out: 108.0\n",
      "lif layer 1 self.abs_max_v: 1033.5\n",
      "lif layer 2 self.abs_max_v: 705.0\n",
      "lif layer 2 self.abs_max_v: 716.5\n",
      "lif layer 2 self.abs_max_v: 734.5\n",
      "lif layer 2 self.abs_max_v: 741.5\n",
      "lif layer 2 self.abs_max_v: 767.0\n",
      "fc layer 2 self.abs_max_out: 514.0\n",
      "lif layer 2 self.abs_max_v: 827.5\n",
      "lif layer 2 self.abs_max_v: 909.0\n",
      "lif layer 2 self.abs_max_v: 921.5\n",
      "lif layer 2 self.abs_max_v: 956.0\n",
      "fc layer 2 self.abs_max_out: 517.0\n",
      "fc layer 2 self.abs_max_out: 522.0\n",
      "fc layer 2 self.abs_max_out: 565.0\n",
      "lif layer 2 self.abs_max_v: 1041.0\n",
      "lif layer 2 self.abs_max_v: 1070.5\n",
      "fc layer 3 self.abs_max_out: 116.0\n",
      "fc layer 2 self.abs_max_out: 609.0\n",
      "fc layer 2 self.abs_max_out: 641.0\n",
      "lif layer 2 self.abs_max_v: 1091.5\n",
      "fc layer 3 self.abs_max_out: 143.0\n",
      "fc layer 2 self.abs_max_out: 646.0\n",
      "lif layer 2 self.abs_max_v: 1192.0\n",
      "fc layer 2 self.abs_max_out: 654.0\n",
      "lif layer 2 self.abs_max_v: 1225.0\n",
      "lif layer 2 self.abs_max_v: 1243.5\n",
      "lif layer 2 self.abs_max_v: 1245.0\n",
      "lif layer 2 self.abs_max_v: 1269.5\n",
      "fc layer 1 self.abs_max_out: 769.0\n",
      "fc layer 1 self.abs_max_out: 860.0\n",
      "fc layer 2 self.abs_max_out: 668.0\n",
      "fc layer 2 self.abs_max_out: 671.0\n",
      "lif layer 2 self.abs_max_v: 1296.5\n",
      "fc layer 2 self.abs_max_out: 696.0\n",
      "lif layer 2 self.abs_max_v: 1318.0\n",
      "lif layer 2 self.abs_max_v: 1344.0\n",
      "fc layer 2 self.abs_max_out: 734.0\n",
      "lif layer 2 self.abs_max_v: 1355.0\n",
      "fc layer 3 self.abs_max_out: 144.0\n",
      "fc layer 3 self.abs_max_out: 150.0\n",
      "fc layer 3 self.abs_max_out: 152.0\n",
      "fc layer 3 self.abs_max_out: 198.0\n",
      "lif layer 1 self.abs_max_v: 1265.0\n",
      "lif layer 1 self.abs_max_v: 1273.0\n",
      "lif layer 1 self.abs_max_v: 1320.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 2 self.abs_max_out: 742.0\n",
      "fc layer 2 self.abs_max_out: 753.0\n",
      "lif layer 2 self.abs_max_v: 1426.5\n",
      "fc layer 2 self.abs_max_out: 757.0\n",
      "lif layer 2 self.abs_max_v: 1470.5\n",
      "fc layer 1 self.abs_max_out: 956.0\n",
      "fc layer 1 self.abs_max_out: 961.0\n",
      "fc layer 2 self.abs_max_out: 759.0\n",
      "fc layer 2 self.abs_max_out: 776.0\n",
      "fc layer 2 self.abs_max_out: 781.0\n",
      "fc layer 1 self.abs_max_out: 1034.0\n",
      "lif layer 1 self.abs_max_v: 1615.0\n",
      "lif layer 1 self.abs_max_v: 1827.5\n",
      "lif layer 2 self.abs_max_v: 1472.5\n",
      "lif layer 2 self.abs_max_v: 1496.5\n",
      "lif layer 2 self.abs_max_v: 1508.5\n",
      "fc layer 1 self.abs_max_out: 1036.0\n",
      "fc layer 1 self.abs_max_out: 1097.0\n",
      "fc layer 1 self.abs_max_out: 1104.0\n",
      "fc layer 2 self.abs_max_out: 795.0\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "fc layer 3 self.abs_max_out: 231.0\n",
      "fc layer 2 self.abs_max_out: 802.0\n",
      "fc layer 2 self.abs_max_out: 840.0\n",
      "fc layer 1 self.abs_max_out: 1163.0\n",
      "lif layer 1 self.abs_max_v: 1882.0\n",
      "fc layer 2 self.abs_max_out: 852.0\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "fc layer 3 self.abs_max_out: 256.0\n",
      "fc layer 3 self.abs_max_out: 257.0\n",
      "lif layer 1 self.abs_max_v: 1977.0\n",
      "lif layer 1 self.abs_max_v: 2049.5\n",
      "lif layer 1 self.abs_max_v: 2058.0\n",
      "fc layer 1 self.abs_max_out: 1178.0\n",
      "lif layer 1 self.abs_max_v: 2061.5\n",
      "lif layer 1 self.abs_max_v: 2094.5\n",
      "fc layer 1 self.abs_max_out: 1246.0\n",
      "lif layer 1 self.abs_max_v: 2174.5\n",
      "lif layer 1 self.abs_max_v: 2250.0\n",
      "lif layer 1 self.abs_max_v: 2348.0\n",
      "fc layer 2 self.abs_max_out: 896.0\n",
      "fc layer 1 self.abs_max_out: 1275.0\n",
      "fc layer 2 self.abs_max_out: 921.0\n",
      "fc layer 2 self.abs_max_out: 974.0\n",
      "fc layer 2 self.abs_max_out: 985.0\n",
      "fc layer 1 self.abs_max_out: 1482.0\n",
      "lif layer 1 self.abs_max_v: 2429.5\n",
      "fc layer 1 self.abs_max_out: 1638.0\n",
      "lif layer 1 self.abs_max_v: 2853.0\n",
      "lif layer 1 self.abs_max_v: 2908.0\n",
      "fc layer 2 self.abs_max_out: 993.0\n",
      "fc layer 2 self.abs_max_out: 1006.0\n",
      "fc layer 2 self.abs_max_out: 1083.0\n",
      "lif layer 2 self.abs_max_v: 1561.5\n",
      "lif layer 1 self.abs_max_v: 2999.0\n",
      "lif layer 2 self.abs_max_v: 1564.0\n",
      "lif layer 2 self.abs_max_v: 1602.0\n",
      "lif layer 2 self.abs_max_v: 1623.0\n",
      "lif layer 2 self.abs_max_v: 1625.0\n",
      "lif layer 2 self.abs_max_v: 1632.5\n",
      "lif layer 2 self.abs_max_v: 1661.0\n",
      "lif layer 2 self.abs_max_v: 1698.5\n",
      "lif layer 2 self.abs_max_v: 1868.5\n",
      "lif layer 2 self.abs_max_v: 1924.5\n",
      "lif layer 2 self.abs_max_v: 1975.5\n",
      "lif layer 2 self.abs_max_v: 2010.0\n",
      "fc layer 2 self.abs_max_out: 1117.0\n",
      "fc layer 2 self.abs_max_out: 1123.0\n",
      "fc layer 2 self.abs_max_out: 1124.0\n",
      "fc layer 2 self.abs_max_out: 1160.0\n",
      "fc layer 2 self.abs_max_out: 1164.0\n",
      "fc layer 1 self.abs_max_out: 1722.0\n",
      "lif layer 1 self.abs_max_v: 3025.0\n",
      "fc layer 1 self.abs_max_out: 1741.0\n",
      "fc layer 2 self.abs_max_out: 1175.0\n",
      "fc layer 2 self.abs_max_out: 1210.0\n",
      "fc layer 2 self.abs_max_out: 1283.0\n",
      "fc layer 2 self.abs_max_out: 1291.0\n",
      "lif layer 2 self.abs_max_v: 2108.0\n",
      "fc layer 2 self.abs_max_out: 1353.0\n",
      "fc layer 2 self.abs_max_out: 1355.0\n",
      "fc layer 2 self.abs_max_out: 1371.0\n",
      "lif layer 2 self.abs_max_v: 2208.5\n",
      "lif layer 2 self.abs_max_v: 2220.5\n",
      "lif layer 2 self.abs_max_v: 2246.5\n",
      "fc layer 1 self.abs_max_out: 1752.0\n",
      "lif layer 1 self.abs_max_v: 3055.5\n",
      "fc layer 2 self.abs_max_out: 1398.0\n",
      "fc layer 2 self.abs_max_out: 1418.0\n",
      "fc layer 2 self.abs_max_out: 1454.0\n",
      "fc layer 2 self.abs_max_out: 1461.0\n",
      "lif layer 2 self.abs_max_v: 2273.5\n",
      "lif layer 2 self.abs_max_v: 2332.0\n",
      "lif layer 2 self.abs_max_v: 2362.0\n",
      "fc layer 2 self.abs_max_out: 1523.0\n",
      "fc layer 3 self.abs_max_out: 267.0\n",
      "fc layer 3 self.abs_max_out: 272.0\n",
      "lif layer 1 self.abs_max_v: 3107.5\n",
      "lif layer 1 self.abs_max_v: 3166.0\n",
      "fc layer 1 self.abs_max_out: 1787.0\n",
      "lif layer 1 self.abs_max_v: 3362.0\n",
      "lif layer 1 self.abs_max_v: 3396.0\n",
      "lif layer 1 self.abs_max_v: 3444.0\n",
      "fc layer 3 self.abs_max_out: 302.0\n",
      "fc layer 2 self.abs_max_out: 1531.0\n",
      "fc layer 2 self.abs_max_out: 1540.0\n",
      "lif layer 2 self.abs_max_v: 2478.5\n",
      "lif layer 2 self.abs_max_v: 2570.5\n",
      "lif layer 2 self.abs_max_v: 2616.5\n",
      "lif layer 2 self.abs_max_v: 2623.0\n",
      "lif layer 2 self.abs_max_v: 2633.0\n",
      "lif layer 2 self.abs_max_v: 2647.5\n",
      "fc layer 1 self.abs_max_out: 1990.0\n",
      "fc layer 1 self.abs_max_out: 2221.0\n",
      "lif layer 1 self.abs_max_v: 3790.5\n",
      "lif layer 1 self.abs_max_v: 3942.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  9.350946/ 55.132885, val:  28.33%, val_best:  28.33%, tr:  98.77%, tr_best:  98.77%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9735%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1741  17.783%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1579.0\n",
      "fc layer 2 self.abs_max_out: 1607.0\n",
      "fc layer 2 self.abs_max_out: 1692.0\n",
      "fc layer 2 self.abs_max_out: 1709.0\n",
      "fc layer 2 self.abs_max_out: 1711.0\n",
      "fc layer 2 self.abs_max_out: 1712.0\n",
      "fc layer 2 self.abs_max_out: 1763.0\n",
      "fc layer 2 self.abs_max_out: 1792.0\n",
      "fc layer 2 self.abs_max_out: 1795.0\n",
      "fc layer 2 self.abs_max_out: 1821.0\n",
      "lif layer 2 self.abs_max_v: 2704.5\n",
      "lif layer 2 self.abs_max_v: 2736.5\n",
      "lif layer 2 self.abs_max_v: 2822.5\n",
      "lif layer 2 self.abs_max_v: 2865.5\n",
      "lif layer 2 self.abs_max_v: 2887.0\n",
      "lif layer 2 self.abs_max_v: 2897.5\n",
      "fc layer 2 self.abs_max_out: 1833.0\n",
      "fc layer 3 self.abs_max_out: 325.0\n",
      "fc layer 2 self.abs_max_out: 1843.0\n",
      "fc layer 2 self.abs_max_out: 1880.0\n",
      "fc layer 2 self.abs_max_out: 1890.0\n",
      "fc layer 2 self.abs_max_out: 1901.0\n",
      "fc layer 2 self.abs_max_out: 1939.0\n",
      "fc layer 2 self.abs_max_out: 1946.0\n",
      "fc layer 2 self.abs_max_out: 1982.0\n",
      "fc layer 2 self.abs_max_out: 2005.0\n",
      "fc layer 2 self.abs_max_out: 2169.0\n",
      "fc layer 1 self.abs_max_out: 2469.0\n",
      "lif layer 1 self.abs_max_v: 4133.0\n",
      "fc layer 1 self.abs_max_out: 2512.0\n",
      "lif layer 1 self.abs_max_v: 4578.5\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  8.959177/ 61.401741, val:  33.33%, val_best:  33.33%, tr:  99.08%, tr_best:  99.08%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7823%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7320%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3272  16.711%\n",
      "fc layer 2 self.abs_max_out: 2207.0\n",
      "fc layer 2 self.abs_max_out: 2232.0\n",
      "fc layer 2 self.abs_max_out: 2264.0\n",
      "fc layer 2 self.abs_max_out: 2288.0\n",
      "fc layer 2 self.abs_max_out: 2366.0\n",
      "fc layer 2 self.abs_max_out: 2392.0\n",
      "fc layer 2 self.abs_max_out: 2420.0\n",
      "fc layer 2 self.abs_max_out: 2429.0\n",
      "fc layer 2 self.abs_max_out: 2431.0\n",
      "fc layer 2 self.abs_max_out: 2440.0\n",
      "fc layer 2 self.abs_max_out: 2456.0\n",
      "fc layer 2 self.abs_max_out: 2472.0\n",
      "fc layer 2 self.abs_max_out: 2600.0\n",
      "lif layer 2 self.abs_max_v: 3085.0\n",
      "lif layer 2 self.abs_max_v: 3185.5\n",
      "fc layer 2 self.abs_max_out: 2736.0\n",
      "lif layer 2 self.abs_max_v: 3282.0\n",
      "lif layer 2 self.abs_max_v: 3349.5\n",
      "lif layer 2 self.abs_max_v: 3467.5\n",
      "lif layer 2 self.abs_max_v: 3472.0\n",
      "lif layer 2 self.abs_max_v: 3548.0\n",
      "fc layer 3 self.abs_max_out: 341.0\n",
      "lif layer 2 self.abs_max_v: 3654.0\n",
      "fc layer 2 self.abs_max_out: 2746.0\n",
      "fc layer 2 self.abs_max_out: 2870.0\n",
      "fc layer 2 self.abs_max_out: 2891.0\n",
      "fc layer 2 self.abs_max_out: 2913.0\n",
      "fc layer 2 self.abs_max_out: 2937.0\n",
      "fc layer 2 self.abs_max_out: 2962.0\n",
      "fc layer 2 self.abs_max_out: 2965.0\n",
      "fc layer 2 self.abs_max_out: 3006.0\n",
      "fc layer 2 self.abs_max_out: 3041.0\n",
      "lif layer 2 self.abs_max_v: 3868.0\n",
      "lif layer 2 self.abs_max_v: 3922.5\n",
      "fc layer 2 self.abs_max_out: 3052.0\n",
      "fc layer 2 self.abs_max_out: 3054.0\n",
      "fc layer 1 self.abs_max_out: 2586.0\n",
      "lif layer 1 self.abs_max_v: 4686.5\n",
      "fc layer 1 self.abs_max_out: 2602.0\n",
      "lif layer 2 self.abs_max_v: 4005.0\n",
      "lif layer 2 self.abs_max_v: 4047.5\n",
      "fc layer 2 self.abs_max_out: 3061.0\n",
      "fc layer 2 self.abs_max_out: 3066.0\n",
      "fc layer 2 self.abs_max_out: 3071.0\n",
      "fc layer 2 self.abs_max_out: 3081.0\n",
      "lif layer 2 self.abs_max_v: 4178.0\n",
      "lif layer 2 self.abs_max_v: 4399.5\n",
      "fc layer 2 self.abs_max_out: 3114.0\n",
      "fc layer 2 self.abs_max_out: 3138.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.142137/ 50.272270, val:  34.17%, val_best:  34.17%, tr:  98.88%, tr_best:  99.08%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0204%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6444%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4748  16.166%\n",
      "fc layer 2 self.abs_max_out: 3148.0\n",
      "fc layer 2 self.abs_max_out: 3174.0\n",
      "fc layer 2 self.abs_max_out: 3185.0\n",
      "fc layer 2 self.abs_max_out: 3265.0\n",
      "fc layer 2 self.abs_max_out: 3287.0\n",
      "fc layer 2 self.abs_max_out: 3341.0\n",
      "fc layer 2 self.abs_max_out: 3422.0\n",
      "fc layer 2 self.abs_max_out: 3425.0\n",
      "fc layer 2 self.abs_max_out: 3434.0\n",
      "fc layer 2 self.abs_max_out: 3471.0\n",
      "fc layer 2 self.abs_max_out: 3499.0\n",
      "fc layer 2 self.abs_max_out: 3500.0\n",
      "fc layer 2 self.abs_max_out: 3518.0\n",
      "fc layer 2 self.abs_max_out: 3533.0\n",
      "fc layer 2 self.abs_max_out: 3575.0\n",
      "fc layer 2 self.abs_max_out: 3582.0\n",
      "lif layer 2 self.abs_max_v: 4454.5\n",
      "lif layer 2 self.abs_max_v: 4765.5\n",
      "fc layer 2 self.abs_max_out: 3593.0\n",
      "fc layer 2 self.abs_max_out: 3637.0\n",
      "lif layer 2 self.abs_max_v: 4913.5\n",
      "lif layer 2 self.abs_max_v: 5071.0\n",
      "fc layer 2 self.abs_max_out: 3668.0\n",
      "fc layer 2 self.abs_max_out: 3740.0\n",
      "lif layer 2 self.abs_max_v: 5184.0\n",
      "lif layer 2 self.abs_max_v: 5376.0\n",
      "lif layer 2 self.abs_max_v: 5472.0\n",
      "fc layer 3 self.abs_max_out: 343.0\n",
      "fc layer 3 self.abs_max_out: 372.0\n",
      "lif layer 2 self.abs_max_v: 5522.5\n",
      "lif layer 2 self.abs_max_v: 5794.5\n",
      "fc layer 1 self.abs_max_out: 2958.0\n",
      "fc layer 1 self.abs_max_out: 2970.0\n",
      "lif layer 1 self.abs_max_v: 5056.5\n",
      "lif layer 1 self.abs_max_v: 5348.5\n",
      "fc layer 1 self.abs_max_out: 3005.0\n",
      "lif layer 1 self.abs_max_v: 5381.5\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.277754/ 95.997528, val:  28.75%, val_best:  34.17%, tr:  99.39%, tr_best:  99.39%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5053%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6176  15.771%\n",
      "lif layer 2 self.abs_max_v: 5824.5\n",
      "lif layer 2 self.abs_max_v: 6002.5\n",
      "fc layer 2 self.abs_max_out: 3825.0\n",
      "lif layer 2 self.abs_max_v: 6008.5\n",
      "lif layer 2 self.abs_max_v: 6073.5\n",
      "lif layer 2 self.abs_max_v: 6247.5\n",
      "lif layer 2 self.abs_max_v: 6268.0\n",
      "lif layer 2 self.abs_max_v: 6321.0\n",
      "lif layer 2 self.abs_max_v: 6772.5\n",
      "lif layer 2 self.abs_max_v: 6998.5\n",
      "lif layer 2 self.abs_max_v: 7111.5\n",
      "lif layer 2 self.abs_max_v: 7168.0\n",
      "lif layer 2 self.abs_max_v: 7194.0\n",
      "fc layer 1 self.abs_max_out: 3026.0\n",
      "fc layer 1 self.abs_max_out: 3041.0\n",
      "lif layer 1 self.abs_max_v: 5384.5\n",
      "fc layer 1 self.abs_max_out: 3295.0\n",
      "lif layer 1 self.abs_max_v: 5794.0\n",
      "fc layer 1 self.abs_max_out: 3392.0\n",
      "lif layer 1 self.abs_max_v: 6289.0\n",
      "fc layer 2 self.abs_max_out: 3871.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  9.337129/ 55.333900, val:  38.75%, val_best:  38.75%, tr:  99.28%, tr_best:  99.39%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3545%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7269%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7608  15.542%\n",
      "lif layer 2 self.abs_max_v: 7273.5\n",
      "fc layer 2 self.abs_max_out: 3887.0\n",
      "fc layer 2 self.abs_max_out: 3962.0\n",
      "fc layer 2 self.abs_max_out: 3977.0\n",
      "fc layer 2 self.abs_max_out: 4123.0\n",
      "lif layer 2 self.abs_max_v: 7623.5\n",
      "fc layer 1 self.abs_max_out: 3473.0\n",
      "fc layer 2 self.abs_max_out: 4149.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  9.233257/ 52.342854, val:  38.75%, val_best:  38.75%, tr:  98.88%, tr_best:  99.39%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4438%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4471%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9038  15.386%\n",
      "lif layer 2 self.abs_max_v: 7843.0\n",
      "fc layer 2 self.abs_max_out: 4168.0\n",
      "lif layer 2 self.abs_max_v: 8005.0\n",
      "fc layer 2 self.abs_max_out: 4216.0\n",
      "fc layer 2 self.abs_max_out: 4220.0\n",
      "fc layer 2 self.abs_max_out: 4240.0\n",
      "fc layer 2 self.abs_max_out: 4459.0\n",
      "fc layer 1 self.abs_max_out: 3514.0\n",
      "fc layer 1 self.abs_max_out: 3635.0\n",
      "lif layer 1 self.abs_max_v: 6413.5\n",
      "fc layer 2 self.abs_max_out: 4876.0\n",
      "lif layer 2 self.abs_max_v: 8016.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  8.759548/ 81.023361, val:  35.00%, val_best:  38.75%, tr:  98.88%, tr_best:  99.39%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7811%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10465  15.271%\n",
      "lif layer 2 self.abs_max_v: 8258.5\n",
      "lif layer 2 self.abs_max_v: 8386.5\n",
      "fc layer 2 self.abs_max_out: 4910.0\n",
      "fc layer 2 self.abs_max_out: 4939.0\n",
      "lif layer 2 self.abs_max_v: 8407.0\n",
      "lif layer 2 self.abs_max_v: 8663.5\n",
      "fc layer 2 self.abs_max_out: 4975.0\n",
      "fc layer 3 self.abs_max_out: 389.0\n",
      "fc layer 1 self.abs_max_out: 3779.0\n",
      "fc layer 1 self.abs_max_out: 3847.0\n",
      "lif layer 1 self.abs_max_v: 6484.5\n",
      "lif layer 1 self.abs_max_v: 6774.5\n",
      "lif layer 2 self.abs_max_v: 8771.0\n",
      "fc layer 2 self.abs_max_out: 5161.0\n",
      "fc layer 2 self.abs_max_out: 5327.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.733128/ 38.789780, val:  44.58%, val_best:  44.58%, tr:  98.98%, tr_best:  99.39%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.5867%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0320%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11837  15.114%\n",
      "lif layer 2 self.abs_max_v: 8861.5\n",
      "lif layer 2 self.abs_max_v: 9083.5\n",
      "lif layer 2 self.abs_max_v: 9269.0\n",
      "lif layer 2 self.abs_max_v: 9644.5\n",
      "fc layer 2 self.abs_max_out: 5542.0\n",
      "fc layer 1 self.abs_max_out: 3920.0\n",
      "fc layer 1 self.abs_max_out: 3963.0\n",
      "lif layer 1 self.abs_max_v: 7092.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.088848/ 36.117588, val:  40.00%, val_best:  44.58%, tr:  97.55%, tr_best:  99.39%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.5785%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.8247%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13198  14.979%\n",
      "fc layer 2 self.abs_max_out: 6666.0\n",
      "lif layer 2 self.abs_max_v: 10781.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  7.874518/ 53.616734, val:  32.08%, val_best:  44.58%, tr:  97.14%, tr_best:  99.39%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.5281%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.8678%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14528  14.840%\n",
      "lif layer 2 self.abs_max_v: 11229.0\n",
      "fc layer 1 self.abs_max_out: 4044.0\n",
      "lif layer 1 self.abs_max_v: 7181.5\n",
      "lif layer 2 self.abs_max_v: 11391.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  7.969586/ 51.430756, val:  36.67%, val_best:  44.58%, tr:  98.26%, tr_best:  99.39%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0688%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3281%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15877  14.743%\n",
      "lif layer 2 self.abs_max_v: 11765.0\n",
      "lif layer 2 self.abs_max_v: 12064.5\n",
      "fc layer 1 self.abs_max_out: 4123.0\n",
      "fc layer 1 self.abs_max_out: 4128.0\n",
      "lif layer 1 self.abs_max_v: 7420.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.424517/ 39.836586, val:  38.75%, val_best:  44.58%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9549%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3774%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 17205  14.645%\n",
      "fc layer 1 self.abs_max_out: 4134.0\n",
      "fc layer 1 self.abs_max_out: 4285.0\n",
      "lif layer 1 self.abs_max_v: 7638.5\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.200382/ 32.053738, val:  42.92%, val_best:  44.58%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.0765%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.7461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 18520  14.552%\n",
      "fc layer 2 self.abs_max_out: 6924.0\n",
      "fc layer 1 self.abs_max_out: 4297.0\n",
      "lif layer 1 self.abs_max_v: 7755.5\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.110027/ 58.303223, val:  36.67%, val_best:  44.58%, tr:  98.26%, tr_best:  99.39%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3612%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.1491%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19800  14.446%\n",
      "fc layer 2 self.abs_max_out: 6962.0\n",
      "fc layer 1 self.abs_max_out: 4317.0\n",
      "fc layer 1 self.abs_max_out: 4406.0\n",
      "lif layer 1 self.abs_max_v: 7985.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  6.811917/ 40.105789, val:  37.50%, val_best:  44.58%, tr:  98.26%, tr_best:  99.39%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4104%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 21049  14.334%\n",
      "fc layer 1 self.abs_max_out: 4462.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  6.754753/ 38.974209, val:  40.42%, val_best:  44.58%, tr:  97.24%, tr_best:  99.39%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1067%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.9048%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 22335  14.259%\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  6.364268/ 43.479229, val:  42.08%, val_best:  44.58%, tr:  98.98%, tr_best:  99.39%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4055%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.2084%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 23570  14.162%\n",
      "lif layer 2 self.abs_max_v: 12383.5\n",
      "lif layer 1 self.abs_max_v: 8141.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  6.403064/ 31.286367, val:  54.17%, val_best:  54.17%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6792%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.8506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 24859  14.107%\n",
      "lif layer 2 self.abs_max_v: 12534.0\n",
      "fc layer 1 self.abs_max_out: 4498.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  6.608840/ 37.564098, val:  42.50%, val_best:  54.17%, tr:  97.55%, tr_best:  99.39%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3826%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2057%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 26165  14.066%\n",
      "fc layer 1 self.abs_max_out: 4536.0\n",
      "lif layer 1 self.abs_max_v: 8251.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  5.915592/ 39.502525, val:  36.25%, val_best:  54.17%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.2634%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.8266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 27377  13.982%\n",
      "lif layer 1 self.abs_max_v: 8282.5\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  5.906467/ 40.836937, val:  40.00%, val_best:  54.17%, tr:  97.96%, tr_best:  99.39%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.8398%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.5459%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 28599  13.911%\n",
      "fc layer 2 self.abs_max_out: 7450.0\n",
      "fc layer 1 self.abs_max_out: 4608.0\n",
      "lif layer 1 self.abs_max_v: 8429.5\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.157726/ 37.391331, val:  42.08%, val_best:  54.17%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1342%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.9134%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 29891  13.878%\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.126168/ 31.251080, val:  47.92%, val_best:  54.17%, tr:  97.85%, tr_best:  99.39%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4085%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.6130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 31154  13.836%\n",
      "fc layer 1 self.abs_max_out: 4694.0\n",
      "fc layer 1 self.abs_max_out: 4700.0\n",
      "lif layer 1 self.abs_max_v: 8559.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.138742/ 30.610958, val:  46.25%, val_best:  54.17%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.4907%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 32380  13.781%\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.314307/ 38.938179, val:  49.17%, val_best:  54.17%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2693%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3080%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 33643  13.746%\n",
      "lif layer 2 self.abs_max_v: 12566.5\n",
      "fc layer 1 self.abs_max_out: 5156.0\n",
      "lif layer 1 self.abs_max_v: 8759.5\n",
      "lif layer 1 self.abs_max_v: 9440.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.417950/ 31.496296, val:  45.83%, val_best:  54.17%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.6013%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 34965  13.737%\n",
      "lif layer 2 self.abs_max_v: 12706.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  5.846397/ 34.780594, val:  52.08%, val_best:  54.17%, tr:  98.37%, tr_best:  99.39%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.0221%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 36180  13.687%\n",
      "fc layer 1 self.abs_max_out: 5177.0\n",
      "fc layer 1 self.abs_max_out: 5183.0\n",
      "lif layer 1 self.abs_max_v: 9530.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.259682/ 33.687248, val:  44.58%, val_best:  54.17%, tr:  98.57%, tr_best:  99.39%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.2869%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.2147%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 37520  13.687%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  6.169603/ 38.732437, val:  46.67%, val_best:  54.17%, tr:  98.37%, tr_best:  99.39%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.5396%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.1952%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 38803  13.667%\n",
      "fc layer 1 self.abs_max_out: 5349.0\n",
      "lif layer 1 self.abs_max_v: 9779.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.054595/ 49.262905, val:  37.92%, val_best:  54.17%, tr:  99.18%, tr_best:  99.39%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1353%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.0286%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 40075  13.645%\n",
      "fc layer 2 self.abs_max_out: 7942.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  6.004594/ 35.691765, val:  50.00%, val_best:  54.17%, tr:  98.26%, tr_best:  99.39%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.5808%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.6663%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 41375  13.633%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  5.984384/ 43.531982, val:  46.25%, val_best:  54.17%, tr:  98.57%, tr_best:  99.39%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.1625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 42681  13.624%\n",
      "lif layer 2 self.abs_max_v: 12804.5\n",
      "fc layer 1 self.abs_max_out: 5416.0\n",
      "fc layer 1 self.abs_max_out: 5483.0\n",
      "lif layer 1 self.abs_max_v: 10103.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.463643/ 53.994003, val:  37.50%, val_best:  54.17%, tr:  98.67%, tr_best:  99.39%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3515%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.4963%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 43889  13.585%\n",
      "lif layer 2 self.abs_max_v: 13685.5\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.691150/ 29.587641, val:  52.50%, val_best:  54.17%, tr:  98.98%, tr_best:  99.39%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3808%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.5196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 45155  13.566%\n",
      "fc layer 1 self.abs_max_out: 5517.0\n",
      "lif layer 1 self.abs_max_v: 10108.5\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.766069/ 38.936420, val:  48.33%, val_best:  54.17%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1130%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.5842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 46449  13.556%\n",
      "fc layer 1 self.abs_max_out: 5575.0\n",
      "fc layer 1 self.abs_max_out: 5635.0\n",
      "fc layer 1 self.abs_max_out: 5646.0\n",
      "lif layer 1 self.abs_max_v: 10399.5\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.462870/ 34.388302, val:  39.58%, val_best:  54.17%, tr:  98.16%, tr_best:  99.39%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.2885%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.8049%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 47681  13.529%\n",
      "fc layer 1 self.abs_max_out: 5746.0\n",
      "lif layer 2 self.abs_max_v: 13947.5\n",
      "lif layer 1 self.abs_max_v: 10546.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.420146/ 32.503540, val:  52.08%, val_best:  54.17%, tr:  98.77%, tr_best:  99.39%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4201%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.8517%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 48919  13.505%\n",
      "fc layer 1 self.abs_max_out: 5837.0\n",
      "fc layer 1 self.abs_max_out: 5914.0\n",
      "fc layer 1 self.abs_max_out: 5946.0\n",
      "lif layer 1 self.abs_max_v: 10940.0\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.216262/ 35.721642, val:  46.25%, val_best:  54.17%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.0731%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.2171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 50100  13.467%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.430684/ 32.767380, val:  52.92%, val_best:  54.17%, tr:  99.28%, tr_best:  99.39%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.9976%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.3268%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 51345  13.448%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.474543/ 35.404823, val:  45.83%, val_best:  54.17%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.0979%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.7055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 52620  13.437%\n",
      "fc layer 1 self.abs_max_out: 6118.0\n",
      "lif layer 1 self.abs_max_v: 11091.5\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.228195/ 34.807003, val:  43.33%, val_best:  54.17%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.0136%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.4500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 53882  13.424%\n",
      "fc layer 1 self.abs_max_out: 6139.0\n",
      "fc layer 1 self.abs_max_out: 6331.0\n",
      "lif layer 1 self.abs_max_v: 11559.5\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.103626/ 29.514805, val:  40.00%, val_best:  54.17%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.4877%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 55121  13.406%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  4.752763/ 32.621006, val:  42.08%, val_best:  54.17%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.7601%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.6030%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 56280  13.369%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  4.917472/ 22.083258, val:  63.75%, val_best:  63.75%, tr:  98.37%, tr_best:  99.39%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.7155%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.8429%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 57529  13.355%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.759390/ 35.071175, val:  48.75%, val_best:  63.75%, tr:  98.47%, tr_best:  99.39%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.5395%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.3021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 58754  13.337%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.959343/ 31.081858, val:  47.92%, val_best:  63.75%, tr:  99.08%, tr_best:  99.39%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.6449%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.1383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 60006  13.325%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.730499/ 35.121044, val:  44.17%, val_best:  63.75%, tr:  98.57%, tr_best:  99.39%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1639%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.1853%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 61266  13.315%\n",
      "fc layer 1 self.abs_max_out: 6414.0\n",
      "fc layer 1 self.abs_max_out: 6421.0\n",
      "lif layer 1 self.abs_max_v: 11779.5\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.589324/ 32.760254, val:  40.83%, val_best:  63.75%, tr:  98.88%, tr_best:  99.39%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.7893%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.1468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 62432  13.286%\n",
      "fc layer 1 self.abs_max_out: 6481.0\n",
      "lif layer 1 self.abs_max_v: 11855.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.630233/ 29.979650, val:  52.50%, val_best:  63.75%, tr:  97.75%, tr_best:  99.39%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4485%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.3567%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 63602  13.258%\n",
      "fc layer 1 self.abs_max_out: 6503.0\n",
      "fc layer 1 self.abs_max_out: 6657.0\n",
      "lif layer 1 self.abs_max_v: 12044.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.663617/ 29.332911, val:  55.83%, val_best:  63.75%, tr:  97.85%, tr_best:  99.39%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.8224%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6515%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 64806  13.239%\n",
      "fc layer 1 self.abs_max_out: 6696.0\n",
      "lif layer 1 self.abs_max_v: 12246.0\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.390628/ 28.391870, val:  59.17%, val_best:  63.75%, tr:  97.65%, tr_best:  99.39%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.7655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6716%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 65968  13.212%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.549483/ 20.426807, val:  55.00%, val_best:  63.75%, tr:  98.06%, tr_best:  99.39%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4430%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5279%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 67143  13.189%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.645811/ 20.050293, val:  58.75%, val_best:  63.75%, tr:  96.94%, tr_best:  99.39%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3254%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6781%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 68346  13.172%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.717308/ 31.084234, val:  47.50%, val_best:  63.75%, tr:  97.96%, tr_best:  99.39%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.8002%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7072%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 69573  13.160%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.672236/ 24.739985, val:  47.92%, val_best:  63.75%, tr:  97.65%, tr_best:  99.39%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0606%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.3022%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 70770  13.143%\n",
      "fc layer 1 self.abs_max_out: 6807.0\n",
      "lif layer 1 self.abs_max_v: 12416.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.909734/ 27.279079, val:  50.00%, val_best:  63.75%, tr:  97.24%, tr_best:  99.39%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4982%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.2395%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 71987  13.131%\n",
      "fc layer 1 self.abs_max_out: 6873.0\n",
      "lif layer 1 self.abs_max_v: 12545.0\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.657746/ 32.315205, val:  40.00%, val_best:  63.75%, tr:  98.98%, tr_best:  99.39%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1544%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6003%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 73194  13.116%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.507164/ 30.828951, val:  51.67%, val_best:  63.75%, tr:  98.06%, tr_best:  99.39%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1186%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9789%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 74363  13.096%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.608974/ 20.796947, val:  54.58%, val_best:  63.75%, tr:  97.34%, tr_best:  99.39%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2738%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8586%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 75538  13.078%\n",
      "fc layer 1 self.abs_max_out: 6949.0\n",
      "fc layer 1 self.abs_max_out: 7172.0\n",
      "lif layer 1 self.abs_max_v: 13025.5\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.518189/ 33.053516, val:  47.50%, val_best:  63.75%, tr:  97.55%, tr_best:  99.39%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8199%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 76704  13.058%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.518524/ 28.371675, val:  43.75%, val_best:  63.75%, tr:  97.75%, tr_best:  99.39%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.6172%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3151%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 77876  13.040%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.631327/ 24.524033, val:  47.92%, val_best:  63.75%, tr:  97.45%, tr_best:  99.39%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3453%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.1932%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 79073  13.027%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.538594/ 31.345238, val:  42.08%, val_best:  63.75%, tr:  97.45%, tr_best:  99.39%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3335%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5719%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 80246  13.011%\n",
      "lif layer 1 self.abs_max_v: 13045.5\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.146228/ 29.185053, val:  45.83%, val_best:  63.75%, tr:  98.16%, tr_best:  99.39%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6853%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6606%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 81379  12.988%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.105985/ 32.552525, val:  44.58%, val_best:  63.75%, tr:  98.37%, tr_best:  99.39%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7604%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 82484  12.962%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  4.268528/ 28.641293, val:  39.58%, val_best:  63.75%, tr:  98.06%, tr_best:  99.39%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7587%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9984%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 83617  12.941%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.359446/ 28.041395, val:  42.08%, val_best:  63.75%, tr:  98.26%, tr_best:  99.39%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0954%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8797%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 84758  12.922%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  4.440722/ 23.600420, val:  42.92%, val_best:  63.75%, tr:  98.47%, tr_best:  99.39%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2431%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5733%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 85931  12.908%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.456620/ 19.691185, val:  50.83%, val_best:  63.75%, tr:  97.85%, tr_best:  99.39%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1005%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4984%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 87079  12.891%\n",
      "fc layer 1 self.abs_max_out: 7186.0\n",
      "fc layer 1 self.abs_max_out: 7300.0\n",
      "lif layer 1 self.abs_max_v: 13318.0\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  4.122508/ 24.156330, val:  41.67%, val_best:  63.75%, tr:  98.26%, tr_best:  99.39%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.5358%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1766%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 88192  12.869%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  4.211493/ 25.324467, val:  52.50%, val_best:  63.75%, tr:  97.14%, tr_best:  99.39%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2188%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2382%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 89320  12.850%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  4.273782/ 24.583380, val:  52.92%, val_best:  63.75%, tr:  97.55%, tr_best:  99.39%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1325%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 90461  12.834%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  4.150482/ 22.517250, val:  58.33%, val_best:  63.75%, tr:  97.34%, tr_best:  99.39%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9341%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9934%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 91581  12.814%\n",
      "fc layer 1 self.abs_max_out: 7391.0\n",
      "lif layer 1 self.abs_max_v: 13408.0\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  4.287800/ 35.170052, val:  42.50%, val_best:  63.75%, tr:  98.16%, tr_best:  99.39%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0953%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 92717  12.798%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  4.179695/ 22.709745, val:  55.00%, val_best:  63.75%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0107%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1123%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 93866  12.784%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  4.201344/ 29.416786, val:  51.25%, val_best:  63.75%, tr:  97.55%, tr_best:  99.39%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4864%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0869%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 95012  12.770%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  4.371642/ 26.835619, val:  50.42%, val_best:  63.75%, tr:  97.14%, tr_best:  99.39%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2293%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7457%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 96172  12.758%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  4.325801/ 34.577286, val:  45.42%, val_best:  63.75%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9583%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0168%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 97340  12.747%\n",
      "fc layer 1 self.abs_max_out: 7431.0\n",
      "fc layer 1 self.abs_max_out: 7632.0\n",
      "lif layer 1 self.abs_max_v: 13879.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  4.048514/ 30.515894, val:  45.00%, val_best:  63.75%, tr:  98.26%, tr_best:  99.39%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1557%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5253%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 98456  12.730%\n",
      "fc layer 1 self.abs_max_out: 7732.0\n",
      "lif layer 1 self.abs_max_v: 14081.5\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.845850/ 18.772829, val:  62.08%, val_best:  63.75%, tr:  97.04%, tr_best:  99.39%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8588%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6663%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 99521  12.707%\n",
      "lif layer 1 self.abs_max_v: 14093.5\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.853499/ 17.892859, val:  62.08%, val_best:  63.75%, tr:  97.75%, tr_best:  99.39%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2563%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 100596  12.686%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.996884/ 35.724098, val:  31.67%, val_best:  63.75%, tr:  97.65%, tr_best:  99.39%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8441%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 101698  12.668%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  4.223367/ 21.849113, val:  46.67%, val_best:  63.75%, tr:  97.85%, tr_best:  99.39%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5655%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 102860  12.659%\n",
      "fc layer 1 self.abs_max_out: 7736.0\n",
      "fc layer 1 self.abs_max_out: 7856.0\n",
      "lif layer 1 self.abs_max_v: 14352.0\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  4.032420/ 24.620136, val:  51.25%, val_best:  63.75%, tr:  97.85%, tr_best:  99.39%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0758%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 103959  12.642%\n",
      "fc layer 1 self.abs_max_out: 7865.0\n",
      "lif layer 1 self.abs_max_v: 14378.5\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  4.196438/ 28.576952, val:  47.50%, val_best:  63.75%, tr:  97.04%, tr_best:  99.39%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7418%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8218%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 105096  12.629%\n",
      "fc layer 1 self.abs_max_out: 7868.0\n",
      "lif layer 1 self.abs_max_v: 14400.5\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  4.252061/ 21.526466, val:  49.17%, val_best:  63.75%, tr:  97.45%, tr_best:  99.39%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.5316%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7616%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 106263  12.621%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.861301/ 30.713253, val:  49.58%, val_best:  63.75%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0328%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6635%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 107345  12.603%\n",
      "fc layer 1 self.abs_max_out: 8065.0\n",
      "lif layer 1 self.abs_max_v: 14707.0\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  4.200078/ 26.785774, val:  53.75%, val_best:  63.75%, tr:  97.34%, tr_best:  99.39%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3465%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 108453  12.589%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.827733/ 25.051836, val:  47.08%, val_best:  63.75%, tr:  97.04%, tr_best:  99.39%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8401%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7887%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 109484  12.565%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  4.334877/ 17.325031, val:  56.25%, val_best:  63.75%, tr:  96.63%, tr_best:  99.39%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6962%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8358%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 110649  12.558%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.969564/ 30.274008, val:  48.33%, val_best:  63.75%, tr:  97.85%, tr_best:  99.39%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1480%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7232%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 111726  12.541%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  4.032736/ 19.749058, val:  56.25%, val_best:  63.75%, tr:  97.65%, tr_best:  99.39%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0849%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4235%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 112804  12.524%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.987291/ 19.938004, val:  57.50%, val_best:  63.75%, tr:  97.75%, tr_best:  99.39%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9903%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6431%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 113898  12.510%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.995414/ 23.221943, val:  55.83%, val_best:  63.75%, tr:  98.06%, tr_best:  99.39%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9493%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7315%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 114965  12.493%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  4.124570/ 24.591394, val:  45.00%, val_best:  63.75%, tr:  96.83%, tr_best:  99.39%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8819%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 116054  12.478%\n",
      "fc layer 1 self.abs_max_out: 8231.0\n",
      "lif layer 1 self.abs_max_v: 15032.5\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  4.087366/ 19.000093, val:  57.92%, val_best:  63.75%, tr:  97.34%, tr_best:  99.39%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1462%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6107%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 117182  12.468%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  3.868565/ 31.771328, val:  41.25%, val_best:  63.75%, tr:  97.14%, tr_best:  99.39%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2454%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9849%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 118225  12.450%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  3.918756/ 21.106186, val:  48.75%, val_best:  63.75%, tr:  97.55%, tr_best:  99.39%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8742%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5635%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 119300  12.435%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  4.018529/ 25.180824, val:  33.33%, val_best:  63.75%, tr:  97.24%, tr_best:  99.39%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6442%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0079%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 120407  12.423%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  3.674034/ 24.390285, val:  47.50%, val_best:  63.75%, tr:  97.45%, tr_best:  99.39%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9856%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0961%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 121409  12.401%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  3.861068/ 17.928802, val:  60.83%, val_best:  63.75%, tr:  97.24%, tr_best:  99.39%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8808%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 122457  12.385%\n",
      "fc layer 1 self.abs_max_out: 8306.0\n",
      "lif layer 1 self.abs_max_v: 15161.0\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  3.886586/ 19.970055, val:  57.08%, val_best:  63.75%, tr:  96.83%, tr_best:  99.39%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0349%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2161%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 123535  12.371%\n",
      "fc layer 1 self.abs_max_out: 8359.0\n",
      "fc layer 1 self.abs_max_out: 8630.0\n",
      "lif layer 1 self.abs_max_v: 15714.0\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  3.801849/ 25.810915, val:  55.00%, val_best:  63.75%, tr:  97.14%, tr_best:  99.39%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3856%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 124599  12.356%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  3.857658/ 24.870327, val:  50.00%, val_best:  63.75%, tr:  97.34%, tr_best:  99.39%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2837%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 125635  12.339%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  4.092687/ 20.841486, val:  59.17%, val_best:  63.75%, tr:  97.55%, tr_best:  99.39%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8757%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8846%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 126717  12.327%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  4.003377/ 23.975246, val:  50.42%, val_best:  63.75%, tr:  97.45%, tr_best:  99.39%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6576%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0584%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 127789  12.314%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  3.625873/ 28.912352, val:  37.50%, val_best:  63.75%, tr:  97.55%, tr_best:  99.39%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1134%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1774%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 128800  12.296%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  3.922515/ 26.497295, val:  45.42%, val_best:  63.75%, tr:  97.24%, tr_best:  99.39%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8905%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7717%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 129850  12.281%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  3.975628/ 18.304932, val:  57.92%, val_best:  63.75%, tr:  97.14%, tr_best:  99.39%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0779%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6534%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 130897  12.266%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  3.816452/ 18.201683, val:  58.33%, val_best:  63.75%, tr:  97.75%, tr_best:  99.39%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8951%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8812%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 131966  12.254%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  3.834966/ 21.367826, val:  55.83%, val_best:  63.75%, tr:  97.45%, tr_best:  99.39%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2195%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 133032  12.242%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  3.687942/ 25.321577, val:  51.67%, val_best:  63.75%, tr:  96.53%, tr_best:  99.39%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8271%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8533%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 134021  12.223%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  3.904912/ 18.093128, val:  58.75%, val_best:  63.75%, tr:  97.85%, tr_best:  99.39%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9198%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9937%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 135068  12.209%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  3.765436/ 27.286978, val:  47.08%, val_best:  63.75%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6982%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1545%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 136093  12.194%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  3.890491/ 28.332617, val:  37.92%, val_best:  63.75%, tr:  96.83%, tr_best:  99.39%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1736%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 137156  12.182%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  4.007866/ 21.695677, val:  48.75%, val_best:  63.75%, tr:  97.45%, tr_best:  99.39%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3470%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3074%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 138247  12.173%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  3.661550/ 27.902378, val:  55.83%, val_best:  63.75%, tr:  97.65%, tr_best:  99.39%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7176%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5367%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 139247  12.157%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  3.454229/ 24.279543, val:  52.92%, val_best:  63.75%, tr:  97.24%, tr_best:  99.39%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6435%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3399%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 140225  12.138%\n",
      "fc layer 1 self.abs_max_out: 8783.0\n",
      "fc layer 1 self.abs_max_out: 8800.0\n",
      "lif layer 1 self.abs_max_v: 16177.5\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  3.613810/ 19.779099, val:  50.83%, val_best:  63.75%, tr:  96.73%, tr_best:  99.39%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 141204  12.120%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  3.567601/ 25.725805, val:  52.50%, val_best:  63.75%, tr:  98.06%, tr_best:  99.39%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6343%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3982%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 142191  12.103%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  3.551019/ 22.107370, val:  51.25%, val_best:  63.75%, tr:  96.94%, tr_best:  99.39%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9935%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4559%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 143130  12.083%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  3.657869/ 25.278099, val:  46.25%, val_best:  63.75%, tr:  96.83%, tr_best:  99.39%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8377%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4462%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 144152  12.069%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  3.674728/ 18.094194, val:  64.17%, val_best:  64.17%, tr:  98.16%, tr_best:  99.39%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6175%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5352%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 145175  12.056%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  3.510519/ 27.666346, val:  42.08%, val_best:  64.17%, tr:  97.65%, tr_best:  99.39%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8036%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3041%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 146153  12.039%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  3.764131/ 14.258694, val:  74.17%, val_best:  74.17%, tr:  97.34%, tr_best:  99.39%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6490%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4534%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 147167  12.026%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  3.638287/ 25.986229, val:  52.08%, val_best:  74.17%, tr:  97.96%, tr_best:  99.39%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6136%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2778%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 148185  12.013%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  3.572446/ 32.848640, val:  46.25%, val_best:  74.17%, tr:  97.34%, tr_best:  99.39%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4437%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2089%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 149156  11.996%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  3.684541/ 26.474199, val:  46.25%, val_best:  74.17%, tr:  97.34%, tr_best:  99.39%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7265%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4325%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 150158  11.983%\n",
      "fc layer 1 self.abs_max_out: 8931.0\n",
      "lif layer 1 self.abs_max_v: 16298.5\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  3.548938/ 23.377514, val:  57.50%, val_best:  74.17%, tr:  98.06%, tr_best:  99.39%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4932%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 151145  11.968%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  3.567944/ 17.290022, val:  56.25%, val_best:  74.17%, tr:  98.26%, tr_best:  99.39%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4854%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 152121  11.953%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  3.737943/ 27.705048, val:  50.83%, val_best:  74.17%, tr:  96.83%, tr_best:  99.39%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2258%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 153124  11.940%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  3.506436/ 24.084980, val:  51.67%, val_best:  74.17%, tr:  97.96%, tr_best:  99.39%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0487%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3084%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 154078  11.923%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  3.555068/ 33.500481, val:  44.58%, val_best:  74.17%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7044%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 155014  11.905%\n",
      "lif layer 1 self.abs_max_v: 16309.0\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  3.765212/ 21.279037, val:  63.75%, val_best:  74.17%, tr:  97.45%, tr_best:  99.39%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6011%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4539%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 156040  11.895%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  3.451070/ 27.086910, val:  45.42%, val_best:  74.17%, tr:  97.45%, tr_best:  99.39%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9178%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6219%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 157006  11.880%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  3.717644/ 22.608501, val:  53.75%, val_best:  74.17%, tr:  97.34%, tr_best:  99.39%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2892%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5972%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 157991  11.866%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  3.453569/ 18.276978, val:  66.67%, val_best:  74.17%, tr:  98.26%, tr_best:  99.39%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4176%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 158892  11.847%\n",
      "fc layer 1 self.abs_max_out: 8940.0\n",
      "fc layer 1 self.abs_max_out: 9028.0\n",
      "lif layer 1 self.abs_max_v: 16542.5\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  3.555592/ 18.261286, val:  53.75%, val_best:  74.17%, tr:  96.94%, tr_best:  99.39%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5821%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 159852  11.832%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  3.473581/ 18.893614, val:  55.00%, val_best:  74.17%, tr:  97.24%, tr_best:  99.39%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7039%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3924%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 160802  11.817%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  3.534000/ 26.556183, val:  39.17%, val_best:  74.17%, tr:  97.14%, tr_best:  99.39%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4396%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4495%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 161795  11.805%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  3.580620/ 20.730858, val:  51.25%, val_best:  74.17%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3641%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 162764  11.791%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  3.471191/ 15.047166, val:  50.83%, val_best:  74.17%, tr:  96.42%, tr_best:  99.39%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7233%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 163727  11.777%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  3.434634/ 26.057384, val:  50.00%, val_best:  74.17%, tr:  97.55%, tr_best:  99.39%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6532%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6926%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 164695  11.764%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  3.452969/ 20.443302, val:  59.17%, val_best:  74.17%, tr:  97.34%, tr_best:  99.39%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6115%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5316%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 165609  11.747%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  3.432635/ 22.972017, val:  49.17%, val_best:  74.17%, tr:  96.94%, tr_best:  99.39%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5048%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5628%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 166550  11.733%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  3.329987/ 25.825939, val:  51.67%, val_best:  74.17%, tr:  96.83%, tr_best:  99.39%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2861%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3513%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 167487  11.718%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  3.443220/ 17.044918, val:  55.83%, val_best:  74.17%, tr:  97.75%, tr_best:  99.39%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5025%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 168418  11.703%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  3.282310/ 28.183134, val:  34.17%, val_best:  74.17%, tr:  96.94%, tr_best:  99.39%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6934%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 169327  11.686%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  3.470603/ 23.623472, val:  58.75%, val_best:  74.17%, tr:  96.12%, tr_best:  99.39%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5767%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5205%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 170274  11.673%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  3.412935/ 21.851933, val:  47.50%, val_best:  74.17%, tr:  96.73%, tr_best:  99.39%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3517%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 171212  11.659%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  3.348369/ 19.399889, val:  61.25%, val_best:  74.17%, tr:  97.04%, tr_best:  99.39%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4001%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6235%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 172139  11.644%\n",
      "fc layer 1 self.abs_max_out: 9101.0\n",
      "lif layer 1 self.abs_max_v: 16686.0\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  3.399005/ 20.659069, val:  65.00%, val_best:  74.17%, tr:  97.55%, tr_best:  99.39%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3019%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 173081  11.631%\n",
      "fc layer 1 self.abs_max_out: 9102.0\n",
      "fc layer 1 self.abs_max_out: 9235.0\n",
      "lif layer 1 self.abs_max_v: 16917.0\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  3.469614/ 26.229216, val:  54.17%, val_best:  74.17%, tr:  97.24%, tr_best:  99.39%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4235%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2391%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 174035  11.619%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  3.487791/ 18.877512, val:  67.08%, val_best:  74.17%, tr:  97.24%, tr_best:  99.39%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5796%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4093%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 174987  11.607%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  3.244550/ 16.122995, val:  61.67%, val_best:  74.17%, tr:  96.94%, tr_best:  99.39%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6498%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5490%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 175900  11.592%\n",
      "fc layer 1 self.abs_max_out: 9294.0\n",
      "lif layer 1 self.abs_max_v: 16990.0\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  3.519466/ 24.388672, val:  50.00%, val_best:  74.17%, tr:  96.94%, tr_best:  99.39%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6383%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2894%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 176826  11.578%\n",
      "fc layer 1 self.abs_max_out: 9328.0\n",
      "lif layer 1 self.abs_max_v: 17063.0\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  3.260174/ 18.501642, val:  64.58%, val_best:  74.17%, tr:  97.14%, tr_best:  99.39%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5515%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6756%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 177747  11.564%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  3.309915/ 21.047016, val:  60.00%, val_best:  74.17%, tr:  97.14%, tr_best:  99.39%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 178659  11.550%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  3.313978/ 21.023573, val:  49.58%, val_best:  74.17%, tr:  97.65%, tr_best:  99.39%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8376%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 179563  11.536%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  3.397254/ 24.340683, val:  48.33%, val_best:  74.17%, tr:  96.94%, tr_best:  99.39%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7693%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6449%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 180508  11.524%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  3.239062/ 18.807962, val:  60.83%, val_best:  74.17%, tr:  97.85%, tr_best:  99.39%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5187%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 181404  11.509%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  3.336422/ 16.067694, val:  72.50%, val_best:  74.17%, tr:  97.55%, tr_best:  99.39%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8521%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 182336  11.497%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  3.194894/ 26.707277, val:  47.08%, val_best:  74.17%, tr:  97.14%, tr_best:  99.39%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7595%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7459%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 183225  11.482%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  3.249980/ 18.560236, val:  51.25%, val_best:  74.17%, tr:  97.55%, tr_best:  99.39%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8089%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6185%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 184113  11.467%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  3.126549/ 21.985558, val:  58.75%, val_best:  74.17%, tr:  97.45%, tr_best:  99.39%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4536%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8472%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 184983  11.452%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  3.398390/ 16.628319, val:  70.42%, val_best:  74.17%, tr:  97.75%, tr_best:  99.39%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3882%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7741%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 185924  11.440%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  3.191888/ 23.267794, val:  48.33%, val_best:  74.17%, tr:  97.34%, tr_best:  99.39%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7781%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8849%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 186795  11.425%\n",
      "fc layer 1 self.abs_max_out: 9350.0\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  3.310807/ 30.882010, val:  50.00%, val_best:  74.17%, tr:  98.37%, tr_best:  99.39%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6809%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6880%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 187712  11.413%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  3.252504/ 24.335279, val:  47.50%, val_best:  74.17%, tr:  97.24%, tr_best:  99.39%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3897%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 188565  11.397%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  3.207333/ 17.211336, val:  57.08%, val_best:  74.17%, tr:  96.42%, tr_best:  99.39%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4819%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 189437  11.382%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  3.270708/ 17.610025, val:  60.42%, val_best:  74.17%, tr:  97.45%, tr_best:  99.39%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4357%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5450%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 190322  11.369%\n",
      "lif layer 1 self.abs_max_v: 17124.5\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  3.337665/ 33.996357, val:  43.33%, val_best:  74.17%, tr:  97.85%, tr_best:  99.39%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8244%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3365%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 191226  11.356%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  3.569423/ 19.259258, val:  65.83%, val_best:  74.17%, tr:  97.04%, tr_best:  99.39%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6103%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5069%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 192187  11.347%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  3.065045/ 23.004936, val:  59.58%, val_best:  74.17%, tr:  97.85%, tr_best:  99.39%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5365%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6757%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 193013  11.331%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  3.082302/ 18.873819, val:  63.33%, val_best:  74.17%, tr:  97.24%, tr_best:  99.39%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4550%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 193875  11.316%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  3.252961/ 18.372681, val:  60.00%, val_best:  74.17%, tr:  98.06%, tr_best:  99.39%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4483%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6024%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 194752  11.303%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  3.371645/ 25.166758, val:  52.92%, val_best:  74.17%, tr:  97.24%, tr_best:  99.39%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5721%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 195678  11.292%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  3.194392/ 21.533079, val:  57.08%, val_best:  74.17%, tr:  97.45%, tr_best:  99.39%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6156%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 196555  11.279%\n",
      "lif layer 1 self.abs_max_v: 17162.0\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  3.313376/ 33.570980, val:  52.08%, val_best:  74.17%, tr:  97.65%, tr_best:  99.39%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6437%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3952%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 197438  11.267%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  3.708023/ 19.627563, val:  48.33%, val_best:  74.17%, tr:  97.14%, tr_best:  99.39%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7495%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5880%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 198395  11.258%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  3.201272/ 16.983276, val:  66.67%, val_best:  74.17%, tr:  97.04%, tr_best:  99.39%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6048%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8897%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 199271  11.246%\n",
      "fc layer 1 self.abs_max_out: 9477.0\n",
      "lif layer 1 self.abs_max_v: 17310.5\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  2.995867/ 19.859478, val:  54.17%, val_best:  74.17%, tr:  98.16%, tr_best:  99.39%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6229%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9527%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 200097  11.230%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  3.164461/ 19.527151, val:  61.25%, val_best:  74.17%, tr:  98.26%, tr_best:  99.39%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7564%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 200976  11.218%\n",
      "lif layer 1 self.abs_max_v: 17338.5\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  3.207419/ 28.685846, val:  50.00%, val_best:  74.17%, tr:  97.45%, tr_best:  99.39%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6126%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 201844  11.205%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  3.105391/ 30.658381, val:  62.50%, val_best:  74.17%, tr:  97.45%, tr_best:  99.39%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5788%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7936%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 202685  11.191%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  3.121144/ 18.504532, val:  63.75%, val_best:  74.17%, tr:  97.96%, tr_best:  99.39%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3346%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5392%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 203535  11.177%\n",
      "fc layer 2 self.abs_max_out: 7960.0\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  3.382912/ 30.028160, val:  52.50%, val_best:  74.17%, tr:  97.04%, tr_best:  99.39%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3882%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 204452  11.168%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  3.297074/ 20.427931, val:  57.08%, val_best:  74.17%, tr:  96.63%, tr_best:  99.39%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6483%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 205321  11.156%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  3.147138/ 23.164921, val:  60.00%, val_best:  74.17%, tr:  97.45%, tr_best:  99.39%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6224%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 206150  11.141%\n",
      "fc layer 1 self.abs_max_out: 9554.0\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  3.194208/ 16.585018, val:  62.92%, val_best:  74.17%, tr:  97.24%, tr_best:  99.39%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4066%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7159%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 207040  11.131%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  3.102518/ 15.760096, val:  65.00%, val_best:  74.17%, tr:  97.45%, tr_best:  99.39%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2914%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 207887  11.118%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  3.205210/ 29.038990, val:  45.00%, val_best:  74.17%, tr:  97.65%, tr_best:  99.39%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0684%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7599%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 208767  11.107%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  3.185815/ 21.421700, val:  64.17%, val_best:  74.17%, tr:  97.55%, tr_best:  99.39%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3978%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 209619  11.094%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  3.234600/ 20.144682, val:  57.92%, val_best:  74.17%, tr:  97.45%, tr_best:  99.39%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3281%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5409%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 210492  11.083%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  2.879017/ 18.582014, val:  60.83%, val_best:  74.17%, tr:  97.45%, tr_best:  99.39%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3368%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7254%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 211301  11.068%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  3.092777/ 24.041010, val:  44.17%, val_best:  74.17%, tr:  97.55%, tr_best:  99.39%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4245%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7045%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 212152  11.056%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  3.099221/ 20.817722, val:  61.67%, val_best:  74.17%, tr:  97.75%, tr_best:  99.39%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1904%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6030%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 213002  11.044%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  3.266814/ 19.196884, val:  57.50%, val_best:  74.17%, tr:  97.45%, tr_best:  99.39%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6767%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6284%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 213872  11.033%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  3.136428/ 27.830183, val:  53.33%, val_best:  74.17%, tr:  97.75%, tr_best:  99.39%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7162%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6945%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 214714  11.021%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  3.015352/ 24.132126, val:  58.33%, val_best:  74.17%, tr:  97.24%, tr_best:  99.39%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7450%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6027%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7782beb21c414070b0039983af8805f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÅ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÅ‚ñà‚ñá‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñà‚ñá‚ñá‚ñÉ‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñà‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñÉ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÅ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÅ‚ñà‚ñá‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.97242</td></tr><tr><td>tr_epoch_loss</td><td>3.01535</td></tr><tr><td>val_acc_best</td><td>0.74167</td></tr><tr><td>val_acc_now</td><td>0.58333</td></tr><tr><td>val_loss</td><td>24.13213</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glad-sweep-9</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bcipyx30' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bcipyx30</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_055813-bcipyx30/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fvga2htt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_101536-fvga2htt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fvga2htt' target=\"_blank\">zesty-sweep-14</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fvga2htt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fvga2htt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251214_101544_426', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 16, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 1, 'lif_layer_v_threshold2': 64, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 16\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 1, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=16, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 41.0\n",
      "lif layer 2 self.abs_max_v: 41.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 51.0\n",
      "fc layer 2 self.abs_max_out: 80.0\n",
      "lif layer 2 self.abs_max_v: 94.5\n",
      "fc layer 3 self.abs_max_out: 5.0\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "lif layer 2 self.abs_max_v: 107.5\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "fc layer 1 self.abs_max_out: 72.0\n",
      "lif layer 1 self.abs_max_v: 109.0\n",
      "lif layer 2 self.abs_max_v: 117.0\n",
      "fc layer 3 self.abs_max_out: 9.0\n",
      "fc layer 1 self.abs_max_out: 102.0\n",
      "lif layer 1 self.abs_max_v: 150.5\n",
      "fc layer 2 self.abs_max_out: 103.0\n",
      "lif layer 2 self.abs_max_v: 160.5\n",
      "fc layer 3 self.abs_max_out: 11.0\n",
      "fc layer 1 self.abs_max_out: 153.0\n",
      "lif layer 1 self.abs_max_v: 193.5\n",
      "fc layer 2 self.abs_max_out: 138.0\n",
      "lif layer 2 self.abs_max_v: 218.5\n",
      "fc layer 3 self.abs_max_out: 12.0\n",
      "fc layer 1 self.abs_max_out: 161.0\n",
      "lif layer 1 self.abs_max_v: 258.0\n",
      "lif layer 2 self.abs_max_v: 231.5\n",
      "fc layer 3 self.abs_max_out: 17.0\n",
      "fc layer 2 self.abs_max_out: 142.0\n",
      "fc layer 3 self.abs_max_out: 24.0\n",
      "fc layer 1 self.abs_max_out: 188.0\n",
      "fc layer 2 self.abs_max_out: 144.0\n",
      "fc layer 2 self.abs_max_out: 153.0\n",
      "fc layer 3 self.abs_max_out: 26.0\n",
      "fc layer 2 self.abs_max_out: 162.0\n",
      "fc layer 3 self.abs_max_out: 29.0\n",
      "fc layer 1 self.abs_max_out: 215.0\n",
      "lif layer 1 self.abs_max_v: 273.0\n",
      "fc layer 2 self.abs_max_out: 167.0\n",
      "fc layer 1 self.abs_max_out: 217.0\n",
      "lif layer 1 self.abs_max_v: 284.5\n",
      "fc layer 2 self.abs_max_out: 170.0\n",
      "fc layer 1 self.abs_max_out: 233.0\n",
      "lif layer 1 self.abs_max_v: 290.0\n",
      "fc layer 2 self.abs_max_out: 172.0\n",
      "fc layer 1 self.abs_max_out: 293.0\n",
      "lif layer 1 self.abs_max_v: 293.0\n",
      "fc layer 1 self.abs_max_out: 369.0\n",
      "lif layer 1 self.abs_max_v: 369.0\n",
      "fc layer 2 self.abs_max_out: 178.0\n",
      "fc layer 2 self.abs_max_out: 180.0\n",
      "lif layer 2 self.abs_max_v: 241.0\n",
      "lif layer 2 self.abs_max_v: 247.0\n",
      "lif layer 2 self.abs_max_v: 251.5\n",
      "fc layer 3 self.abs_max_out: 32.0\n",
      "lif layer 2 self.abs_max_v: 262.0\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "lif layer 2 self.abs_max_v: 278.0\n",
      "lif layer 2 self.abs_max_v: 291.0\n",
      "lif layer 2 self.abs_max_v: 299.5\n",
      "lif layer 2 self.abs_max_v: 309.0\n",
      "fc layer 2 self.abs_max_out: 189.0\n",
      "fc layer 2 self.abs_max_out: 196.0\n",
      "lif layer 2 self.abs_max_v: 342.5\n",
      "fc layer 2 self.abs_max_out: 204.0\n",
      "lif layer 1 self.abs_max_v: 381.0\n",
      "lif layer 1 self.abs_max_v: 427.5\n",
      "lif layer 1 self.abs_max_v: 447.0\n",
      "fc layer 3 self.abs_max_out: 44.0\n",
      "fc layer 2 self.abs_max_out: 215.0\n",
      "fc layer 2 self.abs_max_out: 226.0\n",
      "fc layer 3 self.abs_max_out: 45.0\n",
      "fc layer 2 self.abs_max_out: 253.0\n",
      "fc layer 2 self.abs_max_out: 257.0\n",
      "fc layer 3 self.abs_max_out: 52.0\n",
      "fc layer 3 self.abs_max_out: 57.0\n",
      "fc layer 3 self.abs_max_out: 60.0\n",
      "lif layer 2 self.abs_max_v: 345.0\n",
      "fc layer 3 self.abs_max_out: 72.0\n",
      "fc layer 3 self.abs_max_out: 82.0\n",
      "fc layer 1 self.abs_max_out: 381.0\n",
      "lif layer 2 self.abs_max_v: 350.0\n",
      "fc layer 2 self.abs_max_out: 262.0\n",
      "lif layer 2 self.abs_max_v: 430.0\n",
      "lif layer 1 self.abs_max_v: 463.0\n",
      "lif layer 2 self.abs_max_v: 436.0\n",
      "fc layer 2 self.abs_max_out: 282.0\n",
      "lif layer 1 self.abs_max_v: 549.5\n",
      "fc layer 2 self.abs_max_out: 292.0\n",
      "lif layer 2 self.abs_max_v: 474.0\n",
      "lif layer 1 self.abs_max_v: 599.0\n",
      "lif layer 2 self.abs_max_v: 514.5\n",
      "lif layer 1 self.abs_max_v: 649.0\n",
      "lif layer 2 self.abs_max_v: 520.5\n",
      "fc layer 1 self.abs_max_out: 492.0\n",
      "fc layer 1 self.abs_max_out: 558.0\n",
      "lif layer 1 self.abs_max_v: 707.5\n",
      "fc layer 2 self.abs_max_out: 314.0\n",
      "lif layer 2 self.abs_max_v: 525.5\n",
      "fc layer 1 self.abs_max_out: 630.0\n",
      "lif layer 2 self.abs_max_v: 532.0\n",
      "lif layer 1 self.abs_max_v: 737.5\n",
      "fc layer 2 self.abs_max_out: 318.0\n",
      "lif layer 2 self.abs_max_v: 537.0\n",
      "lif layer 1 self.abs_max_v: 778.5\n",
      "fc layer 3 self.abs_max_out: 87.0\n",
      "fc layer 2 self.abs_max_out: 323.0\n",
      "fc layer 2 self.abs_max_out: 325.0\n",
      "fc layer 2 self.abs_max_out: 370.0\n",
      "fc layer 3 self.abs_max_out: 89.0\n",
      "fc layer 3 self.abs_max_out: 103.0\n",
      "fc layer 3 self.abs_max_out: 106.0\n",
      "lif layer 2 self.abs_max_v: 544.5\n",
      "lif layer 2 self.abs_max_v: 545.5\n",
      "lif layer 2 self.abs_max_v: 558.0\n",
      "lif layer 2 self.abs_max_v: 571.0\n",
      "lif layer 2 self.abs_max_v: 574.5\n",
      "lif layer 2 self.abs_max_v: 576.5\n",
      "lif layer 1 self.abs_max_v: 846.0\n",
      "lif layer 2 self.abs_max_v: 588.5\n",
      "lif layer 1 self.abs_max_v: 896.5\n",
      "lif layer 2 self.abs_max_v: 598.5\n",
      "lif layer 1 self.abs_max_v: 925.5\n",
      "lif layer 2 self.abs_max_v: 602.0\n",
      "fc layer 2 self.abs_max_out: 394.0\n",
      "fc layer 2 self.abs_max_out: 450.0\n",
      "lif layer 2 self.abs_max_v: 638.0\n",
      "lif layer 2 self.abs_max_v: 686.0\n",
      "lif layer 2 self.abs_max_v: 702.0\n",
      "lif layer 2 self.abs_max_v: 704.0\n",
      "fc layer 2 self.abs_max_out: 460.0\n",
      "fc layer 3 self.abs_max_out: 108.0\n",
      "fc layer 3 self.abs_max_out: 112.0\n",
      "fc layer 3 self.abs_max_out: 113.0\n",
      "fc layer 3 self.abs_max_out: 114.0\n",
      "fc layer 3 self.abs_max_out: 115.0\n",
      "fc layer 3 self.abs_max_out: 119.0\n",
      "fc layer 3 self.abs_max_out: 142.0\n",
      "lif layer 2 self.abs_max_v: 732.0\n",
      "lif layer 2 self.abs_max_v: 738.5\n",
      "lif layer 2 self.abs_max_v: 743.0\n",
      "lif layer 2 self.abs_max_v: 745.5\n",
      "lif layer 2 self.abs_max_v: 776.0\n",
      "lif layer 2 self.abs_max_v: 790.0\n",
      "fc layer 3 self.abs_max_out: 146.0\n",
      "lif layer 2 self.abs_max_v: 817.5\n",
      "lif layer 2 self.abs_max_v: 848.0\n",
      "fc layer 3 self.abs_max_out: 148.0\n",
      "fc layer 3 self.abs_max_out: 149.0\n",
      "fc layer 3 self.abs_max_out: 165.0\n",
      "lif layer 2 self.abs_max_v: 853.0\n",
      "fc layer 2 self.abs_max_out: 473.0\n",
      "lif layer 2 self.abs_max_v: 859.5\n",
      "fc layer 3 self.abs_max_out: 174.0\n",
      "lif layer 1 self.abs_max_v: 1019.0\n",
      "lif layer 1 self.abs_max_v: 1058.0\n",
      "fc layer 3 self.abs_max_out: 175.0\n",
      "fc layer 3 self.abs_max_out: 176.0\n",
      "fc layer 3 self.abs_max_out: 181.0\n",
      "fc layer 2 self.abs_max_out: 495.0\n",
      "fc layer 3 self.abs_max_out: 188.0\n",
      "fc layer 3 self.abs_max_out: 191.0\n",
      "lif layer 2 self.abs_max_v: 890.0\n",
      "lif layer 2 self.abs_max_v: 902.0\n",
      "lif layer 2 self.abs_max_v: 923.0\n",
      "lif layer 2 self.abs_max_v: 928.0\n",
      "fc layer 3 self.abs_max_out: 204.0\n",
      "fc layer 1 self.abs_max_out: 636.0\n",
      "fc layer 3 self.abs_max_out: 207.0\n",
      "fc layer 2 self.abs_max_out: 547.0\n",
      "fc layer 3 self.abs_max_out: 219.0\n",
      "fc layer 3 self.abs_max_out: 246.0\n",
      "fc layer 1 self.abs_max_out: 817.0\n",
      "lif layer 1 self.abs_max_v: 1228.0\n",
      "fc layer 1 self.abs_max_out: 917.0\n",
      "lif layer 1 self.abs_max_v: 1378.0\n",
      "fc layer 2 self.abs_max_out: 553.0\n",
      "fc layer 3 self.abs_max_out: 273.0\n",
      "fc layer 2 self.abs_max_out: 580.0\n",
      "lif layer 2 self.abs_max_v: 939.5\n",
      "fc layer 2 self.abs_max_out: 584.0\n",
      "fc layer 2 self.abs_max_out: 585.0\n",
      "fc layer 2 self.abs_max_out: 607.0\n",
      "lif layer 2 self.abs_max_v: 1067.0\n",
      "lif layer 2 self.abs_max_v: 1127.5\n",
      "lif layer 2 self.abs_max_v: 1169.0\n",
      "lif layer 2 self.abs_max_v: 1191.5\n",
      "fc layer 3 self.abs_max_out: 284.0\n",
      "lif layer 1 self.abs_max_v: 1380.0\n",
      "lif layer 1 self.abs_max_v: 1471.5\n",
      "lif layer 1 self.abs_max_v: 1571.0\n",
      "fc layer 2 self.abs_max_out: 623.0\n",
      "lif layer 2 self.abs_max_v: 1193.0\n",
      "fc layer 2 self.abs_max_out: 644.0\n",
      "lif layer 2 self.abs_max_v: 1213.5\n",
      "lif layer 2 self.abs_max_v: 1251.0\n",
      "fc layer 3 self.abs_max_out: 300.0\n",
      "fc layer 2 self.abs_max_out: 648.0\n",
      "fc layer 2 self.abs_max_out: 654.0\n",
      "fc layer 2 self.abs_max_out: 673.0\n",
      "fc layer 2 self.abs_max_out: 680.0\n",
      "fc layer 2 self.abs_max_out: 685.0\n",
      "fc layer 2 self.abs_max_out: 693.0\n",
      "fc layer 2 self.abs_max_out: 721.0\n",
      "fc layer 2 self.abs_max_out: 727.0\n",
      "fc layer 2 self.abs_max_out: 746.0\n",
      "fc layer 2 self.abs_max_out: 785.0\n",
      "fc layer 3 self.abs_max_out: 311.0\n",
      "fc layer 2 self.abs_max_out: 825.0\n",
      "fc layer 2 self.abs_max_out: 848.0\n",
      "fc layer 2 self.abs_max_out: 854.0\n",
      "fc layer 2 self.abs_max_out: 883.0\n",
      "fc layer 2 self.abs_max_out: 912.0\n",
      "fc layer 1 self.abs_max_out: 950.0\n",
      "lif layer 1 self.abs_max_v: 1638.0\n",
      "fc layer 1 self.abs_max_out: 1021.0\n",
      "lif layer 1 self.abs_max_v: 1826.0\n",
      "lif layer 2 self.abs_max_v: 1297.5\n",
      "lif layer 2 self.abs_max_v: 1360.0\n",
      "lif layer 2 self.abs_max_v: 1409.0\n",
      "lif layer 2 self.abs_max_v: 1431.5\n",
      "fc layer 2 self.abs_max_out: 922.0\n",
      "fc layer 2 self.abs_max_out: 968.0\n",
      "fc layer 2 self.abs_max_out: 969.0\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "fc layer 2 self.abs_max_out: 972.0\n",
      "lif layer 2 self.abs_max_v: 1432.0\n",
      "fc layer 2 self.abs_max_out: 976.0\n",
      "fc layer 3 self.abs_max_out: 317.0\n",
      "fc layer 2 self.abs_max_out: 1024.0\n",
      "fc layer 2 self.abs_max_out: 1034.0\n",
      "fc layer 2 self.abs_max_out: 1045.0\n",
      "lif layer 2 self.abs_max_v: 1468.5\n",
      "lif layer 2 self.abs_max_v: 1482.5\n",
      "lif layer 2 self.abs_max_v: 1524.5\n",
      "fc layer 2 self.abs_max_out: 1047.0\n",
      "fc layer 2 self.abs_max_out: 1079.0\n",
      "fc layer 1 self.abs_max_out: 1137.0\n",
      "lif layer 1 self.abs_max_v: 1835.0\n",
      "fc layer 2 self.abs_max_out: 1103.0\n",
      "fc layer 2 self.abs_max_out: 1104.0\n",
      "fc layer 2 self.abs_max_out: 1108.0\n",
      "fc layer 2 self.abs_max_out: 1132.0\n",
      "fc layer 2 self.abs_max_out: 1201.0\n",
      "fc layer 2 self.abs_max_out: 1225.0\n",
      "lif layer 1 self.abs_max_v: 1886.5\n",
      "lif layer 1 self.abs_max_v: 1926.0\n",
      "lif layer 2 self.abs_max_v: 1570.0\n",
      "lif layer 2 self.abs_max_v: 1576.5\n",
      "lif layer 2 self.abs_max_v: 1620.0\n",
      "fc layer 1 self.abs_max_out: 1183.0\n",
      "lif layer 2 self.abs_max_v: 1637.5\n",
      "lif layer 1 self.abs_max_v: 2065.5\n",
      "fc layer 1 self.abs_max_out: 1300.0\n",
      "lif layer 1 self.abs_max_v: 2135.5\n",
      "lif layer 1 self.abs_max_v: 2233.0\n",
      "fc layer 1 self.abs_max_out: 1388.0\n",
      "lif layer 1 self.abs_max_v: 2474.0\n",
      "fc layer 1 self.abs_max_out: 1398.0\n",
      "lif layer 1 self.abs_max_v: 2635.0\n",
      "fc layer 1 self.abs_max_out: 1454.0\n",
      "lif layer 1 self.abs_max_v: 2771.5\n",
      "lif layer 1 self.abs_max_v: 2817.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 10.158495/ 75.453819, val:  30.42%, val_best:  30.42%, tr:  98.88%, tr_best:  98.88%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2110%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9596%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1573  16.067%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 1690.0\n",
      "fc layer 2 self.abs_max_out: 1257.0\n",
      "fc layer 2 self.abs_max_out: 1279.0\n",
      "fc layer 2 self.abs_max_out: 1289.0\n",
      "fc layer 2 self.abs_max_out: 1296.0\n",
      "fc layer 2 self.abs_max_out: 1301.0\n",
      "fc layer 3 self.abs_max_out: 340.0\n",
      "fc layer 2 self.abs_max_out: 1368.0\n",
      "fc layer 2 self.abs_max_out: 1381.0\n",
      "fc layer 2 self.abs_max_out: 1398.0\n",
      "fc layer 2 self.abs_max_out: 1410.0\n",
      "fc layer 2 self.abs_max_out: 1436.0\n",
      "fc layer 2 self.abs_max_out: 1452.0\n",
      "fc layer 3 self.abs_max_out: 355.0\n",
      "fc layer 2 self.abs_max_out: 1455.0\n",
      "fc layer 2 self.abs_max_out: 1489.0\n",
      "fc layer 3 self.abs_max_out: 358.0\n",
      "lif layer 2 self.abs_max_v: 1815.5\n",
      "lif layer 2 self.abs_max_v: 1838.5\n",
      "lif layer 2 self.abs_max_v: 1919.5\n",
      "fc layer 2 self.abs_max_out: 1496.0\n",
      "fc layer 2 self.abs_max_out: 1550.0\n",
      "fc layer 2 self.abs_max_out: 1589.0\n",
      "fc layer 2 self.abs_max_out: 1591.0\n",
      "lif layer 2 self.abs_max_v: 1931.5\n",
      "lif layer 2 self.abs_max_v: 1960.5\n",
      "lif layer 2 self.abs_max_v: 1978.5\n",
      "lif layer 2 self.abs_max_v: 2091.5\n",
      "lif layer 2 self.abs_max_v: 2254.0\n",
      "lif layer 2 self.abs_max_v: 2286.0\n",
      "fc layer 2 self.abs_max_out: 1637.0\n",
      "fc layer 2 self.abs_max_out: 1647.0\n",
      "fc layer 2 self.abs_max_out: 1653.0\n",
      "fc layer 2 self.abs_max_out: 1664.0\n",
      "fc layer 2 self.abs_max_out: 1671.0\n",
      "fc layer 2 self.abs_max_out: 1689.0\n",
      "fc layer 2 self.abs_max_out: 1711.0\n",
      "fc layer 2 self.abs_max_out: 1734.0\n",
      "fc layer 2 self.abs_max_out: 1748.0\n",
      "fc layer 2 self.abs_max_out: 1768.0\n",
      "fc layer 2 self.abs_max_out: 1775.0\n",
      "fc layer 2 self.abs_max_out: 1780.0\n",
      "lif layer 2 self.abs_max_v: 2360.5\n",
      "lif layer 2 self.abs_max_v: 2445.0\n",
      "lif layer 2 self.abs_max_v: 2476.0\n",
      "lif layer 2 self.abs_max_v: 2529.0\n",
      "fc layer 2 self.abs_max_out: 1784.0\n",
      "fc layer 2 self.abs_max_out: 1808.0\n",
      "fc layer 2 self.abs_max_out: 1825.0\n",
      "fc layer 2 self.abs_max_out: 1860.0\n",
      "fc layer 2 self.abs_max_out: 1865.0\n",
      "fc layer 1 self.abs_max_out: 1521.0\n",
      "fc layer 1 self.abs_max_out: 1692.0\n",
      "lif layer 1 self.abs_max_v: 2897.5\n",
      "lif layer 1 self.abs_max_v: 2970.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  9.359103/ 55.966457, val:  37.50%, val_best:  37.50%, tr:  98.06%, tr_best:  98.88%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2186%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0574%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 2946  15.046%\n",
      "fc layer 2 self.abs_max_out: 1882.0\n",
      "fc layer 2 self.abs_max_out: 1889.0\n",
      "fc layer 2 self.abs_max_out: 1895.0\n",
      "fc layer 2 self.abs_max_out: 1923.0\n",
      "fc layer 2 self.abs_max_out: 1945.0\n",
      "fc layer 2 self.abs_max_out: 1950.0\n",
      "fc layer 2 self.abs_max_out: 1976.0\n",
      "fc layer 2 self.abs_max_out: 1984.0\n",
      "fc layer 2 self.abs_max_out: 2002.0\n",
      "fc layer 2 self.abs_max_out: 2021.0\n",
      "fc layer 2 self.abs_max_out: 2048.0\n",
      "lif layer 2 self.abs_max_v: 2572.5\n",
      "fc layer 2 self.abs_max_out: 2078.0\n",
      "fc layer 2 self.abs_max_out: 2104.0\n",
      "fc layer 2 self.abs_max_out: 2115.0\n",
      "fc layer 2 self.abs_max_out: 2118.0\n",
      "fc layer 2 self.abs_max_out: 2182.0\n",
      "fc layer 2 self.abs_max_out: 2198.0\n",
      "lif layer 2 self.abs_max_v: 2662.5\n",
      "lif layer 1 self.abs_max_v: 3045.0\n",
      "fc layer 2 self.abs_max_out: 2204.0\n",
      "fc layer 2 self.abs_max_out: 2215.0\n",
      "fc layer 2 self.abs_max_out: 2286.0\n",
      "fc layer 2 self.abs_max_out: 2338.0\n",
      "fc layer 2 self.abs_max_out: 2374.0\n",
      "lif layer 2 self.abs_max_v: 2786.0\n",
      "lif layer 2 self.abs_max_v: 2882.0\n",
      "fc layer 2 self.abs_max_out: 2405.0\n",
      "fc layer 2 self.abs_max_out: 2429.0\n",
      "fc layer 2 self.abs_max_out: 2474.0\n",
      "lif layer 2 self.abs_max_v: 2896.5\n",
      "fc layer 3 self.abs_max_out: 423.0\n",
      "fc layer 1 self.abs_max_out: 1710.0\n",
      "fc layer 1 self.abs_max_out: 1732.0\n",
      "lif layer 1 self.abs_max_v: 3164.5\n",
      "lif layer 1 self.abs_max_v: 3273.0\n",
      "fc layer 1 self.abs_max_out: 1879.0\n",
      "lif layer 1 self.abs_max_v: 3327.5\n",
      "lif layer 1 self.abs_max_v: 3342.0\n",
      "lif layer 2 self.abs_max_v: 2933.5\n",
      "lif layer 2 self.abs_max_v: 2939.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.439178/ 43.352276, val:  45.42%, val_best:  45.42%, tr:  98.57%, tr_best:  98.88%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4384  14.927%\n",
      "lif layer 2 self.abs_max_v: 3001.0\n",
      "lif layer 2 self.abs_max_v: 3154.0\n",
      "lif layer 2 self.abs_max_v: 3204.5\n",
      "lif layer 2 self.abs_max_v: 3289.5\n",
      "lif layer 2 self.abs_max_v: 3382.0\n",
      "lif layer 2 self.abs_max_v: 3449.0\n",
      "lif layer 2 self.abs_max_v: 3585.5\n",
      "lif layer 2 self.abs_max_v: 3627.0\n",
      "lif layer 2 self.abs_max_v: 3732.0\n",
      "lif layer 2 self.abs_max_v: 3734.0\n",
      "fc layer 1 self.abs_max_out: 2121.0\n",
      "fc layer 1 self.abs_max_out: 2210.0\n",
      "lif layer 1 self.abs_max_v: 3695.0\n",
      "lif layer 1 self.abs_max_v: 3973.5\n",
      "fc layer 1 self.abs_max_out: 2233.0\n",
      "fc layer 1 self.abs_max_out: 2321.0\n",
      "lif layer 1 self.abs_max_v: 4241.5\n",
      "lif layer 2 self.abs_max_v: 3794.5\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  8.320742/ 49.076828, val:  29.17%, val_best:  45.42%, tr:  98.67%, tr_best:  98.88%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8057%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9961%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 5735  14.645%\n",
      "lif layer 2 self.abs_max_v: 3833.5\n",
      "lif layer 2 self.abs_max_v: 3929.0\n",
      "lif layer 2 self.abs_max_v: 3984.0\n",
      "lif layer 2 self.abs_max_v: 4014.0\n",
      "fc layer 2 self.abs_max_out: 2499.0\n",
      "fc layer 2 self.abs_max_out: 2507.0\n",
      "fc layer 2 self.abs_max_out: 2511.0\n",
      "lif layer 1 self.abs_max_v: 4301.0\n",
      "lif layer 1 self.abs_max_v: 4372.5\n",
      "fc layer 1 self.abs_max_out: 2377.0\n",
      "lif layer 1 self.abs_max_v: 4464.5\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  8.218965/ 54.719955, val:  27.50%, val_best:  45.42%, tr:  99.08%, tr_best:  99.08%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7891%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7091  14.486%\n",
      "lif layer 2 self.abs_max_v: 4052.0\n",
      "lif layer 2 self.abs_max_v: 4156.0\n",
      "fc layer 1 self.abs_max_out: 2400.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  7.690357/ 35.868919, val:  36.67%, val_best:  45.42%, tr:  98.37%, tr_best:  99.08%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8769%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 8434  14.358%\n",
      "lif layer 2 self.abs_max_v: 4181.0\n",
      "lif layer 2 self.abs_max_v: 4416.0\n",
      "lif layer 2 self.abs_max_v: 4479.5\n",
      "lif layer 2 self.abs_max_v: 4578.5\n",
      "lif layer 2 self.abs_max_v: 4699.0\n",
      "fc layer 2 self.abs_max_out: 2544.0\n",
      "lif layer 2 self.abs_max_v: 4709.5\n",
      "fc layer 2 self.abs_max_out: 2562.0\n",
      "fc layer 1 self.abs_max_out: 2517.0\n",
      "fc layer 1 self.abs_max_out: 2587.0\n",
      "lif layer 1 self.abs_max_v: 4600.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  6.611122/ 46.400055, val:  42.08%, val_best:  45.42%, tr:  98.16%, tr_best:  99.08%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6741%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.7686%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 9728  14.195%\n",
      "fc layer 2 self.abs_max_out: 2660.0\n",
      "lif layer 2 self.abs_max_v: 4934.5\n",
      "lif layer 2 self.abs_max_v: 4974.5\n",
      "fc layer 2 self.abs_max_out: 2819.0\n",
      "lif layer 2 self.abs_max_v: 4978.5\n",
      "fc layer 1 self.abs_max_out: 2598.0\n",
      "lif layer 1 self.abs_max_v: 4689.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  6.041484/ 33.798206, val:  45.42%, val_best:  45.42%, tr:  98.37%, tr_best:  99.08%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0800%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.3573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11013  14.062%\n",
      "lif layer 2 self.abs_max_v: 4996.0\n",
      "lif layer 2 self.abs_max_v: 5037.0\n",
      "fc layer 2 self.abs_max_out: 2935.0\n",
      "fc layer 2 self.abs_max_out: 3003.0\n",
      "lif layer 2 self.abs_max_v: 5144.5\n",
      "lif layer 2 self.abs_max_v: 5146.5\n",
      "fc layer 1 self.abs_max_out: 2757.0\n",
      "lif layer 1 self.abs_max_v: 4905.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  6.199142/ 35.980366, val:  40.83%, val_best:  45.42%, tr:  98.16%, tr_best:  99.08%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4851%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.2260%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 12376  14.046%\n",
      "lif layer 2 self.abs_max_v: 5148.5\n",
      "fc layer 1 self.abs_max_out: 2803.0\n",
      "fc layer 2 self.abs_max_out: 3005.0\n",
      "lif layer 2 self.abs_max_v: 5149.0\n",
      "lif layer 2 self.abs_max_v: 5301.5\n",
      "lif layer 2 self.abs_max_v: 5393.5\n",
      "lif layer 2 self.abs_max_v: 5476.0\n",
      "lif layer 2 self.abs_max_v: 5581.0\n",
      "fc layer 1 self.abs_max_out: 2879.0\n",
      "fc layer 1 self.abs_max_out: 2966.0\n",
      "lif layer 1 self.abs_max_v: 4921.5\n",
      "lif layer 1 self.abs_max_v: 5091.5\n",
      "fc layer 2 self.abs_max_out: 3184.0\n",
      "fc layer 2 self.abs_max_out: 3185.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  5.529150/ 28.374456, val:  42.50%, val_best:  45.42%, tr:  97.55%, tr_best:  99.08%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4698%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.7235%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 13613  13.905%\n",
      "lif layer 2 self.abs_max_v: 5695.0\n",
      "lif layer 2 self.abs_max_v: 5801.5\n",
      "fc layer 2 self.abs_max_out: 3628.0\n",
      "lif layer 2 self.abs_max_v: 5926.5\n",
      "lif layer 2 self.abs_max_v: 6150.0\n",
      "lif layer 2 self.abs_max_v: 6535.0\n",
      "fc layer 1 self.abs_max_out: 3001.0\n",
      "lif layer 1 self.abs_max_v: 5120.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  5.262057/ 48.910770, val:  35.42%, val_best:  45.42%, tr:  98.26%, tr_best:  99.08%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.0242%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.2683%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 14814  13.756%\n",
      "fc layer 1 self.abs_max_out: 3030.0\n",
      "fc layer 1 self.abs_max_out: 3123.0\n",
      "lif layer 1 self.abs_max_v: 5195.5\n",
      "lif layer 1 self.abs_max_v: 5347.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  5.375267/ 24.571159, val:  52.92%, val_best:  52.92%, tr:  98.67%, tr_best:  99.08%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1689%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.6107%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16039  13.653%\n",
      "fc layer 2 self.abs_max_out: 3749.0\n",
      "fc layer 1 self.abs_max_out: 3130.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  5.176737/ 43.612465, val:  40.42%, val_best:  52.92%, tr:  98.47%, tr_best:  99.08%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.2802%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.8858%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17261  13.563%\n",
      "fc layer 1 self.abs_max_out: 3217.0\n",
      "fc layer 1 self.abs_max_out: 3400.0\n",
      "lif layer 1 self.abs_max_v: 5635.0\n",
      "lif layer 1 self.abs_max_v: 5736.5\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  5.066112/ 46.511108, val:  38.75%, val_best:  52.92%, tr:  98.57%, tr_best:  99.08%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.9600%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.6720%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18488  13.489%\n",
      "fc layer 2 self.abs_max_out: 3760.0\n",
      "lif layer 2 self.abs_max_v: 6551.0\n",
      "lif layer 2 self.abs_max_v: 6694.0\n",
      "lif layer 2 self.abs_max_v: 6927.0\n",
      "fc layer 1 self.abs_max_out: 3492.0\n",
      "lif layer 1 self.abs_max_v: 5744.0\n",
      "lif layer 1 self.abs_max_v: 5777.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  5.010258/ 34.926708, val:  43.33%, val_best:  52.92%, tr:  98.88%, tr_best:  99.08%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3317%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0051%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 19711  13.423%\n",
      "fc layer 2 self.abs_max_out: 3948.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  4.801554/ 20.597744, val:  51.67%, val_best:  52.92%, tr:  98.16%, tr_best:  99.08%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.2260%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 20901  13.343%\n",
      "fc layer 1 self.abs_max_out: 3562.0\n",
      "lif layer 1 self.abs_max_v: 5855.5\n",
      "lif layer 1 self.abs_max_v: 5904.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  4.529329/ 24.109589, val:  48.75%, val_best:  52.92%, tr:  98.88%, tr_best:  99.08%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1009%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4247%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22037  13.241%\n",
      "lif layer 2 self.abs_max_v: 7020.5\n",
      "lif layer 2 self.abs_max_v: 7034.0\n",
      "lif layer 2 self.abs_max_v: 7309.5\n",
      "lif layer 2 self.abs_max_v: 7354.5\n",
      "fc layer 1 self.abs_max_out: 3647.0\n",
      "lif layer 1 self.abs_max_v: 5973.5\n",
      "lif layer 1 self.abs_max_v: 6118.5\n",
      "lif layer 1 self.abs_max_v: 6140.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  4.553738/ 26.881514, val:  47.92%, val_best:  52.92%, tr:  98.26%, tr_best:  99.08%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.7671%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9509%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23234  13.185%\n",
      "lif layer 1 self.abs_max_v: 6391.5\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  4.775415/ 23.295902, val:  48.33%, val_best:  52.92%, tr:  98.67%, tr_best:  99.08%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5179%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24441  13.140%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  4.398646/ 37.033394, val:  39.17%, val_best:  52.92%, tr:  98.67%, tr_best:  99.08%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1809%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25570  13.059%\n",
      "fc layer 2 self.abs_max_out: 4722.0\n",
      "fc layer 1 self.abs_max_out: 3751.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  4.486142/ 37.477489, val:  41.67%, val_best:  52.92%, tr:  98.06%, tr_best:  99.08%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2132%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7308%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26735  13.004%\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  4.774599/ 18.833046, val:  58.75%, val_best:  58.75%, tr:  98.47%, tr_best:  99.08%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8108%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8505%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27944  12.974%\n",
      "fc layer 1 self.abs_max_out: 3923.0\n",
      "lif layer 1 self.abs_max_v: 6477.0\n",
      "lif layer 1 self.abs_max_v: 6717.5\n",
      "lif layer 2 self.abs_max_v: 7356.5\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  4.295997/ 21.569838, val:  58.33%, val_best:  58.75%, tr:  99.08%, tr_best:  99.08%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9148%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0756%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 29046  12.900%\n",
      "fc layer 1 self.abs_max_out: 3934.0\n",
      "lif layer 2 self.abs_max_v: 7450.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  4.089718/ 22.492292, val:  56.67%, val_best:  58.75%, tr:  98.98%, tr_best:  99.08%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.1102%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 30093  12.808%\n",
      "lif layer 2 self.abs_max_v: 7714.0\n",
      "fc layer 1 self.abs_max_out: 4058.0\n",
      "lif layer 1 self.abs_max_v: 6815.5\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  4.143157/ 26.240944, val:  47.08%, val_best:  58.75%, tr:  98.88%, tr_best:  99.08%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2027%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6213%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 31189  12.743%\n",
      "fc layer 1 self.abs_max_out: 4155.0\n",
      "lif layer 1 self.abs_max_v: 7073.5\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  4.143677/ 19.268953, val:  60.83%, val_best:  60.83%, tr:  98.26%, tr_best:  99.08%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4279%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1008%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 32319  12.697%\n",
      "fc layer 1 self.abs_max_out: 4251.0\n",
      "lif layer 1 self.abs_max_v: 7262.5\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  3.954597/ 26.260920, val:  57.08%, val_best:  60.83%, tr:  99.18%, tr_best:  99.18%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0504%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 33382  12.629%\n",
      "lif layer 1 self.abs_max_v: 7279.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  4.139869/ 21.242760, val:  51.25%, val_best:  60.83%, tr:  98.37%, tr_best:  99.18%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9518%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9510%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 34472  12.576%\n",
      "lif layer 1 self.abs_max_v: 7297.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  4.056213/ 29.625423, val:  49.17%, val_best:  60.83%, tr:  98.98%, tr_best:  99.18%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5411%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7505%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 35516  12.510%\n",
      "lif layer 2 self.abs_max_v: 7895.5\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  3.902347/ 28.393230, val:  45.42%, val_best:  60.83%, tr:  98.57%, tr_best:  99.18%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9027%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 36570  12.451%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  4.071476/ 19.646875, val:  48.75%, val_best:  60.83%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6428%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0838%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 37644  12.404%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  4.276172/ 31.536600, val:  53.75%, val_best:  60.83%, tr:  98.77%, tr_best:  99.28%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4530%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4583%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 38732  12.363%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  3.864516/ 21.259125, val:  54.17%, val_best:  60.83%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9527%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9153%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 39736  12.300%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  4.048800/ 24.904711, val:  63.75%, val_best:  63.75%, tr:  99.08%, tr_best:  99.28%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9983%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0602%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 40791  12.255%\n",
      "fc layer 1 self.abs_max_out: 4347.0\n",
      "lif layer 1 self.abs_max_v: 7567.5\n",
      "lif layer 1 self.abs_max_v: 7693.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  4.003863/ 22.193024, val:  55.42%, val_best:  63.75%, tr:  99.18%, tr_best:  99.28%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4116%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 41845  12.212%\n",
      "fc layer 1 self.abs_max_out: 4405.0\n",
      "lif layer 1 self.abs_max_v: 7721.5\n",
      "lif layer 1 self.abs_max_v: 7810.5\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  3.852021/ 20.513140, val:  52.08%, val_best:  63.75%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9445%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4181%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 42874  12.165%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  3.803079/ 16.867060, val:  67.08%, val_best:  67.08%, tr:  99.08%, tr_best:  99.28%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8166%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1131%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 43878  12.113%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  3.543995/ 18.984783, val:  60.00%, val_best:  67.08%, tr:  98.77%, tr_best:  99.28%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3313%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4521%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 44818  12.047%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  3.802590/ 23.406050, val:  52.08%, val_best:  67.08%, tr:  99.08%, tr_best:  99.28%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2483%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 45840  12.006%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  3.645214/ 20.503494, val:  62.92%, val_best:  67.08%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8212%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 46830  11.959%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  3.571561/ 16.370348, val:  57.08%, val_best:  67.08%, tr:  99.39%, tr_best:  99.49%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2321%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7918%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 47796  11.908%\n",
      "lif layer 2 self.abs_max_v: 7990.5\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  3.637853/ 23.521145, val:  52.50%, val_best:  67.08%, tr:  99.28%, tr_best:  99.49%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9828%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4729%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 48783  11.864%\n",
      "fc layer 2 self.abs_max_out: 4855.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  3.477964/ 23.322714, val:  58.33%, val_best:  67.08%, tr:  98.67%, tr_best:  99.49%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7893%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 49721  11.811%\n",
      "fc layer 1 self.abs_max_out: 4506.0\n",
      "lif layer 1 self.abs_max_v: 7922.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  3.779038/ 20.513197, val:  63.33%, val_best:  67.08%, tr:  98.88%, tr_best:  99.49%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6614%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 50758  11.783%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  3.616271/ 15.330380, val:  67.50%, val_best:  67.50%, tr:  98.57%, tr_best:  99.49%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1596%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0795%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 51743  11.745%\n",
      "fc layer 2 self.abs_max_out: 4943.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  3.558891/ 20.654110, val:  60.00%, val_best:  67.50%, tr:  98.77%, tr_best:  99.49%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6313%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8227%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 52726  11.708%\n",
      "fc layer 1 self.abs_max_out: 4530.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  3.747899/ 21.683790, val:  59.58%, val_best:  67.50%, tr:  99.18%, tr_best:  99.49%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0050%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7971%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 53751  11.682%\n",
      "fc layer 1 self.abs_max_out: 4557.0\n",
      "lif layer 1 self.abs_max_v: 7947.0\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  3.589444/ 20.818216, val:  50.42%, val_best:  67.50%, tr:  99.28%, tr_best:  99.49%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 54702  11.641%\n",
      "fc layer 1 self.abs_max_out: 4601.0\n",
      "fc layer 1 self.abs_max_out: 4726.0\n",
      "lif layer 1 self.abs_max_v: 8251.5\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  3.519293/ 26.023401, val:  60.00%, val_best:  67.50%, tr:  99.28%, tr_best:  99.49%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7905%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9770%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 55657  11.602%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  3.558076/ 19.789667, val:  66.67%, val_best:  67.50%, tr:  98.57%, tr_best:  99.49%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8414%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9508%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 56619  11.567%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  3.650401/ 19.290833, val:  61.67%, val_best:  67.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2248%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9194%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 57595  11.535%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  3.329861/ 18.360983, val:  54.58%, val_best:  67.50%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1469%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 58535  11.498%\n",
      "fc layer 1 self.abs_max_out: 4808.0\n",
      "lif layer 1 self.abs_max_v: 8396.5\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  3.782382/ 15.884917, val:  62.92%, val_best:  67.50%, tr:  98.67%, tr_best:  99.69%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8598%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8221%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 59566  11.480%\n",
      "fc layer 1 self.abs_max_out: 4904.0\n",
      "lif layer 1 self.abs_max_v: 8540.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  3.579632/ 18.440247, val:  55.42%, val_best:  67.50%, tr:  98.98%, tr_best:  99.69%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0103%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0024%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 60523  11.448%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  3.643008/ 19.818430, val:  63.75%, val_best:  67.50%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4701%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6584%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 61513  11.424%\n",
      "lif layer 1 self.abs_max_v: 8652.5\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  3.441888/ 21.198332, val:  60.00%, val_best:  67.50%, tr:  98.88%, tr_best:  99.69%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7890%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8085%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 62432  11.388%\n",
      "lif layer 2 self.abs_max_v: 8509.5\n",
      "fc layer 1 self.abs_max_out: 4947.0\n",
      "lif layer 1 self.abs_max_v: 8684.0\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  3.626468/ 22.938295, val:  58.33%, val_best:  67.50%, tr:  98.77%, tr_best:  99.69%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8094%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0528%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 63437  11.368%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  3.351398/ 19.191784, val:  65.00%, val_best:  67.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5349%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3882%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 64345  11.332%\n",
      "fc layer 1 self.abs_max_out: 5003.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  3.443718/ 20.517139, val:  54.58%, val_best:  67.50%, tr:  98.88%, tr_best:  99.69%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4577%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2502%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 65294  11.304%\n",
      "fc layer 1 self.abs_max_out: 5062.0\n",
      "lif layer 1 self.abs_max_v: 8747.0\n",
      "lif layer 1 self.abs_max_v: 8766.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  3.205663/ 20.707033, val:  56.67%, val_best:  67.50%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 66166  11.264%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  3.322785/ 26.666365, val:  48.33%, val_best:  67.50%, tr:  98.77%, tr_best:  99.69%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2894%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 67072  11.231%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  3.610572/ 14.869996, val:  66.25%, val_best:  67.50%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9205%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1699%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 68046  11.211%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  3.309827/ 21.223310, val:  53.33%, val_best:  67.50%, tr:  98.47%, tr_best:  99.69%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7058%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5270%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 68948  11.179%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  3.395302/ 30.614532, val:  57.08%, val_best:  67.50%, tr:  98.98%, tr_best:  99.69%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4442%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3924%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 69879  11.153%\n",
      "lif layer 1 self.abs_max_v: 8769.5\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.551413/ 22.356060, val:  48.33%, val_best:  67.50%, tr:  98.77%, tr_best:  99.69%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8019%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3710%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 70828  11.130%\n",
      "lif layer 1 self.abs_max_v: 8861.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  2.964571/ 17.313253, val:  65.42%, val_best:  67.50%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6466%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5693%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 71660  11.090%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  3.563461/ 18.675859, val:  57.92%, val_best:  67.50%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7094%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5170%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 72623  11.072%\n",
      "lif layer 1 self.abs_max_v: 8916.5\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.354752/ 21.416479, val:  60.83%, val_best:  67.50%, tr:  98.67%, tr_best:  99.69%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5794%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 73519  11.044%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.209365/ 17.212894, val:  68.33%, val_best:  68.33%, tr:  98.37%, tr_best:  99.69%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9095%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6109%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 74407  11.015%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  2.957023/ 22.499165, val:  57.92%, val_best:  68.33%, tr:  98.98%, tr_best:  99.69%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8972%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0199%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 75238  10.979%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.296623/ 27.441713, val:  55.83%, val_best:  68.33%, tr:  98.88%, tr_best:  99.69%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7224%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5681%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 76147  10.955%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.328021/ 28.771912, val:  54.17%, val_best:  68.33%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7464%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7701%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 77055  10.932%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  2.982852/ 13.379105, val:  70.42%, val_best:  70.42%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8238%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 77867  10.896%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.088809/ 20.166904, val:  62.92%, val_best:  70.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4064%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8269%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 78723  10.866%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.189862/ 20.953682, val:  68.33%, val_best:  70.42%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4987%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6217%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 79629  10.845%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.150472/ 15.878212, val:  67.08%, val_best:  70.42%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5922%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3304%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 80468  10.815%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.328544/ 15.444944, val:  74.58%, val_best:  74.58%, tr:  98.98%, tr_best:  99.69%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 81375  10.795%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.363069/ 26.757423, val:  52.92%, val_best:  74.58%, tr:  98.88%, tr_best:  99.69%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0416%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5028%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 82287  10.776%\n",
      "lif layer 1 self.abs_max_v: 8957.5\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  2.990773/ 22.745050, val:  63.33%, val_best:  74.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6327%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7381%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 83117  10.747%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  2.943149/ 17.600969, val:  66.25%, val_best:  74.58%, tr:  98.37%, tr_best:  99.69%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6611%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 83934  10.717%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  2.882635/ 13.666451, val:  70.00%, val_best:  74.58%, tr:  98.67%, tr_best:  99.69%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5951%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0619%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 84722  10.684%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.118490/ 17.674547, val:  48.75%, val_best:  74.58%, tr:  98.16%, tr_best:  99.69%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6424%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8917%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 85588  10.661%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.198988/ 21.013535, val:  65.83%, val_best:  74.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8593%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 86481  10.643%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.336094/ 19.998518, val:  54.17%, val_best:  74.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7131%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8827%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 87367  10.624%\n",
      "fc layer 1 self.abs_max_out: 5256.0\n",
      "fc layer 1 self.abs_max_out: 5276.0\n",
      "lif layer 1 self.abs_max_v: 8969.0\n",
      "lif layer 1 self.abs_max_v: 9381.5\n",
      "lif layer 1 self.abs_max_v: 9419.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.098075/ 24.978008, val:  54.58%, val_best:  74.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2758%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8559%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 88227  10.602%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.347521/ 15.192282, val:  67.08%, val_best:  74.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6254%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4530%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 89088  10.581%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  2.952373/ 19.791182, val:  60.00%, val_best:  74.58%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4616%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8935%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 89920  10.557%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  2.950438/ 23.379225, val:  66.67%, val_best:  74.58%, tr:  98.67%, tr_best:  99.69%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5698%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0149%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 90746  10.533%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.103693/ 18.203901, val:  67.08%, val_best:  74.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3837%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8984%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 91569  10.509%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.221296/ 16.681852, val:  67.08%, val_best:  74.58%, tr:  98.98%, tr_best:  99.69%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2835%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1694%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 92459  10.494%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.117474/ 19.788771, val:  60.00%, val_best:  74.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6485%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1116%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 93309  10.474%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  2.925929/ 12.977503, val:  75.83%, val_best:  75.83%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5569%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2400%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 94086  10.446%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  2.745689/ 22.226442, val:  54.17%, val_best:  75.83%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5351%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1987%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 94839  10.416%\n",
      "fc layer 1 self.abs_max_out: 5319.0\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.058480/ 21.721458, val:  65.83%, val_best:  75.83%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8066%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1332%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 95676  10.397%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.904902/ 20.026157, val:  65.83%, val_best:  75.83%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9022%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1205%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 96461  10.372%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.870739/ 19.546453, val:  71.67%, val_best:  75.83%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6011%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2103%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 97269  10.350%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.900067/ 23.291647, val:  63.75%, val_best:  75.83%, tr:  98.88%, tr_best:  99.69%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6824%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9162%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 98064  10.327%\n",
      "fc layer 1 self.abs_max_out: 5324.0\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  3.026660/ 16.723465, val:  63.33%, val_best:  75.83%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5822%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0594%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 98879  10.306%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.809204/ 18.598164, val:  62.08%, val_best:  75.83%, tr:  98.37%, tr_best:  99.69%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7869%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1839%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 99635  10.280%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.810703/ 22.724279, val:  65.00%, val_best:  75.83%, tr:  99.49%, tr_best:  99.69%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4989%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2539%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 100424  10.258%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  3.078693/ 18.356718, val:  74.58%, val_best:  75.83%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4257%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5988%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 101233  10.238%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  3.098332/ 19.233688, val:  61.67%, val_best:  75.83%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4365%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7009%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 102063  10.221%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.780309/ 23.905441, val:  60.83%, val_best:  75.83%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4918%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0135%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 102840  10.199%\n",
      "fc layer 1 self.abs_max_out: 5388.0\n",
      "fc layer 1 self.abs_max_out: 5397.0\n",
      "lif layer 1 self.abs_max_v: 9578.5\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.898593/ 26.232124, val:  60.42%, val_best:  75.83%, tr:  99.08%, tr_best:  99.69%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4606%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8087%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 103638  10.179%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.982834/ 18.942980, val:  72.08%, val_best:  75.83%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5367%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 104410  10.157%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.908782/ 20.361189, val:  70.00%, val_best:  75.83%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8117%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8134%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 105178  10.135%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.854878/ 28.819147, val:  42.50%, val_best:  75.83%, tr:  98.88%, tr_best:  99.69%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7019%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9204%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 105923  10.112%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  3.051073/ 23.265440, val:  53.33%, val_best:  75.83%, tr:  98.88%, tr_best:  99.69%, epoch time: 75.30 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4673%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8838%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 106768  10.098%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.983082/ 15.337232, val:  77.08%, val_best:  77.08%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6649%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 107579  10.081%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.966274/ 14.864030, val:  71.67%, val_best:  77.08%, tr:  98.88%, tr_best:  99.69%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8186%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8014%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 108366  10.063%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.848284/ 17.333958, val:  69.17%, val_best:  77.08%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9999%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 109102  10.040%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.774487/ 19.893217, val:  71.25%, val_best:  77.08%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4856%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 109858  10.019%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.970124/ 11.381715, val:  79.17%, val_best:  79.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6707%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1530%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 110647  10.002%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.787353/ 22.196217, val:  65.00%, val_best:  79.17%, tr:  98.98%, tr_best:  99.69%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3103%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0340%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 111431   9.984%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.888039/ 24.926165, val:  52.08%, val_best:  79.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3133%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3623%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 112241   9.969%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.785699/ 16.458040, val:  65.83%, val_best:  79.17%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5477%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2089%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 113009   9.951%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.774682/ 21.972313, val:  55.00%, val_best:  79.17%, tr:  98.88%, tr_best:  99.69%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5388%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 113769   9.932%\n",
      "fc layer 1 self.abs_max_out: 5401.0\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.886285/ 17.104824, val:  62.92%, val_best:  79.17%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7179%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4569%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 114555   9.916%\n",
      "fc layer 1 self.abs_max_out: 5443.0\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.809567/ 20.278915, val:  60.00%, val_best:  79.17%, tr:  99.08%, tr_best:  99.69%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8359%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3998%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 115303   9.897%\n",
      "fc layer 1 self.abs_max_out: 5461.0\n",
      "fc layer 1 self.abs_max_out: 5549.0\n",
      "lif layer 1 self.abs_max_v: 9620.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.707423/ 19.884407, val:  55.83%, val_best:  79.17%, tr:  98.67%, tr_best:  99.69%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7362%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4634%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 116038   9.877%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.666419/ 14.536208, val:  71.25%, val_best:  79.17%, tr:  98.67%, tr_best:  99.69%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5387%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5006%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 116759   9.856%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.864872/ 17.458033, val:  56.67%, val_best:  79.17%, tr:  99.28%, tr_best:  99.69%, epoch time: 74.14 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2493%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3356%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 117540   9.841%\n",
      "fc layer 2 self.abs_max_out: 5001.0\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.713459/ 14.925480, val:  71.67%, val_best:  79.17%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4516%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 118269   9.822%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.749274/ 20.101633, val:  61.67%, val_best:  79.17%, tr:  98.88%, tr_best:  99.69%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5895%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4209%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 119024   9.805%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.637782/ 14.564645, val:  74.58%, val_best:  79.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5370%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3495%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 119716   9.783%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.728421/ 19.073597, val:  70.00%, val_best:  79.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3818%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1410%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 120474   9.767%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.583886/ 25.082951, val:  60.00%, val_best:  79.17%, tr:  98.47%, tr_best:  99.69%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3771%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4953%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 121204   9.748%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.718554/ 18.421144, val:  67.08%, val_best:  79.17%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0491%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6185%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 121923   9.730%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.581648/ 16.217859, val:  71.25%, val_best:  79.17%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5439%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7030%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 122633   9.710%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.652768/ 20.401154, val:  60.83%, val_best:  79.17%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3341%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 123368   9.693%\n",
      "fc layer 1 self.abs_max_out: 5564.0\n",
      "lif layer 1 self.abs_max_v: 9677.0\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.771388/ 20.452465, val:  64.58%, val_best:  79.17%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3054%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1406%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 124114   9.678%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  3.007889/ 15.367883, val:  70.42%, val_best:  79.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4222%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1576%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 124911   9.666%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.517682/ 22.843901, val:  56.67%, val_best:  79.17%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6857%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3939%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 125612   9.647%\n",
      "fc layer 1 self.abs_max_out: 5576.0\n",
      "lif layer 1 self.abs_max_v: 9681.0\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.536402/ 19.134346, val:  61.67%, val_best:  79.17%, tr:  98.98%, tr_best:  99.69%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4237%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3764%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 126313   9.629%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.565675/ 21.122898, val:  57.92%, val_best:  79.17%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3925%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0105%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 126995   9.609%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.653827/ 24.233156, val:  62.08%, val_best:  79.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3596%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3122%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 127728   9.593%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  2.599158/ 15.915319, val:  75.00%, val_best:  79.17%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3384%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4116%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 128403   9.574%\n",
      "fc layer 1 self.abs_max_out: 5584.0\n",
      "lif layer 1 self.abs_max_v: 9706.5\n",
      "lif layer 1 self.abs_max_v: 9750.5\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.558979/ 14.312922, val:  76.25%, val_best:  79.17%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2741%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5309%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 129100   9.556%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  2.424315/ 16.281265, val:  72.92%, val_best:  79.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1992%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 129780   9.537%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.711700/ 23.657862, val:  62.92%, val_best:  79.17%, tr:  98.67%, tr_best:  99.69%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3758%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 130528   9.523%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  2.700075/ 15.240115, val:  76.67%, val_best:  79.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4180%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2721%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 131258   9.509%\n",
      "fc layer 2 self.abs_max_out: 5174.0\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.453712/ 17.178343, val:  69.58%, val_best:  79.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6544%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4137%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 131937   9.491%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  2.621410/ 16.919180, val:  74.58%, val_best:  79.17%, tr:  98.77%, tr_best:  99.69%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6242%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5182%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 132671   9.477%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  2.608080/ 17.498075, val:  65.83%, val_best:  79.17%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6274%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4106%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 133353   9.459%\n",
      "fc layer 1 self.abs_max_out: 5597.0\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  2.634656/ 20.386526, val:  59.17%, val_best:  79.17%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6426%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5499%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 134047   9.443%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  2.330598/ 17.543159, val:  57.08%, val_best:  79.17%, tr:  98.98%, tr_best:  99.69%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7575%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 134696   9.424%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  2.455715/ 17.700876, val:  65.42%, val_best:  79.17%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4603%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 135368   9.406%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  2.487617/ 20.680552, val:  53.75%, val_best:  79.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7893%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 136044   9.389%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  2.629504/ 17.815424, val:  69.17%, val_best:  79.17%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4734%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7138%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 136750   9.375%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  2.349262/ 17.237045, val:  66.25%, val_best:  79.17%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5398%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9610%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 137407   9.357%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  2.368656/ 14.836850, val:  76.25%, val_best:  79.17%, tr:  98.88%, tr_best:  99.69%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7937%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 138059   9.339%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  2.499402/ 14.901138, val:  65.83%, val_best:  79.17%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7452%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 138763   9.325%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  2.605230/ 20.816156, val:  61.67%, val_best:  79.17%, tr:  99.28%, tr_best:  99.69%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0035%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5069%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 139456   9.310%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  2.499924/ 19.695276, val:  66.25%, val_best:  79.17%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5460%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 140131   9.295%\n",
      "fc layer 1 self.abs_max_out: 5675.0\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  2.556503/ 17.916586, val:  67.08%, val_best:  79.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5042%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5540%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 140823   9.280%\n",
      "fc layer 1 self.abs_max_out: 5676.0\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  2.599054/ 19.850626, val:  61.67%, val_best:  79.17%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5328%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 141517   9.266%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  2.630456/ 16.135876, val:  71.25%, val_best:  79.17%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6511%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3526%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 142212   9.252%\n",
      "lif layer 1 self.abs_max_v: 9793.5\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  2.524465/ 20.289635, val:  69.58%, val_best:  79.17%, tr:  98.57%, tr_best:  99.69%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5390%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2776%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 142867   9.236%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  2.410589/ 16.383404, val:  75.00%, val_best:  79.17%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6495%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6905%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 143515   9.220%\n",
      "lif layer 1 self.abs_max_v: 9805.5\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  2.471371/ 12.391665, val:  79.17%, val_best:  79.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7226%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8365%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 144169   9.204%\n",
      "fc layer 1 self.abs_max_out: 5812.0\n",
      "lif layer 1 self.abs_max_v: 10076.0\n",
      "lif layer 1 self.abs_max_v: 10269.5\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  2.511074/ 18.414515, val:  69.58%, val_best:  79.17%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7083%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9026%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 144855   9.190%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  2.561201/ 16.165697, val:  79.58%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 70.79 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6066%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6245%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 145551   9.177%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  2.323033/ 22.011234, val:  66.25%, val_best:  79.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7392%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7232%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 146189   9.161%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  2.597857/ 13.768672, val:  78.33%, val_best:  79.58%, tr:  98.98%, tr_best:  99.69%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4403%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 146869   9.148%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  2.413234/ 17.846043, val:  74.58%, val_best:  79.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6209%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 147511   9.132%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  2.481798/ 24.445044, val:  61.67%, val_best:  79.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5372%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6047%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 148166   9.117%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  2.375915/ 17.942492, val:  71.25%, val_best:  79.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5012%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 148816   9.102%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  2.370085/ 23.130301, val:  63.75%, val_best:  79.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7165%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7734%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 149475   9.088%\n",
      "fc layer 1 self.abs_max_out: 5819.0\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  2.403366/ 21.834312, val:  72.08%, val_best:  79.58%, tr:  98.98%, tr_best:  99.80%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8793%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 150116   9.073%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  2.188976/ 23.855640, val:  65.00%, val_best:  79.58%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9493%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 150710   9.055%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  2.451484/ 15.001477, val:  79.58%, val_best:  79.58%, tr:  98.98%, tr_best:  99.80%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6622%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6816%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 151349   9.041%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  2.287476/ 20.247826, val:  64.17%, val_best:  79.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7727%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 151957   9.024%\n",
      "fc layer 1 self.abs_max_out: 5924.0\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  2.649906/ 16.378094, val:  61.67%, val_best:  79.58%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6370%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 152637   9.012%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  2.305193/ 19.754456, val:  64.58%, val_best:  79.58%, tr:  99.08%, tr_best:  99.80%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6328%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7379%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 153271   8.998%\n",
      "fc layer 1 self.abs_max_out: 5979.0\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  2.366765/ 20.810263, val:  67.92%, val_best:  79.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8994%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7212%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 153914   8.984%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  2.450585/ 15.824769, val:  73.75%, val_best:  79.58%, tr:  98.47%, tr_best:  99.80%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9564%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 154570   8.971%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  2.392074/ 18.084665, val:  62.92%, val_best:  79.58%, tr:  98.67%, tr_best:  99.80%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5415%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9915%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 155230   8.958%\n",
      "fc layer 1 self.abs_max_out: 5993.0\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  2.533271/ 17.813795, val:  60.83%, val_best:  79.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8370%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8340%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 155927   8.948%\n",
      "fc layer 1 self.abs_max_out: 6076.0\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  2.186882/ 20.536118, val:  68.33%, val_best:  79.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7895%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9539%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 156518   8.932%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  2.578947/ 16.576439, val:  55.83%, val_best:  79.58%, tr:  98.77%, tr_best:  99.80%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7338%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0282%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 157216   8.922%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  2.482926/ 18.001818, val:  76.25%, val_best:  79.58%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7384%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 157899   8.911%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  2.474051/ 16.767515, val:  78.75%, val_best:  79.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8877%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9289%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 158552   8.899%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  2.213441/ 17.207214, val:  73.33%, val_best:  79.58%, tr:  98.88%, tr_best:  99.80%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6148%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0862%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 159166   8.884%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  2.202189/ 24.812662, val:  66.25%, val_best:  79.58%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7094%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8582%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 159764   8.869%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  2.367403/ 25.465992, val:  60.00%, val_best:  79.58%, tr:  98.98%, tr_best:  99.80%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4949%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 160396   8.856%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  2.498019/ 17.647356, val:  69.58%, val_best:  79.58%, tr:  99.08%, tr_best:  99.80%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5747%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8839%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 161045   8.844%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  2.273176/ 14.994380, val:  76.67%, val_best:  79.58%, tr:  99.08%, tr_best:  99.80%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4041%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.2072%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 161643   8.829%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  2.327505/ 19.377422, val:  72.08%, val_best:  79.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6296%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8115%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 162274   8.817%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  2.249022/ 17.105814, val:  72.08%, val_best:  79.58%, tr:  99.08%, tr_best:  99.80%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6425%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9425%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 162892   8.803%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  2.126812/ 15.354129, val:  75.83%, val_best:  79.58%, tr:  98.98%, tr_best:  99.80%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8580%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0332%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 163487   8.789%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  2.100540/ 20.776817, val:  63.75%, val_best:  79.58%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7117%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9751%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 164059   8.774%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  2.267222/ 23.471457, val:  62.08%, val_best:  79.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4153%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0249%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 164672   8.761%\n",
      "lif layer 2 self.abs_max_v: 8521.5\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  2.321557/ 17.873913, val:  76.25%, val_best:  79.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4332%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 165296   8.748%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  2.203579/ 18.864859, val:  68.33%, val_best:  79.58%, tr:  98.98%, tr_best:  99.80%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5817%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0273%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 165897   8.735%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  2.225494/ 17.375050, val:  67.92%, val_best:  79.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.1006%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 166488   8.721%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  2.105669/ 22.174236, val:  68.33%, val_best:  79.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6406%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9589%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 167054   8.706%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  2.213772/ 16.752953, val:  71.67%, val_best:  79.58%, tr:  98.88%, tr_best:  99.80%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6548%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.2327%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 167629   8.692%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  2.307604/ 15.049897, val:  72.92%, val_best:  79.58%, tr:  98.88%, tr_best:  99.80%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7073%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.1861%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 168264   8.680%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  2.396716/ 24.307829, val:  61.67%, val_best:  79.58%, tr:  98.47%, tr_best:  99.80%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4693%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0634%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 168877   8.668%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  2.285742/ 20.161646, val:  67.08%, val_best:  79.58%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5969%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0394%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f192b6f295a480888b022318d0f5b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÉ‚ñÖ‚ñà‚ñÜ‚ñà‚ñÜ‚ñà‚ñÖ‚ñÜ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99183</td></tr><tr><td>tr_epoch_loss</td><td>2.28574</td></tr><tr><td>val_acc_best</td><td>0.79583</td></tr><tr><td>val_acc_now</td><td>0.67083</td></tr><tr><td>val_loss</td><td>20.16165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-sweep-14</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fvga2htt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fvga2htt</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_101536-fvga2htt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qbe4cp57 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_143153-qbe4cp57</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qbe4cp57' target=\"_blank\">grateful-sweep-22</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qbe4cp57' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qbe4cp57</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251214_143202_955', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 64, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 32, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 2, 'lif_layer_v_threshold2': 128, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 32, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 2, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 2 self.abs_max_out: 6.0\n",
      "lif layer 2 self.abs_max_v: 6.0\n",
      "fc layer 1 self.abs_max_out: 55.0\n",
      "lif layer 1 self.abs_max_v: 81.5\n",
      "fc layer 2 self.abs_max_out: 23.0\n",
      "lif layer 2 self.abs_max_v: 25.0\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 129.5\n",
      "fc layer 2 self.abs_max_out: 25.0\n",
      "lif layer 2 self.abs_max_v: 31.5\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 147.0\n",
      "fc layer 2 self.abs_max_out: 30.0\n",
      "lif layer 2 self.abs_max_v: 37.5\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 200.0\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 54.0\n",
      "fc layer 1 self.abs_max_out: 212.0\n",
      "lif layer 1 self.abs_max_v: 229.0\n",
      "fc layer 2 self.abs_max_out: 45.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "fc layer 1 self.abs_max_out: 242.0\n",
      "lif layer 1 self.abs_max_v: 265.5\n",
      "fc layer 2 self.abs_max_out: 57.0\n",
      "lif layer 2 self.abs_max_v: 93.0\n",
      "fc layer 2 self.abs_max_out: 60.0\n",
      "lif layer 2 self.abs_max_v: 106.5\n",
      "fc layer 1 self.abs_max_out: 246.0\n",
      "fc layer 2 self.abs_max_out: 78.0\n",
      "fc layer 2 self.abs_max_out: 95.0\n",
      "lif layer 2 self.abs_max_v: 134.0\n",
      "fc layer 2 self.abs_max_out: 102.0\n",
      "lif layer 2 self.abs_max_v: 169.0\n",
      "fc layer 3 self.abs_max_out: 3.0\n",
      "fc layer 2 self.abs_max_out: 116.0\n",
      "lif layer 2 self.abs_max_v: 200.5\n",
      "fc layer 3 self.abs_max_out: 10.0\n",
      "fc layer 2 self.abs_max_out: 131.0\n",
      "lif layer 2 self.abs_max_v: 231.5\n",
      "fc layer 3 self.abs_max_out: 11.0\n",
      "fc layer 2 self.abs_max_out: 133.0\n",
      "lif layer 2 self.abs_max_v: 232.0\n",
      "fc layer 3 self.abs_max_out: 13.0\n",
      "fc layer 1 self.abs_max_out: 338.0\n",
      "lif layer 1 self.abs_max_v: 338.0\n",
      "fc layer 2 self.abs_max_out: 138.0\n",
      "lif layer 2 self.abs_max_v: 245.0\n",
      "fc layer 1 self.abs_max_out: 427.0\n",
      "lif layer 1 self.abs_max_v: 427.0\n",
      "fc layer 2 self.abs_max_out: 152.0\n",
      "lif layer 2 self.abs_max_v: 246.5\n",
      "fc layer 3 self.abs_max_out: 14.0\n",
      "fc layer 2 self.abs_max_out: 158.0\n",
      "fc layer 3 self.abs_max_out: 18.0\n",
      "lif layer 2 self.abs_max_v: 256.5\n",
      "fc layer 3 self.abs_max_out: 20.0\n",
      "fc layer 2 self.abs_max_out: 181.0\n",
      "lif layer 2 self.abs_max_v: 295.5\n",
      "fc layer 2 self.abs_max_out: 188.0\n",
      "lif layer 2 self.abs_max_v: 336.0\n",
      "fc layer 2 self.abs_max_out: 189.0\n",
      "lif layer 2 self.abs_max_v: 357.0\n",
      "fc layer 2 self.abs_max_out: 227.0\n",
      "lif layer 2 self.abs_max_v: 386.0\n",
      "fc layer 2 self.abs_max_out: 248.0\n",
      "lif layer 2 self.abs_max_v: 441.0\n",
      "fc layer 2 self.abs_max_out: 269.0\n",
      "fc layer 3 self.abs_max_out: 29.0\n",
      "lif layer 2 self.abs_max_v: 451.0\n",
      "lif layer 2 self.abs_max_v: 485.5\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "lif layer 2 self.abs_max_v: 488.5\n",
      "fc layer 2 self.abs_max_out: 275.0\n",
      "fc layer 3 self.abs_max_out: 34.0\n",
      "lif layer 2 self.abs_max_v: 501.5\n",
      "fc layer 2 self.abs_max_out: 284.0\n",
      "fc layer 2 self.abs_max_out: 295.0\n",
      "lif layer 2 self.abs_max_v: 539.5\n",
      "fc layer 2 self.abs_max_out: 298.0\n",
      "lif layer 2 self.abs_max_v: 568.0\n",
      "fc layer 2 self.abs_max_out: 299.0\n",
      "lif layer 2 self.abs_max_v: 577.0\n",
      "fc layer 2 self.abs_max_out: 311.0\n",
      "fc layer 2 self.abs_max_out: 326.0\n",
      "lif layer 2 self.abs_max_v: 592.0\n",
      "fc layer 2 self.abs_max_out: 331.0\n",
      "fc layer 2 self.abs_max_out: 342.0\n",
      "lif layer 2 self.abs_max_v: 603.5\n",
      "fc layer 2 self.abs_max_out: 418.0\n",
      "lif layer 2 self.abs_max_v: 639.5\n",
      "lif layer 2 self.abs_max_v: 703.0\n",
      "fc layer 3 self.abs_max_out: 37.0\n",
      "fc layer 2 self.abs_max_out: 495.0\n",
      "lif layer 2 self.abs_max_v: 704.5\n",
      "lif layer 2 self.abs_max_v: 761.5\n",
      "fc layer 2 self.abs_max_out: 510.0\n",
      "lif layer 2 self.abs_max_v: 891.0\n",
      "lif layer 2 self.abs_max_v: 916.0\n",
      "lif layer 2 self.abs_max_v: 961.0\n",
      "fc layer 3 self.abs_max_out: 41.0\n",
      "fc layer 2 self.abs_max_out: 524.0\n",
      "fc layer 3 self.abs_max_out: 49.0\n",
      "lif layer 2 self.abs_max_v: 969.0\n",
      "lif layer 2 self.abs_max_v: 997.5\n",
      "fc layer 1 self.abs_max_out: 498.0\n",
      "lif layer 1 self.abs_max_v: 498.0\n",
      "fc layer 2 self.abs_max_out: 565.0\n",
      "fc layer 3 self.abs_max_out: 53.0\n",
      "fc layer 1 self.abs_max_out: 565.0\n",
      "lif layer 1 self.abs_max_v: 565.0\n",
      "lif layer 2 self.abs_max_v: 1012.5\n",
      "fc layer 2 self.abs_max_out: 582.0\n",
      "lif layer 2 self.abs_max_v: 1088.5\n",
      "lif layer 2 self.abs_max_v: 1114.5\n",
      "fc layer 2 self.abs_max_out: 588.0\n",
      "lif layer 2 self.abs_max_v: 1145.5\n",
      "fc layer 3 self.abs_max_out: 59.0\n",
      "fc layer 2 self.abs_max_out: 615.0\n",
      "fc layer 2 self.abs_max_out: 619.0\n",
      "fc layer 3 self.abs_max_out: 93.0\n",
      "fc layer 2 self.abs_max_out: 662.0\n",
      "fc layer 3 self.abs_max_out: 96.0\n",
      "lif layer 2 self.abs_max_v: 1216.0\n",
      "fc layer 3 self.abs_max_out: 107.0\n",
      "fc layer 3 self.abs_max_out: 118.0\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "fc layer 3 self.abs_max_out: 123.0\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 3 self.abs_max_out: 148.0\n",
      "fc layer 3 self.abs_max_out: 149.0\n",
      "fc layer 2 self.abs_max_out: 685.0\n",
      "fc layer 3 self.abs_max_out: 153.0\n",
      "lif layer 2 self.abs_max_v: 1248.0\n",
      "fc layer 2 self.abs_max_out: 696.0\n",
      "lif layer 2 self.abs_max_v: 1320.0\n",
      "lif layer 1 self.abs_max_v: 568.0\n",
      "lif layer 1 self.abs_max_v: 589.5\n",
      "lif layer 1 self.abs_max_v: 592.0\n",
      "lif layer 1 self.abs_max_v: 671.0\n",
      "fc layer 3 self.abs_max_out: 156.0\n",
      "fc layer 2 self.abs_max_out: 736.0\n",
      "fc layer 2 self.abs_max_out: 759.0\n",
      "lif layer 2 self.abs_max_v: 1398.0\n",
      "fc layer 2 self.abs_max_out: 760.0\n",
      "lif layer 2 self.abs_max_v: 1431.5\n",
      "fc layer 3 self.abs_max_out: 173.0\n",
      "fc layer 3 self.abs_max_out: 186.0\n",
      "lif layer 2 self.abs_max_v: 1433.0\n",
      "fc layer 2 self.abs_max_out: 774.0\n",
      "lif layer 2 self.abs_max_v: 1457.0\n",
      "fc layer 1 self.abs_max_out: 677.0\n",
      "lif layer 1 self.abs_max_v: 677.0\n",
      "fc layer 3 self.abs_max_out: 198.0\n",
      "fc layer 2 self.abs_max_out: 789.0\n",
      "lif layer 2 self.abs_max_v: 1491.0\n",
      "lif layer 2 self.abs_max_v: 1519.5\n",
      "lif layer 1 self.abs_max_v: 680.0\n",
      "fc layer 2 self.abs_max_out: 813.0\n",
      "fc layer 3 self.abs_max_out: 223.0\n",
      "fc layer 3 self.abs_max_out: 235.0\n",
      "lif layer 1 self.abs_max_v: 730.0\n",
      "lif layer 1 self.abs_max_v: 738.0\n",
      "lif layer 1 self.abs_max_v: 827.0\n",
      "lif layer 1 self.abs_max_v: 835.0\n",
      "lif layer 1 self.abs_max_v: 860.5\n",
      "lif layer 1 self.abs_max_v: 879.5\n",
      "lif layer 1 self.abs_max_v: 989.0\n",
      "lif layer 1 self.abs_max_v: 1006.5\n",
      "lif layer 1 self.abs_max_v: 1117.5\n",
      "lif layer 1 self.abs_max_v: 1185.0\n",
      "fc layer 1 self.abs_max_out: 679.0\n",
      "fc layer 1 self.abs_max_out: 688.0\n",
      "lif layer 1 self.abs_max_v: 1185.5\n",
      "fc layer 1 self.abs_max_out: 702.0\n",
      "fc layer 1 self.abs_max_out: 720.0\n",
      "fc layer 1 self.abs_max_out: 822.0\n",
      "lif layer 1 self.abs_max_v: 1232.0\n",
      "fc layer 1 self.abs_max_out: 1000.0\n",
      "lif layer 1 self.abs_max_v: 1247.0\n",
      "lif layer 2 self.abs_max_v: 1523.5\n",
      "fc layer 2 self.abs_max_out: 842.0\n",
      "fc layer 1 self.abs_max_out: 1003.0\n",
      "fc layer 3 self.abs_max_out: 248.0\n",
      "lif layer 2 self.abs_max_v: 1565.0\n",
      "fc layer 2 self.abs_max_out: 885.0\n",
      "lif layer 2 self.abs_max_v: 1604.5\n",
      "lif layer 2 self.abs_max_v: 1628.5\n",
      "lif layer 1 self.abs_max_v: 1254.5\n",
      "fc layer 2 self.abs_max_out: 918.0\n",
      "lif layer 2 self.abs_max_v: 1661.0\n",
      "lif layer 2 self.abs_max_v: 1746.5\n",
      "fc layer 2 self.abs_max_out: 932.0\n",
      "lif layer 1 self.abs_max_v: 1269.5\n",
      "lif layer 1 self.abs_max_v: 1376.0\n",
      "lif layer 1 self.abs_max_v: 1445.0\n",
      "lif layer 1 self.abs_max_v: 1528.5\n",
      "lif layer 1 self.abs_max_v: 1622.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  7.606249/ 72.304390, val:  27.50%, val_best:  27.50%, tr:  98.47%, tr_best:  98.47%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.7078%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.9183%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2030  20.735%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 255.0\n",
      "fc layer 3 self.abs_max_out: 259.0\n",
      "fc layer 3 self.abs_max_out: 298.0\n",
      "fc layer 2 self.abs_max_out: 934.0\n",
      "lif layer 2 self.abs_max_v: 1775.5\n",
      "fc layer 2 self.abs_max_out: 1007.0\n",
      "lif layer 2 self.abs_max_v: 1895.0\n",
      "fc layer 3 self.abs_max_out: 305.0\n",
      "fc layer 2 self.abs_max_out: 1031.0\n",
      "fc layer 3 self.abs_max_out: 306.0\n",
      "fc layer 2 self.abs_max_out: 1047.0\n",
      "fc layer 3 self.abs_max_out: 307.0\n",
      "fc layer 3 self.abs_max_out: 320.0\n",
      "fc layer 2 self.abs_max_out: 1139.0\n",
      "fc layer 2 self.abs_max_out: 1144.0\n",
      "fc layer 2 self.abs_max_out: 1147.0\n",
      "fc layer 2 self.abs_max_out: 1200.0\n",
      "fc layer 2 self.abs_max_out: 1231.0\n",
      "fc layer 1 self.abs_max_out: 1007.0\n",
      "fc layer 1 self.abs_max_out: 1066.0\n",
      "lif layer 1 self.abs_max_v: 1640.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  8.887541/ 59.013222, val:  34.58%, val_best:  34.58%, tr:  99.08%, tr_best:  99.08%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2839%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3289%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3583  18.299%\n",
      "fc layer 2 self.abs_max_out: 1265.0\n",
      "fc layer 3 self.abs_max_out: 332.0\n",
      "fc layer 3 self.abs_max_out: 337.0\n",
      "fc layer 3 self.abs_max_out: 358.0\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 2 self.abs_max_out: 1341.0\n",
      "fc layer 2 self.abs_max_out: 1344.0\n",
      "fc layer 2 self.abs_max_out: 1355.0\n",
      "fc layer 2 self.abs_max_out: 1387.0\n",
      "fc layer 2 self.abs_max_out: 1407.0\n",
      "lif layer 2 self.abs_max_v: 1896.5\n",
      "fc layer 2 self.abs_max_out: 1411.0\n",
      "fc layer 2 self.abs_max_out: 1469.0\n",
      "fc layer 3 self.abs_max_out: 388.0\n",
      "fc layer 3 self.abs_max_out: 393.0\n",
      "lif layer 2 self.abs_max_v: 1911.5\n",
      "fc layer 2 self.abs_max_out: 1519.0\n",
      "fc layer 2 self.abs_max_out: 1528.0\n",
      "lif layer 2 self.abs_max_v: 1994.0\n",
      "fc layer 1 self.abs_max_out: 1090.0\n",
      "lif layer 1 self.abs_max_v: 1781.0\n",
      "fc layer 1 self.abs_max_out: 1199.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.394480/ 51.465092, val:  37.50%, val_best:  37.50%, tr:  99.28%, tr_best:  99.28%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.4577%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8768%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5049  17.191%\n",
      "lif layer 2 self.abs_max_v: 1995.5\n",
      "lif layer 2 self.abs_max_v: 2042.0\n",
      "lif layer 2 self.abs_max_v: 2064.0\n",
      "fc layer 2 self.abs_max_out: 1564.0\n",
      "fc layer 2 self.abs_max_out: 1589.0\n",
      "fc layer 2 self.abs_max_out: 1591.0\n",
      "lif layer 1 self.abs_max_v: 1789.0\n",
      "lif layer 2 self.abs_max_v: 2064.5\n",
      "fc layer 2 self.abs_max_out: 1600.0\n",
      "fc layer 2 self.abs_max_out: 1613.0\n",
      "lif layer 2 self.abs_max_v: 2067.0\n",
      "lif layer 2 self.abs_max_v: 2131.5\n",
      "lif layer 2 self.abs_max_v: 2179.0\n",
      "lif layer 2 self.abs_max_v: 2263.5\n",
      "lif layer 2 self.abs_max_v: 2312.0\n",
      "lif layer 2 self.abs_max_v: 2315.0\n",
      "lif layer 2 self.abs_max_v: 2375.5\n",
      "lif layer 2 self.abs_max_v: 2424.0\n",
      "lif layer 1 self.abs_max_v: 1846.5\n",
      "fc layer 1 self.abs_max_out: 1242.0\n",
      "lif layer 1 self.abs_max_v: 2024.5\n",
      "lif layer 1 self.abs_max_v: 2056.5\n",
      "lif layer 1 self.abs_max_v: 2098.5\n",
      "lif layer 1 self.abs_max_v: 2163.5\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.288067/ 66.250397, val:  32.08%, val_best:  37.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0662%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3744%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6497  16.591%\n",
      "fc layer 2 self.abs_max_out: 1627.0\n",
      "fc layer 2 self.abs_max_out: 1654.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  8.526973/ 41.985409, val:  47.08%, val_best:  47.08%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.4115%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7944  16.229%\n",
      "fc layer 2 self.abs_max_out: 1692.0\n",
      "fc layer 2 self.abs_max_out: 1708.0\n",
      "fc layer 2 self.abs_max_out: 1720.0\n",
      "fc layer 1 self.abs_max_out: 1252.0\n",
      "lif layer 1 self.abs_max_v: 2179.0\n",
      "lif layer 1 self.abs_max_v: 2324.0\n",
      "lif layer 1 self.abs_max_v: 2378.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  8.631577/ 45.899105, val:  37.50%, val_best:  47.08%, tr:  98.57%, tr_best:  99.69%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.5071%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9420  16.037%\n",
      "lif layer 2 self.abs_max_v: 2488.5\n",
      "fc layer 1 self.abs_max_out: 1259.0\n",
      "fc layer 1 self.abs_max_out: 1314.0\n",
      "fc layer 1 self.abs_max_out: 1396.0\n",
      "lif layer 1 self.abs_max_v: 2449.0\n",
      "lif layer 1 self.abs_max_v: 2465.0\n",
      "lif layer 1 self.abs_max_v: 2502.5\n",
      "lif layer 1 self.abs_max_v: 2557.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  8.105613/ 50.223938, val:  41.67%, val_best:  47.08%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.4123%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.4242%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10895  15.898%\n",
      "fc layer 1 self.abs_max_out: 1401.0\n",
      "lif layer 2 self.abs_max_v: 2514.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  6.824859/ 48.084427, val:  44.58%, val_best:  47.08%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.2566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.5550%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 12258  15.651%\n",
      "lif layer 2 self.abs_max_v: 2526.0\n",
      "lif layer 2 self.abs_max_v: 2526.5\n",
      "fc layer 2 self.abs_max_out: 1769.0\n",
      "fc layer 1 self.abs_max_out: 1484.0\n",
      "fc layer 1 self.abs_max_out: 1535.0\n",
      "lif layer 1 self.abs_max_v: 2775.0\n",
      "lif layer 1 self.abs_max_v: 2885.0\n",
      "lif layer 1 self.abs_max_v: 2930.5\n",
      "fc layer 1 self.abs_max_out: 1588.0\n",
      "lif layer 1 self.abs_max_v: 2933.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  7.213674/ 37.149017, val:  44.17%, val_best:  47.08%, tr:  98.37%, tr_best:  99.69%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.9865%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.3530%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13677  15.523%\n",
      "lif layer 2 self.abs_max_v: 2535.5\n",
      "fc layer 2 self.abs_max_out: 1832.0\n",
      "fc layer 1 self.abs_max_out: 1620.0\n",
      "lif layer 1 self.abs_max_v: 2971.0\n",
      "fc layer 1 self.abs_max_out: 1727.0\n",
      "lif layer 1 self.abs_max_v: 3212.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  7.335243/ 39.012470, val:  35.42%, val_best:  47.08%, tr:  98.47%, tr_best:  99.69%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.7573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 15102  15.426%\n",
      "lif layer 2 self.abs_max_v: 2542.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  7.047855/ 48.279629, val:  40.00%, val_best:  47.08%, tr:  98.26%, tr_best:  99.69%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.0921%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 16512  15.333%\n",
      "lif layer 2 self.abs_max_v: 2543.0\n",
      "lif layer 2 self.abs_max_v: 2587.0\n",
      "lif layer 2 self.abs_max_v: 2601.5\n",
      "lif layer 2 self.abs_max_v: 2669.0\n",
      "lif layer 2 self.abs_max_v: 2689.5\n",
      "lif layer 2 self.abs_max_v: 2751.5\n",
      "lif layer 2 self.abs_max_v: 2984.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  6.295156/ 40.735306, val:  46.67%, val_best:  47.08%, tr:  98.57%, tr_best:  99.69%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.0735%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.7446%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 17869  15.210%\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  6.404837/ 33.078953, val:  41.25%, val_best:  47.08%, tr:  98.57%, tr_best:  99.69%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.4003%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 19256  15.130%\n",
      "fc layer 2 self.abs_max_out: 1844.0\n",
      "lif layer 2 self.abs_max_v: 3142.5\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  6.135099/ 58.800999, val:  32.92%, val_best:  47.08%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3588%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.3003%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 20642  15.061%\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  5.765250/ 32.172581, val:  46.67%, val_best:  47.08%, tr:  98.47%, tr_best:  99.69%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8710%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.4461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 21976  14.965%\n",
      "fc layer 2 self.abs_max_out: 1892.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  5.969615/ 31.525484, val:  41.67%, val_best:  47.08%, tr:  98.98%, tr_best:  99.69%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7405%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.2848%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 23330  14.894%\n",
      "lif layer 2 self.abs_max_v: 3149.0\n",
      "lif layer 2 self.abs_max_v: 3152.5\n",
      "fc layer 2 self.abs_max_out: 1930.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  5.522231/ 29.222406, val:  45.00%, val_best:  47.08%, tr:  98.98%, tr_best:  99.69%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.9122%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.6476%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 24626  14.797%\n",
      "fc layer 1 self.abs_max_out: 1765.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  5.406507/ 29.881674, val:  52.92%, val_best:  52.92%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5484%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.2169%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 25948  14.725%\n",
      "lif layer 2 self.abs_max_v: 3223.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  5.806101/ 30.887138, val:  45.42%, val_best:  52.92%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5594%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.0848%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 27329  14.692%\n",
      "lif layer 2 self.abs_max_v: 3262.0\n",
      "fc layer 2 self.abs_max_out: 1942.0\n",
      "lif layer 2 self.abs_max_v: 3322.0\n",
      "lif layer 2 self.abs_max_v: 3354.0\n",
      "fc layer 1 self.abs_max_out: 1769.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  5.251099/ 42.721813, val:  36.25%, val_best:  52.92%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8258%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.5759%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 28611  14.612%\n",
      "lif layer 1 self.abs_max_v: 3301.5\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  5.658075/ 31.556566, val:  52.08%, val_best:  52.92%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6779%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.3311%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 29944  14.565%\n",
      "fc layer 1 self.abs_max_out: 1828.0\n",
      "fc layer 2 self.abs_max_out: 1988.0\n",
      "lif layer 2 self.abs_max_v: 3454.0\n",
      "fc layer 1 self.abs_max_out: 1903.0\n",
      "fc layer 2 self.abs_max_out: 2087.0\n",
      "lif layer 1 self.abs_max_v: 3311.0\n",
      "lif layer 1 self.abs_max_v: 3473.5\n",
      "fc layer 1 self.abs_max_out: 1924.0\n",
      "lif layer 1 self.abs_max_v: 3510.5\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  5.733565/ 37.089455, val:  37.92%, val_best:  52.92%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.1322%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.5861%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 31328  14.545%\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  5.552333/ 37.247925, val:  47.50%, val_best:  52.92%, tr:  98.67%, tr_best:  99.69%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.2553%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.8287%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 32687  14.517%\n",
      "lif layer 2 self.abs_max_v: 3521.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  5.304154/ 32.633785, val:  48.33%, val_best:  52.92%, tr:  99.08%, tr_best:  99.69%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.4859%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.6396%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 33962  14.454%\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  5.151198/ 30.626015, val:  44.17%, val_best:  52.92%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.2201%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5681%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 35247  14.401%\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  5.571553/ 18.725210, val:  58.75%, val_best:  58.75%, tr:  98.67%, tr_best:  99.69%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.3973%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.3548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 36645  14.397%\n",
      "fc layer 2 self.abs_max_out: 2159.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  5.174230/ 27.142035, val:  58.33%, val_best:  58.75%, tr:  99.08%, tr_best:  99.69%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.1410%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 37970  14.365%\n",
      "fc layer 1 self.abs_max_out: 1946.0\n",
      "fc layer 1 self.abs_max_out: 1962.0\n",
      "lif layer 1 self.abs_max_v: 3572.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  5.598906/ 23.000666, val:  53.33%, val_best:  58.75%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6130%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 39350  14.355%\n",
      "lif layer 2 self.abs_max_v: 3624.0\n",
      "lif layer 2 self.abs_max_v: 3652.5\n",
      "lif layer 2 self.abs_max_v: 3653.5\n",
      "fc layer 1 self.abs_max_out: 1994.0\n",
      "lif layer 1 self.abs_max_v: 3620.0\n",
      "lif layer 1 self.abs_max_v: 3651.5\n",
      "lif layer 1 self.abs_max_v: 3709.0\n",
      "lif layer 1 self.abs_max_v: 3828.5\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  5.059822/ 30.039965, val:  50.83%, val_best:  58.75%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6749%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.3324%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 40584  14.295%\n",
      "lif layer 2 self.abs_max_v: 3683.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  5.219399/ 38.532841, val:  43.33%, val_best:  58.75%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.2535%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 41853  14.250%\n",
      "fc layer 1 self.abs_max_out: 2040.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  5.041950/ 25.269293, val:  54.58%, val_best:  58.75%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0199%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7425%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 43117  14.207%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  5.067595/ 41.263470, val:  36.25%, val_best:  58.75%, tr:  99.08%, tr_best:  99.69%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5396%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0175%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 44347  14.156%\n",
      "fc layer 2 self.abs_max_out: 2198.0\n",
      "lif layer 2 self.abs_max_v: 3769.0\n",
      "fc layer 1 self.abs_max_out: 2047.0\n",
      "lif layer 1 self.abs_max_v: 3863.5\n",
      "lif layer 1 self.abs_max_v: 3876.0\n",
      "lif layer 1 self.abs_max_v: 3972.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  4.830462/ 24.407562, val:  54.58%, val_best:  58.75%, tr:  98.77%, tr_best:  99.69%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7334%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 45551  14.099%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  4.968958/ 26.078554, val:  55.00%, val_best:  58.75%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.9974%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8457%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 46796  14.059%\n",
      "fc layer 2 self.abs_max_out: 2361.0\n",
      "lif layer 2 self.abs_max_v: 3905.5\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  4.824045/ 27.257856, val:  57.08%, val_best:  58.75%, tr:  99.08%, tr_best:  99.69%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8776%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9841%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 48009  14.011%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  4.718132/ 19.278811, val:  60.83%, val_best:  60.83%, tr:  99.08%, tr_best:  99.69%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.1354%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6406%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 49192  13.958%\n",
      "fc layer 1 self.abs_max_out: 2066.0\n",
      "fc layer 1 self.abs_max_out: 2108.0\n",
      "lif layer 1 self.abs_max_v: 3998.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  4.791233/ 23.144854, val:  58.33%, val_best:  60.83%, tr:  98.77%, tr_best:  99.69%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.5068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6812%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 50371  13.906%\n",
      "lif layer 2 self.abs_max_v: 3943.5\n",
      "lif layer 2 self.abs_max_v: 4129.5\n",
      "lif layer 2 self.abs_max_v: 4161.0\n",
      "lif layer 2 self.abs_max_v: 4403.5\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  4.444997/ 24.025633, val:  56.25%, val_best:  60.83%, tr:  99.08%, tr_best:  99.69%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.3565%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 51522  13.849%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  4.765491/ 32.555962, val:  54.17%, val_best:  60.83%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.3671%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3954%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 52776  13.823%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  4.637173/ 30.378050, val:  45.00%, val_best:  60.83%, tr:  99.18%, tr_best:  99.69%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.5729%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4048%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 53972  13.782%\n",
      "fc layer 1 self.abs_max_out: 2173.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  4.495850/ 20.119623, val:  53.33%, val_best:  60.83%, tr:  98.98%, tr_best:  99.69%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.4279%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 55140  13.737%\n",
      "fc layer 2 self.abs_max_out: 2377.0\n",
      "lif layer 1 self.abs_max_v: 4030.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  4.209249/ 24.344786, val:  59.58%, val_best:  60.83%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.9849%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0972%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 56305  13.694%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  4.610304/ 27.934052, val:  45.83%, val_best:  60.83%, tr:  98.98%, tr_best:  99.69%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8791%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6569%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 57506  13.660%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  4.456391/ 22.077314, val:  53.75%, val_best:  60.83%, tr:  98.57%, tr_best:  99.69%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.9692%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9324%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 58700  13.627%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.496557/ 27.820768, val:  44.58%, val_best:  60.83%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.0146%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6682%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 59860  13.588%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.241676/ 24.403511, val:  55.00%, val_best:  60.83%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6931%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5864%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 60975  13.540%\n",
      "fc layer 1 self.abs_max_out: 2232.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.428268/ 25.252539, val:  62.50%, val_best:  62.50%, tr:  98.88%, tr_best:  99.69%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6768%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 62135  13.504%\n",
      "fc layer 2 self.abs_max_out: 2549.0\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.295460/ 33.235737, val:  42.08%, val_best:  62.50%, tr:  99.49%, tr_best:  99.69%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7496%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6169%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 63275  13.465%\n",
      "fc layer 1 self.abs_max_out: 2238.0\n",
      "lif layer 1 self.abs_max_v: 4094.0\n",
      "lif layer 1 self.abs_max_v: 4105.0\n",
      "lif layer 1 self.abs_max_v: 4220.5\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.153320/ 23.712641, val:  52.08%, val_best:  62.50%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0205%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 64396  13.424%\n",
      "fc layer 2 self.abs_max_out: 2943.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.325953/ 27.404728, val:  53.33%, val_best:  62.50%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5021%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9813%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 65554  13.392%\n",
      "fc layer 1 self.abs_max_out: 2291.0\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.521717/ 24.024078, val:  49.58%, val_best:  62.50%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6077%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7953%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 66747  13.368%\n",
      "lif layer 2 self.abs_max_v: 4523.0\n",
      "fc layer 1 self.abs_max_out: 2310.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.522324/ 15.643772, val:  62.08%, val_best:  62.50%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3326%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7919%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 67953  13.348%\n",
      "lif layer 2 self.abs_max_v: 4573.5\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.341576/ 22.039471, val:  56.25%, val_best:  62.50%, tr:  98.98%, tr_best:  99.69%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.2414%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1365%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 69086  13.315%\n",
      "lif layer 2 self.abs_max_v: 4895.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.056158/ 33.099182, val:  40.42%, val_best:  62.50%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5342%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6555%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 70242  13.287%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.206736/ 23.742062, val:  58.33%, val_best:  62.50%, tr:  98.98%, tr_best:  99.69%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3663%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0438%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 71358  13.252%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.320247/ 24.084452, val:  58.75%, val_best:  62.50%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5602%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8185%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 72477  13.220%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.375955/ 31.484970, val:  50.00%, val_best:  62.50%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.1728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 73635  13.196%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.210203/ 24.168549, val:  58.33%, val_best:  62.50%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0928%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2551%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 74755  13.165%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.217820/ 25.808613, val:  49.58%, val_best:  62.50%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.1893%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1515%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 75844  13.131%\n",
      "fc layer 1 self.abs_max_out: 2325.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.347775/ 26.793732, val:  51.67%, val_best:  62.50%, tr:  99.08%, tr_best:  99.69%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8267%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0665%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 77019  13.112%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.245684/ 39.959812, val:  46.67%, val_best:  62.50%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.9062%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7831%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 78110  13.080%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.325847/ 22.760586, val:  57.08%, val_best:  62.50%, tr:  98.98%, tr_best:  99.69%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8549%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 79256  13.057%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.091173/ 23.131357, val:  62.50%, val_best:  62.50%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0938%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 80398  13.035%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.161520/ 21.104082, val:  54.58%, val_best:  62.50%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5128%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 81493  13.006%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.108315/ 22.057110, val:  55.83%, val_best:  62.50%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.2322%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3795%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 82596  12.980%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.980632/ 20.471014, val:  52.08%, val_best:  62.50%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.4034%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7678%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 83662  12.948%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.121161/ 16.863792, val:  48.75%, val_best:  62.50%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.1200%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8591%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 84832  12.933%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  4.081691/ 20.768530, val:  61.67%, val_best:  62.50%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.9936%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 85958  12.912%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.091434/ 19.835119, val:  70.00%, val_best:  70.00%, tr:  99.08%, tr_best:  99.69%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.1984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2768%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 87039  12.885%\n",
      "fc layer 1 self.abs_max_out: 2326.0\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.721838/ 31.383759, val:  45.83%, val_best:  70.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6537%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 88062  12.850%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  4.217838/ 23.680664, val:  52.50%, val_best:  70.00%, tr:  98.98%, tr_best:  99.69%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.4357%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 89192  12.832%\n",
      "fc layer 1 self.abs_max_out: 2398.0\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  4.084489/ 26.791948, val:  54.58%, val_best:  70.00%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.2061%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4406%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 90291  12.809%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.808019/ 19.715481, val:  65.83%, val_best:  70.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 91325  12.779%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.861716/ 34.120720, val:  54.58%, val_best:  70.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5937%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 92349  12.747%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.945343/ 21.316010, val:  52.92%, val_best:  70.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6714%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9426%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 93421  12.723%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.628690/ 24.503021, val:  65.42%, val_best:  70.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6077%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8110%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 94442  12.693%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  4.178617/ 26.035866, val:  52.08%, val_best:  70.00%, tr:  99.08%, tr_best:  99.69%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.4708%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7513%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 95587  12.680%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  4.042687/ 24.934708, val:  56.25%, val_best:  70.00%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7915%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6759%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 96688  12.662%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  4.156848/ 28.001041, val:  55.42%, val_best:  70.00%, tr:  99.08%, tr_best:  99.69%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7849%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5531%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 97763  12.641%\n",
      "fc layer 1 self.abs_max_out: 2428.0\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.912171/ 24.656612, val:  54.58%, val_best:  70.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8248%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6221%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 98797  12.615%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.939842/ 23.940542, val:  54.17%, val_best:  70.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7293%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0245%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 99881  12.595%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.870547/ 19.919703, val:  57.08%, val_best:  70.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8040%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 100919  12.571%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.862312/ 25.359697, val:  50.42%, val_best:  70.00%, tr:  99.08%, tr_best:  99.69%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.4967%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 102007  12.554%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.704833/ 21.642391, val:  57.50%, val_best:  70.00%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3437%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9532%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 103017  12.527%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  4.064692/ 22.190109, val:  60.83%, val_best:  70.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3574%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7774%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 104131  12.513%\n",
      "lif layer 1 self.abs_max_v: 4392.5\n",
      "fc layer 1 self.abs_max_out: 2484.0\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  4.193788/ 19.463400, val:  57.08%, val_best:  70.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6157%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7160%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 105250  12.501%\n",
      "fc layer 1 self.abs_max_out: 2527.0\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  4.104141/ 20.330940, val:  62.92%, val_best:  70.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5518%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6591%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 106370  12.489%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  4.094264/ 20.159655, val:  62.92%, val_best:  70.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6546%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7179%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 107429  12.470%\n",
      "fc layer 1 self.abs_max_out: 2593.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.802996/ 29.745306, val:  49.17%, val_best:  70.00%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.2789%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0092%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 108470  12.449%\n",
      "lif layer 1 self.abs_max_v: 4398.5\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  4.024651/ 18.602736, val:  63.33%, val_best:  70.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6095%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7694%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 109553  12.434%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  4.052766/ 19.415161, val:  57.50%, val_best:  70.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5897%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 110646  12.420%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.603805/ 23.889854, val:  64.58%, val_best:  70.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5420%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0431%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 111663  12.398%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.706060/ 27.950029, val:  42.92%, val_best:  70.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.4495%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0922%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 112660  12.374%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.776318/ 23.094387, val:  56.67%, val_best:  70.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.4802%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 113665  12.351%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  3.576391/ 25.472889, val:  47.92%, val_best:  70.00%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.4598%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1797%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 114669  12.329%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  3.831479/ 19.813467, val:  67.50%, val_best:  70.00%, tr:  99.18%, tr_best:  99.80%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1122%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 115715  12.312%\n",
      "lif layer 1 self.abs_max_v: 4432.0\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  3.477371/ 31.171213, val:  51.67%, val_best:  70.00%, tr:  98.77%, tr_best:  99.80%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.4216%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2422%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 116681  12.287%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  3.669253/ 17.419941, val:  61.67%, val_best:  70.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1737%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 117655  12.263%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  3.536690/ 17.213682, val:  60.83%, val_best:  70.00%, tr:  98.98%, tr_best:  99.80%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3720%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7516%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 118648  12.242%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  3.563706/ 20.750771, val:  54.58%, val_best:  70.00%, tr:  98.88%, tr_best:  99.80%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.1910%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3786%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 119641  12.221%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  3.570558/ 18.615620, val:  64.17%, val_best:  70.00%, tr:  98.88%, tr_best:  99.80%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2112%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 120624  12.199%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  3.755265/ 19.394272, val:  62.50%, val_best:  70.00%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5202%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2597%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 121678  12.185%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  3.836975/ 23.925871, val:  69.58%, val_best:  70.00%, tr:  98.77%, tr_best:  99.80%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5287%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0751%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 122732  12.171%\n",
      "fc layer 1 self.abs_max_out: 2619.0\n",
      "fc layer 1 self.abs_max_out: 2658.0\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  3.653910/ 25.897749, val:  57.50%, val_best:  70.00%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6197%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9651%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 123711  12.150%\n",
      "fc layer 1 self.abs_max_out: 2664.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  3.738414/ 19.151924, val:  60.42%, val_best:  70.00%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.1744%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 124736  12.134%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  3.650342/ 27.274620, val:  52.08%, val_best:  70.00%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3772%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1848%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 125739  12.117%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  3.597283/ 36.602993, val:  38.33%, val_best:  70.00%, tr:  98.88%, tr_best:  99.80%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.2473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2801%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 126733  12.098%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  3.699578/ 31.364698, val:  44.58%, val_best:  70.00%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5777%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9030%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 127727  12.080%\n",
      "lif layer 1 self.abs_max_v: 4434.5\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  3.786400/ 17.811266, val:  68.33%, val_best:  70.00%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.2664%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2024%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 128770  12.067%\n",
      "fc layer 1 self.abs_max_out: 2702.0\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  3.593334/ 18.301630, val:  68.75%, val_best:  70.00%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5278%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 129750  12.048%\n",
      "fc layer 1 self.abs_max_out: 2705.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  3.763848/ 19.450670, val:  57.50%, val_best:  70.00%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 130759  12.033%\n",
      "lif layer 1 self.abs_max_v: 4572.5\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  3.521160/ 21.783754, val:  63.75%, val_best:  70.00%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5937%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 131761  12.017%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  3.677063/ 11.990526, val:  74.17%, val_best:  74.17%, tr:  99.08%, tr_best:  99.80%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.1717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6753%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 132832  12.007%\n",
      "lif layer 1 self.abs_max_v: 4658.5\n",
      "lif layer 1 self.abs_max_v: 4736.5\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  3.823429/ 32.483788, val:  63.33%, val_best:  74.17%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.1203%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8157%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 133856  11.994%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  3.916251/ 27.598431, val:  49.58%, val_best:  74.17%, tr:  98.98%, tr_best:  99.80%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6376%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4176%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 134881  11.980%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  3.772128/ 17.705008, val:  62.50%, val_best:  74.17%, tr:  98.98%, tr_best:  99.80%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2379%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 135895  11.966%\n",
      "fc layer 1 self.abs_max_out: 2713.0\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  3.483943/ 22.557396, val:  67.08%, val_best:  74.17%, tr:  99.28%, tr_best:  99.80%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3381%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 136906  11.952%\n",
      "lif layer 1 self.abs_max_v: 4749.0\n",
      "lif layer 1 self.abs_max_v: 4777.5\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  3.925327/ 24.540276, val:  51.67%, val_best:  74.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3925%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 137969  11.943%\n",
      "fc layer 1 self.abs_max_out: 2737.0\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  3.488303/ 25.067013, val:  54.58%, val_best:  74.17%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3328%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0859%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 138930  11.925%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  3.680159/ 17.084047, val:  72.92%, val_best:  74.17%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3646%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2345%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 139940  11.912%\n",
      "lif layer 1 self.abs_max_v: 5009.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  3.553773/ 22.786531, val:  55.00%, val_best:  74.17%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3470%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1174%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 140900  11.894%\n",
      "fc layer 1 self.abs_max_out: 2762.0\n",
      "fc layer 1 self.abs_max_out: 2770.0\n",
      "lif layer 1 self.abs_max_v: 5125.5\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  3.856227/ 22.964430, val:  58.75%, val_best:  74.17%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6062%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1890%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 141920  11.882%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  3.720736/ 14.099753, val:  75.83%, val_best:  75.83%, tr:  99.08%, tr_best:  99.80%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6467%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3946%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 142910  11.868%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  3.624214/ 25.953230, val:  58.33%, val_best:  75.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3384%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6966%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 143940  11.857%\n",
      "fc layer 1 self.abs_max_out: 2860.0\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  3.663302/ 22.420372, val:  56.67%, val_best:  75.83%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 144963  11.846%\n",
      "fc layer 1 self.abs_max_out: 2889.0\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  3.834961/ 16.747314, val:  68.75%, val_best:  75.83%, tr:  98.77%, tr_best:  99.80%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7054%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3047%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 146005  11.836%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  3.538374/ 25.402838, val:  68.33%, val_best:  75.83%, tr:  98.88%, tr_best:  99.80%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5526%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7980%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 146950  11.819%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  3.674409/ 18.875458, val:  53.33%, val_best:  75.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.4851%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1801%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 147928  11.805%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  3.646613/ 18.924713, val:  67.08%, val_best:  75.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.4794%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2617%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 148913  11.791%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  3.426788/ 21.743670, val:  55.83%, val_best:  75.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5251%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2925%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 149856  11.775%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  3.564284/ 27.194918, val:  55.83%, val_best:  75.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.4883%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9688%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 150821  11.760%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  3.798321/ 19.894371, val:  74.17%, val_best:  75.83%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6331%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9071%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 151838  11.750%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  3.208536/ 25.338026, val:  50.42%, val_best:  75.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6932%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2749%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 152702  11.728%\n",
      "fc layer 2 self.abs_max_out: 3164.0\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  3.499371/ 21.483120, val:  55.83%, val_best:  75.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6603%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 153612  11.709%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  3.536160/ 23.884256, val:  62.08%, val_best:  75.83%, tr:  98.88%, tr_best:  99.80%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8943%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9610%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 154533  11.692%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  3.578588/ 31.238277, val:  57.92%, val_best:  75.83%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8184%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9005%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 155473  11.677%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  3.255939/ 19.044924, val:  68.33%, val_best:  75.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6726%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1831%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 156381  11.660%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  3.582078/ 23.206839, val:  59.58%, val_best:  75.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7220%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 157347  11.647%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  3.313905/ 18.360609, val:  65.00%, val_best:  75.83%, tr:  99.28%, tr_best:  99.80%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7742%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4869%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 158277  11.631%\n",
      "lif layer 1 self.abs_max_v: 5141.0\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  3.488511/ 19.423744, val:  66.67%, val_best:  75.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7531%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8330%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 159235  11.618%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  3.341800/ 22.307278, val:  54.58%, val_best:  75.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5521%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6939%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 160183  11.604%\n",
      "fc layer 1 self.abs_max_out: 2960.0\n",
      "lif layer 1 self.abs_max_v: 5412.5\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  3.320990/ 19.287954, val:  63.75%, val_best:  75.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7662%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4371%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 161095  11.588%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  3.473665/ 19.806623, val:  65.00%, val_best:  75.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8498%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4424%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 162026  11.574%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  3.385314/ 20.893373, val:  55.42%, val_best:  75.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1774%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 162949  11.559%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  3.660006/ 19.032608, val:  68.75%, val_best:  75.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7087%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1904%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 163904  11.546%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  3.331854/ 25.728779, val:  46.67%, val_best:  75.83%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8260%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3488%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 164810  11.530%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  3.590952/ 18.053305, val:  61.25%, val_best:  75.83%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 165773  11.519%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  3.216195/ 21.977879, val:  62.08%, val_best:  75.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.9577%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3178%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 166664  11.503%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  3.727723/ 20.021057, val:  70.00%, val_best:  75.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7265%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3955%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 167672  11.495%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  3.465855/ 19.974852, val:  71.25%, val_best:  75.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5520%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5973%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 168595  11.481%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  3.434088/ 14.741570, val:  67.92%, val_best:  75.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3040%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3925%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 169524  11.468%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  3.317725/ 21.913063, val:  60.83%, val_best:  75.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5559%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5016%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 170462  11.455%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  3.512473/ 20.714027, val:  76.25%, val_best:  76.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5603%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9886%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 171385  11.442%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  3.738940/ 25.397341, val:  60.83%, val_best:  76.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7115%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4740%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 172383  11.434%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  3.284844/ 21.128895, val:  58.75%, val_best:  76.25%, tr:  99.08%, tr_best:  99.80%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7372%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8591%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 173280  11.419%\n",
      "fc layer 1 self.abs_max_out: 3018.0\n",
      "fc layer 1 self.abs_max_out: 3049.0\n",
      "lif layer 1 self.abs_max_v: 5704.5\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  3.256914/ 22.693260, val:  60.42%, val_best:  76.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8072%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5446%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 174173  11.404%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  3.431955/ 18.852610, val:  62.92%, val_best:  76.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8776%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4516%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 175096  11.392%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  3.259101/ 18.921461, val:  67.92%, val_best:  76.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5591%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5302%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 175977  11.377%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  3.566061/ 14.765196, val:  66.25%, val_best:  76.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5368%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6315%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 176914  11.365%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  3.574772/ 18.732040, val:  67.08%, val_best:  76.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7356%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2777%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 177868  11.355%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  3.408812/ 23.721628, val:  62.92%, val_best:  76.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7096%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2780%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 178771  11.342%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  3.451714/ 17.378197, val:  65.00%, val_best:  76.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5879%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5603%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 179675  11.329%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  3.243239/ 20.679245, val:  62.08%, val_best:  76.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7275%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 180566  11.315%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  3.433946/ 23.122906, val:  62.08%, val_best:  76.25%, tr:  99.08%, tr_best:  99.80%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7588%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3809%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 181491  11.304%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  3.298526/ 18.319548, val:  72.08%, val_best:  76.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7851%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7853%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 182393  11.291%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  3.290965/ 24.509689, val:  61.67%, val_best:  76.25%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7598%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6151%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 183303  11.279%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  3.314742/ 16.792189, val:  67.92%, val_best:  76.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7682%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0443%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 184188  11.266%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  3.497045/ 21.809891, val:  59.17%, val_best:  76.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6165%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6867%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 185147  11.257%\n",
      "fc layer 1 self.abs_max_out: 3065.0\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  3.296728/ 20.877884, val:  68.33%, val_best:  76.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6446%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4797%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 186031  11.244%\n",
      "fc layer 1 self.abs_max_out: 3069.0\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  3.241096/ 17.025341, val:  70.83%, val_best:  76.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7570%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5678%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 186910  11.231%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  3.180084/ 16.348324, val:  71.25%, val_best:  76.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7043%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0665%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 187814  11.219%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  3.040860/ 15.714834, val:  67.92%, val_best:  76.25%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7843%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9621%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 188676  11.205%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  3.112158/ 20.136318, val:  65.83%, val_best:  76.25%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8400%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9800%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 189529  11.190%\n",
      "fc layer 1 self.abs_max_out: 3139.0\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  3.042495/ 15.596889, val:  65.83%, val_best:  76.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1381%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 190373  11.176%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  3.306197/ 21.909502, val:  61.67%, val_best:  76.25%, tr:  98.88%, tr_best:  99.80%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7708%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0177%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 191276  11.165%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  3.294119/ 21.478554, val:  61.25%, val_best:  76.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7718%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6976%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 192150  11.152%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  3.109652/ 20.040031, val:  70.83%, val_best:  76.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6349%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0527%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 193010  11.138%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  3.257934/ 12.522067, val:  63.75%, val_best:  76.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 193932  11.129%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  3.002490/ 19.961082, val:  60.42%, val_best:  76.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8061%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1012%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 194773  11.115%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  3.361592/ 16.714308, val:  69.17%, val_best:  76.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6491%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 195714  11.106%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  3.158959/ 16.093904, val:  74.17%, val_best:  76.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5748%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8067%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 196603  11.095%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  3.094169/ 20.467293, val:  61.25%, val_best:  76.25%, tr:  98.77%, tr_best:  99.80%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.9542%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8627%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 197443  11.081%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  3.102139/ 16.435446, val:  66.67%, val_best:  76.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9800%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 198292  11.068%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  2.877619/ 19.372749, val:  60.42%, val_best:  76.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.9223%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3133%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 199110  11.053%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  2.983053/ 17.389870, val:  61.25%, val_best:  76.25%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.9289%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1714%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 199931  11.039%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  3.283525/ 24.484318, val:  65.42%, val_best:  76.25%, tr:  98.88%, tr_best:  99.80%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7632%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8868%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 200815  11.028%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  3.137104/ 19.289511, val:  73.33%, val_best:  76.25%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7104%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 201681  11.016%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  3.084556/ 20.050062, val:  68.75%, val_best:  76.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7941%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7566%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 202523  11.004%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  2.906694/ 20.468559, val:  69.58%, val_best:  76.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7822%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9930%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 203333  10.989%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  3.222153/ 16.581602, val:  70.83%, val_best:  76.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5689%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9438%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 204227  10.979%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  3.290277/ 19.435566, val:  74.58%, val_best:  76.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7602%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4400%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 205081  10.968%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  3.374184/ 29.687305, val:  46.67%, val_best:  76.25%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6648%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8050%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 205961  10.957%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  3.038107/ 18.197090, val:  67.92%, val_best:  76.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5969%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2440%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 206774  10.943%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  2.890821/ 27.277796, val:  49.17%, val_best:  76.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6277%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2122%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 207595  10.930%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  2.916516/ 20.251902, val:  63.33%, val_best:  76.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5323%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 208406  10.917%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  3.092528/ 17.057398, val:  72.92%, val_best:  76.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6571%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8168%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 209239  10.904%\n",
      "fc layer 1 self.abs_max_out: 3158.0\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  3.181074/ 15.384977, val:  76.67%, val_best:  76.67%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.6954%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 210099  10.894%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  2.988119/ 18.834589, val:  71.67%, val_best:  76.67%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2414%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 210917  10.881%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  2.920097/ 21.761131, val:  56.25%, val_best:  76.67%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8554%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1230%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 211726  10.868%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  2.975312/ 23.098057, val:  67.92%, val_best:  76.67%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8053%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a324810e4344608446c39a121d6e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñÑ‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÑ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñá‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñÖ‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÑ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñÑ‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99387</td></tr><tr><td>tr_epoch_loss</td><td>2.97531</td></tr><tr><td>val_acc_best</td><td>0.76667</td></tr><tr><td>val_acc_now</td><td>0.67917</td></tr><tr><td>val_loss</td><td>23.09806</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-sweep-22</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qbe4cp57' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qbe4cp57</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_143153-qbe4cp57/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: catwm3vd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_185127-catwm3vd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/catwm3vd' target=\"_blank\">desert-sweep-28</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/catwm3vd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/catwm3vd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251214_185135_894', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 32, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 1, 'lif_layer_v_threshold2': 64, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 32, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 1, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 1 self.abs_max_out: 55.0\n",
      "lif layer 1 self.abs_max_v: 75.0\n",
      "lif layer 1 self.abs_max_v: 91.5\n",
      "fc layer 1 self.abs_max_out: 63.0\n",
      "lif layer 1 self.abs_max_v: 101.5\n",
      "fc layer 1 self.abs_max_out: 73.0\n",
      "lif layer 1 self.abs_max_v: 111.5\n",
      "fc layer 1 self.abs_max_out: 114.0\n",
      "lif layer 1 self.abs_max_v: 114.0\n",
      "lif layer 1 self.abs_max_v: 146.0\n",
      "fc layer 2 self.abs_max_out: 11.0\n",
      "lif layer 2 self.abs_max_v: 11.0\n",
      "fc layer 1 self.abs_max_out: 159.0\n",
      "lif layer 1 self.abs_max_v: 159.0\n",
      "lif layer 2 self.abs_max_v: 11.5\n",
      "lif layer 1 self.abs_max_v: 160.5\n",
      "fc layer 2 self.abs_max_out: 13.0\n",
      "lif layer 2 self.abs_max_v: 19.0\n",
      "fc layer 1 self.abs_max_out: 226.0\n",
      "lif layer 1 self.abs_max_v: 267.5\n",
      "fc layer 2 self.abs_max_out: 16.0\n",
      "lif layer 2 self.abs_max_v: 25.0\n",
      "fc layer 1 self.abs_max_out: 419.0\n",
      "lif layer 1 self.abs_max_v: 419.0\n",
      "fc layer 2 self.abs_max_out: 17.0\n",
      "lif layer 2 self.abs_max_v: 29.5\n",
      "fc layer 1 self.abs_max_out: 549.0\n",
      "lif layer 1 self.abs_max_v: 549.0\n",
      "fc layer 2 self.abs_max_out: 23.0\n",
      "lif layer 2 self.abs_max_v: 38.0\n",
      "lif layer 2 self.abs_max_v: 40.0\n",
      "fc layer 2 self.abs_max_out: 25.0\n",
      "fc layer 2 self.abs_max_out: 27.0\n",
      "lif layer 2 self.abs_max_v: 45.5\n",
      "fc layer 2 self.abs_max_out: 29.0\n",
      "lif layer 2 self.abs_max_v: 52.0\n",
      "fc layer 2 self.abs_max_out: 31.0\n",
      "lif layer 2 self.abs_max_v: 57.0\n",
      "fc layer 2 self.abs_max_out: 33.0\n",
      "lif layer 2 self.abs_max_v: 61.5\n",
      "fc layer 2 self.abs_max_out: 38.0\n",
      "lif layer 2 self.abs_max_v: 69.0\n",
      "fc layer 2 self.abs_max_out: 40.0\n",
      "lif layer 2 self.abs_max_v: 74.5\n",
      "fc layer 3 self.abs_max_out: 2.0\n",
      "fc layer 2 self.abs_max_out: 43.0\n",
      "lif layer 2 self.abs_max_v: 80.5\n",
      "fc layer 3 self.abs_max_out: 6.0\n",
      "fc layer 2 self.abs_max_out: 46.0\n",
      "lif layer 2 self.abs_max_v: 86.5\n",
      "fc layer 2 self.abs_max_out: 48.0\n",
      "fc layer 3 self.abs_max_out: 7.0\n",
      "fc layer 2 self.abs_max_out: 49.0\n",
      "fc layer 3 self.abs_max_out: 12.0\n",
      "lif layer 2 self.abs_max_v: 91.0\n",
      "lif layer 2 self.abs_max_v: 94.5\n",
      "lif layer 2 self.abs_max_v: 96.5\n",
      "fc layer 2 self.abs_max_out: 56.0\n",
      "lif layer 2 self.abs_max_v: 98.5\n",
      "lif layer 2 self.abs_max_v: 105.5\n",
      "fc layer 2 self.abs_max_out: 57.0\n",
      "fc layer 2 self.abs_max_out: 60.0\n",
      "fc layer 3 self.abs_max_out: 13.0\n",
      "fc layer 2 self.abs_max_out: 61.0\n",
      "lif layer 2 self.abs_max_v: 111.0\n",
      "fc layer 2 self.abs_max_out: 63.0\n",
      "fc layer 2 self.abs_max_out: 65.0\n",
      "fc layer 2 self.abs_max_out: 69.0\n",
      "lif layer 2 self.abs_max_v: 117.5\n",
      "fc layer 2 self.abs_max_out: 71.0\n",
      "lif layer 2 self.abs_max_v: 130.0\n",
      "fc layer 2 self.abs_max_out: 83.0\n",
      "lif layer 2 self.abs_max_v: 144.0\n",
      "fc layer 3 self.abs_max_out: 20.0\n",
      "lif layer 2 self.abs_max_v: 151.0\n",
      "fc layer 2 self.abs_max_out: 91.0\n",
      "fc layer 3 self.abs_max_out: 22.0\n",
      "fc layer 3 self.abs_max_out: 25.0\n",
      "fc layer 2 self.abs_max_out: 102.0\n",
      "lif layer 2 self.abs_max_v: 176.5\n",
      "fc layer 3 self.abs_max_out: 30.0\n",
      "lif layer 2 self.abs_max_v: 190.5\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "lif layer 2 self.abs_max_v: 195.5\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "fc layer 2 self.abs_max_out: 103.0\n",
      "fc layer 3 self.abs_max_out: 40.0\n",
      "fc layer 2 self.abs_max_out: 129.0\n",
      "lif layer 2 self.abs_max_v: 196.0\n",
      "lif layer 2 self.abs_max_v: 217.0\n",
      "fc layer 3 self.abs_max_out: 42.0\n",
      "fc layer 3 self.abs_max_out: 48.0\n",
      "lif layer 2 self.abs_max_v: 234.0\n",
      "lif layer 2 self.abs_max_v: 242.0\n",
      "fc layer 2 self.abs_max_out: 131.0\n",
      "fc layer 2 self.abs_max_out: 142.0\n",
      "lif layer 2 self.abs_max_v: 242.5\n",
      "lif layer 2 self.abs_max_v: 245.5\n",
      "fc layer 2 self.abs_max_out: 146.0\n",
      "fc layer 1 self.abs_max_out: 573.0\n",
      "lif layer 1 self.abs_max_v: 573.0\n",
      "fc layer 1 self.abs_max_out: 620.0\n",
      "lif layer 1 self.abs_max_v: 620.0\n",
      "lif layer 2 self.abs_max_v: 248.5\n",
      "fc layer 1 self.abs_max_out: 687.0\n",
      "lif layer 1 self.abs_max_v: 687.0\n",
      "fc layer 2 self.abs_max_out: 151.0\n",
      "lif layer 2 self.abs_max_v: 275.5\n",
      "fc layer 3 self.abs_max_out: 65.0\n",
      "fc layer 1 self.abs_max_out: 752.0\n",
      "lif layer 1 self.abs_max_v: 752.0\n",
      "lif layer 2 self.abs_max_v: 289.0\n",
      "lif layer 2 self.abs_max_v: 290.5\n",
      "lif layer 2 self.abs_max_v: 296.5\n",
      "lif layer 2 self.abs_max_v: 299.5\n",
      "fc layer 2 self.abs_max_out: 153.0\n",
      "fc layer 2 self.abs_max_out: 158.0\n",
      "fc layer 3 self.abs_max_out: 73.0\n",
      "fc layer 2 self.abs_max_out: 161.0\n",
      "fc layer 2 self.abs_max_out: 163.0\n",
      "fc layer 2 self.abs_max_out: 165.0\n",
      "fc layer 3 self.abs_max_out: 74.0\n",
      "fc layer 2 self.abs_max_out: 168.0\n",
      "lif layer 2 self.abs_max_v: 300.0\n",
      "lif layer 2 self.abs_max_v: 314.0\n",
      "fc layer 2 self.abs_max_out: 172.0\n",
      "fc layer 2 self.abs_max_out: 177.0\n",
      "lif layer 2 self.abs_max_v: 315.5\n",
      "fc layer 2 self.abs_max_out: 217.0\n",
      "lif layer 2 self.abs_max_v: 345.5\n",
      "lif layer 2 self.abs_max_v: 364.0\n",
      "lif layer 2 self.abs_max_v: 385.0\n",
      "lif layer 2 self.abs_max_v: 402.5\n",
      "lif layer 2 self.abs_max_v: 403.5\n",
      "lif layer 2 self.abs_max_v: 410.5\n",
      "lif layer 2 self.abs_max_v: 413.5\n",
      "fc layer 2 self.abs_max_out: 230.0\n",
      "fc layer 3 self.abs_max_out: 78.0\n",
      "fc layer 2 self.abs_max_out: 232.0\n",
      "fc layer 2 self.abs_max_out: 236.0\n",
      "fc layer 3 self.abs_max_out: 86.0\n",
      "lif layer 2 self.abs_max_v: 417.5\n",
      "fc layer 3 self.abs_max_out: 90.0\n",
      "fc layer 2 self.abs_max_out: 243.0\n",
      "lif layer 2 self.abs_max_v: 445.0\n",
      "fc layer 2 self.abs_max_out: 276.0\n",
      "lif layer 2 self.abs_max_v: 461.5\n",
      "fc layer 2 self.abs_max_out: 286.0\n",
      "lif layer 2 self.abs_max_v: 482.0\n",
      "fc layer 3 self.abs_max_out: 97.0\n",
      "fc layer 3 self.abs_max_out: 101.0\n",
      "fc layer 3 self.abs_max_out: 106.0\n",
      "lif layer 2 self.abs_max_v: 483.0\n",
      "lif layer 2 self.abs_max_v: 495.5\n",
      "fc layer 2 self.abs_max_out: 288.0\n",
      "fc layer 2 self.abs_max_out: 297.0\n",
      "fc layer 2 self.abs_max_out: 329.0\n",
      "lif layer 2 self.abs_max_v: 547.0\n",
      "lif layer 2 self.abs_max_v: 585.5\n",
      "fc layer 3 self.abs_max_out: 110.0\n",
      "fc layer 3 self.abs_max_out: 125.0\n",
      "fc layer 3 self.abs_max_out: 149.0\n",
      "lif layer 2 self.abs_max_v: 597.0\n",
      "fc layer 3 self.abs_max_out: 151.0\n",
      "fc layer 2 self.abs_max_out: 360.0\n",
      "lif layer 2 self.abs_max_v: 608.5\n",
      "lif layer 2 self.abs_max_v: 636.5\n",
      "fc layer 1 self.abs_max_out: 782.0\n",
      "lif layer 1 self.abs_max_v: 782.0\n",
      "lif layer 2 self.abs_max_v: 658.0\n",
      "lif layer 2 self.abs_max_v: 689.0\n",
      "fc layer 3 self.abs_max_out: 158.0\n",
      "fc layer 2 self.abs_max_out: 370.0\n",
      "fc layer 1 self.abs_max_out: 791.0\n",
      "lif layer 1 self.abs_max_v: 791.0\n",
      "fc layer 1 self.abs_max_out: 836.0\n",
      "lif layer 1 self.abs_max_v: 836.0\n",
      "fc layer 3 self.abs_max_out: 168.0\n",
      "fc layer 1 self.abs_max_out: 880.0\n",
      "lif layer 1 self.abs_max_v: 880.0\n",
      "fc layer 2 self.abs_max_out: 400.0\n",
      "fc layer 2 self.abs_max_out: 409.0\n",
      "fc layer 2 self.abs_max_out: 435.0\n",
      "lif layer 2 self.abs_max_v: 717.5\n",
      "lif layer 2 self.abs_max_v: 732.5\n",
      "lif layer 2 self.abs_max_v: 789.0\n",
      "lif layer 2 self.abs_max_v: 826.5\n",
      "fc layer 3 self.abs_max_out: 206.0\n",
      "fc layer 2 self.abs_max_out: 436.0\n",
      "lif layer 2 self.abs_max_v: 849.5\n",
      "fc layer 1 self.abs_max_out: 891.0\n",
      "lif layer 1 self.abs_max_v: 891.0\n",
      "fc layer 3 self.abs_max_out: 207.0\n",
      "fc layer 1 self.abs_max_out: 995.0\n",
      "lif layer 1 self.abs_max_v: 995.0\n",
      "fc layer 1 self.abs_max_out: 1033.0\n",
      "lif layer 1 self.abs_max_v: 1033.0\n",
      "fc layer 2 self.abs_max_out: 439.0\n",
      "fc layer 1 self.abs_max_out: 1040.0\n",
      "lif layer 1 self.abs_max_v: 1040.0\n",
      "fc layer 1 self.abs_max_out: 1082.0\n",
      "lif layer 1 self.abs_max_v: 1082.0\n",
      "fc layer 3 self.abs_max_out: 212.0\n",
      "fc layer 1 self.abs_max_out: 1105.0\n",
      "lif layer 1 self.abs_max_v: 1105.0\n",
      "fc layer 1 self.abs_max_out: 1118.0\n",
      "lif layer 1 self.abs_max_v: 1118.0\n",
      "fc layer 1 self.abs_max_out: 1171.0\n",
      "lif layer 1 self.abs_max_v: 1171.0\n",
      "fc layer 1 self.abs_max_out: 1185.0\n",
      "lif layer 1 self.abs_max_v: 1185.0\n",
      "fc layer 1 self.abs_max_out: 1272.0\n",
      "lif layer 1 self.abs_max_v: 1272.0\n",
      "fc layer 1 self.abs_max_out: 1347.0\n",
      "lif layer 1 self.abs_max_v: 1347.0\n",
      "fc layer 2 self.abs_max_out: 475.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  7.556333/ 48.344868, val:  28.33%, val_best:  28.33%, tr:  94.79%, tr_best:  94.79%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.1001%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.5484%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2597  26.527%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 244.0\n",
      "fc layer 3 self.abs_max_out: 245.0\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "lif layer 1 self.abs_max_v: 1370.5\n",
      "fc layer 2 self.abs_max_out: 476.0\n",
      "fc layer 2 self.abs_max_out: 477.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  8.274406/ 63.858463, val:  33.33%, val_best:  33.33%, tr:  98.88%, tr_best:  98.88%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.9715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.5411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 4536  23.166%\n",
      "fc layer 2 self.abs_max_out: 542.0\n",
      "lif layer 2 self.abs_max_v: 888.5\n",
      "lif layer 2 self.abs_max_v: 936.5\n",
      "fc layer 3 self.abs_max_out: 298.0\n",
      "fc layer 2 self.abs_max_out: 610.0\n",
      "lif layer 2 self.abs_max_v: 951.0\n",
      "lif layer 2 self.abs_max_v: 996.5\n",
      "fc layer 1 self.abs_max_out: 1350.0\n",
      "lif layer 1 self.abs_max_v: 1378.5\n",
      "lif layer 1 self.abs_max_v: 1469.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  8.529887/ 39.488224, val:  38.33%, val_best:  38.33%, tr:  98.98%, tr_best:  98.98%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.0718%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2393%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 6312  21.491%\n",
      "fc layer 3 self.abs_max_out: 302.0\n",
      "fc layer 3 self.abs_max_out: 321.0\n",
      "lif layer 2 self.abs_max_v: 999.5\n",
      "lif layer 2 self.abs_max_v: 1031.5\n",
      "fc layer 2 self.abs_max_out: 665.0\n",
      "lif layer 2 self.abs_max_v: 1074.5\n",
      "lif layer 2 self.abs_max_v: 1085.5\n",
      "lif layer 2 self.abs_max_v: 1131.0\n",
      "fc layer 2 self.abs_max_out: 675.0\n",
      "fc layer 2 self.abs_max_out: 727.0\n",
      "fc layer 2 self.abs_max_out: 752.0\n",
      "lif layer 2 self.abs_max_v: 1147.0\n",
      "lif layer 2 self.abs_max_v: 1259.0\n",
      "fc layer 2 self.abs_max_out: 784.0\n",
      "lif layer 1 self.abs_max_v: 1472.0\n",
      "lif layer 1 self.abs_max_v: 1488.0\n",
      "lif layer 1 self.abs_max_v: 1660.0\n",
      "lif layer 1 self.abs_max_v: 1822.5\n",
      "lif layer 1 self.abs_max_v: 1852.5\n",
      "lif layer 1 self.abs_max_v: 1915.5\n",
      "fc layer 2 self.abs_max_out: 785.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.018795/ 73.956345, val:  27.50%, val_best:  38.33%, tr:  99.08%, tr_best:  99.08%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.7021%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1843%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 7959  20.324%\n",
      "fc layer 3 self.abs_max_out: 332.0\n",
      "fc layer 2 self.abs_max_out: 835.0\n",
      "lif layer 2 self.abs_max_v: 1384.5\n",
      "fc layer 3 self.abs_max_out: 335.0\n",
      "fc layer 1 self.abs_max_out: 1352.0\n",
      "fc layer 2 self.abs_max_out: 874.0\n",
      "lif layer 2 self.abs_max_v: 1421.5\n",
      "lif layer 2 self.abs_max_v: 1472.0\n",
      "fc layer 2 self.abs_max_out: 890.0\n",
      "lif layer 2 self.abs_max_v: 1473.0\n",
      "fc layer 2 self.abs_max_out: 897.0\n",
      "fc layer 2 self.abs_max_out: 912.0\n",
      "fc layer 2 self.abs_max_out: 927.0\n",
      "fc layer 2 self.abs_max_out: 929.0\n",
      "lif layer 2 self.abs_max_v: 1474.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  9.549444/ 63.306229, val:  35.00%, val_best:  38.33%, tr:  98.67%, tr_best:  99.08%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.8303%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5903%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 9609  19.630%\n",
      "lif layer 2 self.abs_max_v: 1567.5\n",
      "fc layer 2 self.abs_max_out: 981.0\n",
      "lif layer 2 self.abs_max_v: 1612.0\n",
      "lif layer 2 self.abs_max_v: 1653.0\n",
      "fc layer 1 self.abs_max_out: 1356.0\n",
      "fc layer 1 self.abs_max_out: 1375.0\n",
      "fc layer 1 self.abs_max_out: 1377.0\n",
      "fc layer 2 self.abs_max_out: 1092.0\n",
      "lif layer 2 self.abs_max_v: 1806.0\n",
      "fc layer 2 self.abs_max_out: 1124.0\n",
      "lif layer 2 self.abs_max_v: 2027.0\n",
      "lif layer 1 self.abs_max_v: 2006.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  8.661489/ 55.926220, val:  40.83%, val_best:  40.83%, tr:  98.37%, tr_best:  99.08%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.8471%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3517%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 11093  18.885%\n",
      "fc layer 3 self.abs_max_out: 349.0\n",
      "fc layer 3 self.abs_max_out: 360.0\n",
      "fc layer 2 self.abs_max_out: 1127.0\n",
      "fc layer 2 self.abs_max_out: 1165.0\n",
      "lif layer 2 self.abs_max_v: 2125.0\n",
      "fc layer 2 self.abs_max_out: 1262.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  9.385386/ 63.059143, val:  37.92%, val_best:  40.83%, tr:  98.77%, tr_best:  99.08%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.7000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5230%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 12626  18.424%\n",
      "fc layer 1 self.abs_max_out: 1402.0\n",
      "lif layer 2 self.abs_max_v: 2263.0\n",
      "fc layer 2 self.abs_max_out: 1312.0\n",
      "lif layer 1 self.abs_max_v: 2057.0\n",
      "fc layer 2 self.abs_max_out: 1482.0\n",
      "fc layer 1 self.abs_max_out: 1433.0\n",
      "lif layer 2 self.abs_max_v: 2305.5\n",
      "lif layer 2 self.abs_max_v: 2374.0\n",
      "lif layer 2 self.abs_max_v: 2623.0\n",
      "lif layer 2 self.abs_max_v: 2764.5\n",
      "fc layer 2 self.abs_max_out: 1580.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.669470/ 51.883259, val:  36.67%, val_best:  40.83%, tr:  98.77%, tr_best:  99.08%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.9689%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6388%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 14030  17.914%\n",
      "fc layer 3 self.abs_max_out: 399.0\n",
      "fc layer 3 self.abs_max_out: 416.0\n",
      "lif layer 2 self.abs_max_v: 2794.0\n",
      "lif layer 1 self.abs_max_v: 2086.0\n",
      "fc layer 2 self.abs_max_out: 1588.0\n",
      "lif layer 1 self.abs_max_v: 2107.5\n",
      "lif layer 2 self.abs_max_v: 2801.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  9.040866/ 42.246349, val:  42.92%, val_best:  42.92%, tr:  97.75%, tr_best:  99.08%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.9174%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 15434  17.517%\n",
      "lif layer 1 self.abs_max_v: 2225.0\n",
      "lif layer 1 self.abs_max_v: 2241.5\n",
      "fc layer 2 self.abs_max_out: 1640.0\n",
      "fc layer 3 self.abs_max_out: 419.0\n",
      "fc layer 2 self.abs_max_out: 1673.0\n",
      "fc layer 2 self.abs_max_out: 1721.0\n",
      "lif layer 1 self.abs_max_v: 2414.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.310943/ 56.075367, val:  42.50%, val_best:  42.92%, tr:  98.26%, tr_best:  99.08%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.0062%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1624%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 16806  17.166%\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "lif layer 2 self.abs_max_v: 2827.5\n",
      "fc layer 2 self.abs_max_out: 1727.0\n",
      "lif layer 2 self.abs_max_v: 3056.5\n",
      "fc layer 1 self.abs_max_out: 1500.0\n",
      "fc layer 2 self.abs_max_out: 1744.0\n",
      "lif layer 2 self.abs_max_v: 3064.5\n",
      "lif layer 2 self.abs_max_v: 3175.5\n",
      "fc layer 2 self.abs_max_out: 1789.0\n",
      "lif layer 2 self.abs_max_v: 3223.5\n",
      "fc layer 2 self.abs_max_out: 1826.0\n",
      "fc layer 1 self.abs_max_out: 1537.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.404334/ 44.464382, val:  36.25%, val_best:  42.92%, tr:  96.63%, tr_best:  99.08%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.6545%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3132%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 18162  16.865%\n",
      "fc layer 2 self.abs_max_out: 1860.0\n",
      "lif layer 1 self.abs_max_v: 2431.5\n",
      "fc layer 2 self.abs_max_out: 1916.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  8.355387/ 58.428078, val:  33.33%, val_best:  42.92%, tr:  98.26%, tr_best:  99.08%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.6770%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0180%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 19474  16.576%\n",
      "lif layer 2 self.abs_max_v: 3325.5\n",
      "fc layer 2 self.abs_max_out: 1931.0\n",
      "lif layer 1 self.abs_max_v: 2533.5\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.788398/ 50.219440, val:  32.50%, val_best:  42.92%, tr:  99.08%, tr_best:  99.08%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.6438%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8998%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 20757  16.309%\n",
      "fc layer 2 self.abs_max_out: 2201.0\n",
      "lif layer 2 self.abs_max_v: 3557.5\n",
      "lif layer 2 self.abs_max_v: 3596.0\n",
      "lif layer 2 self.abs_max_v: 3765.0\n",
      "lif layer 2 self.abs_max_v: 3852.5\n",
      "fc layer 1 self.abs_max_out: 1567.0\n",
      "fc layer 1 self.abs_max_out: 1618.0\n",
      "lif layer 1 self.abs_max_v: 2730.0\n",
      "fc layer 1 self.abs_max_out: 1651.0\n",
      "lif layer 1 self.abs_max_v: 3016.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  8.473237/ 61.330513, val:  28.75%, val_best:  42.92%, tr:  98.37%, tr_best:  99.08%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.7760%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1169%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 22108  16.130%\n",
      "fc layer 3 self.abs_max_out: 442.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  8.255931/ 66.952560, val:  29.17%, val_best:  42.92%, tr:  98.67%, tr_best:  99.08%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3683%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 23414  15.944%\n",
      "lif layer 1 self.abs_max_v: 3097.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.828374/ 60.230976, val:  35.00%, val_best:  42.92%, tr:  98.16%, tr_best:  99.08%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.5026%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 24744  15.797%\n",
      "fc layer 2 self.abs_max_out: 2308.0\n",
      "lif layer 2 self.abs_max_v: 4074.0\n",
      "fc layer 1 self.abs_max_out: 1688.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.341317/ 41.805691, val:  39.58%, val_best:  42.92%, tr:  97.96%, tr_best:  99.08%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.5425%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.9098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 26040  15.646%\n",
      "fc layer 1 self.abs_max_out: 1794.0\n",
      "fc layer 1 self.abs_max_out: 1944.0\n",
      "lif layer 1 self.abs_max_v: 3204.0\n",
      "lif layer 1 self.abs_max_v: 3442.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.284120/ 42.064201, val:  47.08%, val_best:  47.08%, tr:  97.24%, tr_best:  99.08%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.8202%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 27383  15.539%\n",
      "fc layer 1 self.abs_max_out: 1983.0\n",
      "fc layer 2 self.abs_max_out: 2318.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.521075/ 62.323620, val:  25.42%, val_best:  47.08%, tr:  96.32%, tr_best:  99.08%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.6476%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2972%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 28755  15.459%\n",
      "lif layer 1 self.abs_max_v: 3490.5\n",
      "fc layer 2 self.abs_max_out: 2398.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  6.616175/ 44.777737, val:  30.42%, val_best:  47.08%, tr:  97.65%, tr_best:  99.08%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.6489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 30022  15.333%\n",
      "fc layer 1 self.abs_max_out: 2044.0\n",
      "lif layer 1 self.abs_max_v: 3685.5\n",
      "fc layer 2 self.abs_max_out: 2427.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  6.921919/ 49.869720, val:  43.75%, val_best:  47.08%, tr:  97.04%, tr_best:  99.08%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.5952%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9049%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 31306  15.227%\n",
      "lif layer 2 self.abs_max_v: 4327.5\n",
      "fc layer 2 self.abs_max_out: 2589.0\n",
      "lif layer 2 self.abs_max_v: 4539.5\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.940585/ 48.959312, val:  25.42%, val_best:  47.08%, tr:  97.04%, tr_best:  99.08%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4860%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.3289%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 32649  15.159%\n",
      "fc layer 2 self.abs_max_out: 2633.0\n",
      "fc layer 2 self.abs_max_out: 2835.0\n",
      "lif layer 2 self.abs_max_v: 4586.5\n",
      "lif layer 2 self.abs_max_v: 4615.5\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.697964/ 50.319859, val:  42.08%, val_best:  47.08%, tr:  97.65%, tr_best:  99.08%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4525%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.4124%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 33961  15.082%\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.404042/ 26.109573, val:  56.67%, val_best:  56.67%, tr:  96.73%, tr_best:  99.08%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.6233%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.7123%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 35204  14.983%\n",
      "lif layer 2 self.abs_max_v: 4632.5\n",
      "lif layer 2 self.abs_max_v: 4666.5\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.579170/ 30.772675, val:  51.67%, val_best:  56.67%, tr:  96.94%, tr_best:  99.08%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.5298%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.5102%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 36486  14.907%\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.976270/ 47.658764, val:  43.33%, val_best:  56.67%, tr:  97.14%, tr_best:  99.08%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.6247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.8653%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 37857  14.873%\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.330879/ 36.209831, val:  40.00%, val_best:  56.67%, tr:  97.55%, tr_best:  99.08%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.5026%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.6519%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 39172  14.819%\n",
      "lif layer 2 self.abs_max_v: 4737.5\n",
      "fc layer 2 self.abs_max_out: 2904.0\n",
      "lif layer 2 self.abs_max_v: 4792.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.323558/ 29.222866, val:  41.25%, val_best:  56.67%, tr:  96.63%, tr_best:  99.08%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4911%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.6195%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 40483  14.768%\n",
      "fc layer 2 self.abs_max_out: 3027.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  5.765547/ 41.443878, val:  45.83%, val_best:  56.67%, tr:  97.85%, tr_best:  99.08%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.5187%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.6153%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 41728  14.698%\n",
      "lif layer 2 self.abs_max_v: 4877.0\n",
      "lif layer 2 self.abs_max_v: 4922.5\n",
      "fc layer 1 self.abs_max_out: 2126.0\n",
      "fc layer 1 self.abs_max_out: 2188.0\n",
      "lif layer 1 self.abs_max_v: 3938.5\n",
      "fc layer 2 self.abs_max_out: 3116.0\n",
      "lif layer 2 self.abs_max_v: 5041.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.154000/ 46.890099, val:  48.75%, val_best:  56.67%, tr:  97.75%, tr_best:  99.08%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.5721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.9068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 42991  14.638%\n",
      "lif layer 2 self.abs_max_v: 5155.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  5.726567/ 27.617767, val:  47.50%, val_best:  56.67%, tr:  97.24%, tr_best:  99.08%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.6032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.4317%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 44283  14.591%\n",
      "fc layer 1 self.abs_max_out: 2245.0\n",
      "fc layer 1 self.abs_max_out: 2309.0\n",
      "lif layer 1 self.abs_max_v: 4113.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  5.892363/ 33.940453, val:  40.42%, val_best:  56.67%, tr:  96.42%, tr_best:  99.08%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.5904%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.4945%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 45583  14.550%\n",
      "lif layer 2 self.abs_max_v: 5296.0\n",
      "fc layer 1 self.abs_max_out: 2327.0\n",
      "lif layer 1 self.abs_max_v: 4143.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.372370/ 29.630409, val:  36.25%, val_best:  56.67%, tr:  96.12%, tr_best:  99.08%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.6155%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.7759%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 46794  14.484%\n",
      "fc layer 1 self.abs_max_out: 2402.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.449365/ 48.005573, val:  26.25%, val_best:  56.67%, tr:  96.73%, tr_best:  99.08%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4776%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.5763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 48083  14.445%\n",
      "lif layer 1 self.abs_max_v: 4161.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.498715/ 31.655827, val:  48.33%, val_best:  56.67%, tr:  97.34%, tr_best:  99.08%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4672%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.3170%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 49383  14.412%\n",
      "lif layer 1 self.abs_max_v: 4207.5\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.475461/ 40.394581, val:  35.83%, val_best:  56.67%, tr:  97.75%, tr_best:  99.08%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4077%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.1801%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 50652  14.372%\n",
      "fc layer 1 self.abs_max_out: 2595.0\n",
      "lif layer 1 self.abs_max_v: 4248.5\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.144225/ 31.062193, val:  44.17%, val_best:  56.67%, tr:  97.24%, tr_best:  99.08%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.5830%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.9962%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 51897  14.327%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.194582/ 32.653194, val:  40.83%, val_best:  56.67%, tr:  96.73%, tr_best:  99.08%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3550%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.4540%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 53179  14.295%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.601763/ 33.397232, val:  41.67%, val_best:  56.67%, tr:  96.83%, tr_best:  99.08%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4197%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.0076%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 54529  14.282%\n",
      "lif layer 1 self.abs_max_v: 4417.0\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.329304/ 35.754791, val:  40.83%, val_best:  56.67%, tr:  96.63%, tr_best:  99.08%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.5681%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.4118%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 55837  14.259%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.177844/ 37.398373, val:  43.33%, val_best:  56.67%, tr:  96.22%, tr_best:  99.08%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4224%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.9612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 57161  14.241%\n",
      "lif layer 1 self.abs_max_v: 4461.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.084821/ 31.648266, val:  36.67%, val_best:  56.67%, tr:  96.83%, tr_best:  99.08%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.6390%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.9219%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 58462  14.218%\n",
      "fc layer 1 self.abs_max_out: 2597.0\n",
      "lif layer 1 self.abs_max_v: 4739.5\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  4.830260/ 22.055799, val:  48.33%, val_best:  56.67%, tr:  97.04%, tr_best:  99.08%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4752%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.3181%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 59737  14.190%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.047669/ 33.797836, val:  51.25%, val_best:  56.67%, tr:  97.24%, tr_best:  99.08%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.2597%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.2407%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 61080  14.180%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  5.249371/ 28.437984, val:  47.50%, val_best:  56.67%, tr:  97.34%, tr_best:  99.08%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.2642%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.9972%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 62413  14.167%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  5.325633/ 28.380898, val:  42.92%, val_best:  56.67%, tr:  97.85%, tr_best:  99.08%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.7900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 63741  14.154%\n",
      "fc layer 1 self.abs_max_out: 2779.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.977628/ 19.508039, val:  51.67%, val_best:  56.67%, tr:  96.32%, tr_best:  99.08%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.2895%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.2894%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 65046  14.136%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.781729/ 24.750648, val:  48.75%, val_best:  56.67%, tr:  97.24%, tr_best:  99.08%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.0629%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.8920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 66342  14.118%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  5.388905/ 27.123322, val:  42.08%, val_best:  56.67%, tr:  97.24%, tr_best:  99.08%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.1824%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.5899%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 67706  14.114%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  5.198186/ 29.917013, val:  49.17%, val_best:  56.67%, tr:  97.75%, tr_best:  99.08%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.1797%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.4279%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 69076  14.112%\n",
      "lif layer 1 self.abs_max_v: 4931.5\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  5.120365/ 35.573116, val:  40.83%, val_best:  56.67%, tr:  96.73%, tr_best:  99.08%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.1666%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.8687%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 70488  14.118%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  5.063677/ 20.751621, val:  44.17%, val_best:  56.67%, tr:  97.34%, tr_best:  99.08%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.1329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.2558%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 71868  14.117%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.998898/ 22.229027, val:  37.92%, val_best:  56.67%, tr:  97.04%, tr_best:  99.08%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4961%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 73271  14.121%\n",
      "fc layer 1 self.abs_max_out: 2871.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.880680/ 28.326593, val:  45.00%, val_best:  56.67%, tr:  97.85%, tr_best:  99.08%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.5208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7809%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 74698  14.130%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.938690/ 27.396801, val:  47.08%, val_best:  56.67%, tr:  96.42%, tr_best:  99.08%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4296%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7570%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 76126  14.138%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.882398/ 24.938385, val:  43.75%, val_best:  56.67%, tr:  95.91%, tr_best:  99.08%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4385%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 77503  14.137%\n",
      "lif layer 1 self.abs_max_v: 5241.0\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  5.076581/ 28.410290, val:  50.83%, val_best:  56.67%, tr:  97.45%, tr_best:  99.08%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.6198%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5791%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 78935  14.145%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.765571/ 21.796413, val:  43.75%, val_best:  56.67%, tr:  96.42%, tr_best:  99.08%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.7233%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7168%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 80302  14.142%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.838629/ 26.655922, val:  43.33%, val_best:  56.67%, tr:  96.42%, tr_best:  99.08%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4513%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8052%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 81655  14.137%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.436389/ 38.831486, val:  46.67%, val_best:  56.67%, tr:  96.22%, tr_best:  99.08%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3830%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9172%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 82956  14.123%\n",
      "lif layer 2 self.abs_max_v: 5389.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.965002/ 35.218464, val:  26.67%, val_best:  56.67%, tr:  96.83%, tr_best:  99.08%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.2627%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8903%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 84392  14.132%\n",
      "fc layer 1 self.abs_max_out: 2915.0\n",
      "fc layer 1 self.abs_max_out: 2973.0\n",
      "lif layer 1 self.abs_max_v: 5404.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.915625/ 23.952864, val:  40.42%, val_best:  56.67%, tr:  95.91%, tr_best:  99.08%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.1071%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6454%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 85768  14.130%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.858369/ 18.461407, val:  44.58%, val_best:  56.67%, tr:  96.22%, tr_best:  99.08%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.2082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0450%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 87143  14.129%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.375944/ 27.583883, val:  41.25%, val_best:  56.67%, tr:  96.53%, tr_best:  99.08%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3360%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4886%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 88447  14.116%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.595484/ 30.794327, val:  37.50%, val_best:  56.67%, tr:  96.53%, tr_best:  99.08%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4288%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6083%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 89756  14.105%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  4.524124/ 28.797894, val:  43.75%, val_best:  56.67%, tr:  97.04%, tr_best:  99.08%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.2407%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 91100  14.099%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.835923/ 37.940166, val:  47.92%, val_best:  56.67%, tr:  96.42%, tr_best:  99.08%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.2132%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9873%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 92451  14.095%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  4.549779/ 25.180477, val:  48.75%, val_best:  56.67%, tr:  96.83%, tr_best:  99.08%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.2903%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9404%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 93751  14.083%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.918561/ 23.229137, val:  42.50%, val_best:  56.67%, tr:  96.83%, tr_best:  99.08%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3478%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6817%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 95114  14.080%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  4.664032/ 29.079357, val:  41.25%, val_best:  56.67%, tr:  97.24%, tr_best:  99.08%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3804%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5602%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 96393  14.066%\n",
      "fc layer 1 self.abs_max_out: 3056.0\n",
      "fc layer 1 self.abs_max_out: 3150.0\n",
      "lif layer 1 self.abs_max_v: 5728.5\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  4.651816/ 34.882984, val:  51.25%, val_best:  56.67%, tr:  97.14%, tr_best:  99.08%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3221%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 97768  14.066%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  4.756436/ 31.775034, val:  49.58%, val_best:  56.67%, tr:  96.02%, tr_best:  99.08%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3593%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0223%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 99128  14.063%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  4.796291/ 16.775845, val:  50.42%, val_best:  56.67%, tr:  97.24%, tr_best:  99.08%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.2114%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6695%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 100491  14.061%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  4.693595/ 24.725775, val:  45.83%, val_best:  56.67%, tr:  97.75%, tr_best:  99.08%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4958%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 101802  14.052%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  4.679781/ 21.408804, val:  47.50%, val_best:  56.67%, tr:  97.55%, tr_best:  99.08%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4561%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0512%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 103154  14.049%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  4.831445/ 23.186333, val:  52.50%, val_best:  56.67%, tr:  96.94%, tr_best:  99.08%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3822%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 104518  14.047%\n",
      "lif layer 2 self.abs_max_v: 5503.5\n",
      "lif layer 2 self.abs_max_v: 5568.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  4.553476/ 17.877451, val:  50.00%, val_best:  56.67%, tr:  95.71%, tr_best:  99.08%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3179%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4668%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 105884  14.046%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  4.664244/ 29.740843, val:  44.58%, val_best:  56.67%, tr:  96.73%, tr_best:  99.08%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4668%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6776%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 107216  14.040%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  4.554064/ 25.863440, val:  47.50%, val_best:  56.67%, tr:  97.24%, tr_best:  99.08%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3423%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 108542  14.034%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  4.842397/ 27.881163, val:  49.58%, val_best:  56.67%, tr:  97.45%, tr_best:  99.08%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.2934%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7233%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 109861  14.027%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  4.615594/ 28.053188, val:  49.17%, val_best:  56.67%, tr:  96.73%, tr_best:  99.08%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4397%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8140%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 111141  14.015%\n",
      "fc layer 1 self.abs_max_out: 3203.0\n",
      "lif layer 1 self.abs_max_v: 5794.5\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  4.375802/ 34.710972, val:  36.67%, val_best:  56.67%, tr:  96.83%, tr_best:  99.08%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.1811%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5025%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 112435  14.006%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  4.900700/ 18.142490, val:  52.08%, val_best:  56.67%, tr:  96.42%, tr_best:  99.08%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.2183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3735%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 113869  14.013%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  4.544078/ 22.916784, val:  34.17%, val_best:  56.67%, tr:  97.04%, tr_best:  99.08%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.2955%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 115200  14.008%\n",
      "fc layer 1 self.abs_max_out: 3306.0\n",
      "fc layer 1 self.abs_max_out: 3393.0\n",
      "lif layer 1 self.abs_max_v: 6165.5\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  4.613564/ 22.664833, val:  49.58%, val_best:  56.67%, tr:  97.96%, tr_best:  99.08%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4158%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4640%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 116522  14.003%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  4.644897/ 20.763634, val:  54.17%, val_best:  56.67%, tr:  96.53%, tr_best:  99.08%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3545%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8764%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 117910  14.005%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  4.594732/ 30.587946, val:  45.42%, val_best:  56.67%, tr:  95.81%, tr_best:  99.08%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.5060%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 119229  13.998%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  4.727390/ 32.305614, val:  45.42%, val_best:  56.67%, tr:  97.24%, tr_best:  99.08%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.5287%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3294%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 120566  13.995%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  4.647991/ 20.465824, val:  47.08%, val_best:  56.67%, tr:  95.40%, tr_best:  99.08%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.2605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9197%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 121947  13.996%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  4.738820/ 29.115099, val:  40.42%, val_best:  56.67%, tr:  96.53%, tr_best:  99.08%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3620%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7771%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 123340  13.998%\n",
      "fc layer 1 self.abs_max_out: 3454.0\n",
      "lif layer 1 self.abs_max_v: 6289.0\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  4.404748/ 28.609945, val:  47.50%, val_best:  56.67%, tr:  95.51%, tr_best:  99.08%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0534%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 124651  13.992%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [256.0,128.0,64.0,32.0,16.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "\n",
    "        \"learning_rate\": {\"values\": [1.0]}, \n",
    "        # \"lr_factor\": {\"values\": [-6, -7, -8, -9]}, \n",
    "        \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [0]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [0]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [0]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "        \"lif_layer_sg_width2\": {\"values\": [0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]},\n",
    "        \"lif_layer_v_threshold2\": {\"values\": [256.0,128.0,64.0,32.0,16.0]},\n",
    "        \"learning_rate2\": {\"values\": [1.0]},\n",
    "        \"init_scaling_0\": {\"values\": [4/128]},\n",
    "        \"init_scaling_1\": {\"values\": [6/128]},\n",
    "        \"init_scaling_2\": {\"values\": [3/128]},\n",
    "        \n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"3\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "        lif_layer_sg_width2  =  wandb.config.lif_layer_sg_width2,\n",
    "        lif_layer_v_threshold2  =  wandb.config.lif_layer_v_threshold2,\n",
    "        learning_rate2  =  wandb.config.learning_rate2,\n",
    "        init_scaling = [wandb.config.init_scaling_0,wandb.config.init_scaling_1,wandb.config.init_scaling_2],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'e1m59f1o'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
