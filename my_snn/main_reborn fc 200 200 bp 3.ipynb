{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6153/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on:\n",
    "        assert DFA_on and single_step\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #     criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        i = 0\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                # network save\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\")  \n",
    "            else:\n",
    "                iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20250424_205814-95usj6o0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/95usj6o0' target=\"_blank\">dry-bird-6769</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/95usj6o0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/95usj6o0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0.0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4.0, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_main.pth', 'learning_rate': 0.001, 'epoch_num': 10000, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1.0, 'bias': True, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_pooling(\n",
      "        (ann_module): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): DimChanger_for_pooling(\n",
      "        (ann_module): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=False)\n",
      "      (4): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4.0, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False)\n",
      "      (6): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4.0, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False)\n",
      "      (8): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4.0, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.302585/  2.302586, tr:   9.91%, tr_best:   9.91%, val:  10.00%, val_best:  10.00%: 100%|██████████| 62/62 [00:03<00:00, 16.95it/s]\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.302585/  2.302586, tr:   9.91%, tr_best:   9.91%, val:  10.00%, val_best:  10.00%: 100%|██████████| 62/62 [00:02<00:00, 21.11it/s]\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.302585/  2.302586, tr:   9.91%, tr_best:   9.91%, val:  10.00%, val_best:  10.00%: 100%|██████████| 62/62 [00:02<00:00, 21.61it/s]\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.302585/  2.302586, tr:   9.91%, tr_best:   9.91%, val:  10.00%, val_best:  10.00%: 100%|██████████| 62/62 [00:02<00:00, 21.42it/s]\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  2.301063/  2.299998, tr:  10.83%, tr_best:  10.83%, val:  10.83%, val_best:  10.83%: 100%|██████████| 62/62 [00:02<00:00, 21.37it/s]\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  2.288215/  2.288282, tr:  14.71%, tr_best:  14.71%, val:  14.17%, val_best:  14.17%: 100%|██████████| 62/62 [00:02<00:00, 21.28it/s]\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  2.266992/  2.270075, tr:  17.67%, tr_best:  17.67%, val:  16.67%, val_best:  16.67%: 100%|██████████| 62/62 [00:03<00:00, 20.44it/s]\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  2.241206/  2.239012, tr:  21.96%, tr_best:  21.96%, val:  24.17%, val_best:  24.17%: 100%|██████████| 62/62 [00:03<00:00, 20.63it/s]\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  2.194549/  2.193245, tr:  28.91%, tr_best:  28.91%, val:  28.33%, val_best:  28.33%: 100%|██████████| 62/62 [00:02<00:00, 21.67it/s]\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  2.133707/  2.141743, tr:  35.85%, tr_best:  35.85%, val:  34.17%, val_best:  34.17%: 100%|██████████| 62/62 [00:02<00:00, 20.78it/s]\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  2.081668/  2.097003, tr:  42.59%, tr_best:  42.59%, val:  37.08%, val_best:  37.08%: 100%|██████████| 62/62 [00:02<00:00, 21.50it/s]\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  2.045930/  2.058858, tr:  44.02%, tr_best:  44.02%, val:  42.08%, val_best:  42.08%: 100%|██████████| 62/62 [00:03<00:00, 20.48it/s]\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  2.010978/  2.024179, tr:  47.60%, tr_best:  47.60%, val:  45.00%, val_best:  45.00%: 100%|██████████| 62/62 [00:02<00:00, 21.59it/s]\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  1.985732/  2.003715, tr:  48.72%, tr_best:  48.72%, val:  44.58%, val_best:  45.00%: 100%|██████████| 62/62 [00:02<00:00, 21.93it/s]\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  1.965273/  1.985095, tr:  50.46%, tr_best:  50.46%, val:  48.33%, val_best:  48.33%: 100%|██████████| 62/62 [00:03<00:00, 20.66it/s]\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  1.945778/  1.970295, tr:  51.99%, tr_best:  51.99%, val:  50.00%, val_best:  50.00%: 100%|██████████| 62/62 [00:02<00:00, 20.95it/s]\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  1.931206/  1.953694, tr:  53.52%, tr_best:  53.52%, val:  50.00%, val_best:  50.00%: 100%|██████████| 62/62 [00:02<00:00, 21.96it/s]\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  1.917926/  1.943703, tr:  54.34%, tr_best:  54.34%, val:  49.17%, val_best:  50.00%: 100%|██████████| 62/62 [00:03<00:00, 20.60it/s]\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  1.905047/  1.932519, tr:  53.22%, tr_best:  54.34%, val:  52.08%, val_best:  52.08%: 100%|██████████| 62/62 [00:02<00:00, 21.42it/s]\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  1.892768/  1.923558, tr:  54.24%, tr_best:  54.34%, val:  51.67%, val_best:  52.08%: 100%|██████████| 62/62 [00:03<00:00, 20.43it/s]\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  1.881087/  1.917288, tr:  56.08%, tr_best:  56.08%, val:  50.42%, val_best:  52.08%: 100%|██████████| 62/62 [00:02<00:00, 20.89it/s]\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  1.871706/  1.906085, tr:  54.65%, tr_best:  56.08%, val:  50.42%, val_best:  52.08%: 100%|██████████| 62/62 [00:03<00:00, 20.50it/s]\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  1.863264/  1.904792, tr:  55.67%, tr_best:  56.08%, val:  49.58%, val_best:  52.08%: 100%|██████████| 62/62 [00:03<00:00, 20.67it/s]\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  1.858659/  1.898261, tr:  55.98%, tr_best:  56.08%, val:  50.42%, val_best:  52.08%: 100%|██████████| 62/62 [00:03<00:00, 20.63it/s]\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  1.848080/  1.889352, tr:  56.38%, tr_best:  56.38%, val:  49.58%, val_best:  52.08%: 100%|██████████| 62/62 [00:03<00:00, 20.17it/s]\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  1.843220/  1.883506, tr:  55.87%, tr_best:  56.38%, val:  49.17%, val_best:  52.08%: 100%|██████████| 62/62 [00:02<00:00, 21.56it/s]\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  1.834411/  1.878954, tr:  57.81%, tr_best:  57.81%, val:  50.42%, val_best:  52.08%: 100%|██████████| 62/62 [00:02<00:00, 20.97it/s]\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  1.828912/  1.875081, tr:  57.30%, tr_best:  57.81%, val:  52.92%, val_best:  52.92%: 100%|██████████| 62/62 [00:03<00:00, 20.56it/s]\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  1.825550/  1.869149, tr:  57.10%, tr_best:  57.81%, val:  51.25%, val_best:  52.92%: 100%|██████████| 62/62 [00:03<00:00, 19.38it/s]\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  1.817716/  1.866004, tr:  56.79%, tr_best:  57.81%, val:  52.50%, val_best:  52.92%: 100%|██████████| 62/62 [00:03<00:00, 19.18it/s]\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  1.814386/  1.864805, tr:  57.41%, tr_best:  57.81%, val:  53.75%, val_best:  53.75%: 100%|██████████| 62/62 [00:03<00:00, 18.34it/s]\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  1.806477/  1.862636, tr:  58.43%, tr_best:  58.43%, val:  51.25%, val_best:  53.75%: 100%|██████████| 62/62 [00:03<00:00, 18.78it/s]\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  1.805994/  1.858613, tr:  58.32%, tr_best:  58.43%, val:  52.92%, val_best:  53.75%: 100%|██████████| 62/62 [00:03<00:00, 19.34it/s]\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  1.804725/  1.858045, tr:  58.53%, tr_best:  58.53%, val:  49.58%, val_best:  53.75%: 100%|██████████| 62/62 [00:03<00:00, 18.92it/s]\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  1.799745/  1.857443, tr:  58.22%, tr_best:  58.53%, val:  50.00%, val_best:  53.75%: 100%|██████████| 62/62 [00:03<00:00, 19.10it/s]\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  1.796332/  1.857474, tr:  59.04%, tr_best:  59.04%, val:  50.83%, val_best:  53.75%: 100%|██████████| 62/62 [00:03<00:00, 19.60it/s]\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  1.791026/  1.855064, tr:  59.24%, tr_best:  59.24%, val:  50.42%, val_best:  53.75%: 100%|██████████| 62/62 [00:03<00:00, 19.62it/s]\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  1.792785/  1.853151, tr:  59.04%, tr_best:  59.24%, val:  49.58%, val_best:  53.75%: 100%|██████████| 62/62 [00:03<00:00, 19.48it/s]\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  1.788388/  1.846120, tr:  59.55%, tr_best:  59.55%, val:  52.08%, val_best:  53.75%: 100%|██████████| 62/62 [00:03<00:00, 19.33it/s]\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  1.782812/  1.842408, tr:  60.98%, tr_best:  60.98%, val:  52.50%, val_best:  53.75%: 100%|██████████| 62/62 [00:03<00:00, 19.05it/s]\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  1.781720/  1.842852, tr:  59.55%, tr_best:  60.98%, val:  52.50%, val_best:  53.75%: 100%|██████████| 62/62 [00:03<00:00, 19.58it/s]\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  1.780056/  1.839259, tr:  61.59%, tr_best:  61.59%, val:  53.33%, val_best:  53.75%: 100%|██████████| 62/62 [00:03<00:00, 18.89it/s]\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  1.772277/  1.836744, tr:  60.57%, tr_best:  61.59%, val:  52.92%, val_best:  53.75%: 100%|██████████| 62/62 [00:03<00:00, 19.08it/s]\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  1.769986/  1.832611, tr:  61.29%, tr_best:  61.59%, val:  54.17%, val_best:  54.17%: 100%|██████████| 62/62 [00:03<00:00, 19.40it/s]\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  1.766625/  1.837697, tr:  60.88%, tr_best:  61.59%, val:  51.67%, val_best:  54.17%: 100%|██████████| 62/62 [00:03<00:00, 19.67it/s]\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  1.767599/  1.831426, tr:  61.90%, tr_best:  61.90%, val:  54.17%, val_best:  54.17%: 100%|██████████| 62/62 [00:03<00:00, 19.76it/s]\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  1.761213/  1.833835, tr:  64.15%, tr_best:  64.15%, val:  54.17%, val_best:  54.17%: 100%|██████████| 62/62 [00:03<00:00, 19.56it/s]\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  1.760458/  1.832338, tr:  63.23%, tr_best:  64.15%, val:  52.08%, val_best:  54.17%: 100%|██████████| 62/62 [00:03<00:00, 18.93it/s]\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  1.758541/  1.830160, tr:  62.82%, tr_best:  64.15%, val:  52.50%, val_best:  54.17%: 100%|██████████| 62/62 [00:03<00:00, 19.07it/s]\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  1.757912/  1.824140, tr:  63.02%, tr_best:  64.15%, val:  52.08%, val_best:  54.17%: 100%|██████████| 62/62 [00:03<00:00, 19.36it/s]\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  1.753381/  1.826245, tr:  63.33%, tr_best:  64.15%, val:  52.08%, val_best:  54.17%: 100%|██████████| 62/62 [00:03<00:00, 18.82it/s]\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  1.751448/  1.827683, tr:  63.53%, tr_best:  64.15%, val:  51.67%, val_best:  54.17%: 100%|██████████| 62/62 [00:03<00:00, 19.91it/s]\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  1.747275/  1.826030, tr:  63.84%, tr_best:  64.15%, val:  52.92%, val_best:  54.17%: 100%|██████████| 62/62 [00:03<00:00, 19.64it/s]\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  1.742765/  1.813591, tr:  64.76%, tr_best:  64.76%, val:  51.67%, val_best:  54.17%: 100%|██████████| 62/62 [00:03<00:00, 19.14it/s]\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  1.738737/  1.815532, tr:  63.74%, tr_best:  64.76%, val:  55.42%, val_best:  55.42%: 100%|██████████| 62/62 [00:03<00:00, 18.89it/s]\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  1.734269/  1.812558, tr:  64.86%, tr_best:  64.86%, val:  55.83%, val_best:  55.83%: 100%|██████████| 62/62 [00:03<00:00, 19.16it/s]\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  1.732489/  1.813247, tr:  64.15%, tr_best:  64.86%, val:  54.58%, val_best:  55.83%: 100%|██████████| 62/62 [00:03<00:00, 19.25it/s]\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  1.731795/  1.815640, tr:  64.04%, tr_best:  64.86%, val:  55.42%, val_best:  55.83%: 100%|██████████| 62/62 [00:03<00:00, 19.20it/s]\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  1.730093/  1.812770, tr:  63.43%, tr_best:  64.86%, val:  56.67%, val_best:  56.67%: 100%|██████████| 62/62 [00:03<00:00, 19.70it/s]\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  1.727174/  1.809348, tr:  64.45%, tr_best:  64.86%, val:  55.83%, val_best:  56.67%: 100%|██████████| 62/62 [00:03<00:00, 18.57it/s]\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  1.726702/  1.808655, tr:  63.74%, tr_best:  64.86%, val:  53.33%, val_best:  56.67%: 100%|██████████| 62/62 [00:03<00:00, 19.89it/s]\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  1.725414/  1.809639, tr:  64.15%, tr_best:  64.86%, val:  54.58%, val_best:  56.67%: 100%|██████████| 62/62 [00:03<00:00, 19.00it/s]\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  1.725032/  1.808205, tr:  64.86%, tr_best:  64.86%, val:  54.58%, val_best:  56.67%: 100%|██████████| 62/62 [00:03<00:00, 19.90it/s]\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  1.721149/  1.803828, tr:  64.45%, tr_best:  64.86%, val:  55.00%, val_best:  56.67%: 100%|██████████| 62/62 [00:03<00:00, 19.07it/s]\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  1.719162/  1.804930, tr:  64.35%, tr_best:  64.86%, val:  54.58%, val_best:  56.67%: 100%|██████████| 62/62 [00:03<00:00, 19.16it/s]\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  1.719421/  1.803376, tr:  65.07%, tr_best:  65.07%, val:  57.50%, val_best:  57.50%: 100%|██████████| 62/62 [00:03<00:00, 19.85it/s]\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  1.717801/  1.800360, tr:  65.07%, tr_best:  65.07%, val:  58.33%, val_best:  58.33%: 100%|██████████| 62/62 [00:03<00:00, 19.85it/s]\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  1.718135/  1.798909, tr:  64.35%, tr_best:  65.07%, val:  58.75%, val_best:  58.75%: 100%|██████████| 62/62 [00:03<00:00, 19.71it/s]\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  1.713575/  1.803676, tr:  66.09%, tr_best:  66.09%, val:  58.33%, val_best:  58.75%: 100%|██████████| 62/62 [00:03<00:00, 19.35it/s]\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  1.712776/  1.797615, tr:  65.99%, tr_best:  66.09%, val:  57.92%, val_best:  58.75%: 100%|██████████| 62/62 [00:03<00:00, 19.40it/s]\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  1.712664/  1.795568, tr:  66.19%, tr_best:  66.19%, val:  57.50%, val_best:  58.75%: 100%|██████████| 62/62 [00:03<00:00, 19.41it/s]\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  1.710647/  1.795693, tr:  66.39%, tr_best:  66.39%, val:  58.33%, val_best:  58.75%: 100%|██████████| 62/62 [00:03<00:00, 19.27it/s]\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  1.706091/  1.795960, tr:  67.31%, tr_best:  67.31%, val:  56.67%, val_best:  58.75%: 100%|██████████| 62/62 [00:03<00:00, 20.00it/s]\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  1.703827/  1.792477, tr:  68.03%, tr_best:  68.03%, val:  58.33%, val_best:  58.75%: 100%|██████████| 62/62 [00:03<00:00, 19.85it/s]\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  1.704984/  1.795918, tr:  67.21%, tr_best:  68.03%, val:  56.25%, val_best:  58.75%: 100%|██████████| 62/62 [00:03<00:00, 19.59it/s]\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  1.704056/  1.794284, tr:  67.93%, tr_best:  68.03%, val:  55.83%, val_best:  58.75%: 100%|██████████| 62/62 [00:03<00:00, 19.52it/s]\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  1.702226/  1.792323, tr:  67.11%, tr_best:  68.03%, val:  58.33%, val_best:  58.75%: 100%|██████████| 62/62 [00:03<00:00, 19.56it/s]\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  1.703439/  1.790880, tr:  68.74%, tr_best:  68.74%, val:  59.58%, val_best:  59.58%: 100%|██████████| 62/62 [00:03<00:00, 19.43it/s]\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  1.700179/  1.786774, tr:  67.52%, tr_best:  68.74%, val:  58.75%, val_best:  59.58%: 100%|██████████| 62/62 [00:03<00:00, 19.65it/s]\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  1.700842/  1.783471, tr:  67.62%, tr_best:  68.74%, val:  62.92%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 18.86it/s]\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  1.696206/  1.784314, tr:  67.62%, tr_best:  68.74%, val:  60.42%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 19.11it/s]\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  1.695582/  1.785423, tr:  67.52%, tr_best:  68.74%, val:  60.83%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 19.96it/s]\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  1.693142/  1.785749, tr:  68.44%, tr_best:  68.74%, val:  59.58%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 19.53it/s]\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  1.694556/  1.781064, tr:  67.82%, tr_best:  68.74%, val:  60.83%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 20.27it/s]\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  1.694216/  1.783301, tr:  69.36%, tr_best:  69.36%, val:  59.17%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 20.21it/s]\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  1.693018/  1.780450, tr:  68.54%, tr_best:  69.36%, val:  58.75%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 19.18it/s]\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  1.689466/  1.779343, tr:  69.56%, tr_best:  69.56%, val:  61.25%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 19.20it/s]\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  1.689550/  1.782452, tr:  68.74%, tr_best:  69.56%, val:  59.17%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 19.08it/s]\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  1.687260/  1.780223, tr:  68.54%, tr_best:  69.56%, val:  59.58%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 19.13it/s]\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  1.687559/  1.779231, tr:  68.44%, tr_best:  69.56%, val:  58.75%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 19.27it/s]\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  1.686569/  1.782244, tr:  68.64%, tr_best:  69.56%, val:  60.00%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 19.52it/s]\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  1.682465/  1.781020, tr:  69.25%, tr_best:  69.56%, val:  60.42%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 20.17it/s]\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  1.683406/  1.780441, tr:  70.07%, tr_best:  70.07%, val:  59.17%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 18.37it/s]\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  1.686466/  1.779369, tr:  70.28%, tr_best:  70.28%, val:  58.75%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 19.13it/s]\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  1.682947/  1.777370, tr:  69.56%, tr_best:  70.28%, val:  58.33%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 19.23it/s]\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  1.679647/  1.777829, tr:  69.77%, tr_best:  70.28%, val:  58.75%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 19.55it/s]\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  1.684089/  1.775100, tr:  69.05%, tr_best:  70.28%, val:  60.00%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 19.51it/s]\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  1.679725/  1.775757, tr:  70.28%, tr_best:  70.28%, val:  61.25%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 19.77it/s]\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  1.680083/  1.777076, tr:  69.56%, tr_best:  70.28%, val:  60.83%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 18.80it/s]\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  1.679503/  1.776292, tr:  69.97%, tr_best:  70.28%, val:  61.67%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 19.40it/s]\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  1.675001/  1.775236, tr:  70.68%, tr_best:  70.68%, val:  62.92%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 19.43it/s]\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  1.674459/  1.775674, tr:  71.20%, tr_best:  71.20%, val:  60.00%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 20.02it/s]\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  1.675661/  1.774269, tr:  70.17%, tr_best:  71.20%, val:  60.83%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 20.21it/s]\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  1.674037/  1.775229, tr:  69.77%, tr_best:  71.20%, val:  61.67%, val_best:  62.92%: 100%|██████████| 62/62 [00:03<00:00, 20.58it/s]\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  1.672771/  1.773283, tr:  70.28%, tr_best:  71.20%, val:  62.50%, val_best:  62.92%: 100%|██████████| 62/62 [00:02<00:00, 20.81it/s]\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  1.676010/  1.772656, tr:  70.28%, tr_best:  71.20%, val:  63.33%, val_best:  63.33%: 100%|██████████| 62/62 [00:02<00:00, 20.74it/s]\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  1.673762/  1.774490, tr:  69.87%, tr_best:  71.20%, val:  59.58%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 19.81it/s]\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  1.671288/  1.775025, tr:  70.79%, tr_best:  71.20%, val:  61.67%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 20.15it/s]\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  1.669626/  1.772035, tr:  71.60%, tr_best:  71.60%, val:  61.25%, val_best:  63.33%: 100%|██████████| 62/62 [00:02<00:00, 20.93it/s]\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  1.669367/  1.771255, tr:  71.50%, tr_best:  71.60%, val:  61.25%, val_best:  63.33%: 100%|██████████| 62/62 [00:02<00:00, 21.00it/s]\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  1.667767/  1.772563, tr:  70.89%, tr_best:  71.60%, val:  60.00%, val_best:  63.33%: 100%|██████████| 62/62 [00:02<00:00, 20.82it/s]\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  1.667007/  1.773947, tr:  71.81%, tr_best:  71.81%, val:  61.25%, val_best:  63.33%: 100%|██████████| 62/62 [00:02<00:00, 21.11it/s]\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  1.665269/  1.769351, tr:  71.71%, tr_best:  71.81%, val:  59.17%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 19.71it/s]\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  1.665680/  1.768234, tr:  72.73%, tr_best:  72.73%, val:  62.50%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 20.54it/s]\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  1.667845/  1.767873, tr:  72.01%, tr_best:  72.73%, val:  61.67%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 20.34it/s]\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  1.663853/  1.769598, tr:  71.81%, tr_best:  72.73%, val:  61.25%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 19.92it/s]\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  1.661356/  1.767128, tr:  72.83%, tr_best:  72.83%, val:  60.83%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 20.38it/s]\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  1.662317/  1.768367, tr:  72.73%, tr_best:  72.83%, val:  61.25%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 20.63it/s]\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  1.660762/  1.766976, tr:  73.24%, tr_best:  73.24%, val:  61.67%, val_best:  63.33%: 100%|██████████| 62/62 [00:02<00:00, 20.86it/s]\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  1.660904/  1.766968, tr:  73.03%, tr_best:  73.24%, val:  62.50%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 19.46it/s]\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  1.659814/  1.766646, tr:  72.93%, tr_best:  73.24%, val:  61.67%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 20.58it/s]\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  1.657461/  1.764740, tr:  73.24%, tr_best:  73.24%, val:  62.92%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 19.93it/s]\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  1.660742/  1.765437, tr:  72.22%, tr_best:  73.24%, val:  62.08%, val_best:  63.33%: 100%|██████████| 62/62 [00:02<00:00, 21.48it/s]\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  1.658648/  1.763080, tr:  74.16%, tr_best:  74.16%, val:  62.92%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 20.58it/s]\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  1.655440/  1.765389, tr:  73.95%, tr_best:  74.16%, val:  61.67%, val_best:  63.33%: 100%|██████████| 62/62 [00:02<00:00, 20.97it/s]\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  1.655901/  1.764515, tr:  73.65%, tr_best:  74.16%, val:  60.42%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 19.84it/s]\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  1.660338/  1.765393, tr:  73.65%, tr_best:  74.16%, val:  59.17%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 20.01it/s]\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  1.656857/  1.764756, tr:  73.34%, tr_best:  74.16%, val:  61.25%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 19.98it/s]\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  1.652461/  1.763268, tr:  74.26%, tr_best:  74.26%, val:  61.67%, val_best:  63.33%: 100%|██████████| 62/62 [00:02<00:00, 21.39it/s]\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  1.652552/  1.763815, tr:  73.75%, tr_best:  74.26%, val:  62.08%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 20.41it/s]\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  1.657096/  1.766021, tr:  73.65%, tr_best:  74.26%, val:  61.67%, val_best:  63.33%: 100%|██████████| 62/62 [00:02<00:00, 20.71it/s]\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  1.652920/  1.761994, tr:  73.75%, tr_best:  74.26%, val:  63.33%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 20.63it/s]\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  1.650089/  1.762854, tr:  74.67%, tr_best:  74.67%, val:  61.67%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 20.13it/s]\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  1.649022/  1.762320, tr:  73.95%, tr_best:  74.67%, val:  61.25%, val_best:  63.33%: 100%|██████████| 62/62 [00:03<00:00, 20.50it/s]\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  1.649682/  1.761123, tr:  73.75%, tr_best:  74.67%, val:  63.75%, val_best:  63.75%: 100%|██████████| 62/62 [00:03<00:00, 19.93it/s]\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  1.647331/  1.760606, tr:  73.54%, tr_best:  74.67%, val:  64.17%, val_best:  64.17%: 100%|██████████| 62/62 [00:02<00:00, 20.69it/s]\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  1.648947/  1.761459, tr:  73.95%, tr_best:  74.67%, val:  62.92%, val_best:  64.17%: 100%|██████████| 62/62 [00:02<00:00, 20.92it/s]\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  1.648751/  1.758894, tr:  74.26%, tr_best:  74.67%, val:  63.33%, val_best:  64.17%: 100%|██████████| 62/62 [00:02<00:00, 21.09it/s]\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  1.646772/  1.758345, tr:  74.46%, tr_best:  74.67%, val:  62.92%, val_best:  64.17%: 100%|██████████| 62/62 [00:02<00:00, 21.04it/s]\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  1.644845/  1.758588, tr:  74.97%, tr_best:  74.97%, val:  64.17%, val_best:  64.17%: 100%|██████████| 62/62 [00:03<00:00, 19.90it/s]\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  1.647247/  1.759531, tr:  74.46%, tr_best:  74.97%, val:  63.75%, val_best:  64.17%: 100%|██████████| 62/62 [00:02<00:00, 20.73it/s]\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  1.642059/  1.762393, tr:  74.77%, tr_best:  74.97%, val:  62.50%, val_best:  64.17%: 100%|██████████| 62/62 [00:03<00:00, 20.12it/s]\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  1.642627/  1.760691, tr:  75.28%, tr_best:  75.28%, val:  61.67%, val_best:  64.17%: 100%|██████████| 62/62 [00:03<00:00, 20.20it/s]\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  1.642539/  1.759656, tr:  75.89%, tr_best:  75.89%, val:  62.92%, val_best:  64.17%: 100%|██████████| 62/62 [00:03<00:00, 20.55it/s]\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  1.642865/  1.756663, tr:  75.69%, tr_best:  75.89%, val:  62.50%, val_best:  64.17%: 100%|██████████| 62/62 [00:03<00:00, 19.79it/s]\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  1.641663/  1.758459, tr:  75.08%, tr_best:  75.89%, val:  63.33%, val_best:  64.17%: 100%|██████████| 62/62 [00:02<00:00, 21.13it/s]\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  1.641355/  1.760590, tr:  75.38%, tr_best:  75.89%, val:  61.67%, val_best:  64.17%: 100%|██████████| 62/62 [00:03<00:00, 19.51it/s]\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  1.645505/  1.761311, tr:  75.18%, tr_best:  75.89%, val:  62.50%, val_best:  64.17%: 100%|██████████| 62/62 [00:03<00:00, 20.25it/s]\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  1.640211/  1.763066, tr:  75.69%, tr_best:  75.89%, val:  61.67%, val_best:  64.17%: 100%|██████████| 62/62 [00:03<00:00, 20.51it/s]\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  1.639398/  1.763820, tr:  76.40%, tr_best:  76.40%, val:  62.08%, val_best:  64.17%: 100%|██████████| 62/62 [00:02<00:00, 21.13it/s]\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  1.639735/  1.759906, tr:  74.67%, tr_best:  76.40%, val:  64.58%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.41it/s]\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  1.636113/  1.761722, tr:  76.20%, tr_best:  76.40%, val:  62.08%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.50it/s]\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  1.636315/  1.761678, tr:  75.18%, tr_best:  76.40%, val:  62.08%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 21.28it/s]\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  1.637833/  1.761659, tr:  75.59%, tr_best:  76.40%, val:  62.08%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 21.02it/s]\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  1.635653/  1.758670, tr:  75.69%, tr_best:  76.40%, val:  62.92%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 19.92it/s]\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  1.635383/  1.759381, tr:  75.28%, tr_best:  76.40%, val:  62.92%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 20.78it/s]\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  1.635170/  1.755043, tr:  75.38%, tr_best:  76.40%, val:  63.75%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 20.97it/s]\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  1.632137/  1.756096, tr:  76.20%, tr_best:  76.40%, val:  64.17%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.57it/s]\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  1.635809/  1.756457, tr:  76.30%, tr_best:  76.40%, val:  64.17%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.13it/s]\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  1.633974/  1.757298, tr:  76.20%, tr_best:  76.40%, val:  64.17%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.10it/s]\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  1.633686/  1.757492, tr:  76.51%, tr_best:  76.51%, val:  64.58%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 19.66it/s]\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  1.632421/  1.758619, tr:  76.10%, tr_best:  76.51%, val:  62.50%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 20.95it/s]\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  1.633504/  1.760224, tr:  76.51%, tr_best:  76.51%, val:  62.92%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 20.92it/s]\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  1.631169/  1.755125, tr:  76.30%, tr_best:  76.51%, val:  63.75%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 20.81it/s]\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  1.631454/  1.755978, tr:  76.81%, tr_best:  76.81%, val:  64.17%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.60it/s]\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  1.631202/  1.756776, tr:  76.71%, tr_best:  76.81%, val:  64.17%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.43it/s]\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  1.630532/  1.758002, tr:  76.81%, tr_best:  76.81%, val:  62.92%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.00it/s]\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  1.630940/  1.756115, tr:  76.40%, tr_best:  76.81%, val:  64.17%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 20.96it/s]\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  1.629994/  1.757502, tr:  76.00%, tr_best:  76.81%, val:  63.75%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 21.20it/s]\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  1.627347/  1.755071, tr:  76.51%, tr_best:  76.81%, val:  63.75%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 20.96it/s]\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  1.629707/  1.755500, tr:  76.51%, tr_best:  76.81%, val:  63.75%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.34it/s]\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  1.629097/  1.756182, tr:  76.71%, tr_best:  76.81%, val:  62.92%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 20.82it/s]\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  1.627359/  1.753595, tr:  76.40%, tr_best:  76.81%, val:  63.33%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 19.55it/s]\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  1.627978/  1.751561, tr:  76.20%, tr_best:  76.81%, val:  64.58%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 20.86it/s]\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  1.625831/  1.753228, tr:  76.51%, tr_best:  76.81%, val:  63.75%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.55it/s]\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  1.625294/  1.752019, tr:  76.40%, tr_best:  76.81%, val:  64.58%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 19.49it/s]\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  1.625362/  1.754727, tr:  77.12%, tr_best:  77.12%, val:  62.92%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.23it/s]\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  1.629549/  1.753498, tr:  76.30%, tr_best:  77.12%, val:  62.08%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 21.29it/s]\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  1.624748/  1.753596, tr:  76.81%, tr_best:  77.12%, val:  61.67%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 19.27it/s]\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  1.627277/  1.753684, tr:  76.92%, tr_best:  77.12%, val:  60.42%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 19.87it/s]\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  1.623491/  1.751602, tr:  76.40%, tr_best:  77.12%, val:  62.50%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 21.14it/s]\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  1.625165/  1.754378, tr:  76.40%, tr_best:  77.12%, val:  63.75%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.53it/s]\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  1.621827/  1.752317, tr:  76.51%, tr_best:  77.12%, val:  62.92%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 21.08it/s]\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  1.623788/  1.754310, tr:  77.02%, tr_best:  77.12%, val:  63.75%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 21.08it/s]\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  1.623534/  1.752815, tr:  76.51%, tr_best:  77.12%, val:  62.50%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 20.86it/s]\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  1.622754/  1.753466, tr:  77.02%, tr_best:  77.12%, val:  64.58%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.29it/s]\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  1.622718/  1.754236, tr:  77.02%, tr_best:  77.12%, val:  64.17%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 21.05it/s]\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  1.624226/  1.754377, tr:  77.53%, tr_best:  77.53%, val:  62.50%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.16it/s]\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  1.620826/  1.751405, tr:  77.12%, tr_best:  77.53%, val:  64.58%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 20.83it/s]\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  1.620597/  1.753559, tr:  76.71%, tr_best:  77.53%, val:  62.50%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.41it/s]\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  1.620112/  1.752780, tr:  77.94%, tr_best:  77.94%, val:  62.92%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.28it/s]\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  1.622178/  1.750962, tr:  78.24%, tr_best:  78.24%, val:  63.75%, val_best:  64.58%: 100%|██████████| 62/62 [00:03<00:00, 20.13it/s]\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  1.619081/  1.753029, tr:  77.32%, tr_best:  78.24%, val:  63.75%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 21.46it/s]\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  1.618863/  1.753422, tr:  77.53%, tr_best:  78.24%, val:  62.08%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 20.89it/s]\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  1.619153/  1.751280, tr:  77.83%, tr_best:  78.24%, val:  62.92%, val_best:  64.58%: 100%|██████████| 62/62 [00:02<00:00, 20.91it/s]\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  1.619168/  1.748758, tr:  77.63%, tr_best:  78.24%, val:  65.42%, val_best:  65.42%: 100%|██████████| 62/62 [00:03<00:00, 20.41it/s]\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  1.620353/  1.748438, tr:  78.04%, tr_best:  78.24%, val:  64.17%, val_best:  65.42%: 100%|██████████| 62/62 [00:03<00:00, 20.31it/s]\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  1.617419/  1.747552, tr:  77.94%, tr_best:  78.24%, val:  63.75%, val_best:  65.42%: 100%|██████████| 62/62 [00:03<00:00, 19.68it/s]\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  1.617492/  1.747156, tr:  77.83%, tr_best:  78.24%, val:  64.58%, val_best:  65.42%: 100%|██████████| 62/62 [00:03<00:00, 20.16it/s]\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  1.618578/  1.747401, tr:  77.94%, tr_best:  78.24%, val:  62.50%, val_best:  65.42%: 100%|██████████| 62/62 [00:03<00:00, 20.03it/s]\n",
      "epoch-200 lr=['0.0010000'], tr/val_loss:  1.616887/  1.748711, tr:  78.24%, tr_best:  78.24%, val:  61.25%, val_best:  65.42%: 100%|██████████| 62/62 [00:03<00:00, 20.30it/s]\n",
      "epoch-201 lr=['0.0010000'], tr/val_loss:  1.615006/  1.748531, tr:  77.73%, tr_best:  78.24%, val:  63.75%, val_best:  65.42%: 100%|██████████| 62/62 [00:03<00:00, 19.99it/s]\n",
      "epoch-202 lr=['0.0010000'], tr/val_loss:  1.616946/  1.745597, tr:  78.45%, tr_best:  78.45%, val:  62.92%, val_best:  65.42%: 100%|██████████| 62/62 [00:03<00:00, 19.57it/s]\n",
      "epoch-203 lr=['0.0010000'], tr/val_loss:  1.615842/  1.745216, tr:  78.14%, tr_best:  78.45%, val:  65.00%, val_best:  65.42%: 100%|██████████| 62/62 [00:02<00:00, 20.80it/s]\n",
      "epoch-204 lr=['0.0010000'], tr/val_loss:  1.614453/  1.744236, tr:  77.73%, tr_best:  78.45%, val:  65.83%, val_best:  65.83%: 100%|██████████| 62/62 [00:03<00:00, 20.65it/s]\n",
      "epoch-205 lr=['0.0010000'], tr/val_loss:  1.613829/  1.743504, tr:  78.35%, tr_best:  78.45%, val:  64.58%, val_best:  65.83%: 100%|██████████| 62/62 [00:03<00:00, 19.82it/s]\n",
      "epoch-206 lr=['0.0010000'], tr/val_loss:  1.613417/  1.745275, tr:  78.45%, tr_best:  78.45%, val:  63.75%, val_best:  65.83%: 100%|██████████| 62/62 [00:03<00:00, 20.34it/s]\n",
      "epoch-207 lr=['0.0010000'], tr/val_loss:  1.613450/  1.743418, tr:  77.73%, tr_best:  78.45%, val:  64.17%, val_best:  65.83%: 100%|██████████| 62/62 [00:02<00:00, 20.70it/s]\n",
      "epoch-208 lr=['0.0010000'], tr/val_loss:  1.611764/  1.743225, tr:  78.65%, tr_best:  78.65%, val:  64.17%, val_best:  65.83%: 100%|██████████| 62/62 [00:02<00:00, 21.48it/s]\n",
      "epoch-209 lr=['0.0010000'], tr/val_loss:  1.611810/  1.744698, tr:  78.75%, tr_best:  78.75%, val:  64.17%, val_best:  65.83%: 100%|██████████| 62/62 [00:03<00:00, 20.42it/s]\n",
      "epoch-210 lr=['0.0010000'], tr/val_loss:  1.612234/  1.743390, tr:  78.24%, tr_best:  78.75%, val:  64.58%, val_best:  65.83%: 100%|██████████| 62/62 [00:03<00:00, 19.43it/s]\n",
      "epoch-211 lr=['0.0010000'], tr/val_loss:  1.612316/  1.741415, tr:  79.06%, tr_best:  79.06%, val:  65.83%, val_best:  65.83%: 100%|██████████| 62/62 [00:03<00:00, 20.63it/s]\n",
      "epoch-212 lr=['0.0010000'], tr/val_loss:  1.612703/  1.743087, tr:  77.63%, tr_best:  79.06%, val:  65.00%, val_best:  65.83%: 100%|██████████| 62/62 [00:03<00:00, 20.08it/s]\n",
      "epoch-213 lr=['0.0010000'], tr/val_loss:  1.612071/  1.742089, tr:  78.04%, tr_best:  79.06%, val:  67.08%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 20.16it/s]\n",
      "epoch-214 lr=['0.0010000'], tr/val_loss:  1.611744/  1.744773, tr:  78.04%, tr_best:  79.06%, val:  65.00%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 19.61it/s]\n",
      "epoch-215 lr=['0.0010000'], tr/val_loss:  1.610162/  1.744627, tr:  78.14%, tr_best:  79.06%, val:  65.00%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 20.25it/s]\n",
      "epoch-216 lr=['0.0010000'], tr/val_loss:  1.610825/  1.741153, tr:  77.53%, tr_best:  79.06%, val:  66.25%, val_best:  67.08%: 100%|██████████| 62/62 [00:02<00:00, 21.00it/s]\n",
      "epoch-217 lr=['0.0010000'], tr/val_loss:  1.609507/  1.742188, tr:  78.55%, tr_best:  79.06%, val:  65.42%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 19.20it/s]\n",
      "epoch-218 lr=['0.0010000'], tr/val_loss:  1.609172/  1.744237, tr:  78.55%, tr_best:  79.06%, val:  65.42%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 19.63it/s]\n",
      "epoch-219 lr=['0.0010000'], tr/val_loss:  1.610863/  1.744737, tr:  78.86%, tr_best:  79.06%, val:  66.25%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 19.97it/s]\n",
      "epoch-220 lr=['0.0010000'], tr/val_loss:  1.609431/  1.743474, tr:  78.35%, tr_best:  79.06%, val:  65.42%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 20.08it/s]\n",
      "epoch-221 lr=['0.0010000'], tr/val_loss:  1.608188/  1.741892, tr:  79.67%, tr_best:  79.67%, val:  65.83%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 19.62it/s]\n",
      "epoch-222 lr=['0.0010000'], tr/val_loss:  1.609174/  1.740544, tr:  79.88%, tr_best:  79.88%, val:  66.67%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 20.25it/s]\n",
      "epoch-223 lr=['0.0010000'], tr/val_loss:  1.608849/  1.740422, tr:  78.24%, tr_best:  79.88%, val:  66.25%, val_best:  67.08%: 100%|██████████| 62/62 [00:02<00:00, 20.96it/s]\n",
      "epoch-224 lr=['0.0010000'], tr/val_loss:  1.608778/  1.742180, tr:  78.65%, tr_best:  79.88%, val:  65.83%, val_best:  67.08%: 100%|██████████| 62/62 [00:02<00:00, 20.87it/s]\n",
      "epoch-225 lr=['0.0010000'], tr/val_loss:  1.610624/  1.740137, tr:  79.37%, tr_best:  79.88%, val:  65.83%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 20.53it/s]\n",
      "epoch-226 lr=['0.0010000'], tr/val_loss:  1.606122/  1.739755, tr:  79.06%, tr_best:  79.88%, val:  66.25%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 20.28it/s]\n",
      "epoch-227 lr=['0.0010000'], tr/val_loss:  1.606740/  1.738262, tr:  78.86%, tr_best:  79.88%, val:  65.42%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 20.65it/s]\n",
      "epoch-228 lr=['0.0010000'], tr/val_loss:  1.608033/  1.739208, tr:  79.26%, tr_best:  79.88%, val:  65.42%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 20.53it/s]\n",
      "epoch-229 lr=['0.0010000'], tr/val_loss:  1.606494/  1.740544, tr:  79.57%, tr_best:  79.88%, val:  67.08%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 20.00it/s]\n",
      "epoch-230 lr=['0.0010000'], tr/val_loss:  1.605785/  1.738803, tr:  78.75%, tr_best:  79.88%, val:  66.25%, val_best:  67.08%: 100%|██████████| 62/62 [00:02<00:00, 20.80it/s]\n",
      "epoch-231 lr=['0.0010000'], tr/val_loss:  1.607376/  1.740876, tr:  78.65%, tr_best:  79.88%, val:  65.83%, val_best:  67.08%: 100%|██████████| 62/62 [00:02<00:00, 21.24it/s]\n",
      "epoch-232 lr=['0.0010000'], tr/val_loss:  1.604388/  1.740383, tr:  78.96%, tr_best:  79.88%, val:  65.00%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 19.84it/s]\n",
      "epoch-233 lr=['0.0010000'], tr/val_loss:  1.604216/  1.740979, tr:  78.96%, tr_best:  79.88%, val:  65.00%, val_best:  67.08%: 100%|██████████| 62/62 [00:02<00:00, 21.05it/s]\n",
      "epoch-234 lr=['0.0010000'], tr/val_loss:  1.605916/  1.743948, tr:  78.65%, tr_best:  79.88%, val:  66.25%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 20.66it/s]\n",
      "epoch-235 lr=['0.0010000'], tr/val_loss:  1.604040/  1.738961, tr:  79.78%, tr_best:  79.88%, val:  65.42%, val_best:  67.08%: 100%|██████████| 62/62 [00:02<00:00, 20.73it/s]\n",
      "epoch-236 lr=['0.0010000'], tr/val_loss:  1.604017/  1.739551, tr:  79.57%, tr_best:  79.88%, val:  66.25%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 20.30it/s]\n",
      "epoch-237 lr=['0.0010000'], tr/val_loss:  1.604628/  1.740346, tr:  79.16%, tr_best:  79.88%, val:  65.42%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 20.20it/s]\n",
      "epoch-238 lr=['0.0010000'], tr/val_loss:  1.604629/  1.743475, tr:  79.78%, tr_best:  79.88%, val:  64.58%, val_best:  67.08%: 100%|██████████| 62/62 [00:02<00:00, 20.77it/s]\n",
      "epoch-239 lr=['0.0010000'], tr/val_loss:  1.604303/  1.741006, tr:  79.06%, tr_best:  79.88%, val:  64.58%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 20.49it/s]\n",
      "epoch-240 lr=['0.0010000'], tr/val_loss:  1.603446/  1.743171, tr:  79.37%, tr_best:  79.88%, val:  65.83%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 19.76it/s]\n",
      "epoch-241 lr=['0.0010000'], tr/val_loss:  1.603331/  1.741220, tr:  79.57%, tr_best:  79.88%, val:  65.00%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 19.96it/s]\n",
      "epoch-242 lr=['0.0010000'], tr/val_loss:  1.601982/  1.744513, tr:  79.57%, tr_best:  79.88%, val:  64.58%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 19.81it/s]\n",
      "epoch-243 lr=['0.0010000'], tr/val_loss:  1.602320/  1.743247, tr:  79.06%, tr_best:  79.88%, val:  65.00%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 19.96it/s]\n",
      "epoch-244 lr=['0.0010000'], tr/val_loss:  1.602427/  1.746146, tr:  79.98%, tr_best:  79.98%, val:  64.58%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 19.42it/s]\n",
      "epoch-245 lr=['0.0010000'], tr/val_loss:  1.600472/  1.744223, tr:  80.49%, tr_best:  80.49%, val:  65.42%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 19.91it/s]\n",
      "epoch-246 lr=['0.0010000'], tr/val_loss:  1.601141/  1.745626, tr:  80.80%, tr_best:  80.80%, val:  62.92%, val_best:  67.08%: 100%|██████████| 62/62 [00:03<00:00, 20.60it/s]\n",
      "epoch-247 lr=['0.0010000'], tr/val_loss:  1.602844/  1.743313, tr:  80.18%, tr_best:  80.80%, val:  62.92%, val_best:  67.08%: 100%|██████████| 62/62 [00:02<00:00, 20.85it/s]\n",
      "epoch-248 lr=['0.0010000'], tr/val_loss:  1.600232/  1.741921, tr:  80.90%, tr_best:  80.90%, val:  65.83%, val_best:  67.08%: 100%|██████████| 62/62 [00:02<00:00, 20.99it/s]\n",
      "epoch-249 iter_acc:  87.50%, lr=['0.0010000'], iter_loss:  1.579486:  98%|█████████▊| 61/62 [00:02<00:00, 29.68it/s]"
     ]
    }
   ],
   "source": [
    "### my_snn control board (Gesture) ########################\n",
    "decay = 0.5 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "const2 = True # trace 할거면 True, 안할거면 False\n",
    "\n",
    "unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "if const2 == True:\n",
    "    const2 = decay\n",
    "else:\n",
    "    const2 = 0.0\n",
    "\n",
    "DFA_on_True__BPTT_on_False_single_step_True = False # True # False \n",
    "\n",
    "wandb.init(project= f'my_snn {unique_name}',save_code=True)\n",
    "\n",
    "my_snn_system(  devices = \"3\",\n",
    "                single_step = DFA_on_True__BPTT_on_False_single_step_True, # True # False # DFA_on이랑 같이 가라\n",
    "                unique_name = run_name,\n",
    "                my_seed = 42,\n",
    "                TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "                BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 128, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "                # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = 'DVS_GESTURE_TONIC',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "                lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "\n",
    "                synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "                synapse_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "                cfg = ['M', 'M', 200, 200], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "                # cfg = ['M', 'M', 64], \n",
    "                # cfg = [64, 124, 64, 124],\n",
    "                # cfg = ['M','M',512], \n",
    "                # cfg = [512], \n",
    "                # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                # cfg = ['M','M',512],\n",
    "                # cfg = ['M',200],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = ['M','M',200,200],\n",
    "                # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = ['M',200,200],\n",
    "                # cfg = ['M','M',1024,512,256,128,64],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = [12], #fc\n",
    "                # cfg = [12, 'M', 48, 'M', 12], \n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [],        \n",
    "                \n",
    "                net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "                learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                epoch_num = 10000,\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "                BPTT_on = not DFA_on_True__BPTT_on_False_single_step_True,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                dvs_clipping = 5, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "                # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "                dvs_duration = 100_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "                # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "                # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "                DFA_on = DFA_on_True__BPTT_on_False_single_step_True, # True # False # single_step이랑 같이 켜야 됨.\n",
    "                OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용\n",
    "\n",
    "                exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "                extra_train_dataset = 0, \n",
    "\n",
    "                num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "                chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "                pin_memory = True, # True # False \n",
    "\n",
    "                UDA_on = False,  # DECREPATED # uda\n",
    "                alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                bias = True, # True # False \n",
    "                ) \n",
    "\n",
    "# num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# num_workers = batch_size / num_GPU\n",
    "# num_workers = batch_size / num_CPU\n",
    "\n",
    "# sigmoid와 BN이 있어야 잘된다.\n",
    "# average pooling  \n",
    "# 이 낫다. \n",
    "\n",
    "# nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# # 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# # wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# run_name = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'random', # 'random', 'bayes'\n",
    "#     'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         \"learning_rate\": {\"values\": [0.001]}, #0.00936191669529645\n",
    "#         \"BATCH\": {\"values\": [16]},\n",
    "#         \"decay\": {\"values\": [0.25]},\n",
    "#         \"IMAGE_SIZE\": {\"values\": [128]},\n",
    "#         \"TIME\": {\"values\": [10]},\n",
    "#         \"epoch_num\": {\"values\": [200]},\n",
    "#         \"dvs_duration\": {\"values\": [25_000,50_000,100_000]},\n",
    "#         \"dvs_clipping\": {\"values\": [1,2,3,4,5]},\n",
    "#         \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "#         \"const2\": {\"values\": [False]},\n",
    "#         \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "#         \"DFA_on\": {\"values\": [False]},\n",
    "#         \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "#         \"cfg\": {\"values\": [['M','M',200,200]]},\n",
    "#         \"e_transport_swap\": {\"values\": [0]},\n",
    "#         \"e_transport_swap_tr\": {\"values\": [0]},\n",
    "#         \"drop_rate\": {\"values\": [0.0]}, # \"drop_rate\": {\"values\": [0.25,0.5,0.75]}, #\"drop_rate\": {\"min\": 0.25, \"max\": 0.75},\n",
    "#         \"exclude_class\": {\"values\": [True]},\n",
    "#         \"merge_polarities\": {\"values\": [False]},\n",
    "#         \"lif_layer_v_reset\": {\"values\": [10000]},\n",
    "#         \"lif_layer_sg_width\": {\"values\": [3.555718888923306]},\n",
    "#         \"e_transport_swap_coin\": {\"values\": [1]},\n",
    "#         \"lif_layer_v_threshold\": {\"values\": [0.720291189014991]},\n",
    "#         \"scheduler_name\": {\"values\": ['no']},  # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "#         \"denoise_on\": {\"values\": [True,False]}, \n",
    "#         \"I_wanna_sweep_at_this_epoch\": {\"values\": [-1]}, \n",
    "#         \"dvs_duration_domain\": {\"values\": [[]]}, \n",
    "#         \"dvs_relative_timestep\": {\"values\": [[False]]}, \n",
    "#         \"extra_train_dataset\": {\"values\": [0]}, \n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     unique_name = unique_name_hyper ## 이거 설정하면 새로운 경로에 모두 save\n",
    "    \n",
    "#     wandb.init(save_code = True)\n",
    "#     learning_rate  =  wandb.config.learning_rate\n",
    "#     BATCH  =  wandb.config.BATCH\n",
    "#     decay  =  wandb.config.decay\n",
    "#     IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "#     TIME  =  wandb.config.TIME\n",
    "#     epoch_num  =  wandb.config.epoch_num \n",
    "#     dvs_duration  =  wandb.config.dvs_duration\n",
    "#     dvs_clipping  =  wandb.config.dvs_clipping\n",
    "#     which_data  =  wandb.config.which_data\n",
    "#     const2  =  wandb.config.const2\n",
    "#     surrogate  =  wandb.config.surrogate\n",
    "#     DFA_on  =  wandb.config.DFA_on\n",
    "#     OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on\n",
    "#     cfg  =  wandb.config.cfg\n",
    "#     e_transport_swap  =  wandb.config.e_transport_swap\n",
    "#     e_transport_swap_tr  =  wandb.config.e_transport_swap_tr\n",
    "#     drop_rate  =  wandb.config.drop_rate\n",
    "#     exclude_class  =  wandb.config.exclude_class\n",
    "#     merge_polarities  =  wandb.config.merge_polarities\n",
    "#     lif_layer_v_reset  =  wandb.config.lif_layer_v_reset\n",
    "#     lif_layer_sg_width  =  wandb.config.lif_layer_sg_width\n",
    "#     e_transport_swap_coin  =  wandb.config.e_transport_swap_coin\n",
    "#     lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold\n",
    "#     scheduler_name  =  wandb.config.scheduler_name\n",
    "#     denoise_on  =  wandb.config.denoise_on\n",
    "#     I_wanna_sweep_at_this_epoch  =  wandb.config.I_wanna_sweep_at_this_epoch\n",
    "#     dvs_duration_domain  =  wandb.config.dvs_duration_domain\n",
    "#     dvs_relative_timestep  =  wandb.config.dvs_relative_timestep\n",
    "#     extra_train_dataset  =  wandb.config.extra_train_dataset\n",
    "#     if const2 == True:\n",
    "#         const2 = decay\n",
    "#     else:\n",
    "#         const2 = 0.0\n",
    "\n",
    "#     my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = TIME , # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "#                 BATCH = BATCH, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = IMAGE_SIZE, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "#                 #pmnist는 28로 해야 됨. 나머지는 바꿔도 돌아는 감.\n",
    "\n",
    "#                 # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "#                 which_data = which_data,\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = lif_layer_v_threshold,  # 10000이상으로 하면 NDA LIF 씀. #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = lif_layer_v_reset, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "#                 lif_layer_sg_width = lif_layer_sg_width, # # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "#                 synapse_conv_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_conv_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "#                 synapse_fc_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_fc_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "#                 # cfg = [64, 64],\n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 cfg = cfg,\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [], \n",
    "                \n",
    "#                 net_print = True, # True # False # True로 하길 추천\n",
    "#                 weight_count_print = False, # True # False\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 learning_rate = learning_rate, # default 0.001  # ottt 0.1 # nda 0.001 \n",
    "#                 epoch_num = epoch_num,\n",
    "#                 verbose_interval = 999999999, #숫자 크게 하면 꺼짐 #걍 중간중간 iter에서 끊어서 출력\n",
    "#                 validation_interval =  999999999,#999999999, #숫자 크게 하면 에포크 마지막 iter 때 val 함\n",
    "\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = surrogate, # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "#                 gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "#                 BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = scheduler_name, # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False,   # True # False \n",
    "#                 # 지원 DATASET: cifar10, mnist\n",
    "\n",
    "#                 nda_net = False,   # True # False\n",
    "\n",
    "#                 domain_il_epoch = 0, # over 0, then domain il mode on # pmnist 쓸거면 HLOP 코드보고 더 디벨롭하셈. 지금 개발 hold함.\n",
    "                \n",
    "#                 dvs_clipping = dvs_clipping, # 숫자만큼 크면 spike 아니면 걍 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "\n",
    "#                 dvs_duration = dvs_duration, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "#                 # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "\n",
    "#                 DFA_on = DFA_on, # True # False # residual은 dfa지원안함.\n",
    "#                 OTTT_input_trace_on = OTTT_input_trace_on, # True # False # 맨 처음 input에 trace 적용\n",
    "                 \n",
    "#                 e_transport_swap = e_transport_swap, # 1 이상이면 해당 숫자 에포크만큼 val_acc_best가 변화가 없으면 e_transport scheme (BP vs DFA) swap\n",
    "#                 e_transport_swap_tr = e_transport_swap_tr, # 1 이상이면 해당 숫자 에포크만큼 tr_acc_best가 변화가 없으면 e_transport scheme (BP vs DFA) swap\n",
    "#                 e_transport_swap_coin = e_transport_swap_coin, # swap할 수 있는 coin 개수\n",
    "                    \n",
    "#                 drop_rate = drop_rate,\n",
    "\n",
    "#                 exclude_class = exclude_class, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "#                 merge_polarities = merge_polarities, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "#                 denoise_on = denoise_on,\n",
    "\n",
    "#                 I_wanna_sweep_at_this_epoch = I_wanna_sweep_at_this_epoch,\n",
    "#                 dvs_duration_domain = dvs_duration_domain,\n",
    "#                 dvs_relative_timestep = dvs_relative_timestep, # True # False \n",
    "\n",
    "#                 extra_train_dataset = extra_train_dataset,\n",
    "\n",
    "#                 num_workers = 2,\n",
    "#                 chaching_on = True,\n",
    "#                 pin_memory = True, # True # False\n",
    "#                     ) \n",
    "#     # sigmoid와 BN이 있어야 잘된다.\n",
    "#     # average pooling\n",
    "#     # 이 낫다. \n",
    "    \n",
    "#     # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d53c56eaeb842088e9a19c11ceb382b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.446 MB of 0.446 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▃▆▂▅▅▄▅█▆█▄▆▆▇▇██▇▇▇▇██████████▇██████</td></tr><tr><td>summary_val_acc</td><td>▁▂▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇██</td></tr><tr><td>tr_acc</td><td>▁▂▃▄▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▆▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▃▃▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇██</td></tr><tr><td>val_loss</td><td>█▇▅▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>299</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98979</td></tr><tr><td>tr_epoch_loss</td><td>1.50874</td></tr><tr><td>val_acc_best</td><td>0.85417</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>1.7061</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunny-aardvark-6749</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j3tg799j' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j3tg799j</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250424_193044-j3tg799j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import json\n",
    "# run_name = 'main_FINAL_TEST'\n",
    "\n",
    "# unique_name = run_name\n",
    "# def pad_array_to_match_length(array1, array2):\n",
    "#     if len(array1) > len(array2):\n",
    "#         padded_array2 = np.pad(array2, (0, len(array1) - len(array2)), 'constant')\n",
    "#         return array1, padded_array2\n",
    "#     elif len(array2) > len(array1):\n",
    "#         padded_array1 = np.pad(array1, (0, len(array2) - len(array1)), 'constant')\n",
    "#         return padded_array1, array2\n",
    "#     else:\n",
    "#         return array1, array2\n",
    "# def load_hyperparameters(filename=f'result_save/hyperparameters_{unique_name}.json'):\n",
    "#     with open(filename, 'r') as f:\n",
    "#         return json.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# current_time = '20240628_110116'\n",
    "# base_name = f'{current_time}'\n",
    "# iter_acc_file_name = f'result_save/{base_name}_iter_acc_array_{unique_name}.npy'\n",
    "# val_acc_file_name = f'result_save/{base_name}_val_acc_now_array_{unique_name}.npy'\n",
    "# hyperparameters_file_name = f'result_save/{base_name}_hyperparameters_{unique_name}.json'\n",
    "\n",
    "# ### if you want to just see most recent train and val acc###########################\n",
    "# iter_acc_file_name = f'result_save/iter_acc_array_{unique_name}.npy'\n",
    "# tr_acc_file_name = f'result_save/tr_acc_array_{unique_name}.npy'\n",
    "# val_acc_file_name = f'result_save/val_acc_now_array_{unique_name}.npy'\n",
    "# hyperparameters_file_name = f'result_save/hyperparameters_{unique_name}.json'\n",
    "\n",
    "# loaded_iter_acc_array = np.load(iter_acc_file_name)*100\n",
    "# loaded_tr_acc_array = np.load(tr_acc_file_name)*100\n",
    "# loaded_val_acc_array = np.load(val_acc_file_name)*100\n",
    "# hyperparameters = load_hyperparameters(hyperparameters_file_name)\n",
    "\n",
    "# loaded_iter_acc_array, loaded_val_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_val_acc_array)\n",
    "# loaded_iter_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_tr_acc_array)\n",
    "# loaded_val_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_val_acc_array, loaded_tr_acc_array)\n",
    "\n",
    "# top_iter_acc = np.max(loaded_iter_acc_array)\n",
    "# top_tr_acc = np.max(loaded_tr_acc_array)\n",
    "# top_val_acc = np.max(loaded_val_acc_array)\n",
    "\n",
    "# which_data = hyperparameters['which_data']\n",
    "# BPTT_on = hyperparameters['BPTT_on']\n",
    "# current_epoch = hyperparameters['current epoch']\n",
    "# surrogate = hyperparameters['surrogate']\n",
    "# cfg = hyperparameters['cfg']\n",
    "# tdBN_on = hyperparameters['tdBN_on']\n",
    "# BN_on = hyperparameters['BN_on']\n",
    "\n",
    "\n",
    "# iterations = np.arange(len(loaded_iter_acc_array))\n",
    "\n",
    "# # 그래프 그리기\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(iterations, loaded_iter_acc_array, label='Iter Accuracy', color='g', alpha=0.2)\n",
    "# plt.plot(iterations, loaded_tr_acc_array, label='Training Accuracy', color='b')\n",
    "# plt.plot(iterations, loaded_val_acc_array, label='Validation Accuracy', color='r')\n",
    "\n",
    "# # # 텍스트 추가\n",
    "# # plt.text(0.05, 0.95, f'Top Training Accuracy: {100*top_iter_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='blue')\n",
    "# # plt.text(0.05, 0.90, f'Top Validation Accuracy: {100*top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='red')\n",
    "# # 텍스트 추가\n",
    "# plt.text(0.5, 0.10, f'Top Training Accuracy: {top_tr_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='blue')\n",
    "# plt.text(0.5, 0.05, f'Top Validation Accuracy: {top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='red')\n",
    "\n",
    "# plt.xlabel('Iterations')\n",
    "# plt.ylabel('Accuracy [%]')\n",
    "\n",
    "# # 그래프 제목에 하이퍼파라미터 정보 추가\n",
    "# title = f'Training and Validation Accuracy over Iterations\\n\\nData: {which_data}, BPTT: {\"On\" if BPTT_on else \"Off\"}, Current Epoch: {current_epoch}, Surrogate: {surrogate},\\nCFG: {cfg}, tdBN: {\"On\" if tdBN_on else \"Off\"}, BN: {\"On\" if BN_on else \"Off\"}'\n",
    "\n",
    "# plt.title(title)\n",
    "\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.xlim(0)  # x축을 0부터 시작\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
