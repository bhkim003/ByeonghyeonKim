{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4271/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        smallest_now_T = 99999\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                if epoch == 0 and now_T < smallest_now_T:\n",
    "                    smallest_now_T = now_T\n",
    "                    print(f'smallest_now_T updated: {smallest_now_T}')\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                if now_T < now_time_steps:\n",
    "                    # Î∂ÄÏ°±Ìïú timestep Í∞úÏàò\n",
    "                    diff = now_time_steps - now_T\n",
    "\n",
    "                    # ÎßàÏßÄÎßâ timestep Î≥µÏÇ¨ (shape: [B, 1, C, H, W])\n",
    "                    last_frame = inputs[:, -1:, :, :, :]\n",
    "\n",
    "                    # diffÎßåÌÅº repeatÌïòÏó¨ Ìå®Îî© Íµ¨ÏÑ±\n",
    "                    pad_frames = last_frame.repeat(1, diff, 1, 1, 1)\n",
    "\n",
    "                    # ÏõêÎ≥∏ + Ìå®Îî© Í≤∞Ìï©\n",
    "                    inputs = torch.cat([inputs, pad_frames], dim=1)\n",
    "                else:\n",
    "                    # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                    start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                    # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                    inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            smallest_now_T_val = 99999\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            if epoch == 0 and now_T < smallest_now_T_val:\n",
    "                                smallest_now_T_val = now_T\n",
    "                                print(f'smallest_now_T_val updated: {smallest_now_T_val}')\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "\n",
    "                            if now_T < now_time_steps:\n",
    "                                # Î∂ÄÏ°±Ìïú timestep Í∞úÏàò\n",
    "                                diff = now_time_steps - now_T\n",
    "\n",
    "                                # ÎßàÏßÄÎßâ timestep Î≥µÏÇ¨ (shape: [B, 1, C, H, W])\n",
    "                                last_frame = inputs_val[:, -1:, :, :, :]\n",
    "\n",
    "                                # diffÎßåÌÅº repeatÌïòÏó¨ Ìå®Îî© Íµ¨ÏÑ±\n",
    "                                pad_frames = last_frame.repeat(1, diff, 1, 1, 1)\n",
    "\n",
    "                                # ÏõêÎ≥∏ + Ìå®Îî© Í≤∞Ìï©\n",
    "                                inputs_val = torch.cat([inputs_val, pad_frames], dim=1)\n",
    "                            else:\n",
    "                                pass\n",
    "                            \n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "                \n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.8\n",
    "                elif epoch > 150:\n",
    "                    assert val_acc_best > 0.88\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"1\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.25,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-9,-9],[-9,-9],[-8,-8]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mffikdvu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_154424-mffikdvu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mffikdvu' target=\"_blank\">earnest-sweep-73</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mffikdvu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mffikdvu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251117_154432_717', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = df820968b21bc2412e6634ebdd407347\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 67\n",
      "fc layer 1 self.abs_max_out: 385.0\n",
      "lif layer 1 self.abs_max_v: 385.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 383.0\n",
      "lif layer 2 self.abs_max_v: 383.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 65.0\n",
      "lif layer 1 self.abs_max_v: 447.5\n",
      "lif layer 2 self.abs_max_v: 531.5\n",
      "fc layer 3 self.abs_max_out: 165.0\n",
      "lif layer 1 self.abs_max_v: 453.0\n",
      "lif layer 2 self.abs_max_v: 551.0\n",
      "fc layer 1 self.abs_max_out: 420.0\n",
      "lif layer 1 self.abs_max_v: 454.5\n",
      "fc layer 2 self.abs_max_out: 425.0\n",
      "lif layer 2 self.abs_max_v: 700.5\n",
      "lif layer 1 self.abs_max_v: 539.0\n",
      "fc layer 2 self.abs_max_out: 432.0\n",
      "lif layer 2 self.abs_max_v: 731.5\n",
      "smallest_now_T updated: 60\n",
      "fc layer 1 self.abs_max_out: 570.0\n",
      "lif layer 1 self.abs_max_v: 570.0\n",
      "fc layer 2 self.abs_max_out: 443.0\n",
      "lif layer 1 self.abs_max_v: 617.0\n",
      "fc layer 1 self.abs_max_out: 591.0\n",
      "lif layer 1 self.abs_max_v: 725.5\n",
      "fc layer 2 self.abs_max_out: 477.0\n",
      "smallest_now_T updated: 45\n",
      "fc layer 1 self.abs_max_out: 634.0\n",
      "fc layer 1 self.abs_max_out: 653.0\n",
      "fc layer 3 self.abs_max_out: 169.0\n",
      "fc layer 1 self.abs_max_out: 707.0\n",
      "fc layer 2 self.abs_max_out: 520.0\n",
      "lif layer 2 self.abs_max_v: 750.0\n",
      "lif layer 2 self.abs_max_v: 761.0\n",
      "lif layer 1 self.abs_max_v: 761.5\n",
      "lif layer 2 self.abs_max_v: 763.0\n",
      "fc layer 1 self.abs_max_out: 722.0\n",
      "lif layer 1 self.abs_max_v: 962.0\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "fc layer 3 self.abs_max_out: 230.0\n",
      "lif layer 2 self.abs_max_v: 807.0\n",
      "fc layer 1 self.abs_max_out: 786.0\n",
      "fc layer 1 self.abs_max_out: 874.0\n",
      "lif layer 1 self.abs_max_v: 984.5\n",
      "lif layer 2 self.abs_max_v: 836.0\n",
      "fc layer 2 self.abs_max_out: 548.0\n",
      "lif layer 2 self.abs_max_v: 897.0\n",
      "smallest_now_T updated: 37\n",
      "fc layer 2 self.abs_max_out: 551.0\n",
      "fc layer 1 self.abs_max_out: 890.0\n",
      "lif layer 2 self.abs_max_v: 919.0\n",
      "fc layer 1 self.abs_max_out: 917.0\n",
      "fc layer 1 self.abs_max_out: 1004.0\n",
      "lif layer 1 self.abs_max_v: 1004.0\n",
      "fc layer 1 self.abs_max_out: 1120.0\n",
      "lif layer 1 self.abs_max_v: 1120.0\n",
      "fc layer 1 self.abs_max_out: 1177.0\n",
      "lif layer 1 self.abs_max_v: 1177.0\n",
      "lif layer 1 self.abs_max_v: 1397.5\n",
      "fc layer 1 self.abs_max_out: 1202.0\n",
      "lif layer 1 self.abs_max_v: 1402.0\n",
      "lif layer 1 self.abs_max_v: 1609.0\n",
      "smallest_now_T updated: 34\n",
      "lif layer 1 self.abs_max_v: 1666.0\n",
      "lif layer 1 self.abs_max_v: 1773.5\n",
      "lif layer 2 self.abs_max_v: 933.5\n",
      "lif layer 2 self.abs_max_v: 973.0\n",
      "lif layer 1 self.abs_max_v: 1779.0\n",
      "fc layer 2 self.abs_max_out: 600.0\n",
      "smallest_now_T updated: 30\n",
      "fc layer 1 self.abs_max_out: 1209.0\n",
      "fc layer 2 self.abs_max_out: 606.0\n",
      "fc layer 2 self.abs_max_out: 608.0\n",
      "fc layer 2 self.abs_max_out: 659.0\n",
      "fc layer 1 self.abs_max_out: 1237.0\n",
      "fc layer 1 self.abs_max_out: 1431.0\n",
      "fc layer 1 self.abs_max_out: 1510.0\n",
      "lif layer 1 self.abs_max_v: 1936.5\n",
      "lif layer 1 self.abs_max_v: 2024.5\n",
      "smallest_now_T updated: 26\n",
      "smallest_now_T updated: 25\n",
      "lif layer 1 self.abs_max_v: 2031.5\n",
      "fc layer 3 self.abs_max_out: 231.0\n",
      "fc layer 3 self.abs_max_out: 233.0\n",
      "fc layer 3 self.abs_max_out: 243.0\n",
      "fc layer 3 self.abs_max_out: 256.0\n",
      "lif layer 2 self.abs_max_v: 1043.5\n",
      "lif layer 2 self.abs_max_v: 1121.0\n",
      "fc layer 2 self.abs_max_out: 661.0\n",
      "fc layer 2 self.abs_max_out: 669.0\n",
      "fc layer 2 self.abs_max_out: 671.0\n",
      "fc layer 2 self.abs_max_out: 677.0\n",
      "fc layer 2 self.abs_max_out: 748.0\n",
      "fc layer 2 self.abs_max_out: 750.0\n",
      "lif layer 1 self.abs_max_v: 2098.5\n",
      "lif layer 1 self.abs_max_v: 2213.5\n",
      "fc layer 3 self.abs_max_out: 257.0\n",
      "fc layer 3 self.abs_max_out: 264.0\n",
      "lif layer 1 self.abs_max_v: 2333.0\n",
      "lif layer 1 self.abs_max_v: 2505.5\n",
      "lif layer 1 self.abs_max_v: 2735.0\n",
      "lif layer 2 self.abs_max_v: 1138.5\n",
      "fc layer 3 self.abs_max_out: 266.0\n",
      "fc layer 3 self.abs_max_out: 289.0\n",
      "fc layer 3 self.abs_max_out: 290.0\n",
      "fc layer 3 self.abs_max_out: 345.0\n",
      "smallest_now_T_val updated: 62\n",
      "smallest_now_T_val updated: 51\n",
      "smallest_now_T_val updated: 50\n",
      "smallest_now_T_val updated: 49\n",
      "smallest_now_T_val updated: 40\n",
      "smallest_now_T_val updated: 25\n",
      "fc layer 1 self.abs_max_out: 1600.0\n",
      "fc layer 1 self.abs_max_out: 1603.0\n",
      "lif layer 1 self.abs_max_v: 2806.5\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.957598/  2.017641, val:  41.25%, val_best:  41.25%, tr:  80.90%, tr_best:  80.90%, epoch time: 40.83 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9410%\n",
      "layer   2  Sparsity: 79.0312%\n",
      "layer   3  Sparsity: 76.1823%\n",
      "total_backward_count 4895 real_backward_count 1776  36.282%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 1632.0\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "fc layer 3 self.abs_max_out: 353.0\n",
      "fc layer 3 self.abs_max_out: 359.0\n",
      "fc layer 2 self.abs_max_out: 762.0\n",
      "fc layer 2 self.abs_max_out: 763.0\n",
      "fc layer 2 self.abs_max_out: 783.0\n",
      "fc layer 2 self.abs_max_out: 803.0\n",
      "fc layer 3 self.abs_max_out: 405.0\n",
      "lif layer 1 self.abs_max_v: 2829.5\n",
      "fc layer 1 self.abs_max_out: 1909.0\n",
      "fc layer 1 self.abs_max_out: 1983.0\n",
      "lif layer 1 self.abs_max_v: 2832.0\n",
      "lif layer 1 self.abs_max_v: 3069.0\n",
      "lif layer 1 self.abs_max_v: 3317.5\n",
      "lif layer 1 self.abs_max_v: 3364.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.867208/  2.043929, val:  30.83%, val_best:  41.25%, tr:  89.07%, tr_best:  89.07%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9541%\n",
      "layer   2  Sparsity: 83.5728%\n",
      "layer   3  Sparsity: 77.8098%\n",
      "total_backward_count 9790 real_backward_count 3101  31.675%\n",
      "fc layer 1 self.abs_max_out: 2043.0\n",
      "lif layer 1 self.abs_max_v: 3541.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.857092/  1.977839, val:  49.17%, val_best:  49.17%, tr:  90.30%, tr_best:  90.30%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9565%\n",
      "layer   2  Sparsity: 82.9632%\n",
      "layer   3  Sparsity: 78.9760%\n",
      "total_backward_count 14685 real_backward_count 4393  29.915%\n",
      "fc layer 1 self.abs_max_out: 2103.0\n",
      "fc layer 2 self.abs_max_out: 804.0\n",
      "fc layer 2 self.abs_max_out: 823.0\n",
      "fc layer 1 self.abs_max_out: 2153.0\n",
      "lif layer 1 self.abs_max_v: 3667.5\n",
      "lif layer 1 self.abs_max_v: 3773.0\n",
      "fc layer 1 self.abs_max_out: 2165.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.874900/  1.964995, val:  40.83%, val_best:  49.17%, tr:  90.81%, tr_best:  90.81%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9315%\n",
      "layer   2  Sparsity: 82.9250%\n",
      "layer   3  Sparsity: 79.3167%\n",
      "total_backward_count 19580 real_backward_count 5623  28.718%\n",
      "fc layer 2 self.abs_max_out: 838.0\n",
      "fc layer 2 self.abs_max_out: 850.0\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "lif layer 2 self.abs_max_v: 1296.0\n",
      "fc layer 1 self.abs_max_out: 2168.0\n",
      "lif layer 1 self.abs_max_v: 3833.0\n",
      "fc layer 3 self.abs_max_out: 416.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.850811/  1.943470, val:  50.83%, val_best:  50.83%, tr:  91.83%, tr_best:  91.83%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9776%\n",
      "layer   2  Sparsity: 82.9984%\n",
      "layer   3  Sparsity: 79.3645%\n",
      "total_backward_count 24475 real_backward_count 6806  27.808%\n",
      "fc layer 1 self.abs_max_out: 2254.0\n",
      "lif layer 1 self.abs_max_v: 3839.0\n",
      "lif layer 1 self.abs_max_v: 3864.5\n",
      "lif layer 1 self.abs_max_v: 3948.0\n",
      "fc layer 3 self.abs_max_out: 443.0\n",
      "fc layer 1 self.abs_max_out: 2762.0\n",
      "lif layer 1 self.abs_max_v: 4306.0\n",
      "lif layer 1 self.abs_max_v: 4474.0\n",
      "lif layer 1 self.abs_max_v: 4489.0\n",
      "lif layer 1 self.abs_max_v: 4624.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.836023/  1.985206, val:  50.42%, val_best:  50.83%, tr:  91.42%, tr_best:  91.83%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9575%\n",
      "layer   2  Sparsity: 83.8158%\n",
      "layer   3  Sparsity: 81.0605%\n",
      "total_backward_count 29370 real_backward_count 7985  27.188%\n",
      "lif layer 2 self.abs_max_v: 1324.0\n",
      "fc layer 2 self.abs_max_out: 856.0\n",
      "fc layer 1 self.abs_max_out: 2853.0\n",
      "fc layer 1 self.abs_max_out: 2919.0\n",
      "lif layer 1 self.abs_max_v: 4683.0\n",
      "lif layer 1 self.abs_max_v: 4871.5\n",
      "lif layer 1 self.abs_max_v: 4894.0\n",
      "fc layer 2 self.abs_max_out: 859.0\n",
      "fc layer 2 self.abs_max_out: 866.0\n",
      "fc layer 2 self.abs_max_out: 887.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.857908/  1.970584, val:  48.75%, val_best:  50.83%, tr:  91.62%, tr_best:  91.83%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9918%\n",
      "layer   2  Sparsity: 83.7092%\n",
      "layer   3  Sparsity: 81.3958%\n",
      "total_backward_count 34265 real_backward_count 9107  26.578%\n",
      "fc layer 2 self.abs_max_out: 910.0\n",
      "fc layer 3 self.abs_max_out: 446.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.854570/  1.965650, val:  52.08%, val_best:  52.08%, tr:  91.73%, tr_best:  91.83%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9293%\n",
      "layer   2  Sparsity: 84.3164%\n",
      "layer   3  Sparsity: 81.6551%\n",
      "total_backward_count 39160 real_backward_count 10223  26.106%\n",
      "fc layer 1 self.abs_max_out: 3102.0\n",
      "lif layer 1 self.abs_max_v: 4979.5\n",
      "lif layer 1 self.abs_max_v: 5193.0\n",
      "fc layer 2 self.abs_max_out: 941.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.847515/  1.945422, val:  46.25%, val_best:  52.08%, tr:  91.93%, tr_best:  91.93%, epoch time: 39.54 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 78.9451%\n",
      "layer   2  Sparsity: 83.9465%\n",
      "layer   3  Sparsity: 82.9781%\n",
      "total_backward_count 44055 real_backward_count 11347  25.756%\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.845625/  2.007901, val:  40.42%, val_best:  52.08%, tr:  92.75%, tr_best:  92.75%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9651%\n",
      "layer   2  Sparsity: 84.2258%\n",
      "layer   3  Sparsity: 82.5910%\n",
      "total_backward_count 48950 real_backward_count 12389  25.309%\n",
      "fc layer 3 self.abs_max_out: 457.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.861082/  1.955715, val:  45.00%, val_best:  52.08%, tr:  92.75%, tr_best:  92.75%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9447%\n",
      "layer   2  Sparsity: 84.6820%\n",
      "layer   3  Sparsity: 82.0322%\n",
      "total_backward_count 53845 real_backward_count 13381  24.851%\n",
      "lif layer 2 self.abs_max_v: 1326.0\n",
      "lif layer 2 self.abs_max_v: 1347.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.850606/  1.950579, val:  48.33%, val_best:  52.08%, tr:  93.77%, tr_best:  93.77%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9906%\n",
      "layer   2  Sparsity: 83.9062%\n",
      "layer   3  Sparsity: 82.4336%\n",
      "total_backward_count 58740 real_backward_count 14350  24.430%\n",
      "lif layer 2 self.abs_max_v: 1466.5\n",
      "fc layer 3 self.abs_max_out: 470.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.849272/  1.957684, val:  42.92%, val_best:  52.08%, tr:  94.38%, tr_best:  94.38%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9725%\n",
      "layer   2  Sparsity: 84.0034%\n",
      "layer   3  Sparsity: 83.0673%\n",
      "total_backward_count 63635 real_backward_count 15282  24.015%\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.863027/  1.958635, val:  47.92%, val_best:  52.08%, tr:  93.97%, tr_best:  94.38%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9333%\n",
      "layer   2  Sparsity: 83.7899%\n",
      "layer   3  Sparsity: 82.7820%\n",
      "total_backward_count 68530 real_backward_count 16215  23.661%\n",
      "fc layer 2 self.abs_max_out: 953.0\n",
      "fc layer 2 self.abs_max_out: 963.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.837053/  1.937981, val:  49.17%, val_best:  52.08%, tr:  94.48%, tr_best:  94.48%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9897%\n",
      "layer   2  Sparsity: 83.1506%\n",
      "layer   3  Sparsity: 82.6162%\n",
      "total_backward_count 73425 real_backward_count 17074  23.254%\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.863771/  1.966561, val:  54.58%, val_best:  54.58%, tr:  93.36%, tr_best:  94.48%, epoch time: 41.19 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 78.9617%\n",
      "layer   2  Sparsity: 83.7568%\n",
      "layer   3  Sparsity: 84.2975%\n",
      "total_backward_count 78320 real_backward_count 18017  23.004%\n",
      "fc layer 2 self.abs_max_out: 965.0\n",
      "fc layer 2 self.abs_max_out: 974.0\n",
      "fc layer 2 self.abs_max_out: 983.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.866463/  1.940390, val:  54.17%, val_best:  54.58%, tr:  93.05%, tr_best:  94.48%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9233%\n",
      "layer   2  Sparsity: 84.0339%\n",
      "layer   3  Sparsity: 84.2442%\n",
      "total_backward_count 83215 real_backward_count 18969  22.795%\n",
      "fc layer 2 self.abs_max_out: 995.0\n",
      "fc layer 2 self.abs_max_out: 1010.0\n",
      "lif layer 2 self.abs_max_v: 1479.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.865519/  1.966618, val:  52.92%, val_best:  54.58%, tr:  94.28%, tr_best:  94.48%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9149%\n",
      "layer   2  Sparsity: 83.4587%\n",
      "layer   3  Sparsity: 83.9152%\n",
      "total_backward_count 88110 real_backward_count 19889  22.573%\n",
      "fc layer 2 self.abs_max_out: 1031.0\n",
      "fc layer 2 self.abs_max_out: 1039.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.849131/  1.937291, val:  54.58%, val_best:  54.58%, tr:  95.40%, tr_best:  95.40%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 79.0356%\n",
      "layer   2  Sparsity: 83.3503%\n",
      "layer   3  Sparsity: 82.9682%\n",
      "total_backward_count 93005 real_backward_count 20768  22.330%\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.833056/  1.959943, val:  50.42%, val_best:  54.58%, tr:  94.99%, tr_best:  95.40%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9660%\n",
      "layer   2  Sparsity: 83.9086%\n",
      "layer   3  Sparsity: 82.7981%\n",
      "total_backward_count 97900 real_backward_count 21598  22.061%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.832303/  1.930252, val:  54.17%, val_best:  54.58%, tr:  95.91%, tr_best:  95.91%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9306%\n",
      "layer   2  Sparsity: 84.0259%\n",
      "layer   3  Sparsity: 82.8349%\n",
      "total_backward_count 102795 real_backward_count 22408  21.799%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.829366/  1.950784, val:  54.17%, val_best:  54.58%, tr:  94.99%, tr_best:  95.91%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9336%\n",
      "layer   2  Sparsity: 84.0036%\n",
      "layer   3  Sparsity: 82.6876%\n",
      "total_backward_count 107690 real_backward_count 23240  21.580%\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.835113/  1.911758, val:  55.00%, val_best:  55.00%, tr:  96.22%, tr_best:  96.22%, epoch time: 40.71 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9253%\n",
      "layer   2  Sparsity: 84.2880%\n",
      "layer   3  Sparsity: 82.0189%\n",
      "total_backward_count 112585 real_backward_count 24038  21.351%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.815873/  1.936261, val:  59.17%, val_best:  59.17%, tr:  94.79%, tr_best:  96.22%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9651%\n",
      "layer   2  Sparsity: 83.5316%\n",
      "layer   3  Sparsity: 81.5169%\n",
      "total_backward_count 117480 real_backward_count 24841  21.145%\n",
      "lif layer 2 self.abs_max_v: 1540.0\n",
      "lif layer 2 self.abs_max_v: 1547.0\n",
      "lif layer 2 self.abs_max_v: 1598.5\n",
      "lif layer 2 self.abs_max_v: 1601.5\n",
      "lif layer 2 self.abs_max_v: 1625.0\n",
      "lif layer 2 self.abs_max_v: 1647.5\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.814552/  1.918353, val:  55.42%, val_best:  59.17%, tr:  95.10%, tr_best:  96.22%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9730%\n",
      "layer   2  Sparsity: 82.9835%\n",
      "layer   3  Sparsity: 82.8282%\n",
      "total_backward_count 122375 real_backward_count 25697  20.999%\n",
      "lif layer 2 self.abs_max_v: 1718.5\n",
      "lif layer 2 self.abs_max_v: 1763.5\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.818900/  1.900650, val:  60.42%, val_best:  60.42%, tr:  95.51%, tr_best:  96.22%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9347%\n",
      "layer   2  Sparsity: 82.3781%\n",
      "layer   3  Sparsity: 83.7947%\n",
      "total_backward_count 127270 real_backward_count 26509  20.829%\n",
      "fc layer 2 self.abs_max_out: 1057.0\n",
      "lif layer 2 self.abs_max_v: 1823.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.825307/  1.933957, val:  57.50%, val_best:  60.42%, tr:  94.79%, tr_best:  96.22%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 79.0076%\n",
      "layer   2  Sparsity: 82.3676%\n",
      "layer   3  Sparsity: 84.1354%\n",
      "total_backward_count 132165 real_backward_count 27275  20.637%\n",
      "fc layer 1 self.abs_max_out: 3163.0\n",
      "lif layer 1 self.abs_max_v: 5411.5\n",
      "lif layer 1 self.abs_max_v: 5693.5\n",
      "lif layer 1 self.abs_max_v: 5730.0\n",
      "fc layer 1 self.abs_max_out: 3408.0\n",
      "lif layer 2 self.abs_max_v: 1825.0\n",
      "lif layer 2 self.abs_max_v: 1838.5\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.832650/  1.926310, val:  55.42%, val_best:  60.42%, tr:  95.40%, tr_best:  96.22%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9736%\n",
      "layer   2  Sparsity: 81.8479%\n",
      "layer   3  Sparsity: 83.7281%\n",
      "total_backward_count 137060 real_backward_count 28056  20.470%\n",
      "lif layer 2 self.abs_max_v: 1853.5\n",
      "lif layer 2 self.abs_max_v: 1864.0\n",
      "fc layer 2 self.abs_max_out: 1071.0\n",
      "fc layer 2 self.abs_max_out: 1104.0\n",
      "fc layer 2 self.abs_max_out: 1133.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.827128/  1.944577, val:  57.08%, val_best:  60.42%, tr:  95.51%, tr_best:  96.22%, epoch time: 40.94 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9448%\n",
      "layer   2  Sparsity: 81.7543%\n",
      "layer   3  Sparsity: 84.0325%\n",
      "total_backward_count 141955 real_backward_count 28801  20.289%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.846810/  1.939782, val:  53.75%, val_best:  60.42%, tr:  95.81%, tr_best:  96.22%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9496%\n",
      "layer   2  Sparsity: 82.5496%\n",
      "layer   3  Sparsity: 84.6158%\n",
      "total_backward_count 146850 real_backward_count 29536  20.113%\n",
      "fc layer 3 self.abs_max_out: 484.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.852571/  1.968103, val:  60.42%, val_best:  60.42%, tr:  95.91%, tr_best:  96.22%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9750%\n",
      "layer   2  Sparsity: 82.8847%\n",
      "layer   3  Sparsity: 84.3354%\n",
      "total_backward_count 151745 real_backward_count 30289  19.960%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.858346/  1.953772, val:  60.83%, val_best:  60.83%, tr:  96.02%, tr_best:  96.22%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9256%\n",
      "layer   2  Sparsity: 82.5605%\n",
      "layer   3  Sparsity: 84.1306%\n",
      "total_backward_count 156640 real_backward_count 31023  19.805%\n",
      "fc layer 2 self.abs_max_out: 1146.0\n",
      "fc layer 2 self.abs_max_out: 1153.0\n",
      "fc layer 2 self.abs_max_out: 1179.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.846034/  1.944357, val:  60.83%, val_best:  60.83%, tr:  96.22%, tr_best:  96.22%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9809%\n",
      "layer   2  Sparsity: 82.5233%\n",
      "layer   3  Sparsity: 84.4899%\n",
      "total_backward_count 161535 real_backward_count 31695  19.621%\n",
      "fc layer 2 self.abs_max_out: 1202.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.846995/  1.946039, val:  57.92%, val_best:  60.83%, tr:  95.20%, tr_best:  96.22%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9569%\n",
      "layer   2  Sparsity: 82.7080%\n",
      "layer   3  Sparsity: 84.3495%\n",
      "total_backward_count 166430 real_backward_count 32450  19.498%\n",
      "lif layer 2 self.abs_max_v: 1868.5\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.845342/  1.940401, val:  57.50%, val_best:  60.83%, tr:  96.32%, tr_best:  96.32%, epoch time: 39.68 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 78.9690%\n",
      "layer   2  Sparsity: 82.2165%\n",
      "layer   3  Sparsity: 84.6400%\n",
      "total_backward_count 171325 real_backward_count 33126  19.335%\n",
      "lif layer 2 self.abs_max_v: 1882.5\n",
      "lif layer 2 self.abs_max_v: 1926.5\n",
      "lif layer 2 self.abs_max_v: 1991.5\n",
      "lif layer 2 self.abs_max_v: 2067.5\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.852632/  1.954098, val:  60.83%, val_best:  60.83%, tr:  96.02%, tr_best:  96.32%, epoch time: 40.07 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.8952%\n",
      "layer   2  Sparsity: 82.0062%\n",
      "layer   3  Sparsity: 84.1019%\n",
      "total_backward_count 176220 real_backward_count 33859  19.214%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.862746/  1.954204, val:  64.17%, val_best:  64.17%, tr:  97.04%, tr_best:  97.04%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 79.0332%\n",
      "layer   2  Sparsity: 82.4540%\n",
      "layer   3  Sparsity: 84.7407%\n",
      "total_backward_count 181115 real_backward_count 34542  19.072%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.869780/  1.966942, val:  56.25%, val_best:  64.17%, tr:  96.83%, tr_best:  97.04%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9027%\n",
      "layer   2  Sparsity: 82.5756%\n",
      "layer   3  Sparsity: 85.1300%\n",
      "total_backward_count 186010 real_backward_count 35189  18.918%\n",
      "lif layer 1 self.abs_max_v: 5746.0\n",
      "lif layer 1 self.abs_max_v: 5902.0\n",
      "lif layer 1 self.abs_max_v: 5926.0\n",
      "lif layer 1 self.abs_max_v: 5930.5\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.855075/  1.938266, val:  60.00%, val_best:  64.17%, tr:  96.73%, tr_best:  97.04%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9288%\n",
      "layer   2  Sparsity: 82.3124%\n",
      "layer   3  Sparsity: 84.8000%\n",
      "total_backward_count 190905 real_backward_count 35872  18.790%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.843750/  1.947917, val:  59.58%, val_best:  64.17%, tr:  97.04%, tr_best:  97.04%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9464%\n",
      "layer   2  Sparsity: 82.5779%\n",
      "layer   3  Sparsity: 84.8869%\n",
      "total_backward_count 195800 real_backward_count 36559  18.672%\n",
      "fc layer 1 self.abs_max_out: 3411.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.841621/  1.947709, val:  62.08%, val_best:  64.17%, tr:  96.22%, tr_best:  97.04%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9906%\n",
      "layer   2  Sparsity: 82.7961%\n",
      "layer   3  Sparsity: 84.8870%\n",
      "total_backward_count 200695 real_backward_count 37225  18.548%\n",
      "fc layer 1 self.abs_max_out: 3434.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.842693/  1.945069, val:  61.25%, val_best:  64.17%, tr:  96.02%, tr_best:  97.04%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9594%\n",
      "layer   2  Sparsity: 83.3871%\n",
      "layer   3  Sparsity: 84.7235%\n",
      "total_backward_count 205590 real_backward_count 37954  18.461%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.832600/  1.940413, val:  62.50%, val_best:  64.17%, tr:  96.32%, tr_best:  97.04%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9839%\n",
      "layer   2  Sparsity: 83.0501%\n",
      "layer   3  Sparsity: 85.0223%\n",
      "total_backward_count 210485 real_backward_count 38643  18.359%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.851726/  1.948630, val:  62.92%, val_best:  64.17%, tr:  96.32%, tr_best:  97.04%, epoch time: 40.85 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9484%\n",
      "layer   2  Sparsity: 82.6185%\n",
      "layer   3  Sparsity: 85.5651%\n",
      "total_backward_count 215380 real_backward_count 39278  18.237%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.847777/  1.952746, val:  60.83%, val_best:  64.17%, tr:  96.63%, tr_best:  97.04%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9514%\n",
      "layer   2  Sparsity: 82.4899%\n",
      "layer   3  Sparsity: 85.2295%\n",
      "total_backward_count 220275 real_backward_count 39966  18.144%\n",
      "fc layer 1 self.abs_max_out: 3584.0\n",
      "lif layer 1 self.abs_max_v: 6312.0\n",
      "fc layer 1 self.abs_max_out: 3653.0\n",
      "fc layer 1 self.abs_max_out: 3779.0\n",
      "fc layer 1 self.abs_max_out: 3993.0\n",
      "lif layer 1 self.abs_max_v: 6743.5\n",
      "lif layer 1 self.abs_max_v: 6749.5\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.854944/  1.953437, val:  56.25%, val_best:  64.17%, tr:  95.71%, tr_best:  97.04%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9863%\n",
      "layer   2  Sparsity: 82.3705%\n",
      "layer   3  Sparsity: 85.6088%\n",
      "total_backward_count 225170 real_backward_count 40666  18.060%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.843658/  1.926729, val:  60.42%, val_best:  64.17%, tr:  96.53%, tr_best:  97.04%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9691%\n",
      "layer   2  Sparsity: 82.2789%\n",
      "layer   3  Sparsity: 85.4292%\n",
      "total_backward_count 230065 real_backward_count 41316  17.958%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.827298/  1.918122, val:  62.50%, val_best:  64.17%, tr:  96.83%, tr_best:  97.04%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9378%\n",
      "layer   2  Sparsity: 82.2007%\n",
      "layer   3  Sparsity: 85.2334%\n",
      "total_backward_count 234960 real_backward_count 41955  17.856%\n",
      "lif layer 1 self.abs_max_v: 6791.5\n",
      "lif layer 1 self.abs_max_v: 7074.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.824837/  1.932625, val:  62.50%, val_best:  64.17%, tr:  98.06%, tr_best:  98.06%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9600%\n",
      "layer   2  Sparsity: 81.6584%\n",
      "layer   3  Sparsity: 84.8280%\n",
      "total_backward_count 239855 real_backward_count 42557  17.743%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.834720/  1.957062, val:  55.00%, val_best:  64.17%, tr:  96.63%, tr_best:  98.06%, epoch time: 40.13 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9265%\n",
      "layer   2  Sparsity: 81.8889%\n",
      "layer   3  Sparsity: 85.0166%\n",
      "total_backward_count 244750 real_backward_count 43194  17.648%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.832842/  1.927644, val:  58.75%, val_best:  64.17%, tr:  96.53%, tr_best:  98.06%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9712%\n",
      "layer   2  Sparsity: 82.3738%\n",
      "layer   3  Sparsity: 84.8815%\n",
      "total_backward_count 249645 real_backward_count 43831  17.557%\n",
      "lif layer 1 self.abs_max_v: 7086.5\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.830071/  1.923257, val:  61.25%, val_best:  64.17%, tr:  96.73%, tr_best:  98.06%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9450%\n",
      "layer   2  Sparsity: 82.2685%\n",
      "layer   3  Sparsity: 85.0609%\n",
      "total_backward_count 254540 real_backward_count 44466  17.469%\n",
      "fc layer 1 self.abs_max_out: 4040.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.834573/  1.931570, val:  65.42%, val_best:  65.42%, tr:  96.53%, tr_best:  98.06%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9860%\n",
      "layer   2  Sparsity: 81.9849%\n",
      "layer   3  Sparsity: 85.1518%\n",
      "total_backward_count 259435 real_backward_count 45125  17.394%\n",
      "fc layer 1 self.abs_max_out: 4197.0\n",
      "lif layer 1 self.abs_max_v: 7373.0\n",
      "lif layer 1 self.abs_max_v: 7720.5\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.830165/  1.949667, val:  62.92%, val_best:  65.42%, tr:  96.83%, tr_best:  98.06%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9370%\n",
      "layer   2  Sparsity: 82.0509%\n",
      "layer   3  Sparsity: 85.1833%\n",
      "total_backward_count 264330 real_backward_count 45763  17.313%\n",
      "lif layer 2 self.abs_max_v: 2084.5\n",
      "fc layer 1 self.abs_max_out: 4247.0\n",
      "lif layer 1 self.abs_max_v: 7818.0\n",
      "lif layer 2 self.abs_max_v: 2091.5\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.828074/  1.927775, val:  64.58%, val_best:  65.42%, tr:  97.04%, tr_best:  98.06%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9326%\n",
      "layer   2  Sparsity: 82.3528%\n",
      "layer   3  Sparsity: 85.7323%\n",
      "total_backward_count 269225 real_backward_count 46414  17.240%\n",
      "lif layer 2 self.abs_max_v: 2092.5\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.818533/  1.912856, val:  61.25%, val_best:  65.42%, tr:  97.14%, tr_best:  98.06%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9431%\n",
      "layer   2  Sparsity: 82.3517%\n",
      "layer   3  Sparsity: 85.1953%\n",
      "total_backward_count 274120 real_backward_count 47035  17.159%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.799461/  1.907108, val:  53.75%, val_best:  65.42%, tr:  96.94%, tr_best:  98.06%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9301%\n",
      "layer   2  Sparsity: 82.4635%\n",
      "layer   3  Sparsity: 84.9495%\n",
      "total_backward_count 279015 real_backward_count 47670  17.085%\n",
      "lif layer 2 self.abs_max_v: 2133.5\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.809091/  1.921869, val:  59.58%, val_best:  65.42%, tr:  96.02%, tr_best:  98.06%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9803%\n",
      "layer   2  Sparsity: 82.8836%\n",
      "layer   3  Sparsity: 85.4092%\n",
      "total_backward_count 283910 real_backward_count 48270  17.002%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.816464/  1.920901, val:  63.33%, val_best:  65.42%, tr:  97.24%, tr_best:  98.06%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9618%\n",
      "layer   2  Sparsity: 82.9190%\n",
      "layer   3  Sparsity: 85.2852%\n",
      "total_backward_count 288805 real_backward_count 48863  16.919%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.813803/  1.948596, val:  60.00%, val_best:  65.42%, tr:  97.55%, tr_best:  98.06%, epoch time: 39.81 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 78.9383%\n",
      "layer   2  Sparsity: 83.0300%\n",
      "layer   3  Sparsity: 85.8580%\n",
      "total_backward_count 293700 real_backward_count 49428  16.829%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.828452/  1.907977, val:  61.67%, val_best:  65.42%, tr:  96.83%, tr_best:  98.06%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 79.0198%\n",
      "layer   2  Sparsity: 83.4748%\n",
      "layer   3  Sparsity: 85.7090%\n",
      "total_backward_count 298595 real_backward_count 50030  16.755%\n",
      "lif layer 2 self.abs_max_v: 2167.0\n",
      "fc layer 2 self.abs_max_out: 1204.0\n",
      "lif layer 2 self.abs_max_v: 2287.5\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.806300/  1.901399, val:  65.42%, val_best:  65.42%, tr:  97.14%, tr_best:  98.06%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9374%\n",
      "layer   2  Sparsity: 83.0756%\n",
      "layer   3  Sparsity: 85.3568%\n",
      "total_backward_count 303490 real_backward_count 50620  16.679%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.787607/  1.905952, val:  54.17%, val_best:  65.42%, tr:  97.14%, tr_best:  98.06%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9716%\n",
      "layer   2  Sparsity: 82.8535%\n",
      "layer   3  Sparsity: 85.3907%\n",
      "total_backward_count 308385 real_backward_count 51158  16.589%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.789021/  1.897633, val:  58.75%, val_best:  65.42%, tr:  97.14%, tr_best:  98.06%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9352%\n",
      "layer   2  Sparsity: 82.5680%\n",
      "layer   3  Sparsity: 85.2373%\n",
      "total_backward_count 313280 real_backward_count 51703  16.504%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.792220/  1.880113, val:  67.92%, val_best:  67.92%, tr:  97.45%, tr_best:  98.06%, epoch time: 40.38 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9864%\n",
      "layer   2  Sparsity: 82.3743%\n",
      "layer   3  Sparsity: 85.4170%\n",
      "total_backward_count 318175 real_backward_count 52229  16.415%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.791818/  1.885786, val:  65.00%, val_best:  67.92%, tr:  97.96%, tr_best:  98.06%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9562%\n",
      "layer   2  Sparsity: 82.3790%\n",
      "layer   3  Sparsity: 85.5139%\n",
      "total_backward_count 323070 real_backward_count 52805  16.345%\n",
      "fc layer 1 self.abs_max_out: 4263.0\n",
      "fc layer 1 self.abs_max_out: 4327.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.790251/  1.898265, val:  70.83%, val_best:  70.83%, tr:  98.47%, tr_best:  98.47%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9652%\n",
      "layer   2  Sparsity: 82.5816%\n",
      "layer   3  Sparsity: 85.6423%\n",
      "total_backward_count 327965 real_backward_count 53342  16.265%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.786619/  1.879304, val:  60.00%, val_best:  70.83%, tr:  97.24%, tr_best:  98.47%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 79.0071%\n",
      "layer   2  Sparsity: 82.6328%\n",
      "layer   3  Sparsity: 85.3764%\n",
      "total_backward_count 332860 real_backward_count 53894  16.191%\n",
      "fc layer 1 self.abs_max_out: 4351.0\n",
      "fc layer 1 self.abs_max_out: 4418.0\n",
      "fc layer 1 self.abs_max_out: 4658.0\n",
      "lif layer 1 self.abs_max_v: 7835.0\n",
      "fc layer 1 self.abs_max_out: 4744.0\n",
      "lif layer 1 self.abs_max_v: 8124.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.794883/  1.897755, val:  65.42%, val_best:  70.83%, tr:  97.34%, tr_best:  98.47%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9323%\n",
      "layer   2  Sparsity: 82.3277%\n",
      "layer   3  Sparsity: 85.4722%\n",
      "total_backward_count 337755 real_backward_count 54419  16.112%\n",
      "fc layer 1 self.abs_max_out: 4769.0\n",
      "fc layer 1 self.abs_max_out: 5023.0\n",
      "lif layer 1 self.abs_max_v: 8500.5\n",
      "fc layer 1 self.abs_max_out: 5063.0\n",
      "lif layer 1 self.abs_max_v: 8802.5\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.778377/  1.877979, val:  62.50%, val_best:  70.83%, tr:  97.45%, tr_best:  98.47%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9316%\n",
      "layer   2  Sparsity: 81.8825%\n",
      "layer   3  Sparsity: 85.1548%\n",
      "total_backward_count 342650 real_backward_count 54959  16.039%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.785994/  1.905312, val:  60.83%, val_best:  70.83%, tr:  96.94%, tr_best:  98.47%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9748%\n",
      "layer   2  Sparsity: 82.1098%\n",
      "layer   3  Sparsity: 85.3185%\n",
      "total_backward_count 347545 real_backward_count 55533  15.979%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.795520/  1.910826, val:  57.08%, val_best:  70.83%, tr:  97.45%, tr_best:  98.47%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9259%\n",
      "layer   2  Sparsity: 82.3967%\n",
      "layer   3  Sparsity: 85.1418%\n",
      "total_backward_count 352440 real_backward_count 56080  15.912%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.783107/  1.883066, val:  66.67%, val_best:  70.83%, tr:  97.55%, tr_best:  98.47%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9823%\n",
      "layer   2  Sparsity: 82.0618%\n",
      "layer   3  Sparsity: 85.1568%\n",
      "total_backward_count 357335 real_backward_count 56663  15.857%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.779876/  1.889861, val:  59.58%, val_best:  70.83%, tr:  97.24%, tr_best:  98.47%, epoch time: 40.58 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9186%\n",
      "layer   2  Sparsity: 81.7276%\n",
      "layer   3  Sparsity: 85.0896%\n",
      "total_backward_count 362230 real_backward_count 57169  15.783%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.782110/  1.902191, val:  67.92%, val_best:  70.83%, tr:  97.04%, tr_best:  98.47%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9497%\n",
      "layer   2  Sparsity: 81.8884%\n",
      "layer   3  Sparsity: 84.9998%\n",
      "total_backward_count 367125 real_backward_count 57743  15.728%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.771148/  1.878757, val:  63.75%, val_best:  70.83%, tr:  97.65%, tr_best:  98.47%, epoch time: 40.80 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9376%\n",
      "layer   2  Sparsity: 82.2677%\n",
      "layer   3  Sparsity: 84.7793%\n",
      "total_backward_count 372020 real_backward_count 58246  15.657%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.766653/  1.878595, val:  68.75%, val_best:  70.83%, tr:  98.16%, tr_best:  98.47%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9242%\n",
      "layer   2  Sparsity: 82.1572%\n",
      "layer   3  Sparsity: 84.3623%\n",
      "total_backward_count 376915 real_backward_count 58793  15.598%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.765951/  1.887669, val:  64.17%, val_best:  70.83%, tr:  97.55%, tr_best:  98.47%, epoch time: 40.88 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9180%\n",
      "layer   2  Sparsity: 82.2454%\n",
      "layer   3  Sparsity: 84.8401%\n",
      "total_backward_count 381810 real_backward_count 59307  15.533%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.756986/  1.885199, val:  62.50%, val_best:  70.83%, tr:  98.67%, tr_best:  98.67%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9983%\n",
      "layer   2  Sparsity: 82.2180%\n",
      "layer   3  Sparsity: 85.1326%\n",
      "total_backward_count 386705 real_backward_count 59760  15.454%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.767428/  1.884749, val:  64.17%, val_best:  70.83%, tr:  98.26%, tr_best:  98.67%, epoch time: 40.85 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9537%\n",
      "layer   2  Sparsity: 82.5350%\n",
      "layer   3  Sparsity: 84.8829%\n",
      "total_backward_count 391600 real_backward_count 60231  15.381%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.762717/  1.878281, val:  64.58%, val_best:  70.83%, tr:  98.47%, tr_best:  98.67%, epoch time: 40.63 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9596%\n",
      "layer   2  Sparsity: 82.7783%\n",
      "layer   3  Sparsity: 84.9908%\n",
      "total_backward_count 396495 real_backward_count 60738  15.319%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.773353/  1.891172, val:  64.58%, val_best:  70.83%, tr:  97.85%, tr_best:  98.67%, epoch time: 40.74 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9746%\n",
      "layer   2  Sparsity: 82.7322%\n",
      "layer   3  Sparsity: 85.2007%\n",
      "total_backward_count 401390 real_backward_count 61242  15.257%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.776977/  1.903531, val:  58.75%, val_best:  70.83%, tr:  98.06%, tr_best:  98.67%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9876%\n",
      "layer   2  Sparsity: 82.6043%\n",
      "layer   3  Sparsity: 85.1718%\n",
      "total_backward_count 406285 real_backward_count 61721  15.192%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.776578/  1.895727, val:  64.58%, val_best:  70.83%, tr:  98.57%, tr_best:  98.67%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9825%\n",
      "layer   2  Sparsity: 82.3216%\n",
      "layer   3  Sparsity: 84.9532%\n",
      "total_backward_count 411180 real_backward_count 62202  15.128%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.774724/  1.893149, val:  68.33%, val_best:  70.83%, tr:  97.55%, tr_best:  98.67%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9316%\n",
      "layer   2  Sparsity: 82.2150%\n",
      "layer   3  Sparsity: 84.4940%\n",
      "total_backward_count 416075 real_backward_count 62693  15.068%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.764401/  1.887448, val:  65.83%, val_best:  70.83%, tr:  97.96%, tr_best:  98.67%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 79.0207%\n",
      "layer   2  Sparsity: 82.5265%\n",
      "layer   3  Sparsity: 84.3524%\n",
      "total_backward_count 420970 real_backward_count 63154  15.002%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.754961/  1.879244, val:  60.00%, val_best:  70.83%, tr:  97.75%, tr_best:  98.67%, epoch time: 40.65 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 79.0031%\n",
      "layer   2  Sparsity: 82.4984%\n",
      "layer   3  Sparsity: 84.1960%\n",
      "total_backward_count 425865 real_backward_count 63634  14.942%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.745942/  1.866356, val:  66.25%, val_best:  70.83%, tr:  98.06%, tr_best:  98.67%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9475%\n",
      "layer   2  Sparsity: 82.6034%\n",
      "layer   3  Sparsity: 84.3561%\n",
      "total_backward_count 430760 real_backward_count 64098  14.880%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.757138/  1.867646, val:  66.25%, val_best:  70.83%, tr:  97.85%, tr_best:  98.67%, epoch time: 40.10 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9199%\n",
      "layer   2  Sparsity: 82.6040%\n",
      "layer   3  Sparsity: 84.7221%\n",
      "total_backward_count 435655 real_backward_count 64578  14.823%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.757433/  1.865093, val:  67.08%, val_best:  70.83%, tr:  98.47%, tr_best:  98.67%, epoch time: 40.50 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9735%\n",
      "layer   2  Sparsity: 82.6310%\n",
      "layer   3  Sparsity: 84.7231%\n",
      "total_backward_count 440550 real_backward_count 65057  14.767%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.764177/  1.879024, val:  70.00%, val_best:  70.83%, tr:  97.24%, tr_best:  98.67%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9856%\n",
      "layer   2  Sparsity: 82.7640%\n",
      "layer   3  Sparsity: 84.6970%\n",
      "total_backward_count 445445 real_backward_count 65551  14.716%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.763744/  1.883931, val:  67.08%, val_best:  70.83%, tr:  98.37%, tr_best:  98.67%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9285%\n",
      "layer   2  Sparsity: 82.9221%\n",
      "layer   3  Sparsity: 85.0011%\n",
      "total_backward_count 450340 real_backward_count 65986  14.652%\n",
      "fc layer 2 self.abs_max_out: 1210.0\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.754071/  1.854630, val:  72.92%, val_best:  72.92%, tr:  97.55%, tr_best:  98.67%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9764%\n",
      "layer   2  Sparsity: 82.7661%\n",
      "layer   3  Sparsity: 85.0061%\n",
      "total_backward_count 455235 real_backward_count 66490  14.606%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.755077/  1.872960, val:  70.83%, val_best:  72.92%, tr:  97.75%, tr_best:  98.67%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9185%\n",
      "layer   2  Sparsity: 82.7296%\n",
      "layer   3  Sparsity: 85.1664%\n",
      "total_backward_count 460130 real_backward_count 66996  14.560%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.755114/  1.863411, val:  63.33%, val_best:  72.92%, tr:  98.16%, tr_best:  98.67%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 79.0195%\n",
      "layer   2  Sparsity: 83.0584%\n",
      "layer   3  Sparsity: 84.9168%\n",
      "total_backward_count 465025 real_backward_count 67495  14.514%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.745513/  1.854356, val:  67.08%, val_best:  72.92%, tr:  98.57%, tr_best:  98.67%, epoch time: 39.59 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 78.8890%\n",
      "layer   2  Sparsity: 82.8019%\n",
      "layer   3  Sparsity: 84.2660%\n",
      "total_backward_count 469920 real_backward_count 67929  14.455%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.729471/  1.861140, val:  67.92%, val_best:  72.92%, tr:  98.37%, tr_best:  98.67%, epoch time: 40.60 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9634%\n",
      "layer   2  Sparsity: 83.0221%\n",
      "layer   3  Sparsity: 83.7365%\n",
      "total_backward_count 474815 real_backward_count 68418  14.409%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.732974/  1.861670, val:  69.17%, val_best:  72.92%, tr:  98.37%, tr_best:  98.67%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9834%\n",
      "layer   2  Sparsity: 83.1860%\n",
      "layer   3  Sparsity: 83.7621%\n",
      "total_backward_count 479710 real_backward_count 68889  14.361%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.724807/  1.871936, val:  63.75%, val_best:  72.92%, tr:  97.45%, tr_best:  98.67%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9880%\n",
      "layer   2  Sparsity: 82.8282%\n",
      "layer   3  Sparsity: 84.3162%\n",
      "total_backward_count 484605 real_backward_count 69347  14.310%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.730447/  1.853373, val:  68.75%, val_best:  72.92%, tr:  98.16%, tr_best:  98.67%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9166%\n",
      "layer   2  Sparsity: 82.9027%\n",
      "layer   3  Sparsity: 84.5926%\n",
      "total_backward_count 489500 real_backward_count 69765  14.252%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.725573/  1.861186, val:  65.83%, val_best:  72.92%, tr:  98.47%, tr_best:  98.67%, epoch time: 41.17 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 78.9029%\n",
      "layer   2  Sparsity: 82.8967%\n",
      "layer   3  Sparsity: 84.7021%\n",
      "total_backward_count 494395 real_backward_count 70200  14.199%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.732813/  1.864099, val:  61.25%, val_best:  72.92%, tr:  98.26%, tr_best:  98.67%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9711%\n",
      "layer   2  Sparsity: 82.8660%\n",
      "layer   3  Sparsity: 84.3555%\n",
      "total_backward_count 499290 real_backward_count 70682  14.157%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.731520/  1.848024, val:  68.75%, val_best:  72.92%, tr:  97.24%, tr_best:  98.67%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9008%\n",
      "layer   2  Sparsity: 82.5481%\n",
      "layer   3  Sparsity: 84.0019%\n",
      "total_backward_count 504185 real_backward_count 71147  14.111%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.725959/  1.865121, val:  55.83%, val_best:  72.92%, tr:  98.26%, tr_best:  98.67%, epoch time: 40.96 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.8917%\n",
      "layer   2  Sparsity: 82.4845%\n",
      "layer   3  Sparsity: 84.5357%\n",
      "total_backward_count 509080 real_backward_count 71575  14.060%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.734789/  1.864505, val:  65.42%, val_best:  72.92%, tr:  97.75%, tr_best:  98.67%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9641%\n",
      "layer   2  Sparsity: 82.3845%\n",
      "layer   3  Sparsity: 85.0223%\n",
      "total_backward_count 513975 real_backward_count 71988  14.006%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.729869/  1.847340, val:  64.58%, val_best:  72.92%, tr:  98.16%, tr_best:  98.67%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 78.9542%\n",
      "layer   2  Sparsity: 82.5172%\n",
      "layer   3  Sparsity: 85.0240%\n",
      "total_backward_count 518870 real_backward_count 72440  13.961%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.719091/  1.865694, val:  66.25%, val_best:  72.92%, tr:  98.06%, tr_best:  98.67%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9764%\n",
      "layer   2  Sparsity: 82.6901%\n",
      "layer   3  Sparsity: 84.5189%\n",
      "total_backward_count 523765 real_backward_count 72894  13.917%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.733796/  1.873719, val:  59.58%, val_best:  72.92%, tr:  98.98%, tr_best:  98.98%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9291%\n",
      "layer   2  Sparsity: 82.6882%\n",
      "layer   3  Sparsity: 84.4276%\n",
      "total_backward_count 528660 real_backward_count 73344  13.874%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.728733/  1.859488, val:  65.42%, val_best:  72.92%, tr:  97.75%, tr_best:  98.98%, epoch time: 40.65 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9651%\n",
      "layer   2  Sparsity: 82.7885%\n",
      "layer   3  Sparsity: 84.1146%\n",
      "total_backward_count 533555 real_backward_count 73820  13.835%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.716904/  1.842820, val:  65.00%, val_best:  72.92%, tr:  98.37%, tr_best:  98.98%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9817%\n",
      "layer   2  Sparsity: 83.1436%\n",
      "layer   3  Sparsity: 84.2628%\n",
      "total_backward_count 538450 real_backward_count 74248  13.789%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.706161/  1.856441, val:  62.92%, val_best:  72.92%, tr:  97.96%, tr_best:  98.98%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9908%\n",
      "layer   2  Sparsity: 83.2623%\n",
      "layer   3  Sparsity: 83.9455%\n",
      "total_backward_count 543345 real_backward_count 74653  13.740%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.715154/  1.837465, val:  67.08%, val_best:  72.92%, tr:  98.77%, tr_best:  98.98%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9442%\n",
      "layer   2  Sparsity: 82.8956%\n",
      "layer   3  Sparsity: 84.3665%\n",
      "total_backward_count 548240 real_backward_count 75028  13.685%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.706256/  1.831485, val:  61.67%, val_best:  72.92%, tr:  98.47%, tr_best:  98.98%, epoch time: 39.90 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 78.9768%\n",
      "layer   2  Sparsity: 82.6665%\n",
      "layer   3  Sparsity: 84.6829%\n",
      "total_backward_count 553135 real_backward_count 75430  13.637%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.693728/  1.837161, val:  68.75%, val_best:  72.92%, tr:  97.85%, tr_best:  98.98%, epoch time: 39.85 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 78.9221%\n",
      "layer   2  Sparsity: 82.8579%\n",
      "layer   3  Sparsity: 84.7345%\n",
      "total_backward_count 558030 real_backward_count 75842  13.591%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.717381/  1.861673, val:  68.33%, val_best:  72.92%, tr:  98.47%, tr_best:  98.98%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9793%\n",
      "layer   2  Sparsity: 82.8824%\n",
      "layer   3  Sparsity: 84.6999%\n",
      "total_backward_count 562925 real_backward_count 76258  13.547%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.728670/  1.843571, val:  68.33%, val_best:  72.92%, tr:  98.77%, tr_best:  98.98%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9871%\n",
      "layer   2  Sparsity: 82.9799%\n",
      "layer   3  Sparsity: 84.5632%\n",
      "total_backward_count 567820 real_backward_count 76631  13.496%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.723035/  1.857210, val:  59.58%, val_best:  72.92%, tr:  98.47%, tr_best:  98.98%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9817%\n",
      "layer   2  Sparsity: 82.7495%\n",
      "layer   3  Sparsity: 84.5833%\n",
      "total_backward_count 572715 real_backward_count 77060  13.455%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.713810/  1.831446, val:  66.25%, val_best:  72.92%, tr:  98.67%, tr_best:  98.98%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9383%\n",
      "layer   2  Sparsity: 82.6934%\n",
      "layer   3  Sparsity: 84.7189%\n",
      "total_backward_count 577610 real_backward_count 77449  13.409%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.692680/  1.828152, val:  63.75%, val_best:  72.92%, tr:  98.47%, tr_best:  98.98%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9180%\n",
      "layer   2  Sparsity: 82.8094%\n",
      "layer   3  Sparsity: 84.4530%\n",
      "total_backward_count 582505 real_backward_count 77835  13.362%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.686861/  1.834859, val:  69.17%, val_best:  72.92%, tr:  98.57%, tr_best:  98.98%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9088%\n",
      "layer   2  Sparsity: 82.8185%\n",
      "layer   3  Sparsity: 83.8156%\n",
      "total_backward_count 587400 real_backward_count 78224  13.317%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.695611/  1.837969, val:  70.83%, val_best:  72.92%, tr:  98.98%, tr_best:  98.98%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9307%\n",
      "layer   2  Sparsity: 82.9206%\n",
      "layer   3  Sparsity: 83.7942%\n",
      "total_backward_count 592295 real_backward_count 78600  13.270%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.695753/  1.839819, val:  60.83%, val_best:  72.92%, tr:  98.77%, tr_best:  98.98%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9967%\n",
      "layer   2  Sparsity: 82.9463%\n",
      "layer   3  Sparsity: 83.5236%\n",
      "total_backward_count 597190 real_backward_count 78993  13.227%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.689280/  1.829482, val:  70.00%, val_best:  72.92%, tr:  98.77%, tr_best:  98.98%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 79.0350%\n",
      "layer   2  Sparsity: 82.8927%\n",
      "layer   3  Sparsity: 83.7584%\n",
      "total_backward_count 602085 real_backward_count 79372  13.183%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.693117/  1.826168, val:  64.58%, val_best:  72.92%, tr:  98.88%, tr_best:  98.98%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9694%\n",
      "layer   2  Sparsity: 83.0322%\n",
      "layer   3  Sparsity: 83.7683%\n",
      "total_backward_count 606980 real_backward_count 79762  13.141%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.685575/  1.838528, val:  69.17%, val_best:  72.92%, tr:  98.16%, tr_best:  98.98%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9800%\n",
      "layer   2  Sparsity: 83.1316%\n",
      "layer   3  Sparsity: 83.7814%\n",
      "total_backward_count 611875 real_backward_count 80159  13.101%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.691673/  1.838054, val:  70.83%, val_best:  72.92%, tr:  98.88%, tr_best:  98.98%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.8952%\n",
      "layer   2  Sparsity: 83.1565%\n",
      "layer   3  Sparsity: 83.6847%\n",
      "total_backward_count 616770 real_backward_count 80530  13.057%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.674247/  1.826809, val:  66.25%, val_best:  72.92%, tr:  98.67%, tr_best:  98.98%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9432%\n",
      "layer   2  Sparsity: 82.9754%\n",
      "layer   3  Sparsity: 83.2656%\n",
      "total_backward_count 621665 real_backward_count 80896  13.013%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.662006/  1.806712, val:  66.25%, val_best:  72.92%, tr:  98.06%, tr_best:  98.98%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9385%\n",
      "layer   2  Sparsity: 83.2016%\n",
      "layer   3  Sparsity: 83.0404%\n",
      "total_backward_count 626560 real_backward_count 81260  12.969%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.654002/  1.799883, val:  67.50%, val_best:  72.92%, tr:  98.67%, tr_best:  98.98%, epoch time: 40.83 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9925%\n",
      "layer   2  Sparsity: 82.9361%\n",
      "layer   3  Sparsity: 83.3679%\n",
      "total_backward_count 631455 real_backward_count 81607  12.924%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.644759/  1.797458, val:  67.50%, val_best:  72.92%, tr:  99.08%, tr_best:  99.08%, epoch time: 40.38 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9455%\n",
      "layer   2  Sparsity: 82.7322%\n",
      "layer   3  Sparsity: 83.8742%\n",
      "total_backward_count 636350 real_backward_count 81946  12.878%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.651782/  1.806761, val:  65.42%, val_best:  72.92%, tr:  98.88%, tr_best:  99.08%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9938%\n",
      "layer   2  Sparsity: 83.0220%\n",
      "layer   3  Sparsity: 83.8600%\n",
      "total_backward_count 641245 real_backward_count 82306  12.835%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.665337/  1.822193, val:  67.50%, val_best:  72.92%, tr:  98.16%, tr_best:  99.08%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 79.0129%\n",
      "layer   2  Sparsity: 82.9837%\n",
      "layer   3  Sparsity: 83.9450%\n",
      "total_backward_count 646140 real_backward_count 82693  12.798%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.665420/  1.811166, val:  67.92%, val_best:  72.92%, tr:  99.18%, tr_best:  99.18%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9514%\n",
      "layer   2  Sparsity: 83.0204%\n",
      "layer   3  Sparsity: 84.1040%\n",
      "total_backward_count 651035 real_backward_count 83024  12.753%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.667597/  1.818443, val:  71.67%, val_best:  72.92%, tr:  98.88%, tr_best:  99.18%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9532%\n",
      "layer   2  Sparsity: 82.9651%\n",
      "layer   3  Sparsity: 83.9078%\n",
      "total_backward_count 655930 real_backward_count 83338  12.705%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.670491/  1.807564, val:  68.33%, val_best:  72.92%, tr:  98.16%, tr_best:  99.18%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9723%\n",
      "layer   2  Sparsity: 82.9670%\n",
      "layer   3  Sparsity: 83.6728%\n",
      "total_backward_count 660825 real_backward_count 83690  12.664%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.666163/  1.806468, val:  68.33%, val_best:  72.92%, tr:  98.98%, tr_best:  99.18%, epoch time: 40.38 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9655%\n",
      "layer   2  Sparsity: 83.0656%\n",
      "layer   3  Sparsity: 83.1591%\n",
      "total_backward_count 665720 real_backward_count 84033  12.623%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.648601/  1.805502, val:  68.75%, val_best:  72.92%, tr:  98.77%, tr_best:  99.18%, epoch time: 40.60 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9739%\n",
      "layer   2  Sparsity: 82.9984%\n",
      "layer   3  Sparsity: 83.3593%\n",
      "total_backward_count 670615 real_backward_count 84364  12.580%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.645647/  1.800283, val:  69.58%, val_best:  72.92%, tr:  98.77%, tr_best:  99.18%, epoch time: 41.05 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9694%\n",
      "layer   2  Sparsity: 82.9414%\n",
      "layer   3  Sparsity: 83.4002%\n",
      "total_backward_count 675510 real_backward_count 84686  12.537%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.655555/  1.810366, val:  65.42%, val_best:  72.92%, tr:  98.98%, tr_best:  99.18%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9147%\n",
      "layer   2  Sparsity: 83.0280%\n",
      "layer   3  Sparsity: 83.5381%\n",
      "total_backward_count 680405 real_backward_count 85021  12.496%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.642643/  1.782327, val:  67.08%, val_best:  72.92%, tr:  98.57%, tr_best:  99.18%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9770%\n",
      "layer   2  Sparsity: 82.9852%\n",
      "layer   3  Sparsity: 83.4661%\n",
      "total_backward_count 685300 real_backward_count 85358  12.456%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.641081/  1.797127, val:  62.50%, val_best:  72.92%, tr:  98.77%, tr_best:  99.18%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9966%\n",
      "layer   2  Sparsity: 82.7938%\n",
      "layer   3  Sparsity: 83.9023%\n",
      "total_backward_count 690195 real_backward_count 85716  12.419%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.650876/  1.794533, val:  68.75%, val_best:  72.92%, tr:  98.47%, tr_best:  99.18%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9677%\n",
      "layer   2  Sparsity: 82.9772%\n",
      "layer   3  Sparsity: 83.3268%\n",
      "total_backward_count 695090 real_backward_count 86103  12.387%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.640849/  1.798867, val:  68.33%, val_best:  72.92%, tr:  98.98%, tr_best:  99.18%, epoch time: 40.81 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 79.0436%\n",
      "layer   2  Sparsity: 82.9031%\n",
      "layer   3  Sparsity: 84.1308%\n",
      "total_backward_count 699985 real_backward_count 86475  12.354%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.642440/  1.797870, val:  70.00%, val_best:  72.92%, tr:  99.08%, tr_best:  99.18%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9456%\n",
      "layer   2  Sparsity: 82.9927%\n",
      "layer   3  Sparsity: 84.1687%\n",
      "total_backward_count 704880 real_backward_count 86800  12.314%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.643904/  1.784203, val:  72.50%, val_best:  72.92%, tr:  98.57%, tr_best:  99.18%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9420%\n",
      "layer   2  Sparsity: 82.9162%\n",
      "layer   3  Sparsity: 83.9492%\n",
      "total_backward_count 709775 real_backward_count 87129  12.276%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.641914/  1.796719, val:  66.67%, val_best:  72.92%, tr:  98.77%, tr_best:  99.18%, epoch time: 40.48 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9654%\n",
      "layer   2  Sparsity: 82.7637%\n",
      "layer   3  Sparsity: 84.0071%\n",
      "total_backward_count 714670 real_backward_count 87459  12.238%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.642332/  1.785070, val:  64.58%, val_best:  72.92%, tr:  98.88%, tr_best:  99.18%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9625%\n",
      "layer   2  Sparsity: 82.8427%\n",
      "layer   3  Sparsity: 83.9385%\n",
      "total_backward_count 719565 real_backward_count 87801  12.202%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.627738/  1.791335, val:  63.33%, val_best:  72.92%, tr:  98.37%, tr_best:  99.18%, epoch time: 40.87 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9701%\n",
      "layer   2  Sparsity: 83.0655%\n",
      "layer   3  Sparsity: 84.0532%\n",
      "total_backward_count 724460 real_backward_count 88110  12.162%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.633338/  1.791868, val:  67.92%, val_best:  72.92%, tr:  98.47%, tr_best:  99.18%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9656%\n",
      "layer   2  Sparsity: 82.9737%\n",
      "layer   3  Sparsity: 83.8756%\n",
      "total_backward_count 729355 real_backward_count 88451  12.127%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.646289/  1.811081, val:  71.25%, val_best:  72.92%, tr:  99.49%, tr_best:  99.49%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9208%\n",
      "layer   2  Sparsity: 83.1307%\n",
      "layer   3  Sparsity: 83.5769%\n",
      "total_backward_count 734250 real_backward_count 88806  12.095%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.638306/  1.798115, val:  71.67%, val_best:  72.92%, tr:  99.08%, tr_best:  99.49%, epoch time: 40.07 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9767%\n",
      "layer   2  Sparsity: 83.0621%\n",
      "layer   3  Sparsity: 84.2740%\n",
      "total_backward_count 739145 real_backward_count 89128  12.058%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.625943/  1.777382, val:  64.58%, val_best:  72.92%, tr:  98.26%, tr_best:  99.49%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9958%\n",
      "layer   2  Sparsity: 83.0584%\n",
      "layer   3  Sparsity: 83.9601%\n",
      "total_backward_count 744040 real_backward_count 89457  12.023%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.615074/  1.783084, val:  60.00%, val_best:  72.92%, tr:  98.57%, tr_best:  99.49%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9516%\n",
      "layer   2  Sparsity: 83.1231%\n",
      "layer   3  Sparsity: 83.8272%\n",
      "total_backward_count 748935 real_backward_count 89733  11.981%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.622460/  1.779601, val:  65.83%, val_best:  72.92%, tr:  99.49%, tr_best:  99.49%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 79.0028%\n",
      "layer   2  Sparsity: 83.1680%\n",
      "layer   3  Sparsity: 83.8358%\n",
      "total_backward_count 753830 real_backward_count 90033  11.943%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.620950/  1.776844, val:  67.08%, val_best:  72.92%, tr:  98.98%, tr_best:  99.49%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9574%\n",
      "layer   2  Sparsity: 83.1002%\n",
      "layer   3  Sparsity: 83.3331%\n",
      "total_backward_count 758725 real_backward_count 90336  11.906%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.626626/  1.801203, val:  66.67%, val_best:  72.92%, tr:  99.28%, tr_best:  99.49%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9649%\n",
      "layer   2  Sparsity: 83.0836%\n",
      "layer   3  Sparsity: 83.8838%\n",
      "total_backward_count 763620 real_backward_count 90650  11.871%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.630985/  1.796830, val:  67.50%, val_best:  72.92%, tr:  99.08%, tr_best:  99.49%, epoch time: 40.82 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 79.0017%\n",
      "layer   2  Sparsity: 83.0491%\n",
      "layer   3  Sparsity: 83.9723%\n",
      "total_backward_count 768515 real_backward_count 90949  11.834%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.631010/  1.793453, val:  67.92%, val_best:  72.92%, tr:  99.28%, tr_best:  99.49%, epoch time: 40.48 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9713%\n",
      "layer   2  Sparsity: 83.0436%\n",
      "layer   3  Sparsity: 83.6459%\n",
      "total_backward_count 773410 real_backward_count 91224  11.795%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.632913/  1.797835, val:  70.00%, val_best:  72.92%, tr:  98.98%, tr_best:  99.49%, epoch time: 39.81 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 78.9635%\n",
      "layer   2  Sparsity: 83.0909%\n",
      "layer   3  Sparsity: 84.4651%\n",
      "total_backward_count 778305 real_backward_count 91546  11.762%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.646793/  1.813984, val:  69.17%, val_best:  72.92%, tr:  98.16%, tr_best:  99.49%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9810%\n",
      "layer   2  Sparsity: 83.2376%\n",
      "layer   3  Sparsity: 84.5941%\n",
      "total_backward_count 783200 real_backward_count 91839  11.726%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.654005/  1.819650, val:  69.58%, val_best:  72.92%, tr:  98.88%, tr_best:  99.49%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9310%\n",
      "layer   2  Sparsity: 83.1740%\n",
      "layer   3  Sparsity: 84.3531%\n",
      "total_backward_count 788095 real_backward_count 92135  11.691%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.648246/  1.807339, val:  74.58%, val_best:  74.58%, tr:  98.77%, tr_best:  99.49%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9496%\n",
      "layer   2  Sparsity: 83.2275%\n",
      "layer   3  Sparsity: 84.5976%\n",
      "total_backward_count 792990 real_backward_count 92417  11.654%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.645629/  1.798850, val:  68.33%, val_best:  74.58%, tr:  99.08%, tr_best:  99.49%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.8957%\n",
      "layer   2  Sparsity: 83.2599%\n",
      "layer   3  Sparsity: 84.7850%\n",
      "total_backward_count 797885 real_backward_count 92699  11.618%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.636530/  1.798789, val:  65.83%, val_best:  74.58%, tr:  98.06%, tr_best:  99.49%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9579%\n",
      "layer   2  Sparsity: 83.2703%\n",
      "layer   3  Sparsity: 84.2569%\n",
      "total_backward_count 802780 real_backward_count 92980  11.582%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.625739/  1.789958, val:  67.08%, val_best:  74.58%, tr:  98.98%, tr_best:  99.49%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9550%\n",
      "layer   2  Sparsity: 83.3445%\n",
      "layer   3  Sparsity: 84.0250%\n",
      "total_backward_count 807675 real_backward_count 93254  11.546%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.632300/  1.800330, val:  65.00%, val_best:  74.58%, tr:  98.77%, tr_best:  99.49%, epoch time: 41.13 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 78.9402%\n",
      "layer   2  Sparsity: 83.3302%\n",
      "layer   3  Sparsity: 83.7515%\n",
      "total_backward_count 812570 real_backward_count 93516  11.509%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.629091/  1.795727, val:  67.92%, val_best:  74.58%, tr:  99.39%, tr_best:  99.49%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9517%\n",
      "layer   2  Sparsity: 83.2265%\n",
      "layer   3  Sparsity: 84.3503%\n",
      "total_backward_count 817465 real_backward_count 93768  11.471%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.641938/  1.813635, val:  69.17%, val_best:  74.58%, tr:  99.08%, tr_best:  99.49%, epoch time: 40.90 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9228%\n",
      "layer   2  Sparsity: 83.1080%\n",
      "layer   3  Sparsity: 84.4701%\n",
      "total_backward_count 822360 real_backward_count 94026  11.434%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.633623/  1.791114, val:  69.17%, val_best:  74.58%, tr:  98.77%, tr_best:  99.49%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.8924%\n",
      "layer   2  Sparsity: 83.2410%\n",
      "layer   3  Sparsity: 84.0203%\n",
      "total_backward_count 827255 real_backward_count 94332  11.403%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.632882/  1.789593, val:  72.08%, val_best:  74.58%, tr:  99.08%, tr_best:  99.49%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9417%\n",
      "layer   2  Sparsity: 83.2818%\n",
      "layer   3  Sparsity: 84.1527%\n",
      "total_backward_count 832150 real_backward_count 94596  11.368%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.646803/  1.810301, val:  67.08%, val_best:  74.58%, tr:  98.98%, tr_best:  99.49%, epoch time: 40.38 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9468%\n",
      "layer   2  Sparsity: 83.2070%\n",
      "layer   3  Sparsity: 84.4126%\n",
      "total_backward_count 837045 real_backward_count 94866  11.333%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.655767/  1.812756, val:  70.42%, val_best:  74.58%, tr:  98.47%, tr_best:  99.49%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9784%\n",
      "layer   2  Sparsity: 83.3007%\n",
      "layer   3  Sparsity: 84.0981%\n",
      "total_backward_count 841940 real_backward_count 95157  11.302%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.653868/  1.805511, val:  70.83%, val_best:  74.58%, tr:  98.47%, tr_best:  99.49%, epoch time: 40.84 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9271%\n",
      "layer   2  Sparsity: 83.2939%\n",
      "layer   3  Sparsity: 83.9532%\n",
      "total_backward_count 846835 real_backward_count 95462  11.273%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.645823/  1.828939, val:  69.58%, val_best:  74.58%, tr:  98.77%, tr_best:  99.49%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9119%\n",
      "layer   2  Sparsity: 83.4207%\n",
      "layer   3  Sparsity: 83.7651%\n",
      "total_backward_count 851730 real_backward_count 95765  11.244%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.646520/  1.801460, val:  71.67%, val_best:  74.58%, tr:  98.77%, tr_best:  99.49%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9591%\n",
      "layer   2  Sparsity: 83.3315%\n",
      "layer   3  Sparsity: 83.4257%\n",
      "total_backward_count 856625 real_backward_count 96075  11.216%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.639632/  1.801346, val:  69.58%, val_best:  74.58%, tr:  98.47%, tr_best:  99.49%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9806%\n",
      "layer   2  Sparsity: 83.3477%\n",
      "layer   3  Sparsity: 83.5285%\n",
      "total_backward_count 861520 real_backward_count 96374  11.187%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.644465/  1.791290, val:  67.50%, val_best:  74.58%, tr:  98.16%, tr_best:  99.49%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 78.9501%\n",
      "layer   2  Sparsity: 83.3349%\n",
      "layer   3  Sparsity: 83.8061%\n",
      "total_backward_count 866415 real_backward_count 96683  11.159%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.624979/  1.790482, val:  69.58%, val_best:  74.58%, tr:  98.47%, tr_best:  99.49%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9416%\n",
      "layer   2  Sparsity: 83.3784%\n",
      "layer   3  Sparsity: 83.8446%\n",
      "total_backward_count 871310 real_backward_count 96945  11.126%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.632501/  1.782662, val:  71.25%, val_best:  74.58%, tr:  98.57%, tr_best:  99.49%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9718%\n",
      "layer   2  Sparsity: 83.4838%\n",
      "layer   3  Sparsity: 83.9815%\n",
      "total_backward_count 876205 real_backward_count 97234  11.097%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.626888/  1.785837, val:  73.33%, val_best:  74.58%, tr:  98.88%, tr_best:  99.49%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9738%\n",
      "layer   2  Sparsity: 83.6051%\n",
      "layer   3  Sparsity: 83.9500%\n",
      "total_backward_count 881100 real_backward_count 97512  11.067%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.630381/  1.774533, val:  66.67%, val_best:  74.58%, tr:  98.88%, tr_best:  99.49%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9037%\n",
      "layer   2  Sparsity: 83.6231%\n",
      "layer   3  Sparsity: 83.9637%\n",
      "total_backward_count 885995 real_backward_count 97772  11.035%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.616198/  1.781041, val:  70.42%, val_best:  74.58%, tr:  98.88%, tr_best:  99.49%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9762%\n",
      "layer   2  Sparsity: 83.4030%\n",
      "layer   3  Sparsity: 84.0973%\n",
      "total_backward_count 890890 real_backward_count 98031  11.004%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.617130/  1.766424, val:  72.92%, val_best:  74.58%, tr:  98.57%, tr_best:  99.49%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9299%\n",
      "layer   2  Sparsity: 83.4036%\n",
      "layer   3  Sparsity: 84.0451%\n",
      "total_backward_count 895785 real_backward_count 98327  10.977%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.619711/  1.779682, val:  64.17%, val_best:  74.58%, tr:  98.67%, tr_best:  99.49%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9944%\n",
      "layer   2  Sparsity: 83.4676%\n",
      "layer   3  Sparsity: 84.0156%\n",
      "total_backward_count 900680 real_backward_count 98595  10.947%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.612939/  1.766449, val:  70.00%, val_best:  74.58%, tr:  99.28%, tr_best:  99.49%, epoch time: 40.16 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9560%\n",
      "layer   2  Sparsity: 83.4199%\n",
      "layer   3  Sparsity: 83.6795%\n",
      "total_backward_count 905575 real_backward_count 98881  10.919%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.608144/  1.778726, val:  60.00%, val_best:  74.58%, tr:  98.57%, tr_best:  99.49%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9571%\n",
      "layer   2  Sparsity: 83.4587%\n",
      "layer   3  Sparsity: 83.5720%\n",
      "total_backward_count 910470 real_backward_count 99180  10.893%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.601113/  1.759100, val:  68.33%, val_best:  74.58%, tr:  98.88%, tr_best:  99.49%, epoch time: 40.10 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9086%\n",
      "layer   2  Sparsity: 83.4742%\n",
      "layer   3  Sparsity: 83.3587%\n",
      "total_backward_count 915365 real_backward_count 99455  10.865%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.599099/  1.766775, val:  66.25%, val_best:  74.58%, tr:  98.47%, tr_best:  99.49%, epoch time: 40.10 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 79.0153%\n",
      "layer   2  Sparsity: 83.4153%\n",
      "layer   3  Sparsity: 83.7753%\n",
      "total_backward_count 920260 real_backward_count 99763  10.841%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.608305/  1.784744, val:  74.58%, val_best:  74.58%, tr:  98.88%, tr_best:  99.49%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9794%\n",
      "layer   2  Sparsity: 83.5082%\n",
      "layer   3  Sparsity: 84.0135%\n",
      "total_backward_count 925155 real_backward_count 100016  10.811%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.609537/  1.761696, val:  68.75%, val_best:  74.58%, tr:  98.57%, tr_best:  99.49%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9466%\n",
      "layer   2  Sparsity: 83.5024%\n",
      "layer   3  Sparsity: 83.8448%\n",
      "total_backward_count 930050 real_backward_count 100325  10.787%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.595237/  1.758702, val:  67.50%, val_best:  74.58%, tr:  98.57%, tr_best:  99.49%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9606%\n",
      "layer   2  Sparsity: 83.3138%\n",
      "layer   3  Sparsity: 83.8647%\n",
      "total_backward_count 934945 real_backward_count 100592  10.759%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.597694/  1.762931, val:  70.83%, val_best:  74.58%, tr:  98.77%, tr_best:  99.49%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9547%\n",
      "layer   2  Sparsity: 83.4944%\n",
      "layer   3  Sparsity: 83.6912%\n",
      "total_backward_count 939840 real_backward_count 100877  10.733%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.591612/  1.756346, val:  69.17%, val_best:  74.58%, tr:  98.67%, tr_best:  99.49%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9847%\n",
      "layer   2  Sparsity: 83.5121%\n",
      "layer   3  Sparsity: 83.4947%\n",
      "total_backward_count 944735 real_backward_count 101121  10.704%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.584511/  1.756994, val:  70.00%, val_best:  74.58%, tr:  99.18%, tr_best:  99.49%, epoch time: 40.69 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9797%\n",
      "layer   2  Sparsity: 83.4726%\n",
      "layer   3  Sparsity: 83.8532%\n",
      "total_backward_count 949630 real_backward_count 101373  10.675%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.592564/  1.763932, val:  62.50%, val_best:  74.58%, tr:  98.88%, tr_best:  99.49%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9725%\n",
      "layer   2  Sparsity: 83.3705%\n",
      "layer   3  Sparsity: 83.4316%\n",
      "total_backward_count 954525 real_backward_count 101634  10.648%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.590584/  1.755973, val:  69.58%, val_best:  74.58%, tr:  98.77%, tr_best:  99.49%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9627%\n",
      "layer   2  Sparsity: 83.3683%\n",
      "layer   3  Sparsity: 83.4406%\n",
      "total_backward_count 959420 real_backward_count 101919  10.623%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.591526/  1.758370, val:  67.92%, val_best:  74.58%, tr:  98.88%, tr_best:  99.49%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9720%\n",
      "layer   2  Sparsity: 83.4082%\n",
      "layer   3  Sparsity: 83.1136%\n",
      "total_backward_count 964315 real_backward_count 102177  10.596%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.586151/  1.762067, val:  68.33%, val_best:  74.58%, tr:  98.67%, tr_best:  99.49%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9967%\n",
      "layer   2  Sparsity: 83.5031%\n",
      "layer   3  Sparsity: 82.8807%\n",
      "total_backward_count 969210 real_backward_count 102439  10.569%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.584944/  1.753208, val:  71.67%, val_best:  74.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 78.9611%\n",
      "layer   2  Sparsity: 83.4792%\n",
      "layer   3  Sparsity: 83.1250%\n",
      "total_backward_count 974105 real_backward_count 102685  10.541%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.578257/  1.756300, val:  68.33%, val_best:  74.58%, tr:  99.28%, tr_best:  99.59%, epoch time: 40.77 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 78.9825%\n",
      "layer   2  Sparsity: 83.3322%\n",
      "layer   3  Sparsity: 82.8303%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a8d17d858a4b8ba1febfb281e9200c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99285</td></tr><tr><td>tr_epoch_loss</td><td>1.57826</td></tr><tr><td>val_acc_best</td><td>0.74583</td></tr><tr><td>val_acc_now</td><td>0.68333</td></tr><tr><td>val_loss</td><td>1.7563</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earnest-sweep-73</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mffikdvu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mffikdvu</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_154424-mffikdvu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kjubz63m with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 12000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_175948-kjubz63m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kjubz63m' target=\"_blank\">hearty-sweep-78</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kjubz63m' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kjubz63m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251117_175956_542', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 12000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 67f09733060e9328908e01cda0ab3532\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 592\n",
      "fc layer 1 self.abs_max_out: 108.0\n",
      "lif layer 1 self.abs_max_v: 108.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 121.0\n",
      "lif layer 1 self.abs_max_v: 155.0\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 36.0\n",
      "fc layer 1 self.abs_max_out: 135.0\n",
      "lif layer 1 self.abs_max_v: 170.0\n",
      "fc layer 2 self.abs_max_out: 128.0\n",
      "lif layer 2 self.abs_max_v: 132.0\n",
      "fc layer 3 self.abs_max_out: 17.0\n",
      "smallest_now_T updated: 534\n",
      "fc layer 1 self.abs_max_out: 170.0\n",
      "fc layer 1 self.abs_max_out: 189.0\n",
      "lif layer 1 self.abs_max_v: 235.0\n",
      "fc layer 2 self.abs_max_out: 216.0\n",
      "lif layer 2 self.abs_max_v: 233.5\n",
      "fc layer 3 self.abs_max_out: 53.0\n",
      "lif layer 1 self.abs_max_v: 267.5\n",
      "lif layer 2 self.abs_max_v: 244.5\n",
      "fc layer 3 self.abs_max_out: 78.0\n",
      "fc layer 1 self.abs_max_out: 208.0\n",
      "lif layer 1 self.abs_max_v: 340.0\n",
      "fc layer 2 self.abs_max_out: 277.0\n",
      "lif layer 2 self.abs_max_v: 315.5\n",
      "fc layer 3 self.abs_max_out: 88.0\n",
      "lif layer 2 self.abs_max_v: 332.0\n",
      "fc layer 3 self.abs_max_out: 96.0\n",
      "smallest_now_T updated: 407\n",
      "fc layer 1 self.abs_max_out: 220.0\n",
      "fc layer 1 self.abs_max_out: 268.0\n",
      "fc layer 1 self.abs_max_out: 270.0\n",
      "fc layer 2 self.abs_max_out: 317.0\n",
      "fc layer 1 self.abs_max_out: 289.0\n",
      "fc layer 2 self.abs_max_out: 396.0\n",
      "lif layer 2 self.abs_max_v: 396.0\n",
      "fc layer 3 self.abs_max_out: 100.0\n",
      "lif layer 1 self.abs_max_v: 359.5\n",
      "fc layer 3 self.abs_max_out: 125.0\n",
      "lif layer 1 self.abs_max_v: 402.5\n",
      "lif layer 2 self.abs_max_v: 438.0\n",
      "fc layer 1 self.abs_max_out: 299.0\n",
      "fc layer 3 self.abs_max_out: 146.0\n",
      "lif layer 1 self.abs_max_v: 408.5\n",
      "fc layer 1 self.abs_max_out: 309.0\n",
      "smallest_now_T updated: 345\n",
      "fc layer 2 self.abs_max_out: 413.0\n",
      "lif layer 2 self.abs_max_v: 495.5\n",
      "fc layer 3 self.abs_max_out: 161.0\n",
      "lif layer 1 self.abs_max_v: 425.0\n",
      "fc layer 1 self.abs_max_out: 402.0\n",
      "fc layer 1 self.abs_max_out: 443.0\n",
      "lif layer 1 self.abs_max_v: 443.0\n",
      "fc layer 1 self.abs_max_out: 467.0\n",
      "lif layer 1 self.abs_max_v: 467.0\n",
      "smallest_now_T updated: 317\n",
      "fc layer 3 self.abs_max_out: 165.0\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "lif layer 2 self.abs_max_v: 499.5\n",
      "lif layer 2 self.abs_max_v: 501.5\n",
      "lif layer 2 self.abs_max_v: 541.0\n",
      "fc layer 2 self.abs_max_out: 422.0\n",
      "lif layer 2 self.abs_max_v: 692.5\n",
      "smallest_now_T updated: 286\n",
      "fc layer 1 self.abs_max_out: 517.0\n",
      "lif layer 1 self.abs_max_v: 517.0\n",
      "fc layer 2 self.abs_max_out: 498.0\n",
      "lif layer 1 self.abs_max_v: 544.5\n",
      "fc layer 1 self.abs_max_out: 559.0\n",
      "lif layer 1 self.abs_max_v: 559.0\n",
      "fc layer 3 self.abs_max_out: 181.0\n",
      "fc layer 3 self.abs_max_out: 182.0\n",
      "fc layer 1 self.abs_max_out: 572.0\n",
      "lif layer 1 self.abs_max_v: 572.0\n",
      "fc layer 1 self.abs_max_out: 633.0\n",
      "lif layer 1 self.abs_max_v: 633.0\n",
      "fc layer 2 self.abs_max_out: 501.0\n",
      "fc layer 3 self.abs_max_out: 185.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 1 self.abs_max_out: 731.0\n",
      "lif layer 1 self.abs_max_v: 731.0\n",
      "fc layer 2 self.abs_max_out: 553.0\n",
      "lif layer 2 self.abs_max_v: 876.5\n",
      "fc layer 1 self.abs_max_out: 895.0\n",
      "lif layer 1 self.abs_max_v: 895.0\n",
      "fc layer 3 self.abs_max_out: 227.0\n",
      "smallest_now_T updated: 247\n",
      "smallest_now_T updated: 192\n",
      "fc layer 2 self.abs_max_out: 586.0\n",
      "fc layer 3 self.abs_max_out: 255.0\n",
      "fc layer 2 self.abs_max_out: 614.0\n",
      "fc layer 2 self.abs_max_out: 632.0\n",
      "fc layer 2 self.abs_max_out: 667.0\n",
      "fc layer 3 self.abs_max_out: 272.0\n",
      "fc layer 1 self.abs_max_out: 914.0\n",
      "lif layer 1 self.abs_max_v: 914.0\n",
      "fc layer 1 self.abs_max_out: 922.0\n",
      "lif layer 1 self.abs_max_v: 922.0\n",
      "fc layer 1 self.abs_max_out: 935.0\n",
      "lif layer 1 self.abs_max_v: 935.0\n",
      "fc layer 2 self.abs_max_out: 695.0\n",
      "fc layer 2 self.abs_max_out: 793.0\n",
      "fc layer 1 self.abs_max_out: 948.0\n",
      "lif layer 1 self.abs_max_v: 948.0\n",
      "lif layer 2 self.abs_max_v: 878.0\n",
      "fc layer 1 self.abs_max_out: 1027.0\n",
      "lif layer 1 self.abs_max_v: 1027.0\n",
      "fc layer 1 self.abs_max_out: 1098.0\n",
      "lif layer 1 self.abs_max_v: 1098.0\n",
      "lif layer 2 self.abs_max_v: 886.5\n",
      "fc layer 2 self.abs_max_out: 807.0\n",
      "fc layer 2 self.abs_max_out: 814.0\n",
      "smallest_now_T_val updated: 552\n",
      "smallest_now_T_val updated: 456\n",
      "smallest_now_T_val updated: 448\n",
      "smallest_now_T_val updated: 440\n",
      "smallest_now_T_val updated: 368\n",
      "smallest_now_T_val updated: 137\n",
      "fc layer 1 self.abs_max_out: 1149.0\n",
      "lif layer 1 self.abs_max_v: 1149.0\n",
      "lif layer 2 self.abs_max_v: 933.5\n",
      "fc layer 2 self.abs_max_out: 815.0\n",
      "fc layer 2 self.abs_max_out: 827.0\n",
      "fc layer 2 self.abs_max_out: 843.0\n",
      "lif layer 2 self.abs_max_v: 960.5\n",
      "lif layer 2 self.abs_max_v: 1067.5\n",
      "fc layer 1 self.abs_max_out: 1236.0\n",
      "lif layer 1 self.abs_max_v: 1236.0\n",
      "fc layer 2 self.abs_max_out: 862.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.090164/  2.136664, val:  36.67%, val_best:  36.67%, tr:  60.88%, tr_best:  60.88%, epoch time: 40.83 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0699%\n",
      "layer   2  Sparsity: 87.5481%\n",
      "layer   3  Sparsity: 86.3201%\n",
      "total_backward_count 4895 real_backward_count 2604  53.197%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 864.0\n",
      "fc layer 2 self.abs_max_out: 889.0\n",
      "fc layer 2 self.abs_max_out: 893.0\n",
      "fc layer 1 self.abs_max_out: 1307.0\n",
      "lif layer 1 self.abs_max_v: 1307.0\n",
      "fc layer 1 self.abs_max_out: 1498.0\n",
      "lif layer 1 self.abs_max_v: 1498.0\n",
      "fc layer 2 self.abs_max_out: 945.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.019989/  2.105078, val:  41.67%, val_best:  41.67%, tr:  79.16%, tr_best:  79.16%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0836%\n",
      "layer   2  Sparsity: 85.1519%\n",
      "layer   3  Sparsity: 83.2928%\n",
      "total_backward_count 9790 real_backward_count 4444  45.393%\n",
      "fc layer 2 self.abs_max_out: 1007.0\n",
      "lif layer 2 self.abs_max_v: 1124.0\n",
      "fc layer 2 self.abs_max_out: 1010.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.999524/  2.111299, val:  42.50%, val_best:  42.50%, tr:  81.41%, tr_best:  81.41%, epoch time: 40.94 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0968%\n",
      "layer   2  Sparsity: 84.4470%\n",
      "layer   3  Sparsity: 82.6040%\n",
      "total_backward_count 14685 real_backward_count 6141  41.818%\n",
      "fc layer 2 self.abs_max_out: 1035.0\n",
      "fc layer 1 self.abs_max_out: 1596.0\n",
      "lif layer 1 self.abs_max_v: 1596.0\n",
      "lif layer 2 self.abs_max_v: 1152.5\n",
      "fc layer 2 self.abs_max_out: 1086.0\n",
      "lif layer 2 self.abs_max_v: 1157.0\n",
      "lif layer 2 self.abs_max_v: 1157.5\n",
      "lif layer 2 self.abs_max_v: 1302.0\n",
      "fc layer 2 self.abs_max_out: 1111.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.992705/  2.096691, val:  45.00%, val_best:  45.00%, tr:  83.86%, tr_best:  83.86%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0795%\n",
      "layer   2  Sparsity: 84.2546%\n",
      "layer   3  Sparsity: 81.9786%\n",
      "total_backward_count 19580 real_backward_count 7769  39.678%\n",
      "fc layer 1 self.abs_max_out: 1617.0\n",
      "lif layer 1 self.abs_max_v: 1617.0\n",
      "fc layer 2 self.abs_max_out: 1123.0\n",
      "fc layer 1 self.abs_max_out: 1786.0\n",
      "lif layer 1 self.abs_max_v: 1786.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.973729/  2.072798, val:  33.75%, val_best:  45.00%, tr:  85.50%, tr_best:  85.50%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0828%\n",
      "layer   2  Sparsity: 84.2985%\n",
      "layer   3  Sparsity: 81.6391%\n",
      "total_backward_count 24475 real_backward_count 9363  38.255%\n",
      "lif layer 2 self.abs_max_v: 1454.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.978378/  2.074365, val:  37.50%, val_best:  45.00%, tr:  85.50%, tr_best:  85.50%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0688%\n",
      "layer   2  Sparsity: 83.8112%\n",
      "layer   3  Sparsity: 81.3354%\n",
      "total_backward_count 29370 real_backward_count 10903  37.123%\n",
      "fc layer 3 self.abs_max_out: 275.0\n",
      "fc layer 3 self.abs_max_out: 294.0\n",
      "fc layer 3 self.abs_max_out: 305.0\n",
      "fc layer 2 self.abs_max_out: 1171.0\n",
      "fc layer 2 self.abs_max_out: 1206.0\n",
      "fc layer 2 self.abs_max_out: 1208.0\n",
      "fc layer 2 self.abs_max_out: 1234.0\n",
      "lif layer 1 self.abs_max_v: 1808.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.967887/  2.085688, val:  39.58%, val_best:  45.00%, tr:  86.21%, tr_best:  86.21%, epoch time: 40.48 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0980%\n",
      "layer   2  Sparsity: 83.2305%\n",
      "layer   3  Sparsity: 80.8854%\n",
      "total_backward_count 34265 real_backward_count 12411  36.221%\n",
      "fc layer 1 self.abs_max_out: 1808.0\n",
      "fc layer 2 self.abs_max_out: 1243.0\n",
      "fc layer 1 self.abs_max_out: 2049.0\n",
      "lif layer 1 self.abs_max_v: 2049.0\n",
      "fc layer 2 self.abs_max_out: 1312.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.954156/  2.058392, val:  33.75%, val_best:  45.00%, tr:  87.13%, tr_best:  87.13%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0853%\n",
      "layer   2  Sparsity: 82.9782%\n",
      "layer   3  Sparsity: 81.1210%\n",
      "total_backward_count 39160 real_backward_count 13875  35.432%\n",
      "fc layer 2 self.abs_max_out: 1350.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.973363/  2.069162, val:  57.08%, val_best:  57.08%, tr:  85.19%, tr_best:  87.13%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0807%\n",
      "layer   2  Sparsity: 82.8806%\n",
      "layer   3  Sparsity: 81.3578%\n",
      "total_backward_count 44055 real_backward_count 15400  34.956%\n",
      "lif layer 2 self.abs_max_v: 1501.0\n",
      "lif layer 2 self.abs_max_v: 1526.0\n",
      "fc layer 2 self.abs_max_out: 1372.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.963002/  2.094727, val:  38.75%, val_best:  57.08%, tr:  87.33%, tr_best:  87.33%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0659%\n",
      "layer   2  Sparsity: 82.8560%\n",
      "layer   3  Sparsity: 80.8842%\n",
      "total_backward_count 48950 real_backward_count 16869  34.462%\n",
      "lif layer 2 self.abs_max_v: 1540.0\n",
      "lif layer 2 self.abs_max_v: 1643.0\n",
      "lif layer 2 self.abs_max_v: 1655.5\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.944855/  2.084604, val:  39.17%, val_best:  57.08%, tr:  88.25%, tr_best:  88.25%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0488%\n",
      "layer   2  Sparsity: 82.6853%\n",
      "layer   3  Sparsity: 80.7481%\n",
      "total_backward_count 53845 real_backward_count 18324  34.031%\n",
      "fc layer 1 self.abs_max_out: 2141.0\n",
      "lif layer 1 self.abs_max_v: 2141.0\n",
      "lif layer 2 self.abs_max_v: 1719.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.947754/  2.047953, val:  41.25%, val_best:  57.08%, tr:  86.82%, tr_best:  88.25%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0959%\n",
      "layer   2  Sparsity: 82.7559%\n",
      "layer   3  Sparsity: 80.1370%\n",
      "total_backward_count 58740 real_backward_count 19751  33.624%\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.930172/  2.052404, val:  32.50%, val_best:  57.08%, tr:  88.97%, tr_best:  88.97%, epoch time: 40.86 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0593%\n",
      "layer   2  Sparsity: 83.1061%\n",
      "layer   3  Sparsity: 80.0129%\n",
      "total_backward_count 63635 real_backward_count 21135  33.213%\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.930819/  2.047559, val:  35.83%, val_best:  57.08%, tr:  88.36%, tr_best:  88.97%, epoch time: 40.96 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0778%\n",
      "layer   2  Sparsity: 82.8193%\n",
      "layer   3  Sparsity: 80.0783%\n",
      "total_backward_count 68530 real_backward_count 22537  32.886%\n",
      "fc layer 1 self.abs_max_out: 2169.0\n",
      "lif layer 1 self.abs_max_v: 2169.0\n",
      "fc layer 1 self.abs_max_out: 2177.0\n",
      "lif layer 1 self.abs_max_v: 2177.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.917393/  2.046298, val:  42.50%, val_best:  57.08%, tr:  88.76%, tr_best:  88.97%, epoch time: 40.72 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0908%\n",
      "layer   2  Sparsity: 83.0678%\n",
      "layer   3  Sparsity: 79.8165%\n",
      "total_backward_count 73425 real_backward_count 23856  32.490%\n",
      "fc layer 1 self.abs_max_out: 2235.0\n",
      "lif layer 1 self.abs_max_v: 2235.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.911885/  2.026339, val:  36.25%, val_best:  57.08%, tr:  89.07%, tr_best:  89.07%, epoch time: 40.88 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0742%\n",
      "layer   2  Sparsity: 82.4377%\n",
      "layer   3  Sparsity: 79.0687%\n",
      "total_backward_count 78320 real_backward_count 25191  32.164%\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.893789/  2.032425, val:  49.58%, val_best:  57.08%, tr:  90.60%, tr_best:  90.60%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0874%\n",
      "layer   2  Sparsity: 82.1242%\n",
      "layer   3  Sparsity: 79.1468%\n",
      "total_backward_count 83215 real_backward_count 26523  31.873%\n",
      "fc layer 1 self.abs_max_out: 2307.0\n",
      "lif layer 1 self.abs_max_v: 2307.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.900544/  2.029780, val:  53.75%, val_best:  57.08%, tr:  88.87%, tr_best:  90.60%, epoch time: 40.87 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0834%\n",
      "layer   2  Sparsity: 81.9180%\n",
      "layer   3  Sparsity: 79.6896%\n",
      "total_backward_count 88110 real_backward_count 27857  31.616%\n",
      "lif layer 2 self.abs_max_v: 1724.0\n",
      "fc layer 1 self.abs_max_out: 2556.0\n",
      "lif layer 1 self.abs_max_v: 2556.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.897478/  2.008673, val:  43.75%, val_best:  57.08%, tr:  88.87%, tr_best:  90.60%, epoch time: 41.19 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0816%\n",
      "layer   2  Sparsity: 81.7989%\n",
      "layer   3  Sparsity: 79.5800%\n",
      "total_backward_count 93005 real_backward_count 29227  31.425%\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.883724/  2.010493, val:  40.00%, val_best:  57.08%, tr:  89.79%, tr_best:  90.60%, epoch time: 40.82 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0590%\n",
      "layer   2  Sparsity: 82.0090%\n",
      "layer   3  Sparsity: 79.0249%\n",
      "total_backward_count 97900 real_backward_count 30506  31.160%\n",
      "lif layer 2 self.abs_max_v: 1815.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.878881/  2.017642, val:  46.25%, val_best:  57.08%, tr:  90.19%, tr_best:  90.60%, epoch time: 40.70 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0833%\n",
      "layer   2  Sparsity: 81.4949%\n",
      "layer   3  Sparsity: 78.9068%\n",
      "total_backward_count 102795 real_backward_count 31814  30.949%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.890892/  2.023342, val:  44.58%, val_best:  57.08%, tr:  86.82%, tr_best:  90.60%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0888%\n",
      "layer   2  Sparsity: 81.5068%\n",
      "layer   3  Sparsity: 78.4992%\n",
      "total_backward_count 107690 real_backward_count 33205  30.834%\n",
      "fc layer 2 self.abs_max_out: 1398.0\n",
      "fc layer 2 self.abs_max_out: 1482.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.882282/  1.998459, val:  50.83%, val_best:  57.08%, tr:  91.42%, tr_best:  91.42%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0977%\n",
      "layer   2  Sparsity: 81.3673%\n",
      "layer   3  Sparsity: 79.1368%\n",
      "total_backward_count 112585 real_backward_count 34523  30.664%\n",
      "fc layer 1 self.abs_max_out: 2688.0\n",
      "lif layer 1 self.abs_max_v: 2688.0\n",
      "lif layer 2 self.abs_max_v: 1902.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.869291/  2.014051, val:  46.25%, val_best:  57.08%, tr:  90.19%, tr_best:  91.42%, epoch time: 40.71 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0767%\n",
      "layer   2  Sparsity: 81.3955%\n",
      "layer   3  Sparsity: 79.8357%\n",
      "total_backward_count 117480 real_backward_count 35832  30.501%\n",
      "lif layer 2 self.abs_max_v: 1933.0\n",
      "lif layer 2 self.abs_max_v: 1970.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.886736/  2.033615, val:  38.75%, val_best:  57.08%, tr:  90.09%, tr_best:  91.42%, epoch time: 40.26 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0984%\n",
      "layer   2  Sparsity: 81.2083%\n",
      "layer   3  Sparsity: 79.5614%\n",
      "total_backward_count 122375 real_backward_count 37143  30.352%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.887425/  1.993934, val:  51.67%, val_best:  57.08%, tr:  89.48%, tr_best:  91.42%, epoch time: 40.77 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0944%\n",
      "layer   2  Sparsity: 81.3552%\n",
      "layer   3  Sparsity: 79.4422%\n",
      "total_backward_count 127270 real_backward_count 38564  30.301%\n",
      "fc layer 3 self.abs_max_out: 331.0\n",
      "lif layer 2 self.abs_max_v: 1986.5\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.860745/  1.983877, val:  50.42%, val_best:  57.08%, tr:  89.58%, tr_best:  91.42%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0747%\n",
      "layer   2  Sparsity: 81.2777%\n",
      "layer   3  Sparsity: 79.0324%\n",
      "total_backward_count 132165 real_backward_count 39889  30.181%\n",
      "fc layer 2 self.abs_max_out: 1485.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.855755/  1.997882, val:  52.50%, val_best:  57.08%, tr:  90.40%, tr_best:  91.42%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0820%\n",
      "layer   2  Sparsity: 81.3086%\n",
      "layer   3  Sparsity: 78.9011%\n",
      "total_backward_count 137060 real_backward_count 41207  30.065%\n",
      "lif layer 2 self.abs_max_v: 2025.5\n",
      "lif layer 2 self.abs_max_v: 2287.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.850701/  1.994851, val:  51.25%, val_best:  57.08%, tr:  91.22%, tr_best:  91.42%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0858%\n",
      "layer   2  Sparsity: 81.3283%\n",
      "layer   3  Sparsity: 79.0206%\n",
      "total_backward_count 141955 real_backward_count 42443  29.899%\n",
      "fc layer 2 self.abs_max_out: 1498.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.854188/  1.984045, val:  43.33%, val_best:  57.08%, tr:  91.83%, tr_best:  91.83%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0911%\n",
      "layer   2  Sparsity: 81.3730%\n",
      "layer   3  Sparsity: 79.1055%\n",
      "total_backward_count 146850 real_backward_count 43702  29.760%\n",
      "fc layer 1 self.abs_max_out: 2700.0\n",
      "lif layer 1 self.abs_max_v: 2700.0\n",
      "fc layer 2 self.abs_max_out: 1523.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.855785/  1.981767, val:  45.83%, val_best:  57.08%, tr:  92.24%, tr_best:  92.24%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0907%\n",
      "layer   2  Sparsity: 81.2549%\n",
      "layer   3  Sparsity: 78.9569%\n",
      "total_backward_count 151745 real_backward_count 44990  29.648%\n",
      "fc layer 1 self.abs_max_out: 2853.0\n",
      "lif layer 1 self.abs_max_v: 2853.0\n",
      "fc layer 2 self.abs_max_out: 1552.0\n",
      "fc layer 2 self.abs_max_out: 1567.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.857707/  1.994075, val:  53.75%, val_best:  57.08%, tr:  90.70%, tr_best:  92.24%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1021%\n",
      "layer   2  Sparsity: 81.2568%\n",
      "layer   3  Sparsity: 78.8669%\n",
      "total_backward_count 156640 real_backward_count 46279  29.545%\n",
      "fc layer 1 self.abs_max_out: 2889.0\n",
      "lif layer 1 self.abs_max_v: 2889.0\n",
      "fc layer 2 self.abs_max_out: 1576.0\n",
      "fc layer 2 self.abs_max_out: 1645.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.857305/  1.989733, val:  45.83%, val_best:  57.08%, tr:  92.54%, tr_best:  92.54%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0489%\n",
      "layer   2  Sparsity: 81.0216%\n",
      "layer   3  Sparsity: 78.4898%\n",
      "total_backward_count 161535 real_backward_count 47520  29.418%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.847964/  1.981424, val:  48.33%, val_best:  57.08%, tr:  90.91%, tr_best:  92.54%, epoch time: 41.08 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0674%\n",
      "layer   2  Sparsity: 80.8971%\n",
      "layer   3  Sparsity: 78.9671%\n",
      "total_backward_count 166430 real_backward_count 48761  29.298%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.839788/  1.971350, val:  55.42%, val_best:  57.08%, tr:  93.05%, tr_best:  93.05%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0414%\n",
      "layer   2  Sparsity: 80.9651%\n",
      "layer   3  Sparsity: 78.8420%\n",
      "total_backward_count 171325 real_backward_count 49909  29.131%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.836821/  1.958719, val:  65.42%, val_best:  65.42%, tr:  90.40%, tr_best:  93.05%, epoch time: 41.39 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0912%\n",
      "layer   2  Sparsity: 80.9408%\n",
      "layer   3  Sparsity: 78.6037%\n",
      "total_backward_count 176220 real_backward_count 51131  29.015%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.830862/  1.961033, val:  56.67%, val_best:  65.42%, tr:  93.67%, tr_best:  93.67%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0981%\n",
      "layer   2  Sparsity: 80.8381%\n",
      "layer   3  Sparsity: 78.7697%\n",
      "total_backward_count 181115 real_backward_count 52336  28.897%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.833814/  1.990591, val:  41.25%, val_best:  65.42%, tr:  93.56%, tr_best:  93.67%, epoch time: 41.57 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0625%\n",
      "layer   2  Sparsity: 80.9140%\n",
      "layer   3  Sparsity: 78.5340%\n",
      "total_backward_count 186010 real_backward_count 53497  28.760%\n",
      "fc layer 2 self.abs_max_out: 1658.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.831622/  1.947705, val:  61.25%, val_best:  65.42%, tr:  92.95%, tr_best:  93.67%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0904%\n",
      "layer   2  Sparsity: 80.9046%\n",
      "layer   3  Sparsity: 78.6281%\n",
      "total_backward_count 190905 real_backward_count 54679  28.642%\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.815283/  1.942076, val:  55.83%, val_best:  65.42%, tr:  93.56%, tr_best:  93.67%, epoch time: 41.31 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.1028%\n",
      "layer   2  Sparsity: 80.9155%\n",
      "layer   3  Sparsity: 78.3516%\n",
      "total_backward_count 195800 real_backward_count 55867  28.533%\n",
      "fc layer 2 self.abs_max_out: 1668.0\n",
      "fc layer 2 self.abs_max_out: 1754.0\n",
      "fc layer 2 self.abs_max_out: 1795.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.807678/  1.938268, val:  49.58%, val_best:  65.42%, tr:  91.42%, tr_best:  93.67%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0416%\n",
      "layer   2  Sparsity: 81.1119%\n",
      "layer   3  Sparsity: 78.2037%\n",
      "total_backward_count 200695 real_backward_count 57057  28.430%\n",
      "fc layer 2 self.abs_max_out: 1901.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.808817/  1.934582, val:  55.00%, val_best:  65.42%, tr:  92.85%, tr_best:  93.67%, epoch time: 41.17 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0826%\n",
      "layer   2  Sparsity: 81.0756%\n",
      "layer   3  Sparsity: 77.7724%\n",
      "total_backward_count 205590 real_backward_count 58260  28.338%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.789725/  1.941289, val:  58.33%, val_best:  65.42%, tr:  92.24%, tr_best:  93.67%, epoch time: 40.72 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0423%\n",
      "layer   2  Sparsity: 80.9395%\n",
      "layer   3  Sparsity: 77.5812%\n",
      "total_backward_count 210485 real_backward_count 59439  28.239%\n",
      "fc layer 1 self.abs_max_out: 2973.0\n",
      "lif layer 1 self.abs_max_v: 2973.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.812831/  1.944349, val:  59.17%, val_best:  65.42%, tr:  91.42%, tr_best:  93.67%, epoch time: 41.30 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0695%\n",
      "layer   2  Sparsity: 80.9640%\n",
      "layer   3  Sparsity: 77.8176%\n",
      "total_backward_count 215380 real_backward_count 60614  28.143%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.812058/  1.943879, val:  57.50%, val_best:  65.42%, tr:  93.36%, tr_best:  93.67%, epoch time: 40.82 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0868%\n",
      "layer   2  Sparsity: 81.0404%\n",
      "layer   3  Sparsity: 77.4879%\n",
      "total_backward_count 220275 real_backward_count 61795  28.054%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.800223/  1.922454, val:  61.25%, val_best:  65.42%, tr:  93.16%, tr_best:  93.67%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0759%\n",
      "layer   2  Sparsity: 80.7771%\n",
      "layer   3  Sparsity: 77.5845%\n",
      "total_backward_count 225170 real_backward_count 62954  27.958%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.814306/  1.940775, val:  62.50%, val_best:  65.42%, tr:  92.03%, tr_best:  93.67%, epoch time: 40.79 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0823%\n",
      "layer   2  Sparsity: 80.4844%\n",
      "layer   3  Sparsity: 77.7733%\n",
      "total_backward_count 230065 real_backward_count 64119  27.870%\n",
      "lif layer 2 self.abs_max_v: 2326.0\n",
      "fc layer 2 self.abs_max_out: 1926.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.804733/  1.927705, val:  56.67%, val_best:  65.42%, tr:  94.89%, tr_best:  94.89%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1056%\n",
      "layer   2  Sparsity: 80.4913%\n",
      "layer   3  Sparsity: 77.9580%\n",
      "total_backward_count 234960 real_backward_count 65262  27.776%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.802658/  1.922766, val:  60.00%, val_best:  65.42%, tr:  93.16%, tr_best:  94.89%, epoch time: 41.19 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0906%\n",
      "layer   2  Sparsity: 80.7190%\n",
      "layer   3  Sparsity: 78.0468%\n",
      "total_backward_count 239855 real_backward_count 66388  27.678%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.821355/  1.934164, val:  59.58%, val_best:  65.42%, tr:  93.87%, tr_best:  94.89%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0623%\n",
      "layer   2  Sparsity: 80.7213%\n",
      "layer   3  Sparsity: 78.0918%\n",
      "total_backward_count 244750 real_backward_count 67572  27.609%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.818556/  1.962988, val:  55.00%, val_best:  65.42%, tr:  92.54%, tr_best:  94.89%, epoch time: 40.87 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0599%\n",
      "layer   2  Sparsity: 80.7322%\n",
      "layer   3  Sparsity: 78.3411%\n",
      "total_backward_count 249645 real_backward_count 68750  27.539%\n",
      "fc layer 1 self.abs_max_out: 3002.0\n",
      "lif layer 1 self.abs_max_v: 3002.0\n",
      "fc layer 2 self.abs_max_out: 1965.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.820504/  1.925137, val:  59.17%, val_best:  65.42%, tr:  93.87%, tr_best:  94.89%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0512%\n",
      "layer   2  Sparsity: 80.6953%\n",
      "layer   3  Sparsity: 78.1201%\n",
      "total_backward_count 254540 real_backward_count 69889  27.457%\n",
      "fc layer 2 self.abs_max_out: 2012.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.819922/  1.935969, val:  58.33%, val_best:  65.42%, tr:  92.75%, tr_best:  94.89%, epoch time: 40.85 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0880%\n",
      "layer   2  Sparsity: 80.5811%\n",
      "layer   3  Sparsity: 78.0982%\n",
      "total_backward_count 259435 real_backward_count 71093  27.403%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.801327/  1.913906, val:  64.17%, val_best:  65.42%, tr:  94.48%, tr_best:  94.89%, epoch time: 40.74 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0749%\n",
      "layer   2  Sparsity: 80.5639%\n",
      "layer   3  Sparsity: 77.5382%\n",
      "total_backward_count 264330 real_backward_count 72187  27.309%\n",
      "fc layer 1 self.abs_max_out: 3103.0\n",
      "lif layer 1 self.abs_max_v: 3103.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.791377/  1.918162, val:  58.33%, val_best:  65.42%, tr:  92.95%, tr_best:  94.89%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0784%\n",
      "layer   2  Sparsity: 80.7589%\n",
      "layer   3  Sparsity: 77.2953%\n",
      "total_backward_count 269225 real_backward_count 73334  27.239%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.789868/  1.931243, val:  55.00%, val_best:  65.42%, tr:  94.38%, tr_best:  94.89%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0955%\n",
      "layer   2  Sparsity: 80.9050%\n",
      "layer   3  Sparsity: 77.3397%\n",
      "total_backward_count 274120 real_backward_count 74440  27.156%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.782961/  1.919343, val:  51.67%, val_best:  65.42%, tr:  94.18%, tr_best:  94.89%, epoch time: 40.74 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0696%\n",
      "layer   2  Sparsity: 80.7363%\n",
      "layer   3  Sparsity: 77.2045%\n",
      "total_backward_count 279015 real_backward_count 75530  27.070%\n",
      "fc layer 1 self.abs_max_out: 3149.0\n",
      "lif layer 1 self.abs_max_v: 3149.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.795733/  1.922626, val:  60.83%, val_best:  65.42%, tr:  94.79%, tr_best:  94.89%, epoch time: 41.07 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1177%\n",
      "layer   2  Sparsity: 80.8723%\n",
      "layer   3  Sparsity: 77.5791%\n",
      "total_backward_count 283910 real_backward_count 76578  26.973%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.785123/  1.910866, val:  62.08%, val_best:  65.42%, tr:  94.99%, tr_best:  94.99%, epoch time: 41.07 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0912%\n",
      "layer   2  Sparsity: 80.7861%\n",
      "layer   3  Sparsity: 77.1596%\n",
      "total_backward_count 288805 real_backward_count 77646  26.885%\n",
      "fc layer 2 self.abs_max_out: 2049.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.764521/  1.923696, val:  42.50%, val_best:  65.42%, tr:  94.08%, tr_best:  94.99%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0979%\n",
      "layer   2  Sparsity: 80.8410%\n",
      "layer   3  Sparsity: 77.4926%\n",
      "total_backward_count 293700 real_backward_count 78753  26.814%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.785137/  1.920950, val:  44.17%, val_best:  65.42%, tr:  93.87%, tr_best:  94.99%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0701%\n",
      "layer   2  Sparsity: 80.3290%\n",
      "layer   3  Sparsity: 77.7270%\n",
      "total_backward_count 298595 real_backward_count 79864  26.747%\n",
      "fc layer 3 self.abs_max_out: 372.0\n",
      "fc layer 1 self.abs_max_out: 3226.0\n",
      "lif layer 1 self.abs_max_v: 3226.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.763146/  1.894653, val:  52.50%, val_best:  65.42%, tr:  95.30%, tr_best:  95.30%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0457%\n",
      "layer   2  Sparsity: 80.3295%\n",
      "layer   3  Sparsity: 77.7387%\n",
      "total_backward_count 303490 real_backward_count 80942  26.670%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.769956/  1.894697, val:  65.83%, val_best:  65.83%, tr:  94.08%, tr_best:  95.30%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0803%\n",
      "layer   2  Sparsity: 80.5188%\n",
      "layer   3  Sparsity: 77.6126%\n",
      "total_backward_count 308385 real_backward_count 82001  26.590%\n",
      "lif layer 2 self.abs_max_v: 2386.0\n",
      "lif layer 2 self.abs_max_v: 2415.5\n",
      "fc layer 2 self.abs_max_out: 2108.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.772019/  1.926481, val:  46.25%, val_best:  65.83%, tr:  94.08%, tr_best:  95.30%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1007%\n",
      "layer   2  Sparsity: 80.3902%\n",
      "layer   3  Sparsity: 77.5181%\n",
      "total_backward_count 313280 real_backward_count 83073  26.517%\n",
      "lif layer 2 self.abs_max_v: 2473.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.776897/  1.915498, val:  47.92%, val_best:  65.83%, tr:  93.67%, tr_best:  95.30%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0890%\n",
      "layer   2  Sparsity: 80.2405%\n",
      "layer   3  Sparsity: 77.3226%\n",
      "total_backward_count 318175 real_backward_count 84149  26.447%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.760469/  1.908346, val:  57.08%, val_best:  65.83%, tr:  95.40%, tr_best:  95.40%, epoch time: 40.10 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1084%\n",
      "layer   2  Sparsity: 80.3323%\n",
      "layer   3  Sparsity: 77.2277%\n",
      "total_backward_count 323070 real_backward_count 85169  26.362%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.751208/  1.894606, val:  65.42%, val_best:  65.83%, tr:  96.22%, tr_best:  96.22%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0939%\n",
      "layer   2  Sparsity: 80.2007%\n",
      "layer   3  Sparsity: 77.0595%\n",
      "total_backward_count 327965 real_backward_count 86191  26.281%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.757304/  1.916606, val:  65.42%, val_best:  65.83%, tr:  93.56%, tr_best:  96.22%, epoch time: 38.46 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 96.0716%\n",
      "layer   2  Sparsity: 80.4458%\n",
      "layer   3  Sparsity: 76.9783%\n",
      "total_backward_count 332860 real_backward_count 87273  26.219%\n",
      "lif layer 2 self.abs_max_v: 2539.0\n",
      "fc layer 3 self.abs_max_out: 393.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.771294/  1.877740, val:  66.25%, val_best:  66.25%, tr:  93.77%, tr_best:  96.22%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0811%\n",
      "layer   2  Sparsity: 80.5335%\n",
      "layer   3  Sparsity: 77.7550%\n",
      "total_backward_count 337755 real_backward_count 88333  26.153%\n",
      "fc layer 2 self.abs_max_out: 2185.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.766039/  1.907847, val:  65.83%, val_best:  66.25%, tr:  93.67%, tr_best:  96.22%, epoch time: 41.36 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0668%\n",
      "layer   2  Sparsity: 80.4316%\n",
      "layer   3  Sparsity: 77.5989%\n",
      "total_backward_count 342650 real_backward_count 89386  26.087%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.776765/  1.883980, val:  67.08%, val_best:  67.08%, tr:  95.71%, tr_best:  96.22%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0675%\n",
      "layer   2  Sparsity: 80.3065%\n",
      "layer   3  Sparsity: 77.1802%\n",
      "total_backward_count 347545 real_backward_count 90430  26.020%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.758296/  1.912713, val:  49.17%, val_best:  67.08%, tr:  94.69%, tr_best:  96.22%, epoch time: 40.88 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0661%\n",
      "layer   2  Sparsity: 80.2878%\n",
      "layer   3  Sparsity: 77.3342%\n",
      "total_backward_count 352440 real_backward_count 91486  25.958%\n",
      "lif layer 2 self.abs_max_v: 2556.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.760238/  1.889108, val:  63.33%, val_best:  67.08%, tr:  94.08%, tr_best:  96.22%, epoch time: 40.93 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0346%\n",
      "layer   2  Sparsity: 80.2933%\n",
      "layer   3  Sparsity: 77.7107%\n",
      "total_backward_count 357335 real_backward_count 92536  25.896%\n",
      "fc layer 2 self.abs_max_out: 2268.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.748592/  1.881321, val:  52.92%, val_best:  67.08%, tr:  95.81%, tr_best:  96.22%, epoch time: 41.33 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.1057%\n",
      "layer   2  Sparsity: 80.2797%\n",
      "layer   3  Sparsity: 77.4632%\n",
      "total_backward_count 362230 real_backward_count 93508  25.815%\n",
      "lif layer 1 self.abs_max_v: 3265.5\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.752420/  1.908968, val:  50.42%, val_best:  67.08%, tr:  93.87%, tr_best:  96.22%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0454%\n",
      "layer   2  Sparsity: 80.2150%\n",
      "layer   3  Sparsity: 77.1444%\n",
      "total_backward_count 367125 real_backward_count 94582  25.763%\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.740575/  1.893556, val:  55.00%, val_best:  67.08%, tr:  94.99%, tr_best:  96.22%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1008%\n",
      "layer   2  Sparsity: 80.4300%\n",
      "layer   3  Sparsity: 77.3371%\n",
      "total_backward_count 372020 real_backward_count 95600  25.698%\n",
      "lif layer 1 self.abs_max_v: 3274.5\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.767225/  1.903041, val:  57.08%, val_best:  67.08%, tr:  94.28%, tr_best:  96.22%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0735%\n",
      "layer   2  Sparsity: 80.3860%\n",
      "layer   3  Sparsity: 77.3784%\n",
      "total_backward_count 376915 real_backward_count 96700  25.656%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.778359/  1.920950, val:  50.00%, val_best:  67.08%, tr:  94.38%, tr_best:  96.22%, epoch time: 41.15 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0869%\n",
      "layer   2  Sparsity: 80.4230%\n",
      "layer   3  Sparsity: 77.5490%\n",
      "total_backward_count 381810 real_backward_count 97723  25.595%\n",
      "fc layer 2 self.abs_max_out: 2284.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.771916/  1.898774, val:  57.08%, val_best:  67.08%, tr:  96.22%, tr_best:  96.22%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0882%\n",
      "layer   2  Sparsity: 80.6568%\n",
      "layer   3  Sparsity: 77.5174%\n",
      "total_backward_count 386705 real_backward_count 98700  25.523%\n",
      "fc layer 2 self.abs_max_out: 2285.0\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.759420/  1.879896, val:  67.50%, val_best:  67.50%, tr:  95.40%, tr_best:  96.22%, epoch time: 41.14 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0937%\n",
      "layer   2  Sparsity: 80.2963%\n",
      "layer   3  Sparsity: 77.7430%\n",
      "total_backward_count 391600 real_backward_count 99745  25.471%\n",
      "fc layer 2 self.abs_max_out: 2350.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.753682/  1.888391, val:  58.33%, val_best:  67.50%, tr:  95.40%, tr_best:  96.22%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0709%\n",
      "layer   2  Sparsity: 80.2398%\n",
      "layer   3  Sparsity: 77.4385%\n",
      "total_backward_count 396495 real_backward_count 100757  25.412%\n",
      "fc layer 1 self.abs_max_out: 3267.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.755714/  1.896000, val:  62.92%, val_best:  67.50%, tr:  95.10%, tr_best:  96.22%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1009%\n",
      "layer   2  Sparsity: 80.4090%\n",
      "layer   3  Sparsity: 77.8330%\n",
      "total_backward_count 401390 real_backward_count 101813  25.365%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.765293/  1.892696, val:  63.33%, val_best:  67.50%, tr:  94.08%, tr_best:  96.22%, epoch time: 41.35 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0852%\n",
      "layer   2  Sparsity: 80.2680%\n",
      "layer   3  Sparsity: 77.9733%\n",
      "total_backward_count 406285 real_backward_count 102888  25.324%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.750984/  1.892633, val:  67.50%, val_best:  67.50%, tr:  94.18%, tr_best:  96.22%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0490%\n",
      "layer   2  Sparsity: 80.1285%\n",
      "layer   3  Sparsity: 77.8074%\n",
      "total_backward_count 411180 real_backward_count 103910  25.271%\n",
      "fc layer 1 self.abs_max_out: 3299.0\n",
      "lif layer 1 self.abs_max_v: 3299.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.753625/  1.862460, val:  67.92%, val_best:  67.92%, tr:  94.99%, tr_best:  96.22%, epoch time: 42.03 seconds, 0.70 minutes\n",
      "layer   1  Sparsity: 96.0741%\n",
      "layer   2  Sparsity: 80.1108%\n",
      "layer   3  Sparsity: 77.2411%\n",
      "total_backward_count 416075 real_backward_count 104939  25.221%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.742436/  1.909068, val:  63.75%, val_best:  67.92%, tr:  94.59%, tr_best:  96.22%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0961%\n",
      "layer   2  Sparsity: 80.3351%\n",
      "layer   3  Sparsity: 77.7343%\n",
      "total_backward_count 420970 real_backward_count 105959  25.170%\n",
      "lif layer 1 self.abs_max_v: 3313.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.737506/  1.889643, val:  55.00%, val_best:  67.92%, tr:  94.48%, tr_best:  96.22%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0975%\n",
      "layer   2  Sparsity: 80.1837%\n",
      "layer   3  Sparsity: 77.7180%\n",
      "total_backward_count 425865 real_backward_count 106949  25.113%\n",
      "lif layer 1 self.abs_max_v: 3368.0\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.753208/  1.884463, val:  65.00%, val_best:  67.92%, tr:  94.18%, tr_best:  96.22%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0569%\n",
      "layer   2  Sparsity: 80.0273%\n",
      "layer   3  Sparsity: 77.8864%\n",
      "total_backward_count 430760 real_backward_count 107983  25.068%\n",
      "lif layer 1 self.abs_max_v: 3376.5\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.759158/  1.883964, val:  62.92%, val_best:  67.92%, tr:  95.51%, tr_best:  96.22%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0823%\n",
      "layer   2  Sparsity: 80.1423%\n",
      "layer   3  Sparsity: 77.7206%\n",
      "total_backward_count 435655 real_backward_count 108994  25.018%\n",
      "lif layer 1 self.abs_max_v: 3499.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.760861/  1.852532, val:  71.67%, val_best:  71.67%, tr:  96.22%, tr_best:  96.22%, epoch time: 40.75 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1044%\n",
      "layer   2  Sparsity: 80.0231%\n",
      "layer   3  Sparsity: 77.6747%\n",
      "total_backward_count 440550 real_backward_count 109990  24.967%\n",
      "fc layer 1 self.abs_max_out: 3339.0\n",
      "lif layer 1 self.abs_max_v: 3522.5\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.742024/  1.876657, val:  71.25%, val_best:  71.67%, tr:  95.51%, tr_best:  96.22%, epoch time: 41.28 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0893%\n",
      "layer   2  Sparsity: 80.0621%\n",
      "layer   3  Sparsity: 77.5991%\n",
      "total_backward_count 445445 real_backward_count 110973  24.913%\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "fc layer 1 self.abs_max_out: 3344.0\n",
      "lif layer 1 self.abs_max_v: 3579.0\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.722218/  1.833039, val:  66.67%, val_best:  71.67%, tr:  96.02%, tr_best:  96.22%, epoch time: 40.50 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0599%\n",
      "layer   2  Sparsity: 80.0000%\n",
      "layer   3  Sparsity: 77.4186%\n",
      "total_backward_count 450340 real_backward_count 111955  24.860%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.725225/  1.851749, val:  62.50%, val_best:  71.67%, tr:  96.53%, tr_best:  96.53%, epoch time: 41.26 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0882%\n",
      "layer   2  Sparsity: 80.0254%\n",
      "layer   3  Sparsity: 77.6528%\n",
      "total_backward_count 455235 real_backward_count 112908  24.802%\n",
      "fc layer 3 self.abs_max_out: 402.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.715743/  1.856477, val:  66.25%, val_best:  71.67%, tr:  95.91%, tr_best:  96.53%, epoch time: 40.83 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0762%\n",
      "layer   2  Sparsity: 80.1255%\n",
      "layer   3  Sparsity: 77.5364%\n",
      "total_backward_count 460130 real_backward_count 113850  24.743%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.725075/  1.853364, val:  53.33%, val_best:  71.67%, tr:  95.30%, tr_best:  96.53%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1191%\n",
      "layer   2  Sparsity: 80.1984%\n",
      "layer   3  Sparsity: 77.2674%\n",
      "total_backward_count 465025 real_backward_count 114855  24.699%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.714822/  1.862262, val:  73.33%, val_best:  73.33%, tr:  96.22%, tr_best:  96.53%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0747%\n",
      "layer   2  Sparsity: 79.9880%\n",
      "layer   3  Sparsity: 77.3938%\n",
      "total_backward_count 469920 real_backward_count 115831  24.649%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.738456/  1.871770, val:  63.75%, val_best:  73.33%, tr:  95.40%, tr_best:  96.53%, epoch time: 41.00 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0757%\n",
      "layer   2  Sparsity: 80.0016%\n",
      "layer   3  Sparsity: 76.8934%\n",
      "total_backward_count 474815 real_backward_count 116806  24.600%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.718164/  1.847614, val:  71.67%, val_best:  73.33%, tr:  96.63%, tr_best:  96.63%, epoch time: 40.72 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0952%\n",
      "layer   2  Sparsity: 80.0758%\n",
      "layer   3  Sparsity: 76.8663%\n",
      "total_backward_count 479710 real_backward_count 117741  24.544%\n",
      "fc layer 1 self.abs_max_out: 3514.0\n",
      "fc layer 2 self.abs_max_out: 2361.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.706881/  1.867391, val:  60.42%, val_best:  73.33%, tr:  96.02%, tr_best:  96.63%, epoch time: 41.37 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.1165%\n",
      "layer   2  Sparsity: 80.0967%\n",
      "layer   3  Sparsity: 77.0957%\n",
      "total_backward_count 484605 real_backward_count 118724  24.499%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.720354/  1.870059, val:  63.33%, val_best:  73.33%, tr:  94.99%, tr_best:  96.63%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1006%\n",
      "layer   2  Sparsity: 79.9115%\n",
      "layer   3  Sparsity: 77.2695%\n",
      "total_backward_count 489500 real_backward_count 119663  24.446%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.713032/  1.837418, val:  73.33%, val_best:  73.33%, tr:  96.22%, tr_best:  96.63%, epoch time: 40.74 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0889%\n",
      "layer   2  Sparsity: 79.9412%\n",
      "layer   3  Sparsity: 77.3811%\n",
      "total_backward_count 494395 real_backward_count 120643  24.402%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.725428/  1.858278, val:  62.08%, val_best:  73.33%, tr:  96.42%, tr_best:  96.63%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0626%\n",
      "layer   2  Sparsity: 79.9983%\n",
      "layer   3  Sparsity: 77.4309%\n",
      "total_backward_count 499290 real_backward_count 121583  24.351%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.736903/  1.882215, val:  62.50%, val_best:  73.33%, tr:  95.51%, tr_best:  96.63%, epoch time: 41.31 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0698%\n",
      "layer   2  Sparsity: 80.2497%\n",
      "layer   3  Sparsity: 77.4496%\n",
      "total_backward_count 504185 real_backward_count 122563  24.309%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.735669/  1.859649, val:  61.25%, val_best:  73.33%, tr:  96.32%, tr_best:  96.63%, epoch time: 40.58 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0561%\n",
      "layer   2  Sparsity: 80.0464%\n",
      "layer   3  Sparsity: 77.3039%\n",
      "total_backward_count 509080 real_backward_count 123513  24.262%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.729376/  1.862994, val:  67.08%, val_best:  73.33%, tr:  95.51%, tr_best:  96.63%, epoch time: 41.36 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0523%\n",
      "layer   2  Sparsity: 80.1263%\n",
      "layer   3  Sparsity: 77.0695%\n",
      "total_backward_count 513975 real_backward_count 124485  24.220%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.715943/  1.836519, val:  65.42%, val_best:  73.33%, tr:  95.71%, tr_best:  96.63%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1001%\n",
      "layer   2  Sparsity: 80.2375%\n",
      "layer   3  Sparsity: 76.6175%\n",
      "total_backward_count 518870 real_backward_count 125496  24.186%\n",
      "lif layer 1 self.abs_max_v: 3591.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.711669/  1.856037, val:  63.33%, val_best:  73.33%, tr:  96.12%, tr_best:  96.63%, epoch time: 41.33 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0952%\n",
      "layer   2  Sparsity: 80.1317%\n",
      "layer   3  Sparsity: 76.5632%\n",
      "total_backward_count 523765 real_backward_count 126430  24.139%\n",
      "fc layer 2 self.abs_max_out: 2377.0\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.707642/  1.842837, val:  58.75%, val_best:  73.33%, tr:  96.32%, tr_best:  96.63%, epoch time: 40.93 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1034%\n",
      "layer   2  Sparsity: 80.2252%\n",
      "layer   3  Sparsity: 76.8062%\n",
      "total_backward_count 528660 real_backward_count 127353  24.090%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.728246/  1.851010, val:  65.00%, val_best:  73.33%, tr:  95.40%, tr_best:  96.63%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1066%\n",
      "layer   2  Sparsity: 80.0351%\n",
      "layer   3  Sparsity: 77.3206%\n",
      "total_backward_count 533555 real_backward_count 128305  24.047%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.726618/  1.840035, val:  57.92%, val_best:  73.33%, tr:  96.42%, tr_best:  96.63%, epoch time: 40.75 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0839%\n",
      "layer   2  Sparsity: 80.0454%\n",
      "layer   3  Sparsity: 77.4873%\n",
      "total_backward_count 538450 real_backward_count 129257  24.005%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.725793/  1.848976, val:  67.50%, val_best:  73.33%, tr:  96.32%, tr_best:  96.63%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0905%\n",
      "layer   2  Sparsity: 80.1052%\n",
      "layer   3  Sparsity: 77.4494%\n",
      "total_backward_count 543345 real_backward_count 130193  23.961%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.715620/  1.841790, val:  72.50%, val_best:  73.33%, tr:  95.81%, tr_best:  96.63%, epoch time: 40.81 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0699%\n",
      "layer   2  Sparsity: 80.0714%\n",
      "layer   3  Sparsity: 77.6949%\n",
      "total_backward_count 548240 real_backward_count 131141  23.920%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.708556/  1.848482, val:  70.83%, val_best:  73.33%, tr:  96.22%, tr_best:  96.63%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0919%\n",
      "layer   2  Sparsity: 80.1535%\n",
      "layer   3  Sparsity: 77.8364%\n",
      "total_backward_count 553135 real_backward_count 132026  23.869%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.711722/  1.836006, val:  69.58%, val_best:  73.33%, tr:  96.73%, tr_best:  96.73%, epoch time: 40.89 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0676%\n",
      "layer   2  Sparsity: 79.9780%\n",
      "layer   3  Sparsity: 77.4743%\n",
      "total_backward_count 558030 real_backward_count 132981  23.830%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.713979/  1.858173, val:  54.17%, val_best:  73.33%, tr:  95.51%, tr_best:  96.73%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0713%\n",
      "layer   2  Sparsity: 80.1469%\n",
      "layer   3  Sparsity: 77.5592%\n",
      "total_backward_count 562925 real_backward_count 133952  23.796%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.713814/  1.867660, val:  67.08%, val_best:  73.33%, tr:  96.12%, tr_best:  96.73%, epoch time: 41.02 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0918%\n",
      "layer   2  Sparsity: 80.1577%\n",
      "layer   3  Sparsity: 77.6779%\n",
      "total_backward_count 567820 real_backward_count 134881  23.754%\n",
      "lif layer 2 self.abs_max_v: 2580.5\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.722433/  1.861646, val:  54.58%, val_best:  73.33%, tr:  96.32%, tr_best:  96.73%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0841%\n",
      "layer   2  Sparsity: 79.9902%\n",
      "layer   3  Sparsity: 77.6422%\n",
      "total_backward_count 572715 real_backward_count 135813  23.714%\n",
      "lif layer 2 self.abs_max_v: 2599.0\n",
      "lif layer 2 self.abs_max_v: 2618.5\n",
      "lif layer 2 self.abs_max_v: 2728.5\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.720253/  1.865712, val:  53.33%, val_best:  73.33%, tr:  95.40%, tr_best:  96.73%, epoch time: 41.01 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0854%\n",
      "layer   2  Sparsity: 79.9796%\n",
      "layer   3  Sparsity: 77.4155%\n",
      "total_backward_count 577610 real_backward_count 136813  23.686%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.718679/  1.828526, val:  65.00%, val_best:  73.33%, tr:  96.22%, tr_best:  96.73%, epoch time: 40.79 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1080%\n",
      "layer   2  Sparsity: 80.0601%\n",
      "layer   3  Sparsity: 77.2831%\n",
      "total_backward_count 582505 real_backward_count 137757  23.649%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.693358/  1.830366, val:  69.58%, val_best:  73.33%, tr:  96.63%, tr_best:  96.73%, epoch time: 41.10 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0746%\n",
      "layer   2  Sparsity: 80.0316%\n",
      "layer   3  Sparsity: 77.2640%\n",
      "total_backward_count 587400 real_backward_count 138680  23.609%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.706380/  1.853738, val:  67.92%, val_best:  73.33%, tr:  95.61%, tr_best:  96.73%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0854%\n",
      "layer   2  Sparsity: 80.0005%\n",
      "layer   3  Sparsity: 77.2086%\n",
      "total_backward_count 592295 real_backward_count 139628  23.574%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.716778/  1.858811, val:  66.67%, val_best:  73.33%, tr:  96.63%, tr_best:  96.73%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0895%\n",
      "layer   2  Sparsity: 80.0719%\n",
      "layer   3  Sparsity: 77.4030%\n",
      "total_backward_count 597190 real_backward_count 140532  23.532%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.705208/  1.856867, val:  55.42%, val_best:  73.33%, tr:  96.22%, tr_best:  96.73%, epoch time: 39.91 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0506%\n",
      "layer   2  Sparsity: 79.9320%\n",
      "layer   3  Sparsity: 76.9651%\n",
      "total_backward_count 602085 real_backward_count 141461  23.495%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.715525/  1.846100, val:  72.92%, val_best:  73.33%, tr:  96.12%, tr_best:  96.73%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0922%\n",
      "layer   2  Sparsity: 80.1856%\n",
      "layer   3  Sparsity: 77.4237%\n",
      "total_backward_count 606980 real_backward_count 142404  23.461%\n",
      "fc layer 2 self.abs_max_out: 2442.0\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.708654/  1.836656, val:  69.58%, val_best:  73.33%, tr:  96.83%, tr_best:  96.83%, epoch time: 39.94 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0639%\n",
      "layer   2  Sparsity: 79.9461%\n",
      "layer   3  Sparsity: 77.3785%\n",
      "total_backward_count 611875 real_backward_count 143337  23.426%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.702271/  1.833564, val:  56.25%, val_best:  73.33%, tr:  96.53%, tr_best:  96.83%, epoch time: 40.24 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0678%\n",
      "layer   2  Sparsity: 79.9575%\n",
      "layer   3  Sparsity: 77.0859%\n",
      "total_backward_count 616770 real_backward_count 144231  23.385%\n",
      "fc layer 2 self.abs_max_out: 2451.0\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.686251/  1.838414, val:  58.75%, val_best:  73.33%, tr:  95.40%, tr_best:  96.83%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0557%\n",
      "layer   2  Sparsity: 79.8796%\n",
      "layer   3  Sparsity: 77.4014%\n",
      "total_backward_count 621665 real_backward_count 145150  23.349%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.690728/  1.835219, val:  65.42%, val_best:  73.33%, tr:  96.94%, tr_best:  96.94%, epoch time: 40.87 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0629%\n",
      "layer   2  Sparsity: 79.8263%\n",
      "layer   3  Sparsity: 77.0139%\n",
      "total_backward_count 626560 real_backward_count 146034  23.307%\n",
      "lif layer 1 self.abs_max_v: 3638.5\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.705873/  1.852964, val:  60.00%, val_best:  73.33%, tr:  95.71%, tr_best:  96.94%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0712%\n",
      "layer   2  Sparsity: 79.9788%\n",
      "layer   3  Sparsity: 77.1253%\n",
      "total_backward_count 631455 real_backward_count 146932  23.269%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.711122/  1.846384, val:  67.08%, val_best:  73.33%, tr:  96.22%, tr_best:  96.94%, epoch time: 40.82 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0971%\n",
      "layer   2  Sparsity: 79.8663%\n",
      "layer   3  Sparsity: 77.0176%\n",
      "total_backward_count 636350 real_backward_count 147860  23.236%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.715385/  1.827213, val:  46.67%, val_best:  73.33%, tr:  96.53%, tr_best:  96.94%, epoch time: 40.84 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0930%\n",
      "layer   2  Sparsity: 79.9272%\n",
      "layer   3  Sparsity: 76.9614%\n",
      "total_backward_count 641245 real_backward_count 148776  23.201%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.687229/  1.834334, val:  57.50%, val_best:  73.33%, tr:  96.53%, tr_best:  96.94%, epoch time: 41.00 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0960%\n",
      "layer   2  Sparsity: 79.8243%\n",
      "layer   3  Sparsity: 77.0098%\n",
      "total_backward_count 646140 real_backward_count 149645  23.160%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.692908/  1.846686, val:  57.50%, val_best:  73.33%, tr:  96.53%, tr_best:  96.94%, epoch time: 40.89 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0748%\n",
      "layer   2  Sparsity: 79.8114%\n",
      "layer   3  Sparsity: 77.3838%\n",
      "total_backward_count 651035 real_backward_count 150513  23.119%\n",
      "fc layer 1 self.abs_max_out: 3773.0\n",
      "lif layer 1 self.abs_max_v: 3773.0\n",
      "lif layer 1 self.abs_max_v: 3886.0\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.708206/  1.839313, val:  70.83%, val_best:  73.33%, tr:  96.94%, tr_best:  96.94%, epoch time: 40.89 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0448%\n",
      "layer   2  Sparsity: 79.7740%\n",
      "layer   3  Sparsity: 77.4984%\n",
      "total_backward_count 655930 real_backward_count 151407  23.083%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.703996/  1.842727, val:  61.67%, val_best:  73.33%, tr:  96.32%, tr_best:  96.94%, epoch time: 40.72 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0890%\n",
      "layer   2  Sparsity: 80.0336%\n",
      "layer   3  Sparsity: 77.2589%\n",
      "total_backward_count 660825 real_backward_count 152368  23.057%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.710162/  1.836407, val:  67.08%, val_best:  73.33%, tr:  96.53%, tr_best:  96.94%, epoch time: 40.75 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0874%\n",
      "layer   2  Sparsity: 80.0501%\n",
      "layer   3  Sparsity: 76.8934%\n",
      "total_backward_count 665720 real_backward_count 153239  23.019%\n",
      "fc layer 2 self.abs_max_out: 2453.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.697369/  1.828141, val:  66.25%, val_best:  73.33%, tr:  97.34%, tr_best:  97.34%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0833%\n",
      "layer   2  Sparsity: 80.0180%\n",
      "layer   3  Sparsity: 76.9847%\n",
      "total_backward_count 670615 real_backward_count 154131  22.984%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.692236/  1.829732, val:  67.50%, val_best:  73.33%, tr:  96.02%, tr_best:  97.34%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0546%\n",
      "layer   2  Sparsity: 79.9342%\n",
      "layer   3  Sparsity: 77.0077%\n",
      "total_backward_count 675510 real_backward_count 155010  22.947%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.700477/  1.798957, val:  74.17%, val_best:  74.17%, tr:  97.45%, tr_best:  97.45%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0836%\n",
      "layer   2  Sparsity: 80.0251%\n",
      "layer   3  Sparsity: 76.8304%\n",
      "total_backward_count 680405 real_backward_count 155917  22.915%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.678526/  1.838778, val:  65.00%, val_best:  74.17%, tr:  96.42%, tr_best:  97.45%, epoch time: 40.84 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0693%\n",
      "layer   2  Sparsity: 79.8016%\n",
      "layer   3  Sparsity: 76.5244%\n",
      "total_backward_count 685300 real_backward_count 156804  22.881%\n",
      "lif layer 2 self.abs_max_v: 2826.0\n",
      "fc layer 2 self.abs_max_out: 2464.0\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.693632/  1.812723, val:  73.75%, val_best:  74.17%, tr:  96.83%, tr_best:  97.45%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0761%\n",
      "layer   2  Sparsity: 79.5661%\n",
      "layer   3  Sparsity: 76.5262%\n",
      "total_backward_count 690195 real_backward_count 157681  22.846%\n",
      "fc layer 2 self.abs_max_out: 2493.0\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.690792/  1.838780, val:  70.42%, val_best:  74.17%, tr:  96.02%, tr_best:  97.45%, epoch time: 40.89 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0790%\n",
      "layer   2  Sparsity: 79.5875%\n",
      "layer   3  Sparsity: 76.4003%\n",
      "total_backward_count 695090 real_backward_count 158575  22.814%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.686127/  1.815920, val:  60.42%, val_best:  74.17%, tr:  96.32%, tr_best:  97.45%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0826%\n",
      "layer   2  Sparsity: 79.7585%\n",
      "layer   3  Sparsity: 76.5441%\n",
      "total_backward_count 699985 real_backward_count 159435  22.777%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.678646/  1.808670, val:  67.08%, val_best:  74.17%, tr:  96.42%, tr_best:  97.45%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0689%\n",
      "layer   2  Sparsity: 79.9066%\n",
      "layer   3  Sparsity: 76.9735%\n",
      "total_backward_count 704880 real_backward_count 160336  22.747%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.690201/  1.818384, val:  71.67%, val_best:  74.17%, tr:  95.81%, tr_best:  97.45%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0843%\n",
      "layer   2  Sparsity: 79.9563%\n",
      "layer   3  Sparsity: 76.8199%\n",
      "total_backward_count 709775 real_backward_count 161251  22.719%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.672395/  1.801298, val:  69.58%, val_best:  74.17%, tr:  96.63%, tr_best:  97.45%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0887%\n",
      "layer   2  Sparsity: 79.8773%\n",
      "layer   3  Sparsity: 76.9161%\n",
      "total_backward_count 714670 real_backward_count 162115  22.684%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.690798/  1.835612, val:  56.67%, val_best:  74.17%, tr:  96.63%, tr_best:  97.45%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1241%\n",
      "layer   2  Sparsity: 79.9647%\n",
      "layer   3  Sparsity: 77.0628%\n",
      "total_backward_count 719565 real_backward_count 163033  22.657%\n",
      "fc layer 2 self.abs_max_out: 2502.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.672390/  1.830925, val:  48.75%, val_best:  74.17%, tr:  97.14%, tr_best:  97.45%, epoch time: 40.70 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0959%\n",
      "layer   2  Sparsity: 79.8935%\n",
      "layer   3  Sparsity: 77.0260%\n",
      "total_backward_count 724460 real_backward_count 163919  22.626%\n",
      "lif layer 2 self.abs_max_v: 2847.5\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.664111/  1.798512, val:  72.92%, val_best:  74.17%, tr:  96.32%, tr_best:  97.45%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0506%\n",
      "layer   2  Sparsity: 79.6811%\n",
      "layer   3  Sparsity: 76.8870%\n",
      "total_backward_count 729355 real_backward_count 164844  22.601%\n",
      "fc layer 1 self.abs_max_out: 3790.0\n",
      "fc layer 2 self.abs_max_out: 2509.0\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.688557/  1.811038, val:  71.67%, val_best:  74.17%, tr:  96.42%, tr_best:  97.45%, epoch time: 41.02 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1114%\n",
      "layer   2  Sparsity: 79.6153%\n",
      "layer   3  Sparsity: 76.5964%\n",
      "total_backward_count 734250 real_backward_count 165768  22.577%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.675400/  1.788883, val:  65.42%, val_best:  74.17%, tr:  96.42%, tr_best:  97.45%, epoch time: 40.71 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0660%\n",
      "layer   2  Sparsity: 79.6206%\n",
      "layer   3  Sparsity: 76.7043%\n",
      "total_backward_count 739145 real_backward_count 166645  22.546%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.684232/  1.800659, val:  74.58%, val_best:  74.58%, tr:  96.63%, tr_best:  97.45%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0912%\n",
      "layer   2  Sparsity: 79.8035%\n",
      "layer   3  Sparsity: 77.3490%\n",
      "total_backward_count 744040 real_backward_count 167562  22.521%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.695036/  1.826082, val:  67.50%, val_best:  74.58%, tr:  96.73%, tr_best:  97.45%, epoch time: 40.90 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0951%\n",
      "layer   2  Sparsity: 79.7797%\n",
      "layer   3  Sparsity: 77.4801%\n",
      "total_backward_count 748935 real_backward_count 168447  22.492%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.702754/  1.803936, val:  80.42%, val_best:  80.42%, tr:  96.32%, tr_best:  97.45%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1049%\n",
      "layer   2  Sparsity: 79.7322%\n",
      "layer   3  Sparsity: 77.5609%\n",
      "total_backward_count 753830 real_backward_count 169371  22.468%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.700814/  1.823388, val:  75.83%, val_best:  80.42%, tr:  96.73%, tr_best:  97.45%, epoch time: 41.17 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0807%\n",
      "layer   2  Sparsity: 79.7157%\n",
      "layer   3  Sparsity: 77.3865%\n",
      "total_backward_count 758725 real_backward_count 170229  22.436%\n",
      "fc layer 3 self.abs_max_out: 407.0\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.691786/  1.831662, val:  66.67%, val_best:  80.42%, tr:  96.63%, tr_best:  97.45%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0633%\n",
      "layer   2  Sparsity: 79.7277%\n",
      "layer   3  Sparsity: 77.2669%\n",
      "total_backward_count 763620 real_backward_count 171036  22.398%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.698189/  1.849558, val:  61.25%, val_best:  80.42%, tr:  96.63%, tr_best:  97.45%, epoch time: 40.77 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0767%\n",
      "layer   2  Sparsity: 79.4974%\n",
      "layer   3  Sparsity: 77.3584%\n",
      "total_backward_count 768515 real_backward_count 171888  22.366%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.700748/  1.836192, val:  62.08%, val_best:  80.42%, tr:  96.94%, tr_best:  97.45%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1027%\n",
      "layer   2  Sparsity: 79.6920%\n",
      "layer   3  Sparsity: 77.4272%\n",
      "total_backward_count 773410 real_backward_count 172756  22.337%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.696726/  1.811711, val:  74.58%, val_best:  80.42%, tr:  96.22%, tr_best:  97.45%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0820%\n",
      "layer   2  Sparsity: 79.5372%\n",
      "layer   3  Sparsity: 77.3082%\n",
      "total_backward_count 778305 real_backward_count 173619  22.307%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.700130/  1.808737, val:  71.67%, val_best:  80.42%, tr:  96.94%, tr_best:  97.45%, epoch time: 40.63 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0684%\n",
      "layer   2  Sparsity: 79.6489%\n",
      "layer   3  Sparsity: 77.3405%\n",
      "total_backward_count 783200 real_backward_count 174509  22.282%\n",
      "lif layer 1 self.abs_max_v: 3957.5\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.672705/  1.798972, val:  63.33%, val_best:  80.42%, tr:  95.91%, tr_best:  97.45%, epoch time: 40.79 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0825%\n",
      "layer   2  Sparsity: 79.6480%\n",
      "layer   3  Sparsity: 77.0936%\n",
      "total_backward_count 788095 real_backward_count 175386  22.254%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.683409/  1.801162, val:  65.00%, val_best:  80.42%, tr:  97.75%, tr_best:  97.75%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0821%\n",
      "layer   2  Sparsity: 79.6186%\n",
      "layer   3  Sparsity: 76.9870%\n",
      "total_backward_count 792990 real_backward_count 176264  22.228%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.681829/  1.820509, val:  61.67%, val_best:  80.42%, tr:  96.42%, tr_best:  97.75%, epoch time: 41.10 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0815%\n",
      "layer   2  Sparsity: 79.8067%\n",
      "layer   3  Sparsity: 76.9571%\n",
      "total_backward_count 797885 real_backward_count 177133  22.200%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.683174/  1.839875, val:  64.17%, val_best:  80.42%, tr:  97.04%, tr_best:  97.75%, epoch time: 40.87 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0590%\n",
      "layer   2  Sparsity: 79.4674%\n",
      "layer   3  Sparsity: 76.8406%\n",
      "total_backward_count 802780 real_backward_count 177957  22.168%\n",
      "fc layer 2 self.abs_max_out: 2515.0\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.690731/  1.828137, val:  57.08%, val_best:  80.42%, tr:  97.55%, tr_best:  97.75%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0802%\n",
      "layer   2  Sparsity: 79.4710%\n",
      "layer   3  Sparsity: 76.9729%\n",
      "total_backward_count 807675 real_backward_count 178835  22.142%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.696174/  1.831757, val:  72.92%, val_best:  80.42%, tr:  97.14%, tr_best:  97.75%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0681%\n",
      "layer   2  Sparsity: 79.4726%\n",
      "layer   3  Sparsity: 77.1801%\n",
      "total_backward_count 812570 real_backward_count 179722  22.118%\n",
      "lif layer 1 self.abs_max_v: 3975.0\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.695908/  1.818461, val:  72.08%, val_best:  80.42%, tr:  97.45%, tr_best:  97.75%, epoch time: 41.05 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1115%\n",
      "layer   2  Sparsity: 79.4777%\n",
      "layer   3  Sparsity: 77.1749%\n",
      "total_backward_count 817465 real_backward_count 180583  22.091%\n",
      "lif layer 1 self.abs_max_v: 3990.0\n",
      "fc layer 2 self.abs_max_out: 2531.0\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.689922/  1.820477, val:  67.08%, val_best:  80.42%, tr:  96.73%, tr_best:  97.75%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0452%\n",
      "layer   2  Sparsity: 79.4806%\n",
      "layer   3  Sparsity: 76.9334%\n",
      "total_backward_count 822360 real_backward_count 181415  22.060%\n",
      "lif layer 1 self.abs_max_v: 4162.5\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.695709/  1.789532, val:  76.67%, val_best:  80.42%, tr:  97.14%, tr_best:  97.75%, epoch time: 41.55 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0967%\n",
      "layer   2  Sparsity: 79.7005%\n",
      "layer   3  Sparsity: 77.3231%\n",
      "total_backward_count 827255 real_backward_count 182258  22.032%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.690240/  1.811100, val:  73.33%, val_best:  80.42%, tr:  96.63%, tr_best:  97.75%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0538%\n",
      "layer   2  Sparsity: 79.7265%\n",
      "layer   3  Sparsity: 77.4486%\n",
      "total_backward_count 832150 real_backward_count 183136  22.008%\n",
      "lif layer 1 self.abs_max_v: 4247.5\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.691894/  1.826504, val:  68.75%, val_best:  80.42%, tr:  97.24%, tr_best:  97.75%, epoch time: 41.44 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0976%\n",
      "layer   2  Sparsity: 79.7349%\n",
      "layer   3  Sparsity: 77.9253%\n",
      "total_backward_count 837045 real_backward_count 183953  21.976%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.681217/  1.805080, val:  67.92%, val_best:  80.42%, tr:  97.34%, tr_best:  97.75%, epoch time: 40.65 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0818%\n",
      "layer   2  Sparsity: 79.2176%\n",
      "layer   3  Sparsity: 77.5731%\n",
      "total_backward_count 841940 real_backward_count 184763  21.945%\n",
      "fc layer 3 self.abs_max_out: 410.0\n",
      "lif layer 2 self.abs_max_v: 2879.0\n",
      "fc layer 1 self.abs_max_out: 3896.0\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.699324/  1.816653, val:  80.42%, val_best:  80.42%, tr:  97.65%, tr_best:  97.75%, epoch time: 41.24 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0803%\n",
      "layer   2  Sparsity: 79.3118%\n",
      "layer   3  Sparsity: 77.1908%\n",
      "total_backward_count 846835 real_backward_count 185613  21.918%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.677968/  1.804190, val:  75.42%, val_best:  80.42%, tr:  97.24%, tr_best:  97.75%, epoch time: 41.00 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0991%\n",
      "layer   2  Sparsity: 79.3262%\n",
      "layer   3  Sparsity: 76.6692%\n",
      "total_backward_count 851730 real_backward_count 186435  21.889%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.679823/  1.817490, val:  70.83%, val_best:  80.42%, tr:  96.73%, tr_best:  97.75%, epoch time: 40.74 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0641%\n",
      "layer   2  Sparsity: 79.5664%\n",
      "layer   3  Sparsity: 77.1851%\n",
      "total_backward_count 856625 real_backward_count 187266  21.861%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.679032/  1.826436, val:  64.58%, val_best:  80.42%, tr:  97.65%, tr_best:  97.75%, epoch time: 41.25 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0601%\n",
      "layer   2  Sparsity: 79.4217%\n",
      "layer   3  Sparsity: 77.2132%\n",
      "total_backward_count 861520 real_backward_count 188115  21.835%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.670507/  1.785740, val:  74.17%, val_best:  80.42%, tr:  97.65%, tr_best:  97.75%, epoch time: 41.32 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0994%\n",
      "layer   2  Sparsity: 79.5892%\n",
      "layer   3  Sparsity: 77.3011%\n",
      "total_backward_count 866415 real_backward_count 188894  21.802%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.661213/  1.801772, val:  73.33%, val_best:  80.42%, tr:  96.83%, tr_best:  97.75%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0569%\n",
      "layer   2  Sparsity: 79.6509%\n",
      "layer   3  Sparsity: 77.3473%\n",
      "total_backward_count 871310 real_backward_count 189716  21.774%\n",
      "fc layer 3 self.abs_max_out: 432.0\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.669584/  1.828115, val:  47.92%, val_best:  80.42%, tr:  96.63%, tr_best:  97.75%, epoch time: 41.11 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0772%\n",
      "layer   2  Sparsity: 79.6199%\n",
      "layer   3  Sparsity: 77.1982%\n",
      "total_backward_count 876205 real_backward_count 190550  21.747%\n",
      "fc layer 2 self.abs_max_out: 2552.0\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.661879/  1.779869, val:  77.50%, val_best:  80.42%, tr:  97.55%, tr_best:  97.75%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1000%\n",
      "layer   2  Sparsity: 79.5505%\n",
      "layer   3  Sparsity: 77.1395%\n",
      "total_backward_count 881100 real_backward_count 191404  21.723%\n",
      "fc layer 2 self.abs_max_out: 2577.0\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.661678/  1.785290, val:  72.08%, val_best:  80.42%, tr:  96.63%, tr_best:  97.75%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0653%\n",
      "layer   2  Sparsity: 79.4733%\n",
      "layer   3  Sparsity: 76.9814%\n",
      "total_backward_count 885995 real_backward_count 192271  21.701%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.660880/  1.802862, val:  73.75%, val_best:  80.42%, tr:  96.83%, tr_best:  97.75%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0768%\n",
      "layer   2  Sparsity: 79.4863%\n",
      "layer   3  Sparsity: 76.9518%\n",
      "total_backward_count 890890 real_backward_count 193123  21.678%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.654004/  1.814372, val:  67.08%, val_best:  80.42%, tr:  97.34%, tr_best:  97.75%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0624%\n",
      "layer   2  Sparsity: 79.3970%\n",
      "layer   3  Sparsity: 77.0045%\n",
      "total_backward_count 895785 real_backward_count 193943  21.651%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.676647/  1.820883, val:  71.25%, val_best:  80.42%, tr:  96.53%, tr_best:  97.75%, epoch time: 40.86 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0736%\n",
      "layer   2  Sparsity: 79.4285%\n",
      "layer   3  Sparsity: 76.8382%\n",
      "total_backward_count 900680 real_backward_count 194811  21.629%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.662937/  1.795347, val:  60.83%, val_best:  80.42%, tr:  96.83%, tr_best:  97.75%, epoch time: 41.15 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0627%\n",
      "layer   2  Sparsity: 79.4446%\n",
      "layer   3  Sparsity: 76.8807%\n",
      "total_backward_count 905575 real_backward_count 195689  21.609%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.657001/  1.792620, val:  65.00%, val_best:  80.42%, tr:  97.55%, tr_best:  97.75%, epoch time: 40.87 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1035%\n",
      "layer   2  Sparsity: 79.3947%\n",
      "layer   3  Sparsity: 76.9358%\n",
      "total_backward_count 910470 real_backward_count 196562  21.589%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.664799/  1.797801, val:  68.33%, val_best:  80.42%, tr:  96.63%, tr_best:  97.75%, epoch time: 41.04 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0663%\n",
      "layer   2  Sparsity: 79.4306%\n",
      "layer   3  Sparsity: 77.1321%\n",
      "total_backward_count 915365 real_backward_count 197387  21.564%\n",
      "lif layer 1 self.abs_max_v: 4300.5\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.666212/  1.767443, val:  77.92%, val_best:  80.42%, tr:  96.02%, tr_best:  97.75%, epoch time: 41.12 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0794%\n",
      "layer   2  Sparsity: 79.7244%\n",
      "layer   3  Sparsity: 77.4273%\n",
      "total_backward_count 920260 real_backward_count 198222  21.540%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.661262/  1.785032, val:  78.33%, val_best:  80.42%, tr:  97.96%, tr_best:  97.96%, epoch time: 41.22 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0674%\n",
      "layer   2  Sparsity: 79.6326%\n",
      "layer   3  Sparsity: 77.1646%\n",
      "total_backward_count 925155 real_backward_count 199033  21.513%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.653722/  1.787578, val:  72.08%, val_best:  80.42%, tr:  97.34%, tr_best:  97.96%, epoch time: 41.09 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0637%\n",
      "layer   2  Sparsity: 79.5389%\n",
      "layer   3  Sparsity: 77.1228%\n",
      "total_backward_count 930050 real_backward_count 199866  21.490%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.644201/  1.791614, val:  73.75%, val_best:  80.42%, tr:  96.63%, tr_best:  97.96%, epoch time: 41.04 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0963%\n",
      "layer   2  Sparsity: 79.6226%\n",
      "layer   3  Sparsity: 77.0040%\n",
      "total_backward_count 934945 real_backward_count 200707  21.467%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.661549/  1.789954, val:  72.08%, val_best:  80.42%, tr:  96.83%, tr_best:  97.96%, epoch time: 40.80 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1051%\n",
      "layer   2  Sparsity: 79.5277%\n",
      "layer   3  Sparsity: 77.0532%\n",
      "total_backward_count 939840 real_backward_count 201537  21.444%\n",
      "fc layer 2 self.abs_max_out: 2629.0\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.672610/  1.811889, val:  68.33%, val_best:  80.42%, tr:  96.73%, tr_best:  97.96%, epoch time: 40.85 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0606%\n",
      "layer   2  Sparsity: 79.3758%\n",
      "layer   3  Sparsity: 77.3549%\n",
      "total_backward_count 944735 real_backward_count 202384  21.422%\n",
      "fc layer 2 self.abs_max_out: 2680.0\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.682341/  1.830374, val:  67.92%, val_best:  80.42%, tr:  97.14%, tr_best:  97.96%, epoch time: 41.12 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0419%\n",
      "layer   2  Sparsity: 79.3023%\n",
      "layer   3  Sparsity: 77.5360%\n",
      "total_backward_count 949630 real_backward_count 203229  21.401%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.685615/  1.787715, val:  78.33%, val_best:  80.42%, tr:  97.14%, tr_best:  97.96%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0667%\n",
      "layer   2  Sparsity: 79.4596%\n",
      "layer   3  Sparsity: 78.1857%\n",
      "total_backward_count 954525 real_backward_count 204042  21.376%\n",
      "lif layer 1 self.abs_max_v: 4305.5\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.685258/  1.799917, val:  74.58%, val_best:  80.42%, tr:  97.24%, tr_best:  97.96%, epoch time: 41.25 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.1052%\n",
      "layer   2  Sparsity: 79.6142%\n",
      "layer   3  Sparsity: 78.1052%\n",
      "total_backward_count 959420 real_backward_count 204894  21.356%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.673241/  1.809990, val:  78.75%, val_best:  80.42%, tr:  97.34%, tr_best:  97.96%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0844%\n",
      "layer   2  Sparsity: 79.4681%\n",
      "layer   3  Sparsity: 77.9105%\n",
      "total_backward_count 964315 real_backward_count 205707  21.332%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.683195/  1.797058, val:  68.75%, val_best:  80.42%, tr:  97.14%, tr_best:  97.96%, epoch time: 40.80 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0740%\n",
      "layer   2  Sparsity: 79.4696%\n",
      "layer   3  Sparsity: 77.9285%\n",
      "total_backward_count 969210 real_backward_count 206547  21.311%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.659058/  1.805371, val:  66.67%, val_best:  80.42%, tr:  97.24%, tr_best:  97.96%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0493%\n",
      "layer   2  Sparsity: 79.6667%\n",
      "layer   3  Sparsity: 77.7713%\n",
      "total_backward_count 974105 real_backward_count 207354  21.287%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.657442/  1.789461, val:  66.67%, val_best:  80.42%, tr:  97.04%, tr_best:  97.96%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0819%\n",
      "layer   2  Sparsity: 79.7651%\n",
      "layer   3  Sparsity: 77.4353%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96654cbe4fe94f19aeabf3543f717320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.97038</td></tr><tr><td>tr_epoch_loss</td><td>1.65744</td></tr><tr><td>val_acc_best</td><td>0.80417</td></tr><tr><td>val_acc_now</td><td>0.66667</td></tr><tr><td>val_loss</td><td>1.78946</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hearty-sweep-78</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kjubz63m' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kjubz63m</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_175948-kjubz63m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k2x1rj1x with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_201620-k2x1rj1x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k2x1rj1x' target=\"_blank\">toasty-sweep-82</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k2x1rj1x' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k2x1rj1x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251117_201629_165', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 43471ed3c7f31ff42498182012c432a5\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 282\n",
      "fc layer 1 self.abs_max_out: 167.0\n",
      "lif layer 1 self.abs_max_v: 167.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 140.0\n",
      "lif layer 2 self.abs_max_v: 140.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 17.0\n",
      "lif layer 1 self.abs_max_v: 230.5\n",
      "fc layer 1 self.abs_max_out: 196.0\n",
      "lif layer 1 self.abs_max_v: 266.5\n",
      "fc layer 2 self.abs_max_out: 153.0\n",
      "lif layer 2 self.abs_max_v: 216.5\n",
      "fc layer 3 self.abs_max_out: 44.0\n",
      "fc layer 3 self.abs_max_out: 67.0\n",
      "smallest_now_T updated: 254\n",
      "fc layer 1 self.abs_max_out: 218.0\n",
      "lif layer 1 self.abs_max_v: 285.0\n",
      "fc layer 2 self.abs_max_out: 249.0\n",
      "lif layer 2 self.abs_max_v: 269.0\n",
      "fc layer 3 self.abs_max_out: 112.0\n",
      "fc layer 1 self.abs_max_out: 278.0\n",
      "lif layer 1 self.abs_max_v: 334.0\n",
      "fc layer 2 self.abs_max_out: 303.0\n",
      "lif layer 2 self.abs_max_v: 353.5\n",
      "fc layer 1 self.abs_max_out: 336.0\n",
      "lif layer 1 self.abs_max_v: 336.0\n",
      "fc layer 2 self.abs_max_out: 353.0\n",
      "lif layer 2 self.abs_max_v: 396.0\n",
      "fc layer 1 self.abs_max_out: 340.0\n",
      "lif layer 1 self.abs_max_v: 409.5\n",
      "lif layer 2 self.abs_max_v: 465.0\n",
      "smallest_now_T updated: 193\n",
      "fc layer 1 self.abs_max_out: 412.0\n",
      "lif layer 1 self.abs_max_v: 412.0\n",
      "fc layer 3 self.abs_max_out: 147.0\n",
      "fc layer 2 self.abs_max_out: 385.0\n",
      "lif layer 2 self.abs_max_v: 517.0\n",
      "fc layer 1 self.abs_max_out: 434.0\n",
      "lif layer 1 self.abs_max_v: 434.0\n",
      "fc layer 1 self.abs_max_out: 549.0\n",
      "lif layer 1 self.abs_max_v: 549.0\n",
      "fc layer 2 self.abs_max_out: 399.0\n",
      "lif layer 2 self.abs_max_v: 628.5\n",
      "fc layer 2 self.abs_max_out: 404.0\n",
      "lif layer 1 self.abs_max_v: 601.0\n",
      "fc layer 3 self.abs_max_out: 170.0\n",
      "lif layer 1 self.abs_max_v: 611.5\n",
      "fc layer 2 self.abs_max_out: 504.0\n",
      "fc layer 2 self.abs_max_out: 528.0\n",
      "lif layer 2 self.abs_max_v: 796.5\n",
      "smallest_now_T updated: 163\n",
      "fc layer 1 self.abs_max_out: 652.0\n",
      "lif layer 1 self.abs_max_v: 652.0\n",
      "fc layer 3 self.abs_max_out: 217.0\n",
      "smallest_now_T updated: 150\n",
      "fc layer 2 self.abs_max_out: 577.0\n",
      "fc layer 1 self.abs_max_out: 767.0\n",
      "lif layer 1 self.abs_max_v: 767.0\n",
      "lif layer 2 self.abs_max_v: 805.0\n",
      "lif layer 2 self.abs_max_v: 840.5\n",
      "lif layer 2 self.abs_max_v: 850.5\n",
      "lif layer 2 self.abs_max_v: 859.0\n",
      "smallest_now_T updated: 135\n",
      "fc layer 3 self.abs_max_out: 221.0\n",
      "fc layer 3 self.abs_max_out: 247.0\n",
      "lif layer 2 self.abs_max_v: 863.0\n",
      "fc layer 2 self.abs_max_out: 587.0\n",
      "lif layer 2 self.abs_max_v: 926.5\n",
      "lif layer 1 self.abs_max_v: 778.0\n",
      "fc layer 2 self.abs_max_out: 594.0\n",
      "fc layer 1 self.abs_max_out: 794.0\n",
      "lif layer 1 self.abs_max_v: 794.0\n",
      "fc layer 2 self.abs_max_out: 610.0\n",
      "fc layer 3 self.abs_max_out: 257.0\n",
      "fc layer 2 self.abs_max_out: 625.0\n",
      "fc layer 1 self.abs_max_out: 838.0\n",
      "lif layer 1 self.abs_max_v: 838.0\n",
      "lif layer 2 self.abs_max_v: 939.5\n",
      "fc layer 1 self.abs_max_out: 940.0\n",
      "lif layer 1 self.abs_max_v: 940.0\n",
      "fc layer 2 self.abs_max_out: 658.0\n",
      "fc layer 2 self.abs_max_out: 725.0\n",
      "fc layer 3 self.abs_max_out: 286.0\n",
      "lif layer 2 self.abs_max_v: 972.5\n",
      "smallest_now_T updated: 116\n",
      "smallest_now_T updated: 90\n",
      "fc layer 3 self.abs_max_out: 309.0\n",
      "lif layer 2 self.abs_max_v: 1063.0\n",
      "fc layer 2 self.abs_max_out: 743.0\n",
      "fc layer 3 self.abs_max_out: 311.0\n",
      "fc layer 1 self.abs_max_out: 941.0\n",
      "lif layer 1 self.abs_max_v: 941.0\n",
      "fc layer 1 self.abs_max_out: 988.0\n",
      "lif layer 1 self.abs_max_v: 988.0\n",
      "fc layer 2 self.abs_max_out: 809.0\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "fc layer 3 self.abs_max_out: 326.0\n",
      "lif layer 1 self.abs_max_v: 1078.0\n",
      "lif layer 1 self.abs_max_v: 1230.0\n",
      "fc layer 2 self.abs_max_out: 887.0\n",
      "fc layer 1 self.abs_max_out: 1014.0\n",
      "fc layer 2 self.abs_max_out: 888.0\n",
      "lif layer 2 self.abs_max_v: 1070.5\n",
      "lif layer 2 self.abs_max_v: 1193.5\n",
      "fc layer 1 self.abs_max_out: 1027.0\n",
      "fc layer 1 self.abs_max_out: 1034.0\n",
      "fc layer 1 self.abs_max_out: 1125.0\n",
      "fc layer 2 self.abs_max_out: 901.0\n",
      "fc layer 2 self.abs_max_out: 943.0\n",
      "fc layer 1 self.abs_max_out: 1127.0\n",
      "fc layer 1 self.abs_max_out: 1222.0\n",
      "fc layer 2 self.abs_max_out: 950.0\n",
      "lif layer 1 self.abs_max_v: 1235.0\n",
      "fc layer 1 self.abs_max_out: 1256.0\n",
      "lif layer 1 self.abs_max_v: 1256.0\n",
      "lif layer 1 self.abs_max_v: 1350.5\n",
      "lif layer 2 self.abs_max_v: 1239.5\n",
      "fc layer 2 self.abs_max_out: 1050.0\n",
      "fc layer 2 self.abs_max_out: 1078.0\n",
      "fc layer 1 self.abs_max_out: 1455.0\n",
      "lif layer 1 self.abs_max_v: 1455.0\n",
      "fc layer 3 self.abs_max_out: 333.0\n",
      "fc layer 3 self.abs_max_out: 357.0\n",
      "smallest_now_T_val updated: 262\n",
      "smallest_now_T_val updated: 217\n",
      "smallest_now_T_val updated: 213\n",
      "smallest_now_T_val updated: 209\n",
      "smallest_now_T_val updated: 174\n",
      "smallest_now_T_val updated: 63\n",
      "lif layer 2 self.abs_max_v: 1242.5\n",
      "lif layer 1 self.abs_max_v: 1619.0\n",
      "lif layer 1 self.abs_max_v: 1737.5\n",
      "lif layer 2 self.abs_max_v: 1297.5\n",
      "lif layer 1 self.abs_max_v: 1791.0\n",
      "lif layer 2 self.abs_max_v: 1346.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.934794/  2.005991, val:  38.75%, val_best:  38.75%, tr:  76.40%, tr_best:  76.40%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.1953%\n",
      "layer   2  Sparsity: 78.9889%\n",
      "layer   3  Sparsity: 77.2732%\n",
      "total_backward_count 4895 real_backward_count 1999  40.838%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 1520.0\n",
      "fc layer 2 self.abs_max_out: 1121.0\n",
      "fc layer 2 self.abs_max_out: 1143.0\n",
      "fc layer 3 self.abs_max_out: 360.0\n",
      "fc layer 1 self.abs_max_out: 1555.0\n",
      "lif layer 2 self.abs_max_v: 1347.0\n",
      "lif layer 1 self.abs_max_v: 1865.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.821548/  1.992086, val:  41.25%, val_best:  41.25%, tr:  88.87%, tr_best:  88.87%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1880%\n",
      "layer   2  Sparsity: 76.6572%\n",
      "layer   3  Sparsity: 74.3540%\n",
      "total_backward_count 9790 real_backward_count 3402  34.750%\n",
      "fc layer 3 self.abs_max_out: 362.0\n",
      "fc layer 3 self.abs_max_out: 401.0\n",
      "fc layer 1 self.abs_max_out: 1610.0\n",
      "lif layer 2 self.abs_max_v: 1369.5\n",
      "lif layer 2 self.abs_max_v: 1507.0\n",
      "lif layer 1 self.abs_max_v: 2058.0\n",
      "lif layer 1 self.abs_max_v: 2076.0\n",
      "lif layer 2 self.abs_max_v: 1563.0\n",
      "fc layer 2 self.abs_max_out: 1166.0\n",
      "lif layer 2 self.abs_max_v: 1601.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.820123/  1.997233, val:  37.50%, val_best:  41.25%, tr:  89.38%, tr_best:  89.38%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1758%\n",
      "layer   2  Sparsity: 76.9760%\n",
      "layer   3  Sparsity: 73.9463%\n",
      "total_backward_count 14685 real_backward_count 4666  31.774%\n",
      "fc layer 1 self.abs_max_out: 1675.0\n",
      "fc layer 1 self.abs_max_out: 1709.0\n",
      "fc layer 1 self.abs_max_out: 1722.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.789755/  1.951888, val:  48.75%, val_best:  48.75%, tr:  91.73%, tr_best:  91.73%, epoch time: 40.95 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1868%\n",
      "layer   2  Sparsity: 76.4647%\n",
      "layer   3  Sparsity: 73.4509%\n",
      "total_backward_count 19580 real_backward_count 5927  30.271%\n",
      "lif layer 2 self.abs_max_v: 1625.0\n",
      "lif layer 2 self.abs_max_v: 1696.0\n",
      "fc layer 1 self.abs_max_out: 1725.0\n",
      "fc layer 1 self.abs_max_out: 1777.0\n",
      "lif layer 2 self.abs_max_v: 1749.0\n",
      "fc layer 1 self.abs_max_out: 1826.0\n",
      "lif layer 2 self.abs_max_v: 1823.0\n",
      "fc layer 2 self.abs_max_out: 1172.0\n",
      "fc layer 2 self.abs_max_out: 1262.0\n",
      "lif layer 2 self.abs_max_v: 1897.0\n",
      "lif layer 1 self.abs_max_v: 2195.5\n",
      "lif layer 1 self.abs_max_v: 2208.5\n",
      "lif layer 1 self.abs_max_v: 2291.5\n",
      "lif layer 2 self.abs_max_v: 1967.0\n",
      "lif layer 1 self.abs_max_v: 2452.5\n",
      "lif layer 2 self.abs_max_v: 2085.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.784434/  1.976291, val:  41.67%, val_best:  48.75%, tr:  92.95%, tr_best:  92.95%, epoch time: 40.93 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1720%\n",
      "layer   2  Sparsity: 76.2626%\n",
      "layer   3  Sparsity: 73.5457%\n",
      "total_backward_count 24475 real_backward_count 7093  28.981%\n",
      "lif layer 2 self.abs_max_v: 2115.0\n",
      "lif layer 2 self.abs_max_v: 2127.5\n",
      "fc layer 1 self.abs_max_out: 1846.0\n",
      "fc layer 1 self.abs_max_out: 1873.0\n",
      "lif layer 1 self.abs_max_v: 2499.0\n",
      "lif layer 1 self.abs_max_v: 2579.5\n",
      "lif layer 1 self.abs_max_v: 2595.5\n",
      "lif layer 1 self.abs_max_v: 2803.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.772176/  1.967838, val:  48.33%, val_best:  48.75%, tr:  92.54%, tr_best:  92.95%, epoch time: 41.05 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1888%\n",
      "layer   2  Sparsity: 76.0790%\n",
      "layer   3  Sparsity: 72.8676%\n",
      "total_backward_count 29370 real_backward_count 8190  27.886%\n",
      "fc layer 3 self.abs_max_out: 411.0\n",
      "fc layer 2 self.abs_max_out: 1272.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.769087/  1.940461, val:  45.42%, val_best:  48.75%, tr:  92.24%, tr_best:  92.95%, epoch time: 41.58 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.1541%\n",
      "layer   2  Sparsity: 76.2358%\n",
      "layer   3  Sparsity: 73.2891%\n",
      "total_backward_count 34265 real_backward_count 9317  27.191%\n",
      "fc layer 1 self.abs_max_out: 1939.0\n",
      "fc layer 2 self.abs_max_out: 1297.0\n",
      "fc layer 2 self.abs_max_out: 1305.0\n",
      "fc layer 3 self.abs_max_out: 422.0\n",
      "fc layer 2 self.abs_max_out: 1316.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.738571/  1.923596, val:  42.50%, val_best:  48.75%, tr:  92.85%, tr_best:  92.95%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1698%\n",
      "layer   2  Sparsity: 76.3605%\n",
      "layer   3  Sparsity: 72.9919%\n",
      "total_backward_count 39160 real_backward_count 10413  26.591%\n",
      "fc layer 1 self.abs_max_out: 1992.0\n",
      "fc layer 3 self.abs_max_out: 467.0\n",
      "fc layer 2 self.abs_max_out: 1325.0\n",
      "fc layer 2 self.abs_max_out: 1346.0\n",
      "lif layer 1 self.abs_max_v: 2854.0\n",
      "lif layer 1 self.abs_max_v: 2935.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.739183/  1.896841, val:  54.58%, val_best:  54.58%, tr:  92.65%, tr_best:  92.95%, epoch time: 41.16 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.1810%\n",
      "layer   2  Sparsity: 75.8843%\n",
      "layer   3  Sparsity: 72.6332%\n",
      "total_backward_count 44055 real_backward_count 11513  26.133%\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.739682/  1.905220, val:  51.67%, val_best:  54.58%, tr:  93.87%, tr_best:  93.87%, epoch time: 40.89 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1565%\n",
      "layer   2  Sparsity: 75.5572%\n",
      "layer   3  Sparsity: 72.7723%\n",
      "total_backward_count 48950 real_backward_count 12564  25.667%\n",
      "fc layer 1 self.abs_max_out: 2009.0\n",
      "fc layer 1 self.abs_max_out: 2078.0\n",
      "fc layer 2 self.abs_max_out: 1404.0\n",
      "fc layer 2 self.abs_max_out: 1435.0\n",
      "fc layer 2 self.abs_max_out: 1462.0\n",
      "fc layer 2 self.abs_max_out: 1494.0\n",
      "fc layer 1 self.abs_max_out: 2312.0\n",
      "lif layer 1 self.abs_max_v: 3243.5\n",
      "lif layer 1 self.abs_max_v: 3669.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.700987/  1.897227, val:  47.08%, val_best:  54.58%, tr:  94.99%, tr_best:  94.99%, epoch time: 40.69 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1589%\n",
      "layer   2  Sparsity: 75.4888%\n",
      "layer   3  Sparsity: 72.3521%\n",
      "total_backward_count 53845 real_backward_count 13579  25.219%\n",
      "fc layer 2 self.abs_max_out: 1515.0\n",
      "lif layer 2 self.abs_max_v: 2139.0\n",
      "lif layer 2 self.abs_max_v: 2237.5\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.709084/  1.872198, val:  57.50%, val_best:  57.50%, tr:  95.71%, tr_best:  95.71%, epoch time: 41.25 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.1673%\n",
      "layer   2  Sparsity: 75.1807%\n",
      "layer   3  Sparsity: 72.8968%\n",
      "total_backward_count 58740 real_backward_count 14613  24.877%\n",
      "fc layer 3 self.abs_max_out: 472.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.692057/  1.862508, val:  47.92%, val_best:  57.50%, tr:  94.79%, tr_best:  95.71%, epoch time: 40.86 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1916%\n",
      "layer   2  Sparsity: 75.2064%\n",
      "layer   3  Sparsity: 73.4550%\n",
      "total_backward_count 63635 real_backward_count 15619  24.545%\n",
      "lif layer 2 self.abs_max_v: 2329.0\n",
      "fc layer 1 self.abs_max_out: 2424.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.696716/  1.889722, val:  42.92%, val_best:  57.50%, tr:  93.46%, tr_best:  95.71%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1948%\n",
      "layer   2  Sparsity: 75.3852%\n",
      "layer   3  Sparsity: 73.3583%\n",
      "total_backward_count 68530 real_backward_count 16630  24.267%\n",
      "fc layer 3 self.abs_max_out: 473.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.693492/  1.863698, val:  42.50%, val_best:  57.50%, tr:  94.18%, tr_best:  95.71%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.2009%\n",
      "layer   2  Sparsity: 75.5802%\n",
      "layer   3  Sparsity: 73.4509%\n",
      "total_backward_count 73425 real_backward_count 17559  23.914%\n",
      "fc layer 3 self.abs_max_out: 483.0\n",
      "fc layer 1 self.abs_max_out: 2808.0\n",
      "lif layer 1 self.abs_max_v: 4000.5\n",
      "lif layer 1 self.abs_max_v: 4447.5\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.687876/  1.863958, val:  47.92%, val_best:  57.50%, tr:  94.08%, tr_best:  95.71%, epoch time: 40.86 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1816%\n",
      "layer   2  Sparsity: 74.8809%\n",
      "layer   3  Sparsity: 73.5330%\n",
      "total_backward_count 78320 real_backward_count 18534  23.664%\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.649318/  1.818003, val:  54.58%, val_best:  57.50%, tr:  96.73%, tr_best:  96.73%, epoch time: 40.77 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1623%\n",
      "layer   2  Sparsity: 74.8816%\n",
      "layer   3  Sparsity: 73.7378%\n",
      "total_backward_count 83215 real_backward_count 19471  23.398%\n",
      "fc layer 3 self.abs_max_out: 516.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.655374/  1.804736, val:  72.92%, val_best:  72.92%, tr:  95.51%, tr_best:  96.73%, epoch time: 40.63 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1738%\n",
      "layer   2  Sparsity: 74.4491%\n",
      "layer   3  Sparsity: 73.9023%\n",
      "total_backward_count 88110 real_backward_count 20428  23.185%\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.669324/  1.797525, val:  67.50%, val_best:  72.92%, tr:  96.22%, tr_best:  96.73%, epoch time: 41.00 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1584%\n",
      "layer   2  Sparsity: 73.9779%\n",
      "layer   3  Sparsity: 74.5486%\n",
      "total_backward_count 93005 real_backward_count 21372  22.979%\n",
      "lif layer 1 self.abs_max_v: 4526.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.655616/  1.839707, val:  47.92%, val_best:  72.92%, tr:  96.32%, tr_best:  96.73%, epoch time: 40.93 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1658%\n",
      "layer   2  Sparsity: 73.9587%\n",
      "layer   3  Sparsity: 74.3577%\n",
      "total_backward_count 97900 real_backward_count 22303  22.781%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.648104/  1.830230, val:  57.92%, val_best:  72.92%, tr:  96.83%, tr_best:  96.83%, epoch time: 41.56 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.1760%\n",
      "layer   2  Sparsity: 74.1913%\n",
      "layer   3  Sparsity: 74.0161%\n",
      "total_backward_count 102795 real_backward_count 23185  22.555%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.635511/  1.785269, val:  61.67%, val_best:  72.92%, tr:  96.53%, tr_best:  96.83%, epoch time: 41.47 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.1725%\n",
      "layer   2  Sparsity: 74.5311%\n",
      "layer   3  Sparsity: 73.6569%\n",
      "total_backward_count 107690 real_backward_count 24076  22.357%\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.630587/  1.765853, val:  70.00%, val_best:  72.92%, tr:  95.91%, tr_best:  96.83%, epoch time: 41.23 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.1475%\n",
      "layer   2  Sparsity: 73.9544%\n",
      "layer   3  Sparsity: 73.5929%\n",
      "total_backward_count 112585 real_backward_count 24988  22.195%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.625589/  1.772435, val:  70.83%, val_best:  72.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 41.12 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.1957%\n",
      "layer   2  Sparsity: 74.1253%\n",
      "layer   3  Sparsity: 74.2741%\n",
      "total_backward_count 117480 real_backward_count 25859  22.011%\n",
      "fc layer 2 self.abs_max_out: 1519.0\n",
      "fc layer 2 self.abs_max_out: 1559.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.614856/  1.772829, val:  65.42%, val_best:  72.92%, tr:  96.42%, tr_best:  97.75%, epoch time: 41.09 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1747%\n",
      "layer   2  Sparsity: 74.0247%\n",
      "layer   3  Sparsity: 74.2347%\n",
      "total_backward_count 122375 real_backward_count 26734  21.846%\n",
      "fc layer 3 self.abs_max_out: 526.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.604419/  1.740184, val:  75.00%, val_best:  75.00%, tr:  96.42%, tr_best:  97.75%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1368%\n",
      "layer   2  Sparsity: 73.4390%\n",
      "layer   3  Sparsity: 74.4673%\n",
      "total_backward_count 127270 real_backward_count 27623  21.704%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.599396/  1.761909, val:  59.17%, val_best:  75.00%, tr:  97.55%, tr_best:  97.75%, epoch time: 41.05 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1754%\n",
      "layer   2  Sparsity: 73.4058%\n",
      "layer   3  Sparsity: 74.4326%\n",
      "total_backward_count 132165 real_backward_count 28436  21.516%\n",
      "fc layer 2 self.abs_max_out: 1597.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.607465/  1.757860, val:  71.25%, val_best:  75.00%, tr:  97.04%, tr_best:  97.75%, epoch time: 41.04 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1610%\n",
      "layer   2  Sparsity: 73.5644%\n",
      "layer   3  Sparsity: 74.3554%\n",
      "total_backward_count 137060 real_backward_count 29292  21.372%\n",
      "fc layer 2 self.abs_max_out: 1623.0\n",
      "fc layer 2 self.abs_max_out: 1626.0\n",
      "lif layer 2 self.abs_max_v: 2435.5\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.576915/  1.770820, val:  57.50%, val_best:  75.00%, tr:  97.85%, tr_best:  97.85%, epoch time: 41.01 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1542%\n",
      "layer   2  Sparsity: 73.7887%\n",
      "layer   3  Sparsity: 74.7688%\n",
      "total_backward_count 141955 real_backward_count 30040  21.162%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.598566/  1.736020, val:  65.00%, val_best:  75.00%, tr:  97.34%, tr_best:  97.85%, epoch time: 41.14 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.1797%\n",
      "layer   2  Sparsity: 73.7094%\n",
      "layer   3  Sparsity: 74.3705%\n",
      "total_backward_count 146850 real_backward_count 30833  20.996%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.590617/  1.764139, val:  60.83%, val_best:  75.00%, tr:  97.75%, tr_best:  97.85%, epoch time: 40.58 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1497%\n",
      "layer   2  Sparsity: 73.6231%\n",
      "layer   3  Sparsity: 73.8841%\n",
      "total_backward_count 151745 real_backward_count 31592  20.819%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.595445/  1.746896, val:  64.17%, val_best:  75.00%, tr:  97.14%, tr_best:  97.85%, epoch time: 40.90 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1810%\n",
      "layer   2  Sparsity: 73.3533%\n",
      "layer   3  Sparsity: 73.7516%\n",
      "total_backward_count 156640 real_backward_count 32326  20.637%\n",
      "fc layer 2 self.abs_max_out: 1641.0\n",
      "fc layer 2 self.abs_max_out: 1651.0\n",
      "lif layer 2 self.abs_max_v: 2481.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.585882/  1.748639, val:  69.58%, val_best:  75.00%, tr:  97.45%, tr_best:  97.85%, epoch time: 40.96 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.2111%\n",
      "layer   2  Sparsity: 73.6031%\n",
      "layer   3  Sparsity: 73.8208%\n",
      "total_backward_count 161535 real_backward_count 33100  20.491%\n",
      "fc layer 2 self.abs_max_out: 1657.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.595899/  1.740081, val:  67.08%, val_best:  75.00%, tr:  98.06%, tr_best:  98.06%, epoch time: 40.69 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.1761%\n",
      "layer   2  Sparsity: 73.7342%\n",
      "layer   3  Sparsity: 74.0473%\n",
      "total_backward_count 166430 real_backward_count 33830  20.327%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.584058/  1.760841, val:  63.75%, val_best:  75.00%, tr:  97.14%, tr_best:  98.06%, epoch time: 41.34 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.1637%\n",
      "layer   2  Sparsity: 73.7902%\n",
      "layer   3  Sparsity: 74.3457%\n",
      "total_backward_count 171325 real_backward_count 34548  20.165%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.596892/  1.761022, val:  70.00%, val_best:  75.00%, tr:  97.96%, tr_best:  98.06%, epoch time: 39.72 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 93.1609%\n",
      "layer   2  Sparsity: 73.5293%\n",
      "layer   3  Sparsity: 74.7096%\n",
      "total_backward_count 176220 real_backward_count 35300  20.032%\n",
      "fc layer 1 self.abs_max_out: 2828.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.583467/  1.738688, val:  68.75%, val_best:  75.00%, tr:  97.65%, tr_best:  98.06%, epoch time: 41.26 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.1684%\n",
      "layer   2  Sparsity: 73.5753%\n",
      "layer   3  Sparsity: 74.3955%\n",
      "total_backward_count 181115 real_backward_count 35968  19.859%\n",
      "fc layer 3 self.abs_max_out: 560.0\n",
      "lif layer 1 self.abs_max_v: 4600.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.572283/  1.753403, val:  64.17%, val_best:  75.00%, tr:  97.55%, tr_best:  98.06%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.1449%\n",
      "layer   2  Sparsity: 73.5359%\n",
      "layer   3  Sparsity: 74.2260%\n",
      "total_backward_count 186010 real_backward_count 36641  19.698%\n",
      "fc layer 1 self.abs_max_out: 2862.0\n",
      "fc layer 1 self.abs_max_out: 3447.0\n",
      "lif layer 1 self.abs_max_v: 4878.0\n",
      "lif layer 1 self.abs_max_v: 5401.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.582193/  1.733426, val:  75.83%, val_best:  75.83%, tr:  98.47%, tr_best:  98.47%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.1579%\n",
      "layer   2  Sparsity: 73.3595%\n",
      "layer   3  Sparsity: 74.2944%\n",
      "total_backward_count 190905 real_backward_count 37326  19.552%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.592864/  1.753650, val:  65.00%, val_best:  75.83%, tr:  97.34%, tr_best:  98.47%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.1883%\n",
      "layer   2  Sparsity: 73.4132%\n",
      "layer   3  Sparsity: 74.3445%\n",
      "total_backward_count 195800 real_backward_count 38055  19.436%\n",
      "lif layer 2 self.abs_max_v: 2536.5\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.594309/  1.781737, val:  60.00%, val_best:  75.83%, tr:  97.75%, tr_best:  98.47%, epoch time: 41.20 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.1812%\n",
      "layer   2  Sparsity: 73.5501%\n",
      "layer   3  Sparsity: 74.3221%\n",
      "total_backward_count 200695 real_backward_count 38764  19.315%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.604227/  1.747833, val:  70.83%, val_best:  75.83%, tr:  98.88%, tr_best:  98.88%, epoch time: 43.45 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 93.1816%\n",
      "layer   2  Sparsity: 73.5731%\n",
      "layer   3  Sparsity: 74.2149%\n",
      "total_backward_count 205590 real_backward_count 39441  19.184%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.590063/  1.755207, val:  73.33%, val_best:  75.83%, tr:  97.04%, tr_best:  98.88%, epoch time: 45.49 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.2049%\n",
      "layer   2  Sparsity: 73.3418%\n",
      "layer   3  Sparsity: 74.3096%\n",
      "total_backward_count 210485 real_backward_count 40118  19.060%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.588658/  1.734751, val:  77.92%, val_best:  77.92%, tr:  98.57%, tr_best:  98.88%, epoch time: 42.08 seconds, 0.70 minutes\n",
      "layer   1  Sparsity: 93.1384%\n",
      "layer   2  Sparsity: 73.2041%\n",
      "layer   3  Sparsity: 73.7227%\n",
      "total_backward_count 215380 real_backward_count 40799  18.943%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.577927/  1.734102, val:  75.42%, val_best:  77.92%, tr:  98.88%, tr_best:  98.88%, epoch time: 44.20 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1672%\n",
      "layer   2  Sparsity: 73.4321%\n",
      "layer   3  Sparsity: 74.0952%\n",
      "total_backward_count 220275 real_backward_count 41488  18.835%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.587752/  1.734868, val:  72.08%, val_best:  77.92%, tr:  98.37%, tr_best:  98.88%, epoch time: 44.70 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1701%\n",
      "layer   2  Sparsity: 73.1985%\n",
      "layer   3  Sparsity: 74.1577%\n",
      "total_backward_count 225170 real_backward_count 42167  18.727%\n",
      "fc layer 2 self.abs_max_out: 1670.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.574647/  1.768590, val:  60.42%, val_best:  77.92%, tr:  98.26%, tr_best:  98.88%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1912%\n",
      "layer   2  Sparsity: 73.2112%\n",
      "layer   3  Sparsity: 73.9783%\n",
      "total_backward_count 230065 real_backward_count 42815  18.610%\n",
      "fc layer 2 self.abs_max_out: 1706.0\n",
      "fc layer 2 self.abs_max_out: 1755.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.570076/  1.759022, val:  62.92%, val_best:  77.92%, tr:  98.26%, tr_best:  98.88%, epoch time: 45.02 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1620%\n",
      "layer   2  Sparsity: 73.2366%\n",
      "layer   3  Sparsity: 74.4753%\n",
      "total_backward_count 234960 real_backward_count 43432  18.485%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.577059/  1.738183, val:  78.33%, val_best:  78.33%, tr:  98.06%, tr_best:  98.88%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1547%\n",
      "layer   2  Sparsity: 73.1042%\n",
      "layer   3  Sparsity: 74.3581%\n",
      "total_backward_count 239855 real_backward_count 44120  18.394%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.551228/  1.701242, val:  68.33%, val_best:  78.33%, tr:  98.67%, tr_best:  98.88%, epoch time: 45.31 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.1183%\n",
      "layer   2  Sparsity: 73.4647%\n",
      "layer   3  Sparsity: 74.0984%\n",
      "total_backward_count 244750 real_backward_count 44759  18.288%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.546063/  1.709619, val:  72.08%, val_best:  78.33%, tr:  98.77%, tr_best:  98.88%, epoch time: 45.26 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1874%\n",
      "layer   2  Sparsity: 73.5158%\n",
      "layer   3  Sparsity: 74.3645%\n",
      "total_backward_count 249645 real_backward_count 45432  18.199%\n",
      "lif layer 2 self.abs_max_v: 2568.5\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.564515/  1.755887, val:  63.75%, val_best:  78.33%, tr:  97.85%, tr_best:  98.88%, epoch time: 45.42 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.1809%\n",
      "layer   2  Sparsity: 73.1197%\n",
      "layer   3  Sparsity: 73.7370%\n",
      "total_backward_count 254540 real_backward_count 46061  18.096%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.555663/  1.688842, val:  76.25%, val_best:  78.33%, tr:  98.47%, tr_best:  98.88%, epoch time: 45.06 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1955%\n",
      "layer   2  Sparsity: 73.0960%\n",
      "layer   3  Sparsity: 73.9445%\n",
      "total_backward_count 259435 real_backward_count 46672  17.990%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.543736/  1.699626, val:  74.58%, val_best:  78.33%, tr:  98.57%, tr_best:  98.88%, epoch time: 45.46 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.1277%\n",
      "layer   2  Sparsity: 72.7797%\n",
      "layer   3  Sparsity: 73.9529%\n",
      "total_backward_count 264330 real_backward_count 47284  17.888%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.548481/  1.736656, val:  70.83%, val_best:  78.33%, tr:  98.98%, tr_best:  98.98%, epoch time: 45.02 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1844%\n",
      "layer   2  Sparsity: 72.5678%\n",
      "layer   3  Sparsity: 73.8802%\n",
      "total_backward_count 269225 real_backward_count 47883  17.785%\n",
      "lif layer 2 self.abs_max_v: 2619.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.540893/  1.704422, val:  75.42%, val_best:  78.33%, tr:  98.98%, tr_best:  98.98%, epoch time: 45.03 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1629%\n",
      "layer   2  Sparsity: 72.8693%\n",
      "layer   3  Sparsity: 74.0556%\n",
      "total_backward_count 274120 real_backward_count 48462  17.679%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.524084/  1.686015, val:  77.08%, val_best:  78.33%, tr:  98.88%, tr_best:  98.98%, epoch time: 45.03 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.2085%\n",
      "layer   2  Sparsity: 72.7202%\n",
      "layer   3  Sparsity: 74.0080%\n",
      "total_backward_count 279015 real_backward_count 49074  17.588%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.516243/  1.688875, val:  78.75%, val_best:  78.75%, tr:  99.08%, tr_best:  99.08%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1711%\n",
      "layer   2  Sparsity: 72.5888%\n",
      "layer   3  Sparsity: 73.9113%\n",
      "total_backward_count 283910 real_backward_count 49623  17.478%\n",
      "fc layer 3 self.abs_max_out: 567.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.513969/  1.651735, val:  77.08%, val_best:  78.75%, tr:  99.39%, tr_best:  99.39%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1487%\n",
      "layer   2  Sparsity: 72.4955%\n",
      "layer   3  Sparsity: 73.7498%\n",
      "total_backward_count 288805 real_backward_count 50166  17.370%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.510683/  1.708678, val:  63.75%, val_best:  78.75%, tr:  98.88%, tr_best:  99.39%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1282%\n",
      "layer   2  Sparsity: 72.6954%\n",
      "layer   3  Sparsity: 74.1499%\n",
      "total_backward_count 293700 real_backward_count 50703  17.264%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.505576/  1.674307, val:  75.00%, val_best:  78.75%, tr:  98.88%, tr_best:  99.39%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1766%\n",
      "layer   2  Sparsity: 72.7619%\n",
      "layer   3  Sparsity: 73.6617%\n",
      "total_backward_count 298595 real_backward_count 51259  17.167%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.502365/  1.663026, val:  78.33%, val_best:  78.75%, tr:  99.18%, tr_best:  99.39%, epoch time: 44.78 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1726%\n",
      "layer   2  Sparsity: 72.7884%\n",
      "layer   3  Sparsity: 73.6841%\n",
      "total_backward_count 303490 real_backward_count 51821  17.075%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.475790/  1.654985, val:  75.42%, val_best:  78.75%, tr:  98.98%, tr_best:  99.39%, epoch time: 45.04 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.2056%\n",
      "layer   2  Sparsity: 72.8083%\n",
      "layer   3  Sparsity: 73.4859%\n",
      "total_backward_count 308385 real_backward_count 52397  16.991%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.477069/  1.641309, val:  78.33%, val_best:  78.75%, tr:  99.08%, tr_best:  99.39%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1694%\n",
      "layer   2  Sparsity: 72.7228%\n",
      "layer   3  Sparsity: 73.7550%\n",
      "total_backward_count 313280 real_backward_count 52936  16.897%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.481268/  1.650292, val:  75.42%, val_best:  78.75%, tr:  98.47%, tr_best:  99.39%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.2091%\n",
      "layer   2  Sparsity: 72.8461%\n",
      "layer   3  Sparsity: 73.8126%\n",
      "total_backward_count 318175 real_backward_count 53513  16.819%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.469560/  1.654129, val:  74.58%, val_best:  78.75%, tr:  99.18%, tr_best:  99.39%, epoch time: 45.22 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.2058%\n",
      "layer   2  Sparsity: 72.9036%\n",
      "layer   3  Sparsity: 74.3017%\n",
      "total_backward_count 323070 real_backward_count 54003  16.716%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.480818/  1.641712, val:  79.58%, val_best:  79.58%, tr:  99.08%, tr_best:  99.39%, epoch time: 45.01 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.2008%\n",
      "layer   2  Sparsity: 72.7688%\n",
      "layer   3  Sparsity: 73.9804%\n",
      "total_backward_count 327965 real_backward_count 54544  16.631%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.476031/  1.648041, val:  78.33%, val_best:  79.58%, tr:  99.18%, tr_best:  99.39%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1614%\n",
      "layer   2  Sparsity: 72.8662%\n",
      "layer   3  Sparsity: 73.8531%\n",
      "total_backward_count 332860 real_backward_count 55100  16.554%\n",
      "fc layer 3 self.abs_max_out: 569.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.475048/  1.645054, val:  78.75%, val_best:  79.58%, tr:  98.06%, tr_best:  99.39%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1649%\n",
      "layer   2  Sparsity: 72.8426%\n",
      "layer   3  Sparsity: 73.8453%\n",
      "total_backward_count 337755 real_backward_count 55649  16.476%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.482852/  1.646000, val:  75.42%, val_best:  79.58%, tr:  98.67%, tr_best:  99.39%, epoch time: 45.09 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1943%\n",
      "layer   2  Sparsity: 72.9936%\n",
      "layer   3  Sparsity: 73.6144%\n",
      "total_backward_count 342650 real_backward_count 56171  16.393%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.467836/  1.631057, val:  78.75%, val_best:  79.58%, tr:  99.39%, tr_best:  99.39%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1736%\n",
      "layer   2  Sparsity: 72.8317%\n",
      "layer   3  Sparsity: 73.6056%\n",
      "total_backward_count 347545 real_backward_count 56672  16.306%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.462623/  1.654423, val:  71.25%, val_best:  79.58%, tr:  99.08%, tr_best:  99.39%, epoch time: 45.42 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.1774%\n",
      "layer   2  Sparsity: 72.9710%\n",
      "layer   3  Sparsity: 73.9272%\n",
      "total_backward_count 352440 real_backward_count 57192  16.227%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.472324/  1.652490, val:  80.42%, val_best:  80.42%, tr:  99.28%, tr_best:  99.39%, epoch time: 44.91 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1727%\n",
      "layer   2  Sparsity: 72.7222%\n",
      "layer   3  Sparsity: 74.0061%\n",
      "total_backward_count 357335 real_backward_count 57704  16.148%\n",
      "lif layer 2 self.abs_max_v: 2708.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.474743/  1.664619, val:  62.50%, val_best:  80.42%, tr:  98.98%, tr_best:  99.39%, epoch time: 45.52 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.1601%\n",
      "layer   2  Sparsity: 72.6737%\n",
      "layer   3  Sparsity: 74.0679%\n",
      "total_backward_count 362230 real_backward_count 58200  16.067%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.484750/  1.648526, val:  79.58%, val_best:  80.42%, tr:  98.88%, tr_best:  99.39%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1379%\n",
      "layer   2  Sparsity: 72.4660%\n",
      "layer   3  Sparsity: 73.7642%\n",
      "total_backward_count 367125 real_backward_count 58731  15.998%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.480538/  1.628472, val:  79.58%, val_best:  80.42%, tr:  98.77%, tr_best:  99.39%, epoch time: 45.45 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.1847%\n",
      "layer   2  Sparsity: 72.4969%\n",
      "layer   3  Sparsity: 73.8707%\n",
      "total_backward_count 372020 real_backward_count 59217  15.918%\n",
      "lif layer 1 self.abs_max_v: 5415.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.468520/  1.656811, val:  75.83%, val_best:  80.42%, tr:  98.88%, tr_best:  99.39%, epoch time: 44.94 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1708%\n",
      "layer   2  Sparsity: 72.4484%\n",
      "layer   3  Sparsity: 73.8861%\n",
      "total_backward_count 376915 real_backward_count 59708  15.841%\n",
      "fc layer 2 self.abs_max_out: 1762.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.476888/  1.683888, val:  61.67%, val_best:  80.42%, tr:  98.98%, tr_best:  99.39%, epoch time: 45.20 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1910%\n",
      "layer   2  Sparsity: 72.1420%\n",
      "layer   3  Sparsity: 73.9159%\n",
      "total_backward_count 381810 real_backward_count 60212  15.770%\n",
      "fc layer 2 self.abs_max_out: 1778.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.461848/  1.631731, val:  79.17%, val_best:  80.42%, tr:  99.18%, tr_best:  99.39%, epoch time: 44.28 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1714%\n",
      "layer   2  Sparsity: 72.3470%\n",
      "layer   3  Sparsity: 73.7686%\n",
      "total_backward_count 386705 real_backward_count 60662  15.687%\n",
      "fc layer 2 self.abs_max_out: 1807.0\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.454793/  1.616425, val:  77.50%, val_best:  80.42%, tr:  98.98%, tr_best:  99.39%, epoch time: 45.37 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.1864%\n",
      "layer   2  Sparsity: 72.5144%\n",
      "layer   3  Sparsity: 73.5856%\n",
      "total_backward_count 391600 real_backward_count 61132  15.611%\n",
      "fc layer 2 self.abs_max_out: 1816.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.474679/  1.618575, val:  83.33%, val_best:  83.33%, tr:  98.98%, tr_best:  99.39%, epoch time: 45.00 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1197%\n",
      "layer   2  Sparsity: 72.2814%\n",
      "layer   3  Sparsity: 73.5722%\n",
      "total_backward_count 396495 real_backward_count 61605  15.537%\n",
      "fc layer 2 self.abs_max_out: 1817.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.474457/  1.653433, val:  74.17%, val_best:  83.33%, tr:  99.08%, tr_best:  99.39%, epoch time: 45.09 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1913%\n",
      "layer   2  Sparsity: 72.1099%\n",
      "layer   3  Sparsity: 73.6730%\n",
      "total_backward_count 401390 real_backward_count 62106  15.473%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.481565/  1.610871, val:  78.33%, val_best:  83.33%, tr:  98.57%, tr_best:  99.39%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.2159%\n",
      "layer   2  Sparsity: 72.0949%\n",
      "layer   3  Sparsity: 72.9747%\n",
      "total_backward_count 406285 real_backward_count 62658  15.422%\n",
      "fc layer 2 self.abs_max_out: 1820.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.463100/  1.655130, val:  73.33%, val_best:  83.33%, tr:  99.28%, tr_best:  99.39%, epoch time: 45.37 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.1825%\n",
      "layer   2  Sparsity: 72.2014%\n",
      "layer   3  Sparsity: 73.3583%\n",
      "total_backward_count 411180 real_backward_count 63140  15.356%\n",
      "fc layer 2 self.abs_max_out: 1867.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.459882/  1.625230, val:  78.33%, val_best:  83.33%, tr:  99.39%, tr_best:  99.39%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1803%\n",
      "layer   2  Sparsity: 72.5774%\n",
      "layer   3  Sparsity: 73.0613%\n",
      "total_backward_count 416075 real_backward_count 63651  15.298%\n",
      "fc layer 3 self.abs_max_out: 572.0\n",
      "fc layer 3 self.abs_max_out: 587.0\n",
      "fc layer 2 self.abs_max_out: 1893.0\n",
      "lif layer 2 self.abs_max_v: 2724.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.455943/  1.622328, val:  75.42%, val_best:  83.33%, tr:  99.18%, tr_best:  99.39%, epoch time: 45.67 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.1834%\n",
      "layer   2  Sparsity: 72.7188%\n",
      "layer   3  Sparsity: 73.0128%\n",
      "total_backward_count 420970 real_backward_count 64134  15.235%\n",
      "fc layer 3 self.abs_max_out: 610.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.441286/  1.637255, val:  67.50%, val_best:  83.33%, tr:  99.28%, tr_best:  99.39%, epoch time: 44.59 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1953%\n",
      "layer   2  Sparsity: 73.0498%\n",
      "layer   3  Sparsity: 73.5095%\n",
      "total_backward_count 425865 real_backward_count 64592  15.167%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.454582/  1.615313, val:  78.75%, val_best:  83.33%, tr:  99.59%, tr_best:  99.59%, epoch time: 45.59 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.1501%\n",
      "layer   2  Sparsity: 72.8372%\n",
      "layer   3  Sparsity: 74.2492%\n",
      "total_backward_count 430760 real_backward_count 65059  15.103%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.451923/  1.616604, val:  82.08%, val_best:  83.33%, tr:  98.98%, tr_best:  99.59%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1751%\n",
      "layer   2  Sparsity: 72.7064%\n",
      "layer   3  Sparsity: 73.9039%\n",
      "total_backward_count 435655 real_backward_count 65518  15.039%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.457697/  1.597289, val:  82.50%, val_best:  83.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1241%\n",
      "layer   2  Sparsity: 72.6706%\n",
      "layer   3  Sparsity: 73.6807%\n",
      "total_backward_count 440550 real_backward_count 65979  14.977%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.443642/  1.609545, val:  78.75%, val_best:  83.33%, tr:  99.49%, tr_best:  99.69%, epoch time: 45.01 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1647%\n",
      "layer   2  Sparsity: 72.8047%\n",
      "layer   3  Sparsity: 73.7430%\n",
      "total_backward_count 445445 real_backward_count 66423  14.912%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.448505/  1.604220, val:  80.83%, val_best:  83.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1954%\n",
      "layer   2  Sparsity: 72.6582%\n",
      "layer   3  Sparsity: 74.0537%\n",
      "total_backward_count 450340 real_backward_count 66887  14.853%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.433065/  1.585824, val:  81.67%, val_best:  83.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 45.22 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.2061%\n",
      "layer   2  Sparsity: 72.6221%\n",
      "layer   3  Sparsity: 73.9881%\n",
      "total_backward_count 455235 real_backward_count 67322  14.788%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.430756/  1.609888, val:  81.25%, val_best:  83.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1416%\n",
      "layer   2  Sparsity: 72.3710%\n",
      "layer   3  Sparsity: 73.7265%\n",
      "total_backward_count 460130 real_backward_count 67737  14.721%\n",
      "fc layer 2 self.abs_max_out: 1894.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.415542/  1.599422, val:  75.00%, val_best:  83.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 45.79 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.2011%\n",
      "layer   2  Sparsity: 72.4794%\n",
      "layer   3  Sparsity: 73.7368%\n",
      "total_backward_count 465025 real_backward_count 68172  14.660%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.422877/  1.609261, val:  76.25%, val_best:  83.33%, tr:  98.67%, tr_best:  99.69%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1696%\n",
      "layer   2  Sparsity: 72.6981%\n",
      "layer   3  Sparsity: 73.8655%\n",
      "total_backward_count 469920 real_backward_count 68659  14.611%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.426446/  1.607248, val:  83.33%, val_best:  83.33%, tr:  99.59%, tr_best:  99.69%, epoch time: 45.41 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.1681%\n",
      "layer   2  Sparsity: 72.4865%\n",
      "layer   3  Sparsity: 74.7217%\n",
      "total_backward_count 474815 real_backward_count 69098  14.553%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.439424/  1.600917, val:  82.92%, val_best:  83.33%, tr:  99.28%, tr_best:  99.69%, epoch time: 44.78 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1819%\n",
      "layer   2  Sparsity: 72.5427%\n",
      "layer   3  Sparsity: 74.6558%\n",
      "total_backward_count 479710 real_backward_count 69552  14.499%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.445342/  1.632394, val:  79.58%, val_best:  83.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1443%\n",
      "layer   2  Sparsity: 72.4788%\n",
      "layer   3  Sparsity: 75.1078%\n",
      "total_backward_count 484605 real_backward_count 69979  14.440%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.432639/  1.578290, val:  87.08%, val_best:  87.08%, tr:  99.49%, tr_best:  99.69%, epoch time: 45.12 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1486%\n",
      "layer   2  Sparsity: 72.7427%\n",
      "layer   3  Sparsity: 74.5588%\n",
      "total_backward_count 489500 real_backward_count 70366  14.375%\n",
      "fc layer 1 self.abs_max_out: 3527.0\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.425893/  1.595294, val:  83.33%, val_best:  87.08%, tr:  99.39%, tr_best:  99.69%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.2324%\n",
      "layer   2  Sparsity: 72.3698%\n",
      "layer   3  Sparsity: 74.2642%\n",
      "total_backward_count 494395 real_backward_count 70787  14.318%\n",
      "fc layer 1 self.abs_max_out: 3615.0\n",
      "fc layer 1 self.abs_max_out: 3711.0\n",
      "lif layer 1 self.abs_max_v: 6010.0\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.425492/  1.594697, val:  80.00%, val_best:  87.08%, tr:  99.49%, tr_best:  99.69%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1554%\n",
      "layer   2  Sparsity: 72.3491%\n",
      "layer   3  Sparsity: 74.2734%\n",
      "total_backward_count 499290 real_backward_count 71221  14.264%\n",
      "lif layer 2 self.abs_max_v: 2815.0\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.428797/  1.598716, val:  74.17%, val_best:  87.08%, tr:  99.39%, tr_best:  99.69%, epoch time: 45.06 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1711%\n",
      "layer   2  Sparsity: 72.6060%\n",
      "layer   3  Sparsity: 74.7209%\n",
      "total_backward_count 504185 real_backward_count 71591  14.199%\n",
      "lif layer 2 self.abs_max_v: 2849.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.432637/  1.615504, val:  78.75%, val_best:  87.08%, tr:  99.08%, tr_best:  99.69%, epoch time: 43.96 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 93.1748%\n",
      "layer   2  Sparsity: 72.5400%\n",
      "layer   3  Sparsity: 74.5476%\n",
      "total_backward_count 509080 real_backward_count 71992  14.142%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.454944/  1.604177, val:  79.17%, val_best:  87.08%, tr:  99.08%, tr_best:  99.69%, epoch time: 45.14 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1695%\n",
      "layer   2  Sparsity: 72.4527%\n",
      "layer   3  Sparsity: 73.8652%\n",
      "total_backward_count 513975 real_backward_count 72425  14.091%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.447970/  1.622963, val:  75.42%, val_best:  87.08%, tr:  99.28%, tr_best:  99.69%, epoch time: 44.06 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 93.1583%\n",
      "layer   2  Sparsity: 72.4231%\n",
      "layer   3  Sparsity: 74.0942%\n",
      "total_backward_count 518870 real_backward_count 72866  14.043%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.450165/  1.622242, val:  75.83%, val_best:  87.08%, tr:  99.49%, tr_best:  99.69%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1646%\n",
      "layer   2  Sparsity: 72.4631%\n",
      "layer   3  Sparsity: 74.1278%\n",
      "total_backward_count 523765 real_backward_count 73316  13.998%\n",
      "fc layer 1 self.abs_max_out: 3729.0\n",
      "lif layer 1 self.abs_max_v: 6023.0\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.450263/  1.614137, val:  82.08%, val_best:  87.08%, tr:  99.08%, tr_best:  99.69%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1242%\n",
      "layer   2  Sparsity: 72.2477%\n",
      "layer   3  Sparsity: 74.4345%\n",
      "total_backward_count 528660 real_backward_count 73779  13.956%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.431947/  1.578644, val:  80.00%, val_best:  87.08%, tr:  99.08%, tr_best:  99.69%, epoch time: 45.04 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1725%\n",
      "layer   2  Sparsity: 72.2704%\n",
      "layer   3  Sparsity: 74.4484%\n",
      "total_backward_count 533555 real_backward_count 74231  13.913%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.414762/  1.598144, val:  76.67%, val_best:  87.08%, tr:  99.59%, tr_best:  99.69%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1687%\n",
      "layer   2  Sparsity: 72.5336%\n",
      "layer   3  Sparsity: 74.7603%\n",
      "total_backward_count 538450 real_backward_count 74625  13.859%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.430672/  1.610002, val:  82.92%, val_best:  87.08%, tr:  99.28%, tr_best:  99.69%, epoch time: 44.84 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1425%\n",
      "layer   2  Sparsity: 72.3674%\n",
      "layer   3  Sparsity: 74.8708%\n",
      "total_backward_count 543345 real_backward_count 75008  13.805%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.432224/  1.588653, val:  77.08%, val_best:  87.08%, tr:  99.69%, tr_best:  99.69%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1748%\n",
      "layer   2  Sparsity: 72.1290%\n",
      "layer   3  Sparsity: 74.7970%\n",
      "total_backward_count 548240 real_backward_count 75415  13.756%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.429016/  1.576322, val:  85.00%, val_best:  87.08%, tr:  99.69%, tr_best:  99.69%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1878%\n",
      "layer   2  Sparsity: 72.3224%\n",
      "layer   3  Sparsity: 74.6528%\n",
      "total_backward_count 553135 real_backward_count 75835  13.710%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.418655/  1.604276, val:  79.58%, val_best:  87.08%, tr:  99.69%, tr_best:  99.69%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1556%\n",
      "layer   2  Sparsity: 72.2863%\n",
      "layer   3  Sparsity: 74.4302%\n",
      "total_backward_count 558030 real_backward_count 76238  13.662%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.421772/  1.583852, val:  77.50%, val_best:  87.08%, tr:  99.49%, tr_best:  99.69%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1770%\n",
      "layer   2  Sparsity: 72.4209%\n",
      "layer   3  Sparsity: 74.6059%\n",
      "total_backward_count 562925 real_backward_count 76642  13.615%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.417293/  1.592236, val:  84.17%, val_best:  87.08%, tr:  99.08%, tr_best:  99.69%, epoch time: 45.22 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1756%\n",
      "layer   2  Sparsity: 72.3900%\n",
      "layer   3  Sparsity: 74.6734%\n",
      "total_backward_count 567820 real_backward_count 77045  13.569%\n",
      "fc layer 2 self.abs_max_out: 1923.0\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.426055/  1.583117, val:  78.75%, val_best:  87.08%, tr:  99.28%, tr_best:  99.69%, epoch time: 44.54 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1600%\n",
      "layer   2  Sparsity: 72.3945%\n",
      "layer   3  Sparsity: 74.4340%\n",
      "total_backward_count 572715 real_backward_count 77455  13.524%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.424217/  1.580402, val:  81.25%, val_best:  87.08%, tr:  99.39%, tr_best:  99.69%, epoch time: 45.11 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1593%\n",
      "layer   2  Sparsity: 72.7488%\n",
      "layer   3  Sparsity: 74.4698%\n",
      "total_backward_count 577610 real_backward_count 77874  13.482%\n",
      "fc layer 3 self.abs_max_out: 625.0\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.423422/  1.598047, val:  85.42%, val_best:  87.08%, tr:  99.49%, tr_best:  99.69%, epoch time: 45.10 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1237%\n",
      "layer   2  Sparsity: 72.6313%\n",
      "layer   3  Sparsity: 74.6929%\n",
      "total_backward_count 582505 real_backward_count 78250  13.433%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.414200/  1.559130, val:  82.50%, val_best:  87.08%, tr:  99.49%, tr_best:  99.69%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1280%\n",
      "layer   2  Sparsity: 72.2720%\n",
      "layer   3  Sparsity: 74.5596%\n",
      "total_backward_count 587400 real_backward_count 78651  13.390%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.404303/  1.601106, val:  77.92%, val_best:  87.08%, tr:  99.49%, tr_best:  99.69%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.2002%\n",
      "layer   2  Sparsity: 72.1945%\n",
      "layer   3  Sparsity: 74.1605%\n",
      "total_backward_count 592295 real_backward_count 79043  13.345%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.409902/  1.595683, val:  82.92%, val_best:  87.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1829%\n",
      "layer   2  Sparsity: 72.2454%\n",
      "layer   3  Sparsity: 74.3549%\n",
      "total_backward_count 597190 real_backward_count 79420  13.299%\n",
      "fc layer 2 self.abs_max_out: 1926.0\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.416558/  1.572883, val:  79.58%, val_best:  87.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1594%\n",
      "layer   2  Sparsity: 72.1874%\n",
      "layer   3  Sparsity: 74.5631%\n",
      "total_backward_count 602085 real_backward_count 79769  13.249%\n",
      "fc layer 2 self.abs_max_out: 1950.0\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.394073/  1.557416, val:  84.17%, val_best:  87.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 44.70 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1603%\n",
      "layer   2  Sparsity: 72.4596%\n",
      "layer   3  Sparsity: 74.5336%\n",
      "total_backward_count 606980 real_backward_count 80124  13.200%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.394164/  1.553762, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1922%\n",
      "layer   2  Sparsity: 72.5027%\n",
      "layer   3  Sparsity: 74.6573%\n",
      "total_backward_count 611875 real_backward_count 80506  13.157%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.406038/  1.576253, val:  80.42%, val_best:  87.08%, tr:  99.28%, tr_best: 100.00%, epoch time: 44.97 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1796%\n",
      "layer   2  Sparsity: 72.6434%\n",
      "layer   3  Sparsity: 74.5688%\n",
      "total_backward_count 616770 real_backward_count 80903  13.117%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.406589/  1.584413, val:  79.17%, val_best:  87.08%, tr:  99.18%, tr_best: 100.00%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1620%\n",
      "layer   2  Sparsity: 72.5365%\n",
      "layer   3  Sparsity: 74.6385%\n",
      "total_backward_count 621665 real_backward_count 81274  13.074%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.388098/  1.578251, val:  77.92%, val_best:  87.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1769%\n",
      "layer   2  Sparsity: 72.6902%\n",
      "layer   3  Sparsity: 74.1564%\n",
      "total_backward_count 626560 real_backward_count 81628  13.028%\n",
      "fc layer 1 self.abs_max_out: 3754.0\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.393416/  1.548159, val:  81.67%, val_best:  87.08%, tr:  99.28%, tr_best: 100.00%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1939%\n",
      "layer   2  Sparsity: 72.6353%\n",
      "layer   3  Sparsity: 74.2358%\n",
      "total_backward_count 631455 real_backward_count 82002  12.986%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.386490/  1.567070, val:  82.50%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1487%\n",
      "layer   2  Sparsity: 72.4028%\n",
      "layer   3  Sparsity: 74.5354%\n",
      "total_backward_count 636350 real_backward_count 82387  12.947%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.392639/  1.568570, val:  74.58%, val_best:  87.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 44.39 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1661%\n",
      "layer   2  Sparsity: 72.4462%\n",
      "layer   3  Sparsity: 74.5625%\n",
      "total_backward_count 641245 real_backward_count 82766  12.907%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.396077/  1.583491, val:  83.33%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 44.84 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1902%\n",
      "layer   2  Sparsity: 72.4250%\n",
      "layer   3  Sparsity: 74.3484%\n",
      "total_backward_count 646140 real_backward_count 83134  12.866%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.389405/  1.594600, val:  80.00%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1845%\n",
      "layer   2  Sparsity: 72.4224%\n",
      "layer   3  Sparsity: 74.2513%\n",
      "total_backward_count 651035 real_backward_count 83502  12.826%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.397722/  1.555163, val:  83.33%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 45.30 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.1715%\n",
      "layer   2  Sparsity: 72.4982%\n",
      "layer   3  Sparsity: 74.3801%\n",
      "total_backward_count 655930 real_backward_count 83836  12.781%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.381389/  1.571813, val:  82.92%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 44.54 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1534%\n",
      "layer   2  Sparsity: 72.4536%\n",
      "layer   3  Sparsity: 74.2696%\n",
      "total_backward_count 660825 real_backward_count 84146  12.733%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.409350/  1.574791, val:  80.83%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 45.83 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.1506%\n",
      "layer   2  Sparsity: 72.2013%\n",
      "layer   3  Sparsity: 74.3416%\n",
      "total_backward_count 665720 real_backward_count 84505  12.694%\n",
      "fc layer 2 self.abs_max_out: 2000.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.391116/  1.566388, val:  84.58%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1859%\n",
      "layer   2  Sparsity: 72.1409%\n",
      "layer   3  Sparsity: 74.3442%\n",
      "total_backward_count 670615 real_backward_count 84832  12.650%\n",
      "fc layer 1 self.abs_max_out: 3803.0\n",
      "lif layer 2 self.abs_max_v: 2909.0\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.397558/  1.571357, val:  82.92%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 45.72 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.1672%\n",
      "layer   2  Sparsity: 72.2467%\n",
      "layer   3  Sparsity: 74.1507%\n",
      "total_backward_count 675510 real_backward_count 85188  12.611%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.399123/  1.574357, val:  82.08%, val_best:  87.08%, tr:  99.18%, tr_best: 100.00%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1924%\n",
      "layer   2  Sparsity: 72.0125%\n",
      "layer   3  Sparsity: 74.1452%\n",
      "total_backward_count 680405 real_backward_count 85544  12.573%\n",
      "fc layer 1 self.abs_max_out: 3818.0\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.398746/  1.569650, val:  81.25%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1756%\n",
      "layer   2  Sparsity: 72.1411%\n",
      "layer   3  Sparsity: 74.1024%\n",
      "total_backward_count 685300 real_backward_count 85861  12.529%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.392723/  1.582169, val:  81.25%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1841%\n",
      "layer   2  Sparsity: 72.4925%\n",
      "layer   3  Sparsity: 74.2979%\n",
      "total_backward_count 690195 real_backward_count 86204  12.490%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.401234/  1.587344, val:  83.33%, val_best:  87.08%, tr:  99.08%, tr_best: 100.00%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1480%\n",
      "layer   2  Sparsity: 72.5049%\n",
      "layer   3  Sparsity: 73.6877%\n",
      "total_backward_count 695090 real_backward_count 86553  12.452%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.401274/  1.604160, val:  77.92%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.2188%\n",
      "layer   2  Sparsity: 72.8880%\n",
      "layer   3  Sparsity: 73.5773%\n",
      "total_backward_count 699985 real_backward_count 86941  12.420%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.387509/  1.568061, val:  82.08%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.70 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1789%\n",
      "layer   2  Sparsity: 72.6947%\n",
      "layer   3  Sparsity: 73.8650%\n",
      "total_backward_count 704880 real_backward_count 87254  12.379%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.382064/  1.559698, val:  81.67%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1901%\n",
      "layer   2  Sparsity: 72.6554%\n",
      "layer   3  Sparsity: 73.5240%\n",
      "total_backward_count 709775 real_backward_count 87604  12.343%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.383633/  1.558072, val:  72.08%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 45.04 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1881%\n",
      "layer   2  Sparsity: 72.6777%\n",
      "layer   3  Sparsity: 73.7381%\n",
      "total_backward_count 714670 real_backward_count 87946  12.306%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.362065/  1.554051, val:  75.83%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 44.29 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1627%\n",
      "layer   2  Sparsity: 72.6478%\n",
      "layer   3  Sparsity: 74.0331%\n",
      "total_backward_count 719565 real_backward_count 88258  12.265%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.365701/  1.569685, val:  80.83%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 45.25 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.2114%\n",
      "layer   2  Sparsity: 72.5710%\n",
      "layer   3  Sparsity: 73.9513%\n",
      "total_backward_count 724460 real_backward_count 88610  12.231%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.383982/  1.546898, val:  78.33%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1789%\n",
      "layer   2  Sparsity: 72.5596%\n",
      "layer   3  Sparsity: 73.7860%\n",
      "total_backward_count 729355 real_backward_count 88950  12.196%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.384067/  1.574344, val:  76.25%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1747%\n",
      "layer   2  Sparsity: 72.5261%\n",
      "layer   3  Sparsity: 73.8522%\n",
      "total_backward_count 734250 real_backward_count 89284  12.160%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.392649/  1.567839, val:  80.42%, val_best:  87.08%, tr:  99.28%, tr_best: 100.00%, epoch time: 44.45 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1898%\n",
      "layer   2  Sparsity: 72.3950%\n",
      "layer   3  Sparsity: 73.9940%\n",
      "total_backward_count 739145 real_backward_count 89641  12.128%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.376347/  1.541930, val:  85.00%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1540%\n",
      "layer   2  Sparsity: 72.3911%\n",
      "layer   3  Sparsity: 74.1045%\n",
      "total_backward_count 744040 real_backward_count 89936  12.088%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.391998/  1.577306, val:  78.75%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1754%\n",
      "layer   2  Sparsity: 72.3225%\n",
      "layer   3  Sparsity: 74.4552%\n",
      "total_backward_count 748935 real_backward_count 90261  12.052%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.396482/  1.566145, val:  81.67%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1270%\n",
      "layer   2  Sparsity: 72.2203%\n",
      "layer   3  Sparsity: 74.5583%\n",
      "total_backward_count 753830 real_backward_count 90583  12.016%\n",
      "fc layer 2 self.abs_max_out: 2072.0\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.388577/  1.571498, val:  80.42%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1485%\n",
      "layer   2  Sparsity: 72.0431%\n",
      "layer   3  Sparsity: 74.4390%\n",
      "total_backward_count 758725 real_backward_count 90918  11.983%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.372632/  1.560974, val:  80.00%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1664%\n",
      "layer   2  Sparsity: 72.0407%\n",
      "layer   3  Sparsity: 74.3195%\n",
      "total_backward_count 763620 real_backward_count 91194  11.942%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.369624/  1.578019, val:  79.58%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 44.84 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1823%\n",
      "layer   2  Sparsity: 72.2481%\n",
      "layer   3  Sparsity: 74.5141%\n",
      "total_backward_count 768515 real_backward_count 91521  11.909%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.379658/  1.561150, val:  79.17%, val_best:  87.08%, tr:  99.18%, tr_best: 100.00%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.2301%\n",
      "layer   2  Sparsity: 72.3291%\n",
      "layer   3  Sparsity: 74.4652%\n",
      "total_backward_count 773410 real_backward_count 91855  11.877%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.364979/  1.539399, val:  84.58%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.26 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1684%\n",
      "layer   2  Sparsity: 72.4100%\n",
      "layer   3  Sparsity: 74.3015%\n",
      "total_backward_count 778305 real_backward_count 92153  11.840%\n",
      "lif layer 2 self.abs_max_v: 2917.0\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.371398/  1.571934, val:  76.25%, val_best:  87.08%, tr:  99.28%, tr_best: 100.00%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1816%\n",
      "layer   2  Sparsity: 72.4411%\n",
      "layer   3  Sparsity: 74.2477%\n",
      "total_backward_count 783200 real_backward_count 92457  11.805%\n",
      "fc layer 1 self.abs_max_out: 3875.0\n",
      "lif layer 1 self.abs_max_v: 6156.5\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.370338/  1.549934, val:  82.08%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.54 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1727%\n",
      "layer   2  Sparsity: 72.5979%\n",
      "layer   3  Sparsity: 74.1370%\n",
      "total_backward_count 788095 real_backward_count 92795  11.775%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.343536/  1.516210, val:  81.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1926%\n",
      "layer   2  Sparsity: 72.3231%\n",
      "layer   3  Sparsity: 74.2147%\n",
      "total_backward_count 792990 real_backward_count 93103  11.741%\n",
      "fc layer 1 self.abs_max_out: 3878.0\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.333793/  1.537588, val:  79.17%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.24 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1524%\n",
      "layer   2  Sparsity: 72.4477%\n",
      "layer   3  Sparsity: 74.2428%\n",
      "total_backward_count 797885 real_backward_count 93418  11.708%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.325907/  1.524733, val:  82.50%, val_best:  87.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 45.09 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1557%\n",
      "layer   2  Sparsity: 72.3660%\n",
      "layer   3  Sparsity: 74.3785%\n",
      "total_backward_count 802780 real_backward_count 93731  11.676%\n",
      "fc layer 1 self.abs_max_out: 3891.0\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.333004/  1.534734, val:  74.58%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.2064%\n",
      "layer   2  Sparsity: 72.5852%\n",
      "layer   3  Sparsity: 74.3425%\n",
      "total_backward_count 807675 real_backward_count 94066  11.647%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.324830/  1.508296, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.97 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1692%\n",
      "layer   2  Sparsity: 72.2824%\n",
      "layer   3  Sparsity: 74.5637%\n",
      "total_backward_count 812570 real_backward_count 94365  11.613%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.313323/  1.512514, val:  80.42%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1578%\n",
      "layer   2  Sparsity: 72.5888%\n",
      "layer   3  Sparsity: 74.9059%\n",
      "total_backward_count 817465 real_backward_count 94628  11.576%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.339437/  1.541860, val:  74.58%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 45.25 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1662%\n",
      "layer   2  Sparsity: 72.6176%\n",
      "layer   3  Sparsity: 74.6800%\n",
      "total_backward_count 822360 real_backward_count 94944  11.545%\n",
      "fc layer 1 self.abs_max_out: 3924.0\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.345776/  1.532513, val:  80.83%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1736%\n",
      "layer   2  Sparsity: 72.2584%\n",
      "layer   3  Sparsity: 74.9345%\n",
      "total_backward_count 827255 real_backward_count 95248  11.514%\n",
      "fc layer 1 self.abs_max_out: 3961.0\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.337794/  1.509737, val:  82.08%, val_best:  87.08%, tr:  99.08%, tr_best: 100.00%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1739%\n",
      "layer   2  Sparsity: 72.2280%\n",
      "layer   3  Sparsity: 74.3378%\n",
      "total_backward_count 832150 real_backward_count 95551  11.482%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.323127/  1.511916, val:  81.25%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.2047%\n",
      "layer   2  Sparsity: 72.3046%\n",
      "layer   3  Sparsity: 74.0426%\n",
      "total_backward_count 837045 real_backward_count 95847  11.451%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.320502/  1.493880, val:  82.50%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 45.29 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1843%\n",
      "layer   2  Sparsity: 72.2656%\n",
      "layer   3  Sparsity: 74.2378%\n",
      "total_backward_count 841940 real_backward_count 96159  11.421%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.320319/  1.509395, val:  80.83%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1674%\n",
      "layer   2  Sparsity: 72.3924%\n",
      "layer   3  Sparsity: 74.6093%\n",
      "total_backward_count 846835 real_backward_count 96475  11.392%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.339376/  1.523735, val:  80.83%, val_best:  87.08%, tr:  99.18%, tr_best: 100.00%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1702%\n",
      "layer   2  Sparsity: 72.3346%\n",
      "layer   3  Sparsity: 74.7430%\n",
      "total_backward_count 851730 real_backward_count 96776  11.362%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.339074/  1.498515, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1584%\n",
      "layer   2  Sparsity: 72.2456%\n",
      "layer   3  Sparsity: 74.7258%\n",
      "total_backward_count 856625 real_backward_count 97053  11.330%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.334961/  1.515962, val:  79.17%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 45.47 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 93.1665%\n",
      "layer   2  Sparsity: 72.3509%\n",
      "layer   3  Sparsity: 74.7292%\n",
      "total_backward_count 861520 real_backward_count 97367  11.302%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.339009/  1.530824, val:  79.17%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.89 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.2058%\n",
      "layer   2  Sparsity: 72.3455%\n",
      "layer   3  Sparsity: 74.4853%\n",
      "total_backward_count 866415 real_backward_count 97653  11.271%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.338633/  1.520044, val:  81.67%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 45.12 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1451%\n",
      "layer   2  Sparsity: 72.3246%\n",
      "layer   3  Sparsity: 74.3724%\n",
      "total_backward_count 871310 real_backward_count 97936  11.240%\n",
      "fc layer 2 self.abs_max_out: 2082.0\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.339558/  1.543529, val:  77.08%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 45.04 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1665%\n",
      "layer   2  Sparsity: 72.4700%\n",
      "layer   3  Sparsity: 74.4390%\n",
      "total_backward_count 876205 real_backward_count 98242  11.212%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.332033/  1.518346, val:  80.00%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1405%\n",
      "layer   2  Sparsity: 72.4030%\n",
      "layer   3  Sparsity: 74.6593%\n",
      "total_backward_count 881100 real_backward_count 98499  11.179%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.325975/  1.487560, val:  82.92%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 45.03 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.2028%\n",
      "layer   2  Sparsity: 72.5207%\n",
      "layer   3  Sparsity: 74.4197%\n",
      "total_backward_count 885995 real_backward_count 98759  11.147%\n",
      "lif layer 1 self.abs_max_v: 6180.5\n",
      "fc layer 1 self.abs_max_out: 3966.0\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.325188/  1.505314, val:  84.58%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1719%\n",
      "layer   2  Sparsity: 72.3234%\n",
      "layer   3  Sparsity: 74.5377%\n",
      "total_backward_count 890890 real_backward_count 99039  11.117%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.323080/  1.514984, val:  83.75%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 45.29 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1985%\n",
      "layer   2  Sparsity: 72.3783%\n",
      "layer   3  Sparsity: 74.6072%\n",
      "total_backward_count 895785 real_backward_count 99312  11.087%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.321577/  1.507875, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.43 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1323%\n",
      "layer   2  Sparsity: 72.2495%\n",
      "layer   3  Sparsity: 74.5338%\n",
      "total_backward_count 900680 real_backward_count 99582  11.056%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.320650/  1.491373, val:  85.42%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1776%\n",
      "layer   2  Sparsity: 72.1269%\n",
      "layer   3  Sparsity: 74.5747%\n",
      "total_backward_count 905575 real_backward_count 99822  11.023%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.313590/  1.490965, val:  85.00%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1416%\n",
      "layer   2  Sparsity: 71.8685%\n",
      "layer   3  Sparsity: 74.5659%\n",
      "total_backward_count 910470 real_backward_count 100067  10.991%\n",
      "lif layer 1 self.abs_max_v: 6224.0\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.305068/  1.518383, val:  79.58%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1636%\n",
      "layer   2  Sparsity: 72.0380%\n",
      "layer   3  Sparsity: 74.7427%\n",
      "total_backward_count 915365 real_backward_count 100360  10.964%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.307652/  1.515563, val:  81.25%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1666%\n",
      "layer   2  Sparsity: 72.0482%\n",
      "layer   3  Sparsity: 74.6171%\n",
      "total_backward_count 920260 real_backward_count 100635  10.935%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.331671/  1.523251, val:  82.92%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1907%\n",
      "layer   2  Sparsity: 72.0123%\n",
      "layer   3  Sparsity: 74.9922%\n",
      "total_backward_count 925155 real_backward_count 100937  10.910%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.328484/  1.515621, val:  85.42%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1878%\n",
      "layer   2  Sparsity: 72.2599%\n",
      "layer   3  Sparsity: 74.9084%\n",
      "total_backward_count 930050 real_backward_count 101227  10.884%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.328138/  1.521957, val:  82.08%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.2104%\n",
      "layer   2  Sparsity: 72.3821%\n",
      "layer   3  Sparsity: 75.1239%\n",
      "total_backward_count 934945 real_backward_count 101495  10.856%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.336444/  1.527950, val:  80.42%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1934%\n",
      "layer   2  Sparsity: 72.3961%\n",
      "layer   3  Sparsity: 74.6440%\n",
      "total_backward_count 939840 real_backward_count 101767  10.828%\n",
      "fc layer 3 self.abs_max_out: 645.0\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.336442/  1.520424, val:  85.42%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1412%\n",
      "layer   2  Sparsity: 72.3623%\n",
      "layer   3  Sparsity: 74.6382%\n",
      "total_backward_count 944735 real_backward_count 102037  10.801%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.323048/  1.512194, val:  85.42%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.03 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 93.1662%\n",
      "layer   2  Sparsity: 72.3888%\n",
      "layer   3  Sparsity: 74.7228%\n",
      "total_backward_count 949630 real_backward_count 102291  10.772%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.341620/  1.506249, val:  82.50%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 93.1561%\n",
      "layer   2  Sparsity: 72.1909%\n",
      "layer   3  Sparsity: 74.7386%\n",
      "total_backward_count 954525 real_backward_count 102586  10.747%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.328191/  1.516011, val:  82.08%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 44.03 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 93.1221%\n",
      "layer   2  Sparsity: 72.2178%\n",
      "layer   3  Sparsity: 74.4907%\n",
      "total_backward_count 959420 real_backward_count 102841  10.719%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.327766/  1.515114, val:  83.75%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.23 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1505%\n",
      "layer   2  Sparsity: 72.2286%\n",
      "layer   3  Sparsity: 74.9541%\n",
      "total_backward_count 964315 real_backward_count 103109  10.692%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.320727/  1.505960, val:  81.67%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1826%\n",
      "layer   2  Sparsity: 72.4317%\n",
      "layer   3  Sparsity: 74.7523%\n",
      "total_backward_count 969210 real_backward_count 103385  10.667%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.320705/  1.507010, val:  83.33%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.54 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1639%\n",
      "layer   2  Sparsity: 72.3890%\n",
      "layer   3  Sparsity: 74.6018%\n",
      "total_backward_count 974105 real_backward_count 103648  10.640%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.321562/  1.509989, val:  80.42%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 93.1643%\n",
      "layer   2  Sparsity: 72.1571%\n",
      "layer   3  Sparsity: 75.1446%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7fc8af65874fe0a898c29fec94f2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99694</td></tr><tr><td>tr_epoch_loss</td><td>1.32156</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.80417</td></tr><tr><td>val_loss</td><td>1.50999</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">toasty-sweep-82</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k2x1rj1x' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k2x1rj1x</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_201620-k2x1rj1x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zwclchig with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 50000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_224343-zwclchig</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zwclchig' target=\"_blank\">zesty-sweep-86</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zwclchig' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zwclchig</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251117_224352_247', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 50000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = dac77cc348b2d880ae59906e26f08f17\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 139\n",
      "fc layer 1 self.abs_max_out: 232.0\n",
      "lif layer 1 self.abs_max_v: 232.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 202.0\n",
      "lif layer 2 self.abs_max_v: 202.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 62.0\n",
      "fc layer 1 self.abs_max_out: 261.0\n",
      "lif layer 1 self.abs_max_v: 274.0\n",
      "fc layer 2 self.abs_max_out: 292.0\n",
      "lif layer 2 self.abs_max_v: 328.0\n",
      "fc layer 1 self.abs_max_out: 275.0\n",
      "lif layer 1 self.abs_max_v: 345.0\n",
      "fc layer 2 self.abs_max_out: 363.0\n",
      "lif layer 2 self.abs_max_v: 408.5\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "fc layer 1 self.abs_max_out: 429.0\n",
      "lif layer 1 self.abs_max_v: 429.0\n",
      "fc layer 2 self.abs_max_out: 432.0\n",
      "lif layer 2 self.abs_max_v: 455.5\n",
      "smallest_now_T updated: 125\n",
      "fc layer 3 self.abs_max_out: 149.0\n",
      "lif layer 1 self.abs_max_v: 465.5\n",
      "fc layer 1 self.abs_max_out: 564.0\n",
      "lif layer 1 self.abs_max_v: 564.0\n",
      "lif layer 2 self.abs_max_v: 478.0\n",
      "fc layer 1 self.abs_max_out: 576.0\n",
      "lif layer 1 self.abs_max_v: 576.0\n",
      "fc layer 2 self.abs_max_out: 434.0\n",
      "lif layer 2 self.abs_max_v: 665.0\n",
      "smallest_now_T updated: 94\n",
      "fc layer 3 self.abs_max_out: 217.0\n",
      "fc layer 1 self.abs_max_out: 691.0\n",
      "lif layer 1 self.abs_max_v: 691.0\n",
      "fc layer 2 self.abs_max_out: 479.0\n",
      "lif layer 2 self.abs_max_v: 728.5\n",
      "lif layer 1 self.abs_max_v: 738.5\n",
      "lif layer 1 self.abs_max_v: 770.0\n",
      "fc layer 2 self.abs_max_out: 522.0\n",
      "lif layer 1 self.abs_max_v: 870.5\n",
      "fc layer 2 self.abs_max_out: 524.0\n",
      "lif layer 2 self.abs_max_v: 789.5\n",
      "fc layer 1 self.abs_max_out: 763.0\n",
      "fc layer 2 self.abs_max_out: 554.0\n",
      "lif layer 2 self.abs_max_v: 816.0\n",
      "smallest_now_T updated: 79\n",
      "lif layer 2 self.abs_max_v: 837.0\n",
      "fc layer 1 self.abs_max_out: 842.0\n",
      "fc layer 3 self.abs_max_out: 225.0\n",
      "fc layer 3 self.abs_max_out: 241.0\n",
      "fc layer 1 self.abs_max_out: 960.0\n",
      "lif layer 1 self.abs_max_v: 960.0\n",
      "fc layer 1 self.abs_max_out: 972.0\n",
      "lif layer 1 self.abs_max_v: 1089.5\n",
      "lif layer 2 self.abs_max_v: 846.0\n",
      "fc layer 1 self.abs_max_out: 984.0\n",
      "lif layer 1 self.abs_max_v: 1131.5\n",
      "fc layer 1 self.abs_max_out: 1007.0\n",
      "lif layer 1 self.abs_max_v: 1152.0\n",
      "lif layer 2 self.abs_max_v: 864.0\n",
      "fc layer 2 self.abs_max_out: 594.0\n",
      "fc layer 3 self.abs_max_out: 251.0\n",
      "fc layer 2 self.abs_max_out: 606.0\n",
      "fc layer 2 self.abs_max_out: 681.0\n",
      "fc layer 1 self.abs_max_out: 1104.0\n",
      "lif layer 1 self.abs_max_v: 1159.0\n",
      "lif layer 2 self.abs_max_v: 865.0\n",
      "lif layer 2 self.abs_max_v: 913.5\n",
      "lif layer 2 self.abs_max_v: 914.5\n",
      "lif layer 2 self.abs_max_v: 929.5\n",
      "smallest_now_T updated: 73\n",
      "lif layer 1 self.abs_max_v: 1162.0\n",
      "lif layer 2 self.abs_max_v: 1057.0\n",
      "lif layer 1 self.abs_max_v: 1162.5\n",
      "lif layer 1 self.abs_max_v: 1302.5\n",
      "fc layer 2 self.abs_max_out: 702.0\n",
      "lif layer 1 self.abs_max_v: 1483.0\n",
      "lif layer 2 self.abs_max_v: 1127.5\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "fc layer 2 self.abs_max_out: 718.0\n",
      "fc layer 2 self.abs_max_out: 741.0\n",
      "fc layer 2 self.abs_max_out: 750.0\n",
      "lif layer 2 self.abs_max_v: 1132.0\n",
      "fc layer 2 self.abs_max_out: 760.0\n",
      "smallest_now_T updated: 65\n",
      "lif layer 2 self.abs_max_v: 1135.5\n",
      "lif layer 2 self.abs_max_v: 1231.0\n",
      "fc layer 2 self.abs_max_out: 809.0\n",
      "fc layer 2 self.abs_max_out: 813.0\n",
      "fc layer 2 self.abs_max_out: 885.0\n",
      "fc layer 1 self.abs_max_out: 1111.0\n",
      "lif layer 1 self.abs_max_v: 1729.0\n",
      "fc layer 2 self.abs_max_out: 947.0\n",
      "fc layer 2 self.abs_max_out: 962.0\n",
      "fc layer 2 self.abs_max_out: 982.0\n",
      "fc layer 1 self.abs_max_out: 1195.0\n",
      "fc layer 3 self.abs_max_out: 295.0\n",
      "fc layer 1 self.abs_max_out: 1216.0\n",
      "fc layer 1 self.abs_max_out: 1265.0\n",
      "fc layer 2 self.abs_max_out: 1014.0\n",
      "lif layer 1 self.abs_max_v: 1745.0\n",
      "smallest_now_T updated: 56\n",
      "smallest_now_T updated: 43\n",
      "lif layer 2 self.abs_max_v: 1281.5\n",
      "lif layer 2 self.abs_max_v: 1507.5\n",
      "lif layer 2 self.abs_max_v: 1525.0\n",
      "lif layer 2 self.abs_max_v: 1570.5\n",
      "fc layer 1 self.abs_max_out: 1494.0\n",
      "fc layer 3 self.abs_max_out: 299.0\n",
      "fc layer 2 self.abs_max_out: 1046.0\n",
      "fc layer 1 self.abs_max_out: 1548.0\n",
      "fc layer 3 self.abs_max_out: 330.0\n",
      "fc layer 2 self.abs_max_out: 1064.0\n",
      "lif layer 1 self.abs_max_v: 1902.0\n",
      "lif layer 1 self.abs_max_v: 1947.0\n",
      "fc layer 1 self.abs_max_out: 1588.0\n",
      "fc layer 3 self.abs_max_out: 334.0\n",
      "fc layer 2 self.abs_max_out: 1085.0\n",
      "lif layer 1 self.abs_max_v: 2071.5\n",
      "lif layer 2 self.abs_max_v: 1581.5\n",
      "lif layer 1 self.abs_max_v: 2353.0\n",
      "lif layer 2 self.abs_max_v: 1590.0\n",
      "fc layer 1 self.abs_max_out: 1899.0\n",
      "fc layer 2 self.abs_max_out: 1086.0\n",
      "lif layer 2 self.abs_max_v: 1642.5\n",
      "fc layer 3 self.abs_max_out: 335.0\n",
      "fc layer 3 self.abs_max_out: 343.0\n",
      "lif layer 2 self.abs_max_v: 1658.5\n",
      "lif layer 2 self.abs_max_v: 1705.5\n",
      "lif layer 2 self.abs_max_v: 1802.0\n",
      "lif layer 1 self.abs_max_v: 2359.0\n",
      "fc layer 3 self.abs_max_out: 351.0\n",
      "lif layer 1 self.abs_max_v: 2383.0\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "lif layer 1 self.abs_max_v: 2529.5\n",
      "lif layer 1 self.abs_max_v: 2620.0\n",
      "smallest_now_T_val updated: 129\n",
      "smallest_now_T_val updated: 106\n",
      "smallest_now_T_val updated: 104\n",
      "smallest_now_T_val updated: 102\n",
      "smallest_now_T_val updated: 85\n",
      "smallest_now_T_val updated: 29\n",
      "lif layer 1 self.abs_max_v: 2629.5\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.863029/  1.979462, val:  33.75%, val_best:  33.75%, tr:  81.21%, tr_best:  81.21%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8491%\n",
      "layer   2  Sparsity: 70.9498%\n",
      "layer   3  Sparsity: 72.2161%\n",
      "total_backward_count 4895 real_backward_count 1660  33.912%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1095.0\n",
      "fc layer 2 self.abs_max_out: 1105.0\n",
      "fc layer 1 self.abs_max_out: 1929.0\n",
      "lif layer 1 self.abs_max_v: 2656.0\n",
      "fc layer 2 self.abs_max_out: 1112.0\n",
      "fc layer 2 self.abs_max_out: 1120.0\n",
      "fc layer 2 self.abs_max_out: 1121.0\n",
      "fc layer 2 self.abs_max_out: 1125.0\n",
      "fc layer 2 self.abs_max_out: 1160.0\n",
      "fc layer 2 self.abs_max_out: 1176.0\n",
      "fc layer 1 self.abs_max_out: 2037.0\n",
      "lif layer 1 self.abs_max_v: 2797.0\n",
      "lif layer 2 self.abs_max_v: 1843.5\n",
      "lif layer 1 self.abs_max_v: 2820.5\n",
      "lif layer 1 self.abs_max_v: 3141.5\n",
      "fc layer 1 self.abs_max_out: 2142.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.762167/  1.912093, val:  44.17%, val_best:  44.17%, tr:  90.19%, tr_best:  90.19%, epoch time: 45.20 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8379%\n",
      "layer   2  Sparsity: 74.0264%\n",
      "layer   3  Sparsity: 72.5130%\n",
      "total_backward_count 9790 real_backward_count 2819  28.795%\n",
      "fc layer 2 self.abs_max_out: 1188.0\n",
      "lif layer 1 self.abs_max_v: 3223.0\n",
      "lif layer 1 self.abs_max_v: 3324.5\n",
      "fc layer 2 self.abs_max_out: 1218.0\n",
      "fc layer 2 self.abs_max_out: 1220.0\n",
      "fc layer 2 self.abs_max_out: 1221.0\n",
      "fc layer 2 self.abs_max_out: 1235.0\n",
      "fc layer 2 self.abs_max_out: 1236.0\n",
      "fc layer 2 self.abs_max_out: 1293.0\n",
      "fc layer 1 self.abs_max_out: 2259.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.728760/  1.877680, val:  48.33%, val_best:  48.33%, tr:  92.03%, tr_best:  92.03%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.9140%\n",
      "layer   2  Sparsity: 74.4702%\n",
      "layer   3  Sparsity: 72.2838%\n",
      "total_backward_count 14685 real_backward_count 3954  26.925%\n",
      "fc layer 3 self.abs_max_out: 409.0\n",
      "fc layer 3 self.abs_max_out: 433.0\n",
      "fc layer 3 self.abs_max_out: 447.0\n",
      "lif layer 2 self.abs_max_v: 1868.5\n",
      "fc layer 2 self.abs_max_out: 1304.0\n",
      "fc layer 3 self.abs_max_out: 451.0\n",
      "lif layer 2 self.abs_max_v: 1871.0\n",
      "lif layer 2 self.abs_max_v: 1880.5\n",
      "fc layer 3 self.abs_max_out: 452.0\n",
      "lif layer 1 self.abs_max_v: 3380.0\n",
      "fc layer 1 self.abs_max_out: 2543.0\n",
      "lif layer 1 self.abs_max_v: 3407.5\n",
      "lif layer 1 self.abs_max_v: 3518.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.705840/  1.867091, val:  44.58%, val_best:  48.33%, tr:  92.75%, tr_best:  92.75%, epoch time: 44.98 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8813%\n",
      "layer   2  Sparsity: 75.4771%\n",
      "layer   3  Sparsity: 71.8209%\n",
      "total_backward_count 19580 real_backward_count 4964  25.352%\n",
      "lif layer 1 self.abs_max_v: 3534.5\n",
      "lif layer 1 self.abs_max_v: 3671.0\n",
      "fc layer 2 self.abs_max_out: 1348.0\n",
      "fc layer 2 self.abs_max_out: 1445.0\n",
      "lif layer 1 self.abs_max_v: 3740.0\n",
      "lif layer 2 self.abs_max_v: 1946.0\n",
      "lif layer 1 self.abs_max_v: 3849.5\n",
      "fc layer 1 self.abs_max_out: 2725.0\n",
      "lif layer 1 self.abs_max_v: 3921.5\n",
      "lif layer 1 self.abs_max_v: 4206.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.693602/  1.847745, val:  57.92%, val_best:  57.92%, tr:  95.51%, tr_best:  95.51%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8118%\n",
      "layer   2  Sparsity: 74.9768%\n",
      "layer   3  Sparsity: 70.6412%\n",
      "total_backward_count 24475 real_backward_count 5924  24.204%\n",
      "lif layer 2 self.abs_max_v: 1969.0\n",
      "fc layer 2 self.abs_max_out: 1447.0\n",
      "lif layer 2 self.abs_max_v: 1978.5\n",
      "fc layer 3 self.abs_max_out: 487.0\n",
      "fc layer 1 self.abs_max_out: 2869.0\n",
      "lif layer 1 self.abs_max_v: 4241.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.680444/  1.834462, val:  59.58%, val_best:  59.58%, tr:  93.87%, tr_best:  95.51%, epoch time: 45.30 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.8913%\n",
      "layer   2  Sparsity: 75.3735%\n",
      "layer   3  Sparsity: 71.0282%\n",
      "total_backward_count 29370 real_backward_count 6873  23.401%\n",
      "fc layer 2 self.abs_max_out: 1461.0\n",
      "fc layer 2 self.abs_max_out: 1464.0\n",
      "lif layer 2 self.abs_max_v: 2092.5\n",
      "fc layer 2 self.abs_max_out: 1468.0\n",
      "lif layer 2 self.abs_max_v: 2196.0\n",
      "fc layer 2 self.abs_max_out: 1472.0\n",
      "lif layer 1 self.abs_max_v: 4265.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.654705/  1.810996, val:  50.42%, val_best:  59.58%, tr:  93.87%, tr_best:  95.51%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8583%\n",
      "layer   2  Sparsity: 75.6555%\n",
      "layer   3  Sparsity: 71.9205%\n",
      "total_backward_count 34265 real_backward_count 7785  22.720%\n",
      "lif layer 2 self.abs_max_v: 2207.5\n",
      "fc layer 3 self.abs_max_out: 494.0\n",
      "fc layer 3 self.abs_max_out: 501.0\n",
      "fc layer 2 self.abs_max_out: 1480.0\n",
      "lif layer 1 self.abs_max_v: 4774.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.627102/  1.789748, val:  55.83%, val_best:  59.58%, tr:  95.40%, tr_best:  95.51%, epoch time: 45.09 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8775%\n",
      "layer   2  Sparsity: 75.4537%\n",
      "layer   3  Sparsity: 71.6376%\n",
      "total_backward_count 39160 real_backward_count 8647  22.081%\n",
      "fc layer 2 self.abs_max_out: 1483.0\n",
      "fc layer 2 self.abs_max_out: 1494.0\n",
      "fc layer 2 self.abs_max_out: 1585.0\n",
      "fc layer 2 self.abs_max_out: 1645.0\n",
      "fc layer 1 self.abs_max_out: 2876.0\n",
      "lif layer 1 self.abs_max_v: 5003.5\n",
      "fc layer 1 self.abs_max_out: 3128.0\n",
      "fc layer 1 self.abs_max_out: 3283.0\n",
      "lif layer 1 self.abs_max_v: 5142.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.633677/  1.759275, val:  69.17%, val_best:  69.17%, tr:  95.20%, tr_best:  95.51%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8470%\n",
      "layer   2  Sparsity: 75.0650%\n",
      "layer   3  Sparsity: 72.2008%\n",
      "total_backward_count 44055 real_backward_count 9545  21.666%\n",
      "lif layer 2 self.abs_max_v: 2268.0\n",
      "lif layer 2 self.abs_max_v: 2289.0\n",
      "fc layer 3 self.abs_max_out: 504.0\n",
      "fc layer 3 self.abs_max_out: 506.0\n",
      "fc layer 3 self.abs_max_out: 518.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.595181/  1.802662, val:  52.50%, val_best:  69.17%, tr:  95.61%, tr_best:  95.61%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8945%\n",
      "layer   2  Sparsity: 74.9082%\n",
      "layer   3  Sparsity: 73.8028%\n",
      "total_backward_count 48950 real_backward_count 10376  21.197%\n",
      "fc layer 3 self.abs_max_out: 522.0\n",
      "fc layer 2 self.abs_max_out: 1664.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.591300/  1.778837, val:  50.42%, val_best:  69.17%, tr:  96.22%, tr_best:  96.22%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8241%\n",
      "layer   2  Sparsity: 75.0758%\n",
      "layer   3  Sparsity: 73.7449%\n",
      "total_backward_count 53845 real_backward_count 11201  20.802%\n",
      "lif layer 2 self.abs_max_v: 2338.0\n",
      "fc layer 3 self.abs_max_out: 544.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.582479/  1.731406, val:  65.00%, val_best:  69.17%, tr:  97.65%, tr_best:  97.65%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8326%\n",
      "layer   2  Sparsity: 74.3500%\n",
      "layer   3  Sparsity: 73.2857%\n",
      "total_backward_count 58740 real_backward_count 11950  20.344%\n",
      "fc layer 3 self.abs_max_out: 583.0\n",
      "fc layer 1 self.abs_max_out: 3284.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.568660/  1.753416, val:  64.17%, val_best:  69.17%, tr:  97.14%, tr_best:  97.65%, epoch time: 44.07 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 88.8258%\n",
      "layer   2  Sparsity: 74.5272%\n",
      "layer   3  Sparsity: 74.4649%\n",
      "total_backward_count 63635 real_backward_count 12683  19.931%\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.590620/  1.752238, val:  66.67%, val_best:  69.17%, tr:  96.94%, tr_best:  97.65%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8427%\n",
      "layer   2  Sparsity: 73.9888%\n",
      "layer   3  Sparsity: 74.4392%\n",
      "total_backward_count 68530 real_backward_count 13417  19.578%\n",
      "fc layer 1 self.abs_max_out: 3290.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.576464/  1.738778, val:  67.50%, val_best:  69.17%, tr:  97.45%, tr_best:  97.65%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8979%\n",
      "layer   2  Sparsity: 74.2952%\n",
      "layer   3  Sparsity: 74.1871%\n",
      "total_backward_count 73425 real_backward_count 14149  19.270%\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.555276/  1.717542, val:  63.33%, val_best:  69.17%, tr:  97.45%, tr_best:  97.65%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8514%\n",
      "layer   2  Sparsity: 74.3201%\n",
      "layer   3  Sparsity: 74.3939%\n",
      "total_backward_count 78320 real_backward_count 14852  18.963%\n",
      "fc layer 2 self.abs_max_out: 1737.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.529109/  1.706398, val:  71.67%, val_best:  71.67%, tr:  97.96%, tr_best:  97.96%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8359%\n",
      "layer   2  Sparsity: 74.5753%\n",
      "layer   3  Sparsity: 75.0188%\n",
      "total_backward_count 83215 real_backward_count 15510  18.638%\n",
      "fc layer 2 self.abs_max_out: 1765.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.562279/  1.696474, val:  83.33%, val_best:  83.33%, tr:  97.14%, tr_best:  97.96%, epoch time: 45.06 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8467%\n",
      "layer   2  Sparsity: 74.5503%\n",
      "layer   3  Sparsity: 75.7874%\n",
      "total_backward_count 88110 real_backward_count 16179  18.362%\n",
      "fc layer 1 self.abs_max_out: 3304.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.554400/  1.697708, val:  74.17%, val_best:  83.33%, tr:  97.55%, tr_best:  97.96%, epoch time: 45.00 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8874%\n",
      "layer   2  Sparsity: 74.0048%\n",
      "layer   3  Sparsity: 75.1236%\n",
      "total_backward_count 93005 real_backward_count 16783  18.045%\n",
      "lif layer 1 self.abs_max_v: 5254.0\n",
      "fc layer 1 self.abs_max_out: 3412.0\n",
      "fc layer 2 self.abs_max_out: 1770.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.523560/  1.707926, val:  59.58%, val_best:  83.33%, tr:  98.06%, tr_best:  98.06%, epoch time: 45.33 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.8833%\n",
      "layer   2  Sparsity: 73.4872%\n",
      "layer   3  Sparsity: 74.2717%\n",
      "total_backward_count 97900 real_backward_count 17359  17.731%\n",
      "lif layer 2 self.abs_max_v: 2441.5\n",
      "lif layer 2 self.abs_max_v: 2442.0\n",
      "lif layer 2 self.abs_max_v: 2461.0\n",
      "lif layer 1 self.abs_max_v: 5307.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.509952/  1.664263, val:  69.17%, val_best:  83.33%, tr:  98.37%, tr_best:  98.37%, epoch time: 45.13 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8434%\n",
      "layer   2  Sparsity: 73.4609%\n",
      "layer   3  Sparsity: 74.6379%\n",
      "total_backward_count 102795 real_backward_count 17941  17.453%\n",
      "fc layer 2 self.abs_max_out: 1798.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.490499/  1.668522, val:  77.92%, val_best:  83.33%, tr:  99.08%, tr_best:  99.08%, epoch time: 45.12 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8668%\n",
      "layer   2  Sparsity: 73.7527%\n",
      "layer   3  Sparsity: 74.7974%\n",
      "total_backward_count 107690 real_backward_count 18470  17.151%\n",
      "lif layer 2 self.abs_max_v: 2487.0\n",
      "lif layer 2 self.abs_max_v: 2627.5\n",
      "fc layer 1 self.abs_max_out: 3442.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.533594/  1.667195, val:  70.42%, val_best:  83.33%, tr:  98.77%, tr_best:  99.08%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8557%\n",
      "layer   2  Sparsity: 73.3825%\n",
      "layer   3  Sparsity: 74.7304%\n",
      "total_backward_count 112585 real_backward_count 19067  16.936%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.513968/  1.662331, val:  82.50%, val_best:  83.33%, tr:  98.47%, tr_best:  99.08%, epoch time: 45.03 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8237%\n",
      "layer   2  Sparsity: 73.5818%\n",
      "layer   3  Sparsity: 75.1979%\n",
      "total_backward_count 117480 real_backward_count 19600  16.684%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.513020/  1.657322, val:  71.67%, val_best:  83.33%, tr:  98.88%, tr_best:  99.08%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8199%\n",
      "layer   2  Sparsity: 73.3660%\n",
      "layer   3  Sparsity: 75.0923%\n",
      "total_backward_count 122375 real_backward_count 20159  16.473%\n",
      "fc layer 1 self.abs_max_out: 3532.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.506686/  1.676925, val:  76.25%, val_best:  83.33%, tr:  98.77%, tr_best:  99.08%, epoch time: 44.94 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8771%\n",
      "layer   2  Sparsity: 73.7057%\n",
      "layer   3  Sparsity: 75.6121%\n",
      "total_backward_count 127270 real_backward_count 20695  16.261%\n",
      "fc layer 1 self.abs_max_out: 3572.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.497576/  1.661163, val:  78.33%, val_best:  83.33%, tr:  98.37%, tr_best:  99.08%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8770%\n",
      "layer   2  Sparsity: 73.4644%\n",
      "layer   3  Sparsity: 75.9393%\n",
      "total_backward_count 132165 real_backward_count 21191  16.034%\n",
      "lif layer 1 self.abs_max_v: 5643.5\n",
      "fc layer 1 self.abs_max_out: 3643.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.497135/  1.673668, val:  75.00%, val_best:  83.33%, tr:  98.98%, tr_best:  99.08%, epoch time: 45.34 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.8544%\n",
      "layer   2  Sparsity: 73.7463%\n",
      "layer   3  Sparsity: 75.8383%\n",
      "total_backward_count 137060 real_backward_count 21670  15.811%\n",
      "fc layer 1 self.abs_max_out: 3679.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.484349/  1.660517, val:  77.50%, val_best:  83.33%, tr:  99.39%, tr_best:  99.39%, epoch time: 44.47 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8781%\n",
      "layer   2  Sparsity: 73.6653%\n",
      "layer   3  Sparsity: 75.7872%\n",
      "total_backward_count 141955 real_backward_count 22121  15.583%\n",
      "fc layer 2 self.abs_max_out: 1806.0\n",
      "lif layer 1 self.abs_max_v: 5725.0\n",
      "fc layer 1 self.abs_max_out: 3713.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.493938/  1.651363, val:  76.67%, val_best:  83.33%, tr:  99.18%, tr_best:  99.39%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8778%\n",
      "layer   2  Sparsity: 72.8002%\n",
      "layer   3  Sparsity: 75.2946%\n",
      "total_backward_count 146850 real_backward_count 22572  15.371%\n",
      "fc layer 2 self.abs_max_out: 1828.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.497036/  1.649683, val:  75.00%, val_best:  83.33%, tr:  98.88%, tr_best:  99.39%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8190%\n",
      "layer   2  Sparsity: 72.9080%\n",
      "layer   3  Sparsity: 75.3206%\n",
      "total_backward_count 151745 real_backward_count 23054  15.193%\n",
      "fc layer 2 self.abs_max_out: 1906.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.473939/  1.632842, val:  80.42%, val_best:  83.33%, tr:  99.08%, tr_best:  99.39%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8897%\n",
      "layer   2  Sparsity: 73.2024%\n",
      "layer   3  Sparsity: 75.8683%\n",
      "total_backward_count 156640 real_backward_count 23511  15.010%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.469364/  1.666764, val:  79.17%, val_best:  83.33%, tr:  99.08%, tr_best:  99.39%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8318%\n",
      "layer   2  Sparsity: 72.8698%\n",
      "layer   3  Sparsity: 75.9842%\n",
      "total_backward_count 161535 real_backward_count 23941  14.821%\n",
      "lif layer 2 self.abs_max_v: 2639.5\n",
      "fc layer 1 self.abs_max_out: 3729.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.479566/  1.626321, val:  77.50%, val_best:  83.33%, tr:  99.18%, tr_best:  99.39%, epoch time: 45.36 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.8477%\n",
      "layer   2  Sparsity: 72.8089%\n",
      "layer   3  Sparsity: 75.9680%\n",
      "total_backward_count 166430 real_backward_count 24342  14.626%\n",
      "lif layer 1 self.abs_max_v: 5727.0\n",
      "fc layer 2 self.abs_max_out: 1926.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.478960/  1.621875, val:  82.08%, val_best:  83.33%, tr:  99.08%, tr_best:  99.39%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.9184%\n",
      "layer   2  Sparsity: 73.0879%\n",
      "layer   3  Sparsity: 76.5789%\n",
      "total_backward_count 171325 real_backward_count 24759  14.451%\n",
      "fc layer 1 self.abs_max_out: 3821.0\n",
      "lif layer 1 self.abs_max_v: 6323.5\n",
      "fc layer 2 self.abs_max_out: 1937.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.458533/  1.620870, val:  78.33%, val_best:  83.33%, tr:  99.18%, tr_best:  99.39%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8496%\n",
      "layer   2  Sparsity: 72.9533%\n",
      "layer   3  Sparsity: 76.4444%\n",
      "total_backward_count 176220 real_backward_count 25146  14.270%\n",
      "fc layer 2 self.abs_max_out: 1943.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.442317/  1.617811, val:  79.58%, val_best:  83.33%, tr:  99.18%, tr_best:  99.39%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.9181%\n",
      "layer   2  Sparsity: 72.6798%\n",
      "layer   3  Sparsity: 76.4282%\n",
      "total_backward_count 181115 real_backward_count 25533  14.098%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.455000/  1.627812, val:  77.08%, val_best:  83.33%, tr:  99.08%, tr_best:  99.39%, epoch time: 44.42 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8904%\n",
      "layer   2  Sparsity: 72.7281%\n",
      "layer   3  Sparsity: 76.5127%\n",
      "total_backward_count 186010 real_backward_count 25914  13.932%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.456804/  1.627346, val:  82.92%, val_best:  83.33%, tr:  99.49%, tr_best:  99.49%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8601%\n",
      "layer   2  Sparsity: 72.8604%\n",
      "layer   3  Sparsity: 76.6927%\n",
      "total_backward_count 190905 real_backward_count 26292  13.772%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.456032/  1.640729, val:  72.08%, val_best:  83.33%, tr:  98.98%, tr_best:  99.49%, epoch time: 45.23 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8740%\n",
      "layer   2  Sparsity: 72.8890%\n",
      "layer   3  Sparsity: 76.9393%\n",
      "total_backward_count 195800 real_backward_count 26623  13.597%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.462578/  1.622914, val:  84.58%, val_best:  84.58%, tr:  99.28%, tr_best:  99.49%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8879%\n",
      "layer   2  Sparsity: 72.6336%\n",
      "layer   3  Sparsity: 77.3618%\n",
      "total_backward_count 200695 real_backward_count 26947  13.427%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.456170/  1.649452, val:  80.42%, val_best:  84.58%, tr:  99.18%, tr_best:  99.49%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8772%\n",
      "layer   2  Sparsity: 72.6987%\n",
      "layer   3  Sparsity: 77.0117%\n",
      "total_backward_count 205590 real_backward_count 27293  13.275%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.456115/  1.589240, val:  84.58%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8626%\n",
      "layer   2  Sparsity: 72.4650%\n",
      "layer   3  Sparsity: 76.5621%\n",
      "total_backward_count 210485 real_backward_count 27603  13.114%\n",
      "lif layer 2 self.abs_max_v: 2759.5\n",
      "lif layer 2 self.abs_max_v: 2878.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.442049/  1.592845, val:  83.75%, val_best:  84.58%, tr:  99.18%, tr_best:  99.80%, epoch time: 44.47 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8516%\n",
      "layer   2  Sparsity: 72.4994%\n",
      "layer   3  Sparsity: 76.5609%\n",
      "total_backward_count 215380 real_backward_count 27940  12.972%\n",
      "fc layer 1 self.abs_max_out: 4140.0\n",
      "lif layer 1 self.abs_max_v: 6406.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.445501/  1.623137, val:  76.25%, val_best:  84.58%, tr:  99.18%, tr_best:  99.80%, epoch time: 44.31 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.9153%\n",
      "layer   2  Sparsity: 72.4094%\n",
      "layer   3  Sparsity: 76.8733%\n",
      "total_backward_count 220275 real_backward_count 28257  12.828%\n",
      "lif layer 2 self.abs_max_v: 2911.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.449424/  1.615693, val:  83.33%, val_best:  84.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8325%\n",
      "layer   2  Sparsity: 72.4770%\n",
      "layer   3  Sparsity: 77.1947%\n",
      "total_backward_count 225170 real_backward_count 28561  12.684%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.460753/  1.612188, val:  83.75%, val_best:  84.58%, tr:  99.18%, tr_best:  99.80%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8490%\n",
      "layer   2  Sparsity: 72.3823%\n",
      "layer   3  Sparsity: 76.9485%\n",
      "total_backward_count 230065 real_backward_count 28875  12.551%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.444106/  1.626108, val:  75.42%, val_best:  84.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 44.54 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8545%\n",
      "layer   2  Sparsity: 72.8600%\n",
      "layer   3  Sparsity: 77.0797%\n",
      "total_backward_count 234960 real_backward_count 29143  12.403%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.448509/  1.616144, val:  85.00%, val_best:  85.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8681%\n",
      "layer   2  Sparsity: 72.5697%\n",
      "layer   3  Sparsity: 77.1858%\n",
      "total_backward_count 239855 real_backward_count 29424  12.267%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.459570/  1.619532, val:  84.58%, val_best:  85.00%, tr:  98.98%, tr_best:  99.90%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8446%\n",
      "layer   2  Sparsity: 72.6153%\n",
      "layer   3  Sparsity: 77.4848%\n",
      "total_backward_count 244750 real_backward_count 29771  12.164%\n",
      "lif layer 2 self.abs_max_v: 3029.5\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.447148/  1.604070, val:  82.08%, val_best:  85.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 45.08 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.9149%\n",
      "layer   2  Sparsity: 72.4893%\n",
      "layer   3  Sparsity: 77.4952%\n",
      "total_backward_count 249645 real_backward_count 30053  12.038%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.432112/  1.580949, val:  84.17%, val_best:  85.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8718%\n",
      "layer   2  Sparsity: 72.1917%\n",
      "layer   3  Sparsity: 76.9890%\n",
      "total_backward_count 254540 real_backward_count 30347  11.922%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.438707/  1.611253, val:  75.83%, val_best:  85.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8335%\n",
      "layer   2  Sparsity: 72.4161%\n",
      "layer   3  Sparsity: 77.0794%\n",
      "total_backward_count 259435 real_backward_count 30624  11.804%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.417814/  1.574141, val:  80.83%, val_best:  85.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.24 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8604%\n",
      "layer   2  Sparsity: 72.6518%\n",
      "layer   3  Sparsity: 77.3272%\n",
      "total_backward_count 264330 real_backward_count 30879  11.682%\n",
      "lif layer 2 self.abs_max_v: 3144.5\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.420840/  1.592927, val:  82.92%, val_best:  85.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 45.46 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.8794%\n",
      "layer   2  Sparsity: 72.7953%\n",
      "layer   3  Sparsity: 77.2819%\n",
      "total_backward_count 269225 real_backward_count 31130  11.563%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.414662/  1.588018, val:  83.33%, val_best:  85.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.59 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8278%\n",
      "layer   2  Sparsity: 72.9103%\n",
      "layer   3  Sparsity: 76.9917%\n",
      "total_backward_count 274120 real_backward_count 31364  11.442%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.436288/  1.590449, val:  79.58%, val_best:  85.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8651%\n",
      "layer   2  Sparsity: 72.6040%\n",
      "layer   3  Sparsity: 77.1895%\n",
      "total_backward_count 279015 real_backward_count 31613  11.330%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.434897/  1.595772, val:  82.08%, val_best:  85.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 42.82 seconds, 0.71 minutes\n",
      "layer   1  Sparsity: 88.8060%\n",
      "layer   2  Sparsity: 72.1052%\n",
      "layer   3  Sparsity: 77.3038%\n",
      "total_backward_count 283910 real_backward_count 31873  11.226%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.413431/  1.580072, val:  86.25%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.14 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8818%\n",
      "layer   2  Sparsity: 72.4851%\n",
      "layer   3  Sparsity: 76.9710%\n",
      "total_backward_count 288805 real_backward_count 32103  11.116%\n",
      "fc layer 1 self.abs_max_out: 4193.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.412584/  1.593703, val:  77.92%, val_best:  86.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8368%\n",
      "layer   2  Sparsity: 72.4656%\n",
      "layer   3  Sparsity: 76.9604%\n",
      "total_backward_count 293700 real_backward_count 32322  11.005%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.401561/  1.579119, val:  82.92%, val_best:  86.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 44.71 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8464%\n",
      "layer   2  Sparsity: 72.7321%\n",
      "layer   3  Sparsity: 76.8786%\n",
      "total_backward_count 298595 real_backward_count 32568  10.907%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.390670/  1.548059, val:  82.08%, val_best:  86.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 45.09 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8696%\n",
      "layer   2  Sparsity: 72.9357%\n",
      "layer   3  Sparsity: 76.9043%\n",
      "total_backward_count 303490 real_backward_count 32794  10.806%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.373419/  1.547993, val:  86.25%, val_best:  86.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8453%\n",
      "layer   2  Sparsity: 73.0111%\n",
      "layer   3  Sparsity: 76.7356%\n",
      "total_backward_count 308385 real_backward_count 33015  10.706%\n",
      "fc layer 1 self.abs_max_out: 4262.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.387974/  1.568933, val:  82.08%, val_best:  86.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.7822%\n",
      "layer   2  Sparsity: 72.5314%\n",
      "layer   3  Sparsity: 76.7286%\n",
      "total_backward_count 313280 real_backward_count 33250  10.614%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.385961/  1.558721, val:  84.58%, val_best:  86.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8521%\n",
      "layer   2  Sparsity: 72.1873%\n",
      "layer   3  Sparsity: 77.1434%\n",
      "total_backward_count 318175 real_backward_count 33462  10.517%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.391186/  1.571496, val:  80.42%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.35 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8733%\n",
      "layer   2  Sparsity: 71.9514%\n",
      "layer   3  Sparsity: 77.2231%\n",
      "total_backward_count 323070 real_backward_count 33710  10.434%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.397433/  1.562331, val:  85.42%, val_best:  86.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.9290%\n",
      "layer   2  Sparsity: 71.9423%\n",
      "layer   3  Sparsity: 77.1436%\n",
      "total_backward_count 327965 real_backward_count 33927  10.345%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.399724/  1.571920, val:  79.17%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8769%\n",
      "layer   2  Sparsity: 72.0043%\n",
      "layer   3  Sparsity: 77.5889%\n",
      "total_backward_count 332860 real_backward_count 34132  10.254%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.389031/  1.561470, val:  84.17%, val_best:  86.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 45.03 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8589%\n",
      "layer   2  Sparsity: 72.3098%\n",
      "layer   3  Sparsity: 77.7581%\n",
      "total_backward_count 337755 real_backward_count 34332  10.165%\n",
      "lif layer 1 self.abs_max_v: 6408.5\n",
      "fc layer 3 self.abs_max_out: 597.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.391237/  1.571505, val:  82.50%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8243%\n",
      "layer   2  Sparsity: 72.4249%\n",
      "layer   3  Sparsity: 77.5368%\n",
      "total_backward_count 342650 real_backward_count 34490  10.066%\n",
      "lif layer 2 self.abs_max_v: 3449.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.403296/  1.551869, val:  85.83%, val_best:  86.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8597%\n",
      "layer   2  Sparsity: 72.2934%\n",
      "layer   3  Sparsity: 77.8574%\n",
      "total_backward_count 347545 real_backward_count 34708   9.987%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.387277/  1.562940, val:  84.58%, val_best:  86.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 45.04 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8668%\n",
      "layer   2  Sparsity: 72.0137%\n",
      "layer   3  Sparsity: 77.6505%\n",
      "total_backward_count 352440 real_backward_count 34902   9.903%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.395716/  1.581752, val:  85.00%, val_best:  86.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8844%\n",
      "layer   2  Sparsity: 71.9677%\n",
      "layer   3  Sparsity: 77.7503%\n",
      "total_backward_count 357335 real_backward_count 35096   9.822%\n",
      "lif layer 1 self.abs_max_v: 6779.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.391962/  1.574747, val:  78.33%, val_best:  86.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8231%\n",
      "layer   2  Sparsity: 72.2879%\n",
      "layer   3  Sparsity: 77.7577%\n",
      "total_backward_count 362230 real_backward_count 35281   9.740%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.385138/  1.570598, val:  85.42%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8892%\n",
      "layer   2  Sparsity: 72.0680%\n",
      "layer   3  Sparsity: 77.8404%\n",
      "total_backward_count 367125 real_backward_count 35467   9.661%\n",
      "lif layer 1 self.abs_max_v: 6937.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.364021/  1.530705, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.97 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8237%\n",
      "layer   2  Sparsity: 72.4269%\n",
      "layer   3  Sparsity: 77.7670%\n",
      "total_backward_count 372020 real_backward_count 35656   9.584%\n",
      "fc layer 3 self.abs_max_out: 608.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.357438/  1.523546, val:  82.50%, val_best:  86.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8917%\n",
      "layer   2  Sparsity: 72.5320%\n",
      "layer   3  Sparsity: 77.9057%\n",
      "total_backward_count 376915 real_backward_count 35851   9.512%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.351471/  1.528447, val:  84.17%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 45.39 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.9032%\n",
      "layer   2  Sparsity: 72.3016%\n",
      "layer   3  Sparsity: 77.7742%\n",
      "total_backward_count 381810 real_backward_count 36040   9.439%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.359909/  1.536631, val:  88.75%, val_best:  88.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8504%\n",
      "layer   2  Sparsity: 72.2813%\n",
      "layer   3  Sparsity: 77.5710%\n",
      "total_backward_count 386705 real_backward_count 36228   9.368%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.357581/  1.515314, val:  84.17%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8860%\n",
      "layer   2  Sparsity: 72.6618%\n",
      "layer   3  Sparsity: 77.6062%\n",
      "total_backward_count 391600 real_backward_count 36374   9.289%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.347147/  1.502636, val:  85.83%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.43 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8665%\n",
      "layer   2  Sparsity: 72.2977%\n",
      "layer   3  Sparsity: 77.2661%\n",
      "total_backward_count 396495 real_backward_count 36543   9.217%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.347537/  1.539723, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8784%\n",
      "layer   2  Sparsity: 72.4030%\n",
      "layer   3  Sparsity: 77.3586%\n",
      "total_backward_count 401390 real_backward_count 36689   9.140%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.347566/  1.517565, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8501%\n",
      "layer   2  Sparsity: 72.6371%\n",
      "layer   3  Sparsity: 77.4272%\n",
      "total_backward_count 406285 real_backward_count 36821   9.063%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.329074/  1.510511, val:  84.58%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8629%\n",
      "layer   2  Sparsity: 72.6207%\n",
      "layer   3  Sparsity: 77.2978%\n",
      "total_backward_count 411180 real_backward_count 36965   8.990%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.339928/  1.538679, val:  86.25%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8267%\n",
      "layer   2  Sparsity: 72.1529%\n",
      "layer   3  Sparsity: 77.1092%\n",
      "total_backward_count 416075 real_backward_count 37116   8.921%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.337834/  1.526626, val:  85.83%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8899%\n",
      "layer   2  Sparsity: 72.6098%\n",
      "layer   3  Sparsity: 77.0866%\n",
      "total_backward_count 420970 real_backward_count 37283   8.856%\n",
      "fc layer 3 self.abs_max_out: 614.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.325056/  1.495758, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.9045%\n",
      "layer   2  Sparsity: 72.7050%\n",
      "layer   3  Sparsity: 76.9996%\n",
      "total_backward_count 425865 real_backward_count 37459   8.796%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.313815/  1.512397, val:  80.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.35 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.9119%\n",
      "layer   2  Sparsity: 72.3677%\n",
      "layer   3  Sparsity: 77.2411%\n",
      "total_backward_count 430760 real_backward_count 37643   8.739%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.302983/  1.487399, val:  85.00%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8433%\n",
      "layer   2  Sparsity: 72.2827%\n",
      "layer   3  Sparsity: 77.6011%\n",
      "total_backward_count 435655 real_backward_count 37810   8.679%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.306802/  1.492617, val:  82.92%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8404%\n",
      "layer   2  Sparsity: 72.4210%\n",
      "layer   3  Sparsity: 77.6595%\n",
      "total_backward_count 440550 real_backward_count 37956   8.616%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.302301/  1.488263, val:  85.42%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8924%\n",
      "layer   2  Sparsity: 72.2275%\n",
      "layer   3  Sparsity: 77.9610%\n",
      "total_backward_count 445445 real_backward_count 38086   8.550%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.298110/  1.481120, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.30 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8641%\n",
      "layer   2  Sparsity: 72.1240%\n",
      "layer   3  Sparsity: 78.3149%\n",
      "total_backward_count 450340 real_backward_count 38235   8.490%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.284767/  1.482459, val:  82.92%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8146%\n",
      "layer   2  Sparsity: 72.5749%\n",
      "layer   3  Sparsity: 78.4050%\n",
      "total_backward_count 455235 real_backward_count 38360   8.426%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.279665/  1.485473, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8677%\n",
      "layer   2  Sparsity: 72.3947%\n",
      "layer   3  Sparsity: 78.3757%\n",
      "total_backward_count 460130 real_backward_count 38472   8.361%\n",
      "fc layer 1 self.abs_max_out: 4351.0\n",
      "fc layer 3 self.abs_max_out: 619.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.282125/  1.488257, val:  85.00%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8234%\n",
      "layer   2  Sparsity: 72.1886%\n",
      "layer   3  Sparsity: 78.3442%\n",
      "total_backward_count 465025 real_backward_count 38586   8.298%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.276790/  1.503053, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.19 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8831%\n",
      "layer   2  Sparsity: 72.1682%\n",
      "layer   3  Sparsity: 78.2570%\n",
      "total_backward_count 469920 real_backward_count 38707   8.237%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.278889/  1.486541, val:  85.00%, val_best:  88.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.54 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8758%\n",
      "layer   2  Sparsity: 72.0232%\n",
      "layer   3  Sparsity: 78.1851%\n",
      "total_backward_count 474815 real_backward_count 38821   8.176%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.275965/  1.496477, val:  82.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.9146%\n",
      "layer   2  Sparsity: 72.1212%\n",
      "layer   3  Sparsity: 78.2705%\n",
      "total_backward_count 479710 real_backward_count 38946   8.119%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.280851/  1.488348, val:  82.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.31 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8788%\n",
      "layer   2  Sparsity: 72.5395%\n",
      "layer   3  Sparsity: 78.0194%\n",
      "total_backward_count 484605 real_backward_count 39080   8.064%\n",
      "fc layer 1 self.abs_max_out: 4361.0\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.265204/  1.459956, val:  86.67%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.35 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8845%\n",
      "layer   2  Sparsity: 72.2623%\n",
      "layer   3  Sparsity: 77.6637%\n",
      "total_backward_count 489500 real_backward_count 39230   8.014%\n",
      "fc layer 2 self.abs_max_out: 1974.0\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.254119/  1.477252, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8460%\n",
      "layer   2  Sparsity: 72.2418%\n",
      "layer   3  Sparsity: 77.5527%\n",
      "total_backward_count 494395 real_backward_count 39347   7.959%\n",
      "lif layer 1 self.abs_max_v: 7001.0\n",
      "fc layer 2 self.abs_max_out: 1991.0\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.260262/  1.474229, val:  83.33%, val_best:  88.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8549%\n",
      "layer   2  Sparsity: 72.5349%\n",
      "layer   3  Sparsity: 77.8172%\n",
      "total_backward_count 499290 real_backward_count 39482   7.908%\n",
      "lif layer 1 self.abs_max_v: 7275.0\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.265504/  1.479188, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 45.06 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8554%\n",
      "layer   2  Sparsity: 72.4731%\n",
      "layer   3  Sparsity: 77.6920%\n",
      "total_backward_count 504185 real_backward_count 39624   7.859%\n",
      "fc layer 3 self.abs_max_out: 624.0\n",
      "fc layer 3 self.abs_max_out: 642.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.262616/  1.498580, val:  78.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.45 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8394%\n",
      "layer   2  Sparsity: 72.2054%\n",
      "layer   3  Sparsity: 78.0597%\n",
      "total_backward_count 509080 real_backward_count 39768   7.812%\n",
      "fc layer 2 self.abs_max_out: 2006.0\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.267095/  1.472872, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.33 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.8572%\n",
      "layer   2  Sparsity: 72.3286%\n",
      "layer   3  Sparsity: 77.6211%\n",
      "total_backward_count 513975 real_backward_count 39908   7.765%\n",
      "fc layer 1 self.abs_max_out: 4442.0\n",
      "lif layer 1 self.abs_max_v: 7498.5\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.259186/  1.484296, val:  83.33%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.29 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8398%\n",
      "layer   2  Sparsity: 72.3715%\n",
      "layer   3  Sparsity: 77.7433%\n",
      "total_backward_count 518870 real_backward_count 40043   7.717%\n",
      "fc layer 1 self.abs_max_out: 4468.0\n",
      "lif layer 1 self.abs_max_v: 7536.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.265970/  1.483102, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.47 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.8595%\n",
      "layer   2  Sparsity: 72.2608%\n",
      "layer   3  Sparsity: 77.9017%\n",
      "total_backward_count 523765 real_backward_count 40146   7.665%\n",
      "fc layer 3 self.abs_max_out: 655.0\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.261320/  1.455361, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.33 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.9072%\n",
      "layer   2  Sparsity: 72.1969%\n",
      "layer   3  Sparsity: 77.9746%\n",
      "total_backward_count 528660 real_backward_count 40262   7.616%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.250712/  1.457152, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8416%\n",
      "layer   2  Sparsity: 72.1478%\n",
      "layer   3  Sparsity: 78.0824%\n",
      "total_backward_count 533555 real_backward_count 40352   7.563%\n",
      "fc layer 3 self.abs_max_out: 659.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.250106/  1.460374, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8736%\n",
      "layer   2  Sparsity: 72.2436%\n",
      "layer   3  Sparsity: 77.9587%\n",
      "total_backward_count 538450 real_backward_count 40438   7.510%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.257110/  1.462246, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8588%\n",
      "layer   2  Sparsity: 72.3844%\n",
      "layer   3  Sparsity: 78.0696%\n",
      "total_backward_count 543345 real_backward_count 40533   7.460%\n",
      "fc layer 3 self.abs_max_out: 671.0\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.251637/  1.446344, val:  84.58%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8693%\n",
      "layer   2  Sparsity: 72.4216%\n",
      "layer   3  Sparsity: 78.2205%\n",
      "total_backward_count 548240 real_backward_count 40632   7.411%\n",
      "fc layer 3 self.abs_max_out: 676.0\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.241542/  1.439222, val:  85.00%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8683%\n",
      "layer   2  Sparsity: 72.2938%\n",
      "layer   3  Sparsity: 78.1101%\n",
      "total_backward_count 553135 real_backward_count 40730   7.363%\n",
      "fc layer 2 self.abs_max_out: 2009.0\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.244440/  1.433277, val:  84.58%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8899%\n",
      "layer   2  Sparsity: 72.1098%\n",
      "layer   3  Sparsity: 77.7045%\n",
      "total_backward_count 558030 real_backward_count 40835   7.318%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.228470/  1.433789, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8554%\n",
      "layer   2  Sparsity: 72.0937%\n",
      "layer   3  Sparsity: 77.6981%\n",
      "total_backward_count 562925 real_backward_count 40931   7.271%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.230361/  1.433591, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8525%\n",
      "layer   2  Sparsity: 72.0810%\n",
      "layer   3  Sparsity: 77.6426%\n",
      "total_backward_count 567820 real_backward_count 41026   7.225%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.237548/  1.442490, val:  84.58%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.05 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 88.9155%\n",
      "layer   2  Sparsity: 71.7663%\n",
      "layer   3  Sparsity: 77.5481%\n",
      "total_backward_count 572715 real_backward_count 41104   7.177%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.238663/  1.420201, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8725%\n",
      "layer   2  Sparsity: 71.8364%\n",
      "layer   3  Sparsity: 77.7356%\n",
      "total_backward_count 577610 real_backward_count 41217   7.136%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.230638/  1.431071, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.36 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8551%\n",
      "layer   2  Sparsity: 71.8322%\n",
      "layer   3  Sparsity: 77.6527%\n",
      "total_backward_count 582505 real_backward_count 41322   7.094%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.244280/  1.455174, val:  87.92%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.90 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8921%\n",
      "layer   2  Sparsity: 71.9541%\n",
      "layer   3  Sparsity: 77.8551%\n",
      "total_backward_count 587400 real_backward_count 41427   7.053%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.244744/  1.439300, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8458%\n",
      "layer   2  Sparsity: 72.0840%\n",
      "layer   3  Sparsity: 77.8578%\n",
      "total_backward_count 592295 real_backward_count 41507   7.008%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.236988/  1.447171, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.23 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8343%\n",
      "layer   2  Sparsity: 72.1134%\n",
      "layer   3  Sparsity: 77.9610%\n",
      "total_backward_count 597190 real_backward_count 41602   6.966%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.231099/  1.421736, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8755%\n",
      "layer   2  Sparsity: 72.1684%\n",
      "layer   3  Sparsity: 78.1870%\n",
      "total_backward_count 602085 real_backward_count 41688   6.924%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.227915/  1.428667, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8819%\n",
      "layer   2  Sparsity: 72.0943%\n",
      "layer   3  Sparsity: 78.0605%\n",
      "total_backward_count 606980 real_backward_count 41768   6.881%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.223110/  1.421296, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.7995%\n",
      "layer   2  Sparsity: 71.7573%\n",
      "layer   3  Sparsity: 77.9430%\n",
      "total_backward_count 611875 real_backward_count 41849   6.839%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.221644/  1.422420, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.71 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8186%\n",
      "layer   2  Sparsity: 71.7135%\n",
      "layer   3  Sparsity: 77.7290%\n",
      "total_backward_count 616770 real_backward_count 41931   6.798%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.208209/  1.420262, val:  83.33%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8489%\n",
      "layer   2  Sparsity: 71.9949%\n",
      "layer   3  Sparsity: 77.9639%\n",
      "total_backward_count 621665 real_backward_count 42011   6.758%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.206484/  1.432425, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.09 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8653%\n",
      "layer   2  Sparsity: 71.9066%\n",
      "layer   3  Sparsity: 77.9810%\n",
      "total_backward_count 626560 real_backward_count 42084   6.717%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.211863/  1.424428, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8650%\n",
      "layer   2  Sparsity: 71.8469%\n",
      "layer   3  Sparsity: 77.9970%\n",
      "total_backward_count 631455 real_backward_count 42153   6.676%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.217408/  1.418993, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8589%\n",
      "layer   2  Sparsity: 72.1071%\n",
      "layer   3  Sparsity: 78.1024%\n",
      "total_backward_count 636350 real_backward_count 42245   6.639%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.219955/  1.429623, val:  83.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.71 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8669%\n",
      "layer   2  Sparsity: 72.2104%\n",
      "layer   3  Sparsity: 77.8906%\n",
      "total_backward_count 641245 real_backward_count 42336   6.602%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.215811/  1.416182, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.28 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8929%\n",
      "layer   2  Sparsity: 72.3578%\n",
      "layer   3  Sparsity: 77.8929%\n",
      "total_backward_count 646140 real_backward_count 42406   6.563%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.211101/  1.418848, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.07 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 88.8528%\n",
      "layer   2  Sparsity: 72.2061%\n",
      "layer   3  Sparsity: 78.1422%\n",
      "total_backward_count 651035 real_backward_count 42488   6.526%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.207909/  1.428209, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.71 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.9000%\n",
      "layer   2  Sparsity: 71.9974%\n",
      "layer   3  Sparsity: 77.9792%\n",
      "total_backward_count 655930 real_backward_count 42549   6.487%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.216564/  1.423875, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.39 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8680%\n",
      "layer   2  Sparsity: 71.9643%\n",
      "layer   3  Sparsity: 77.9070%\n",
      "total_backward_count 660825 real_backward_count 42645   6.453%\n",
      "fc layer 2 self.abs_max_out: 2041.0\n",
      "lif layer 2 self.abs_max_v: 3466.5\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.210990/  1.417269, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8526%\n",
      "layer   2  Sparsity: 72.0653%\n",
      "layer   3  Sparsity: 78.1130%\n",
      "total_backward_count 665720 real_backward_count 42731   6.419%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.203169/  1.419202, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 43.95 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 88.8654%\n",
      "layer   2  Sparsity: 72.1745%\n",
      "layer   3  Sparsity: 78.0535%\n",
      "total_backward_count 670615 real_backward_count 42803   6.383%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.208354/  1.420830, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8402%\n",
      "layer   2  Sparsity: 72.1952%\n",
      "layer   3  Sparsity: 78.1016%\n",
      "total_backward_count 675510 real_backward_count 42896   6.350%\n",
      "fc layer 1 self.abs_max_out: 4505.0\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.218341/  1.433326, val:  84.58%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8594%\n",
      "layer   2  Sparsity: 71.9888%\n",
      "layer   3  Sparsity: 78.3363%\n",
      "total_backward_count 680405 real_backward_count 42985   6.318%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.221818/  1.430608, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.33 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8733%\n",
      "layer   2  Sparsity: 72.0235%\n",
      "layer   3  Sparsity: 78.2332%\n",
      "total_backward_count 685300 real_backward_count 43050   6.282%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.217197/  1.437050, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8719%\n",
      "layer   2  Sparsity: 72.2267%\n",
      "layer   3  Sparsity: 78.4422%\n",
      "total_backward_count 690195 real_backward_count 43125   6.248%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.220805/  1.429646, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.17 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8507%\n",
      "layer   2  Sparsity: 72.1084%\n",
      "layer   3  Sparsity: 78.1927%\n",
      "total_backward_count 695090 real_backward_count 43203   6.215%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.213941/  1.418090, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.70 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8505%\n",
      "layer   2  Sparsity: 72.1856%\n",
      "layer   3  Sparsity: 78.2436%\n",
      "total_backward_count 699985 real_backward_count 43260   6.180%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.205166/  1.424946, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8651%\n",
      "layer   2  Sparsity: 72.2753%\n",
      "layer   3  Sparsity: 78.2544%\n",
      "total_backward_count 704880 real_backward_count 43326   6.147%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.211960/  1.427096, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.9177%\n",
      "layer   2  Sparsity: 71.9058%\n",
      "layer   3  Sparsity: 78.1057%\n",
      "total_backward_count 709775 real_backward_count 43389   6.113%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.203402/  1.421921, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.05 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8903%\n",
      "layer   2  Sparsity: 71.9885%\n",
      "layer   3  Sparsity: 78.2226%\n",
      "total_backward_count 714670 real_backward_count 43464   6.082%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.204134/  1.420173, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.21 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8330%\n",
      "layer   2  Sparsity: 72.0743%\n",
      "layer   3  Sparsity: 78.4780%\n",
      "total_backward_count 719565 real_backward_count 43538   6.051%\n",
      "fc layer 1 self.abs_max_out: 4507.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.204271/  1.439815, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.70 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8384%\n",
      "layer   2  Sparsity: 72.3821%\n",
      "layer   3  Sparsity: 78.6238%\n",
      "total_backward_count 724460 real_backward_count 43598   6.018%\n",
      "lif layer 2 self.abs_max_v: 3496.0\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.201822/  1.433578, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8154%\n",
      "layer   2  Sparsity: 72.3591%\n",
      "layer   3  Sparsity: 78.6029%\n",
      "total_backward_count 729355 real_backward_count 43647   5.984%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.195510/  1.432090, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.31 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8546%\n",
      "layer   2  Sparsity: 72.1576%\n",
      "layer   3  Sparsity: 78.5027%\n",
      "total_backward_count 734250 real_backward_count 43712   5.953%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.198692/  1.433613, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.16 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8559%\n",
      "layer   2  Sparsity: 72.2636%\n",
      "layer   3  Sparsity: 78.6427%\n",
      "total_backward_count 739145 real_backward_count 43777   5.923%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.192172/  1.419174, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8694%\n",
      "layer   2  Sparsity: 72.2920%\n",
      "layer   3  Sparsity: 78.8930%\n",
      "total_backward_count 744040 real_backward_count 43841   5.892%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.182435/  1.408102, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8722%\n",
      "layer   2  Sparsity: 72.1164%\n",
      "layer   3  Sparsity: 78.7554%\n",
      "total_backward_count 748935 real_backward_count 43883   5.859%\n",
      "fc layer 1 self.abs_max_out: 4519.0\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.181324/  1.411635, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.17 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8937%\n",
      "layer   2  Sparsity: 72.1237%\n",
      "layer   3  Sparsity: 78.3443%\n",
      "total_backward_count 753830 real_backward_count 43945   5.830%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.181453/  1.417611, val:  84.17%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8184%\n",
      "layer   2  Sparsity: 72.0177%\n",
      "layer   3  Sparsity: 78.5247%\n",
      "total_backward_count 758725 real_backward_count 44010   5.801%\n",
      "lif layer 2 self.abs_max_v: 3519.0\n",
      "lif layer 2 self.abs_max_v: 3606.5\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.175580/  1.418705, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.47 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8856%\n",
      "layer   2  Sparsity: 72.1607%\n",
      "layer   3  Sparsity: 78.6088%\n",
      "total_backward_count 763620 real_backward_count 44069   5.771%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.172010/  1.428008, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8573%\n",
      "layer   2  Sparsity: 72.0493%\n",
      "layer   3  Sparsity: 78.4950%\n",
      "total_backward_count 768515 real_backward_count 44125   5.742%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.188896/  1.419913, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8612%\n",
      "layer   2  Sparsity: 72.4921%\n",
      "layer   3  Sparsity: 78.3578%\n",
      "total_backward_count 773410 real_backward_count 44187   5.713%\n",
      "fc layer 3 self.abs_max_out: 683.0\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.188815/  1.408478, val:  84.17%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8592%\n",
      "layer   2  Sparsity: 72.4158%\n",
      "layer   3  Sparsity: 78.5925%\n",
      "total_backward_count 778305 real_backward_count 44251   5.686%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.180365/  1.405636, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8435%\n",
      "layer   2  Sparsity: 72.2219%\n",
      "layer   3  Sparsity: 78.7258%\n",
      "total_backward_count 783200 real_backward_count 44299   5.656%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.186474/  1.405754, val:  85.00%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.9394%\n",
      "layer   2  Sparsity: 72.2367%\n",
      "layer   3  Sparsity: 78.5760%\n",
      "total_backward_count 788095 real_backward_count 44371   5.630%\n",
      "lif layer 1 self.abs_max_v: 7618.0\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.178916/  1.388755, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.09 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8288%\n",
      "layer   2  Sparsity: 72.0149%\n",
      "layer   3  Sparsity: 78.5270%\n",
      "total_backward_count 792990 real_backward_count 44424   5.602%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.170671/  1.398269, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8600%\n",
      "layer   2  Sparsity: 72.0901%\n",
      "layer   3  Sparsity: 78.3420%\n",
      "total_backward_count 797885 real_backward_count 44468   5.573%\n",
      "lif layer 1 self.abs_max_v: 7699.5\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.167780/  1.386896, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.9106%\n",
      "layer   2  Sparsity: 72.0705%\n",
      "layer   3  Sparsity: 78.1132%\n",
      "total_backward_count 802780 real_backward_count 44535   5.548%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.155602/  1.393928, val:  81.25%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.45 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8253%\n",
      "layer   2  Sparsity: 72.1226%\n",
      "layer   3  Sparsity: 78.0323%\n",
      "total_backward_count 807675 real_backward_count 44599   5.522%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.153873/  1.376634, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8552%\n",
      "layer   2  Sparsity: 72.0376%\n",
      "layer   3  Sparsity: 78.0608%\n",
      "total_backward_count 812570 real_backward_count 44650   5.495%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.159176/  1.390901, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.9076%\n",
      "layer   2  Sparsity: 72.0619%\n",
      "layer   3  Sparsity: 78.1407%\n",
      "total_backward_count 817465 real_backward_count 44722   5.471%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.149973/  1.385691, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8907%\n",
      "layer   2  Sparsity: 72.1234%\n",
      "layer   3  Sparsity: 77.9798%\n",
      "total_backward_count 822360 real_backward_count 44774   5.445%\n",
      "fc layer 1 self.abs_max_out: 4671.0\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.161047/  1.396268, val:  84.17%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 43.91 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 88.8238%\n",
      "layer   2  Sparsity: 72.0143%\n",
      "layer   3  Sparsity: 77.8927%\n",
      "total_backward_count 827255 real_backward_count 44837   5.420%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.157358/  1.393965, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8839%\n",
      "layer   2  Sparsity: 71.9504%\n",
      "layer   3  Sparsity: 77.9234%\n",
      "total_backward_count 832150 real_backward_count 44903   5.396%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.148498/  1.393800, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.7795%\n",
      "layer   2  Sparsity: 71.8874%\n",
      "layer   3  Sparsity: 78.1222%\n",
      "total_backward_count 837045 real_backward_count 44975   5.373%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.155067/  1.383498, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.07 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 88.8609%\n",
      "layer   2  Sparsity: 71.9781%\n",
      "layer   3  Sparsity: 78.5721%\n",
      "total_backward_count 841940 real_backward_count 45029   5.348%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.149254/  1.375109, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8533%\n",
      "layer   2  Sparsity: 72.0788%\n",
      "layer   3  Sparsity: 78.8241%\n",
      "total_backward_count 846835 real_backward_count 45084   5.324%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.138925/  1.366444, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8653%\n",
      "layer   2  Sparsity: 72.0542%\n",
      "layer   3  Sparsity: 78.7571%\n",
      "total_backward_count 851730 real_backward_count 45150   5.301%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.131943/  1.358112, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8844%\n",
      "layer   2  Sparsity: 72.1507%\n",
      "layer   3  Sparsity: 78.6783%\n",
      "total_backward_count 856625 real_backward_count 45209   5.278%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.128129/  1.365732, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8893%\n",
      "layer   2  Sparsity: 72.2878%\n",
      "layer   3  Sparsity: 78.9235%\n",
      "total_backward_count 861520 real_backward_count 45255   5.253%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.130646/  1.357991, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.9205%\n",
      "layer   2  Sparsity: 72.2046%\n",
      "layer   3  Sparsity: 78.9327%\n",
      "total_backward_count 866415 real_backward_count 45298   5.228%\n",
      "lif layer 1 self.abs_max_v: 7728.0\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.137222/  1.367307, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8411%\n",
      "layer   2  Sparsity: 72.1533%\n",
      "layer   3  Sparsity: 78.9184%\n",
      "total_backward_count 871310 real_backward_count 45335   5.203%\n",
      "fc layer 3 self.abs_max_out: 702.0\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.134215/  1.369958, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8369%\n",
      "layer   2  Sparsity: 72.0401%\n",
      "layer   3  Sparsity: 78.7197%\n",
      "total_backward_count 876205 real_backward_count 45377   5.179%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.140582/  1.372076, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 43.85 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 88.8910%\n",
      "layer   2  Sparsity: 72.1568%\n",
      "layer   3  Sparsity: 78.5683%\n",
      "total_backward_count 881100 real_backward_count 45413   5.154%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.142210/  1.364902, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8480%\n",
      "layer   2  Sparsity: 72.2597%\n",
      "layer   3  Sparsity: 78.5674%\n",
      "total_backward_count 885995 real_backward_count 45450   5.130%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.143949/  1.373199, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.21 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8035%\n",
      "layer   2  Sparsity: 72.1572%\n",
      "layer   3  Sparsity: 78.4593%\n",
      "total_backward_count 890890 real_backward_count 45483   5.105%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.141570/  1.368616, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8373%\n",
      "layer   2  Sparsity: 72.1237%\n",
      "layer   3  Sparsity: 78.4332%\n",
      "total_backward_count 895785 real_backward_count 45511   5.081%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.141695/  1.372640, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8959%\n",
      "layer   2  Sparsity: 72.0441%\n",
      "layer   3  Sparsity: 78.3699%\n",
      "total_backward_count 900680 real_backward_count 45537   5.056%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.148741/  1.366950, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8702%\n",
      "layer   2  Sparsity: 71.9470%\n",
      "layer   3  Sparsity: 78.5036%\n",
      "total_backward_count 905575 real_backward_count 45601   5.036%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.144175/  1.383941, val:  85.00%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.71 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8088%\n",
      "layer   2  Sparsity: 72.0962%\n",
      "layer   3  Sparsity: 78.6394%\n",
      "total_backward_count 910470 real_backward_count 45643   5.013%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.142378/  1.377227, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.10 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8874%\n",
      "layer   2  Sparsity: 71.9383%\n",
      "layer   3  Sparsity: 78.8544%\n",
      "total_backward_count 915365 real_backward_count 45700   4.993%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.132622/  1.373415, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.29 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8775%\n",
      "layer   2  Sparsity: 71.8155%\n",
      "layer   3  Sparsity: 78.9990%\n",
      "total_backward_count 920260 real_backward_count 45753   4.972%\n",
      "fc layer 2 self.abs_max_out: 2112.0\n",
      "fc layer 1 self.abs_max_out: 4701.0\n",
      "lif layer 1 self.abs_max_v: 7992.0\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.131776/  1.371232, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.43 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.8535%\n",
      "layer   2  Sparsity: 71.8302%\n",
      "layer   3  Sparsity: 78.7526%\n",
      "total_backward_count 925155 real_backward_count 45806   4.951%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.128390/  1.386871, val:  84.58%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8437%\n",
      "layer   2  Sparsity: 71.8733%\n",
      "layer   3  Sparsity: 78.7194%\n",
      "total_backward_count 930050 real_backward_count 45865   4.931%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.135036/  1.372704, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.52 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.8464%\n",
      "layer   2  Sparsity: 71.8562%\n",
      "layer   3  Sparsity: 78.6259%\n",
      "total_backward_count 934945 real_backward_count 45904   4.910%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.134343/  1.380337, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.39 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8463%\n",
      "layer   2  Sparsity: 72.0115%\n",
      "layer   3  Sparsity: 78.7430%\n",
      "total_backward_count 939840 real_backward_count 45951   4.889%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.127447/  1.374238, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.03 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8605%\n",
      "layer   2  Sparsity: 72.1855%\n",
      "layer   3  Sparsity: 78.7729%\n",
      "total_backward_count 944735 real_backward_count 45981   4.867%\n",
      "fc layer 3 self.abs_max_out: 717.0\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.125451/  1.360355, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8641%\n",
      "layer   2  Sparsity: 72.2874%\n",
      "layer   3  Sparsity: 78.8261%\n",
      "total_backward_count 949630 real_backward_count 46015   4.846%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.130544/  1.378135, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8705%\n",
      "layer   2  Sparsity: 72.3229%\n",
      "layer   3  Sparsity: 78.9059%\n",
      "total_backward_count 954525 real_backward_count 46050   4.824%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.129235/  1.372643, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8267%\n",
      "layer   2  Sparsity: 72.2923%\n",
      "layer   3  Sparsity: 79.0081%\n",
      "total_backward_count 959420 real_backward_count 46094   4.804%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.132125/  1.377082, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8760%\n",
      "layer   2  Sparsity: 72.3459%\n",
      "layer   3  Sparsity: 78.9874%\n",
      "total_backward_count 964315 real_backward_count 46139   4.785%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.131379/  1.376426, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8955%\n",
      "layer   2  Sparsity: 72.2925%\n",
      "layer   3  Sparsity: 78.9120%\n",
      "total_backward_count 969210 real_backward_count 46172   4.764%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.120502/  1.351923, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8483%\n",
      "layer   2  Sparsity: 72.2902%\n",
      "layer   3  Sparsity: 78.9425%\n",
      "total_backward_count 974105 real_backward_count 46213   4.744%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.111118/  1.343800, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.24 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8678%\n",
      "layer   2  Sparsity: 72.3315%\n",
      "layer   3  Sparsity: 78.7875%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12bd8e011084ae3ad030a8ddc18c482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.11112</td></tr><tr><td>val_acc_best</td><td>0.8875</td></tr><tr><td>val_acc_now</td><td>0.84583</td></tr><tr><td>val_loss</td><td>1.3438</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-sweep-86</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zwclchig' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zwclchig</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_224343-zwclchig/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ax6887zj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 50000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_011315-ax6887zj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ax6887zj' target=\"_blank\">worldly-sweep-89</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ax6887zj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ax6887zj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_011324_646', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 15, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 50000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = dac77cc348b2d880ae59906e26f08f17\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=15, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=15, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 139\n",
      "fc layer 1 self.abs_max_out: 232.0\n",
      "lif layer 1 self.abs_max_v: 232.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 202.0\n",
      "lif layer 2 self.abs_max_v: 202.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 62.0\n",
      "fc layer 1 self.abs_max_out: 261.0\n",
      "lif layer 1 self.abs_max_v: 274.0\n",
      "fc layer 2 self.abs_max_out: 292.0\n",
      "lif layer 2 self.abs_max_v: 328.0\n",
      "fc layer 1 self.abs_max_out: 275.0\n",
      "lif layer 1 self.abs_max_v: 345.0\n",
      "fc layer 2 self.abs_max_out: 363.0\n",
      "lif layer 2 self.abs_max_v: 408.5\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "fc layer 1 self.abs_max_out: 336.0\n",
      "lif layer 1 self.abs_max_v: 375.5\n",
      "lif layer 2 self.abs_max_v: 416.0\n",
      "fc layer 3 self.abs_max_out: 144.0\n",
      "smallest_now_T updated: 125\n",
      "lif layer 1 self.abs_max_v: 414.5\n",
      "fc layer 1 self.abs_max_out: 451.0\n",
      "lif layer 1 self.abs_max_v: 505.5\n",
      "lif layer 2 self.abs_max_v: 465.5\n",
      "fc layer 1 self.abs_max_out: 456.0\n",
      "lif layer 1 self.abs_max_v: 538.0\n",
      "fc layer 2 self.abs_max_out: 420.0\n",
      "lif layer 2 self.abs_max_v: 653.0\n",
      "smallest_now_T updated: 94\n",
      "fc layer 1 self.abs_max_out: 508.0\n",
      "fc layer 1 self.abs_max_out: 530.0\n",
      "fc layer 2 self.abs_max_out: 456.0\n",
      "fc layer 3 self.abs_max_out: 161.0\n",
      "lif layer 1 self.abs_max_v: 567.5\n",
      "lif layer 1 self.abs_max_v: 628.0\n",
      "fc layer 2 self.abs_max_out: 469.0\n",
      "lif layer 2 self.abs_max_v: 727.5\n",
      "lif layer 1 self.abs_max_v: 677.0\n",
      "lif layer 1 self.abs_max_v: 686.5\n",
      "fc layer 2 self.abs_max_out: 505.0\n",
      "lif layer 1 self.abs_max_v: 738.5\n",
      "fc layer 3 self.abs_max_out: 230.0\n",
      "fc layer 1 self.abs_max_out: 623.0\n",
      "lif layer 1 self.abs_max_v: 760.5\n",
      "fc layer 1 self.abs_max_out: 657.0\n",
      "fc layer 2 self.abs_max_out: 540.0\n",
      "lif layer 2 self.abs_max_v: 841.5\n",
      "smallest_now_T updated: 79\n",
      "fc layer 1 self.abs_max_out: 721.0\n",
      "lif layer 1 self.abs_max_v: 913.0\n",
      "fc layer 2 self.abs_max_out: 628.0\n",
      "lif layer 1 self.abs_max_v: 1015.5\n",
      "fc layer 1 self.abs_max_out: 728.0\n",
      "smallest_now_T updated: 73\n",
      "fc layer 1 self.abs_max_out: 732.0\n",
      "fc layer 1 self.abs_max_out: 808.0\n",
      "fc layer 2 self.abs_max_out: 636.0\n",
      "smallest_now_T updated: 65\n",
      "lif layer 2 self.abs_max_v: 849.0\n",
      "fc layer 1 self.abs_max_out: 932.0\n",
      "lif layer 2 self.abs_max_v: 863.5\n",
      "fc layer 3 self.abs_max_out: 254.0\n",
      "lif layer 1 self.abs_max_v: 1020.5\n",
      "fc layer 3 self.abs_max_out: 262.0\n",
      "lif layer 2 self.abs_max_v: 864.5\n",
      "fc layer 1 self.abs_max_out: 991.0\n",
      "fc layer 1 self.abs_max_out: 1062.0\n",
      "lif layer 1 self.abs_max_v: 1062.0\n",
      "smallest_now_T updated: 56\n",
      "smallest_now_T updated: 43\n",
      "fc layer 3 self.abs_max_out: 263.0\n",
      "fc layer 3 self.abs_max_out: 265.0\n",
      "fc layer 3 self.abs_max_out: 273.0\n",
      "fc layer 3 self.abs_max_out: 283.0\n",
      "lif layer 2 self.abs_max_v: 894.0\n",
      "lif layer 1 self.abs_max_v: 1066.0\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "fc layer 3 self.abs_max_out: 349.0\n",
      "lif layer 2 self.abs_max_v: 914.0\n",
      "lif layer 1 self.abs_max_v: 1167.5\n",
      "lif layer 1 self.abs_max_v: 1194.0\n",
      "lif layer 2 self.abs_max_v: 945.5\n",
      "lif layer 1 self.abs_max_v: 1322.5\n",
      "fc layer 2 self.abs_max_out: 658.0\n",
      "lif layer 1 self.abs_max_v: 1417.0\n",
      "fc layer 1 self.abs_max_out: 1074.0\n",
      "lif layer 2 self.abs_max_v: 991.0\n",
      "fc layer 1 self.abs_max_out: 1079.0\n",
      "fc layer 1 self.abs_max_out: 1131.0\n",
      "fc layer 2 self.abs_max_out: 686.0\n",
      "fc layer 1 self.abs_max_out: 1134.0\n",
      "fc layer 1 self.abs_max_out: 1170.0\n",
      "lif layer 1 self.abs_max_v: 1420.0\n",
      "fc layer 2 self.abs_max_out: 692.0\n",
      "fc layer 1 self.abs_max_out: 1184.0\n",
      "lif layer 1 self.abs_max_v: 1434.5\n",
      "lif layer 1 self.abs_max_v: 1472.5\n",
      "smallest_now_T_val updated: 129\n",
      "smallest_now_T_val updated: 106\n",
      "smallest_now_T_val updated: 104\n",
      "smallest_now_T_val updated: 102\n",
      "smallest_now_T_val updated: 85\n",
      "smallest_now_T_val updated: 29\n",
      "fc layer 1 self.abs_max_out: 1205.0\n",
      "lif layer 1 self.abs_max_v: 1540.5\n",
      "lif layer 1 self.abs_max_v: 1629.0\n",
      "fc layer 1 self.abs_max_out: 1216.0\n",
      "fc layer 2 self.abs_max_out: 713.0\n",
      "fc layer 2 self.abs_max_out: 730.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.986257/  2.037317, val:  37.50%, val_best:  37.50%, tr:  74.67%, tr_best:  74.67%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8491%\n",
      "layer   2  Sparsity: 83.2150%\n",
      "layer   3  Sparsity: 80.9649%\n",
      "total_backward_count 4895 real_backward_count 2057  42.022%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 1659.0\n",
      "fc layer 1 self.abs_max_out: 1239.0\n",
      "fc layer 1 self.abs_max_out: 1268.0\n",
      "lif layer 1 self.abs_max_v: 1669.0\n",
      "fc layer 1 self.abs_max_out: 1342.0\n",
      "lif layer 1 self.abs_max_v: 1706.5\n",
      "lif layer 1 self.abs_max_v: 1775.5\n",
      "lif layer 1 self.abs_max_v: 1901.0\n",
      "lif layer 1 self.abs_max_v: 1984.5\n",
      "lif layer 2 self.abs_max_v: 996.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.914846/  2.019793, val:  40.42%, val_best:  40.42%, tr:  89.79%, tr_best:  89.79%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8379%\n",
      "layer   2  Sparsity: 84.4317%\n",
      "layer   3  Sparsity: 79.8913%\n",
      "total_backward_count 9790 real_backward_count 3434  35.077%\n",
      "fc layer 2 self.abs_max_out: 738.0\n",
      "lif layer 2 self.abs_max_v: 1068.5\n",
      "fc layer 1 self.abs_max_out: 1361.0\n",
      "lif layer 1 self.abs_max_v: 2072.5\n",
      "fc layer 3 self.abs_max_out: 362.0\n",
      "fc layer 3 self.abs_max_out: 370.0\n",
      "fc layer 2 self.abs_max_out: 741.0\n",
      "fc layer 2 self.abs_max_out: 742.0\n",
      "fc layer 3 self.abs_max_out: 376.0\n",
      "fc layer 2 self.abs_max_out: 758.0\n",
      "fc layer 1 self.abs_max_out: 1374.0\n",
      "fc layer 1 self.abs_max_out: 1442.0\n",
      "fc layer 1 self.abs_max_out: 1532.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.891145/  2.004252, val:  46.67%, val_best:  46.67%, tr:  88.87%, tr_best:  89.79%, epoch time: 44.26 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.9140%\n",
      "layer   2  Sparsity: 84.9277%\n",
      "layer   3  Sparsity: 79.7974%\n",
      "total_backward_count 14685 real_backward_count 4824  32.850%\n",
      "fc layer 3 self.abs_max_out: 393.0\n",
      "fc layer 2 self.abs_max_out: 764.0\n",
      "fc layer 2 self.abs_max_out: 786.0\n",
      "fc layer 2 self.abs_max_out: 797.0\n",
      "fc layer 1 self.abs_max_out: 1646.0\n",
      "fc layer 2 self.abs_max_out: 833.0\n",
      "fc layer 1 self.abs_max_out: 1663.0\n",
      "fc layer 2 self.abs_max_out: 850.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.854901/  1.968770, val:  49.58%, val_best:  49.58%, tr:  91.93%, tr_best:  91.93%, epoch time: 45.16 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8813%\n",
      "layer   2  Sparsity: 84.2171%\n",
      "layer   3  Sparsity: 78.5760%\n",
      "total_backward_count 19580 real_backward_count 6077  31.037%\n",
      "lif layer 1 self.abs_max_v: 2079.5\n",
      "lif layer 1 self.abs_max_v: 2167.0\n",
      "lif layer 1 self.abs_max_v: 2186.0\n",
      "lif layer 1 self.abs_max_v: 2416.0\n",
      "lif layer 1 self.abs_max_v: 2612.0\n",
      "fc layer 1 self.abs_max_out: 1712.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.845304/  1.974341, val:  54.17%, val_best:  54.17%, tr:  92.13%, tr_best:  92.13%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8118%\n",
      "layer   2  Sparsity: 84.6050%\n",
      "layer   3  Sparsity: 79.2473%\n",
      "total_backward_count 24475 real_backward_count 7270  29.704%\n",
      "lif layer 1 self.abs_max_v: 2872.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.824827/  1.958569, val:  50.42%, val_best:  54.17%, tr:  91.73%, tr_best:  92.13%, epoch time: 45.18 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8913%\n",
      "layer   2  Sparsity: 84.7089%\n",
      "layer   3  Sparsity: 79.4153%\n",
      "total_backward_count 29370 real_backward_count 8484  28.887%\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.827402/  1.955647, val:  50.83%, val_best:  54.17%, tr:  93.67%, tr_best:  93.67%, epoch time: 44.36 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8583%\n",
      "layer   2  Sparsity: 83.9646%\n",
      "layer   3  Sparsity: 78.8847%\n",
      "total_backward_count 34265 real_backward_count 9625  28.090%\n",
      "lif layer 1 self.abs_max_v: 3001.0\n",
      "fc layer 1 self.abs_max_out: 1720.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.806326/  1.950978, val:  52.50%, val_best:  54.17%, tr:  92.65%, tr_best:  93.67%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8775%\n",
      "layer   2  Sparsity: 84.6959%\n",
      "layer   3  Sparsity: 78.8546%\n",
      "total_backward_count 39160 real_backward_count 10712  27.354%\n",
      "fc layer 1 self.abs_max_out: 1736.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.819175/  1.928462, val:  56.25%, val_best:  56.25%, tr:  93.05%, tr_best:  93.67%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8470%\n",
      "layer   2  Sparsity: 85.0580%\n",
      "layer   3  Sparsity: 78.5295%\n",
      "total_backward_count 44055 real_backward_count 11829  26.851%\n",
      "lif layer 2 self.abs_max_v: 1117.5\n",
      "fc layer 2 self.abs_max_out: 856.0\n",
      "fc layer 2 self.abs_max_out: 860.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.793260/  1.935825, val:  46.67%, val_best:  56.25%, tr:  94.28%, tr_best:  94.28%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8945%\n",
      "layer   2  Sparsity: 85.1963%\n",
      "layer   3  Sparsity: 78.5755%\n",
      "total_backward_count 48950 real_backward_count 12892  26.337%\n",
      "fc layer 2 self.abs_max_out: 868.0\n",
      "fc layer 1 self.abs_max_out: 1796.0\n",
      "fc layer 2 self.abs_max_out: 869.0\n",
      "lif layer 2 self.abs_max_v: 1138.5\n",
      "fc layer 2 self.abs_max_out: 876.0\n",
      "fc layer 2 self.abs_max_out: 878.0\n",
      "fc layer 2 self.abs_max_out: 946.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.770715/  1.924328, val:  50.83%, val_best:  56.25%, tr:  92.65%, tr_best:  94.28%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8241%\n",
      "layer   2  Sparsity: 84.5332%\n",
      "layer   3  Sparsity: 77.8416%\n",
      "total_backward_count 53845 real_backward_count 13929  25.869%\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.767143/  1.892043, val:  61.25%, val_best:  61.25%, tr:  94.79%, tr_best:  94.79%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8326%\n",
      "layer   2  Sparsity: 84.3845%\n",
      "layer   3  Sparsity: 78.0992%\n",
      "total_backward_count 58740 real_backward_count 14947  25.446%\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.755410/  1.919773, val:  51.25%, val_best:  61.25%, tr:  93.77%, tr_best:  94.79%, epoch time: 45.09 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8258%\n",
      "layer   2  Sparsity: 84.7175%\n",
      "layer   3  Sparsity: 78.8397%\n",
      "total_backward_count 63635 real_backward_count 15952  25.068%\n",
      "fc layer 1 self.abs_max_out: 1834.0\n",
      "lif layer 2 self.abs_max_v: 1153.5\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.774373/  1.920727, val:  47.08%, val_best:  61.25%, tr:  93.05%, tr_best:  94.79%, epoch time: 44.24 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8427%\n",
      "layer   2  Sparsity: 84.8760%\n",
      "layer   3  Sparsity: 80.2555%\n",
      "total_backward_count 68530 real_backward_count 16955  24.741%\n",
      "fc layer 1 self.abs_max_out: 1885.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.788453/  1.915272, val:  59.17%, val_best:  61.25%, tr:  93.26%, tr_best:  94.79%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8979%\n",
      "layer   2  Sparsity: 85.4410%\n",
      "layer   3  Sparsity: 80.8193%\n",
      "total_backward_count 73425 real_backward_count 17984  24.493%\n",
      "fc layer 1 self.abs_max_out: 1895.0\n",
      "fc layer 1 self.abs_max_out: 1966.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.770128/  1.888822, val:  51.25%, val_best:  61.25%, tr:  93.87%, tr_best:  94.79%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8514%\n",
      "layer   2  Sparsity: 84.9797%\n",
      "layer   3  Sparsity: 79.8792%\n",
      "total_backward_count 78320 real_backward_count 18975  24.228%\n",
      "fc layer 3 self.abs_max_out: 442.0\n",
      "lif layer 1 self.abs_max_v: 3005.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.741271/  1.872270, val:  56.25%, val_best:  61.25%, tr:  95.81%, tr_best:  95.81%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8359%\n",
      "layer   2  Sparsity: 84.5128%\n",
      "layer   3  Sparsity: 79.5809%\n",
      "total_backward_count 83215 real_backward_count 19912  23.928%\n",
      "fc layer 3 self.abs_max_out: 452.0\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "lif layer 1 self.abs_max_v: 3085.0\n",
      "fc layer 1 self.abs_max_out: 1987.0\n",
      "lif layer 1 self.abs_max_v: 3113.0\n",
      "lif layer 1 self.abs_max_v: 3326.5\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.737096/  1.859697, val:  65.42%, val_best:  65.42%, tr:  95.91%, tr_best:  95.91%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8467%\n",
      "layer   2  Sparsity: 84.3687%\n",
      "layer   3  Sparsity: 79.4352%\n",
      "total_backward_count 88110 real_backward_count 20840  23.652%\n",
      "fc layer 1 self.abs_max_out: 2005.0\n",
      "fc layer 1 self.abs_max_out: 2203.0\n",
      "lif layer 2 self.abs_max_v: 1188.0\n",
      "lif layer 2 self.abs_max_v: 1237.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.738253/  1.896829, val:  42.08%, val_best:  65.42%, tr:  93.77%, tr_best:  95.91%, epoch time: 45.11 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8874%\n",
      "layer   2  Sparsity: 85.0003%\n",
      "layer   3  Sparsity: 79.3174%\n",
      "total_backward_count 93005 real_backward_count 21809  23.449%\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.727379/  1.888297, val:  49.58%, val_best:  65.42%, tr:  95.51%, tr_best:  95.91%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8833%\n",
      "layer   2  Sparsity: 84.8557%\n",
      "layer   3  Sparsity: 78.7664%\n",
      "total_backward_count 97900 real_backward_count 22681  23.168%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.727805/  1.863744, val:  64.17%, val_best:  65.42%, tr:  96.32%, tr_best:  96.32%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8434%\n",
      "layer   2  Sparsity: 84.8182%\n",
      "layer   3  Sparsity: 79.0581%\n",
      "total_backward_count 102795 real_backward_count 23531  22.891%\n",
      "fc layer 2 self.abs_max_out: 970.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.727393/  1.873339, val:  57.08%, val_best:  65.42%, tr:  95.51%, tr_best:  96.32%, epoch time: 44.11 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8668%\n",
      "layer   2  Sparsity: 84.8735%\n",
      "layer   3  Sparsity: 80.0642%\n",
      "total_backward_count 107690 real_backward_count 24451  22.705%\n",
      "fc layer 3 self.abs_max_out: 469.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.734500/  1.834550, val:  60.42%, val_best:  65.42%, tr:  96.02%, tr_best:  96.32%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8557%\n",
      "layer   2  Sparsity: 84.4541%\n",
      "layer   3  Sparsity: 80.0018%\n",
      "total_backward_count 112585 real_backward_count 25321  22.491%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.724982/  1.855070, val:  66.25%, val_best:  66.25%, tr:  95.91%, tr_best:  96.32%, epoch time: 44.12 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8237%\n",
      "layer   2  Sparsity: 84.5978%\n",
      "layer   3  Sparsity: 80.0052%\n",
      "total_backward_count 117480 real_backward_count 26181  22.285%\n",
      "lif layer 1 self.abs_max_v: 3442.5\n",
      "fc layer 1 self.abs_max_out: 2256.0\n",
      "lif layer 1 self.abs_max_v: 3958.5\n",
      "fc layer 2 self.abs_max_out: 985.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.707967/  1.852961, val:  66.67%, val_best:  66.67%, tr:  95.40%, tr_best:  96.32%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8199%\n",
      "layer   2  Sparsity: 84.8053%\n",
      "layer   3  Sparsity: 79.3702%\n",
      "total_backward_count 122375 real_backward_count 27037  22.094%\n",
      "lif layer 2 self.abs_max_v: 1283.0\n",
      "fc layer 2 self.abs_max_out: 999.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.698471/  1.830098, val:  73.75%, val_best:  73.75%, tr:  95.20%, tr_best:  96.32%, epoch time: 41.94 seconds, 0.70 minutes\n",
      "layer   1  Sparsity: 88.8771%\n",
      "layer   2  Sparsity: 85.3709%\n",
      "layer   3  Sparsity: 79.4826%\n",
      "total_backward_count 127270 real_backward_count 27972  21.978%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.704269/  1.845166, val:  68.33%, val_best:  73.75%, tr:  96.32%, tr_best:  96.32%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8770%\n",
      "layer   2  Sparsity: 85.4503%\n",
      "layer   3  Sparsity: 79.5358%\n",
      "total_backward_count 132165 real_backward_count 28797  21.789%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.696880/  1.825583, val:  59.58%, val_best:  73.75%, tr:  96.42%, tr_best:  96.42%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8544%\n",
      "layer   2  Sparsity: 85.2827%\n",
      "layer   3  Sparsity: 79.5037%\n",
      "total_backward_count 137060 real_backward_count 29611  21.604%\n",
      "fc layer 3 self.abs_max_out: 498.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.683763/  1.815989, val:  62.50%, val_best:  73.75%, tr:  96.32%, tr_best:  96.42%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8781%\n",
      "layer   2  Sparsity: 84.9161%\n",
      "layer   3  Sparsity: 79.5195%\n",
      "total_backward_count 141955 real_backward_count 30401  21.416%\n",
      "fc layer 2 self.abs_max_out: 1000.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.686695/  1.820524, val:  57.08%, val_best:  73.75%, tr:  96.22%, tr_best:  96.42%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8778%\n",
      "layer   2  Sparsity: 84.7491%\n",
      "layer   3  Sparsity: 79.4620%\n",
      "total_backward_count 146850 real_backward_count 31191  21.240%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.689843/  1.814463, val:  67.08%, val_best:  73.75%, tr:  97.14%, tr_best:  97.14%, epoch time: 44.54 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8190%\n",
      "layer   2  Sparsity: 84.1903%\n",
      "layer   3  Sparsity: 79.0092%\n",
      "total_backward_count 151745 real_backward_count 31997  21.086%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.668364/  1.803102, val:  73.75%, val_best:  73.75%, tr:  96.73%, tr_best:  97.14%, epoch time: 44.31 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8897%\n",
      "layer   2  Sparsity: 85.0397%\n",
      "layer   3  Sparsity: 79.1311%\n",
      "total_backward_count 156640 real_backward_count 32800  20.940%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.676677/  1.821515, val:  60.83%, val_best:  73.75%, tr:  97.24%, tr_best:  97.24%, epoch time: 44.59 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8318%\n",
      "layer   2  Sparsity: 85.1529%\n",
      "layer   3  Sparsity: 79.9602%\n",
      "total_backward_count 161535 real_backward_count 33568  20.781%\n",
      "fc layer 1 self.abs_max_out: 2340.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.680056/  1.788165, val:  72.08%, val_best:  73.75%, tr:  97.55%, tr_best:  97.55%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8477%\n",
      "layer   2  Sparsity: 84.6355%\n",
      "layer   3  Sparsity: 79.7916%\n",
      "total_backward_count 166430 real_backward_count 34353  20.641%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.651850/  1.774886, val:  67.92%, val_best:  73.75%, tr:  96.12%, tr_best:  97.55%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.9184%\n",
      "layer   2  Sparsity: 84.6282%\n",
      "layer   3  Sparsity: 79.2633%\n",
      "total_backward_count 171325 real_backward_count 35165  20.525%\n",
      "fc layer 2 self.abs_max_out: 1024.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.657230/  1.805401, val:  72.50%, val_best:  73.75%, tr:  96.63%, tr_best:  97.55%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8496%\n",
      "layer   2  Sparsity: 84.7333%\n",
      "layer   3  Sparsity: 78.7874%\n",
      "total_backward_count 176220 real_backward_count 35904  20.375%\n",
      "fc layer 1 self.abs_max_out: 2374.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.669161/  1.798840, val:  77.08%, val_best:  77.08%, tr:  97.85%, tr_best:  97.85%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.9181%\n",
      "layer   2  Sparsity: 84.7990%\n",
      "layer   3  Sparsity: 79.5966%\n",
      "total_backward_count 181115 real_backward_count 36628  20.224%\n",
      "fc layer 2 self.abs_max_out: 1026.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.670727/  1.824156, val:  64.17%, val_best:  77.08%, tr:  96.42%, tr_best:  97.85%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8904%\n",
      "layer   2  Sparsity: 84.4272%\n",
      "layer   3  Sparsity: 80.0135%\n",
      "total_backward_count 186010 real_backward_count 37327  20.067%\n",
      "fc layer 1 self.abs_max_out: 2404.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.691337/  1.841115, val:  69.58%, val_best:  77.08%, tr:  96.94%, tr_best:  97.85%, epoch time: 45.28 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8601%\n",
      "layer   2  Sparsity: 84.3029%\n",
      "layer   3  Sparsity: 79.5531%\n",
      "total_backward_count 190905 real_backward_count 38109  19.962%\n",
      "fc layer 3 self.abs_max_out: 502.0\n",
      "fc layer 1 self.abs_max_out: 2426.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.700552/  1.848738, val:  70.83%, val_best:  77.08%, tr:  97.34%, tr_best:  97.85%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8740%\n",
      "layer   2  Sparsity: 84.2746%\n",
      "layer   3  Sparsity: 79.6358%\n",
      "total_backward_count 195800 real_backward_count 38794  19.813%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.682961/  1.835171, val:  66.67%, val_best:  77.08%, tr:  98.26%, tr_best:  98.26%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8879%\n",
      "layer   2  Sparsity: 84.4172%\n",
      "layer   3  Sparsity: 79.2582%\n",
      "total_backward_count 200695 real_backward_count 39480  19.672%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.667120/  1.818187, val:  68.75%, val_best:  77.08%, tr:  97.45%, tr_best:  98.26%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8772%\n",
      "layer   2  Sparsity: 84.7859%\n",
      "layer   3  Sparsity: 79.4314%\n",
      "total_backward_count 205590 real_backward_count 40157  19.533%\n",
      "fc layer 1 self.abs_max_out: 2489.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.650159/  1.771257, val:  70.42%, val_best:  77.08%, tr:  97.75%, tr_best:  98.26%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8626%\n",
      "layer   2  Sparsity: 84.7653%\n",
      "layer   3  Sparsity: 79.2820%\n",
      "total_backward_count 210485 real_backward_count 40806  19.387%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.645650/  1.785672, val:  71.67%, val_best:  77.08%, tr:  97.65%, tr_best:  98.26%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8516%\n",
      "layer   2  Sparsity: 84.6732%\n",
      "layer   3  Sparsity: 79.1893%\n",
      "total_backward_count 215380 real_backward_count 41505  19.271%\n",
      "fc layer 3 self.abs_max_out: 510.0\n",
      "fc layer 2 self.abs_max_out: 1089.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.637895/  1.780276, val:  71.25%, val_best:  77.08%, tr:  98.47%, tr_best:  98.47%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.9153%\n",
      "layer   2  Sparsity: 84.3771%\n",
      "layer   3  Sparsity: 79.0062%\n",
      "total_backward_count 220275 real_backward_count 42184  19.151%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.638656/  1.790338, val:  64.17%, val_best:  77.08%, tr:  96.94%, tr_best:  98.47%, epoch time: 44.90 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8325%\n",
      "layer   2  Sparsity: 84.1583%\n",
      "layer   3  Sparsity: 78.8833%\n",
      "total_backward_count 225170 real_backward_count 42900  19.052%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.636210/  1.780804, val:  69.58%, val_best:  77.08%, tr:  97.96%, tr_best:  98.47%, epoch time: 45.18 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8490%\n",
      "layer   2  Sparsity: 84.3174%\n",
      "layer   3  Sparsity: 78.9139%\n",
      "total_backward_count 230065 real_backward_count 43564  18.936%\n",
      "fc layer 2 self.abs_max_out: 1100.0\n",
      "lif layer 2 self.abs_max_v: 1368.5\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.618270/  1.770769, val:  65.00%, val_best:  77.08%, tr:  98.06%, tr_best:  98.47%, epoch time: 45.39 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.8545%\n",
      "layer   2  Sparsity: 84.2596%\n",
      "layer   3  Sparsity: 78.4355%\n",
      "total_backward_count 234960 real_backward_count 44208  18.815%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.611376/  1.751645, val:  79.17%, val_best:  79.17%, tr:  97.75%, tr_best:  98.47%, epoch time: 45.08 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8681%\n",
      "layer   2  Sparsity: 83.9536%\n",
      "layer   3  Sparsity: 78.5647%\n",
      "total_backward_count 239855 real_backward_count 44848  18.698%\n",
      "fc layer 3 self.abs_max_out: 513.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.600495/  1.742376, val:  68.75%, val_best:  79.17%, tr:  98.26%, tr_best:  98.47%, epoch time: 44.70 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8446%\n",
      "layer   2  Sparsity: 83.8559%\n",
      "layer   3  Sparsity: 78.7827%\n",
      "total_backward_count 244750 real_backward_count 45476  18.581%\n",
      "fc layer 1 self.abs_max_out: 2613.0\n",
      "lif layer 2 self.abs_max_v: 1404.5\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.592819/  1.727063, val:  70.42%, val_best:  79.17%, tr:  96.94%, tr_best:  98.47%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.9149%\n",
      "layer   2  Sparsity: 83.9603%\n",
      "layer   3  Sparsity: 79.2390%\n",
      "total_backward_count 249645 real_backward_count 46113  18.471%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.576695/  1.744437, val:  74.58%, val_best:  79.17%, tr:  97.85%, tr_best:  98.47%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8718%\n",
      "layer   2  Sparsity: 83.9859%\n",
      "layer   3  Sparsity: 79.5064%\n",
      "total_backward_count 254540 real_backward_count 46715  18.353%\n",
      "lif layer 2 self.abs_max_v: 1442.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.600053/  1.718532, val:  80.00%, val_best:  80.00%, tr:  98.57%, tr_best:  98.57%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8335%\n",
      "layer   2  Sparsity: 84.2259%\n",
      "layer   3  Sparsity: 79.2049%\n",
      "total_backward_count 259435 real_backward_count 47314  18.237%\n",
      "fc layer 2 self.abs_max_out: 1108.0\n",
      "lif layer 2 self.abs_max_v: 1443.5\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.576176/  1.724784, val:  70.00%, val_best:  80.00%, tr:  98.26%, tr_best:  98.57%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8604%\n",
      "layer   2  Sparsity: 84.3290%\n",
      "layer   3  Sparsity: 78.8285%\n",
      "total_backward_count 264330 real_backward_count 47903  18.122%\n",
      "fc layer 3 self.abs_max_out: 515.0\n",
      "fc layer 2 self.abs_max_out: 1122.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.578082/  1.729836, val:  74.58%, val_best:  80.00%, tr:  98.06%, tr_best:  98.57%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8794%\n",
      "layer   2  Sparsity: 84.5258%\n",
      "layer   3  Sparsity: 78.6368%\n",
      "total_backward_count 269225 real_backward_count 48507  18.017%\n",
      "fc layer 3 self.abs_max_out: 516.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.589437/  1.733560, val:  80.83%, val_best:  80.83%, tr:  97.96%, tr_best:  98.57%, epoch time: 45.02 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8278%\n",
      "layer   2  Sparsity: 84.6564%\n",
      "layer   3  Sparsity: 79.0937%\n",
      "total_backward_count 274120 real_backward_count 49126  17.921%\n",
      "fc layer 3 self.abs_max_out: 548.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.582394/  1.747371, val:  72.50%, val_best:  80.83%, tr:  97.65%, tr_best:  98.57%, epoch time: 45.08 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8651%\n",
      "layer   2  Sparsity: 84.3847%\n",
      "layer   3  Sparsity: 78.7419%\n",
      "total_backward_count 279015 real_backward_count 49747  17.830%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.590353/  1.756111, val:  76.67%, val_best:  80.83%, tr:  98.16%, tr_best:  98.57%, epoch time: 45.04 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8060%\n",
      "layer   2  Sparsity: 84.2937%\n",
      "layer   3  Sparsity: 78.8719%\n",
      "total_backward_count 283910 real_backward_count 50292  17.714%\n",
      "fc layer 1 self.abs_max_out: 2667.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.590969/  1.723724, val:  82.92%, val_best:  82.92%, tr:  98.26%, tr_best:  98.57%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8818%\n",
      "layer   2  Sparsity: 84.4737%\n",
      "layer   3  Sparsity: 78.9030%\n",
      "total_backward_count 288805 real_backward_count 50843  17.605%\n",
      "fc layer 1 self.abs_max_out: 2696.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.568949/  1.741492, val:  67.08%, val_best:  82.92%, tr:  98.47%, tr_best:  98.57%, epoch time: 44.89 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8368%\n",
      "layer   2  Sparsity: 84.4887%\n",
      "layer   3  Sparsity: 78.8657%\n",
      "total_backward_count 293700 real_backward_count 51410  17.504%\n",
      "fc layer 1 self.abs_max_out: 2736.0\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.557460/  1.718874, val:  71.67%, val_best:  82.92%, tr:  98.16%, tr_best:  98.57%, epoch time: 45.17 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8464%\n",
      "layer   2  Sparsity: 84.5267%\n",
      "layer   3  Sparsity: 78.3400%\n",
      "total_backward_count 298595 real_backward_count 51984  17.410%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.551783/  1.706334, val:  83.33%, val_best:  83.33%, tr:  98.88%, tr_best:  98.88%, epoch time: 45.04 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8696%\n",
      "layer   2  Sparsity: 84.5530%\n",
      "layer   3  Sparsity: 78.7867%\n",
      "total_backward_count 303490 real_backward_count 52515  17.304%\n",
      "lif layer 1 self.abs_max_v: 4143.5\n",
      "fc layer 2 self.abs_max_out: 1149.0\n",
      "fc layer 1 self.abs_max_out: 2802.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.556157/  1.709575, val:  79.17%, val_best:  83.33%, tr:  98.47%, tr_best:  98.88%, epoch time: 44.97 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8453%\n",
      "layer   2  Sparsity: 84.5579%\n",
      "layer   3  Sparsity: 79.1701%\n",
      "total_backward_count 308385 real_backward_count 53062  17.206%\n",
      "lif layer 1 self.abs_max_v: 4226.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.558862/  1.706229, val:  81.67%, val_best:  83.33%, tr:  99.08%, tr_best:  99.08%, epoch time: 45.49 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.7822%\n",
      "layer   2  Sparsity: 84.5077%\n",
      "layer   3  Sparsity: 79.0943%\n",
      "total_backward_count 313280 real_backward_count 53587  17.105%\n",
      "fc layer 1 self.abs_max_out: 2821.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.548644/  1.710656, val:  80.83%, val_best:  83.33%, tr:  98.67%, tr_best:  99.08%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8521%\n",
      "layer   2  Sparsity: 84.1486%\n",
      "layer   3  Sparsity: 78.9858%\n",
      "total_backward_count 318175 real_backward_count 54098  17.003%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.539952/  1.716069, val:  75.83%, val_best:  83.33%, tr:  97.96%, tr_best:  99.08%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8733%\n",
      "layer   2  Sparsity: 84.1745%\n",
      "layer   3  Sparsity: 78.9950%\n",
      "total_backward_count 323070 real_backward_count 54615  16.905%\n",
      "lif layer 2 self.abs_max_v: 1504.5\n",
      "fc layer 1 self.abs_max_out: 2891.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.547058/  1.731406, val:  78.33%, val_best:  83.33%, tr:  99.28%, tr_best:  99.28%, epoch time: 44.43 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.9290%\n",
      "layer   2  Sparsity: 84.4105%\n",
      "layer   3  Sparsity: 79.0050%\n",
      "total_backward_count 327965 real_backward_count 55102  16.801%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.541829/  1.714824, val:  77.50%, val_best:  83.33%, tr:  98.57%, tr_best:  99.28%, epoch time: 45.06 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8769%\n",
      "layer   2  Sparsity: 84.3783%\n",
      "layer   3  Sparsity: 78.5473%\n",
      "total_backward_count 332860 real_backward_count 55635  16.714%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.528215/  1.687444, val:  78.75%, val_best:  83.33%, tr:  98.88%, tr_best:  99.28%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8589%\n",
      "layer   2  Sparsity: 84.6263%\n",
      "layer   3  Sparsity: 78.5954%\n",
      "total_backward_count 337755 real_backward_count 56108  16.612%\n",
      "lif layer 1 self.abs_max_v: 4311.5\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.536685/  1.704109, val:  79.58%, val_best:  83.33%, tr:  98.77%, tr_best:  99.28%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8243%\n",
      "layer   2  Sparsity: 84.5123%\n",
      "layer   3  Sparsity: 78.5187%\n",
      "total_backward_count 342650 real_backward_count 56618  16.524%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.550556/  1.706151, val:  75.83%, val_best:  83.33%, tr:  97.96%, tr_best:  99.28%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8597%\n",
      "layer   2  Sparsity: 84.4582%\n",
      "layer   3  Sparsity: 78.7267%\n",
      "total_backward_count 347545 real_backward_count 57175  16.451%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.554668/  1.712602, val:  77.50%, val_best:  83.33%, tr:  98.47%, tr_best:  99.28%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8668%\n",
      "layer   2  Sparsity: 84.1239%\n",
      "layer   3  Sparsity: 78.6030%\n",
      "total_backward_count 352440 real_backward_count 57682  16.366%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.549271/  1.690982, val:  74.17%, val_best:  83.33%, tr:  98.67%, tr_best:  99.28%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8844%\n",
      "layer   2  Sparsity: 84.1460%\n",
      "layer   3  Sparsity: 77.9800%\n",
      "total_backward_count 357335 real_backward_count 58206  16.289%\n",
      "fc layer 3 self.abs_max_out: 549.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.534243/  1.708455, val:  74.17%, val_best:  83.33%, tr:  98.98%, tr_best:  99.28%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8231%\n",
      "layer   2  Sparsity: 84.3678%\n",
      "layer   3  Sparsity: 77.9895%\n",
      "total_backward_count 362230 real_backward_count 58695  16.204%\n",
      "lif layer 1 self.abs_max_v: 4562.5\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.525765/  1.683450, val:  79.58%, val_best:  83.33%, tr:  99.39%, tr_best:  99.39%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8892%\n",
      "layer   2  Sparsity: 84.4655%\n",
      "layer   3  Sparsity: 78.3847%\n",
      "total_backward_count 367125 real_backward_count 59173  16.118%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.516682/  1.680379, val:  82.92%, val_best:  83.33%, tr:  98.88%, tr_best:  99.39%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8237%\n",
      "layer   2  Sparsity: 84.7030%\n",
      "layer   3  Sparsity: 78.2495%\n",
      "total_backward_count 372020 real_backward_count 59651  16.034%\n",
      "lif layer 1 self.abs_max_v: 4808.5\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.511985/  1.672846, val:  76.67%, val_best:  83.33%, tr:  98.98%, tr_best:  99.39%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8917%\n",
      "layer   2  Sparsity: 84.9800%\n",
      "layer   3  Sparsity: 78.3163%\n",
      "total_backward_count 376915 real_backward_count 60132  15.954%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.509825/  1.687668, val:  79.58%, val_best:  83.33%, tr:  99.28%, tr_best:  99.39%, epoch time: 45.50 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.9032%\n",
      "layer   2  Sparsity: 84.9090%\n",
      "layer   3  Sparsity: 77.9051%\n",
      "total_backward_count 381810 real_backward_count 60590  15.869%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.519323/  1.687408, val:  80.00%, val_best:  83.33%, tr:  98.57%, tr_best:  99.39%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8504%\n",
      "layer   2  Sparsity: 84.5712%\n",
      "layer   3  Sparsity: 77.8690%\n",
      "total_backward_count 386705 real_backward_count 61063  15.791%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.508590/  1.678754, val:  82.50%, val_best:  83.33%, tr:  99.39%, tr_best:  99.39%, epoch time: 44.84 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8860%\n",
      "layer   2  Sparsity: 84.4673%\n",
      "layer   3  Sparsity: 78.2815%\n",
      "total_backward_count 391600 real_backward_count 61523  15.711%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.517219/  1.673945, val:  83.33%, val_best:  83.33%, tr:  98.16%, tr_best:  99.39%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8665%\n",
      "layer   2  Sparsity: 84.2323%\n",
      "layer   3  Sparsity: 78.9254%\n",
      "total_backward_count 396495 real_backward_count 61990  15.634%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.534069/  1.689880, val:  77.08%, val_best:  83.33%, tr:  98.98%, tr_best:  99.39%, epoch time: 44.78 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8784%\n",
      "layer   2  Sparsity: 84.1297%\n",
      "layer   3  Sparsity: 79.1614%\n",
      "total_backward_count 401390 real_backward_count 62450  15.558%\n",
      "fc layer 1 self.abs_max_out: 3115.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.522062/  1.695097, val:  74.58%, val_best:  83.33%, tr:  98.67%, tr_best:  99.39%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8501%\n",
      "layer   2  Sparsity: 84.3219%\n",
      "layer   3  Sparsity: 79.0733%\n",
      "total_backward_count 406285 real_backward_count 62926  15.488%\n",
      "fc layer 3 self.abs_max_out: 558.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.527152/  1.691312, val:  80.42%, val_best:  83.33%, tr:  99.08%, tr_best:  99.39%, epoch time: 45.19 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8629%\n",
      "layer   2  Sparsity: 84.4035%\n",
      "layer   3  Sparsity: 79.7214%\n",
      "total_backward_count 411180 real_backward_count 63381  15.414%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.531057/  1.707768, val:  83.75%, val_best:  83.75%, tr:  98.98%, tr_best:  99.39%, epoch time: 44.91 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8267%\n",
      "layer   2  Sparsity: 84.3325%\n",
      "layer   3  Sparsity: 80.1659%\n",
      "total_backward_count 416075 real_backward_count 63835  15.342%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.556013/  1.713789, val:  82.92%, val_best:  83.75%, tr:  98.47%, tr_best:  99.39%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8899%\n",
      "layer   2  Sparsity: 84.5680%\n",
      "layer   3  Sparsity: 80.1947%\n",
      "total_backward_count 420970 real_backward_count 64306  15.276%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.542268/  1.665576, val:  84.58%, val_best:  84.58%, tr:  98.88%, tr_best:  99.39%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.9045%\n",
      "layer   2  Sparsity: 84.6244%\n",
      "layer   3  Sparsity: 80.0525%\n",
      "total_backward_count 425865 real_backward_count 64757  15.206%\n",
      "fc layer 1 self.abs_max_out: 3183.0\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.528853/  1.693918, val:  80.00%, val_best:  84.58%, tr:  98.98%, tr_best:  99.39%, epoch time: 45.35 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.9119%\n",
      "layer   2  Sparsity: 84.9216%\n",
      "layer   3  Sparsity: 79.9828%\n",
      "total_backward_count 430760 real_backward_count 65205  15.137%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.527110/  1.660178, val:  77.08%, val_best:  84.58%, tr:  98.98%, tr_best:  99.39%, epoch time: 45.13 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8433%\n",
      "layer   2  Sparsity: 84.9168%\n",
      "layer   3  Sparsity: 79.6341%\n",
      "total_backward_count 435655 real_backward_count 65647  15.069%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.532587/  1.693848, val:  83.33%, val_best:  84.58%, tr:  99.39%, tr_best:  99.39%, epoch time: 45.37 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.8404%\n",
      "layer   2  Sparsity: 84.7800%\n",
      "layer   3  Sparsity: 79.7922%\n",
      "total_backward_count 440550 real_backward_count 66082  15.000%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.523460/  1.684112, val:  83.75%, val_best:  84.58%, tr:  99.28%, tr_best:  99.39%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8924%\n",
      "layer   2  Sparsity: 84.8241%\n",
      "layer   3  Sparsity: 79.6914%\n",
      "total_backward_count 445445 real_backward_count 66469  14.922%\n",
      "fc layer 1 self.abs_max_out: 3410.0\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.526104/  1.692838, val:  84.58%, val_best:  84.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 45.11 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8641%\n",
      "layer   2  Sparsity: 84.8048%\n",
      "layer   3  Sparsity: 79.6381%\n",
      "total_backward_count 450340 real_backward_count 66849  14.844%\n",
      "fc layer 2 self.abs_max_out: 1168.0\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.529018/  1.689075, val:  78.75%, val_best:  84.58%, tr:  99.18%, tr_best:  99.49%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8146%\n",
      "layer   2  Sparsity: 84.8070%\n",
      "layer   3  Sparsity: 79.7285%\n",
      "total_backward_count 455235 real_backward_count 67258  14.774%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.508460/  1.661472, val:  82.08%, val_best:  84.58%, tr:  98.77%, tr_best:  99.49%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8677%\n",
      "layer   2  Sparsity: 84.6240%\n",
      "layer   3  Sparsity: 79.6112%\n",
      "total_backward_count 460130 real_backward_count 67678  14.708%\n",
      "fc layer 1 self.abs_max_out: 3503.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.508734/  1.677466, val:  75.83%, val_best:  84.58%, tr:  99.18%, tr_best:  99.49%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8234%\n",
      "layer   2  Sparsity: 84.6101%\n",
      "layer   3  Sparsity: 78.9144%\n",
      "total_backward_count 465025 real_backward_count 68066  14.637%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.509700/  1.680427, val:  80.00%, val_best:  84.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 44.89 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8831%\n",
      "layer   2  Sparsity: 84.6866%\n",
      "layer   3  Sparsity: 78.8038%\n",
      "total_backward_count 469920 real_backward_count 68469  14.570%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.499881/  1.678426, val:  77.08%, val_best:  84.58%, tr:  99.28%, tr_best:  99.49%, epoch time: 44.78 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8758%\n",
      "layer   2  Sparsity: 84.5961%\n",
      "layer   3  Sparsity: 78.6668%\n",
      "total_backward_count 474815 real_backward_count 68827  14.496%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.493492/  1.675209, val:  83.33%, val_best:  84.58%, tr:  98.88%, tr_best:  99.49%, epoch time: 44.43 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.9146%\n",
      "layer   2  Sparsity: 84.6723%\n",
      "layer   3  Sparsity: 78.6642%\n",
      "total_backward_count 479710 real_backward_count 69213  14.428%\n",
      "lif layer 1 self.abs_max_v: 4991.5\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.500425/  1.684783, val:  73.75%, val_best:  84.58%, tr:  98.98%, tr_best:  99.49%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8788%\n",
      "layer   2  Sparsity: 84.8328%\n",
      "layer   3  Sparsity: 78.6890%\n",
      "total_backward_count 484605 real_backward_count 69611  14.364%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.506018/  1.679036, val:  86.25%, val_best:  86.25%, tr:  99.28%, tr_best:  99.49%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8845%\n",
      "layer   2  Sparsity: 84.7553%\n",
      "layer   3  Sparsity: 79.1728%\n",
      "total_backward_count 489500 real_backward_count 69997  14.300%\n",
      "fc layer 1 self.abs_max_out: 3540.0\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.500376/  1.638771, val:  85.42%, val_best:  86.25%, tr:  99.49%, tr_best:  99.49%, epoch time: 45.23 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8460%\n",
      "layer   2  Sparsity: 84.5932%\n",
      "layer   3  Sparsity: 79.3158%\n",
      "total_backward_count 494395 real_backward_count 70382  14.236%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.485795/  1.642282, val:  87.08%, val_best:  87.08%, tr:  99.28%, tr_best:  99.49%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8549%\n",
      "layer   2  Sparsity: 84.8586%\n",
      "layer   3  Sparsity: 79.5992%\n",
      "total_backward_count 499290 real_backward_count 70773  14.175%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.496783/  1.682196, val:  82.92%, val_best:  87.08%, tr:  99.49%, tr_best:  99.49%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8554%\n",
      "layer   2  Sparsity: 84.9071%\n",
      "layer   3  Sparsity: 79.3626%\n",
      "total_backward_count 504185 real_backward_count 71123  14.107%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.509290/  1.691443, val:  75.83%, val_best:  87.08%, tr:  98.98%, tr_best:  99.49%, epoch time: 44.91 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8394%\n",
      "layer   2  Sparsity: 84.9544%\n",
      "layer   3  Sparsity: 79.1655%\n",
      "total_backward_count 509080 real_backward_count 71540  14.053%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.511720/  1.673271, val:  80.83%, val_best:  87.08%, tr:  99.28%, tr_best:  99.49%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8572%\n",
      "layer   2  Sparsity: 84.5602%\n",
      "layer   3  Sparsity: 78.9526%\n",
      "total_backward_count 513975 real_backward_count 71936  13.996%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.493690/  1.655816, val:  79.17%, val_best:  87.08%, tr:  99.28%, tr_best:  99.49%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8398%\n",
      "layer   2  Sparsity: 84.7465%\n",
      "layer   3  Sparsity: 78.5966%\n",
      "total_backward_count 518870 real_backward_count 72293  13.933%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.483548/  1.645702, val:  85.42%, val_best:  87.08%, tr:  98.57%, tr_best:  99.49%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8624%\n",
      "layer   2  Sparsity: 84.9718%\n",
      "layer   3  Sparsity: 78.7386%\n",
      "total_backward_count 523765 real_backward_count 72679  13.876%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.497577/  1.658370, val:  81.67%, val_best:  87.08%, tr:  98.67%, tr_best:  99.49%, epoch time: 45.00 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.9072%\n",
      "layer   2  Sparsity: 84.7928%\n",
      "layer   3  Sparsity: 78.6560%\n",
      "total_backward_count 528660 real_backward_count 73065  13.821%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.492262/  1.647458, val:  84.17%, val_best:  87.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 45.11 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8416%\n",
      "layer   2  Sparsity: 84.9747%\n",
      "layer   3  Sparsity: 78.4823%\n",
      "total_backward_count 533555 real_backward_count 73440  13.764%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.486183/  1.666176, val:  84.17%, val_best:  87.08%, tr:  99.08%, tr_best:  99.59%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8736%\n",
      "layer   2  Sparsity: 85.0082%\n",
      "layer   3  Sparsity: 78.6879%\n",
      "total_backward_count 538450 real_backward_count 73806  13.707%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.483453/  1.650402, val:  85.42%, val_best:  87.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8588%\n",
      "layer   2  Sparsity: 84.9906%\n",
      "layer   3  Sparsity: 78.8106%\n",
      "total_backward_count 543345 real_backward_count 74146  13.646%\n",
      "lif layer 1 self.abs_max_v: 4994.0\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.465233/  1.634233, val:  77.50%, val_best:  87.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8683%\n",
      "layer   2  Sparsity: 85.0029%\n",
      "layer   3  Sparsity: 79.0208%\n",
      "total_backward_count 548240 real_backward_count 74438  13.578%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.466938/  1.640981, val:  86.67%, val_best:  87.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 45.29 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8683%\n",
      "layer   2  Sparsity: 84.8915%\n",
      "layer   3  Sparsity: 79.0129%\n",
      "total_backward_count 553135 real_backward_count 74763  13.516%\n",
      "fc layer 1 self.abs_max_out: 3546.0\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.478553/  1.658929, val:  82.08%, val_best:  87.08%, tr:  99.49%, tr_best:  99.59%, epoch time: 45.04 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8899%\n",
      "layer   2  Sparsity: 84.8826%\n",
      "layer   3  Sparsity: 78.6155%\n",
      "total_backward_count 558030 real_backward_count 75095  13.457%\n",
      "lif layer 1 self.abs_max_v: 5093.5\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.465551/  1.625305, val:  82.08%, val_best:  87.08%, tr:  99.39%, tr_best:  99.59%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8554%\n",
      "layer   2  Sparsity: 84.9920%\n",
      "layer   3  Sparsity: 78.6874%\n",
      "total_backward_count 562925 real_backward_count 75418  13.398%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.442561/  1.627323, val:  81.25%, val_best:  87.08%, tr:  99.39%, tr_best:  99.59%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8525%\n",
      "layer   2  Sparsity: 84.9463%\n",
      "layer   3  Sparsity: 78.9287%\n",
      "total_backward_count 567820 real_backward_count 75750  13.340%\n",
      "lif layer 1 self.abs_max_v: 5326.0\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.458750/  1.616183, val:  80.83%, val_best:  87.08%, tr:  99.39%, tr_best:  99.59%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.9155%\n",
      "layer   2  Sparsity: 84.8241%\n",
      "layer   3  Sparsity: 78.9371%\n",
      "total_backward_count 572715 real_backward_count 76094  13.287%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.450067/  1.603570, val:  83.75%, val_best:  87.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 44.33 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8725%\n",
      "layer   2  Sparsity: 84.7905%\n",
      "layer   3  Sparsity: 79.1070%\n",
      "total_backward_count 577610 real_backward_count 76434  13.233%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.439428/  1.602663, val:  84.17%, val_best:  87.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8551%\n",
      "layer   2  Sparsity: 84.8297%\n",
      "layer   3  Sparsity: 78.9958%\n",
      "total_backward_count 582505 real_backward_count 76755  13.177%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.443010/  1.597022, val:  82.50%, val_best:  87.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 44.33 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8921%\n",
      "layer   2  Sparsity: 84.8115%\n",
      "layer   3  Sparsity: 78.6635%\n",
      "total_backward_count 587400 real_backward_count 77086  13.123%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.432720/  1.619518, val:  82.92%, val_best:  87.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 45.18 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8458%\n",
      "layer   2  Sparsity: 84.8148%\n",
      "layer   3  Sparsity: 78.4630%\n",
      "total_backward_count 592295 real_backward_count 77405  13.069%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.454319/  1.604357, val:  86.25%, val_best:  87.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8343%\n",
      "layer   2  Sparsity: 84.6060%\n",
      "layer   3  Sparsity: 78.4252%\n",
      "total_backward_count 597190 real_backward_count 77782  13.025%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.424504/  1.602597, val:  84.58%, val_best:  87.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 44.91 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8755%\n",
      "layer   2  Sparsity: 84.4124%\n",
      "layer   3  Sparsity: 78.6603%\n",
      "total_backward_count 602085 real_backward_count 78110  12.973%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.437943/  1.599684, val:  82.50%, val_best:  87.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 45.01 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8819%\n",
      "layer   2  Sparsity: 84.4677%\n",
      "layer   3  Sparsity: 78.6122%\n",
      "total_backward_count 606980 real_backward_count 78439  12.923%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.428666/  1.591299, val:  83.75%, val_best:  87.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.7995%\n",
      "layer   2  Sparsity: 84.5266%\n",
      "layer   3  Sparsity: 78.7105%\n",
      "total_backward_count 611875 real_backward_count 78747  12.870%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.431426/  1.603097, val:  84.17%, val_best:  87.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8186%\n",
      "layer   2  Sparsity: 84.4897%\n",
      "layer   3  Sparsity: 78.4578%\n",
      "total_backward_count 616770 real_backward_count 79038  12.815%\n",
      "fc layer 2 self.abs_max_out: 1175.0\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.433281/  1.618978, val:  84.58%, val_best:  87.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8489%\n",
      "layer   2  Sparsity: 84.3429%\n",
      "layer   3  Sparsity: 78.5080%\n",
      "total_backward_count 621665 real_backward_count 79363  12.766%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.432777/  1.607429, val:  77.50%, val_best:  87.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 44.70 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8653%\n",
      "layer   2  Sparsity: 84.5450%\n",
      "layer   3  Sparsity: 78.7792%\n",
      "total_backward_count 626560 real_backward_count 79652  12.713%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.436955/  1.612248, val:  81.67%, val_best:  87.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8650%\n",
      "layer   2  Sparsity: 84.3337%\n",
      "layer   3  Sparsity: 78.5761%\n",
      "total_backward_count 631455 real_backward_count 79969  12.664%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.432451/  1.595727, val:  82.50%, val_best:  87.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8589%\n",
      "layer   2  Sparsity: 84.2760%\n",
      "layer   3  Sparsity: 78.4842%\n",
      "total_backward_count 636350 real_backward_count 80265  12.613%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.428218/  1.592412, val:  80.83%, val_best:  87.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 45.01 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8669%\n",
      "layer   2  Sparsity: 84.2446%\n",
      "layer   3  Sparsity: 78.0940%\n",
      "total_backward_count 641245 real_backward_count 80562  12.563%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.430003/  1.615171, val:  79.58%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8929%\n",
      "layer   2  Sparsity: 84.2581%\n",
      "layer   3  Sparsity: 78.0853%\n",
      "total_backward_count 646140 real_backward_count 80873  12.516%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.433508/  1.627841, val:  83.75%, val_best:  87.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 45.35 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.8528%\n",
      "layer   2  Sparsity: 84.4547%\n",
      "layer   3  Sparsity: 78.2707%\n",
      "total_backward_count 651035 real_backward_count 81196  12.472%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.428100/  1.605208, val:  82.50%, val_best:  87.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.9000%\n",
      "layer   2  Sparsity: 84.5133%\n",
      "layer   3  Sparsity: 78.7181%\n",
      "total_backward_count 655930 real_backward_count 81502  12.425%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.420712/  1.617079, val:  83.75%, val_best:  87.08%, tr:  99.18%, tr_best:  99.90%, epoch time: 45.26 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8680%\n",
      "layer   2  Sparsity: 84.4004%\n",
      "layer   3  Sparsity: 78.5508%\n",
      "total_backward_count 660825 real_backward_count 81814  12.381%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.417139/  1.587667, val:  80.42%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8526%\n",
      "layer   2  Sparsity: 84.3886%\n",
      "layer   3  Sparsity: 78.5206%\n",
      "total_backward_count 665720 real_backward_count 82100  12.333%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.398418/  1.594419, val:  81.25%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8654%\n",
      "layer   2  Sparsity: 84.4028%\n",
      "layer   3  Sparsity: 78.4938%\n",
      "total_backward_count 670615 real_backward_count 82376  12.284%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.402841/  1.596154, val:  83.33%, val_best:  87.08%, tr:  99.28%, tr_best:  99.90%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8402%\n",
      "layer   2  Sparsity: 84.3192%\n",
      "layer   3  Sparsity: 78.4337%\n",
      "total_backward_count 675510 real_backward_count 82652  12.235%\n",
      "fc layer 2 self.abs_max_out: 1189.0\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.392051/  1.574370, val:  86.25%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.71 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8594%\n",
      "layer   2  Sparsity: 84.3955%\n",
      "layer   3  Sparsity: 78.4979%\n",
      "total_backward_count 680405 real_backward_count 82940  12.190%\n",
      "fc layer 3 self.abs_max_out: 572.0\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.398431/  1.577533, val:  79.58%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8733%\n",
      "layer   2  Sparsity: 84.2180%\n",
      "layer   3  Sparsity: 78.3709%\n",
      "total_backward_count 685300 real_backward_count 83219  12.143%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.415837/  1.608204, val:  83.33%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.45 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8719%\n",
      "layer   2  Sparsity: 84.2504%\n",
      "layer   3  Sparsity: 78.4219%\n",
      "total_backward_count 690195 real_backward_count 83478  12.095%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.426343/  1.600086, val:  83.75%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.91 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8507%\n",
      "layer   2  Sparsity: 84.4976%\n",
      "layer   3  Sparsity: 78.4185%\n",
      "total_backward_count 695090 real_backward_count 83774  12.052%\n",
      "lif layer 2 self.abs_max_v: 1534.5\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.410638/  1.580533, val:  84.58%, val_best:  87.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 45.28 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8505%\n",
      "layer   2  Sparsity: 84.4755%\n",
      "layer   3  Sparsity: 78.3951%\n",
      "total_backward_count 699985 real_backward_count 84068  12.010%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.396590/  1.583914, val:  82.92%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 45.01 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8651%\n",
      "layer   2  Sparsity: 84.4887%\n",
      "layer   3  Sparsity: 78.3952%\n",
      "total_backward_count 704880 real_backward_count 84342  11.965%\n",
      "fc layer 3 self.abs_max_out: 577.0\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.400234/  1.578599, val:  81.67%, val_best:  87.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 44.70 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.9177%\n",
      "layer   2  Sparsity: 84.3826%\n",
      "layer   3  Sparsity: 78.1450%\n",
      "total_backward_count 709775 real_backward_count 84607  11.920%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.396015/  1.596408, val:  80.42%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 45.18 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8903%\n",
      "layer   2  Sparsity: 84.5980%\n",
      "layer   3  Sparsity: 78.2450%\n",
      "total_backward_count 714670 real_backward_count 84919  11.882%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.417873/  1.585732, val:  78.33%, val_best:  87.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8330%\n",
      "layer   2  Sparsity: 84.5525%\n",
      "layer   3  Sparsity: 78.3274%\n",
      "total_backward_count 719565 real_backward_count 85227  11.844%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.392651/  1.572162, val:  85.83%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8384%\n",
      "layer   2  Sparsity: 84.4530%\n",
      "layer   3  Sparsity: 77.8680%\n",
      "total_backward_count 724460 real_backward_count 85486  11.800%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.401555/  1.578653, val:  80.42%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8154%\n",
      "layer   2  Sparsity: 84.4759%\n",
      "layer   3  Sparsity: 77.7381%\n",
      "total_backward_count 729355 real_backward_count 85746  11.756%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.395036/  1.575941, val:  82.08%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8546%\n",
      "layer   2  Sparsity: 84.5744%\n",
      "layer   3  Sparsity: 77.6113%\n",
      "total_backward_count 734250 real_backward_count 86012  11.714%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.385995/  1.586525, val:  77.92%, val_best:  87.08%, tr:  99.18%, tr_best:  99.90%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8559%\n",
      "layer   2  Sparsity: 84.5365%\n",
      "layer   3  Sparsity: 77.6350%\n",
      "total_backward_count 739145 real_backward_count 86301  11.676%\n",
      "fc layer 2 self.abs_max_out: 1192.0\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.395525/  1.575123, val:  82.08%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 45.12 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8694%\n",
      "layer   2  Sparsity: 84.3001%\n",
      "layer   3  Sparsity: 77.6121%\n",
      "total_backward_count 744040 real_backward_count 86583  11.637%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.374706/  1.581632, val:  77.92%, val_best:  87.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8722%\n",
      "layer   2  Sparsity: 84.4115%\n",
      "layer   3  Sparsity: 77.6355%\n",
      "total_backward_count 748935 real_backward_count 86843  11.596%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.383318/  1.572290, val:  83.33%, val_best:  87.08%, tr:  99.28%, tr_best:  99.90%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8937%\n",
      "layer   2  Sparsity: 84.4913%\n",
      "layer   3  Sparsity: 77.6959%\n",
      "total_backward_count 753830 real_backward_count 87097  11.554%\n",
      "lif layer 1 self.abs_max_v: 5482.5\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.376836/  1.547833, val:  85.00%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.24 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8184%\n",
      "layer   2  Sparsity: 84.3843%\n",
      "layer   3  Sparsity: 77.6155%\n",
      "total_backward_count 758725 real_backward_count 87347  11.512%\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.359362/  1.554120, val:  81.25%, val_best:  87.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 45.04 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8856%\n",
      "layer   2  Sparsity: 84.3141%\n",
      "layer   3  Sparsity: 77.5719%\n",
      "total_backward_count 763620 real_backward_count 87598  11.471%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.366729/  1.553643, val:  84.17%, val_best:  87.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 44.28 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8573%\n",
      "layer   2  Sparsity: 84.2136%\n",
      "layer   3  Sparsity: 77.6527%\n",
      "total_backward_count 768515 real_backward_count 87873  11.434%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.367353/  1.565332, val:  84.58%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 45.05 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8612%\n",
      "layer   2  Sparsity: 84.0518%\n",
      "layer   3  Sparsity: 77.4681%\n",
      "total_backward_count 773410 real_backward_count 88129  11.395%\n",
      "fc layer 3 self.abs_max_out: 616.0\n",
      "fc layer 2 self.abs_max_out: 1207.0\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.353712/  1.545754, val:  84.58%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.43 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8592%\n",
      "layer   2  Sparsity: 83.9727%\n",
      "layer   3  Sparsity: 77.7354%\n",
      "total_backward_count 778305 real_backward_count 88351  11.352%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.363198/  1.575284, val:  85.00%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 45.13 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8435%\n",
      "layer   2  Sparsity: 84.1107%\n",
      "layer   3  Sparsity: 78.0710%\n",
      "total_backward_count 783200 real_backward_count 88591  11.311%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.369372/  1.554121, val:  82.50%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.70 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.9394%\n",
      "layer   2  Sparsity: 84.1051%\n",
      "layer   3  Sparsity: 78.1725%\n",
      "total_backward_count 788095 real_backward_count 88846  11.274%\n",
      "fc layer 2 self.abs_max_out: 1217.0\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.376654/  1.571301, val:  85.42%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 45.13 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8288%\n",
      "layer   2  Sparsity: 84.1646%\n",
      "layer   3  Sparsity: 78.3989%\n",
      "total_backward_count 792990 real_backward_count 89072  11.232%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.369350/  1.573824, val:  84.17%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8600%\n",
      "layer   2  Sparsity: 84.2148%\n",
      "layer   3  Sparsity: 78.6065%\n",
      "total_backward_count 797885 real_backward_count 89291  11.191%\n",
      "lif layer 1 self.abs_max_v: 5568.5\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.365384/  1.582985, val:  79.17%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.9106%\n",
      "layer   2  Sparsity: 84.2864%\n",
      "layer   3  Sparsity: 78.8815%\n",
      "total_backward_count 802780 real_backward_count 89539  11.154%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.375490/  1.564685, val:  84.58%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.30 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8253%\n",
      "layer   2  Sparsity: 84.1296%\n",
      "layer   3  Sparsity: 78.7694%\n",
      "total_backward_count 807675 real_backward_count 89797  11.118%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.354543/  1.560959, val:  84.17%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8552%\n",
      "layer   2  Sparsity: 84.2314%\n",
      "layer   3  Sparsity: 78.6089%\n",
      "total_backward_count 812570 real_backward_count 90042  11.081%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.356543/  1.568777, val:  80.00%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.9076%\n",
      "layer   2  Sparsity: 84.3449%\n",
      "layer   3  Sparsity: 78.5925%\n",
      "total_backward_count 817465 real_backward_count 90255  11.041%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.366192/  1.562667, val:  82.50%, val_best:  87.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 44.90 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8907%\n",
      "layer   2  Sparsity: 84.2853%\n",
      "layer   3  Sparsity: 78.5195%\n",
      "total_backward_count 822360 real_backward_count 90495  11.004%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.362368/  1.551676, val:  88.33%, val_best:  88.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8238%\n",
      "layer   2  Sparsity: 84.3934%\n",
      "layer   3  Sparsity: 78.7293%\n",
      "total_backward_count 827255 real_backward_count 90723  10.967%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.354945/  1.550821, val:  79.58%, val_best:  88.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8839%\n",
      "layer   2  Sparsity: 84.4477%\n",
      "layer   3  Sparsity: 79.0102%\n",
      "total_backward_count 832150 real_backward_count 90961  10.931%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.347867/  1.554949, val:  87.08%, val_best:  88.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.91 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.7795%\n",
      "layer   2  Sparsity: 84.4418%\n",
      "layer   3  Sparsity: 78.9750%\n",
      "total_backward_count 837045 real_backward_count 91190  10.894%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.340359/  1.532645, val:  83.75%, val_best:  88.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 45.09 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8609%\n",
      "layer   2  Sparsity: 84.6498%\n",
      "layer   3  Sparsity: 79.0184%\n",
      "total_backward_count 841940 real_backward_count 91413  10.857%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.348046/  1.556176, val:  82.50%, val_best:  88.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8533%\n",
      "layer   2  Sparsity: 84.6378%\n",
      "layer   3  Sparsity: 79.4612%\n",
      "total_backward_count 846835 real_backward_count 91646  10.822%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.349732/  1.556450, val:  82.50%, val_best:  88.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 45.06 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8653%\n",
      "layer   2  Sparsity: 84.7180%\n",
      "layer   3  Sparsity: 79.1219%\n",
      "total_backward_count 851730 real_backward_count 91878  10.787%\n",
      "lif layer 2 self.abs_max_v: 1544.5\n",
      "fc layer 3 self.abs_max_out: 634.0\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.345345/  1.554631, val:  80.83%, val_best:  88.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8844%\n",
      "layer   2  Sparsity: 84.6061%\n",
      "layer   3  Sparsity: 78.8204%\n",
      "total_backward_count 856625 real_backward_count 92115  10.753%\n",
      "lif layer 1 self.abs_max_v: 5690.5\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.345118/  1.528260, val:  85.00%, val_best:  88.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 45.14 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8893%\n",
      "layer   2  Sparsity: 84.6228%\n",
      "layer   3  Sparsity: 78.7524%\n",
      "total_backward_count 861520 real_backward_count 92354  10.720%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.334975/  1.536700, val:  84.17%, val_best:  88.33%, tr:  99.39%, tr_best:  99.90%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.9205%\n",
      "layer   2  Sparsity: 84.6012%\n",
      "layer   3  Sparsity: 78.8093%\n",
      "total_backward_count 866415 real_backward_count 92620  10.690%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.334530/  1.562209, val:  80.83%, val_best:  88.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8411%\n",
      "layer   2  Sparsity: 84.5559%\n",
      "layer   3  Sparsity: 78.9801%\n",
      "total_backward_count 871310 real_backward_count 92819  10.653%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.350401/  1.563553, val:  82.08%, val_best:  88.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8369%\n",
      "layer   2  Sparsity: 84.5005%\n",
      "layer   3  Sparsity: 78.9820%\n",
      "total_backward_count 876205 real_backward_count 93028  10.617%\n",
      "fc layer 2 self.abs_max_out: 1221.0\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.347064/  1.557254, val:  85.42%, val_best:  88.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8910%\n",
      "layer   2  Sparsity: 84.5062%\n",
      "layer   3  Sparsity: 78.9588%\n",
      "total_backward_count 881100 real_backward_count 93224  10.580%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.337714/  1.553432, val:  80.00%, val_best:  88.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8480%\n",
      "layer   2  Sparsity: 84.5383%\n",
      "layer   3  Sparsity: 78.7515%\n",
      "total_backward_count 885995 real_backward_count 93437  10.546%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.340925/  1.549140, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.89 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8035%\n",
      "layer   2  Sparsity: 84.8324%\n",
      "layer   3  Sparsity: 78.8386%\n",
      "total_backward_count 890890 real_backward_count 93642  10.511%\n",
      "lif layer 2 self.abs_max_v: 1613.5\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.340316/  1.569883, val:  80.00%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 45.01 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8373%\n",
      "layer   2  Sparsity: 84.8427%\n",
      "layer   3  Sparsity: 78.9367%\n",
      "total_backward_count 895785 real_backward_count 93841  10.476%\n",
      "lif layer 1 self.abs_max_v: 5735.0\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.347960/  1.550024, val:  80.00%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8959%\n",
      "layer   2  Sparsity: 84.7696%\n",
      "layer   3  Sparsity: 78.9943%\n",
      "total_backward_count 900680 real_backward_count 94039  10.441%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.338843/  1.544000, val:  86.25%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 45.25 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8702%\n",
      "layer   2  Sparsity: 84.8511%\n",
      "layer   3  Sparsity: 78.9106%\n",
      "total_backward_count 905575 real_backward_count 94245  10.407%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.331633/  1.542050, val:  82.08%, val_best:  88.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8088%\n",
      "layer   2  Sparsity: 84.9197%\n",
      "layer   3  Sparsity: 78.8659%\n",
      "total_backward_count 910470 real_backward_count 94461  10.375%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.334403/  1.539385, val:  84.17%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 45.01 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8952%\n",
      "layer   2  Sparsity: 84.8956%\n",
      "layer   3  Sparsity: 78.7761%\n",
      "total_backward_count 915365 real_backward_count 94671  10.342%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.343087/  1.561158, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8775%\n",
      "layer   2  Sparsity: 84.8172%\n",
      "layer   3  Sparsity: 78.6655%\n",
      "total_backward_count 920260 real_backward_count 94876  10.310%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.342836/  1.542669, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8535%\n",
      "layer   2  Sparsity: 84.8681%\n",
      "layer   3  Sparsity: 78.6336%\n",
      "total_backward_count 925155 real_backward_count 95073  10.276%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.325592/  1.533408, val:  81.25%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8437%\n",
      "layer   2  Sparsity: 84.8957%\n",
      "layer   3  Sparsity: 78.6523%\n",
      "total_backward_count 930050 real_backward_count 95257  10.242%\n",
      "fc layer 2 self.abs_max_out: 1222.0\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.314485/  1.539433, val:  83.75%, val_best:  88.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 45.75 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.8464%\n",
      "layer   2  Sparsity: 84.9422%\n",
      "layer   3  Sparsity: 78.9767%\n",
      "total_backward_count 934945 real_backward_count 95440  10.208%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.310047/  1.520777, val:  82.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8463%\n",
      "layer   2  Sparsity: 84.9934%\n",
      "layer   3  Sparsity: 78.8952%\n",
      "total_backward_count 939840 real_backward_count 95601  10.172%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.311449/  1.538851, val:  82.92%, val_best:  88.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8605%\n",
      "layer   2  Sparsity: 85.0710%\n",
      "layer   3  Sparsity: 78.8771%\n",
      "total_backward_count 944735 real_backward_count 95798  10.140%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.311453/  1.515255, val:  82.50%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8641%\n",
      "layer   2  Sparsity: 85.1747%\n",
      "layer   3  Sparsity: 78.9567%\n",
      "total_backward_count 949630 real_backward_count 95989  10.108%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.316383/  1.512858, val:  83.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 45.06 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8705%\n",
      "layer   2  Sparsity: 85.1944%\n",
      "layer   3  Sparsity: 79.0619%\n",
      "total_backward_count 954525 real_backward_count 96160  10.074%\n",
      "lif layer 2 self.abs_max_v: 1641.0\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.309415/  1.524558, val:  82.92%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.59 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 88.8267%\n",
      "layer   2  Sparsity: 85.1191%\n",
      "layer   3  Sparsity: 79.2212%\n",
      "total_backward_count 959420 real_backward_count 96332  10.041%\n",
      "lif layer 2 self.abs_max_v: 1659.0\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.321976/  1.538881, val:  85.42%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 45.35 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 88.8760%\n",
      "layer   2  Sparsity: 85.0967%\n",
      "layer   3  Sparsity: 79.4089%\n",
      "total_backward_count 964315 real_backward_count 96501  10.007%\n",
      "lif layer 2 self.abs_max_v: 1806.5\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.316321/  1.519865, val:  84.58%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 45.08 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8955%\n",
      "layer   2  Sparsity: 85.1046%\n",
      "layer   3  Sparsity: 79.1612%\n",
      "total_backward_count 969210 real_backward_count 96684   9.976%\n",
      "fc layer 3 self.abs_max_out: 639.0\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.310960/  1.512673, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.03 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8483%\n",
      "layer   2  Sparsity: 85.0127%\n",
      "layer   3  Sparsity: 78.7382%\n",
      "total_backward_count 974105 real_backward_count 96883   9.946%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.297386/  1.513989, val:  79.58%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 88.8678%\n",
      "layer   2  Sparsity: 85.0911%\n",
      "layer   3  Sparsity: 78.7904%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d4f34634c54b879a0f408e308a3ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>1.29739</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.79583</td></tr><tr><td>val_loss</td><td>1.51399</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worldly-sweep-89</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ax6887zj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ax6887zj</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_011315-ax6887zj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ancreadw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 75000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_034313-ancreadw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ancreadw' target=\"_blank\">rare-sweep-93</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ancreadw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ancreadw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_034323_280', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 75000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 4761a05d236841af8daa3414213005d1\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 91\n",
      "fc layer 1 self.abs_max_out: 273.0\n",
      "lif layer 1 self.abs_max_v: 273.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 157.0\n",
      "lif layer 2 self.abs_max_v: 157.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 22.0\n",
      "fc layer 1 self.abs_max_out: 284.0\n",
      "lif layer 1 self.abs_max_v: 335.5\n",
      "fc layer 2 self.abs_max_out: 410.0\n",
      "lif layer 2 self.abs_max_v: 410.0\n",
      "fc layer 3 self.abs_max_out: 98.0\n",
      "lif layer 1 self.abs_max_v: 366.0\n",
      "lif layer 1 self.abs_max_v: 382.5\n",
      "lif layer 2 self.abs_max_v: 519.5\n",
      "smallest_now_T updated: 82\n",
      "fc layer 1 self.abs_max_out: 400.0\n",
      "lif layer 1 self.abs_max_v: 400.0\n",
      "lif layer 1 self.abs_max_v: 510.0\n",
      "fc layer 1 self.abs_max_out: 555.0\n",
      "lif layer 1 self.abs_max_v: 619.5\n",
      "fc layer 3 self.abs_max_out: 113.0\n",
      "smallest_now_T updated: 61\n",
      "fc layer 1 self.abs_max_out: 587.0\n",
      "fc layer 1 self.abs_max_out: 588.0\n",
      "fc layer 1 self.abs_max_out: 636.0\n",
      "lif layer 1 self.abs_max_v: 636.0\n",
      "lif layer 2 self.abs_max_v: 594.5\n",
      "fc layer 1 self.abs_max_out: 705.0\n",
      "lif layer 1 self.abs_max_v: 705.0\n",
      "lif layer 2 self.abs_max_v: 626.5\n",
      "fc layer 3 self.abs_max_out: 146.0\n",
      "fc layer 2 self.abs_max_out: 474.0\n",
      "lif layer 2 self.abs_max_v: 681.0\n",
      "lif layer 2 self.abs_max_v: 758.5\n",
      "lif layer 1 self.abs_max_v: 816.0\n",
      "lif layer 1 self.abs_max_v: 937.0\n",
      "lif layer 2 self.abs_max_v: 761.5\n",
      "fc layer 3 self.abs_max_out: 177.0\n",
      "fc layer 2 self.abs_max_out: 574.0\n",
      "fc layer 1 self.abs_max_out: 743.0\n",
      "lif layer 1 self.abs_max_v: 949.5\n",
      "lif layer 2 self.abs_max_v: 817.5\n",
      "lif layer 2 self.abs_max_v: 828.0\n",
      "fc layer 3 self.abs_max_out: 217.0\n",
      "fc layer 1 self.abs_max_out: 850.0\n",
      "lif layer 2 self.abs_max_v: 870.0\n",
      "fc layer 2 self.abs_max_out: 668.0\n",
      "lif layer 2 self.abs_max_v: 983.5\n",
      "smallest_now_T updated: 51\n",
      "lif layer 1 self.abs_max_v: 974.5\n",
      "fc layer 1 self.abs_max_out: 883.0\n",
      "smallest_now_T updated: 47\n",
      "fc layer 1 self.abs_max_out: 933.0\n",
      "lif layer 1 self.abs_max_v: 1055.0\n",
      "lif layer 1 self.abs_max_v: 1140.0\n",
      "fc layer 3 self.abs_max_out: 227.0\n",
      "fc layer 1 self.abs_max_out: 962.0\n",
      "lif layer 2 self.abs_max_v: 988.5\n",
      "lif layer 1 self.abs_max_v: 1238.5\n",
      "smallest_now_T updated: 42\n",
      "fc layer 1 self.abs_max_out: 997.0\n",
      "lif layer 1 self.abs_max_v: 1254.0\n",
      "lif layer 1 self.abs_max_v: 1309.0\n",
      "lif layer 2 self.abs_max_v: 995.5\n",
      "fc layer 1 self.abs_max_out: 1071.0\n",
      "lif layer 1 self.abs_max_v: 1362.0\n",
      "lif layer 1 self.abs_max_v: 1451.5\n",
      "fc layer 3 self.abs_max_out: 245.0\n",
      "lif layer 1 self.abs_max_v: 1456.0\n",
      "lif layer 1 self.abs_max_v: 1526.5\n",
      "smallest_now_T updated: 36\n",
      "smallest_now_T updated: 27\n",
      "fc layer 2 self.abs_max_out: 696.0\n",
      "fc layer 3 self.abs_max_out: 321.0\n",
      "fc layer 1 self.abs_max_out: 1100.0\n",
      "fc layer 1 self.abs_max_out: 1128.0\n",
      "lif layer 1 self.abs_max_v: 1608.5\n",
      "lif layer 1 self.abs_max_v: 1648.5\n",
      "lif layer 1 self.abs_max_v: 1711.0\n",
      "lif layer 1 self.abs_max_v: 1711.5\n",
      "lif layer 1 self.abs_max_v: 1720.5\n",
      "lif layer 1 self.abs_max_v: 1779.5\n",
      "fc layer 1 self.abs_max_out: 1198.0\n",
      "fc layer 1 self.abs_max_out: 1378.0\n",
      "lif layer 1 self.abs_max_v: 1814.0\n",
      "lif layer 1 self.abs_max_v: 1834.5\n",
      "lif layer 1 self.abs_max_v: 1849.5\n",
      "lif layer 1 self.abs_max_v: 1902.0\n",
      "lif layer 1 self.abs_max_v: 1966.5\n",
      "lif layer 1 self.abs_max_v: 2042.5\n",
      "fc layer 1 self.abs_max_out: 1420.0\n",
      "lif layer 1 self.abs_max_v: 2118.0\n",
      "smallest_now_T_val updated: 84\n",
      "smallest_now_T_val updated: 69\n",
      "smallest_now_T_val updated: 68\n",
      "smallest_now_T_val updated: 67\n",
      "smallest_now_T_val updated: 55\n",
      "smallest_now_T_val updated: 25\n",
      "lif layer 1 self.abs_max_v: 2132.0\n",
      "lif layer 1 self.abs_max_v: 2173.5\n",
      "lif layer 1 self.abs_max_v: 2226.0\n",
      "fc layer 1 self.abs_max_out: 1449.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.978269/  2.015716, val:  35.42%, val_best:  35.42%, tr:  75.59%, tr_best:  75.59%, epoch time: 45.44 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 85.3396%\n",
      "layer   2  Sparsity: 82.7260%\n",
      "layer   3  Sparsity: 81.6413%\n",
      "total_backward_count 4895 real_backward_count 1936  39.551%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 2316.0\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "fc layer 2 self.abs_max_out: 709.0\n",
      "fc layer 2 self.abs_max_out: 711.0\n",
      "fc layer 2 self.abs_max_out: 724.0\n",
      "fc layer 2 self.abs_max_out: 790.0\n",
      "lif layer 1 self.abs_max_v: 2340.0\n",
      "lif layer 2 self.abs_max_v: 1043.0\n",
      "fc layer 1 self.abs_max_out: 1464.0\n",
      "lif layer 2 self.abs_max_v: 1056.5\n",
      "lif layer 1 self.abs_max_v: 2377.0\n",
      "lif layer 2 self.abs_max_v: 1098.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.896439/  2.009657, val:  41.25%, val_best:  41.25%, tr:  88.36%, tr_best:  88.36%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3716%\n",
      "layer   2  Sparsity: 83.7153%\n",
      "layer   3  Sparsity: 82.3335%\n",
      "total_backward_count 9790 real_backward_count 3268  33.381%\n",
      "fc layer 1 self.abs_max_out: 1517.0\n",
      "lif layer 1 self.abs_max_v: 2446.0\n",
      "fc layer 1 self.abs_max_out: 1556.0\n",
      "lif layer 1 self.abs_max_v: 2611.5\n",
      "fc layer 1 self.abs_max_out: 1585.0\n",
      "lif layer 2 self.abs_max_v: 1122.5\n",
      "fc layer 3 self.abs_max_out: 351.0\n",
      "fc layer 1 self.abs_max_out: 1712.0\n",
      "fc layer 2 self.abs_max_out: 840.0\n",
      "fc layer 3 self.abs_max_out: 362.0\n",
      "lif layer 2 self.abs_max_v: 1144.0\n",
      "lif layer 2 self.abs_max_v: 1150.0\n",
      "lif layer 1 self.abs_max_v: 2668.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.892322/  1.971769, val:  48.33%, val_best:  48.33%, tr:  90.19%, tr_best:  90.19%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.4240%\n",
      "layer   2  Sparsity: 82.9696%\n",
      "layer   3  Sparsity: 82.3486%\n",
      "total_backward_count 14685 real_backward_count 4536  30.889%\n",
      "fc layer 1 self.abs_max_out: 1734.0\n",
      "lif layer 2 self.abs_max_v: 1188.0\n",
      "lif layer 2 self.abs_max_v: 1233.0\n",
      "fc layer 2 self.abs_max_out: 858.0\n",
      "fc layer 2 self.abs_max_out: 905.0\n",
      "fc layer 1 self.abs_max_out: 1937.0\n",
      "lif layer 1 self.abs_max_v: 2768.5\n",
      "fc layer 2 self.abs_max_out: 931.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.868335/  2.010910, val:  48.75%, val_best:  48.75%, tr:  91.93%, tr_best:  91.93%, epoch time: 45.51 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 85.3729%\n",
      "layer   2  Sparsity: 82.7118%\n",
      "layer   3  Sparsity: 82.2801%\n",
      "total_backward_count 19580 real_backward_count 5713  29.178%\n",
      "lif layer 1 self.abs_max_v: 2833.5\n",
      "fc layer 3 self.abs_max_out: 366.0\n",
      "lif layer 1 self.abs_max_v: 2898.5\n",
      "fc layer 1 self.abs_max_out: 1975.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.886097/  1.977106, val:  50.00%, val_best:  50.00%, tr:  91.93%, tr_best:  91.93%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3293%\n",
      "layer   2  Sparsity: 84.6849%\n",
      "layer   3  Sparsity: 83.6972%\n",
      "total_backward_count 24475 real_backward_count 6830  27.906%\n",
      "fc layer 3 self.abs_max_out: 369.0\n",
      "fc layer 3 self.abs_max_out: 377.0\n",
      "lif layer 1 self.abs_max_v: 2975.0\n",
      "lif layer 1 self.abs_max_v: 3065.0\n",
      "fc layer 1 self.abs_max_out: 2029.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.862814/  1.970230, val:  55.42%, val_best:  55.42%, tr:  92.85%, tr_best:  92.85%, epoch time: 45.68 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 85.3704%\n",
      "layer   2  Sparsity: 83.6198%\n",
      "layer   3  Sparsity: 82.9714%\n",
      "total_backward_count 29370 real_backward_count 7936  27.021%\n",
      "lif layer 1 self.abs_max_v: 3114.5\n",
      "lif layer 1 self.abs_max_v: 3126.5\n",
      "lif layer 1 self.abs_max_v: 3270.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.872681/  1.987504, val:  49.58%, val_best:  55.42%, tr:  92.44%, tr_best:  92.85%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3766%\n",
      "layer   2  Sparsity: 83.4337%\n",
      "layer   3  Sparsity: 83.0877%\n",
      "total_backward_count 34265 real_backward_count 9007  26.286%\n",
      "lif layer 1 self.abs_max_v: 3283.5\n",
      "fc layer 1 self.abs_max_out: 2038.0\n",
      "lif layer 1 self.abs_max_v: 3327.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.876014/  1.966992, val:  46.67%, val_best:  55.42%, tr:  94.69%, tr_best:  94.69%, epoch time: 44.39 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3508%\n",
      "layer   2  Sparsity: 83.4153%\n",
      "layer   3  Sparsity: 83.0925%\n",
      "total_backward_count 39160 real_backward_count 9957  25.426%\n",
      "fc layer 1 self.abs_max_out: 2053.0\n",
      "fc layer 1 self.abs_max_out: 2121.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.855593/  1.969811, val:  60.42%, val_best:  60.42%, tr:  94.48%, tr_best:  94.69%, epoch time: 44.33 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.4456%\n",
      "layer   2  Sparsity: 83.3336%\n",
      "layer   3  Sparsity: 83.4002%\n",
      "total_backward_count 44055 real_backward_count 10908  24.760%\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.855855/  1.968122, val:  58.75%, val_best:  60.42%, tr:  93.36%, tr_best:  94.69%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3394%\n",
      "layer   2  Sparsity: 83.7199%\n",
      "layer   3  Sparsity: 83.6548%\n",
      "total_backward_count 48950 real_backward_count 11902  24.315%\n",
      "fc layer 2 self.abs_max_out: 967.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.835653/  1.966469, val:  54.58%, val_best:  60.42%, tr:  94.28%, tr_best:  94.69%, epoch time: 43.99 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 85.3578%\n",
      "layer   2  Sparsity: 84.1306%\n",
      "layer   3  Sparsity: 83.7343%\n",
      "total_backward_count 53845 real_backward_count 12783  23.740%\n",
      "lif layer 1 self.abs_max_v: 3373.0\n",
      "lif layer 1 self.abs_max_v: 3616.5\n",
      "fc layer 1 self.abs_max_out: 2195.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.855294/  1.943232, val:  61.25%, val_best:  61.25%, tr:  94.38%, tr_best:  94.69%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3782%\n",
      "layer   2  Sparsity: 83.3760%\n",
      "layer   3  Sparsity: 83.3668%\n",
      "total_backward_count 58740 real_backward_count 13706  23.333%\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.832552/  1.957392, val:  51.25%, val_best:  61.25%, tr:  96.02%, tr_best:  96.02%, epoch time: 45.32 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 85.3560%\n",
      "layer   2  Sparsity: 82.6989%\n",
      "layer   3  Sparsity: 82.4948%\n",
      "total_backward_count 63635 real_backward_count 14584  22.918%\n",
      "lif layer 2 self.abs_max_v: 1291.5\n",
      "fc layer 1 self.abs_max_out: 2234.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.832883/  1.937459, val:  56.25%, val_best:  61.25%, tr:  95.71%, tr_best:  96.02%, epoch time: 45.00 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3994%\n",
      "layer   2  Sparsity: 83.3021%\n",
      "layer   3  Sparsity: 83.2283%\n",
      "total_backward_count 68530 real_backward_count 15419  22.500%\n",
      "lif layer 1 self.abs_max_v: 3776.5\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.842914/  1.950208, val:  55.83%, val_best:  61.25%, tr:  95.71%, tr_best:  96.02%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3479%\n",
      "layer   2  Sparsity: 83.9560%\n",
      "layer   3  Sparsity: 83.3654%\n",
      "total_backward_count 73425 real_backward_count 16228  22.101%\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.829698/  1.931092, val:  63.33%, val_best:  63.33%, tr:  96.02%, tr_best:  96.02%, epoch time: 45.29 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3601%\n",
      "layer   2  Sparsity: 84.3118%\n",
      "layer   3  Sparsity: 83.0176%\n",
      "total_backward_count 78320 real_backward_count 17006  21.713%\n",
      "fc layer 1 self.abs_max_out: 2289.0\n",
      "lif layer 1 self.abs_max_v: 3815.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.812653/  1.921231, val:  62.50%, val_best:  63.33%, tr:  95.81%, tr_best:  96.02%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3622%\n",
      "layer   2  Sparsity: 84.1668%\n",
      "layer   3  Sparsity: 83.0346%\n",
      "total_backward_count 83215 real_backward_count 17749  21.329%\n",
      "fc layer 1 self.abs_max_out: 2298.0\n",
      "lif layer 2 self.abs_max_v: 1295.0\n",
      "lif layer 2 self.abs_max_v: 1324.5\n",
      "lif layer 2 self.abs_max_v: 1410.5\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.816291/  1.914516, val:  71.25%, val_best:  71.25%, tr:  96.22%, tr_best:  96.22%, epoch time: 45.72 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 85.3124%\n",
      "layer   2  Sparsity: 84.3783%\n",
      "layer   3  Sparsity: 83.0516%\n",
      "total_backward_count 88110 real_backward_count 18545  21.048%\n",
      "fc layer 1 self.abs_max_out: 2488.0\n",
      "lif layer 1 self.abs_max_v: 3844.0\n",
      "fc layer 1 self.abs_max_out: 2545.0\n",
      "fc layer 1 self.abs_max_out: 2697.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.823546/  1.904366, val:  64.17%, val_best:  71.25%, tr:  96.73%, tr_best:  96.73%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3719%\n",
      "layer   2  Sparsity: 84.7054%\n",
      "layer   3  Sparsity: 83.6537%\n",
      "total_backward_count 93005 real_backward_count 19327  20.781%\n",
      "lif layer 1 self.abs_max_v: 3994.5\n",
      "fc layer 3 self.abs_max_out: 388.0\n",
      "fc layer 3 self.abs_max_out: 391.0\n",
      "fc layer 3 self.abs_max_out: 401.0\n",
      "fc layer 3 self.abs_max_out: 430.0\n",
      "fc layer 2 self.abs_max_out: 970.0\n",
      "fc layer 2 self.abs_max_out: 976.0\n",
      "fc layer 2 self.abs_max_out: 1016.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.804149/  1.929226, val:  49.58%, val_best:  71.25%, tr:  96.53%, tr_best:  96.73%, epoch time: 45.36 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 85.3374%\n",
      "layer   2  Sparsity: 85.0000%\n",
      "layer   3  Sparsity: 83.9149%\n",
      "total_backward_count 97900 real_backward_count 20043  20.473%\n",
      "lif layer 1 self.abs_max_v: 4053.5\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.794688/  1.931408, val:  63.33%, val_best:  71.25%, tr:  96.53%, tr_best:  96.73%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3547%\n",
      "layer   2  Sparsity: 84.4411%\n",
      "layer   3  Sparsity: 83.7847%\n",
      "total_backward_count 102795 real_backward_count 20774  20.209%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.819244/  1.911669, val:  65.83%, val_best:  71.25%, tr:  96.83%, tr_best:  96.83%, epoch time: 44.98 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3534%\n",
      "layer   2  Sparsity: 84.6343%\n",
      "layer   3  Sparsity: 84.5473%\n",
      "total_backward_count 107690 real_backward_count 21521  19.984%\n",
      "lif layer 1 self.abs_max_v: 4112.5\n",
      "fc layer 2 self.abs_max_out: 1046.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.810960/  1.898360, val:  61.25%, val_best:  71.25%, tr:  96.22%, tr_best:  96.83%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3745%\n",
      "layer   2  Sparsity: 84.2698%\n",
      "layer   3  Sparsity: 84.0235%\n",
      "total_backward_count 112585 real_backward_count 22269  19.780%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.820527/  1.921190, val:  56.25%, val_best:  71.25%, tr:  97.34%, tr_best:  97.34%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3131%\n",
      "layer   2  Sparsity: 84.9988%\n",
      "layer   3  Sparsity: 84.0462%\n",
      "total_backward_count 117480 real_backward_count 23051  19.621%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.808257/  1.911791, val:  67.08%, val_best:  71.25%, tr:  97.14%, tr_best:  97.34%, epoch time: 40.98 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 85.3891%\n",
      "layer   2  Sparsity: 85.1613%\n",
      "layer   3  Sparsity: 84.2457%\n",
      "total_backward_count 122375 real_backward_count 23730  19.391%\n",
      "fc layer 1 self.abs_max_out: 2698.0\n",
      "lif layer 1 self.abs_max_v: 4124.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.810741/  1.897183, val:  74.58%, val_best:  74.58%, tr:  97.14%, tr_best:  97.34%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3739%\n",
      "layer   2  Sparsity: 85.2366%\n",
      "layer   3  Sparsity: 83.9519%\n",
      "total_backward_count 127270 real_backward_count 24476  19.232%\n",
      "lif layer 1 self.abs_max_v: 4241.5\n",
      "lif layer 1 self.abs_max_v: 4249.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.790459/  1.882851, val:  63.75%, val_best:  74.58%, tr:  97.14%, tr_best:  97.34%, epoch time: 45.01 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.4129%\n",
      "layer   2  Sparsity: 84.8994%\n",
      "layer   3  Sparsity: 83.2646%\n",
      "total_backward_count 132165 real_backward_count 25157  19.035%\n",
      "fc layer 1 self.abs_max_out: 2784.0\n",
      "fc layer 1 self.abs_max_out: 2840.0\n",
      "lif layer 1 self.abs_max_v: 4318.5\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.781580/  1.906473, val:  63.33%, val_best:  74.58%, tr:  97.45%, tr_best:  97.45%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3560%\n",
      "layer   2  Sparsity: 85.2287%\n",
      "layer   3  Sparsity: 83.5734%\n",
      "total_backward_count 137060 real_backward_count 25851  18.861%\n",
      "lif layer 1 self.abs_max_v: 4333.5\n",
      "lif layer 1 self.abs_max_v: 4689.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.778187/  1.889590, val:  66.67%, val_best:  74.58%, tr:  96.73%, tr_best:  97.45%, epoch time: 44.43 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3821%\n",
      "layer   2  Sparsity: 85.1069%\n",
      "layer   3  Sparsity: 84.0358%\n",
      "total_backward_count 141955 real_backward_count 26547  18.701%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.779685/  1.899888, val:  61.67%, val_best:  74.58%, tr:  96.83%, tr_best:  97.45%, epoch time: 44.89 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3276%\n",
      "layer   2  Sparsity: 85.3654%\n",
      "layer   3  Sparsity: 83.9956%\n",
      "total_backward_count 146850 real_backward_count 27202  18.524%\n",
      "fc layer 1 self.abs_max_out: 2856.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.779082/  1.878501, val:  72.92%, val_best:  74.58%, tr:  97.55%, tr_best:  97.55%, epoch time: 44.08 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 85.3925%\n",
      "layer   2  Sparsity: 85.2332%\n",
      "layer   3  Sparsity: 84.0217%\n",
      "total_backward_count 151745 real_backward_count 27847  18.351%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.771302/  1.881823, val:  70.42%, val_best:  74.58%, tr:  97.85%, tr_best:  97.85%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3596%\n",
      "layer   2  Sparsity: 84.9080%\n",
      "layer   3  Sparsity: 84.3120%\n",
      "total_backward_count 156640 real_backward_count 28476  18.179%\n",
      "fc layer 2 self.abs_max_out: 1050.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.795782/  1.883905, val:  67.50%, val_best:  74.58%, tr:  97.45%, tr_best:  97.85%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.4027%\n",
      "layer   2  Sparsity: 84.7773%\n",
      "layer   3  Sparsity: 84.2199%\n",
      "total_backward_count 161535 real_backward_count 29060  17.990%\n",
      "fc layer 1 self.abs_max_out: 2948.0\n",
      "lif layer 1 self.abs_max_v: 4772.0\n",
      "fc layer 1 self.abs_max_out: 2990.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.770495/  1.876060, val:  74.58%, val_best:  74.58%, tr:  97.96%, tr_best:  97.96%, epoch time: 44.98 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3647%\n",
      "layer   2  Sparsity: 84.4210%\n",
      "layer   3  Sparsity: 83.7296%\n",
      "total_backward_count 166430 real_backward_count 29721  17.858%\n",
      "fc layer 2 self.abs_max_out: 1093.0\n",
      "lif layer 1 self.abs_max_v: 4820.5\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.775366/  1.873856, val:  67.92%, val_best:  74.58%, tr:  97.24%, tr_best:  97.96%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3712%\n",
      "layer   2  Sparsity: 84.7111%\n",
      "layer   3  Sparsity: 84.2113%\n",
      "total_backward_count 171325 real_backward_count 30316  17.695%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.767050/  1.887730, val:  77.08%, val_best:  77.08%, tr:  97.75%, tr_best:  97.96%, epoch time: 45.25 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3844%\n",
      "layer   2  Sparsity: 84.3144%\n",
      "layer   3  Sparsity: 83.8707%\n",
      "total_backward_count 176220 real_backward_count 30891  17.530%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.750819/  1.852241, val:  81.25%, val_best:  81.25%, tr:  98.26%, tr_best:  98.26%, epoch time: 44.19 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3702%\n",
      "layer   2  Sparsity: 84.0747%\n",
      "layer   3  Sparsity: 83.4213%\n",
      "total_backward_count 181115 real_backward_count 31467  17.374%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.742237/  1.860952, val:  68.75%, val_best:  81.25%, tr:  98.16%, tr_best:  98.26%, epoch time: 45.52 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 85.4221%\n",
      "layer   2  Sparsity: 83.8034%\n",
      "layer   3  Sparsity: 83.2104%\n",
      "total_backward_count 186010 real_backward_count 31969  17.187%\n",
      "fc layer 2 self.abs_max_out: 1118.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.750956/  1.854531, val:  71.67%, val_best:  81.25%, tr:  97.55%, tr_best:  98.26%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3892%\n",
      "layer   2  Sparsity: 83.8591%\n",
      "layer   3  Sparsity: 82.8492%\n",
      "total_backward_count 190905 real_backward_count 32550  17.050%\n",
      "lif layer 2 self.abs_max_v: 1502.0\n",
      "lif layer 2 self.abs_max_v: 1503.5\n",
      "fc layer 2 self.abs_max_out: 1151.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.732525/  1.852656, val:  70.00%, val_best:  81.25%, tr:  98.06%, tr_best:  98.26%, epoch time: 45.66 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 85.3422%\n",
      "layer   2  Sparsity: 83.5396%\n",
      "layer   3  Sparsity: 82.9023%\n",
      "total_backward_count 195800 real_backward_count 33077  16.893%\n",
      "fc layer 1 self.abs_max_out: 3128.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.729084/  1.863164, val:  77.92%, val_best:  81.25%, tr:  98.77%, tr_best:  98.77%, epoch time: 44.22 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3447%\n",
      "layer   2  Sparsity: 83.2847%\n",
      "layer   3  Sparsity: 83.0810%\n",
      "total_backward_count 200695 real_backward_count 33623  16.753%\n",
      "fc layer 2 self.abs_max_out: 1172.0\n",
      "fc layer 2 self.abs_max_out: 1221.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.747264/  1.883445, val:  71.67%, val_best:  81.25%, tr:  98.26%, tr_best:  98.77%, epoch time: 45.17 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3742%\n",
      "layer   2  Sparsity: 83.5133%\n",
      "layer   3  Sparsity: 83.5148%\n",
      "total_backward_count 205590 real_backward_count 34125  16.599%\n",
      "lif layer 1 self.abs_max_v: 4859.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.739127/  1.852391, val:  72.50%, val_best:  81.25%, tr:  98.67%, tr_best:  98.77%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.4280%\n",
      "layer   2  Sparsity: 83.3925%\n",
      "layer   3  Sparsity: 83.2959%\n",
      "total_backward_count 210485 real_backward_count 34621  16.448%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.748503/  1.862923, val:  81.25%, val_best:  81.25%, tr:  99.08%, tr_best:  99.08%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3793%\n",
      "layer   2  Sparsity: 83.5139%\n",
      "layer   3  Sparsity: 83.4769%\n",
      "total_backward_count 215380 real_backward_count 35082  16.288%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.745170/  1.878830, val:  64.17%, val_best:  81.25%, tr:  98.67%, tr_best:  99.08%, epoch time: 44.28 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3678%\n",
      "layer   2  Sparsity: 83.3362%\n",
      "layer   3  Sparsity: 83.6016%\n",
      "total_backward_count 220275 real_backward_count 35594  16.159%\n",
      "lif layer 2 self.abs_max_v: 1513.0\n",
      "lif layer 1 self.abs_max_v: 4984.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.753160/  1.858396, val:  74.58%, val_best:  81.25%, tr:  98.57%, tr_best:  99.08%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3308%\n",
      "layer   2  Sparsity: 83.3796%\n",
      "layer   3  Sparsity: 83.1040%\n",
      "total_backward_count 225170 real_backward_count 36089  16.027%\n",
      "lif layer 2 self.abs_max_v: 1554.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.748891/  1.852259, val:  72.92%, val_best:  81.25%, tr:  98.06%, tr_best:  99.08%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3370%\n",
      "layer   2  Sparsity: 83.2002%\n",
      "layer   3  Sparsity: 82.9002%\n",
      "total_backward_count 230065 real_backward_count 36593  15.906%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.737952/  1.852428, val:  80.83%, val_best:  81.25%, tr:  98.37%, tr_best:  99.08%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3853%\n",
      "layer   2  Sparsity: 83.1358%\n",
      "layer   3  Sparsity: 83.4794%\n",
      "total_backward_count 234960 real_backward_count 37075  15.779%\n",
      "lif layer 2 self.abs_max_v: 1558.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.733014/  1.856847, val:  80.42%, val_best:  81.25%, tr:  98.47%, tr_best:  99.08%, epoch time: 41.03 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 85.3633%\n",
      "layer   2  Sparsity: 82.8225%\n",
      "layer   3  Sparsity: 83.4904%\n",
      "total_backward_count 239855 real_backward_count 37531  15.647%\n",
      "fc layer 1 self.abs_max_out: 3167.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.724333/  1.842873, val:  75.00%, val_best:  81.25%, tr:  98.77%, tr_best:  99.08%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3740%\n",
      "layer   2  Sparsity: 82.9059%\n",
      "layer   3  Sparsity: 83.4275%\n",
      "total_backward_count 244750 real_backward_count 38000  15.526%\n",
      "lif layer 1 self.abs_max_v: 4996.5\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.736771/  1.864900, val:  72.08%, val_best:  81.25%, tr:  98.37%, tr_best:  99.08%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3484%\n",
      "layer   2  Sparsity: 83.2331%\n",
      "layer   3  Sparsity: 83.1152%\n",
      "total_backward_count 249645 real_backward_count 38480  15.414%\n",
      "lif layer 2 self.abs_max_v: 1594.5\n",
      "lif layer 1 self.abs_max_v: 5040.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.733036/  1.826691, val:  84.17%, val_best:  84.17%, tr:  98.88%, tr_best:  99.08%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3421%\n",
      "layer   2  Sparsity: 82.9507%\n",
      "layer   3  Sparsity: 83.3667%\n",
      "total_backward_count 254540 real_backward_count 38897  15.281%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.717705/  1.834516, val:  76.67%, val_best:  84.17%, tr:  98.88%, tr_best:  99.08%, epoch time: 44.70 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.4449%\n",
      "layer   2  Sparsity: 82.8636%\n",
      "layer   3  Sparsity: 83.6754%\n",
      "total_backward_count 259435 real_backward_count 39357  15.170%\n",
      "fc layer 1 self.abs_max_out: 3196.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.716669/  1.825597, val:  72.92%, val_best:  84.17%, tr:  98.37%, tr_best:  99.08%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3408%\n",
      "layer   2  Sparsity: 82.6550%\n",
      "layer   3  Sparsity: 83.6640%\n",
      "total_backward_count 264330 real_backward_count 39801  15.057%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.700128/  1.832939, val:  75.83%, val_best:  84.17%, tr:  98.26%, tr_best:  99.08%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3633%\n",
      "layer   2  Sparsity: 82.8747%\n",
      "layer   3  Sparsity: 83.0760%\n",
      "total_backward_count 269225 real_backward_count 40244  14.948%\n",
      "fc layer 1 self.abs_max_out: 3204.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.705052/  1.807109, val:  74.17%, val_best:  84.17%, tr:  98.98%, tr_best:  99.08%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3578%\n",
      "layer   2  Sparsity: 83.0888%\n",
      "layer   3  Sparsity: 82.8704%\n",
      "total_backward_count 274120 real_backward_count 40674  14.838%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.688065/  1.824336, val:  77.92%, val_best:  84.17%, tr:  98.57%, tr_best:  99.08%, epoch time: 44.24 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.4144%\n",
      "layer   2  Sparsity: 83.0582%\n",
      "layer   3  Sparsity: 83.0756%\n",
      "total_backward_count 279015 real_backward_count 41134  14.743%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.708147/  1.833807, val:  77.08%, val_best:  84.17%, tr:  98.98%, tr_best:  99.08%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.4142%\n",
      "layer   2  Sparsity: 83.2905%\n",
      "layer   3  Sparsity: 83.7157%\n",
      "total_backward_count 283910 real_backward_count 41539  14.631%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.708738/  1.819369, val:  80.00%, val_best:  84.17%, tr:  98.77%, tr_best:  99.08%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.4092%\n",
      "layer   2  Sparsity: 83.1817%\n",
      "layer   3  Sparsity: 83.2664%\n",
      "total_backward_count 288805 real_backward_count 41913  14.513%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.694709/  1.836316, val:  70.00%, val_best:  84.17%, tr:  98.16%, tr_best:  99.08%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3701%\n",
      "layer   2  Sparsity: 82.9955%\n",
      "layer   3  Sparsity: 83.1651%\n",
      "total_backward_count 293700 real_backward_count 42368  14.426%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.690014/  1.810098, val:  74.17%, val_best:  84.17%, tr:  99.18%, tr_best:  99.18%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3579%\n",
      "layer   2  Sparsity: 83.1549%\n",
      "layer   3  Sparsity: 83.0501%\n",
      "total_backward_count 298595 real_backward_count 42810  14.337%\n",
      "lif layer 2 self.abs_max_v: 1611.5\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.687273/  1.799634, val:  77.08%, val_best:  84.17%, tr:  98.98%, tr_best:  99.18%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3522%\n",
      "layer   2  Sparsity: 83.0188%\n",
      "layer   3  Sparsity: 82.7792%\n",
      "total_backward_count 303490 real_backward_count 43243  14.249%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.672914/  1.816316, val:  79.17%, val_best:  84.17%, tr:  98.98%, tr_best:  99.18%, epoch time: 44.26 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3576%\n",
      "layer   2  Sparsity: 83.4584%\n",
      "layer   3  Sparsity: 83.0228%\n",
      "total_backward_count 308385 real_backward_count 43636  14.150%\n",
      "lif layer 2 self.abs_max_v: 1691.5\n",
      "lif layer 2 self.abs_max_v: 1765.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.695727/  1.807381, val:  74.58%, val_best:  84.17%, tr:  99.08%, tr_best:  99.18%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3167%\n",
      "layer   2  Sparsity: 83.5482%\n",
      "layer   3  Sparsity: 82.6265%\n",
      "total_backward_count 313280 real_backward_count 44038  14.057%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.676909/  1.791087, val:  84.58%, val_best:  84.58%, tr:  99.18%, tr_best:  99.18%, epoch time: 44.71 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3823%\n",
      "layer   2  Sparsity: 83.6870%\n",
      "layer   3  Sparsity: 83.0011%\n",
      "total_backward_count 318175 real_backward_count 44427  13.963%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.656210/  1.785705, val:  79.17%, val_best:  84.58%, tr:  99.08%, tr_best:  99.18%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3954%\n",
      "layer   2  Sparsity: 83.8148%\n",
      "layer   3  Sparsity: 83.0831%\n",
      "total_backward_count 323070 real_backward_count 44837  13.878%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.667073/  1.794972, val:  76.25%, val_best:  84.58%, tr:  99.39%, tr_best:  99.39%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3864%\n",
      "layer   2  Sparsity: 83.9496%\n",
      "layer   3  Sparsity: 82.9166%\n",
      "total_backward_count 327965 real_backward_count 45251  13.798%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.672820/  1.784298, val:  80.83%, val_best:  84.58%, tr:  98.37%, tr_best:  99.39%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3630%\n",
      "layer   2  Sparsity: 83.9945%\n",
      "layer   3  Sparsity: 82.9386%\n",
      "total_backward_count 332860 real_backward_count 45681  13.724%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.679390/  1.806578, val:  84.17%, val_best:  84.58%, tr:  99.39%, tr_best:  99.39%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.4142%\n",
      "layer   2  Sparsity: 83.7894%\n",
      "layer   3  Sparsity: 83.2538%\n",
      "total_backward_count 337755 real_backward_count 46030  13.628%\n",
      "fc layer 2 self.abs_max_out: 1238.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.683990/  1.822602, val:  71.25%, val_best:  84.58%, tr:  99.08%, tr_best:  99.39%, epoch time: 44.54 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3934%\n",
      "layer   2  Sparsity: 83.8658%\n",
      "layer   3  Sparsity: 83.4532%\n",
      "total_backward_count 342650 real_backward_count 46397  13.541%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.688085/  1.816099, val:  75.00%, val_best:  84.58%, tr:  98.98%, tr_best:  99.39%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3720%\n",
      "layer   2  Sparsity: 83.6077%\n",
      "layer   3  Sparsity: 83.1792%\n",
      "total_backward_count 347545 real_backward_count 46758  13.454%\n",
      "fc layer 1 self.abs_max_out: 3358.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.678613/  1.819753, val:  76.67%, val_best:  84.58%, tr:  98.98%, tr_best:  99.39%, epoch time: 44.35 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3991%\n",
      "layer   2  Sparsity: 83.2937%\n",
      "layer   3  Sparsity: 83.1144%\n",
      "total_backward_count 352440 real_backward_count 47122  13.370%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.691309/  1.830697, val:  79.58%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 45.05 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3329%\n",
      "layer   2  Sparsity: 83.4071%\n",
      "layer   3  Sparsity: 83.1045%\n",
      "total_backward_count 357335 real_backward_count 47425  13.272%\n",
      "lif layer 1 self.abs_max_v: 5265.0\n",
      "lif layer 1 self.abs_max_v: 5291.5\n",
      "fc layer 2 self.abs_max_out: 1273.0\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.676629/  1.808612, val:  75.42%, val_best:  84.58%, tr:  99.08%, tr_best:  99.80%, epoch time: 44.70 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3015%\n",
      "layer   2  Sparsity: 83.3299%\n",
      "layer   3  Sparsity: 82.6153%\n",
      "total_backward_count 362230 real_backward_count 47795  13.195%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.680218/  1.820942, val:  75.42%, val_best:  84.58%, tr:  98.57%, tr_best:  99.80%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.4058%\n",
      "layer   2  Sparsity: 82.9743%\n",
      "layer   3  Sparsity: 82.4549%\n",
      "total_backward_count 367125 real_backward_count 48126  13.109%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.670955/  1.789104, val:  82.50%, val_best:  84.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 45.21 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.4049%\n",
      "layer   2  Sparsity: 82.9493%\n",
      "layer   3  Sparsity: 82.3232%\n",
      "total_backward_count 372020 real_backward_count 48503  13.038%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.657203/  1.795335, val:  77.50%, val_best:  84.58%, tr:  99.08%, tr_best:  99.80%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3660%\n",
      "layer   2  Sparsity: 83.1801%\n",
      "layer   3  Sparsity: 82.2537%\n",
      "total_backward_count 376915 real_backward_count 48867  12.965%\n",
      "fc layer 2 self.abs_max_out: 1285.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.639084/  1.799508, val:  71.67%, val_best:  84.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 45.21 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3899%\n",
      "layer   2  Sparsity: 83.2911%\n",
      "layer   3  Sparsity: 82.4720%\n",
      "total_backward_count 381810 real_backward_count 49197  12.885%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.648074/  1.782169, val:  74.58%, val_best:  84.58%, tr:  98.98%, tr_best:  99.80%, epoch time: 44.54 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3732%\n",
      "layer   2  Sparsity: 83.4258%\n",
      "layer   3  Sparsity: 82.3891%\n",
      "total_backward_count 386705 real_backward_count 49530  12.808%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.619018/  1.767037, val:  79.58%, val_best:  84.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 45.52 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 85.4068%\n",
      "layer   2  Sparsity: 83.3439%\n",
      "layer   3  Sparsity: 82.2040%\n",
      "total_backward_count 391600 real_backward_count 49833  12.725%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.622495/  1.766769, val:  78.33%, val_best:  84.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 44.29 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3370%\n",
      "layer   2  Sparsity: 83.3892%\n",
      "layer   3  Sparsity: 82.8659%\n",
      "total_backward_count 396495 real_backward_count 50138  12.645%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.625205/  1.779791, val:  82.92%, val_best:  84.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3557%\n",
      "layer   2  Sparsity: 83.5121%\n",
      "layer   3  Sparsity: 83.3179%\n",
      "total_backward_count 401390 real_backward_count 50451  12.569%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.634152/  1.770810, val:  81.67%, val_best:  84.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 44.18 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3749%\n",
      "layer   2  Sparsity: 83.4533%\n",
      "layer   3  Sparsity: 83.0347%\n",
      "total_backward_count 406285 real_backward_count 50731  12.487%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.645670/  1.790358, val:  79.17%, val_best:  84.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3528%\n",
      "layer   2  Sparsity: 83.0990%\n",
      "layer   3  Sparsity: 83.1742%\n",
      "total_backward_count 411180 real_backward_count 51039  12.413%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.649966/  1.782268, val:  80.42%, val_best:  84.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 44.13 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3500%\n",
      "layer   2  Sparsity: 82.9278%\n",
      "layer   3  Sparsity: 83.2515%\n",
      "total_backward_count 416075 real_backward_count 51349  12.341%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.637406/  1.767493, val:  81.67%, val_best:  84.58%, tr:  99.08%, tr_best:  99.80%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3745%\n",
      "layer   2  Sparsity: 82.8126%\n",
      "layer   3  Sparsity: 83.2691%\n",
      "total_backward_count 420970 real_backward_count 51661  12.272%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.627748/  1.757268, val:  78.75%, val_best:  84.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3546%\n",
      "layer   2  Sparsity: 83.0644%\n",
      "layer   3  Sparsity: 83.1713%\n",
      "total_backward_count 425865 real_backward_count 51963  12.202%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.627887/  1.759652, val:  71.67%, val_best:  84.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 45.14 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.4083%\n",
      "layer   2  Sparsity: 83.0351%\n",
      "layer   3  Sparsity: 82.8209%\n",
      "total_backward_count 430760 real_backward_count 52223  12.123%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.619913/  1.767151, val:  77.50%, val_best:  84.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3515%\n",
      "layer   2  Sparsity: 83.1347%\n",
      "layer   3  Sparsity: 82.5330%\n",
      "total_backward_count 435655 real_backward_count 52518  12.055%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.621123/  1.753388, val:  80.00%, val_best:  84.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 45.66 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 85.3553%\n",
      "layer   2  Sparsity: 82.9738%\n",
      "layer   3  Sparsity: 82.4362%\n",
      "total_backward_count 440550 real_backward_count 52823  11.990%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.610253/  1.759488, val:  78.75%, val_best:  84.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3660%\n",
      "layer   2  Sparsity: 83.1126%\n",
      "layer   3  Sparsity: 82.6598%\n",
      "total_backward_count 445445 real_backward_count 53110  11.923%\n",
      "fc layer 1 self.abs_max_out: 3579.0\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.624447/  1.775793, val:  82.50%, val_best:  84.58%, tr:  98.67%, tr_best:  99.90%, epoch time: 45.37 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 85.3653%\n",
      "layer   2  Sparsity: 83.2660%\n",
      "layer   3  Sparsity: 82.7532%\n",
      "total_backward_count 450340 real_backward_count 53404  11.859%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.633520/  1.765341, val:  81.67%, val_best:  84.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3745%\n",
      "layer   2  Sparsity: 83.4276%\n",
      "layer   3  Sparsity: 82.8960%\n",
      "total_backward_count 455235 real_backward_count 53685  11.793%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.623561/  1.763310, val:  80.42%, val_best:  84.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 45.22 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3681%\n",
      "layer   2  Sparsity: 83.1874%\n",
      "layer   3  Sparsity: 82.9161%\n",
      "total_backward_count 460130 real_backward_count 53983  11.732%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.623875/  1.752929, val:  81.67%, val_best:  84.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3497%\n",
      "layer   2  Sparsity: 83.2940%\n",
      "layer   3  Sparsity: 83.0918%\n",
      "total_backward_count 465025 real_backward_count 54250  11.666%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.626138/  1.768130, val:  76.67%, val_best:  84.58%, tr:  99.18%, tr_best:  99.90%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3824%\n",
      "layer   2  Sparsity: 83.3212%\n",
      "layer   3  Sparsity: 83.4748%\n",
      "total_backward_count 469920 real_backward_count 54554  11.609%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.611978/  1.768229, val:  71.25%, val_best:  84.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3702%\n",
      "layer   2  Sparsity: 83.2559%\n",
      "layer   3  Sparsity: 83.7723%\n",
      "total_backward_count 474815 real_backward_count 54820  11.546%\n",
      "fc layer 2 self.abs_max_out: 1320.0\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.620648/  1.755379, val:  80.83%, val_best:  84.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.4334%\n",
      "layer   2  Sparsity: 83.1463%\n",
      "layer   3  Sparsity: 83.6150%\n",
      "total_backward_count 479710 real_backward_count 55108  11.488%\n",
      "lif layer 1 self.abs_max_v: 5317.5\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.620492/  1.752596, val:  85.00%, val_best:  85.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3856%\n",
      "layer   2  Sparsity: 83.2995%\n",
      "layer   3  Sparsity: 83.7690%\n",
      "total_backward_count 484605 real_backward_count 55375  11.427%\n",
      "fc layer 2 self.abs_max_out: 1340.0\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.615700/  1.752872, val:  82.50%, val_best:  85.00%, tr:  99.08%, tr_best:  99.90%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3760%\n",
      "layer   2  Sparsity: 83.3826%\n",
      "layer   3  Sparsity: 83.4056%\n",
      "total_backward_count 489500 real_backward_count 55680  11.375%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.601539/  1.749628, val:  81.67%, val_best:  85.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3731%\n",
      "layer   2  Sparsity: 83.3080%\n",
      "layer   3  Sparsity: 83.2922%\n",
      "total_backward_count 494395 real_backward_count 55922  11.311%\n",
      "lif layer 2 self.abs_max_v: 1868.0\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.597702/  1.747796, val:  80.00%, val_best:  85.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 44.21 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.4154%\n",
      "layer   2  Sparsity: 83.2517%\n",
      "layer   3  Sparsity: 83.5274%\n",
      "total_backward_count 499290 real_backward_count 56172  11.250%\n",
      "lif layer 1 self.abs_max_v: 5336.0\n",
      "lif layer 1 self.abs_max_v: 5589.5\n",
      "lif layer 1 self.abs_max_v: 5814.0\n",
      "lif layer 1 self.abs_max_v: 5988.0\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.596231/  1.752248, val:  79.58%, val_best:  85.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3712%\n",
      "layer   2  Sparsity: 83.4096%\n",
      "layer   3  Sparsity: 83.6569%\n",
      "total_backward_count 504185 real_backward_count 56454  11.197%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.592101/  1.738160, val:  82.92%, val_best:  85.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3877%\n",
      "layer   2  Sparsity: 83.4484%\n",
      "layer   3  Sparsity: 83.1048%\n",
      "total_backward_count 509080 real_backward_count 56702  11.138%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.584627/  1.739466, val:  78.33%, val_best:  85.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 45.00 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.4171%\n",
      "layer   2  Sparsity: 83.4141%\n",
      "layer   3  Sparsity: 83.0724%\n",
      "total_backward_count 513975 real_backward_count 56953  11.081%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.582094/  1.733124, val:  77.92%, val_best:  85.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3830%\n",
      "layer   2  Sparsity: 83.5707%\n",
      "layer   3  Sparsity: 82.6436%\n",
      "total_backward_count 518870 real_backward_count 57201  11.024%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.584260/  1.753290, val:  82.50%, val_best:  85.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 45.10 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3594%\n",
      "layer   2  Sparsity: 83.5646%\n",
      "layer   3  Sparsity: 82.2302%\n",
      "total_backward_count 523765 real_backward_count 57461  10.971%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.601795/  1.743924, val:  78.33%, val_best:  85.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3433%\n",
      "layer   2  Sparsity: 83.3563%\n",
      "layer   3  Sparsity: 82.3615%\n",
      "total_backward_count 528660 real_backward_count 57697  10.914%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.586913/  1.730099, val:  77.08%, val_best:  85.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 45.33 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 85.3774%\n",
      "layer   2  Sparsity: 83.3486%\n",
      "layer   3  Sparsity: 82.3445%\n",
      "total_backward_count 533555 real_backward_count 57931  10.858%\n",
      "lif layer 1 self.abs_max_v: 6130.5\n",
      "lif layer 1 self.abs_max_v: 6329.5\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.593578/  1.737222, val:  76.67%, val_best:  85.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3908%\n",
      "layer   2  Sparsity: 83.3852%\n",
      "layer   3  Sparsity: 82.2828%\n",
      "total_backward_count 538450 real_backward_count 58174  10.804%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.580556/  1.727278, val:  77.08%, val_best:  85.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 45.17 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3729%\n",
      "layer   2  Sparsity: 83.5014%\n",
      "layer   3  Sparsity: 81.8797%\n",
      "total_backward_count 543345 real_backward_count 58406  10.749%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.568750/  1.715117, val:  78.33%, val_best:  85.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3748%\n",
      "layer   2  Sparsity: 83.6966%\n",
      "layer   3  Sparsity: 81.9933%\n",
      "total_backward_count 548240 real_backward_count 58661  10.700%\n",
      "fc layer 3 self.abs_max_out: 439.0\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.571102/  1.714311, val:  83.75%, val_best:  85.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3548%\n",
      "layer   2  Sparsity: 83.6321%\n",
      "layer   3  Sparsity: 82.6049%\n",
      "total_backward_count 553135 real_backward_count 58877  10.644%\n",
      "fc layer 3 self.abs_max_out: 448.0\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.559032/  1.723149, val:  81.67%, val_best:  85.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3459%\n",
      "layer   2  Sparsity: 83.6768%\n",
      "layer   3  Sparsity: 82.5889%\n",
      "total_backward_count 558030 real_backward_count 59075  10.586%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.560201/  1.715982, val:  80.83%, val_best:  85.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3755%\n",
      "layer   2  Sparsity: 83.5681%\n",
      "layer   3  Sparsity: 82.2281%\n",
      "total_backward_count 562925 real_backward_count 59287  10.532%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.562467/  1.726849, val:  80.83%, val_best:  85.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.10 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3318%\n",
      "layer   2  Sparsity: 83.5326%\n",
      "layer   3  Sparsity: 82.3368%\n",
      "total_backward_count 567820 real_backward_count 59522  10.483%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.563269/  1.722290, val:  77.92%, val_best:  85.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3468%\n",
      "layer   2  Sparsity: 83.5622%\n",
      "layer   3  Sparsity: 82.6879%\n",
      "total_backward_count 572715 real_backward_count 59725  10.428%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.558782/  1.730157, val:  82.08%, val_best:  85.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 44.24 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3595%\n",
      "layer   2  Sparsity: 83.4772%\n",
      "layer   3  Sparsity: 82.6067%\n",
      "total_backward_count 577610 real_backward_count 59933  10.376%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.559567/  1.719597, val:  83.33%, val_best:  85.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 45.11 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3868%\n",
      "layer   2  Sparsity: 83.6779%\n",
      "layer   3  Sparsity: 82.6769%\n",
      "total_backward_count 582505 real_backward_count 60134  10.323%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.559642/  1.729517, val:  82.08%, val_best:  85.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3987%\n",
      "layer   2  Sparsity: 83.6775%\n",
      "layer   3  Sparsity: 82.7634%\n",
      "total_backward_count 587400 real_backward_count 60309  10.267%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.546866/  1.706452, val:  81.25%, val_best:  85.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 45.35 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 85.4384%\n",
      "layer   2  Sparsity: 83.5685%\n",
      "layer   3  Sparsity: 82.5880%\n",
      "total_backward_count 592295 real_backward_count 60501  10.215%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.543158/  1.699981, val:  81.67%, val_best:  85.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3586%\n",
      "layer   2  Sparsity: 83.5144%\n",
      "layer   3  Sparsity: 82.5774%\n",
      "total_backward_count 597190 real_backward_count 60726  10.169%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.532319/  1.692052, val:  81.67%, val_best:  85.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3949%\n",
      "layer   2  Sparsity: 83.6704%\n",
      "layer   3  Sparsity: 82.4466%\n",
      "total_backward_count 602085 real_backward_count 60914  10.117%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.521545/  1.700608, val:  79.58%, val_best:  85.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3828%\n",
      "layer   2  Sparsity: 83.6483%\n",
      "layer   3  Sparsity: 82.3481%\n",
      "total_backward_count 606980 real_backward_count 61097  10.066%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.520958/  1.691329, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3463%\n",
      "layer   2  Sparsity: 83.4490%\n",
      "layer   3  Sparsity: 82.3034%\n",
      "total_backward_count 611875 real_backward_count 61253  10.011%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.526424/  1.700452, val:  80.42%, val_best:  85.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3939%\n",
      "layer   2  Sparsity: 83.5154%\n",
      "layer   3  Sparsity: 82.2070%\n",
      "total_backward_count 616770 real_backward_count 61429   9.960%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.537979/  1.711385, val:  80.42%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3382%\n",
      "layer   2  Sparsity: 83.6400%\n",
      "layer   3  Sparsity: 82.3783%\n",
      "total_backward_count 621665 real_backward_count 61591   9.907%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.539482/  1.696821, val:  79.17%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3749%\n",
      "layer   2  Sparsity: 83.5152%\n",
      "layer   3  Sparsity: 82.0665%\n",
      "total_backward_count 626560 real_backward_count 61783   9.861%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.527705/  1.682336, val:  81.67%, val_best:  85.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3969%\n",
      "layer   2  Sparsity: 83.3600%\n",
      "layer   3  Sparsity: 82.1428%\n",
      "total_backward_count 631455 real_backward_count 61994   9.818%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.524647/  1.687829, val:  85.42%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.59 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3871%\n",
      "layer   2  Sparsity: 83.5271%\n",
      "layer   3  Sparsity: 82.1267%\n",
      "total_backward_count 636350 real_backward_count 62169   9.770%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.519852/  1.682861, val:  79.58%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3443%\n",
      "layer   2  Sparsity: 83.5300%\n",
      "layer   3  Sparsity: 81.9733%\n",
      "total_backward_count 641245 real_backward_count 62351   9.723%\n",
      "fc layer 2 self.abs_max_out: 1353.0\n",
      "lif layer 1 self.abs_max_v: 6461.0\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.523874/  1.700388, val:  77.50%, val_best:  85.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3588%\n",
      "layer   2  Sparsity: 83.6713%\n",
      "layer   3  Sparsity: 82.0579%\n",
      "total_backward_count 646140 real_backward_count 62531   9.678%\n",
      "fc layer 2 self.abs_max_out: 1366.0\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.519642/  1.673451, val:  84.58%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.4316%\n",
      "layer   2  Sparsity: 83.7692%\n",
      "layer   3  Sparsity: 82.2023%\n",
      "total_backward_count 651035 real_backward_count 62694   9.630%\n",
      "fc layer 1 self.abs_max_out: 3580.0\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.506000/  1.672981, val:  80.42%, val_best:  85.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.45 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3642%\n",
      "layer   2  Sparsity: 83.6678%\n",
      "layer   3  Sparsity: 81.7653%\n",
      "total_backward_count 655930 real_backward_count 62868   9.585%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.506397/  1.677994, val:  80.00%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3619%\n",
      "layer   2  Sparsity: 83.5688%\n",
      "layer   3  Sparsity: 81.6489%\n",
      "total_backward_count 660825 real_backward_count 63063   9.543%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.502434/  1.667954, val:  80.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3279%\n",
      "layer   2  Sparsity: 83.6959%\n",
      "layer   3  Sparsity: 82.1310%\n",
      "total_backward_count 665720 real_backward_count 63231   9.498%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.494812/  1.694258, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.45 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.4172%\n",
      "layer   2  Sparsity: 83.6890%\n",
      "layer   3  Sparsity: 82.4021%\n",
      "total_backward_count 670615 real_backward_count 63377   9.451%\n",
      "fc layer 1 self.abs_max_out: 3583.0\n",
      "fc layer 1 self.abs_max_out: 3602.0\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.498363/  1.671579, val:  79.58%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.71 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3548%\n",
      "layer   2  Sparsity: 83.5690%\n",
      "layer   3  Sparsity: 82.5277%\n",
      "total_backward_count 675510 real_backward_count 63533   9.405%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.490977/  1.676179, val:  84.17%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3885%\n",
      "layer   2  Sparsity: 83.5488%\n",
      "layer   3  Sparsity: 82.2093%\n",
      "total_backward_count 680405 real_backward_count 63685   9.360%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.498494/  1.667051, val:  79.17%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3432%\n",
      "layer   2  Sparsity: 83.5167%\n",
      "layer   3  Sparsity: 81.8995%\n",
      "total_backward_count 685300 real_backward_count 63855   9.318%\n",
      "fc layer 1 self.abs_max_out: 3663.0\n",
      "fc layer 1 self.abs_max_out: 3669.0\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.507813/  1.680634, val:  80.00%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3917%\n",
      "layer   2  Sparsity: 83.6847%\n",
      "layer   3  Sparsity: 81.8850%\n",
      "total_backward_count 690195 real_backward_count 64062   9.282%\n",
      "fc layer 1 self.abs_max_out: 3731.0\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.490347/  1.661785, val:  80.42%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 45.07 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3936%\n",
      "layer   2  Sparsity: 83.8297%\n",
      "layer   3  Sparsity: 81.9254%\n",
      "total_backward_count 695090 real_backward_count 64236   9.241%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.493956/  1.668750, val:  83.33%, val_best:  85.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3610%\n",
      "layer   2  Sparsity: 83.8070%\n",
      "layer   3  Sparsity: 81.6515%\n",
      "total_backward_count 699985 real_backward_count 64417   9.203%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.485824/  1.667533, val:  82.08%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3744%\n",
      "layer   2  Sparsity: 83.8610%\n",
      "layer   3  Sparsity: 81.4162%\n",
      "total_backward_count 704880 real_backward_count 64559   9.159%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.491099/  1.660509, val:  80.00%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3705%\n",
      "layer   2  Sparsity: 83.6975%\n",
      "layer   3  Sparsity: 81.3699%\n",
      "total_backward_count 709775 real_backward_count 64753   9.123%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.481692/  1.658604, val:  77.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.37 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3548%\n",
      "layer   2  Sparsity: 83.6162%\n",
      "layer   3  Sparsity: 81.1130%\n",
      "total_backward_count 714670 real_backward_count 64933   9.086%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.479351/  1.659334, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3936%\n",
      "layer   2  Sparsity: 83.5818%\n",
      "layer   3  Sparsity: 81.0984%\n",
      "total_backward_count 719565 real_backward_count 65109   9.048%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.472824/  1.662156, val:  83.33%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.4134%\n",
      "layer   2  Sparsity: 83.4415%\n",
      "layer   3  Sparsity: 81.1111%\n",
      "total_backward_count 724460 real_backward_count 65258   9.008%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.476894/  1.664497, val:  78.33%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3616%\n",
      "layer   2  Sparsity: 83.4132%\n",
      "layer   3  Sparsity: 81.2619%\n",
      "total_backward_count 729355 real_backward_count 65399   8.967%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.473116/  1.660913, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.84 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3943%\n",
      "layer   2  Sparsity: 83.5167%\n",
      "layer   3  Sparsity: 81.7187%\n",
      "total_backward_count 734250 real_backward_count 65572   8.930%\n",
      "fc layer 3 self.abs_max_out: 449.0\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.472480/  1.660870, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3787%\n",
      "layer   2  Sparsity: 83.4450%\n",
      "layer   3  Sparsity: 81.7243%\n",
      "total_backward_count 739145 real_backward_count 65711   8.890%\n",
      "fc layer 3 self.abs_max_out: 452.0\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.466649/  1.645248, val:  81.25%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3712%\n",
      "layer   2  Sparsity: 83.5337%\n",
      "layer   3  Sparsity: 81.8006%\n",
      "total_backward_count 744040 real_backward_count 65863   8.852%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.468892/  1.662478, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.4176%\n",
      "layer   2  Sparsity: 83.5883%\n",
      "layer   3  Sparsity: 82.3938%\n",
      "total_backward_count 748935 real_backward_count 66006   8.813%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.472093/  1.652228, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.15 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3457%\n",
      "layer   2  Sparsity: 83.7765%\n",
      "layer   3  Sparsity: 82.5719%\n",
      "total_backward_count 753830 real_backward_count 66133   8.773%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.460807/  1.650229, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.04 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3747%\n",
      "layer   2  Sparsity: 83.9010%\n",
      "layer   3  Sparsity: 82.4104%\n",
      "total_backward_count 758725 real_backward_count 66260   8.733%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.458361/  1.654876, val:  75.00%, val_best:  85.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3754%\n",
      "layer   2  Sparsity: 83.7326%\n",
      "layer   3  Sparsity: 82.1030%\n",
      "total_backward_count 763620 real_backward_count 66401   8.696%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.462538/  1.653374, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3970%\n",
      "layer   2  Sparsity: 83.7321%\n",
      "layer   3  Sparsity: 81.9131%\n",
      "total_backward_count 768515 real_backward_count 66537   8.658%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.459055/  1.651082, val:  84.58%, val_best:  85.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3991%\n",
      "layer   2  Sparsity: 83.8621%\n",
      "layer   3  Sparsity: 82.0196%\n",
      "total_backward_count 773410 real_backward_count 66655   8.618%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.460671/  1.642603, val:  76.67%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3981%\n",
      "layer   2  Sparsity: 83.8286%\n",
      "layer   3  Sparsity: 82.1395%\n",
      "total_backward_count 778305 real_backward_count 66769   8.579%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.456522/  1.642037, val:  81.67%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.71 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.4142%\n",
      "layer   2  Sparsity: 83.7285%\n",
      "layer   3  Sparsity: 82.4202%\n",
      "total_backward_count 783200 real_backward_count 66923   8.545%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.455867/  1.645511, val:  82.08%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3617%\n",
      "layer   2  Sparsity: 83.8026%\n",
      "layer   3  Sparsity: 82.3704%\n",
      "total_backward_count 788095 real_backward_count 67065   8.510%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.474201/  1.645029, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.12 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3347%\n",
      "layer   2  Sparsity: 83.8110%\n",
      "layer   3  Sparsity: 82.0860%\n",
      "total_backward_count 792990 real_backward_count 67218   8.477%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.463273/  1.643762, val:  79.58%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.4444%\n",
      "layer   2  Sparsity: 83.8264%\n",
      "layer   3  Sparsity: 81.8107%\n",
      "total_backward_count 797885 real_backward_count 67375   8.444%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.457610/  1.644871, val:  79.17%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 45.06 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3588%\n",
      "layer   2  Sparsity: 83.9153%\n",
      "layer   3  Sparsity: 81.7818%\n",
      "total_backward_count 802780 real_backward_count 67518   8.411%\n",
      "fc layer 3 self.abs_max_out: 455.0\n",
      "fc layer 2 self.abs_max_out: 1377.0\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.458424/  1.652562, val:  79.58%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.36 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.4140%\n",
      "layer   2  Sparsity: 83.7825%\n",
      "layer   3  Sparsity: 81.5696%\n",
      "total_backward_count 807675 real_backward_count 67680   8.380%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.463431/  1.655048, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3198%\n",
      "layer   2  Sparsity: 83.9348%\n",
      "layer   3  Sparsity: 82.0252%\n",
      "total_backward_count 812570 real_backward_count 67838   8.349%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.464226/  1.650367, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3052%\n",
      "layer   2  Sparsity: 83.7559%\n",
      "layer   3  Sparsity: 82.2601%\n",
      "total_backward_count 817465 real_backward_count 68007   8.319%\n",
      "fc layer 3 self.abs_max_out: 461.0\n",
      "fc layer 1 self.abs_max_out: 3743.0\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.463451/  1.641580, val:  75.83%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3421%\n",
      "layer   2  Sparsity: 83.6984%\n",
      "layer   3  Sparsity: 82.0939%\n",
      "total_backward_count 822360 real_backward_count 68138   8.286%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.459798/  1.637293, val:  81.67%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3543%\n",
      "layer   2  Sparsity: 83.6336%\n",
      "layer   3  Sparsity: 82.0235%\n",
      "total_backward_count 827255 real_backward_count 68286   8.255%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.462703/  1.639607, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.78 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3670%\n",
      "layer   2  Sparsity: 83.6238%\n",
      "layer   3  Sparsity: 81.8284%\n",
      "total_backward_count 832150 real_backward_count 68446   8.225%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.441934/  1.627431, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3943%\n",
      "layer   2  Sparsity: 83.6022%\n",
      "layer   3  Sparsity: 81.8321%\n",
      "total_backward_count 837045 real_backward_count 68555   8.190%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.441387/  1.634192, val:  79.58%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 45.02 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.4024%\n",
      "layer   2  Sparsity: 83.4276%\n",
      "layer   3  Sparsity: 81.8451%\n",
      "total_backward_count 841940 real_backward_count 68678   8.157%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.436674/  1.622025, val:  84.58%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.4252%\n",
      "layer   2  Sparsity: 83.5515%\n",
      "layer   3  Sparsity: 81.8148%\n",
      "total_backward_count 846835 real_backward_count 68805   8.125%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.443287/  1.639944, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3266%\n",
      "layer   2  Sparsity: 83.6602%\n",
      "layer   3  Sparsity: 81.7835%\n",
      "total_backward_count 851730 real_backward_count 68916   8.091%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.446627/  1.627347, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3552%\n",
      "layer   2  Sparsity: 83.5274%\n",
      "layer   3  Sparsity: 81.7355%\n",
      "total_backward_count 856625 real_backward_count 69059   8.062%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.444893/  1.632750, val:  81.67%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 45.13 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.4182%\n",
      "layer   2  Sparsity: 83.5181%\n",
      "layer   3  Sparsity: 81.6358%\n",
      "total_backward_count 861520 real_backward_count 69192   8.031%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.444761/  1.651065, val:  82.50%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.71 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3646%\n",
      "layer   2  Sparsity: 83.5773%\n",
      "layer   3  Sparsity: 82.0025%\n",
      "total_backward_count 866415 real_backward_count 69293   7.998%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.445198/  1.638260, val:  81.67%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 45.57 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 85.3301%\n",
      "layer   2  Sparsity: 83.7524%\n",
      "layer   3  Sparsity: 82.1866%\n",
      "total_backward_count 871310 real_backward_count 69378   7.962%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.445958/  1.633471, val:  80.83%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.23 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3596%\n",
      "layer   2  Sparsity: 83.6820%\n",
      "layer   3  Sparsity: 81.9674%\n",
      "total_backward_count 876205 real_backward_count 69479   7.930%\n",
      "fc layer 2 self.abs_max_out: 1381.0\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.432290/  1.632241, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.08 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3731%\n",
      "layer   2  Sparsity: 83.5537%\n",
      "layer   3  Sparsity: 82.0473%\n",
      "total_backward_count 881100 real_backward_count 69586   7.898%\n",
      "fc layer 2 self.abs_max_out: 1384.0\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.434395/  1.631695, val:  79.17%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3522%\n",
      "layer   2  Sparsity: 83.7890%\n",
      "layer   3  Sparsity: 81.8058%\n",
      "total_backward_count 885995 real_backward_count 69686   7.865%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.435964/  1.629098, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.24 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3233%\n",
      "layer   2  Sparsity: 83.7784%\n",
      "layer   3  Sparsity: 82.0219%\n",
      "total_backward_count 890890 real_backward_count 69788   7.834%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.435814/  1.636474, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.4293%\n",
      "layer   2  Sparsity: 83.7004%\n",
      "layer   3  Sparsity: 82.1852%\n",
      "total_backward_count 895785 real_backward_count 69902   7.803%\n",
      "fc layer 3 self.abs_max_out: 465.0\n",
      "fc layer 3 self.abs_max_out: 477.0\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.433442/  1.635102, val:  80.42%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3875%\n",
      "layer   2  Sparsity: 83.7295%\n",
      "layer   3  Sparsity: 82.1801%\n",
      "total_backward_count 900680 real_backward_count 70019   7.774%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.428962/  1.639248, val:  80.00%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.78 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3741%\n",
      "layer   2  Sparsity: 83.7968%\n",
      "layer   3  Sparsity: 81.9132%\n",
      "total_backward_count 905575 real_backward_count 70136   7.745%\n",
      "fc layer 2 self.abs_max_out: 1391.0\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.432556/  1.634292, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3847%\n",
      "layer   2  Sparsity: 83.9417%\n",
      "layer   3  Sparsity: 82.0354%\n",
      "total_backward_count 910470 real_backward_count 70245   7.715%\n",
      "lif layer 1 self.abs_max_v: 6653.5\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.430595/  1.618074, val:  85.83%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3497%\n",
      "layer   2  Sparsity: 83.8591%\n",
      "layer   3  Sparsity: 81.8957%\n",
      "total_backward_count 915365 real_backward_count 70357   7.686%\n",
      "fc layer 3 self.abs_max_out: 487.0\n",
      "fc layer 2 self.abs_max_out: 1401.0\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.427101/  1.631721, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3947%\n",
      "layer   2  Sparsity: 83.7202%\n",
      "layer   3  Sparsity: 82.1028%\n",
      "total_backward_count 920260 real_backward_count 70473   7.658%\n",
      "fc layer 2 self.abs_max_out: 1412.0\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.426100/  1.611595, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.36 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3436%\n",
      "layer   2  Sparsity: 83.7614%\n",
      "layer   3  Sparsity: 82.3224%\n",
      "total_backward_count 925155 real_backward_count 70551   7.626%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.421992/  1.615607, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3799%\n",
      "layer   2  Sparsity: 83.6319%\n",
      "layer   3  Sparsity: 82.4185%\n",
      "total_backward_count 930050 real_backward_count 70653   7.597%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.423445/  1.631178, val:  80.83%, val_best:  85.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.4023%\n",
      "layer   2  Sparsity: 83.5885%\n",
      "layer   3  Sparsity: 82.3694%\n",
      "total_backward_count 934945 real_backward_count 70745   7.567%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.426683/  1.617716, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.89 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3444%\n",
      "layer   2  Sparsity: 83.5970%\n",
      "layer   3  Sparsity: 82.3377%\n",
      "total_backward_count 939840 real_backward_count 70835   7.537%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.425244/  1.623580, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.09 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 85.3964%\n",
      "layer   2  Sparsity: 83.5881%\n",
      "layer   3  Sparsity: 82.4724%\n",
      "total_backward_count 944735 real_backward_count 70914   7.506%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.430847/  1.628758, val:  85.00%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3530%\n",
      "layer   2  Sparsity: 83.6963%\n",
      "layer   3  Sparsity: 82.6404%\n",
      "total_backward_count 949630 real_backward_count 70978   7.474%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.437346/  1.641994, val:  79.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3190%\n",
      "layer   2  Sparsity: 83.6633%\n",
      "layer   3  Sparsity: 82.9025%\n",
      "total_backward_count 954525 real_backward_count 71045   7.443%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.430427/  1.625746, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3930%\n",
      "layer   2  Sparsity: 83.6395%\n",
      "layer   3  Sparsity: 82.8991%\n",
      "total_backward_count 959420 real_backward_count 71123   7.413%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.430024/  1.627284, val:  81.25%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3724%\n",
      "layer   2  Sparsity: 83.6740%\n",
      "layer   3  Sparsity: 82.7513%\n",
      "total_backward_count 964315 real_backward_count 71228   7.386%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.424172/  1.620712, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3542%\n",
      "layer   2  Sparsity: 83.6253%\n",
      "layer   3  Sparsity: 82.8196%\n",
      "total_backward_count 969210 real_backward_count 71305   7.357%\n",
      "fc layer 3 self.abs_max_out: 494.0\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.418858/  1.611591, val:  83.33%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 85.3819%\n",
      "layer   2  Sparsity: 83.6343%\n",
      "layer   3  Sparsity: 82.6426%\n",
      "total_backward_count 974105 real_backward_count 71381   7.328%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.414934/  1.603197, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 85.3303%\n",
      "layer   2  Sparsity: 83.5973%\n",
      "layer   3  Sparsity: 82.2733%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94755c7abde480b94612497fd4d7921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.41493</td></tr><tr><td>val_acc_best</td><td>0.85833</td></tr><tr><td>val_acc_now</td><td>0.82083</td></tr><tr><td>val_loss</td><td>1.6032</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rare-sweep-93</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ancreadw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ancreadw</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_034313-ancreadw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ptvi7si6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_061250-ptvi7si6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ptvi7si6' target=\"_blank\">treasured-sweep-96</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ptvi7si6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ptvi7si6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_061259_810', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = df820968b21bc2412e6634ebdd407347\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 67\n",
      "fc layer 1 self.abs_max_out: 388.0\n",
      "lif layer 1 self.abs_max_v: 388.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 333.0\n",
      "lif layer 2 self.abs_max_v: 333.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 105.0\n",
      "lif layer 1 self.abs_max_v: 399.5\n",
      "fc layer 2 self.abs_max_out: 393.0\n",
      "lif layer 2 self.abs_max_v: 488.5\n",
      "fc layer 3 self.abs_max_out: 126.0\n",
      "lif layer 1 self.abs_max_v: 440.5\n",
      "lif layer 2 self.abs_max_v: 511.0\n",
      "lif layer 2 self.abs_max_v: 590.5\n",
      "fc layer 1 self.abs_max_out: 501.0\n",
      "lif layer 1 self.abs_max_v: 501.0\n",
      "fc layer 2 self.abs_max_out: 410.0\n",
      "lif layer 2 self.abs_max_v: 591.5\n",
      "smallest_now_T updated: 60\n",
      "fc layer 1 self.abs_max_out: 728.0\n",
      "lif layer 1 self.abs_max_v: 728.0\n",
      "fc layer 2 self.abs_max_out: 447.0\n",
      "lif layer 1 self.abs_max_v: 769.5\n",
      "fc layer 2 self.abs_max_out: 516.0\n",
      "fc layer 3 self.abs_max_out: 131.0\n",
      "fc layer 1 self.abs_max_out: 899.0\n",
      "lif layer 1 self.abs_max_v: 899.0\n",
      "smallest_now_T updated: 45\n",
      "fc layer 3 self.abs_max_out: 139.0\n",
      "fc layer 1 self.abs_max_out: 905.0\n",
      "lif layer 1 self.abs_max_v: 905.0\n",
      "lif layer 2 self.abs_max_v: 596.0\n",
      "fc layer 3 self.abs_max_out: 146.0\n",
      "lif layer 2 self.abs_max_v: 596.5\n",
      "lif layer 1 self.abs_max_v: 935.0\n",
      "lif layer 2 self.abs_max_v: 674.5\n",
      "fc layer 2 self.abs_max_out: 565.0\n",
      "fc layer 3 self.abs_max_out: 163.0\n",
      "lif layer 2 self.abs_max_v: 745.0\n",
      "fc layer 1 self.abs_max_out: 1067.0\n",
      "lif layer 1 self.abs_max_v: 1067.0\n",
      "fc layer 1 self.abs_max_out: 1095.0\n",
      "lif layer 1 self.abs_max_v: 1095.0\n",
      "fc layer 1 self.abs_max_out: 1113.0\n",
      "lif layer 1 self.abs_max_v: 1113.0\n",
      "fc layer 3 self.abs_max_out: 216.0\n",
      "fc layer 2 self.abs_max_out: 572.0\n",
      "fc layer 1 self.abs_max_out: 1242.0\n",
      "lif layer 1 self.abs_max_v: 1242.0\n",
      "lif layer 2 self.abs_max_v: 759.5\n",
      "lif layer 2 self.abs_max_v: 858.0\n",
      "fc layer 2 self.abs_max_out: 597.0\n",
      "lif layer 2 self.abs_max_v: 895.5\n",
      "fc layer 2 self.abs_max_out: 612.0\n",
      "fc layer 2 self.abs_max_out: 640.0\n",
      "fc layer 3 self.abs_max_out: 272.0\n",
      "fc layer 2 self.abs_max_out: 658.0\n",
      "smallest_now_T updated: 37\n",
      "fc layer 2 self.abs_max_out: 664.0\n",
      "fc layer 2 self.abs_max_out: 672.0\n",
      "lif layer 2 self.abs_max_v: 914.0\n",
      "lif layer 2 self.abs_max_v: 921.0\n",
      "lif layer 1 self.abs_max_v: 1288.0\n",
      "lif layer 1 self.abs_max_v: 1354.5\n",
      "lif layer 1 self.abs_max_v: 1549.0\n",
      "fc layer 2 self.abs_max_out: 715.0\n",
      "fc layer 1 self.abs_max_out: 1392.0\n",
      "lif layer 2 self.abs_max_v: 936.0\n",
      "lif layer 2 self.abs_max_v: 978.0\n",
      "lif layer 2 self.abs_max_v: 1066.5\n",
      "smallest_now_T updated: 34\n",
      "fc layer 2 self.abs_max_out: 761.0\n",
      "lif layer 1 self.abs_max_v: 1570.5\n",
      "lif layer 2 self.abs_max_v: 1145.0\n",
      "lif layer 2 self.abs_max_v: 1295.5\n",
      "fc layer 2 self.abs_max_out: 772.0\n",
      "fc layer 2 self.abs_max_out: 777.0\n",
      "fc layer 1 self.abs_max_out: 1430.0\n",
      "lif layer 1 self.abs_max_v: 1580.0\n",
      "fc layer 2 self.abs_max_out: 783.0\n",
      "lif layer 2 self.abs_max_v: 1339.0\n",
      "lif layer 2 self.abs_max_v: 1400.5\n",
      "lif layer 2 self.abs_max_v: 1419.5\n",
      "lif layer 1 self.abs_max_v: 1893.5\n",
      "fc layer 2 self.abs_max_out: 786.0\n",
      "fc layer 2 self.abs_max_out: 831.0\n",
      "smallest_now_T updated: 30\n",
      "fc layer 2 self.abs_max_out: 837.0\n",
      "fc layer 2 self.abs_max_out: 861.0\n",
      "lif layer 2 self.abs_max_v: 1535.5\n",
      "lif layer 2 self.abs_max_v: 1562.0\n",
      "fc layer 1 self.abs_max_out: 1548.0\n",
      "fc layer 2 self.abs_max_out: 886.0\n",
      "lif layer 1 self.abs_max_v: 1921.0\n",
      "fc layer 2 self.abs_max_out: 947.0\n",
      "lif layer 1 self.abs_max_v: 1943.0\n",
      "lif layer 1 self.abs_max_v: 2078.0\n",
      "lif layer 1 self.abs_max_v: 2116.0\n",
      "lif layer 1 self.abs_max_v: 2251.0\n",
      "fc layer 3 self.abs_max_out: 296.0\n",
      "lif layer 1 self.abs_max_v: 2266.5\n",
      "lif layer 1 self.abs_max_v: 2513.0\n",
      "lif layer 1 self.abs_max_v: 2537.5\n",
      "lif layer 1 self.abs_max_v: 2749.5\n",
      "lif layer 1 self.abs_max_v: 2769.0\n",
      "fc layer 3 self.abs_max_out: 300.0\n",
      "fc layer 2 self.abs_max_out: 956.0\n",
      "smallest_now_T updated: 26\n",
      "fc layer 3 self.abs_max_out: 304.0\n",
      "smallest_now_T updated: 25\n",
      "fc layer 3 self.abs_max_out: 358.0\n",
      "lif layer 2 self.abs_max_v: 1603.0\n",
      "fc layer 3 self.abs_max_out: 387.0\n",
      "fc layer 2 self.abs_max_out: 966.0\n",
      "lif layer 1 self.abs_max_v: 2801.0\n",
      "fc layer 1 self.abs_max_out: 1603.0\n",
      "lif layer 2 self.abs_max_v: 1609.5\n",
      "fc layer 1 self.abs_max_out: 1683.0\n",
      "lif layer 1 self.abs_max_v: 2864.0\n",
      "lif layer 1 self.abs_max_v: 3013.0\n",
      "lif layer 1 self.abs_max_v: 3065.5\n",
      "fc layer 1 self.abs_max_out: 1812.0\n",
      "lif layer 1 self.abs_max_v: 3100.0\n",
      "lif layer 1 self.abs_max_v: 3173.0\n",
      "lif layer 1 self.abs_max_v: 3210.5\n",
      "fc layer 2 self.abs_max_out: 1048.0\n",
      "fc layer 1 self.abs_max_out: 2092.0\n",
      "lif layer 1 self.abs_max_v: 3581.5\n",
      "lif layer 1 self.abs_max_v: 3679.0\n",
      "lif layer 1 self.abs_max_v: 3705.5\n",
      "fc layer 2 self.abs_max_out: 1060.0\n",
      "fc layer 2 self.abs_max_out: 1106.0\n",
      "smallest_now_T_val updated: 62\n",
      "smallest_now_T_val updated: 51\n",
      "smallest_now_T_val updated: 50\n",
      "smallest_now_T_val updated: 49\n",
      "fc layer 2 self.abs_max_out: 1153.0\n",
      "fc layer 2 self.abs_max_out: 1178.0\n",
      "smallest_now_T_val updated: 40\n",
      "smallest_now_T_val updated: 25\n",
      "fc layer 2 self.abs_max_out: 1183.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.836263/  1.879952, val:  42.50%, val_best:  42.50%, tr:  83.76%, tr_best:  83.76%, epoch time: 45.26 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3988%\n",
      "layer   2  Sparsity: 71.9589%\n",
      "layer   3  Sparsity: 70.3234%\n",
      "total_backward_count 4895 real_backward_count 1536  31.379%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 2212.0\n",
      "fc layer 1 self.abs_max_out: 2344.0\n",
      "lif layer 1 self.abs_max_v: 3833.5\n",
      "fc layer 3 self.abs_max_out: 407.0\n",
      "fc layer 3 self.abs_max_out: 415.0\n",
      "fc layer 3 self.abs_max_out: 418.0\n",
      "lif layer 2 self.abs_max_v: 1625.5\n",
      "fc layer 2 self.abs_max_out: 1190.0\n",
      "fc layer 2 self.abs_max_out: 1197.0\n",
      "fc layer 2 self.abs_max_out: 1210.0\n",
      "fc layer 2 self.abs_max_out: 1220.0\n",
      "lif layer 1 self.abs_max_v: 3973.0\n",
      "fc layer 2 self.abs_max_out: 1226.0\n",
      "fc layer 1 self.abs_max_out: 2360.0\n",
      "lif layer 1 self.abs_max_v: 3994.5\n",
      "fc layer 1 self.abs_max_out: 2745.0\n",
      "lif layer 1 self.abs_max_v: 4742.5\n",
      "lif layer 1 self.abs_max_v: 4860.5\n",
      "lif layer 2 self.abs_max_v: 1657.0\n",
      "lif layer 2 self.abs_max_v: 1667.5\n",
      "fc layer 3 self.abs_max_out: 423.0\n",
      "fc layer 3 self.abs_max_out: 438.0\n",
      "lif layer 2 self.abs_max_v: 1678.5\n",
      "lif layer 2 self.abs_max_v: 1700.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.696223/  1.839163, val:  49.58%, val_best:  49.58%, tr:  90.60%, tr_best:  90.60%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4084%\n",
      "layer   2  Sparsity: 74.9785%\n",
      "layer   3  Sparsity: 70.5375%\n",
      "total_backward_count 9790 real_backward_count 2627  26.834%\n",
      "lif layer 2 self.abs_max_v: 1758.5\n",
      "lif layer 2 self.abs_max_v: 1792.5\n",
      "lif layer 2 self.abs_max_v: 1807.5\n",
      "lif layer 2 self.abs_max_v: 1851.0\n",
      "lif layer 2 self.abs_max_v: 1875.0\n",
      "lif layer 2 self.abs_max_v: 1878.0\n",
      "lif layer 2 self.abs_max_v: 1914.5\n",
      "lif layer 2 self.abs_max_v: 1940.0\n",
      "fc layer 2 self.abs_max_out: 1230.0\n",
      "lif layer 2 self.abs_max_v: 1950.0\n",
      "fc layer 2 self.abs_max_out: 1235.0\n",
      "lif layer 2 self.abs_max_v: 1966.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.671024/  1.833274, val:  54.58%, val_best:  54.58%, tr:  93.56%, tr_best:  93.56%, epoch time: 45.14 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4078%\n",
      "layer   2  Sparsity: 76.0131%\n",
      "layer   3  Sparsity: 70.3003%\n",
      "total_backward_count 14685 real_backward_count 3639  24.780%\n",
      "fc layer 2 self.abs_max_out: 1264.0\n",
      "fc layer 2 self.abs_max_out: 1301.0\n",
      "fc layer 3 self.abs_max_out: 441.0\n",
      "fc layer 3 self.abs_max_out: 455.0\n",
      "fc layer 2 self.abs_max_out: 1332.0\n",
      "lif layer 1 self.abs_max_v: 4947.0\n",
      "fc layer 1 self.abs_max_out: 2768.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.649410/  1.790128, val:  50.00%, val_best:  54.58%, tr:  92.75%, tr_best:  93.56%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3874%\n",
      "layer   2  Sparsity: 78.3158%\n",
      "layer   3  Sparsity: 71.2067%\n",
      "total_backward_count 19580 real_backward_count 4585  23.417%\n",
      "lif layer 1 self.abs_max_v: 5071.5\n",
      "fc layer 3 self.abs_max_out: 467.0\n",
      "fc layer 3 self.abs_max_out: 475.0\n",
      "fc layer 3 self.abs_max_out: 482.0\n",
      "lif layer 1 self.abs_max_v: 5099.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.612641/  1.774401, val:  55.83%, val_best:  55.83%, tr:  93.97%, tr_best:  93.97%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4378%\n",
      "layer   2  Sparsity: 78.6853%\n",
      "layer   3  Sparsity: 72.1060%\n",
      "total_backward_count 24475 real_backward_count 5494  22.447%\n",
      "fc layer 1 self.abs_max_out: 2780.0\n",
      "fc layer 1 self.abs_max_out: 2843.0\n",
      "lif layer 1 self.abs_max_v: 5378.5\n",
      "fc layer 1 self.abs_max_out: 2856.0\n",
      "fc layer 1 self.abs_max_out: 2880.0\n",
      "fc layer 1 self.abs_max_out: 2916.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.607514/  1.787337, val:  55.00%, val_best:  55.83%, tr:  94.18%, tr_best:  94.18%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4186%\n",
      "layer   2  Sparsity: 80.0746%\n",
      "layer   3  Sparsity: 72.4927%\n",
      "total_backward_count 29370 real_backward_count 6336  21.573%\n",
      "fc layer 1 self.abs_max_out: 2956.0\n",
      "lif layer 1 self.abs_max_v: 5511.5\n",
      "fc layer 1 self.abs_max_out: 3067.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.605736/  1.766770, val:  52.50%, val_best:  55.83%, tr:  95.40%, tr_best:  95.40%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4401%\n",
      "layer   2  Sparsity: 79.6982%\n",
      "layer   3  Sparsity: 71.0667%\n",
      "total_backward_count 34265 real_backward_count 7169  20.922%\n",
      "fc layer 2 self.abs_max_out: 1385.0\n",
      "lif layer 1 self.abs_max_v: 5543.0\n",
      "fc layer 1 self.abs_max_out: 3110.0\n",
      "lif layer 1 self.abs_max_v: 5584.5\n",
      "lif layer 1 self.abs_max_v: 5692.5\n",
      "fc layer 2 self.abs_max_out: 1422.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.579142/  1.742418, val:  61.67%, val_best:  61.67%, tr:  95.30%, tr_best:  95.40%, epoch time: 45.20 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3862%\n",
      "layer   2  Sparsity: 79.4973%\n",
      "layer   3  Sparsity: 72.1824%\n",
      "total_backward_count 39160 real_backward_count 7942  20.281%\n",
      "fc layer 1 self.abs_max_out: 3177.0\n",
      "fc layer 1 self.abs_max_out: 3221.0\n",
      "fc layer 3 self.abs_max_out: 489.0\n",
      "fc layer 3 self.abs_max_out: 510.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.578730/  1.727030, val:  61.67%, val_best:  61.67%, tr:  96.02%, tr_best:  96.02%, epoch time: 45.04 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4034%\n",
      "layer   2  Sparsity: 79.5943%\n",
      "layer   3  Sparsity: 72.8364%\n",
      "total_backward_count 44055 real_backward_count 8709  19.768%\n",
      "fc layer 1 self.abs_max_out: 3327.0\n",
      "fc layer 1 self.abs_max_out: 3332.0\n",
      "lif layer 1 self.abs_max_v: 5817.5\n",
      "fc layer 3 self.abs_max_out: 528.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.579691/  1.765634, val:  55.00%, val_best:  61.67%, tr:  95.81%, tr_best:  96.02%, epoch time: 45.38 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.4175%\n",
      "layer   2  Sparsity: 79.1465%\n",
      "layer   3  Sparsity: 73.5148%\n",
      "total_backward_count 48950 real_backward_count 9408  19.220%\n",
      "fc layer 1 self.abs_max_out: 3387.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.596979/  1.773127, val:  50.42%, val_best:  61.67%, tr:  95.81%, tr_best:  96.02%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3973%\n",
      "layer   2  Sparsity: 78.3479%\n",
      "layer   3  Sparsity: 73.8243%\n",
      "total_backward_count 53845 real_backward_count 10137  18.826%\n",
      "fc layer 1 self.abs_max_out: 3464.0\n",
      "fc layer 1 self.abs_max_out: 3475.0\n",
      "lif layer 1 self.abs_max_v: 6031.5\n",
      "lif layer 1 self.abs_max_v: 6285.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.580197/  1.738091, val:  64.17%, val_best:  64.17%, tr:  96.73%, tr_best:  96.73%, epoch time: 45.09 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4340%\n",
      "layer   2  Sparsity: 78.8651%\n",
      "layer   3  Sparsity: 74.3189%\n",
      "total_backward_count 58740 real_backward_count 10794  18.376%\n",
      "fc layer 3 self.abs_max_out: 579.0\n",
      "fc layer 1 self.abs_max_out: 3512.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.584792/  1.722347, val:  67.08%, val_best:  67.08%, tr:  97.24%, tr_best:  97.24%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4205%\n",
      "layer   2  Sparsity: 79.5609%\n",
      "layer   3  Sparsity: 75.1074%\n",
      "total_backward_count 63635 real_backward_count 11475  18.033%\n",
      "fc layer 1 self.abs_max_out: 3528.0\n",
      "fc layer 1 self.abs_max_out: 3610.0\n",
      "lif layer 1 self.abs_max_v: 6673.0\n",
      "fc layer 1 self.abs_max_out: 3699.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.557723/  1.722036, val:  68.33%, val_best:  68.33%, tr:  96.12%, tr_best:  97.24%, epoch time: 45.22 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3903%\n",
      "layer   2  Sparsity: 79.6053%\n",
      "layer   3  Sparsity: 75.0377%\n",
      "total_backward_count 68530 real_backward_count 12095  17.649%\n",
      "fc layer 1 self.abs_max_out: 3968.0\n",
      "fc layer 1 self.abs_max_out: 4089.0\n",
      "lif layer 1 self.abs_max_v: 6766.5\n",
      "lif layer 1 self.abs_max_v: 6846.5\n",
      "lif layer 1 self.abs_max_v: 7105.5\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.548610/  1.683725, val:  65.42%, val_best:  68.33%, tr:  97.65%, tr_best:  97.65%, epoch time: 44.59 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4460%\n",
      "layer   2  Sparsity: 80.0638%\n",
      "layer   3  Sparsity: 75.3899%\n",
      "total_backward_count 73425 real_backward_count 12661  17.243%\n",
      "fc layer 1 self.abs_max_out: 4214.0\n",
      "fc layer 1 self.abs_max_out: 4375.0\n",
      "lif layer 1 self.abs_max_v: 7180.0\n",
      "lif layer 1 self.abs_max_v: 7260.0\n",
      "lif layer 1 self.abs_max_v: 7493.0\n",
      "lif layer 2 self.abs_max_v: 1990.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.533843/  1.702761, val:  63.75%, val_best:  68.33%, tr:  97.14%, tr_best:  97.65%, epoch time: 44.70 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4200%\n",
      "layer   2  Sparsity: 79.9116%\n",
      "layer   3  Sparsity: 75.6249%\n",
      "total_backward_count 78320 real_backward_count 13268  16.941%\n",
      "lif layer 2 self.abs_max_v: 1994.5\n",
      "lif layer 2 self.abs_max_v: 1996.5\n",
      "lif layer 2 self.abs_max_v: 2031.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.527912/  1.711342, val:  66.67%, val_best:  68.33%, tr:  97.85%, tr_best:  97.85%, epoch time: 44.28 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3839%\n",
      "layer   2  Sparsity: 79.7502%\n",
      "layer   3  Sparsity: 76.4623%\n",
      "total_backward_count 83215 real_backward_count 13815  16.602%\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.535317/  1.689209, val:  70.00%, val_best:  70.00%, tr:  97.85%, tr_best:  97.85%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3821%\n",
      "layer   2  Sparsity: 77.6463%\n",
      "layer   3  Sparsity: 75.7564%\n",
      "total_backward_count 88110 real_backward_count 14362  16.300%\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.528051/  1.695633, val:  67.92%, val_best:  70.00%, tr:  97.75%, tr_best:  97.85%, epoch time: 44.30 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4820%\n",
      "layer   2  Sparsity: 78.6856%\n",
      "layer   3  Sparsity: 75.8272%\n",
      "total_backward_count 93005 real_backward_count 14897  16.017%\n",
      "fc layer 2 self.abs_max_out: 1454.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.494933/  1.665819, val:  57.50%, val_best:  70.00%, tr:  98.57%, tr_best:  98.57%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4177%\n",
      "layer   2  Sparsity: 78.7156%\n",
      "layer   3  Sparsity: 75.9203%\n",
      "total_backward_count 97900 real_backward_count 15394  15.724%\n",
      "lif layer 2 self.abs_max_v: 2142.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.479866/  1.684293, val:  70.42%, val_best:  70.42%, tr:  98.26%, tr_best:  98.57%, epoch time: 44.24 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4014%\n",
      "layer   2  Sparsity: 78.4610%\n",
      "layer   3  Sparsity: 76.3301%\n",
      "total_backward_count 102795 real_backward_count 15855  15.424%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.493820/  1.658438, val:  61.25%, val_best:  70.42%, tr:  98.16%, tr_best:  98.57%, epoch time: 45.36 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.3959%\n",
      "layer   2  Sparsity: 78.6695%\n",
      "layer   3  Sparsity: 76.7021%\n",
      "total_backward_count 107690 real_backward_count 16372  15.203%\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.503072/  1.623419, val:  72.92%, val_best:  72.92%, tr:  97.55%, tr_best:  98.57%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3809%\n",
      "layer   2  Sparsity: 79.0125%\n",
      "layer   3  Sparsity: 76.7725%\n",
      "total_backward_count 112585 real_backward_count 16841  14.958%\n",
      "lif layer 2 self.abs_max_v: 2151.5\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.474795/  1.626735, val:  71.67%, val_best:  72.92%, tr:  98.67%, tr_best:  98.67%, epoch time: 45.10 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4127%\n",
      "layer   2  Sparsity: 80.0104%\n",
      "layer   3  Sparsity: 77.3307%\n",
      "total_backward_count 117480 real_backward_count 17283  14.711%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.470021/  1.626627, val:  72.08%, val_best:  72.92%, tr:  98.67%, tr_best:  98.67%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4269%\n",
      "layer   2  Sparsity: 80.1751%\n",
      "layer   3  Sparsity: 77.3938%\n",
      "total_backward_count 122375 real_backward_count 17738  14.495%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.457769/  1.608129, val:  76.67%, val_best:  76.67%, tr:  98.88%, tr_best:  98.88%, epoch time: 45.33 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.3889%\n",
      "layer   2  Sparsity: 80.1890%\n",
      "layer   3  Sparsity: 77.6066%\n",
      "total_backward_count 127270 real_backward_count 18154  14.264%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.458255/  1.610009, val:  69.17%, val_best:  76.67%, tr:  98.26%, tr_best:  98.88%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4579%\n",
      "layer   2  Sparsity: 80.8564%\n",
      "layer   3  Sparsity: 77.6639%\n",
      "total_backward_count 132165 real_backward_count 18589  14.065%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.463766/  1.618074, val:  78.75%, val_best:  78.75%, tr:  98.77%, tr_best:  98.88%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4264%\n",
      "layer   2  Sparsity: 80.8280%\n",
      "layer   3  Sparsity: 77.4628%\n",
      "total_backward_count 137060 real_backward_count 18957  13.831%\n",
      "lif layer 2 self.abs_max_v: 2168.5\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.460706/  1.595797, val:  72.92%, val_best:  78.75%, tr:  98.98%, tr_best:  98.98%, epoch time: 45.64 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.3960%\n",
      "layer   2  Sparsity: 80.4016%\n",
      "layer   3  Sparsity: 77.2891%\n",
      "total_backward_count 141955 real_backward_count 19319  13.609%\n",
      "lif layer 1 self.abs_max_v: 7558.5\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.450052/  1.600179, val:  70.42%, val_best:  78.75%, tr:  98.98%, tr_best:  98.98%, epoch time: 45.12 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4008%\n",
      "layer   2  Sparsity: 80.3934%\n",
      "layer   3  Sparsity: 77.0767%\n",
      "total_backward_count 146850 real_backward_count 19666  13.392%\n",
      "fc layer 2 self.abs_max_out: 1461.0\n",
      "lif layer 2 self.abs_max_v: 2177.0\n",
      "lif layer 1 self.abs_max_v: 7815.0\n",
      "fc layer 1 self.abs_max_out: 4525.0\n",
      "fc layer 1 self.abs_max_out: 4621.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.427778/  1.582433, val:  79.17%, val_best:  79.17%, tr:  99.18%, tr_best:  99.18%, epoch time: 44.91 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4210%\n",
      "layer   2  Sparsity: 80.6970%\n",
      "layer   3  Sparsity: 77.4047%\n",
      "total_backward_count 151745 real_backward_count 20038  13.205%\n",
      "fc layer 2 self.abs_max_out: 1492.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.409593/  1.589680, val:  66.67%, val_best:  79.17%, tr:  99.18%, tr_best:  99.18%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3950%\n",
      "layer   2  Sparsity: 80.4587%\n",
      "layer   3  Sparsity: 77.3546%\n",
      "total_backward_count 156640 real_backward_count 20410  13.030%\n",
      "lif layer 2 self.abs_max_v: 2217.5\n",
      "fc layer 1 self.abs_max_out: 4760.0\n",
      "lif layer 2 self.abs_max_v: 2252.0\n",
      "lif layer 2 self.abs_max_v: 2347.0\n",
      "lif layer 1 self.abs_max_v: 7888.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.412161/  1.594507, val:  67.50%, val_best:  79.17%, tr:  98.67%, tr_best:  99.18%, epoch time: 44.70 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4353%\n",
      "layer   2  Sparsity: 80.1494%\n",
      "layer   3  Sparsity: 77.4158%\n",
      "total_backward_count 161535 real_backward_count 20728  12.832%\n",
      "fc layer 1 self.abs_max_out: 4823.0\n",
      "lif layer 1 self.abs_max_v: 7962.5\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.417210/  1.586826, val:  71.25%, val_best:  79.17%, tr:  99.49%, tr_best:  99.49%, epoch time: 44.45 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4177%\n",
      "layer   2  Sparsity: 80.6126%\n",
      "layer   3  Sparsity: 77.4648%\n",
      "total_backward_count 166430 real_backward_count 21057  12.652%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.415344/  1.578189, val:  70.42%, val_best:  79.17%, tr:  98.67%, tr_best:  99.49%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4212%\n",
      "layer   2  Sparsity: 80.8997%\n",
      "layer   3  Sparsity: 77.5530%\n",
      "total_backward_count 171325 real_backward_count 21402  12.492%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.407814/  1.568474, val:  73.75%, val_best:  79.17%, tr:  99.28%, tr_best:  99.49%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3651%\n",
      "layer   2  Sparsity: 80.8670%\n",
      "layer   3  Sparsity: 77.6515%\n",
      "total_backward_count 176220 real_backward_count 21679  12.302%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.393437/  1.551441, val:  74.58%, val_best:  79.17%, tr:  99.28%, tr_best:  99.49%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4786%\n",
      "layer   2  Sparsity: 80.7809%\n",
      "layer   3  Sparsity: 78.0188%\n",
      "total_backward_count 181115 real_backward_count 21997  12.145%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.374308/  1.562496, val:  72.92%, val_best:  79.17%, tr:  99.39%, tr_best:  99.49%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3715%\n",
      "layer   2  Sparsity: 81.1193%\n",
      "layer   3  Sparsity: 78.0398%\n",
      "total_backward_count 186010 real_backward_count 22237  11.955%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.375710/  1.549861, val:  80.83%, val_best:  80.83%, tr:  99.08%, tr_best:  99.49%, epoch time: 44.29 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3871%\n",
      "layer   2  Sparsity: 81.2849%\n",
      "layer   3  Sparsity: 78.2084%\n",
      "total_backward_count 190905 real_backward_count 22502  11.787%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.371964/  1.532623, val:  80.00%, val_best:  80.83%, tr:  99.59%, tr_best:  99.59%, epoch time: 44.59 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4012%\n",
      "layer   2  Sparsity: 80.9747%\n",
      "layer   3  Sparsity: 78.4014%\n",
      "total_backward_count 195800 real_backward_count 22731  11.609%\n",
      "lif layer 2 self.abs_max_v: 2350.5\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.366569/  1.548669, val:  72.92%, val_best:  80.83%, tr:  99.39%, tr_best:  99.59%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4394%\n",
      "layer   2  Sparsity: 81.1225%\n",
      "layer   3  Sparsity: 78.2658%\n",
      "total_backward_count 200695 real_backward_count 23005  11.463%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.361978/  1.549041, val:  73.33%, val_best:  80.83%, tr:  99.49%, tr_best:  99.59%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4074%\n",
      "layer   2  Sparsity: 81.3266%\n",
      "layer   3  Sparsity: 78.3130%\n",
      "total_backward_count 205590 real_backward_count 23278  11.323%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.362044/  1.541692, val:  72.50%, val_best:  80.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4446%\n",
      "layer   2  Sparsity: 81.0444%\n",
      "layer   3  Sparsity: 78.7616%\n",
      "total_backward_count 210485 real_backward_count 23526  11.177%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.362195/  1.536730, val:  77.92%, val_best:  80.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 45.15 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4128%\n",
      "layer   2  Sparsity: 81.4710%\n",
      "layer   3  Sparsity: 78.9368%\n",
      "total_backward_count 215380 real_backward_count 23752  11.028%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.359377/  1.533617, val:  76.25%, val_best:  80.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 44.78 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4092%\n",
      "layer   2  Sparsity: 81.3449%\n",
      "layer   3  Sparsity: 78.4952%\n",
      "total_backward_count 220275 real_backward_count 23978  10.885%\n",
      "lif layer 2 self.abs_max_v: 2373.5\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.357819/  1.542410, val:  76.25%, val_best:  80.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 44.78 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4407%\n",
      "layer   2  Sparsity: 81.1197%\n",
      "layer   3  Sparsity: 78.6356%\n",
      "total_backward_count 225170 real_backward_count 24173  10.735%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.349462/  1.539855, val:  73.75%, val_best:  80.83%, tr:  99.69%, tr_best:  99.80%, epoch time: 45.16 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4184%\n",
      "layer   2  Sparsity: 81.1142%\n",
      "layer   3  Sparsity: 78.5962%\n",
      "total_backward_count 230065 real_backward_count 24362  10.589%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.343441/  1.537730, val:  71.67%, val_best:  80.83%, tr:  99.69%, tr_best:  99.80%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3897%\n",
      "layer   2  Sparsity: 80.8619%\n",
      "layer   3  Sparsity: 78.6993%\n",
      "total_backward_count 234960 real_backward_count 24539  10.444%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.339749/  1.524641, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.42 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4235%\n",
      "layer   2  Sparsity: 81.1145%\n",
      "layer   3  Sparsity: 78.7059%\n",
      "total_backward_count 239855 real_backward_count 24710  10.302%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.327530/  1.508999, val:  78.75%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3920%\n",
      "layer   2  Sparsity: 80.9894%\n",
      "layer   3  Sparsity: 78.6463%\n",
      "total_backward_count 244750 real_backward_count 24867  10.160%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.326147/  1.511143, val:  80.42%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4189%\n",
      "layer   2  Sparsity: 80.8387%\n",
      "layer   3  Sparsity: 78.6931%\n",
      "total_backward_count 249645 real_backward_count 25063  10.039%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.329287/  1.515636, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4056%\n",
      "layer   2  Sparsity: 81.0459%\n",
      "layer   3  Sparsity: 78.8813%\n",
      "total_backward_count 254540 real_backward_count 25231   9.912%\n",
      "fc layer 2 self.abs_max_out: 1514.0\n",
      "lif layer 2 self.abs_max_v: 2397.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.333725/  1.531949, val:  76.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.21 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4445%\n",
      "layer   2  Sparsity: 81.0209%\n",
      "layer   3  Sparsity: 78.6992%\n",
      "total_backward_count 259435 real_backward_count 25378   9.782%\n",
      "fc layer 2 self.abs_max_out: 1526.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.327284/  1.517676, val:  78.75%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4027%\n",
      "layer   2  Sparsity: 81.0153%\n",
      "layer   3  Sparsity: 78.9045%\n",
      "total_backward_count 264330 real_backward_count 25551   9.666%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.320322/  1.510347, val:  78.75%, val_best:  81.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3930%\n",
      "layer   2  Sparsity: 80.5632%\n",
      "layer   3  Sparsity: 78.7952%\n",
      "total_backward_count 269225 real_backward_count 25680   9.538%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.325497/  1.507826, val:  80.00%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4055%\n",
      "layer   2  Sparsity: 80.3104%\n",
      "layer   3  Sparsity: 78.7553%\n",
      "total_backward_count 274120 real_backward_count 25819   9.419%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.330869/  1.532011, val:  75.42%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3935%\n",
      "layer   2  Sparsity: 80.2942%\n",
      "layer   3  Sparsity: 78.7471%\n",
      "total_backward_count 279015 real_backward_count 25951   9.301%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.313601/  1.515980, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4421%\n",
      "layer   2  Sparsity: 80.6452%\n",
      "layer   3  Sparsity: 78.6852%\n",
      "total_backward_count 283910 real_backward_count 26084   9.187%\n",
      "lif layer 1 self.abs_max_v: 7975.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.315114/  1.509154, val:  77.92%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4183%\n",
      "layer   2  Sparsity: 80.7144%\n",
      "layer   3  Sparsity: 78.8060%\n",
      "total_backward_count 288805 real_backward_count 26218   9.078%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.316401/  1.514076, val:  78.33%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3902%\n",
      "layer   2  Sparsity: 80.7933%\n",
      "layer   3  Sparsity: 79.0930%\n",
      "total_backward_count 293700 real_backward_count 26322   8.962%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.314442/  1.511224, val:  80.00%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4670%\n",
      "layer   2  Sparsity: 80.7701%\n",
      "layer   3  Sparsity: 79.0619%\n",
      "total_backward_count 298595 real_backward_count 26454   8.859%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.297483/  1.512244, val:  75.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3919%\n",
      "layer   2  Sparsity: 80.9756%\n",
      "layer   3  Sparsity: 78.7984%\n",
      "total_backward_count 303490 real_backward_count 26580   8.758%\n",
      "fc layer 3 self.abs_max_out: 580.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.303936/  1.507817, val:  76.67%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4254%\n",
      "layer   2  Sparsity: 80.6916%\n",
      "layer   3  Sparsity: 78.9162%\n",
      "total_backward_count 308385 real_backward_count 26683   8.652%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.296193/  1.519470, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3968%\n",
      "layer   2  Sparsity: 80.9358%\n",
      "layer   3  Sparsity: 79.1569%\n",
      "total_backward_count 313280 real_backward_count 26798   8.554%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.297012/  1.509744, val:  77.92%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 45.11 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4355%\n",
      "layer   2  Sparsity: 80.8694%\n",
      "layer   3  Sparsity: 78.7996%\n",
      "total_backward_count 318175 real_backward_count 26919   8.460%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.293397/  1.508857, val:  78.33%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4029%\n",
      "layer   2  Sparsity: 80.7540%\n",
      "layer   3  Sparsity: 78.8054%\n",
      "total_backward_count 323070 real_backward_count 27037   8.369%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.286304/  1.486323, val:  77.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4200%\n",
      "layer   2  Sparsity: 80.8500%\n",
      "layer   3  Sparsity: 78.8670%\n",
      "total_backward_count 327965 real_backward_count 27166   8.283%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.287092/  1.499125, val:  77.92%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.19 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4554%\n",
      "layer   2  Sparsity: 80.6372%\n",
      "layer   3  Sparsity: 78.7956%\n",
      "total_backward_count 332860 real_backward_count 27283   8.197%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.285847/  1.506180, val:  74.17%, val_best:  81.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3912%\n",
      "layer   2  Sparsity: 80.7913%\n",
      "layer   3  Sparsity: 78.9678%\n",
      "total_backward_count 337755 real_backward_count 27396   8.111%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.279482/  1.490950, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3923%\n",
      "layer   2  Sparsity: 80.8405%\n",
      "layer   3  Sparsity: 79.0921%\n",
      "total_backward_count 342650 real_backward_count 27515   8.030%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.283661/  1.489007, val:  77.92%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.24 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4260%\n",
      "layer   2  Sparsity: 80.8255%\n",
      "layer   3  Sparsity: 79.0125%\n",
      "total_backward_count 347545 real_backward_count 27606   7.943%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.282168/  1.501387, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3907%\n",
      "layer   2  Sparsity: 81.1518%\n",
      "layer   3  Sparsity: 79.0633%\n",
      "total_backward_count 352440 real_backward_count 27684   7.855%\n",
      "fc layer 2 self.abs_max_out: 1540.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.266974/  1.487248, val:  76.25%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4335%\n",
      "layer   2  Sparsity: 81.1209%\n",
      "layer   3  Sparsity: 79.0700%\n",
      "total_backward_count 357335 real_backward_count 27746   7.765%\n",
      "fc layer 2 self.abs_max_out: 1544.0\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.265094/  1.492499, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3831%\n",
      "layer   2  Sparsity: 81.0979%\n",
      "layer   3  Sparsity: 78.9173%\n",
      "total_backward_count 362230 real_backward_count 27843   7.687%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.263093/  1.487798, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.30 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4140%\n",
      "layer   2  Sparsity: 81.0461%\n",
      "layer   3  Sparsity: 78.7619%\n",
      "total_backward_count 367125 real_backward_count 27928   7.607%\n",
      "fc layer 3 self.abs_max_out: 598.0\n",
      "fc layer 3 self.abs_max_out: 605.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.254281/  1.476655, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4024%\n",
      "layer   2  Sparsity: 80.9768%\n",
      "layer   3  Sparsity: 78.7993%\n",
      "total_backward_count 372020 real_backward_count 28008   7.529%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.259549/  1.467564, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3814%\n",
      "layer   2  Sparsity: 80.7337%\n",
      "layer   3  Sparsity: 78.8901%\n",
      "total_backward_count 376915 real_backward_count 28097   7.454%\n",
      "fc layer 2 self.abs_max_out: 1550.0\n",
      "fc layer 2 self.abs_max_out: 1562.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.254297/  1.481253, val:  77.08%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.13 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3815%\n",
      "layer   2  Sparsity: 80.7231%\n",
      "layer   3  Sparsity: 78.9059%\n",
      "total_backward_count 381810 real_backward_count 28154   7.374%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.262899/  1.482022, val:  77.08%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 45.02 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4558%\n",
      "layer   2  Sparsity: 80.8002%\n",
      "layer   3  Sparsity: 79.0985%\n",
      "total_backward_count 386705 real_backward_count 28223   7.298%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.262730/  1.471399, val:  79.17%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.28 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4175%\n",
      "layer   2  Sparsity: 80.9899%\n",
      "layer   3  Sparsity: 78.9883%\n",
      "total_backward_count 391600 real_backward_count 28298   7.226%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.252597/  1.478830, val:  75.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.16 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4144%\n",
      "layer   2  Sparsity: 81.3405%\n",
      "layer   3  Sparsity: 78.9363%\n",
      "total_backward_count 396495 real_backward_count 28366   7.154%\n",
      "fc layer 2 self.abs_max_out: 1611.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.245139/  1.469333, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.33 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4293%\n",
      "layer   2  Sparsity: 81.2393%\n",
      "layer   3  Sparsity: 78.9935%\n",
      "total_backward_count 401390 real_backward_count 28427   7.082%\n",
      "lif layer 1 self.abs_max_v: 8344.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.246785/  1.463315, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4456%\n",
      "layer   2  Sparsity: 81.0368%\n",
      "layer   3  Sparsity: 78.7991%\n",
      "total_backward_count 406285 real_backward_count 28486   7.011%\n",
      "lif layer 1 self.abs_max_v: 8373.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.235391/  1.460550, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.71 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4298%\n",
      "layer   2  Sparsity: 81.3575%\n",
      "layer   3  Sparsity: 78.8006%\n",
      "total_backward_count 411180 real_backward_count 28538   6.941%\n",
      "lif layer 1 self.abs_max_v: 8688.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.236495/  1.468785, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3911%\n",
      "layer   2  Sparsity: 81.2244%\n",
      "layer   3  Sparsity: 78.9111%\n",
      "total_backward_count 416075 real_backward_count 28600   6.874%\n",
      "lif layer 2 self.abs_max_v: 2460.5\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.237091/  1.464657, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.35 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4639%\n",
      "layer   2  Sparsity: 80.9373%\n",
      "layer   3  Sparsity: 78.8733%\n",
      "total_backward_count 420970 real_backward_count 28663   6.809%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.231718/  1.460641, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.98 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4539%\n",
      "layer   2  Sparsity: 80.8912%\n",
      "layer   3  Sparsity: 78.9760%\n",
      "total_backward_count 425865 real_backward_count 28733   6.747%\n",
      "lif layer 2 self.abs_max_v: 2569.5\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.233059/  1.450775, val:  78.33%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4004%\n",
      "layer   2  Sparsity: 80.9395%\n",
      "layer   3  Sparsity: 78.8380%\n",
      "total_backward_count 430760 real_backward_count 28803   6.687%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.233141/  1.446228, val:  77.08%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.31 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.3759%\n",
      "layer   2  Sparsity: 81.1988%\n",
      "layer   3  Sparsity: 78.7295%\n",
      "total_backward_count 435655 real_backward_count 28866   6.626%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.228001/  1.462272, val:  80.83%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4335%\n",
      "layer   2  Sparsity: 81.1148%\n",
      "layer   3  Sparsity: 79.0142%\n",
      "total_backward_count 440550 real_backward_count 28919   6.564%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.241932/  1.463258, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.02 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4368%\n",
      "layer   2  Sparsity: 81.1121%\n",
      "layer   3  Sparsity: 78.7990%\n",
      "total_backward_count 445445 real_backward_count 29011   6.513%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.240378/  1.460636, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.42 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3883%\n",
      "layer   2  Sparsity: 81.2472%\n",
      "layer   3  Sparsity: 78.7747%\n",
      "total_backward_count 450340 real_backward_count 29068   6.455%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.245155/  1.451032, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4240%\n",
      "layer   2  Sparsity: 81.2077%\n",
      "layer   3  Sparsity: 78.8346%\n",
      "total_backward_count 455235 real_backward_count 29126   6.398%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.235969/  1.450230, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.29 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3819%\n",
      "layer   2  Sparsity: 81.1140%\n",
      "layer   3  Sparsity: 78.7816%\n",
      "total_backward_count 460130 real_backward_count 29194   6.345%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.232600/  1.442121, val:  77.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.23 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4573%\n",
      "layer   2  Sparsity: 80.9313%\n",
      "layer   3  Sparsity: 78.9523%\n",
      "total_backward_count 465025 real_backward_count 29261   6.292%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.224436/  1.441428, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.25 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3552%\n",
      "layer   2  Sparsity: 80.6537%\n",
      "layer   3  Sparsity: 79.0334%\n",
      "total_backward_count 469920 real_backward_count 29323   6.240%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.229430/  1.442006, val:  79.17%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4171%\n",
      "layer   2  Sparsity: 80.5828%\n",
      "layer   3  Sparsity: 79.1107%\n",
      "total_backward_count 474815 real_backward_count 29385   6.189%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.219084/  1.440087, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.32 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.4334%\n",
      "layer   2  Sparsity: 80.7975%\n",
      "layer   3  Sparsity: 79.2107%\n",
      "total_backward_count 479710 real_backward_count 29435   6.136%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.211851/  1.433961, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.90 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4291%\n",
      "layer   2  Sparsity: 81.0842%\n",
      "layer   3  Sparsity: 79.0944%\n",
      "total_backward_count 484605 real_backward_count 29483   6.084%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.213998/  1.425800, val:  80.42%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 45.07 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3883%\n",
      "layer   2  Sparsity: 81.2906%\n",
      "layer   3  Sparsity: 79.4432%\n",
      "total_backward_count 489500 real_backward_count 29541   6.035%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.201664/  1.423886, val:  77.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3715%\n",
      "layer   2  Sparsity: 81.1749%\n",
      "layer   3  Sparsity: 79.3547%\n",
      "total_backward_count 494395 real_backward_count 29593   5.986%\n",
      "lif layer 1 self.abs_max_v: 8879.5\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.200166/  1.420743, val:  78.75%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 45.06 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4313%\n",
      "layer   2  Sparsity: 81.2040%\n",
      "layer   3  Sparsity: 79.4591%\n",
      "total_backward_count 499290 real_backward_count 29645   5.937%\n",
      "fc layer 1 self.abs_max_out: 4825.0\n",
      "fc layer 1 self.abs_max_out: 4981.0\n",
      "lif layer 1 self.abs_max_v: 9289.0\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.205497/  1.430659, val:  77.92%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3617%\n",
      "layer   2  Sparsity: 81.1489%\n",
      "layer   3  Sparsity: 79.2559%\n",
      "total_backward_count 504185 real_backward_count 29706   5.892%\n",
      "fc layer 1 self.abs_max_out: 5056.0\n",
      "lif layer 1 self.abs_max_v: 9435.5\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.202048/  1.428473, val:  81.25%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 45.21 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3605%\n",
      "layer   2  Sparsity: 81.2371%\n",
      "layer   3  Sparsity: 79.3560%\n",
      "total_backward_count 509080 real_backward_count 29752   5.844%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.206201/  1.430129, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4107%\n",
      "layer   2  Sparsity: 81.4226%\n",
      "layer   3  Sparsity: 79.2452%\n",
      "total_backward_count 513975 real_backward_count 29822   5.802%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.208466/  1.421866, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.03 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4085%\n",
      "layer   2  Sparsity: 81.2347%\n",
      "layer   3  Sparsity: 79.2130%\n",
      "total_backward_count 518870 real_backward_count 29880   5.759%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.203652/  1.418578, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4327%\n",
      "layer   2  Sparsity: 81.4132%\n",
      "layer   3  Sparsity: 79.3008%\n",
      "total_backward_count 523765 real_backward_count 29933   5.715%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.198098/  1.427129, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.15 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3905%\n",
      "layer   2  Sparsity: 81.4325%\n",
      "layer   3  Sparsity: 79.4726%\n",
      "total_backward_count 528660 real_backward_count 29959   5.667%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.205135/  1.433176, val:  79.58%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.29 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4199%\n",
      "layer   2  Sparsity: 81.3914%\n",
      "layer   3  Sparsity: 79.4907%\n",
      "total_backward_count 533555 real_backward_count 29994   5.622%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.211531/  1.441497, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.30 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4325%\n",
      "layer   2  Sparsity: 81.2690%\n",
      "layer   3  Sparsity: 79.3624%\n",
      "total_backward_count 538450 real_backward_count 30039   5.579%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.205695/  1.417171, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4513%\n",
      "layer   2  Sparsity: 81.1453%\n",
      "layer   3  Sparsity: 79.1833%\n",
      "total_backward_count 543345 real_backward_count 30083   5.537%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.199209/  1.416665, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.25 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4114%\n",
      "layer   2  Sparsity: 81.3353%\n",
      "layer   3  Sparsity: 79.3635%\n",
      "total_backward_count 548240 real_backward_count 30127   5.495%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.196122/  1.423906, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.35 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4271%\n",
      "layer   2  Sparsity: 81.2198%\n",
      "layer   3  Sparsity: 79.5439%\n",
      "total_backward_count 553135 real_backward_count 30160   5.453%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.203261/  1.412670, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.70 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3730%\n",
      "layer   2  Sparsity: 81.1685%\n",
      "layer   3  Sparsity: 79.5208%\n",
      "total_backward_count 558030 real_backward_count 30205   5.413%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.202903/  1.418308, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4302%\n",
      "layer   2  Sparsity: 81.0921%\n",
      "layer   3  Sparsity: 79.4102%\n",
      "total_backward_count 562925 real_backward_count 30239   5.372%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.192457/  1.413791, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4435%\n",
      "layer   2  Sparsity: 81.0660%\n",
      "layer   3  Sparsity: 79.3751%\n",
      "total_backward_count 567820 real_backward_count 30271   5.331%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.196274/  1.422387, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4302%\n",
      "layer   2  Sparsity: 81.0192%\n",
      "layer   3  Sparsity: 79.4349%\n",
      "total_backward_count 572715 real_backward_count 30309   5.292%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.187968/  1.416711, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3963%\n",
      "layer   2  Sparsity: 81.1850%\n",
      "layer   3  Sparsity: 79.6334%\n",
      "total_backward_count 577610 real_backward_count 30331   5.251%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.181328/  1.421690, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.01 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3821%\n",
      "layer   2  Sparsity: 81.4216%\n",
      "layer   3  Sparsity: 79.6690%\n",
      "total_backward_count 582505 real_backward_count 30359   5.212%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.184896/  1.412710, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.91 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3693%\n",
      "layer   2  Sparsity: 81.5019%\n",
      "layer   3  Sparsity: 79.7167%\n",
      "total_backward_count 587400 real_backward_count 30384   5.173%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.189522/  1.428283, val:  76.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3914%\n",
      "layer   2  Sparsity: 81.3604%\n",
      "layer   3  Sparsity: 79.7336%\n",
      "total_backward_count 592295 real_backward_count 30417   5.135%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.192794/  1.431490, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4430%\n",
      "layer   2  Sparsity: 81.2705%\n",
      "layer   3  Sparsity: 79.8997%\n",
      "total_backward_count 597190 real_backward_count 30446   5.098%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.193159/  1.437237, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4764%\n",
      "layer   2  Sparsity: 81.4083%\n",
      "layer   3  Sparsity: 79.8902%\n",
      "total_backward_count 602085 real_backward_count 30481   5.063%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.190753/  1.435427, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4226%\n",
      "layer   2  Sparsity: 81.3213%\n",
      "layer   3  Sparsity: 79.7134%\n",
      "total_backward_count 606980 real_backward_count 30525   5.029%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.185517/  1.426299, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.43 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4366%\n",
      "layer   2  Sparsity: 81.1593%\n",
      "layer   3  Sparsity: 79.5249%\n",
      "total_backward_count 611875 real_backward_count 30557   4.994%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.187636/  1.429713, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3559%\n",
      "layer   2  Sparsity: 81.0401%\n",
      "layer   3  Sparsity: 79.6114%\n",
      "total_backward_count 616770 real_backward_count 30586   4.959%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.186228/  1.424206, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4021%\n",
      "layer   2  Sparsity: 81.1693%\n",
      "layer   3  Sparsity: 79.6503%\n",
      "total_backward_count 621665 real_backward_count 30616   4.925%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.188671/  1.424256, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3908%\n",
      "layer   2  Sparsity: 81.0630%\n",
      "layer   3  Sparsity: 79.6382%\n",
      "total_backward_count 626560 real_backward_count 30645   4.891%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.186720/  1.415555, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4529%\n",
      "layer   2  Sparsity: 80.9739%\n",
      "layer   3  Sparsity: 79.5934%\n",
      "total_backward_count 631455 real_backward_count 30680   4.859%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.183136/  1.419166, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.08 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.4017%\n",
      "layer   2  Sparsity: 81.1642%\n",
      "layer   3  Sparsity: 79.5224%\n",
      "total_backward_count 636350 real_backward_count 30707   4.825%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.184304/  1.411582, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4377%\n",
      "layer   2  Sparsity: 81.1760%\n",
      "layer   3  Sparsity: 79.4645%\n",
      "total_backward_count 641245 real_backward_count 30750   4.795%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.178606/  1.414884, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.78 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4663%\n",
      "layer   2  Sparsity: 81.1194%\n",
      "layer   3  Sparsity: 79.5593%\n",
      "total_backward_count 646140 real_backward_count 30778   4.763%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.176789/  1.408934, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.11 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4094%\n",
      "layer   2  Sparsity: 81.1117%\n",
      "layer   3  Sparsity: 79.5188%\n",
      "total_backward_count 651035 real_backward_count 30808   4.732%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.173231/  1.403012, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4079%\n",
      "layer   2  Sparsity: 80.9981%\n",
      "layer   3  Sparsity: 79.4690%\n",
      "total_backward_count 655930 real_backward_count 30833   4.701%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.170257/  1.396305, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.12 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4286%\n",
      "layer   2  Sparsity: 81.1235%\n",
      "layer   3  Sparsity: 79.5154%\n",
      "total_backward_count 660825 real_backward_count 30850   4.668%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.171438/  1.399639, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.84 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4217%\n",
      "layer   2  Sparsity: 81.2813%\n",
      "layer   3  Sparsity: 79.5982%\n",
      "total_backward_count 665720 real_backward_count 30877   4.638%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.167292/  1.405425, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.08 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4236%\n",
      "layer   2  Sparsity: 81.2242%\n",
      "layer   3  Sparsity: 79.5417%\n",
      "total_backward_count 670615 real_backward_count 30896   4.607%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.171642/  1.402156, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4196%\n",
      "layer   2  Sparsity: 81.3439%\n",
      "layer   3  Sparsity: 79.5823%\n",
      "total_backward_count 675510 real_backward_count 30928   4.578%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.169509/  1.399850, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3845%\n",
      "layer   2  Sparsity: 81.5030%\n",
      "layer   3  Sparsity: 79.6916%\n",
      "total_backward_count 680405 real_backward_count 30942   4.548%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.169155/  1.414595, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4279%\n",
      "layer   2  Sparsity: 81.2864%\n",
      "layer   3  Sparsity: 79.6292%\n",
      "total_backward_count 685300 real_backward_count 30968   4.519%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.161561/  1.403801, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4442%\n",
      "layer   2  Sparsity: 81.2381%\n",
      "layer   3  Sparsity: 79.5194%\n",
      "total_backward_count 690195 real_backward_count 30995   4.491%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.159974/  1.400774, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4266%\n",
      "layer   2  Sparsity: 81.3491%\n",
      "layer   3  Sparsity: 79.5582%\n",
      "total_backward_count 695090 real_backward_count 31012   4.462%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.158907/  1.396613, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4793%\n",
      "layer   2  Sparsity: 81.3544%\n",
      "layer   3  Sparsity: 79.5942%\n",
      "total_backward_count 699985 real_backward_count 31028   4.433%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.161649/  1.407977, val:  77.08%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3977%\n",
      "layer   2  Sparsity: 81.2576%\n",
      "layer   3  Sparsity: 79.6275%\n",
      "total_backward_count 704880 real_backward_count 31048   4.405%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.152056/  1.388897, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3970%\n",
      "layer   2  Sparsity: 81.2517%\n",
      "layer   3  Sparsity: 79.6127%\n",
      "total_backward_count 709775 real_backward_count 31077   4.378%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.153064/  1.391486, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.35 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4151%\n",
      "layer   2  Sparsity: 81.0654%\n",
      "layer   3  Sparsity: 79.6432%\n",
      "total_backward_count 714670 real_backward_count 31103   4.352%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.155625/  1.388346, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4144%\n",
      "layer   2  Sparsity: 81.2107%\n",
      "layer   3  Sparsity: 79.6477%\n",
      "total_backward_count 719565 real_backward_count 31125   4.326%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.156766/  1.391226, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.24 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4316%\n",
      "layer   2  Sparsity: 81.2455%\n",
      "layer   3  Sparsity: 79.6706%\n",
      "total_backward_count 724460 real_backward_count 31142   4.299%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.161404/  1.404894, val:  79.17%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4154%\n",
      "layer   2  Sparsity: 81.2840%\n",
      "layer   3  Sparsity: 79.6958%\n",
      "total_backward_count 729355 real_backward_count 31163   4.273%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.167936/  1.398812, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3871%\n",
      "layer   2  Sparsity: 81.3106%\n",
      "layer   3  Sparsity: 79.7979%\n",
      "total_backward_count 734250 real_backward_count 31186   4.247%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.168985/  1.403717, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.06 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4315%\n",
      "layer   2  Sparsity: 81.2166%\n",
      "layer   3  Sparsity: 79.7693%\n",
      "total_backward_count 739145 real_backward_count 31213   4.223%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.162429/  1.399148, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4497%\n",
      "layer   2  Sparsity: 81.3725%\n",
      "layer   3  Sparsity: 79.8177%\n",
      "total_backward_count 744040 real_backward_count 31236   4.198%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.156201/  1.390190, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4088%\n",
      "layer   2  Sparsity: 81.4378%\n",
      "layer   3  Sparsity: 79.9326%\n",
      "total_backward_count 748935 real_backward_count 31253   4.173%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.154590/  1.391371, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.02 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4481%\n",
      "layer   2  Sparsity: 81.4788%\n",
      "layer   3  Sparsity: 80.0005%\n",
      "total_backward_count 753830 real_backward_count 31266   4.148%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.154150/  1.387431, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4132%\n",
      "layer   2  Sparsity: 81.4726%\n",
      "layer   3  Sparsity: 79.9637%\n",
      "total_backward_count 758725 real_backward_count 31282   4.123%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.153140/  1.389078, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.89 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4197%\n",
      "layer   2  Sparsity: 81.5453%\n",
      "layer   3  Sparsity: 79.8415%\n",
      "total_backward_count 763620 real_backward_count 31305   4.100%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.153944/  1.388756, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4462%\n",
      "layer   2  Sparsity: 81.6017%\n",
      "layer   3  Sparsity: 79.8942%\n",
      "total_backward_count 768515 real_backward_count 31337   4.078%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.150044/  1.388050, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.09 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4262%\n",
      "layer   2  Sparsity: 81.5606%\n",
      "layer   3  Sparsity: 79.8922%\n",
      "total_backward_count 773410 real_backward_count 31354   4.054%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.154779/  1.392617, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.35 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4181%\n",
      "layer   2  Sparsity: 81.4788%\n",
      "layer   3  Sparsity: 80.0043%\n",
      "total_backward_count 778305 real_backward_count 31371   4.031%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.155234/  1.394291, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.12 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4378%\n",
      "layer   2  Sparsity: 81.3801%\n",
      "layer   3  Sparsity: 79.8886%\n",
      "total_backward_count 783200 real_backward_count 31390   4.008%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.157326/  1.395544, val:  77.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3808%\n",
      "layer   2  Sparsity: 81.4500%\n",
      "layer   3  Sparsity: 80.0897%\n",
      "total_backward_count 788095 real_backward_count 31405   3.985%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.155806/  1.397012, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4072%\n",
      "layer   2  Sparsity: 81.5062%\n",
      "layer   3  Sparsity: 80.1181%\n",
      "total_backward_count 792990 real_backward_count 31418   3.962%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.160204/  1.396608, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3552%\n",
      "layer   2  Sparsity: 81.4539%\n",
      "layer   3  Sparsity: 80.1517%\n",
      "total_backward_count 797885 real_backward_count 31432   3.939%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.161001/  1.400956, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4188%\n",
      "layer   2  Sparsity: 81.4709%\n",
      "layer   3  Sparsity: 80.1120%\n",
      "total_backward_count 802780 real_backward_count 31445   3.917%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.159041/  1.390697, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4194%\n",
      "layer   2  Sparsity: 81.3180%\n",
      "layer   3  Sparsity: 80.1490%\n",
      "total_backward_count 807675 real_backward_count 31462   3.895%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.150350/  1.382207, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.42 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4012%\n",
      "layer   2  Sparsity: 81.3716%\n",
      "layer   3  Sparsity: 80.1123%\n",
      "total_backward_count 812570 real_backward_count 31479   3.874%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.144861/  1.385757, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.98 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4202%\n",
      "layer   2  Sparsity: 81.4654%\n",
      "layer   3  Sparsity: 80.0171%\n",
      "total_backward_count 817465 real_backward_count 31507   3.854%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.143310/  1.385413, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3870%\n",
      "layer   2  Sparsity: 81.5702%\n",
      "layer   3  Sparsity: 79.9431%\n",
      "total_backward_count 822360 real_backward_count 31532   3.834%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.142899/  1.385457, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.19 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3687%\n",
      "layer   2  Sparsity: 81.7363%\n",
      "layer   3  Sparsity: 80.0334%\n",
      "total_backward_count 827255 real_backward_count 31554   3.814%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.140060/  1.387046, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4049%\n",
      "layer   2  Sparsity: 81.6208%\n",
      "layer   3  Sparsity: 80.1032%\n",
      "total_backward_count 832150 real_backward_count 31567   3.793%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.139757/  1.380996, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4089%\n",
      "layer   2  Sparsity: 81.4722%\n",
      "layer   3  Sparsity: 80.0486%\n",
      "total_backward_count 837045 real_backward_count 31590   3.774%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.136956/  1.378467, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4268%\n",
      "layer   2  Sparsity: 81.4874%\n",
      "layer   3  Sparsity: 80.1125%\n",
      "total_backward_count 841940 real_backward_count 31612   3.755%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.139707/  1.382294, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3873%\n",
      "layer   2  Sparsity: 81.2681%\n",
      "layer   3  Sparsity: 80.0564%\n",
      "total_backward_count 846835 real_backward_count 31637   3.736%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.135751/  1.378641, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.35 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3657%\n",
      "layer   2  Sparsity: 81.2169%\n",
      "layer   3  Sparsity: 79.8880%\n",
      "total_backward_count 851730 real_backward_count 31658   3.717%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.135369/  1.380423, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4100%\n",
      "layer   2  Sparsity: 81.2705%\n",
      "layer   3  Sparsity: 79.8362%\n",
      "total_backward_count 856625 real_backward_count 31675   3.698%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.134669/  1.375263, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.18 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4305%\n",
      "layer   2  Sparsity: 81.2885%\n",
      "layer   3  Sparsity: 79.9094%\n",
      "total_backward_count 861520 real_backward_count 31688   3.678%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.131423/  1.381830, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4130%\n",
      "layer   2  Sparsity: 81.3113%\n",
      "layer   3  Sparsity: 79.9875%\n",
      "total_backward_count 866415 real_backward_count 31701   3.659%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.130497/  1.384732, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.28 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3993%\n",
      "layer   2  Sparsity: 81.3404%\n",
      "layer   3  Sparsity: 80.0419%\n",
      "total_backward_count 871310 real_backward_count 31712   3.640%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.131932/  1.379631, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4203%\n",
      "layer   2  Sparsity: 81.4092%\n",
      "layer   3  Sparsity: 80.0130%\n",
      "total_backward_count 876205 real_backward_count 31733   3.622%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.131589/  1.377192, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.54 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4212%\n",
      "layer   2  Sparsity: 81.4614%\n",
      "layer   3  Sparsity: 80.0312%\n",
      "total_backward_count 881100 real_backward_count 31754   3.604%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.132690/  1.374887, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3669%\n",
      "layer   2  Sparsity: 81.6181%\n",
      "layer   3  Sparsity: 80.0346%\n",
      "total_backward_count 885995 real_backward_count 31771   3.586%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.128790/  1.371540, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.20 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4264%\n",
      "layer   2  Sparsity: 81.6033%\n",
      "layer   3  Sparsity: 79.9925%\n",
      "total_backward_count 890890 real_backward_count 31781   3.567%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.126239/  1.379744, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3936%\n",
      "layer   2  Sparsity: 81.5991%\n",
      "layer   3  Sparsity: 80.0511%\n",
      "total_backward_count 895785 real_backward_count 31796   3.550%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.129685/  1.376882, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4319%\n",
      "layer   2  Sparsity: 81.7129%\n",
      "layer   3  Sparsity: 80.1789%\n",
      "total_backward_count 900680 real_backward_count 31805   3.531%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.131976/  1.377588, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.50 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.4065%\n",
      "layer   2  Sparsity: 81.6565%\n",
      "layer   3  Sparsity: 80.1085%\n",
      "total_backward_count 905575 real_backward_count 31817   3.513%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.131636/  1.372740, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.00 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4121%\n",
      "layer   2  Sparsity: 81.4863%\n",
      "layer   3  Sparsity: 80.1482%\n",
      "total_backward_count 910470 real_backward_count 31835   3.497%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.130692/  1.369179, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 46.07 seconds, 0.77 minutes\n",
      "layer   1  Sparsity: 82.3638%\n",
      "layer   2  Sparsity: 81.4589%\n",
      "layer   3  Sparsity: 80.1103%\n",
      "total_backward_count 915365 real_backward_count 31858   3.480%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.129606/  1.374116, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.71 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.4656%\n",
      "layer   2  Sparsity: 81.5054%\n",
      "layer   3  Sparsity: 80.1503%\n",
      "total_backward_count 920260 real_backward_count 31872   3.463%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.129529/  1.374473, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.32 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.4276%\n",
      "layer   2  Sparsity: 81.5756%\n",
      "layer   3  Sparsity: 80.2668%\n",
      "total_backward_count 925155 real_backward_count 31883   3.446%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.129912/  1.374481, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4090%\n",
      "layer   2  Sparsity: 81.6341%\n",
      "layer   3  Sparsity: 80.3125%\n",
      "total_backward_count 930050 real_backward_count 31890   3.429%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.124285/  1.375357, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.78 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4139%\n",
      "layer   2  Sparsity: 81.6269%\n",
      "layer   3  Sparsity: 80.2284%\n",
      "total_backward_count 934945 real_backward_count 31904   3.412%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.125277/  1.371495, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4108%\n",
      "layer   2  Sparsity: 81.6624%\n",
      "layer   3  Sparsity: 80.1742%\n",
      "total_backward_count 939840 real_backward_count 31911   3.395%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.127008/  1.370564, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.05 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4349%\n",
      "layer   2  Sparsity: 81.7221%\n",
      "layer   3  Sparsity: 80.2828%\n",
      "total_backward_count 944735 real_backward_count 31924   3.379%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.122665/  1.375038, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4296%\n",
      "layer   2  Sparsity: 81.7956%\n",
      "layer   3  Sparsity: 80.3392%\n",
      "total_backward_count 949630 real_backward_count 31933   3.363%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.123017/  1.377197, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.31 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4194%\n",
      "layer   2  Sparsity: 81.7083%\n",
      "layer   3  Sparsity: 80.3758%\n",
      "total_backward_count 954525 real_backward_count 31946   3.347%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.126005/  1.374470, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4176%\n",
      "layer   2  Sparsity: 81.6052%\n",
      "layer   3  Sparsity: 80.3493%\n",
      "total_backward_count 959420 real_backward_count 31959   3.331%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.125269/  1.370861, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.39 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4277%\n",
      "layer   2  Sparsity: 81.5872%\n",
      "layer   3  Sparsity: 80.3217%\n",
      "total_backward_count 964315 real_backward_count 31969   3.315%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.124353/  1.376669, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.28 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4452%\n",
      "layer   2  Sparsity: 81.6470%\n",
      "layer   3  Sparsity: 80.3375%\n",
      "total_backward_count 969210 real_backward_count 31984   3.300%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.123900/  1.380784, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4231%\n",
      "layer   2  Sparsity: 81.6354%\n",
      "layer   3  Sparsity: 80.3140%\n",
      "total_backward_count 974105 real_backward_count 31998   3.285%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.126558/  1.379420, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.43 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.4271%\n",
      "layer   2  Sparsity: 81.6523%\n",
      "layer   3  Sparsity: 80.3240%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feea77bc6eb445b5876272d102e3806d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.12656</td></tr><tr><td>val_acc_best</td><td>0.81667</td></tr><tr><td>val_acc_now</td><td>0.79583</td></tr><tr><td>val_loss</td><td>1.37942</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">treasured-sweep-96</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ptvi7si6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ptvi7si6</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_061250-ptvi7si6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8guowcp8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_084240-8guowcp8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8guowcp8' target=\"_blank\">misty-sweep-99</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8guowcp8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8guowcp8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_084249_199', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 15, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = df820968b21bc2412e6634ebdd407347\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=15, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=15, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 67\n",
      "fc layer 1 self.abs_max_out: 388.0\n",
      "lif layer 1 self.abs_max_v: 388.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 333.0\n",
      "lif layer 2 self.abs_max_v: 333.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 105.0\n",
      "lif layer 1 self.abs_max_v: 399.5\n",
      "fc layer 2 self.abs_max_out: 393.0\n",
      "lif layer 2 self.abs_max_v: 488.5\n",
      "fc layer 3 self.abs_max_out: 126.0\n",
      "lif layer 1 self.abs_max_v: 440.5\n",
      "lif layer 2 self.abs_max_v: 511.0\n",
      "lif layer 2 self.abs_max_v: 567.0\n",
      "lif layer 1 self.abs_max_v: 477.0\n",
      "fc layer 2 self.abs_max_out: 400.0\n",
      "lif layer 2 self.abs_max_v: 580.5\n",
      "smallest_now_T updated: 60\n",
      "fc layer 1 self.abs_max_out: 511.0\n",
      "lif layer 1 self.abs_max_v: 511.0\n",
      "fc layer 2 self.abs_max_out: 422.0\n",
      "fc layer 3 self.abs_max_out: 178.0\n",
      "lif layer 1 self.abs_max_v: 577.5\n",
      "fc layer 1 self.abs_max_out: 666.0\n",
      "lif layer 1 self.abs_max_v: 666.0\n",
      "fc layer 2 self.abs_max_out: 427.0\n",
      "lif layer 2 self.abs_max_v: 592.0\n",
      "smallest_now_T updated: 45\n",
      "fc layer 1 self.abs_max_out: 777.0\n",
      "lif layer 1 self.abs_max_v: 777.0\n",
      "lif layer 2 self.abs_max_v: 605.0\n",
      "fc layer 2 self.abs_max_out: 458.0\n",
      "lif layer 2 self.abs_max_v: 626.0\n",
      "fc layer 2 self.abs_max_out: 474.0\n",
      "lif layer 2 self.abs_max_v: 639.0\n",
      "fc layer 2 self.abs_max_out: 508.0\n",
      "fc layer 3 self.abs_max_out: 193.0\n",
      "lif layer 1 self.abs_max_v: 875.5\n",
      "lif layer 1 self.abs_max_v: 890.0\n",
      "lif layer 2 self.abs_max_v: 649.5\n",
      "lif layer 2 self.abs_max_v: 706.0\n",
      "lif layer 2 self.abs_max_v: 714.0\n",
      "lif layer 2 self.abs_max_v: 757.0\n",
      "fc layer 1 self.abs_max_out: 801.0\n",
      "fc layer 2 self.abs_max_out: 548.0\n",
      "fc layer 2 self.abs_max_out: 624.0\n",
      "lif layer 2 self.abs_max_v: 795.5\n",
      "lif layer 1 self.abs_max_v: 896.5\n",
      "lif layer 2 self.abs_max_v: 839.0\n",
      "fc layer 3 self.abs_max_out: 215.0\n",
      "smallest_now_T updated: 37\n",
      "fc layer 1 self.abs_max_out: 825.0\n",
      "fc layer 1 self.abs_max_out: 833.0\n",
      "lif layer 1 self.abs_max_v: 988.5\n",
      "fc layer 1 self.abs_max_out: 854.0\n",
      "lif layer 2 self.abs_max_v: 853.5\n",
      "lif layer 1 self.abs_max_v: 1170.0\n",
      "lif layer 2 self.abs_max_v: 858.0\n",
      "lif layer 2 self.abs_max_v: 894.5\n",
      "smallest_now_T updated: 34\n",
      "fc layer 1 self.abs_max_out: 885.0\n",
      "fc layer 1 self.abs_max_out: 906.0\n",
      "fc layer 1 self.abs_max_out: 1013.0\n",
      "lif layer 2 self.abs_max_v: 901.0\n",
      "lif layer 2 self.abs_max_v: 954.5\n",
      "lif layer 2 self.abs_max_v: 1058.5\n",
      "fc layer 1 self.abs_max_out: 1021.0\n",
      "fc layer 1 self.abs_max_out: 1089.0\n",
      "fc layer 2 self.abs_max_out: 653.0\n",
      "lif layer 2 self.abs_max_v: 1133.0\n",
      "smallest_now_T updated: 30\n",
      "fc layer 1 self.abs_max_out: 1149.0\n",
      "lif layer 1 self.abs_max_v: 1187.5\n",
      "fc layer 1 self.abs_max_out: 1164.0\n",
      "lif layer 1 self.abs_max_v: 1204.5\n",
      "fc layer 1 self.abs_max_out: 1203.0\n",
      "lif layer 1 self.abs_max_v: 1323.5\n",
      "lif layer 1 self.abs_max_v: 1423.5\n",
      "lif layer 1 self.abs_max_v: 1597.0\n",
      "lif layer 2 self.abs_max_v: 1180.5\n",
      "smallest_now_T updated: 26\n",
      "smallest_now_T updated: 25\n",
      "fc layer 2 self.abs_max_out: 759.0\n",
      "lif layer 2 self.abs_max_v: 1219.0\n",
      "lif layer 2 self.abs_max_v: 1229.5\n",
      "fc layer 2 self.abs_max_out: 807.0\n",
      "lif layer 2 self.abs_max_v: 1299.5\n",
      "lif layer 2 self.abs_max_v: 1302.0\n",
      "lif layer 2 self.abs_max_v: 1376.0\n",
      "lif layer 1 self.abs_max_v: 1601.0\n",
      "fc layer 3 self.abs_max_out: 230.0\n",
      "fc layer 3 self.abs_max_out: 239.0\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "lif layer 2 self.abs_max_v: 1445.0\n",
      "lif layer 2 self.abs_max_v: 1447.5\n",
      "fc layer 2 self.abs_max_out: 837.0\n",
      "fc layer 1 self.abs_max_out: 1237.0\n",
      "lif layer 1 self.abs_max_v: 1619.0\n",
      "lif layer 1 self.abs_max_v: 1652.5\n",
      "lif layer 1 self.abs_max_v: 1973.5\n",
      "fc layer 1 self.abs_max_out: 1248.0\n",
      "lif layer 1 self.abs_max_v: 1975.5\n",
      "lif layer 1 self.abs_max_v: 2015.5\n",
      "lif layer 1 self.abs_max_v: 2018.0\n",
      "fc layer 3 self.abs_max_out: 256.0\n",
      "lif layer 1 self.abs_max_v: 2028.0\n",
      "lif layer 1 self.abs_max_v: 2040.5\n",
      "fc layer 1 self.abs_max_out: 1257.0\n",
      "fc layer 1 self.abs_max_out: 1324.0\n",
      "lif layer 1 self.abs_max_v: 2097.0\n",
      "fc layer 1 self.abs_max_out: 1382.0\n",
      "lif layer 1 self.abs_max_v: 2108.5\n",
      "smallest_now_T_val updated: 62\n",
      "smallest_now_T_val updated: 51\n",
      "lif layer 1 self.abs_max_v: 2188.5\n",
      "smallest_now_T_val updated: 50\n",
      "smallest_now_T_val updated: 49\n",
      "smallest_now_T_val updated: 40\n",
      "smallest_now_T_val updated: 25\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.003269/  2.071053, val:  45.42%, val_best:  45.42%, tr:  75.89%, tr_best:  75.89%, epoch time: 45.56 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.3988%\n",
      "layer   2  Sparsity: 79.5371%\n",
      "layer   3  Sparsity: 81.6600%\n",
      "total_backward_count 4895 real_backward_count 2011  41.083%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 1406.0\n",
      "fc layer 1 self.abs_max_out: 1579.0\n",
      "lif layer 1 self.abs_max_v: 2282.0\n",
      "lif layer 1 self.abs_max_v: 2504.0\n",
      "fc layer 3 self.abs_max_out: 263.0\n",
      "fc layer 3 self.abs_max_out: 266.0\n",
      "fc layer 3 self.abs_max_out: 267.0\n",
      "fc layer 3 self.abs_max_out: 288.0\n",
      "lif layer 1 self.abs_max_v: 2536.5\n",
      "lif layer 1 self.abs_max_v: 2589.5\n",
      "lif layer 1 self.abs_max_v: 2738.0\n",
      "fc layer 1 self.abs_max_out: 1724.0\n",
      "lif layer 1 self.abs_max_v: 2770.5\n",
      "fc layer 1 self.abs_max_out: 1868.0\n",
      "lif layer 1 self.abs_max_v: 3253.5\n",
      "lif layer 1 self.abs_max_v: 3277.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.960887/  2.077066, val:  46.67%, val_best:  46.67%, tr:  86.62%, tr_best:  86.62%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4084%\n",
      "layer   2  Sparsity: 83.8258%\n",
      "layer   3  Sparsity: 85.2227%\n",
      "total_backward_count 9790 real_backward_count 3471  35.455%\n",
      "fc layer 3 self.abs_max_out: 300.0\n",
      "fc layer 1 self.abs_max_out: 1946.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.961302/  2.032232, val:  50.42%, val_best:  50.42%, tr:  88.56%, tr_best:  88.56%, epoch time: 45.51 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.4078%\n",
      "layer   2  Sparsity: 84.5705%\n",
      "layer   3  Sparsity: 84.9217%\n",
      "total_backward_count 14685 real_backward_count 4802  32.700%\n",
      "fc layer 3 self.abs_max_out: 301.0\n",
      "fc layer 3 self.abs_max_out: 306.0\n",
      "fc layer 3 self.abs_max_out: 336.0\n",
      "fc layer 1 self.abs_max_out: 1992.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.947811/  2.025834, val:  43.33%, val_best:  50.42%, tr:  89.79%, tr_best:  89.79%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3874%\n",
      "layer   2  Sparsity: 85.5409%\n",
      "layer   3  Sparsity: 84.8298%\n",
      "total_backward_count 19580 real_backward_count 6144  31.379%\n",
      "lif layer 1 self.abs_max_v: 3427.5\n",
      "fc layer 3 self.abs_max_out: 353.0\n",
      "lif layer 1 self.abs_max_v: 3561.0\n",
      "fc layer 1 self.abs_max_out: 2106.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.942769/  2.002943, val:  55.83%, val_best:  55.83%, tr:  91.32%, tr_best:  91.32%, epoch time: 45.14 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4378%\n",
      "layer   2  Sparsity: 85.0152%\n",
      "layer   3  Sparsity: 85.8663%\n",
      "total_backward_count 24475 real_backward_count 7363  30.084%\n",
      "fc layer 1 self.abs_max_out: 2188.0\n",
      "lif layer 1 self.abs_max_v: 3582.5\n",
      "lif layer 1 self.abs_max_v: 3674.5\n",
      "fc layer 1 self.abs_max_out: 2223.0\n",
      "fc layer 1 self.abs_max_out: 2230.0\n",
      "lif layer 1 self.abs_max_v: 3696.0\n",
      "lif layer 1 self.abs_max_v: 3723.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.917610/  2.035950, val:  46.67%, val_best:  55.83%, tr:  91.11%, tr_best:  91.32%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4186%\n",
      "layer   2  Sparsity: 84.9358%\n",
      "layer   3  Sparsity: 83.4704%\n",
      "total_backward_count 29370 real_backward_count 8551  29.115%\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.933477/  2.028095, val:  49.58%, val_best:  55.83%, tr:  94.08%, tr_best:  94.08%, epoch time: 45.03 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4401%\n",
      "layer   2  Sparsity: 85.1095%\n",
      "layer   3  Sparsity: 84.2280%\n",
      "total_backward_count 34265 real_backward_count 9675  28.236%\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.932622/  2.015284, val:  49.58%, val_best:  55.83%, tr:  93.97%, tr_best:  94.08%, epoch time: 43.38 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 82.3862%\n",
      "layer   2  Sparsity: 83.6710%\n",
      "layer   3  Sparsity: 84.0992%\n",
      "total_backward_count 39160 real_backward_count 10751  27.454%\n",
      "lif layer 1 self.abs_max_v: 3792.5\n",
      "lif layer 1 self.abs_max_v: 3825.5\n",
      "lif layer 1 self.abs_max_v: 3837.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.915642/  1.982727, val:  54.17%, val_best:  55.83%, tr:  94.38%, tr_best:  94.38%, epoch time: 43.31 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 82.4034%\n",
      "layer   2  Sparsity: 84.5466%\n",
      "layer   3  Sparsity: 83.5502%\n",
      "total_backward_count 44055 real_backward_count 11799  26.782%\n",
      "fc layer 1 self.abs_max_out: 2260.0\n",
      "fc layer 1 self.abs_max_out: 2289.0\n",
      "lif layer 1 self.abs_max_v: 3910.0\n",
      "fc layer 3 self.abs_max_out: 361.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.872242/  1.988319, val:  53.75%, val_best:  55.83%, tr:  93.16%, tr_best:  94.38%, epoch time: 45.19 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4175%\n",
      "layer   2  Sparsity: 84.4510%\n",
      "layer   3  Sparsity: 83.4637%\n",
      "total_backward_count 48950 real_backward_count 12816  26.182%\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "lif layer 1 self.abs_max_v: 3983.5\n",
      "fc layer 1 self.abs_max_out: 2334.0\n",
      "lif layer 1 self.abs_max_v: 4008.5\n",
      "fc layer 1 self.abs_max_out: 2422.0\n",
      "lif layer 1 self.abs_max_v: 4118.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.879120/  1.981373, val:  52.92%, val_best:  55.83%, tr:  95.20%, tr_best:  95.20%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3973%\n",
      "layer   2  Sparsity: 85.1296%\n",
      "layer   3  Sparsity: 83.2438%\n",
      "total_backward_count 53845 real_backward_count 13780  25.592%\n",
      "fc layer 3 self.abs_max_out: 401.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.871720/  1.969010, val:  51.25%, val_best:  55.83%, tr:  94.48%, tr_best:  95.20%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4340%\n",
      "layer   2  Sparsity: 84.7622%\n",
      "layer   3  Sparsity: 82.8190%\n",
      "total_backward_count 58740 real_backward_count 14737  25.089%\n",
      "fc layer 2 self.abs_max_out: 844.0\n",
      "lif layer 1 self.abs_max_v: 4132.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.862041/  1.978686, val:  51.67%, val_best:  55.83%, tr:  95.10%, tr_best:  95.20%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4205%\n",
      "layer   2  Sparsity: 84.9660%\n",
      "layer   3  Sparsity: 82.8352%\n",
      "total_backward_count 63635 real_backward_count 15690  24.656%\n",
      "fc layer 1 self.abs_max_out: 2469.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.846894/  1.952408, val:  50.83%, val_best:  55.83%, tr:  93.97%, tr_best:  95.20%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3903%\n",
      "layer   2  Sparsity: 85.3911%\n",
      "layer   3  Sparsity: 82.6156%\n",
      "total_backward_count 68530 real_backward_count 16641  24.283%\n",
      "lif layer 1 self.abs_max_v: 4198.5\n",
      "fc layer 2 self.abs_max_out: 871.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.841831/  1.959924, val:  49.58%, val_best:  55.83%, tr:  95.61%, tr_best:  95.61%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4460%\n",
      "layer   2  Sparsity: 85.0030%\n",
      "layer   3  Sparsity: 82.4477%\n",
      "total_backward_count 73425 real_backward_count 17502  23.837%\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.845190/  1.956165, val:  55.83%, val_best:  55.83%, tr:  96.32%, tr_best:  96.32%, epoch time: 44.78 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4200%\n",
      "layer   2  Sparsity: 84.5967%\n",
      "layer   3  Sparsity: 82.4547%\n",
      "total_backward_count 78320 real_backward_count 18369  23.454%\n",
      "lif layer 1 self.abs_max_v: 4206.0\n",
      "lif layer 1 self.abs_max_v: 4274.0\n",
      "lif layer 1 self.abs_max_v: 4347.0\n",
      "fc layer 1 self.abs_max_out: 2491.0\n",
      "lif layer 1 self.abs_max_v: 4378.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.832915/  1.918216, val:  60.83%, val_best:  60.83%, tr:  94.79%, tr_best:  96.32%, epoch time: 45.43 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.3839%\n",
      "layer   2  Sparsity: 84.5831%\n",
      "layer   3  Sparsity: 82.5765%\n",
      "total_backward_count 83215 real_backward_count 19213  23.088%\n",
      "lif layer 1 self.abs_max_v: 4384.5\n",
      "lif layer 1 self.abs_max_v: 4408.5\n",
      "lif layer 1 self.abs_max_v: 4486.5\n",
      "fc layer 1 self.abs_max_out: 2493.0\n",
      "fc layer 1 self.abs_max_out: 2538.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.821574/  1.906401, val:  59.58%, val_best:  60.83%, tr:  95.30%, tr_best:  96.32%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3821%\n",
      "layer   2  Sparsity: 84.9488%\n",
      "layer   3  Sparsity: 81.9408%\n",
      "total_backward_count 88110 real_backward_count 20070  22.778%\n",
      "lif layer 1 self.abs_max_v: 4496.5\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.812991/  1.932491, val:  54.58%, val_best:  60.83%, tr:  96.22%, tr_best:  96.32%, epoch time: 45.13 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4820%\n",
      "layer   2  Sparsity: 85.3372%\n",
      "layer   3  Sparsity: 82.2009%\n",
      "total_backward_count 93005 real_backward_count 20960  22.536%\n",
      "fc layer 3 self.abs_max_out: 410.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.824005/  1.945805, val:  53.33%, val_best:  60.83%, tr:  95.81%, tr_best:  96.32%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4177%\n",
      "layer   2  Sparsity: 85.3086%\n",
      "layer   3  Sparsity: 82.8620%\n",
      "total_backward_count 97900 real_backward_count 21781  22.248%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.818316/  1.919614, val:  58.75%, val_best:  60.83%, tr:  94.89%, tr_best:  96.32%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4014%\n",
      "layer   2  Sparsity: 84.9835%\n",
      "layer   3  Sparsity: 83.4425%\n",
      "total_backward_count 102795 real_backward_count 22574  21.960%\n",
      "lif layer 1 self.abs_max_v: 4529.5\n",
      "fc layer 1 self.abs_max_out: 2556.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.823930/  1.932606, val:  59.58%, val_best:  60.83%, tr:  94.99%, tr_best:  96.32%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3959%\n",
      "layer   2  Sparsity: 85.3451%\n",
      "layer   3  Sparsity: 83.5566%\n",
      "total_backward_count 107690 real_backward_count 23398  21.727%\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.837920/  1.897862, val:  58.33%, val_best:  60.83%, tr:  95.20%, tr_best:  96.32%, epoch time: 44.45 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3809%\n",
      "layer   2  Sparsity: 85.2358%\n",
      "layer   3  Sparsity: 83.9185%\n",
      "total_backward_count 112585 real_backward_count 24250  21.539%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.841251/  1.935567, val:  65.42%, val_best:  65.42%, tr:  94.48%, tr_best:  96.32%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4127%\n",
      "layer   2  Sparsity: 84.8325%\n",
      "layer   3  Sparsity: 84.7074%\n",
      "total_backward_count 117480 real_backward_count 25102  21.367%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.857928/  1.962978, val:  58.75%, val_best:  65.42%, tr:  95.10%, tr_best:  96.32%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4269%\n",
      "layer   2  Sparsity: 85.1683%\n",
      "layer   3  Sparsity: 84.7982%\n",
      "total_backward_count 122375 real_backward_count 25944  21.200%\n",
      "fc layer 2 self.abs_max_out: 886.0\n",
      "fc layer 2 self.abs_max_out: 892.0\n",
      "fc layer 2 self.abs_max_out: 938.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.865789/  1.945982, val:  59.58%, val_best:  65.42%, tr:  94.38%, tr_best:  96.32%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3889%\n",
      "layer   2  Sparsity: 85.2823%\n",
      "layer   3  Sparsity: 85.4013%\n",
      "total_backward_count 127270 real_backward_count 26758  21.025%\n",
      "lif layer 1 self.abs_max_v: 4549.5\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.834569/  1.925483, val:  61.25%, val_best:  65.42%, tr:  96.12%, tr_best:  96.32%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4579%\n",
      "layer   2  Sparsity: 85.0770%\n",
      "layer   3  Sparsity: 84.3668%\n",
      "total_backward_count 132165 real_backward_count 27478  20.791%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.824205/  1.909732, val:  61.67%, val_best:  65.42%, tr:  95.20%, tr_best:  96.32%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4264%\n",
      "layer   2  Sparsity: 84.9048%\n",
      "layer   3  Sparsity: 83.9307%\n",
      "total_backward_count 137060 real_backward_count 28270  20.626%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.836064/  1.934842, val:  68.33%, val_best:  68.33%, tr:  96.12%, tr_best:  96.32%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3960%\n",
      "layer   2  Sparsity: 85.4692%\n",
      "layer   3  Sparsity: 84.4754%\n",
      "total_backward_count 141955 real_backward_count 29014  20.439%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.839841/  1.919112, val:  63.33%, val_best:  68.33%, tr:  96.42%, tr_best:  96.42%, epoch time: 45.00 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4008%\n",
      "layer   2  Sparsity: 85.7614%\n",
      "layer   3  Sparsity: 84.8069%\n",
      "total_backward_count 146850 real_backward_count 29748  20.257%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.830282/  1.936585, val:  61.67%, val_best:  68.33%, tr:  96.02%, tr_best:  96.42%, epoch time: 45.15 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4210%\n",
      "layer   2  Sparsity: 86.0616%\n",
      "layer   3  Sparsity: 85.0131%\n",
      "total_backward_count 151745 real_backward_count 30489  20.092%\n",
      "fc layer 3 self.abs_max_out: 417.0\n",
      "fc layer 1 self.abs_max_out: 2564.0\n",
      "fc layer 1 self.abs_max_out: 2570.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.834327/  1.919979, val:  61.25%, val_best:  68.33%, tr:  95.71%, tr_best:  96.42%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3950%\n",
      "layer   2  Sparsity: 85.9449%\n",
      "layer   3  Sparsity: 84.9406%\n",
      "total_backward_count 156640 real_backward_count 31247  19.948%\n",
      "fc layer 1 self.abs_max_out: 2611.0\n",
      "lif layer 1 self.abs_max_v: 4803.5\n",
      "fc layer 1 self.abs_max_out: 2633.0\n",
      "lif layer 1 self.abs_max_v: 4914.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.820461/  1.912885, val:  61.25%, val_best:  68.33%, tr:  95.91%, tr_best:  96.42%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4353%\n",
      "layer   2  Sparsity: 86.2944%\n",
      "layer   3  Sparsity: 84.6064%\n",
      "total_backward_count 161535 real_backward_count 31958  19.784%\n",
      "fc layer 1 self.abs_max_out: 2688.0\n",
      "fc layer 1 self.abs_max_out: 2866.0\n",
      "lif layer 1 self.abs_max_v: 4952.5\n",
      "lif layer 1 self.abs_max_v: 5021.5\n",
      "lif layer 1 self.abs_max_v: 5158.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.827544/  1.912029, val:  64.58%, val_best:  68.33%, tr:  95.81%, tr_best:  96.42%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4177%\n",
      "layer   2  Sparsity: 86.6466%\n",
      "layer   3  Sparsity: 85.0794%\n",
      "total_backward_count 166430 real_backward_count 32707  19.652%\n",
      "lif layer 1 self.abs_max_v: 5173.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.838410/  1.943374, val:  53.75%, val_best:  68.33%, tr:  95.51%, tr_best:  96.42%, epoch time: 45.22 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4212%\n",
      "layer   2  Sparsity: 86.3861%\n",
      "layer   3  Sparsity: 85.6107%\n",
      "total_backward_count 171325 real_backward_count 33418  19.506%\n",
      "lif layer 2 self.abs_max_v: 1453.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.843966/  1.926103, val:  69.58%, val_best:  69.58%, tr:  96.22%, tr_best:  96.42%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3651%\n",
      "layer   2  Sparsity: 86.2800%\n",
      "layer   3  Sparsity: 85.6885%\n",
      "total_backward_count 176220 real_backward_count 34154  19.381%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.836328/  1.924717, val:  62.50%, val_best:  69.58%, tr:  95.91%, tr_best:  96.42%, epoch time: 45.69 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.4786%\n",
      "layer   2  Sparsity: 86.5242%\n",
      "layer   3  Sparsity: 85.6971%\n",
      "total_backward_count 181115 real_backward_count 34843  19.238%\n",
      "lif layer 1 self.abs_max_v: 5181.5\n",
      "fc layer 3 self.abs_max_out: 422.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.833002/  1.919085, val:  61.67%, val_best:  69.58%, tr:  96.53%, tr_best:  96.53%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3715%\n",
      "layer   2  Sparsity: 86.3285%\n",
      "layer   3  Sparsity: 85.7228%\n",
      "total_backward_count 186010 real_backward_count 35525  19.098%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.841416/  1.917102, val:  71.25%, val_best:  71.25%, tr:  95.71%, tr_best:  96.53%, epoch time: 45.22 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3871%\n",
      "layer   2  Sparsity: 86.6650%\n",
      "layer   3  Sparsity: 85.8240%\n",
      "total_backward_count 190905 real_backward_count 36222  18.974%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.839899/  1.910323, val:  67.08%, val_best:  71.25%, tr:  96.73%, tr_best:  96.73%, epoch time: 41.83 seconds, 0.70 minutes\n",
      "layer   1  Sparsity: 82.4012%\n",
      "layer   2  Sparsity: 86.5633%\n",
      "layer   3  Sparsity: 85.8942%\n",
      "total_backward_count 195800 real_backward_count 36899  18.845%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.825346/  1.922838, val:  57.50%, val_best:  71.25%, tr:  96.02%, tr_best:  96.73%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4394%\n",
      "layer   2  Sparsity: 86.8727%\n",
      "layer   3  Sparsity: 85.3871%\n",
      "total_backward_count 200695 real_backward_count 37564  18.717%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.811999/  1.904183, val:  65.00%, val_best:  71.25%, tr:  96.83%, tr_best:  96.83%, epoch time: 45.09 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4074%\n",
      "layer   2  Sparsity: 86.7646%\n",
      "layer   3  Sparsity: 84.4445%\n",
      "total_backward_count 205590 real_backward_count 38210  18.586%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.810095/  1.900722, val:  64.58%, val_best:  71.25%, tr:  96.53%, tr_best:  96.83%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4446%\n",
      "layer   2  Sparsity: 86.1161%\n",
      "layer   3  Sparsity: 84.6714%\n",
      "total_backward_count 210485 real_backward_count 38877  18.470%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.801437/  1.901518, val:  62.08%, val_best:  71.25%, tr:  96.73%, tr_best:  96.83%, epoch time: 45.19 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4128%\n",
      "layer   2  Sparsity: 85.9518%\n",
      "layer   3  Sparsity: 85.0867%\n",
      "total_backward_count 215380 real_backward_count 39512  18.345%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.809308/  1.898454, val:  71.25%, val_best:  71.25%, tr:  96.94%, tr_best:  96.94%, epoch time: 44.89 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4092%\n",
      "layer   2  Sparsity: 86.7550%\n",
      "layer   3  Sparsity: 85.7534%\n",
      "total_backward_count 220275 real_backward_count 40162  18.233%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.829749/  1.932794, val:  70.42%, val_best:  71.25%, tr:  95.71%, tr_best:  96.94%, epoch time: 45.02 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4407%\n",
      "layer   2  Sparsity: 86.7185%\n",
      "layer   3  Sparsity: 86.1215%\n",
      "total_backward_count 225170 real_backward_count 40790  18.115%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.825402/  1.912737, val:  70.42%, val_best:  71.25%, tr:  96.63%, tr_best:  96.94%, epoch time: 44.37 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4184%\n",
      "layer   2  Sparsity: 86.6320%\n",
      "layer   3  Sparsity: 85.5370%\n",
      "total_backward_count 230065 real_backward_count 41426  18.006%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.822028/  1.900758, val:  69.17%, val_best:  71.25%, tr:  96.83%, tr_best:  96.94%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3897%\n",
      "layer   2  Sparsity: 86.4331%\n",
      "layer   3  Sparsity: 85.3576%\n",
      "total_backward_count 234960 real_backward_count 42031  17.889%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.812255/  1.904731, val:  68.75%, val_best:  71.25%, tr:  96.12%, tr_best:  96.94%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4235%\n",
      "layer   2  Sparsity: 86.2699%\n",
      "layer   3  Sparsity: 85.7326%\n",
      "total_backward_count 239855 real_backward_count 42659  17.785%\n",
      "lif layer 1 self.abs_max_v: 5260.5\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.810508/  1.903204, val:  62.92%, val_best:  71.25%, tr:  96.73%, tr_best:  96.94%, epoch time: 44.35 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3920%\n",
      "layer   2  Sparsity: 86.6888%\n",
      "layer   3  Sparsity: 86.6686%\n",
      "total_backward_count 244750 real_backward_count 43336  17.706%\n",
      "fc layer 1 self.abs_max_out: 2944.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.811019/  1.921005, val:  62.50%, val_best:  71.25%, tr:  96.53%, tr_best:  96.94%, epoch time: 45.02 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4189%\n",
      "layer   2  Sparsity: 86.9424%\n",
      "layer   3  Sparsity: 86.6574%\n",
      "total_backward_count 249645 real_backward_count 43944  17.603%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.819495/  1.922990, val:  67.92%, val_best:  71.25%, tr:  96.12%, tr_best:  96.94%, epoch time: 44.91 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4056%\n",
      "layer   2  Sparsity: 86.9597%\n",
      "layer   3  Sparsity: 86.1171%\n",
      "total_backward_count 254540 real_backward_count 44547  17.501%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.820185/  1.920231, val:  68.75%, val_best:  71.25%, tr:  96.42%, tr_best:  96.94%, epoch time: 45.46 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.4445%\n",
      "layer   2  Sparsity: 87.2904%\n",
      "layer   3  Sparsity: 86.4227%\n",
      "total_backward_count 259435 real_backward_count 45171  17.411%\n",
      "lif layer 2 self.abs_max_v: 1508.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.811823/  1.923324, val:  63.75%, val_best:  71.25%, tr:  96.42%, tr_best:  96.94%, epoch time: 45.18 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4027%\n",
      "layer   2  Sparsity: 87.2175%\n",
      "layer   3  Sparsity: 86.2160%\n",
      "total_backward_count 264330 real_backward_count 45812  17.331%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.815038/  1.915314, val:  69.58%, val_best:  71.25%, tr:  97.04%, tr_best:  97.04%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3930%\n",
      "layer   2  Sparsity: 87.1871%\n",
      "layer   3  Sparsity: 85.9838%\n",
      "total_backward_count 269225 real_backward_count 46400  17.235%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.824527/  1.906238, val:  63.33%, val_best:  71.25%, tr:  96.22%, tr_best:  97.04%, epoch time: 45.11 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4055%\n",
      "layer   2  Sparsity: 86.7918%\n",
      "layer   3  Sparsity: 85.4530%\n",
      "total_backward_count 274120 real_backward_count 47005  17.148%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.800618/  1.899010, val:  71.25%, val_best:  71.25%, tr:  97.96%, tr_best:  97.96%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3935%\n",
      "layer   2  Sparsity: 86.5937%\n",
      "layer   3  Sparsity: 85.3638%\n",
      "total_backward_count 279015 real_backward_count 47565  17.047%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.799344/  1.898958, val:  63.75%, val_best:  71.25%, tr:  97.55%, tr_best:  97.96%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4421%\n",
      "layer   2  Sparsity: 86.9087%\n",
      "layer   3  Sparsity: 85.8568%\n",
      "total_backward_count 283910 real_backward_count 48118  16.948%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.794686/  1.908462, val:  68.33%, val_best:  71.25%, tr:  98.16%, tr_best:  98.16%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4183%\n",
      "layer   2  Sparsity: 86.9832%\n",
      "layer   3  Sparsity: 85.6197%\n",
      "total_backward_count 288805 real_backward_count 48650  16.845%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.804058/  1.912316, val:  66.67%, val_best:  71.25%, tr:  97.34%, tr_best:  98.16%, epoch time: 45.12 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3902%\n",
      "layer   2  Sparsity: 86.9303%\n",
      "layer   3  Sparsity: 85.7088%\n",
      "total_backward_count 293700 real_backward_count 49227  16.761%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.803478/  1.896836, val:  64.58%, val_best:  71.25%, tr:  97.24%, tr_best:  98.16%, epoch time: 44.36 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4670%\n",
      "layer   2  Sparsity: 86.9839%\n",
      "layer   3  Sparsity: 85.3811%\n",
      "total_backward_count 298595 real_backward_count 49784  16.673%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.786664/  1.885473, val:  65.83%, val_best:  71.25%, tr:  97.96%, tr_best:  98.16%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3919%\n",
      "layer   2  Sparsity: 86.9815%\n",
      "layer   3  Sparsity: 85.6162%\n",
      "total_backward_count 303490 real_backward_count 50315  16.579%\n",
      "fc layer 1 self.abs_max_out: 3215.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.788174/  1.898347, val:  68.33%, val_best:  71.25%, tr:  97.24%, tr_best:  98.16%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4254%\n",
      "layer   2  Sparsity: 86.9655%\n",
      "layer   3  Sparsity: 85.6548%\n",
      "total_backward_count 308385 real_backward_count 50864  16.494%\n",
      "lif layer 2 self.abs_max_v: 1534.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.804913/  1.899581, val:  63.75%, val_best:  71.25%, tr:  96.73%, tr_best:  98.16%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3968%\n",
      "layer   2  Sparsity: 87.1275%\n",
      "layer   3  Sparsity: 85.7033%\n",
      "total_backward_count 313280 real_backward_count 51396  16.406%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.793233/  1.909462, val:  70.42%, val_best:  71.25%, tr:  97.14%, tr_best:  98.16%, epoch time: 45.45 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.4355%\n",
      "layer   2  Sparsity: 87.2927%\n",
      "layer   3  Sparsity: 85.5431%\n",
      "total_backward_count 318175 real_backward_count 51918  16.317%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.786595/  1.909763, val:  66.67%, val_best:  71.25%, tr:  97.85%, tr_best:  98.16%, epoch time: 44.30 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4029%\n",
      "layer   2  Sparsity: 87.5982%\n",
      "layer   3  Sparsity: 85.6441%\n",
      "total_backward_count 323070 real_backward_count 52439  16.231%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.790704/  1.890423, val:  71.67%, val_best:  71.67%, tr:  97.45%, tr_best:  98.16%, epoch time: 45.05 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4200%\n",
      "layer   2  Sparsity: 87.7152%\n",
      "layer   3  Sparsity: 85.0251%\n",
      "total_backward_count 327965 real_backward_count 52994  16.158%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.784170/  1.879205, val:  69.17%, val_best:  71.67%, tr:  97.24%, tr_best:  98.16%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4554%\n",
      "layer   2  Sparsity: 87.7389%\n",
      "layer   3  Sparsity: 84.9014%\n",
      "total_backward_count 332860 real_backward_count 53527  16.081%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.764958/  1.879959, val:  63.75%, val_best:  71.67%, tr:  97.55%, tr_best:  98.16%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3912%\n",
      "layer   2  Sparsity: 87.4683%\n",
      "layer   3  Sparsity: 84.7899%\n",
      "total_backward_count 337755 real_backward_count 54001  15.988%\n",
      "lif layer 1 self.abs_max_v: 5266.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.766684/  1.879357, val:  67.92%, val_best:  71.67%, tr:  97.65%, tr_best:  98.16%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3923%\n",
      "layer   2  Sparsity: 87.1856%\n",
      "layer   3  Sparsity: 84.7960%\n",
      "total_backward_count 342650 real_backward_count 54524  15.912%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.765984/  1.869483, val:  68.33%, val_best:  71.67%, tr:  97.96%, tr_best:  98.16%, epoch time: 45.11 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4260%\n",
      "layer   2  Sparsity: 87.2258%\n",
      "layer   3  Sparsity: 84.6829%\n",
      "total_backward_count 347545 real_backward_count 54994  15.824%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.741342/  1.861645, val:  67.50%, val_best:  71.67%, tr:  97.55%, tr_best:  98.16%, epoch time: 45.03 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3907%\n",
      "layer   2  Sparsity: 87.3674%\n",
      "layer   3  Sparsity: 84.6509%\n",
      "total_backward_count 352440 real_backward_count 55488  15.744%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.737407/  1.851650, val:  70.00%, val_best:  71.67%, tr:  98.47%, tr_best:  98.47%, epoch time: 45.40 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.4335%\n",
      "layer   2  Sparsity: 87.3874%\n",
      "layer   3  Sparsity: 84.8326%\n",
      "total_backward_count 357335 real_backward_count 56001  15.672%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.725150/  1.838128, val:  65.00%, val_best:  71.67%, tr:  97.96%, tr_best:  98.47%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3831%\n",
      "layer   2  Sparsity: 87.4167%\n",
      "layer   3  Sparsity: 84.8313%\n",
      "total_backward_count 362230 real_backward_count 56484  15.593%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.741389/  1.871126, val:  68.75%, val_best:  71.67%, tr:  97.55%, tr_best:  98.47%, epoch time: 45.57 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.4140%\n",
      "layer   2  Sparsity: 87.6279%\n",
      "layer   3  Sparsity: 84.7317%\n",
      "total_backward_count 367125 real_backward_count 57023  15.532%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.736001/  1.855862, val:  71.67%, val_best:  71.67%, tr:  97.24%, tr_best:  98.47%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4024%\n",
      "layer   2  Sparsity: 87.4345%\n",
      "layer   3  Sparsity: 83.8950%\n",
      "total_backward_count 372020 real_backward_count 57508  15.458%\n",
      "lif layer 2 self.abs_max_v: 1549.5\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.745109/  1.852911, val:  71.67%, val_best:  71.67%, tr:  98.16%, tr_best:  98.47%, epoch time: 44.47 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3814%\n",
      "layer   2  Sparsity: 87.3359%\n",
      "layer   3  Sparsity: 84.0183%\n",
      "total_backward_count 376915 real_backward_count 57994  15.386%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.724290/  1.853945, val:  70.42%, val_best:  71.67%, tr:  97.24%, tr_best:  98.47%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3815%\n",
      "layer   2  Sparsity: 87.6180%\n",
      "layer   3  Sparsity: 83.9593%\n",
      "total_backward_count 381810 real_backward_count 58451  15.309%\n",
      "lif layer 2 self.abs_max_v: 1578.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.720151/  1.848871, val:  67.08%, val_best:  71.67%, tr:  97.96%, tr_best:  98.47%, epoch time: 44.97 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4558%\n",
      "layer   2  Sparsity: 87.2283%\n",
      "layer   3  Sparsity: 83.5178%\n",
      "total_backward_count 386705 real_backward_count 58944  15.243%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.716811/  1.844437, val:  65.83%, val_best:  71.67%, tr:  97.55%, tr_best:  98.47%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4175%\n",
      "layer   2  Sparsity: 86.7764%\n",
      "layer   3  Sparsity: 83.3491%\n",
      "total_backward_count 391600 real_backward_count 59417  15.173%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.712170/  1.852133, val:  71.25%, val_best:  71.67%, tr:  98.47%, tr_best:  98.47%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4144%\n",
      "layer   2  Sparsity: 86.6409%\n",
      "layer   3  Sparsity: 83.1841%\n",
      "total_backward_count 396495 real_backward_count 59875  15.101%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.705187/  1.843970, val:  62.08%, val_best:  71.67%, tr:  97.96%, tr_best:  98.47%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4293%\n",
      "layer   2  Sparsity: 86.9889%\n",
      "layer   3  Sparsity: 83.5434%\n",
      "total_backward_count 401390 real_backward_count 60321  15.028%\n",
      "lif layer 2 self.abs_max_v: 1633.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.716690/  1.841329, val:  69.17%, val_best:  71.67%, tr:  97.24%, tr_best:  98.47%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4456%\n",
      "layer   2  Sparsity: 87.0031%\n",
      "layer   3  Sparsity: 83.6367%\n",
      "total_backward_count 406285 real_backward_count 60775  14.959%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.727326/  1.855119, val:  78.33%, val_best:  78.33%, tr:  98.37%, tr_best:  98.47%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4298%\n",
      "layer   2  Sparsity: 86.9596%\n",
      "layer   3  Sparsity: 83.3425%\n",
      "total_backward_count 411180 real_backward_count 61222  14.889%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.723271/  1.837499, val:  75.42%, val_best:  78.33%, tr:  98.26%, tr_best:  98.47%, epoch time: 44.71 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3911%\n",
      "layer   2  Sparsity: 86.8628%\n",
      "layer   3  Sparsity: 83.0588%\n",
      "total_backward_count 416075 real_backward_count 61664  14.820%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.706621/  1.831568, val:  72.92%, val_best:  78.33%, tr:  98.57%, tr_best:  98.57%, epoch time: 44.45 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4639%\n",
      "layer   2  Sparsity: 86.8882%\n",
      "layer   3  Sparsity: 82.5787%\n",
      "total_backward_count 420970 real_backward_count 62063  14.743%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.699361/  1.817343, val:  75.83%, val_best:  78.33%, tr:  97.96%, tr_best:  98.57%, epoch time: 45.17 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4539%\n",
      "layer   2  Sparsity: 86.9947%\n",
      "layer   3  Sparsity: 82.7946%\n",
      "total_backward_count 425865 real_backward_count 62499  14.676%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.693631/  1.823743, val:  70.83%, val_best:  78.33%, tr:  97.65%, tr_best:  98.57%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4004%\n",
      "layer   2  Sparsity: 87.1516%\n",
      "layer   3  Sparsity: 83.2165%\n",
      "total_backward_count 430760 real_backward_count 62925  14.608%\n",
      "fc layer 3 self.abs_max_out: 446.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.683986/  1.806115, val:  71.25%, val_best:  78.33%, tr:  98.67%, tr_best:  98.67%, epoch time: 45.01 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3759%\n",
      "layer   2  Sparsity: 87.3150%\n",
      "layer   3  Sparsity: 82.9406%\n",
      "total_backward_count 435655 real_backward_count 63368  14.545%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.675065/  1.805085, val:  70.42%, val_best:  78.33%, tr:  97.24%, tr_best:  98.67%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4335%\n",
      "layer   2  Sparsity: 87.1052%\n",
      "layer   3  Sparsity: 83.0144%\n",
      "total_backward_count 440550 real_backward_count 63828  14.488%\n",
      "fc layer 3 self.abs_max_out: 452.0\n",
      "fc layer 2 self.abs_max_out: 972.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.682442/  1.811359, val:  72.50%, val_best:  78.33%, tr:  98.26%, tr_best:  98.67%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4368%\n",
      "layer   2  Sparsity: 87.2171%\n",
      "layer   3  Sparsity: 83.2174%\n",
      "total_backward_count 445445 real_backward_count 64253  14.424%\n",
      "fc layer 3 self.abs_max_out: 477.0\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.671511/  1.793829, val:  70.42%, val_best:  78.33%, tr:  97.75%, tr_best:  98.67%, epoch time: 45.36 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 82.3883%\n",
      "layer   2  Sparsity: 87.3608%\n",
      "layer   3  Sparsity: 83.3825%\n",
      "total_backward_count 450340 real_backward_count 64709  14.369%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.675301/  1.799840, val:  70.42%, val_best:  78.33%, tr:  97.45%, tr_best:  98.67%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4240%\n",
      "layer   2  Sparsity: 87.4598%\n",
      "layer   3  Sparsity: 84.0853%\n",
      "total_backward_count 455235 real_backward_count 65177  14.317%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.672651/  1.797905, val:  72.50%, val_best:  78.33%, tr:  98.06%, tr_best:  98.67%, epoch time: 45.04 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3819%\n",
      "layer   2  Sparsity: 87.3818%\n",
      "layer   3  Sparsity: 84.1063%\n",
      "total_backward_count 460130 real_backward_count 65623  14.262%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.662795/  1.799857, val:  64.17%, val_best:  78.33%, tr:  98.37%, tr_best:  98.67%, epoch time: 44.06 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.4573%\n",
      "layer   2  Sparsity: 87.5079%\n",
      "layer   3  Sparsity: 84.0419%\n",
      "total_backward_count 465025 real_backward_count 66020  14.197%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.672691/  1.810787, val:  71.67%, val_best:  78.33%, tr:  98.06%, tr_best:  98.67%, epoch time: 44.70 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3552%\n",
      "layer   2  Sparsity: 87.4204%\n",
      "layer   3  Sparsity: 84.2107%\n",
      "total_backward_count 469920 real_backward_count 66454  14.142%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.682724/  1.824095, val:  69.58%, val_best:  78.33%, tr:  98.57%, tr_best:  98.67%, epoch time: 43.87 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.4171%\n",
      "layer   2  Sparsity: 87.9803%\n",
      "layer   3  Sparsity: 84.6372%\n",
      "total_backward_count 474815 real_backward_count 66887  14.087%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.680096/  1.822714, val:  73.75%, val_best:  78.33%, tr:  98.57%, tr_best:  98.67%, epoch time: 44.91 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4334%\n",
      "layer   2  Sparsity: 88.0783%\n",
      "layer   3  Sparsity: 84.4779%\n",
      "total_backward_count 479710 real_backward_count 67311  14.032%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.680600/  1.818641, val:  74.17%, val_best:  78.33%, tr:  98.37%, tr_best:  98.67%, epoch time: 44.20 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4291%\n",
      "layer   2  Sparsity: 88.0502%\n",
      "layer   3  Sparsity: 84.5379%\n",
      "total_backward_count 484605 real_backward_count 67710  13.972%\n",
      "fc layer 1 self.abs_max_out: 3281.0\n",
      "lif layer 2 self.abs_max_v: 1636.5\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.684935/  1.821222, val:  70.00%, val_best:  78.33%, tr:  98.47%, tr_best:  98.67%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3883%\n",
      "layer   2  Sparsity: 88.1298%\n",
      "layer   3  Sparsity: 84.5278%\n",
      "total_backward_count 489500 real_backward_count 68096  13.911%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.678144/  1.813834, val:  71.25%, val_best:  78.33%, tr:  97.75%, tr_best:  98.67%, epoch time: 44.19 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3715%\n",
      "layer   2  Sparsity: 88.0289%\n",
      "layer   3  Sparsity: 84.0488%\n",
      "total_backward_count 494395 real_backward_count 68523  13.860%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.677642/  1.811958, val:  69.58%, val_best:  78.33%, tr:  98.16%, tr_best:  98.67%, epoch time: 44.29 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4313%\n",
      "layer   2  Sparsity: 87.9164%\n",
      "layer   3  Sparsity: 84.2176%\n",
      "total_backward_count 499290 real_backward_count 68926  13.805%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.681376/  1.811178, val:  67.92%, val_best:  78.33%, tr:  98.77%, tr_best:  98.77%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3617%\n",
      "layer   2  Sparsity: 87.9133%\n",
      "layer   3  Sparsity: 84.4995%\n",
      "total_backward_count 504185 real_backward_count 69313  13.748%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.682025/  1.823022, val:  68.75%, val_best:  78.33%, tr:  98.26%, tr_best:  98.77%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3605%\n",
      "layer   2  Sparsity: 88.2912%\n",
      "layer   3  Sparsity: 84.8539%\n",
      "total_backward_count 509080 real_backward_count 69683  13.688%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.678095/  1.810790, val:  71.25%, val_best:  78.33%, tr:  98.77%, tr_best:  98.77%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4107%\n",
      "layer   2  Sparsity: 88.2408%\n",
      "layer   3  Sparsity: 84.5724%\n",
      "total_backward_count 513975 real_backward_count 70074  13.634%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.675128/  1.809678, val:  72.08%, val_best:  78.33%, tr:  97.75%, tr_best:  98.77%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4085%\n",
      "layer   2  Sparsity: 88.2945%\n",
      "layer   3  Sparsity: 84.7875%\n",
      "total_backward_count 518870 real_backward_count 70463  13.580%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.672354/  1.806468, val:  69.17%, val_best:  78.33%, tr:  97.96%, tr_best:  98.77%, epoch time: 44.17 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4327%\n",
      "layer   2  Sparsity: 88.5291%\n",
      "layer   3  Sparsity: 84.6002%\n",
      "total_backward_count 523765 real_backward_count 70853  13.528%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.681297/  1.818716, val:  66.25%, val_best:  78.33%, tr:  98.47%, tr_best:  98.77%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3905%\n",
      "layer   2  Sparsity: 88.6349%\n",
      "layer   3  Sparsity: 84.8624%\n",
      "total_backward_count 528660 real_backward_count 71236  13.475%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.688585/  1.811933, val:  70.42%, val_best:  78.33%, tr:  98.37%, tr_best:  98.77%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4199%\n",
      "layer   2  Sparsity: 88.6826%\n",
      "layer   3  Sparsity: 85.2081%\n",
      "total_backward_count 533555 real_backward_count 71603  13.420%\n",
      "lif layer 1 self.abs_max_v: 5330.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.689015/  1.810291, val:  75.00%, val_best:  78.33%, tr:  98.88%, tr_best:  98.88%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4325%\n",
      "layer   2  Sparsity: 88.6203%\n",
      "layer   3  Sparsity: 85.1024%\n",
      "total_backward_count 538450 real_backward_count 71963  13.365%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.685258/  1.807545, val:  70.42%, val_best:  78.33%, tr:  97.55%, tr_best:  98.88%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4513%\n",
      "layer   2  Sparsity: 88.7463%\n",
      "layer   3  Sparsity: 84.8909%\n",
      "total_backward_count 543345 real_backward_count 72347  13.315%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.681080/  1.800403, val:  71.25%, val_best:  78.33%, tr:  98.77%, tr_best:  98.88%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4114%\n",
      "layer   2  Sparsity: 88.6354%\n",
      "layer   3  Sparsity: 85.0740%\n",
      "total_backward_count 548240 real_backward_count 72746  13.269%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.683139/  1.813982, val:  66.25%, val_best:  78.33%, tr:  98.47%, tr_best:  98.88%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4271%\n",
      "layer   2  Sparsity: 88.5785%\n",
      "layer   3  Sparsity: 85.0446%\n",
      "total_backward_count 553135 real_backward_count 73153  13.225%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.682994/  1.800498, val:  72.50%, val_best:  78.33%, tr:  99.18%, tr_best:  99.18%, epoch time: 44.36 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3730%\n",
      "layer   2  Sparsity: 88.3678%\n",
      "layer   3  Sparsity: 84.8858%\n",
      "total_backward_count 558030 real_backward_count 73527  13.176%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.663224/  1.790252, val:  76.67%, val_best:  78.33%, tr:  97.96%, tr_best:  99.18%, epoch time: 44.33 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4302%\n",
      "layer   2  Sparsity: 88.1019%\n",
      "layer   3  Sparsity: 84.7211%\n",
      "total_backward_count 562925 real_backward_count 73900  13.128%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.659932/  1.799420, val:  74.58%, val_best:  78.33%, tr:  98.98%, tr_best:  99.18%, epoch time: 43.97 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.4435%\n",
      "layer   2  Sparsity: 87.9936%\n",
      "layer   3  Sparsity: 84.8902%\n",
      "total_backward_count 567820 real_backward_count 74229  13.073%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.663918/  1.798188, val:  70.00%, val_best:  78.33%, tr:  98.88%, tr_best:  99.18%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4302%\n",
      "layer   2  Sparsity: 88.0045%\n",
      "layer   3  Sparsity: 84.9187%\n",
      "total_backward_count 572715 real_backward_count 74583  13.023%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.664448/  1.798335, val:  75.42%, val_best:  78.33%, tr:  98.16%, tr_best:  99.18%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3963%\n",
      "layer   2  Sparsity: 88.2721%\n",
      "layer   3  Sparsity: 84.7153%\n",
      "total_backward_count 577610 real_backward_count 74946  12.975%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.667648/  1.800891, val:  72.08%, val_best:  78.33%, tr:  98.67%, tr_best:  99.18%, epoch time: 45.00 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3821%\n",
      "layer   2  Sparsity: 88.4514%\n",
      "layer   3  Sparsity: 84.8663%\n",
      "total_backward_count 582505 real_backward_count 75301  12.927%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.665487/  1.789922, val:  75.83%, val_best:  78.33%, tr:  98.37%, tr_best:  99.18%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3693%\n",
      "layer   2  Sparsity: 88.4546%\n",
      "layer   3  Sparsity: 84.8797%\n",
      "total_backward_count 587400 real_backward_count 75660  12.880%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.659152/  1.802936, val:  78.33%, val_best:  78.33%, tr:  97.96%, tr_best:  99.18%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3914%\n",
      "layer   2  Sparsity: 88.5064%\n",
      "layer   3  Sparsity: 84.9597%\n",
      "total_backward_count 592295 real_backward_count 75972  12.827%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.668354/  1.803373, val:  72.50%, val_best:  78.33%, tr:  98.57%, tr_best:  99.18%, epoch time: 44.15 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4430%\n",
      "layer   2  Sparsity: 88.7663%\n",
      "layer   3  Sparsity: 85.0128%\n",
      "total_backward_count 597190 real_backward_count 76349  12.785%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.661943/  1.800821, val:  74.17%, val_best:  78.33%, tr:  98.26%, tr_best:  99.18%, epoch time: 44.30 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4764%\n",
      "layer   2  Sparsity: 88.8534%\n",
      "layer   3  Sparsity: 84.9127%\n",
      "total_backward_count 602085 real_backward_count 76731  12.744%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.673055/  1.811161, val:  73.75%, val_best:  78.33%, tr:  98.67%, tr_best:  99.18%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4226%\n",
      "layer   2  Sparsity: 88.8650%\n",
      "layer   3  Sparsity: 85.0960%\n",
      "total_backward_count 606980 real_backward_count 77067  12.697%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.673376/  1.792742, val:  74.17%, val_best:  78.33%, tr:  98.47%, tr_best:  99.18%, epoch time: 44.59 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4366%\n",
      "layer   2  Sparsity: 88.8725%\n",
      "layer   3  Sparsity: 85.1133%\n",
      "total_backward_count 611875 real_backward_count 77427  12.654%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.661115/  1.797817, val:  73.75%, val_best:  78.33%, tr:  97.75%, tr_best:  99.18%, epoch time: 44.71 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3559%\n",
      "layer   2  Sparsity: 88.5610%\n",
      "layer   3  Sparsity: 84.7693%\n",
      "total_backward_count 616770 real_backward_count 77799  12.614%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.659538/  1.792271, val:  70.83%, val_best:  78.33%, tr:  98.47%, tr_best:  99.18%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4021%\n",
      "layer   2  Sparsity: 88.6811%\n",
      "layer   3  Sparsity: 84.6208%\n",
      "total_backward_count 621665 real_backward_count 78165  12.573%\n",
      "lif layer 1 self.abs_max_v: 5428.0\n",
      "lif layer 1 self.abs_max_v: 5450.0\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.657551/  1.798450, val:  73.33%, val_best:  78.33%, tr:  98.57%, tr_best:  99.18%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3908%\n",
      "layer   2  Sparsity: 88.7161%\n",
      "layer   3  Sparsity: 84.8897%\n",
      "total_backward_count 626560 real_backward_count 78514  12.531%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.651703/  1.786469, val:  76.25%, val_best:  78.33%, tr:  98.57%, tr_best:  99.18%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4529%\n",
      "layer   2  Sparsity: 88.8207%\n",
      "layer   3  Sparsity: 85.0598%\n",
      "total_backward_count 631455 real_backward_count 78833  12.484%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.649237/  1.794201, val:  72.92%, val_best:  78.33%, tr:  98.77%, tr_best:  99.18%, epoch time: 43.88 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.4017%\n",
      "layer   2  Sparsity: 89.1845%\n",
      "layer   3  Sparsity: 85.1351%\n",
      "total_backward_count 636350 real_backward_count 79152  12.438%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.662394/  1.787007, val:  71.25%, val_best:  78.33%, tr:  98.47%, tr_best:  99.18%, epoch time: 44.19 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4377%\n",
      "layer   2  Sparsity: 89.3817%\n",
      "layer   3  Sparsity: 85.2391%\n",
      "total_backward_count 641245 real_backward_count 79505  12.399%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.656668/  1.809818, val:  72.50%, val_best:  78.33%, tr:  97.55%, tr_best:  99.18%, epoch time: 44.10 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4663%\n",
      "layer   2  Sparsity: 89.1521%\n",
      "layer   3  Sparsity: 85.3846%\n",
      "total_backward_count 646140 real_backward_count 79868  12.361%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.670043/  1.809869, val:  72.08%, val_best:  78.33%, tr:  98.16%, tr_best:  99.18%, epoch time: 44.43 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4094%\n",
      "layer   2  Sparsity: 89.1787%\n",
      "layer   3  Sparsity: 85.4094%\n",
      "total_backward_count 651035 real_backward_count 80177  12.315%\n",
      "lif layer 1 self.abs_max_v: 5475.5\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.660957/  1.788671, val:  70.83%, val_best:  78.33%, tr:  99.08%, tr_best:  99.18%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4079%\n",
      "layer   2  Sparsity: 88.8873%\n",
      "layer   3  Sparsity: 84.7681%\n",
      "total_backward_count 655930 real_backward_count 80528  12.277%\n",
      "lif layer 1 self.abs_max_v: 5564.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.669614/  1.796211, val:  71.67%, val_best:  78.33%, tr:  97.96%, tr_best:  99.18%, epoch time: 44.47 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4286%\n",
      "layer   2  Sparsity: 88.9121%\n",
      "layer   3  Sparsity: 85.1261%\n",
      "total_backward_count 660825 real_backward_count 80880  12.239%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.658168/  1.793380, val:  72.92%, val_best:  78.33%, tr:  98.47%, tr_best:  99.18%, epoch time: 45.01 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4217%\n",
      "layer   2  Sparsity: 88.8942%\n",
      "layer   3  Sparsity: 84.6870%\n",
      "total_backward_count 665720 real_backward_count 81242  12.204%\n",
      "lif layer 1 self.abs_max_v: 5598.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.657842/  1.776498, val:  79.17%, val_best:  79.17%, tr:  98.98%, tr_best:  99.18%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4236%\n",
      "layer   2  Sparsity: 89.0275%\n",
      "layer   3  Sparsity: 84.6354%\n",
      "total_backward_count 670615 real_backward_count 81586  12.166%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.656335/  1.784010, val:  71.67%, val_best:  79.17%, tr:  98.57%, tr_best:  99.18%, epoch time: 44.47 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4196%\n",
      "layer   2  Sparsity: 88.9103%\n",
      "layer   3  Sparsity: 84.4578%\n",
      "total_backward_count 675510 real_backward_count 81918  12.127%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.660580/  1.788563, val:  74.58%, val_best:  79.17%, tr:  99.18%, tr_best:  99.18%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3845%\n",
      "layer   2  Sparsity: 88.9773%\n",
      "layer   3  Sparsity: 84.4712%\n",
      "total_backward_count 680405 real_backward_count 82261  12.090%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.655082/  1.785560, val:  71.25%, val_best:  79.17%, tr:  99.08%, tr_best:  99.18%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4279%\n",
      "layer   2  Sparsity: 88.9323%\n",
      "layer   3  Sparsity: 84.6783%\n",
      "total_backward_count 685300 real_backward_count 82603  12.054%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.657852/  1.788743, val:  78.33%, val_best:  79.17%, tr:  99.08%, tr_best:  99.18%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4442%\n",
      "layer   2  Sparsity: 89.2590%\n",
      "layer   3  Sparsity: 84.9494%\n",
      "total_backward_count 690195 real_backward_count 82946  12.018%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.667667/  1.800851, val:  76.67%, val_best:  79.17%, tr:  98.77%, tr_best:  99.18%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4266%\n",
      "layer   2  Sparsity: 89.3802%\n",
      "layer   3  Sparsity: 85.0206%\n",
      "total_backward_count 695090 real_backward_count 83255  11.978%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.669751/  1.791750, val:  75.42%, val_best:  79.17%, tr:  98.67%, tr_best:  99.18%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4793%\n",
      "layer   2  Sparsity: 89.2669%\n",
      "layer   3  Sparsity: 84.8231%\n",
      "total_backward_count 699985 real_backward_count 83595  11.942%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.661211/  1.795812, val:  79.17%, val_best:  79.17%, tr:  99.18%, tr_best:  99.18%, epoch time: 45.14 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3977%\n",
      "layer   2  Sparsity: 89.2308%\n",
      "layer   3  Sparsity: 84.5705%\n",
      "total_backward_count 704880 real_backward_count 83912  11.904%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.661428/  1.804097, val:  73.75%, val_best:  79.17%, tr:  98.57%, tr_best:  99.18%, epoch time: 44.54 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3970%\n",
      "layer   2  Sparsity: 89.0441%\n",
      "layer   3  Sparsity: 84.7266%\n",
      "total_backward_count 709775 real_backward_count 84225  11.866%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.651754/  1.781679, val:  66.67%, val_best:  79.17%, tr:  98.37%, tr_best:  99.18%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4151%\n",
      "layer   2  Sparsity: 89.1031%\n",
      "layer   3  Sparsity: 84.7212%\n",
      "total_backward_count 714670 real_backward_count 84519  11.826%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.653798/  1.790140, val:  78.33%, val_best:  79.17%, tr:  98.77%, tr_best:  99.18%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4144%\n",
      "layer   2  Sparsity: 89.3092%\n",
      "layer   3  Sparsity: 84.9395%\n",
      "total_backward_count 719565 real_backward_count 84865  11.794%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.650877/  1.776262, val:  72.08%, val_best:  79.17%, tr:  98.98%, tr_best:  99.18%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4316%\n",
      "layer   2  Sparsity: 89.3594%\n",
      "layer   3  Sparsity: 84.9820%\n",
      "total_backward_count 724460 real_backward_count 85141  11.752%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.647497/  1.772143, val:  70.42%, val_best:  79.17%, tr:  98.57%, tr_best:  99.18%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4154%\n",
      "layer   2  Sparsity: 89.3031%\n",
      "layer   3  Sparsity: 84.5835%\n",
      "total_backward_count 729355 real_backward_count 85461  11.717%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.635262/  1.772615, val:  74.17%, val_best:  79.17%, tr:  98.98%, tr_best:  99.18%, epoch time: 44.37 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3871%\n",
      "layer   2  Sparsity: 89.3564%\n",
      "layer   3  Sparsity: 84.7657%\n",
      "total_backward_count 734250 real_backward_count 85765  11.681%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.635105/  1.780759, val:  79.17%, val_best:  79.17%, tr:  99.08%, tr_best:  99.18%, epoch time: 43.95 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.4315%\n",
      "layer   2  Sparsity: 89.3881%\n",
      "layer   3  Sparsity: 84.7112%\n",
      "total_backward_count 739145 real_backward_count 86078  11.646%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.639155/  1.788961, val:  66.67%, val_best:  79.17%, tr:  99.39%, tr_best:  99.39%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4497%\n",
      "layer   2  Sparsity: 89.4281%\n",
      "layer   3  Sparsity: 84.8120%\n",
      "total_backward_count 744040 real_backward_count 86398  11.612%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.637436/  1.771476, val:  75.00%, val_best:  79.17%, tr:  98.77%, tr_best:  99.39%, epoch time: 44.11 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4088%\n",
      "layer   2  Sparsity: 89.2092%\n",
      "layer   3  Sparsity: 84.5469%\n",
      "total_backward_count 748935 real_backward_count 86692  11.575%\n",
      "lif layer 1 self.abs_max_v: 5652.5\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.631320/  1.767325, val:  71.25%, val_best:  79.17%, tr:  98.67%, tr_best:  99.39%, epoch time: 44.18 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4481%\n",
      "layer   2  Sparsity: 89.4533%\n",
      "layer   3  Sparsity: 84.5032%\n",
      "total_backward_count 753830 real_backward_count 86991  11.540%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.620408/  1.755265, val:  76.67%, val_best:  79.17%, tr:  98.67%, tr_best:  99.39%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4132%\n",
      "layer   2  Sparsity: 89.5273%\n",
      "layer   3  Sparsity: 84.4471%\n",
      "total_backward_count 758725 real_backward_count 87304  11.507%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.616673/  1.763091, val:  71.67%, val_best:  79.17%, tr:  98.98%, tr_best:  99.39%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4197%\n",
      "layer   2  Sparsity: 89.5495%\n",
      "layer   3  Sparsity: 84.4383%\n",
      "total_backward_count 763620 real_backward_count 87590  11.470%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.613410/  1.761631, val:  67.92%, val_best:  79.17%, tr:  98.88%, tr_best:  99.39%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4462%\n",
      "layer   2  Sparsity: 89.4610%\n",
      "layer   3  Sparsity: 84.4486%\n",
      "total_backward_count 768515 real_backward_count 87880  11.435%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.615830/  1.768190, val:  74.58%, val_best:  79.17%, tr:  98.57%, tr_best:  99.39%, epoch time: 44.36 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4262%\n",
      "layer   2  Sparsity: 89.1516%\n",
      "layer   3  Sparsity: 84.3970%\n",
      "total_backward_count 773410 real_backward_count 88160  11.399%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.616167/  1.764222, val:  79.58%, val_best:  79.58%, tr:  98.37%, tr_best:  99.39%, epoch time: 44.10 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.4181%\n",
      "layer   2  Sparsity: 88.9675%\n",
      "layer   3  Sparsity: 84.0946%\n",
      "total_backward_count 778305 real_backward_count 88461  11.366%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.620286/  1.772760, val:  70.83%, val_best:  79.58%, tr:  98.98%, tr_best:  99.39%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4378%\n",
      "layer   2  Sparsity: 88.8774%\n",
      "layer   3  Sparsity: 84.3277%\n",
      "total_backward_count 783200 real_backward_count 88759  11.333%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.618417/  1.763651, val:  70.00%, val_best:  79.58%, tr:  99.39%, tr_best:  99.39%, epoch time: 44.16 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3808%\n",
      "layer   2  Sparsity: 88.8025%\n",
      "layer   3  Sparsity: 84.0577%\n",
      "total_backward_count 788095 real_backward_count 89059  11.301%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.619261/  1.757638, val:  75.00%, val_best:  79.58%, tr:  99.18%, tr_best:  99.39%, epoch time: 44.10 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4072%\n",
      "layer   2  Sparsity: 88.7931%\n",
      "layer   3  Sparsity: 84.2877%\n",
      "total_backward_count 792990 real_backward_count 89328  11.265%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.601534/  1.743747, val:  75.83%, val_best:  79.58%, tr:  98.57%, tr_best:  99.39%, epoch time: 44.22 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3552%\n",
      "layer   2  Sparsity: 88.6701%\n",
      "layer   3  Sparsity: 84.3340%\n",
      "total_backward_count 797885 real_backward_count 89606  11.230%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.608314/  1.741448, val:  77.50%, val_best:  79.58%, tr:  98.88%, tr_best:  99.39%, epoch time: 44.07 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.4188%\n",
      "layer   2  Sparsity: 88.7755%\n",
      "layer   3  Sparsity: 84.3502%\n",
      "total_backward_count 802780 real_backward_count 89901  11.199%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.607446/  1.749841, val:  72.92%, val_best:  79.58%, tr:  98.98%, tr_best:  99.39%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4194%\n",
      "layer   2  Sparsity: 88.8825%\n",
      "layer   3  Sparsity: 84.0678%\n",
      "total_backward_count 807675 real_backward_count 90189  11.166%\n",
      "fc layer 2 self.abs_max_out: 983.0\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.609007/  1.759509, val:  70.00%, val_best:  79.58%, tr:  99.18%, tr_best:  99.39%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4012%\n",
      "layer   2  Sparsity: 88.9061%\n",
      "layer   3  Sparsity: 84.1913%\n",
      "total_backward_count 812570 real_backward_count 90471  11.134%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.602191/  1.754511, val:  70.83%, val_best:  79.58%, tr:  98.98%, tr_best:  99.39%, epoch time: 44.89 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4202%\n",
      "layer   2  Sparsity: 88.9196%\n",
      "layer   3  Sparsity: 84.5500%\n",
      "total_backward_count 817465 real_backward_count 90722  11.098%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.608065/  1.762367, val:  71.25%, val_best:  79.58%, tr:  99.39%, tr_best:  99.39%, epoch time: 44.89 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3870%\n",
      "layer   2  Sparsity: 89.0988%\n",
      "layer   3  Sparsity: 84.9516%\n",
      "total_backward_count 822360 real_backward_count 90980  11.063%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.612245/  1.757195, val:  77.08%, val_best:  79.58%, tr:  98.98%, tr_best:  99.39%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3687%\n",
      "layer   2  Sparsity: 89.0851%\n",
      "layer   3  Sparsity: 84.7810%\n",
      "total_backward_count 827255 real_backward_count 91246  11.030%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.608689/  1.761634, val:  70.00%, val_best:  79.58%, tr:  98.26%, tr_best:  99.39%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4049%\n",
      "layer   2  Sparsity: 88.8243%\n",
      "layer   3  Sparsity: 84.8046%\n",
      "total_backward_count 832150 real_backward_count 91523  10.998%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.599987/  1.755278, val:  73.75%, val_best:  79.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4089%\n",
      "layer   2  Sparsity: 88.7754%\n",
      "layer   3  Sparsity: 84.5358%\n",
      "total_backward_count 837045 real_backward_count 91774  10.964%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.604505/  1.759415, val:  72.50%, val_best:  79.58%, tr:  98.88%, tr_best:  99.49%, epoch time: 44.18 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4268%\n",
      "layer   2  Sparsity: 88.7084%\n",
      "layer   3  Sparsity: 84.3734%\n",
      "total_backward_count 841940 real_backward_count 92029  10.931%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.605284/  1.756134, val:  70.42%, val_best:  79.58%, tr:  99.28%, tr_best:  99.49%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.3873%\n",
      "layer   2  Sparsity: 88.7678%\n",
      "layer   3  Sparsity: 84.5811%\n",
      "total_backward_count 846835 real_backward_count 92282  10.897%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.597256/  1.745608, val:  71.25%, val_best:  79.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 44.16 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3657%\n",
      "layer   2  Sparsity: 88.7169%\n",
      "layer   3  Sparsity: 84.5299%\n",
      "total_backward_count 851730 real_backward_count 92550  10.866%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.600825/  1.749469, val:  77.50%, val_best:  79.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4100%\n",
      "layer   2  Sparsity: 88.7745%\n",
      "layer   3  Sparsity: 84.6661%\n",
      "total_backward_count 856625 real_backward_count 92817  10.835%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.588353/  1.761426, val:  69.58%, val_best:  79.58%, tr:  98.98%, tr_best:  99.59%, epoch time: 43.92 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.4305%\n",
      "layer   2  Sparsity: 88.9110%\n",
      "layer   3  Sparsity: 84.3824%\n",
      "total_backward_count 861520 real_backward_count 93078  10.804%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.600469/  1.751833, val:  75.83%, val_best:  79.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 44.28 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4130%\n",
      "layer   2  Sparsity: 89.2073%\n",
      "layer   3  Sparsity: 84.7039%\n",
      "total_backward_count 866415 real_backward_count 93344  10.774%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.598905/  1.761999, val:  73.33%, val_best:  79.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3993%\n",
      "layer   2  Sparsity: 89.2952%\n",
      "layer   3  Sparsity: 84.7145%\n",
      "total_backward_count 871310 real_backward_count 93601  10.743%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.598870/  1.756310, val:  78.33%, val_best:  79.58%, tr:  98.88%, tr_best:  99.59%, epoch time: 44.07 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.4203%\n",
      "layer   2  Sparsity: 89.2764%\n",
      "layer   3  Sparsity: 84.5199%\n",
      "total_backward_count 876205 real_backward_count 93859  10.712%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.594626/  1.740500, val:  74.58%, val_best:  79.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4212%\n",
      "layer   2  Sparsity: 89.4806%\n",
      "layer   3  Sparsity: 84.5019%\n",
      "total_backward_count 881100 real_backward_count 94134  10.684%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.589205/  1.742702, val:  73.33%, val_best:  79.58%, tr:  99.28%, tr_best:  99.59%, epoch time: 43.98 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.3669%\n",
      "layer   2  Sparsity: 89.4074%\n",
      "layer   3  Sparsity: 84.6564%\n",
      "total_backward_count 885995 real_backward_count 94409  10.656%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.588431/  1.744285, val:  80.00%, val_best:  80.00%, tr:  99.49%, tr_best:  99.59%, epoch time: 44.01 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.4264%\n",
      "layer   2  Sparsity: 89.3916%\n",
      "layer   3  Sparsity: 84.6739%\n",
      "total_backward_count 890890 real_backward_count 94668  10.626%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.601523/  1.752264, val:  72.92%, val_best:  80.00%, tr:  98.47%, tr_best:  99.59%, epoch time: 43.93 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.3936%\n",
      "layer   2  Sparsity: 89.3518%\n",
      "layer   3  Sparsity: 84.6034%\n",
      "total_backward_count 895785 real_backward_count 94962  10.601%\n",
      "fc layer 2 self.abs_max_out: 984.0\n",
      "fc layer 1 self.abs_max_out: 3344.0\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.605704/  1.752327, val:  68.75%, val_best:  80.00%, tr:  99.18%, tr_best:  99.59%, epoch time: 43.84 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.4319%\n",
      "layer   2  Sparsity: 89.2175%\n",
      "layer   3  Sparsity: 84.6181%\n",
      "total_backward_count 900680 real_backward_count 95239  10.574%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.597372/  1.749686, val:  77.08%, val_best:  80.00%, tr:  99.08%, tr_best:  99.59%, epoch time: 43.99 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.4065%\n",
      "layer   2  Sparsity: 89.0290%\n",
      "layer   3  Sparsity: 84.4085%\n",
      "total_backward_count 905575 real_backward_count 95500  10.546%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.589544/  1.750992, val:  77.92%, val_best:  80.00%, tr:  99.39%, tr_best:  99.59%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4121%\n",
      "layer   2  Sparsity: 88.9598%\n",
      "layer   3  Sparsity: 84.3262%\n",
      "total_backward_count 910470 real_backward_count 95725  10.514%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.584301/  1.731009, val:  77.08%, val_best:  80.00%, tr:  99.18%, tr_best:  99.59%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.3638%\n",
      "layer   2  Sparsity: 89.0254%\n",
      "layer   3  Sparsity: 84.2184%\n",
      "total_backward_count 915365 real_backward_count 95988  10.486%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.576845/  1.751078, val:  75.83%, val_best:  80.00%, tr:  99.28%, tr_best:  99.59%, epoch time: 44.05 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.4656%\n",
      "layer   2  Sparsity: 89.1264%\n",
      "layer   3  Sparsity: 84.4993%\n",
      "total_backward_count 920260 real_backward_count 96262  10.460%\n",
      "fc layer 2 self.abs_max_out: 986.0\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.584581/  1.738714, val:  70.83%, val_best:  80.00%, tr:  99.18%, tr_best:  99.59%, epoch time: 44.70 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4276%\n",
      "layer   2  Sparsity: 89.2773%\n",
      "layer   3  Sparsity: 84.4527%\n",
      "total_backward_count 925155 real_backward_count 96514  10.432%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.586481/  1.737844, val:  75.42%, val_best:  80.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 44.21 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4090%\n",
      "layer   2  Sparsity: 89.0984%\n",
      "layer   3  Sparsity: 84.4354%\n",
      "total_backward_count 930050 real_backward_count 96739  10.401%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.581893/  1.738282, val:  72.92%, val_best:  80.00%, tr:  99.49%, tr_best:  99.59%, epoch time: 44.39 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4139%\n",
      "layer   2  Sparsity: 88.9332%\n",
      "layer   3  Sparsity: 84.7304%\n",
      "total_backward_count 934945 real_backward_count 96967  10.371%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.573987/  1.726149, val:  77.50%, val_best:  80.00%, tr:  99.28%, tr_best:  99.59%, epoch time: 44.42 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4108%\n",
      "layer   2  Sparsity: 88.8527%\n",
      "layer   3  Sparsity: 84.7969%\n",
      "total_backward_count 939840 real_backward_count 97198  10.342%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.571560/  1.727569, val:  76.67%, val_best:  80.00%, tr:  99.49%, tr_best:  99.59%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4349%\n",
      "layer   2  Sparsity: 88.9220%\n",
      "layer   3  Sparsity: 84.6461%\n",
      "total_backward_count 944735 real_backward_count 97432  10.313%\n",
      "fc layer 2 self.abs_max_out: 1012.0\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.569596/  1.729849, val:  76.25%, val_best:  80.00%, tr:  98.77%, tr_best:  99.59%, epoch time: 45.22 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4296%\n",
      "layer   2  Sparsity: 88.9306%\n",
      "layer   3  Sparsity: 84.5213%\n",
      "total_backward_count 949630 real_backward_count 97635  10.281%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.580296/  1.737583, val:  71.25%, val_best:  80.00%, tr:  98.98%, tr_best:  99.59%, epoch time: 43.98 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 82.4194%\n",
      "layer   2  Sparsity: 89.2193%\n",
      "layer   3  Sparsity: 84.9399%\n",
      "total_backward_count 954525 real_backward_count 97856  10.252%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.574051/  1.728034, val:  77.50%, val_best:  80.00%, tr:  99.08%, tr_best:  99.59%, epoch time: 45.15 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4176%\n",
      "layer   2  Sparsity: 89.1062%\n",
      "layer   3  Sparsity: 84.8302%\n",
      "total_backward_count 959420 real_backward_count 98096  10.225%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.570334/  1.730796, val:  74.58%, val_best:  80.00%, tr:  99.39%, tr_best:  99.59%, epoch time: 44.20 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4277%\n",
      "layer   2  Sparsity: 88.8624%\n",
      "layer   3  Sparsity: 84.6847%\n",
      "total_backward_count 964315 real_backward_count 98305  10.194%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.569504/  1.737092, val:  75.42%, val_best:  80.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 44.47 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4452%\n",
      "layer   2  Sparsity: 88.8861%\n",
      "layer   3  Sparsity: 84.7352%\n",
      "total_backward_count 969210 real_backward_count 98517  10.165%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.567424/  1.732894, val:  76.25%, val_best:  80.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4231%\n",
      "layer   2  Sparsity: 88.8495%\n",
      "layer   3  Sparsity: 84.8794%\n",
      "total_backward_count 974105 real_backward_count 98714  10.134%\n",
      "fc layer 1 self.abs_max_out: 3361.0\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.568993/  1.720731, val:  75.00%, val_best:  80.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 82.4271%\n",
      "layer   2  Sparsity: 88.8057%\n",
      "layer   3  Sparsity: 84.6746%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4a7c11d4f044f883c3d411577496b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99285</td></tr><tr><td>tr_epoch_loss</td><td>1.56899</td></tr><tr><td>val_acc_best</td><td>0.8</td></tr><tr><td>val_acc_now</td><td>0.75</td></tr><tr><td>val_loss</td><td>1.72073</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misty-sweep-99</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8guowcp8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8guowcp8</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_084240-8guowcp8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: imdb8psw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 12000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_111204-imdb8psw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/imdb8psw' target=\"_blank\">exalted-sweep-102</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/imdb8psw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/imdb8psw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_111213_733', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 12000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 67f09733060e9328908e01cda0ab3532\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 592\n",
      "fc layer 1 self.abs_max_out: 119.0\n",
      "lif layer 1 self.abs_max_v: 119.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 121.0\n",
      "lif layer 1 self.abs_max_v: 144.5\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 36.0\n",
      "lif layer 1 self.abs_max_v: 146.0\n",
      "fc layer 2 self.abs_max_out: 65.0\n",
      "lif layer 2 self.abs_max_v: 69.0\n",
      "smallest_now_T updated: 534\n",
      "fc layer 1 self.abs_max_out: 169.0\n",
      "lif layer 1 self.abs_max_v: 169.0\n",
      "fc layer 1 self.abs_max_out: 178.0\n",
      "lif layer 1 self.abs_max_v: 241.5\n",
      "fc layer 2 self.abs_max_out: 162.0\n",
      "lif layer 2 self.abs_max_v: 170.0\n",
      "fc layer 3 self.abs_max_out: 33.0\n",
      "fc layer 1 self.abs_max_out: 196.0\n",
      "lif layer 1 self.abs_max_v: 259.0\n",
      "fc layer 2 self.abs_max_out: 195.0\n",
      "lif layer 2 self.abs_max_v: 234.5\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "lif layer 1 self.abs_max_v: 299.5\n",
      "fc layer 3 self.abs_max_out: 70.0\n",
      "lif layer 1 self.abs_max_v: 317.0\n",
      "fc layer 2 self.abs_max_out: 249.0\n",
      "lif layer 2 self.abs_max_v: 251.0\n",
      "fc layer 3 self.abs_max_out: 73.0\n",
      "smallest_now_T updated: 407\n",
      "fc layer 1 self.abs_max_out: 197.0\n",
      "fc layer 1 self.abs_max_out: 275.0\n",
      "lif layer 2 self.abs_max_v: 272.5\n",
      "lif layer 2 self.abs_max_v: 299.0\n",
      "fc layer 2 self.abs_max_out: 264.0\n",
      "fc layer 3 self.abs_max_out: 82.0\n",
      "fc layer 1 self.abs_max_out: 282.0\n",
      "lif layer 2 self.abs_max_v: 301.0\n",
      "fc layer 3 self.abs_max_out: 91.0\n",
      "lif layer 1 self.abs_max_v: 343.0\n",
      "lif layer 2 self.abs_max_v: 353.0\n",
      "fc layer 2 self.abs_max_out: 290.0\n",
      "fc layer 1 self.abs_max_out: 299.0\n",
      "fc layer 2 self.abs_max_out: 310.0\n",
      "lif layer 2 self.abs_max_v: 402.0\n",
      "fc layer 3 self.abs_max_out: 114.0\n",
      "lif layer 1 self.abs_max_v: 351.5\n",
      "fc layer 2 self.abs_max_out: 322.0\n",
      "fc layer 1 self.abs_max_out: 353.0\n",
      "lif layer 1 self.abs_max_v: 353.0\n",
      "fc layer 2 self.abs_max_out: 362.0\n",
      "lif layer 2 self.abs_max_v: 454.0\n",
      "lif layer 2 self.abs_max_v: 517.0\n",
      "fc layer 1 self.abs_max_out: 366.0\n",
      "lif layer 1 self.abs_max_v: 366.0\n",
      "lif layer 1 self.abs_max_v: 375.5\n",
      "smallest_now_T updated: 345\n",
      "fc layer 3 self.abs_max_out: 127.0\n",
      "fc layer 1 self.abs_max_out: 502.0\n",
      "lif layer 1 self.abs_max_v: 502.0\n",
      "fc layer 2 self.abs_max_out: 408.0\n",
      "lif layer 2 self.abs_max_v: 611.5\n",
      "fc layer 3 self.abs_max_out: 137.0\n",
      "fc layer 3 self.abs_max_out: 141.0\n",
      "smallest_now_T updated: 317\n",
      "fc layer 3 self.abs_max_out: 186.0\n",
      "fc layer 3 self.abs_max_out: 198.0\n",
      "fc layer 2 self.abs_max_out: 437.0\n",
      "fc layer 2 self.abs_max_out: 452.0\n",
      "lif layer 2 self.abs_max_v: 662.5\n",
      "lif layer 2 self.abs_max_v: 680.0\n",
      "fc layer 2 self.abs_max_out: 467.0\n",
      "fc layer 2 self.abs_max_out: 489.0\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "fc layer 1 self.abs_max_out: 514.0\n",
      "lif layer 1 self.abs_max_v: 514.0\n",
      "smallest_now_T updated: 286\n",
      "fc layer 2 self.abs_max_out: 491.0\n",
      "fc layer 2 self.abs_max_out: 619.0\n",
      "fc layer 1 self.abs_max_out: 580.0\n",
      "lif layer 1 self.abs_max_v: 580.0\n",
      "fc layer 1 self.abs_max_out: 678.0\n",
      "lif layer 1 self.abs_max_v: 678.0\n",
      "fc layer 3 self.abs_max_out: 228.0\n",
      "lif layer 2 self.abs_max_v: 711.0\n",
      "fc layer 1 self.abs_max_out: 699.0\n",
      "lif layer 1 self.abs_max_v: 699.0\n",
      "lif layer 2 self.abs_max_v: 732.5\n",
      "fc layer 1 self.abs_max_out: 753.0\n",
      "lif layer 1 self.abs_max_v: 753.0\n",
      "fc layer 3 self.abs_max_out: 229.0\n",
      "fc layer 1 self.abs_max_out: 931.0\n",
      "lif layer 1 self.abs_max_v: 931.0\n",
      "lif layer 2 self.abs_max_v: 749.5\n",
      "smallest_now_T updated: 247\n",
      "smallest_now_T updated: 192\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "lif layer 2 self.abs_max_v: 767.5\n",
      "fc layer 2 self.abs_max_out: 647.0\n",
      "lif layer 2 self.abs_max_v: 799.5\n",
      "fc layer 3 self.abs_max_out: 258.0\n",
      "fc layer 3 self.abs_max_out: 272.0\n",
      "fc layer 2 self.abs_max_out: 670.0\n",
      "fc layer 2 self.abs_max_out: 681.0\n",
      "lif layer 2 self.abs_max_v: 816.5\n",
      "fc layer 2 self.abs_max_out: 693.0\n",
      "lif layer 2 self.abs_max_v: 820.0\n",
      "fc layer 2 self.abs_max_out: 718.0\n",
      "fc layer 2 self.abs_max_out: 740.0\n",
      "fc layer 1 self.abs_max_out: 995.0\n",
      "lif layer 1 self.abs_max_v: 995.0\n",
      "lif layer 2 self.abs_max_v: 840.5\n",
      "lif layer 2 self.abs_max_v: 888.5\n",
      "fc layer 2 self.abs_max_out: 848.0\n",
      "fc layer 3 self.abs_max_out: 290.0\n",
      "fc layer 2 self.abs_max_out: 944.0\n",
      "lif layer 2 self.abs_max_v: 944.0\n",
      "fc layer 3 self.abs_max_out: 294.0\n",
      "fc layer 1 self.abs_max_out: 1055.0\n",
      "lif layer 1 self.abs_max_v: 1055.0\n",
      "fc layer 1 self.abs_max_out: 1103.0\n",
      "lif layer 1 self.abs_max_v: 1103.0\n",
      "fc layer 1 self.abs_max_out: 1210.0\n",
      "lif layer 1 self.abs_max_v: 1210.0\n",
      "lif layer 2 self.abs_max_v: 1076.5\n",
      "smallest_now_T_val updated: 552\n",
      "smallest_now_T_val updated: 456\n",
      "smallest_now_T_val updated: 448\n",
      "smallest_now_T_val updated: 440\n",
      "smallest_now_T_val updated: 368\n",
      "fc layer 2 self.abs_max_out: 956.0\n",
      "smallest_now_T_val updated: 137\n",
      "fc layer 2 self.abs_max_out: 975.0\n",
      "fc layer 2 self.abs_max_out: 991.0\n",
      "lif layer 2 self.abs_max_v: 1078.0\n",
      "lif layer 2 self.abs_max_v: 1090.0\n",
      "fc layer 2 self.abs_max_out: 1030.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.061400/  2.106605, val:  30.42%, val_best:  30.42%, tr:  63.23%, tr_best:  63.23%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7628%\n",
      "layer   2  Sparsity: 85.4631%\n",
      "layer   3  Sparsity: 83.3578%\n",
      "total_backward_count 4895 real_backward_count 2527  51.624%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1078.0\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "fc layer 2 self.abs_max_out: 1110.0\n",
      "lif layer 2 self.abs_max_v: 1110.0\n",
      "fc layer 2 self.abs_max_out: 1143.0\n",
      "lif layer 2 self.abs_max_v: 1143.0\n",
      "fc layer 2 self.abs_max_out: 1188.0\n",
      "lif layer 2 self.abs_max_v: 1188.0\n",
      "fc layer 2 self.abs_max_out: 1211.0\n",
      "lif layer 2 self.abs_max_v: 1211.0\n",
      "fc layer 1 self.abs_max_out: 1212.0\n",
      "lif layer 1 self.abs_max_v: 1212.0\n",
      "fc layer 1 self.abs_max_out: 1307.0\n",
      "lif layer 1 self.abs_max_v: 1307.0\n",
      "fc layer 2 self.abs_max_out: 1289.0\n",
      "lif layer 2 self.abs_max_v: 1289.0\n",
      "fc layer 1 self.abs_max_out: 1437.0\n",
      "lif layer 1 self.abs_max_v: 1437.0\n",
      "fc layer 2 self.abs_max_out: 1294.0\n",
      "lif layer 2 self.abs_max_v: 1294.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.963716/  2.070366, val:  38.75%, val_best:  38.75%, tr:  76.00%, tr_best:  76.00%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7794%\n",
      "layer   2  Sparsity: 82.0343%\n",
      "layer   3  Sparsity: 78.6814%\n",
      "total_backward_count 9790 real_backward_count 4442  45.373%\n",
      "fc layer 2 self.abs_max_out: 1356.0\n",
      "lif layer 2 self.abs_max_v: 1356.0\n",
      "fc layer 1 self.abs_max_out: 1661.0\n",
      "lif layer 1 self.abs_max_v: 1661.0\n",
      "fc layer 2 self.abs_max_out: 1402.0\n",
      "lif layer 2 self.abs_max_v: 1402.0\n",
      "lif layer 2 self.abs_max_v: 1456.5\n",
      "fc layer 2 self.abs_max_out: 1440.0\n",
      "fc layer 2 self.abs_max_out: 1454.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.943856/  2.073971, val:  31.67%, val_best:  38.75%, tr:  81.92%, tr_best:  81.92%, epoch time: 44.17 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7912%\n",
      "layer   2  Sparsity: 81.0401%\n",
      "layer   3  Sparsity: 76.8696%\n",
      "total_backward_count 14685 real_backward_count 6135  41.777%\n",
      "lif layer 2 self.abs_max_v: 1504.5\n",
      "fc layer 2 self.abs_max_out: 1500.0\n",
      "fc layer 1 self.abs_max_out: 1742.0\n",
      "lif layer 1 self.abs_max_v: 1742.0\n",
      "fc layer 3 self.abs_max_out: 360.0\n",
      "lif layer 2 self.abs_max_v: 1631.5\n",
      "fc layer 2 self.abs_max_out: 1516.0\n",
      "fc layer 2 self.abs_max_out: 1566.0\n",
      "lif layer 2 self.abs_max_v: 1652.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.924011/  2.001340, val:  42.50%, val_best:  42.50%, tr:  83.96%, tr_best:  83.96%, epoch time: 44.33 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7670%\n",
      "layer   2  Sparsity: 80.0801%\n",
      "layer   3  Sparsity: 75.5797%\n",
      "total_backward_count 19580 real_backward_count 7795  39.811%\n",
      "fc layer 2 self.abs_max_out: 1586.0\n",
      "fc layer 2 self.abs_max_out: 1594.0\n",
      "lif layer 2 self.abs_max_v: 1763.5\n",
      "fc layer 2 self.abs_max_out: 1604.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.888878/  2.035875, val:  30.83%, val_best:  42.50%, tr:  83.66%, tr_best:  83.96%, epoch time: 44.42 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7764%\n",
      "layer   2  Sparsity: 80.0678%\n",
      "layer   3  Sparsity: 75.0626%\n",
      "total_backward_count 24475 real_backward_count 9348  38.194%\n",
      "fc layer 1 self.abs_max_out: 2013.0\n",
      "lif layer 1 self.abs_max_v: 2013.0\n",
      "fc layer 3 self.abs_max_out: 392.0\n",
      "fc layer 2 self.abs_max_out: 1621.0\n",
      "fc layer 2 self.abs_max_out: 1682.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.897149/  2.022202, val:  41.25%, val_best:  42.50%, tr:  85.19%, tr_best:  85.19%, epoch time: 43.92 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 96.7738%\n",
      "layer   2  Sparsity: 80.0009%\n",
      "layer   3  Sparsity: 74.3661%\n",
      "total_backward_count 29370 real_backward_count 10924  37.194%\n",
      "fc layer 2 self.abs_max_out: 1716.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.872830/  2.027140, val:  34.17%, val_best:  42.50%, tr:  85.29%, tr_best:  85.29%, epoch time: 44.35 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7942%\n",
      "layer   2  Sparsity: 79.8813%\n",
      "layer   3  Sparsity: 73.5801%\n",
      "total_backward_count 34265 real_backward_count 12495  36.466%\n",
      "fc layer 2 self.abs_max_out: 1779.0\n",
      "lif layer 2 self.abs_max_v: 1779.0\n",
      "fc layer 1 self.abs_max_out: 2084.0\n",
      "lif layer 1 self.abs_max_v: 2084.0\n",
      "lif layer 2 self.abs_max_v: 1782.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.871150/  1.999633, val:  41.67%, val_best:  42.50%, tr:  86.41%, tr_best:  86.41%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7838%\n",
      "layer   2  Sparsity: 79.2384%\n",
      "layer   3  Sparsity: 73.1062%\n",
      "total_backward_count 39160 real_backward_count 13974  35.684%\n",
      "lif layer 2 self.abs_max_v: 1838.0\n",
      "fc layer 2 self.abs_max_out: 1828.0\n",
      "fc layer 2 self.abs_max_out: 1832.0\n",
      "lif layer 2 self.abs_max_v: 1988.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.874990/  1.961522, val:  45.42%, val_best:  45.42%, tr:  85.70%, tr_best:  86.41%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7725%\n",
      "layer   2  Sparsity: 79.1929%\n",
      "layer   3  Sparsity: 72.8377%\n",
      "total_backward_count 44055 real_backward_count 15553  35.304%\n",
      "fc layer 2 self.abs_max_out: 1866.0\n",
      "lif layer 2 self.abs_max_v: 2033.0\n",
      "lif layer 2 self.abs_max_v: 2075.5\n",
      "fc layer 1 self.abs_max_out: 2104.0\n",
      "lif layer 1 self.abs_max_v: 2104.0\n",
      "lif layer 2 self.abs_max_v: 2092.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.835795/  1.999154, val:  42.50%, val_best:  45.42%, tr:  86.82%, tr_best:  86.82%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7658%\n",
      "layer   2  Sparsity: 78.6400%\n",
      "layer   3  Sparsity: 72.5266%\n",
      "total_backward_count 48950 real_backward_count 17035  34.801%\n",
      "fc layer 2 self.abs_max_out: 1935.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.831709/  1.979228, val:  46.67%, val_best:  46.67%, tr:  87.03%, tr_best:  87.03%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7454%\n",
      "layer   2  Sparsity: 78.7470%\n",
      "layer   3  Sparsity: 72.0916%\n",
      "total_backward_count 53845 real_backward_count 18522  34.399%\n",
      "fc layer 1 self.abs_max_out: 2308.0\n",
      "lif layer 1 self.abs_max_v: 2308.0\n",
      "lif layer 2 self.abs_max_v: 2148.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.846605/  1.986323, val:  51.67%, val_best:  51.67%, tr:  87.54%, tr_best:  87.54%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7900%\n",
      "layer   2  Sparsity: 78.3232%\n",
      "layer   3  Sparsity: 72.0472%\n",
      "total_backward_count 58740 real_backward_count 19974  34.004%\n",
      "lif layer 2 self.abs_max_v: 2181.5\n",
      "lif layer 2 self.abs_max_v: 2183.5\n",
      "lif layer 2 self.abs_max_v: 2300.0\n",
      "fc layer 2 self.abs_max_out: 1970.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.819199/  1.999921, val:  32.08%, val_best:  51.67%, tr:  87.64%, tr_best:  87.64%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7570%\n",
      "layer   2  Sparsity: 78.3367%\n",
      "layer   3  Sparsity: 71.8039%\n",
      "total_backward_count 63635 real_backward_count 21411  33.647%\n",
      "fc layer 3 self.abs_max_out: 407.0\n",
      "fc layer 3 self.abs_max_out: 409.0\n",
      "lif layer 2 self.abs_max_v: 2344.0\n",
      "fc layer 2 self.abs_max_out: 2030.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.790437/  1.958159, val:  42.92%, val_best:  51.67%, tr:  88.05%, tr_best:  88.05%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7753%\n",
      "layer   2  Sparsity: 77.9711%\n",
      "layer   3  Sparsity: 71.1493%\n",
      "total_backward_count 68530 real_backward_count 22823  33.304%\n",
      "fc layer 3 self.abs_max_out: 418.0\n",
      "lif layer 2 self.abs_max_v: 2364.0\n",
      "lif layer 2 self.abs_max_v: 2542.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.787387/  1.962307, val:  38.75%, val_best:  51.67%, tr:  88.25%, tr_best:  88.25%, epoch time: 44.19 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7799%\n",
      "layer   2  Sparsity: 78.2828%\n",
      "layer   3  Sparsity: 71.4494%\n",
      "total_backward_count 73425 real_backward_count 24189  32.944%\n",
      "fc layer 1 self.abs_max_out: 2345.0\n",
      "lif layer 1 self.abs_max_v: 2345.0\n",
      "fc layer 2 self.abs_max_out: 2105.0\n",
      "lif layer 2 self.abs_max_v: 2756.5\n",
      "lif layer 2 self.abs_max_v: 3044.5\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.779280/  1.971899, val:  37.50%, val_best:  51.67%, tr:  88.36%, tr_best:  88.36%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7685%\n",
      "layer   2  Sparsity: 77.8285%\n",
      "layer   3  Sparsity: 70.6961%\n",
      "total_backward_count 78320 real_backward_count 25573  32.652%\n",
      "fc layer 1 self.abs_max_out: 2565.0\n",
      "lif layer 1 self.abs_max_v: 2565.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.762792/  1.944648, val:  40.83%, val_best:  51.67%, tr:  89.58%, tr_best:  89.58%, epoch time: 44.37 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7829%\n",
      "layer   2  Sparsity: 77.6457%\n",
      "layer   3  Sparsity: 70.4366%\n",
      "total_backward_count 83215 real_backward_count 26960  32.398%\n",
      "fc layer 2 self.abs_max_out: 2195.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.776195/  1.945926, val:  41.25%, val_best:  51.67%, tr:  90.09%, tr_best:  90.09%, epoch time: 45.15 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7783%\n",
      "layer   2  Sparsity: 77.2180%\n",
      "layer   3  Sparsity: 69.7130%\n",
      "total_backward_count 88110 real_backward_count 28351  32.177%\n",
      "fc layer 3 self.abs_max_out: 423.0\n",
      "fc layer 1 self.abs_max_out: 2845.0\n",
      "lif layer 1 self.abs_max_v: 2845.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.766859/  1.909157, val:  45.83%, val_best:  51.67%, tr:  88.56%, tr_best:  90.09%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7750%\n",
      "layer   2  Sparsity: 77.1189%\n",
      "layer   3  Sparsity: 70.0173%\n",
      "total_backward_count 93005 real_backward_count 29726  31.962%\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.750733/  1.974565, val:  36.67%, val_best:  51.67%, tr:  88.66%, tr_best:  90.09%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7572%\n",
      "layer   2  Sparsity: 77.3767%\n",
      "layer   3  Sparsity: 70.4263%\n",
      "total_backward_count 97900 real_backward_count 31047  31.713%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.774015/  1.954253, val:  40.42%, val_best:  51.67%, tr:  88.76%, tr_best:  90.09%, epoch time: 44.21 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7784%\n",
      "layer   2  Sparsity: 77.1400%\n",
      "layer   3  Sparsity: 70.6932%\n",
      "total_backward_count 102795 real_backward_count 32417  31.536%\n",
      "fc layer 2 self.abs_max_out: 2210.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.764818/  1.954589, val:  37.50%, val_best:  51.67%, tr:  87.23%, tr_best:  90.09%, epoch time: 44.91 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7877%\n",
      "layer   2  Sparsity: 77.2531%\n",
      "layer   3  Sparsity: 70.3788%\n",
      "total_backward_count 107690 real_backward_count 33836  31.420%\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.751261/  1.940896, val:  50.42%, val_best:  51.67%, tr:  88.87%, tr_best:  90.09%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7900%\n",
      "layer   2  Sparsity: 77.4027%\n",
      "layer   3  Sparsity: 70.3739%\n",
      "total_backward_count 112585 real_backward_count 35185  31.252%\n",
      "fc layer 2 self.abs_max_out: 2222.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.763075/  1.948056, val:  50.42%, val_best:  51.67%, tr:  89.99%, tr_best:  90.09%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7649%\n",
      "layer   2  Sparsity: 77.5302%\n",
      "layer   3  Sparsity: 70.3099%\n",
      "total_backward_count 117480 real_backward_count 36504  31.073%\n",
      "fc layer 3 self.abs_max_out: 430.0\n",
      "fc layer 2 self.abs_max_out: 2241.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.756339/  1.961951, val:  40.00%, val_best:  51.67%, tr:  89.48%, tr_best:  90.09%, epoch time: 44.14 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7973%\n",
      "layer   2  Sparsity: 77.4214%\n",
      "layer   3  Sparsity: 69.4496%\n",
      "total_backward_count 122375 real_backward_count 37862  30.939%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.766281/  1.896713, val:  51.25%, val_best:  51.67%, tr:  90.30%, tr_best:  90.30%, epoch time: 45.11 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7806%\n",
      "layer   2  Sparsity: 77.3733%\n",
      "layer   3  Sparsity: 70.1024%\n",
      "total_backward_count 127270 real_backward_count 39220  30.816%\n",
      "fc layer 2 self.abs_max_out: 2283.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.743739/  1.907483, val:  57.50%, val_best:  57.50%, tr:  89.79%, tr_best:  90.30%, epoch time: 44.19 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7734%\n",
      "layer   2  Sparsity: 77.1390%\n",
      "layer   3  Sparsity: 70.1459%\n",
      "total_backward_count 132165 real_backward_count 40557  30.687%\n",
      "fc layer 3 self.abs_max_out: 434.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.741578/  1.897096, val:  57.08%, val_best:  57.50%, tr:  90.30%, tr_best:  90.30%, epoch time: 45.04 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7799%\n",
      "layer   2  Sparsity: 77.3053%\n",
      "layer   3  Sparsity: 70.3128%\n",
      "total_backward_count 137060 real_backward_count 41910  30.578%\n",
      "fc layer 3 self.abs_max_out: 484.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.724351/  1.915612, val:  46.25%, val_best:  57.50%, tr:  90.19%, tr_best:  90.30%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7798%\n",
      "layer   2  Sparsity: 77.1750%\n",
      "layer   3  Sparsity: 70.1919%\n",
      "total_backward_count 141955 real_backward_count 43182  30.419%\n",
      "fc layer 1 self.abs_max_out: 2969.0\n",
      "lif layer 1 self.abs_max_v: 2969.0\n",
      "fc layer 2 self.abs_max_out: 2437.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.722199/  1.898269, val:  38.75%, val_best:  57.50%, tr:  91.22%, tr_best:  91.22%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7793%\n",
      "layer   2  Sparsity: 77.3500%\n",
      "layer   3  Sparsity: 70.2643%\n",
      "total_backward_count 146850 real_backward_count 44470  30.283%\n",
      "fc layer 3 self.abs_max_out: 503.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.730985/  1.865299, val:  54.17%, val_best:  57.50%, tr:  90.70%, tr_best:  91.22%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7878%\n",
      "layer   2  Sparsity: 77.2922%\n",
      "layer   3  Sparsity: 70.0765%\n",
      "total_backward_count 151745 real_backward_count 45769  30.162%\n",
      "fc layer 1 self.abs_max_out: 3234.0\n",
      "lif layer 1 self.abs_max_v: 3234.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.733107/  1.885457, val:  56.67%, val_best:  57.50%, tr:  89.99%, tr_best:  91.22%, epoch time: 44.54 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7930%\n",
      "layer   2  Sparsity: 76.8778%\n",
      "layer   3  Sparsity: 69.6784%\n",
      "total_backward_count 156640 real_backward_count 47084  30.059%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.731345/  1.922971, val:  36.25%, val_best:  57.50%, tr:  92.54%, tr_best:  92.54%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7537%\n",
      "layer   2  Sparsity: 76.7845%\n",
      "layer   3  Sparsity: 69.5906%\n",
      "total_backward_count 161535 real_backward_count 48345  29.928%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.718311/  1.884068, val:  47.92%, val_best:  57.50%, tr:  89.89%, tr_best:  92.54%, epoch time: 44.33 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7623%\n",
      "layer   2  Sparsity: 76.8150%\n",
      "layer   3  Sparsity: 69.5050%\n",
      "total_backward_count 166430 real_backward_count 49645  29.829%\n",
      "fc layer 2 self.abs_max_out: 2480.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.699241/  1.878882, val:  53.33%, val_best:  57.50%, tr:  91.22%, tr_best:  92.54%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7453%\n",
      "layer   2  Sparsity: 76.7664%\n",
      "layer   3  Sparsity: 69.5204%\n",
      "total_backward_count 171325 real_backward_count 50853  29.682%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.704352/  1.889512, val:  54.58%, val_best:  57.50%, tr:  92.75%, tr_best:  92.75%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7864%\n",
      "layer   2  Sparsity: 76.9724%\n",
      "layer   3  Sparsity: 69.5744%\n",
      "total_backward_count 176220 real_backward_count 52129  29.582%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.696095/  1.855716, val:  52.08%, val_best:  57.50%, tr:  91.42%, tr_best:  92.75%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7963%\n",
      "layer   2  Sparsity: 76.8837%\n",
      "layer   3  Sparsity: 69.3847%\n",
      "total_backward_count 181115 real_backward_count 53365  29.465%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.692626/  1.899970, val:  35.00%, val_best:  57.50%, tr:  93.46%, tr_best:  93.46%, epoch time: 44.29 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7629%\n",
      "layer   2  Sparsity: 76.5997%\n",
      "layer   3  Sparsity: 69.8494%\n",
      "total_backward_count 186010 real_backward_count 54530  29.316%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.711483/  1.862519, val:  47.50%, val_best:  57.50%, tr:  91.73%, tr_best:  93.46%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7891%\n",
      "layer   2  Sparsity: 76.6797%\n",
      "layer   3  Sparsity: 70.3505%\n",
      "total_backward_count 190905 real_backward_count 55802  29.230%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.696160/  1.841741, val:  59.17%, val_best:  59.17%, tr:  92.95%, tr_best:  93.46%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7935%\n",
      "layer   2  Sparsity: 76.6272%\n",
      "layer   3  Sparsity: 70.4472%\n",
      "total_backward_count 195800 real_backward_count 57036  29.130%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.670585/  1.834744, val:  54.58%, val_best:  59.17%, tr:  92.34%, tr_best:  93.46%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7436%\n",
      "layer   2  Sparsity: 76.4071%\n",
      "layer   3  Sparsity: 69.7650%\n",
      "total_backward_count 200695 real_backward_count 58271  29.035%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.676768/  1.833723, val:  47.50%, val_best:  59.17%, tr:  92.13%, tr_best:  93.46%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7795%\n",
      "layer   2  Sparsity: 76.2824%\n",
      "layer   3  Sparsity: 69.7831%\n",
      "total_backward_count 205590 real_backward_count 59497  28.940%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.667727/  1.876877, val:  52.50%, val_best:  59.17%, tr:  91.83%, tr_best:  93.46%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7406%\n",
      "layer   2  Sparsity: 76.2655%\n",
      "layer   3  Sparsity: 69.5662%\n",
      "total_backward_count 210485 real_backward_count 60725  28.850%\n",
      "fc layer 2 self.abs_max_out: 2535.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.670439/  1.850861, val:  46.67%, val_best:  59.17%, tr:  92.95%, tr_best:  93.46%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7721%\n",
      "layer   2  Sparsity: 76.3536%\n",
      "layer   3  Sparsity: 69.9105%\n",
      "total_backward_count 215380 real_backward_count 61924  28.751%\n",
      "fc layer 2 self.abs_max_out: 2560.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.673522/  1.832317, val:  50.42%, val_best:  59.17%, tr:  92.03%, tr_best:  93.46%, epoch time: 44.89 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7769%\n",
      "layer   2  Sparsity: 76.3014%\n",
      "layer   3  Sparsity: 69.8037%\n",
      "total_backward_count 220275 real_backward_count 63168  28.677%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.671344/  1.854039, val:  55.83%, val_best:  59.17%, tr:  91.42%, tr_best:  93.46%, epoch time: 44.31 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7648%\n",
      "layer   2  Sparsity: 76.3474%\n",
      "layer   3  Sparsity: 70.1642%\n",
      "total_backward_count 225170 real_backward_count 64371  28.588%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.688097/  1.842817, val:  55.00%, val_best:  59.17%, tr:  91.73%, tr_best:  93.46%, epoch time: 45.13 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7837%\n",
      "layer   2  Sparsity: 76.2209%\n",
      "layer   3  Sparsity: 70.2692%\n",
      "total_backward_count 230065 real_backward_count 65567  28.499%\n",
      "lif layer 2 self.abs_max_v: 3063.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.688742/  1.842795, val:  45.83%, val_best:  59.17%, tr:  93.97%, tr_best:  93.97%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.8029%\n",
      "layer   2  Sparsity: 76.3443%\n",
      "layer   3  Sparsity: 70.1266%\n",
      "total_backward_count 234960 real_backward_count 66742  28.406%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.664155/  1.863252, val:  44.58%, val_best:  59.17%, tr:  93.67%, tr_best:  93.97%, epoch time: 44.43 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7831%\n",
      "layer   2  Sparsity: 76.5221%\n",
      "layer   3  Sparsity: 69.5496%\n",
      "total_backward_count 239855 real_backward_count 67919  28.317%\n",
      "lif layer 2 self.abs_max_v: 3096.5\n",
      "fc layer 2 self.abs_max_out: 2586.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.671354/  1.821312, val:  55.00%, val_best:  59.17%, tr:  92.85%, tr_best:  93.97%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7586%\n",
      "layer   2  Sparsity: 76.2267%\n",
      "layer   3  Sparsity: 69.5330%\n",
      "total_backward_count 244750 real_backward_count 69121  28.241%\n",
      "fc layer 2 self.abs_max_out: 2602.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.671706/  1.844588, val:  46.25%, val_best:  59.17%, tr:  93.05%, tr_best:  93.97%, epoch time: 44.43 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7523%\n",
      "layer   2  Sparsity: 76.2226%\n",
      "layer   3  Sparsity: 69.3573%\n",
      "total_backward_count 249645 real_backward_count 70293  28.157%\n",
      "fc layer 1 self.abs_max_out: 3531.0\n",
      "lif layer 1 self.abs_max_v: 3531.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.661711/  1.813379, val:  65.42%, val_best:  65.42%, tr:  92.54%, tr_best:  93.97%, epoch time: 45.18 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7488%\n",
      "layer   2  Sparsity: 75.9601%\n",
      "layer   3  Sparsity: 69.4207%\n",
      "total_backward_count 254540 real_backward_count 71477  28.081%\n",
      "lif layer 2 self.abs_max_v: 3119.0\n",
      "fc layer 2 self.abs_max_out: 2685.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.662192/  1.818981, val:  51.67%, val_best:  65.42%, tr:  92.95%, tr_best:  93.97%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7837%\n",
      "layer   2  Sparsity: 76.0601%\n",
      "layer   3  Sparsity: 69.7986%\n",
      "total_backward_count 259435 real_backward_count 72708  28.026%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.660344/  1.805373, val:  62.08%, val_best:  65.42%, tr:  93.46%, tr_best:  93.97%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7695%\n",
      "layer   2  Sparsity: 76.0727%\n",
      "layer   3  Sparsity: 70.0221%\n",
      "total_backward_count 264330 real_backward_count 73903  27.959%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.659827/  1.827420, val:  57.92%, val_best:  65.42%, tr:  92.34%, tr_best:  93.97%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7720%\n",
      "layer   2  Sparsity: 76.1084%\n",
      "layer   3  Sparsity: 70.1080%\n",
      "total_backward_count 269225 real_backward_count 75080  27.887%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.660207/  1.875427, val:  47.08%, val_best:  65.42%, tr:  93.26%, tr_best:  93.97%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7888%\n",
      "layer   2  Sparsity: 76.1438%\n",
      "layer   3  Sparsity: 69.8469%\n",
      "total_backward_count 274120 real_backward_count 76227  27.808%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.649255/  1.790486, val:  48.33%, val_best:  65.42%, tr:  93.77%, tr_best:  93.97%, epoch time: 43.97 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 96.7681%\n",
      "layer   2  Sparsity: 75.9507%\n",
      "layer   3  Sparsity: 69.2752%\n",
      "total_backward_count 279015 real_backward_count 77388  27.736%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.650622/  1.785033, val:  67.92%, val_best:  67.92%, tr:  93.56%, tr_best:  93.97%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.8082%\n",
      "layer   2  Sparsity: 76.0078%\n",
      "layer   3  Sparsity: 69.6149%\n",
      "total_backward_count 283910 real_backward_count 78544  27.665%\n",
      "lif layer 2 self.abs_max_v: 3149.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.645589/  1.788172, val:  65.83%, val_best:  67.92%, tr:  92.54%, tr_best:  93.97%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7827%\n",
      "layer   2  Sparsity: 75.9628%\n",
      "layer   3  Sparsity: 69.5989%\n",
      "total_backward_count 288805 real_backward_count 79679  27.589%\n",
      "lif layer 2 self.abs_max_v: 3269.5\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.642398/  1.837886, val:  43.75%, val_best:  67.92%, tr:  93.36%, tr_best:  93.97%, epoch time: 45.30 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 96.7941%\n",
      "layer   2  Sparsity: 75.9267%\n",
      "layer   3  Sparsity: 69.0130%\n",
      "total_backward_count 293700 real_backward_count 80818  27.517%\n",
      "lif layer 2 self.abs_max_v: 3331.5\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.651267/  1.829215, val:  44.17%, val_best:  67.92%, tr:  93.46%, tr_best:  93.97%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7679%\n",
      "layer   2  Sparsity: 75.6749%\n",
      "layer   3  Sparsity: 68.7527%\n",
      "total_backward_count 298595 real_backward_count 81954  27.447%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.638521/  1.783313, val:  62.50%, val_best:  67.92%, tr:  94.69%, tr_best:  94.69%, epoch time: 45.53 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 96.7482%\n",
      "layer   2  Sparsity: 75.6024%\n",
      "layer   3  Sparsity: 69.0739%\n",
      "total_backward_count 303490 real_backward_count 83007  27.351%\n",
      "lif layer 2 self.abs_max_v: 3380.0\n",
      "fc layer 1 self.abs_max_out: 3600.0\n",
      "lif layer 1 self.abs_max_v: 3600.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.635719/  1.797784, val:  62.50%, val_best:  67.92%, tr:  93.36%, tr_best:  94.69%, epoch time: 45.17 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7780%\n",
      "layer   2  Sparsity: 75.8249%\n",
      "layer   3  Sparsity: 69.5073%\n",
      "total_backward_count 308385 real_backward_count 84090  27.268%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.649369/  1.820414, val:  59.58%, val_best:  67.92%, tr:  92.65%, tr_best:  94.69%, epoch time: 46.02 seconds, 0.77 minutes\n",
      "layer   1  Sparsity: 96.7922%\n",
      "layer   2  Sparsity: 75.9104%\n",
      "layer   3  Sparsity: 69.0947%\n",
      "total_backward_count 313280 real_backward_count 85199  27.196%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.654179/  1.788501, val:  51.25%, val_best:  67.92%, tr:  93.87%, tr_best:  94.69%, epoch time: 44.39 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7850%\n",
      "layer   2  Sparsity: 75.5951%\n",
      "layer   3  Sparsity: 69.0705%\n",
      "total_backward_count 318175 real_backward_count 86275  27.116%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.616840/  1.773419, val:  57.50%, val_best:  67.92%, tr:  95.40%, tr_best:  95.40%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7990%\n",
      "layer   2  Sparsity: 75.6697%\n",
      "layer   3  Sparsity: 69.5519%\n",
      "total_backward_count 323070 real_backward_count 87335  27.033%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.607950/  1.782563, val:  57.50%, val_best:  67.92%, tr:  94.89%, tr_best:  95.40%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7880%\n",
      "layer   2  Sparsity: 75.4977%\n",
      "layer   3  Sparsity: 69.2409%\n",
      "total_backward_count 327965 real_backward_count 88437  26.965%\n",
      "fc layer 3 self.abs_max_out: 510.0\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.627236/  1.800387, val:  65.42%, val_best:  67.92%, tr:  93.56%, tr_best:  95.40%, epoch time: 45.25 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7720%\n",
      "layer   2  Sparsity: 75.5088%\n",
      "layer   3  Sparsity: 69.3703%\n",
      "total_backward_count 332860 real_backward_count 89558  26.906%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.618057/  1.780328, val:  67.92%, val_best:  67.92%, tr:  93.56%, tr_best:  95.40%, epoch time: 44.21 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7762%\n",
      "layer   2  Sparsity: 75.6993%\n",
      "layer   3  Sparsity: 69.4459%\n",
      "total_backward_count 337755 real_backward_count 90630  26.833%\n",
      "fc layer 2 self.abs_max_out: 2778.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.627298/  1.803652, val:  57.08%, val_best:  67.92%, tr:  94.28%, tr_best:  95.40%, epoch time: 44.98 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7657%\n",
      "layer   2  Sparsity: 75.7183%\n",
      "layer   3  Sparsity: 69.2993%\n",
      "total_backward_count 342650 real_backward_count 91735  26.772%\n",
      "fc layer 2 self.abs_max_out: 2833.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.629293/  1.777250, val:  52.92%, val_best:  67.92%, tr:  95.10%, tr_best:  95.40%, epoch time: 45.02 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7661%\n",
      "layer   2  Sparsity: 75.6904%\n",
      "layer   3  Sparsity: 68.5272%\n",
      "total_backward_count 347545 real_backward_count 92891  26.728%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.594532/  1.800041, val:  55.00%, val_best:  67.92%, tr:  94.59%, tr_best:  95.40%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7657%\n",
      "layer   2  Sparsity: 75.7075%\n",
      "layer   3  Sparsity: 68.6061%\n",
      "total_backward_count 352440 real_backward_count 93960  26.660%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.611514/  1.739742, val:  75.00%, val_best:  75.00%, tr:  94.28%, tr_best:  95.40%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7340%\n",
      "layer   2  Sparsity: 75.7523%\n",
      "layer   3  Sparsity: 68.9849%\n",
      "total_backward_count 357335 real_backward_count 95024  26.592%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.597892/  1.777274, val:  49.17%, val_best:  75.00%, tr:  94.89%, tr_best:  95.40%, epoch time: 44.42 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7995%\n",
      "layer   2  Sparsity: 75.9916%\n",
      "layer   3  Sparsity: 69.3477%\n",
      "total_backward_count 362230 real_backward_count 96113  26.534%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.598234/  1.763178, val:  64.17%, val_best:  75.00%, tr:  94.48%, tr_best:  95.40%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7412%\n",
      "layer   2  Sparsity: 75.7302%\n",
      "layer   3  Sparsity: 68.9989%\n",
      "total_backward_count 367125 real_backward_count 97184  26.472%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.601365/  1.763177, val:  55.83%, val_best:  75.00%, tr:  93.36%, tr_best:  95.40%, epoch time: 44.94 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7893%\n",
      "layer   2  Sparsity: 75.9592%\n",
      "layer   3  Sparsity: 68.3428%\n",
      "total_backward_count 372020 real_backward_count 98228  26.404%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.603444/  1.771049, val:  55.83%, val_best:  75.00%, tr:  93.97%, tr_best:  95.40%, epoch time: 45.42 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 96.7735%\n",
      "layer   2  Sparsity: 75.9052%\n",
      "layer   3  Sparsity: 67.9679%\n",
      "total_backward_count 376915 real_backward_count 99293  26.344%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.619541/  1.797481, val:  49.58%, val_best:  75.00%, tr:  93.26%, tr_best:  95.40%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7863%\n",
      "layer   2  Sparsity: 75.7443%\n",
      "layer   3  Sparsity: 67.5595%\n",
      "total_backward_count 381810 real_backward_count 100369  26.288%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.587872/  1.751559, val:  57.08%, val_best:  75.00%, tr:  94.28%, tr_best:  95.40%, epoch time: 45.36 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 96.7773%\n",
      "layer   2  Sparsity: 75.6845%\n",
      "layer   3  Sparsity: 67.4806%\n",
      "total_backward_count 386705 real_backward_count 101450  26.234%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.586997/  1.769966, val:  61.25%, val_best:  75.00%, tr:  94.08%, tr_best:  95.40%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7832%\n",
      "layer   2  Sparsity: 75.6303%\n",
      "layer   3  Sparsity: 68.3342%\n",
      "total_backward_count 391600 real_backward_count 102516  26.179%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.592314/  1.777035, val:  63.33%, val_best:  75.00%, tr:  95.10%, tr_best:  95.40%, epoch time: 45.70 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 96.7666%\n",
      "layer   2  Sparsity: 75.6554%\n",
      "layer   3  Sparsity: 68.4504%\n",
      "total_backward_count 396495 real_backward_count 103563  26.120%\n",
      "lif layer 2 self.abs_max_v: 3390.5\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.607702/  1.754672, val:  65.83%, val_best:  75.00%, tr:  93.77%, tr_best:  95.40%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7914%\n",
      "layer   2  Sparsity: 75.6197%\n",
      "layer   3  Sparsity: 68.6765%\n",
      "total_backward_count 401390 real_backward_count 104646  26.071%\n",
      "fc layer 3 self.abs_max_out: 514.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.604321/  1.771483, val:  56.25%, val_best:  75.00%, tr:  94.38%, tr_best:  95.40%, epoch time: 45.26 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7799%\n",
      "layer   2  Sparsity: 75.4573%\n",
      "layer   3  Sparsity: 68.3904%\n",
      "total_backward_count 406285 real_backward_count 105720  26.021%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.599714/  1.784441, val:  58.75%, val_best:  75.00%, tr:  94.89%, tr_best:  95.40%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7472%\n",
      "layer   2  Sparsity: 75.4963%\n",
      "layer   3  Sparsity: 68.6820%\n",
      "total_backward_count 411180 real_backward_count 106772  25.967%\n",
      "fc layer 3 self.abs_max_out: 522.0\n",
      "fc layer 3 self.abs_max_out: 527.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.601942/  1.746753, val:  68.75%, val_best:  75.00%, tr:  93.97%, tr_best:  95.40%, epoch time: 45.02 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7654%\n",
      "layer   2  Sparsity: 75.4899%\n",
      "layer   3  Sparsity: 68.7633%\n",
      "total_backward_count 416075 real_backward_count 107814  25.912%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.591223/  1.806903, val:  54.58%, val_best:  75.00%, tr:  94.18%, tr_best:  95.40%, epoch time: 44.39 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7814%\n",
      "layer   2  Sparsity: 75.7560%\n",
      "layer   3  Sparsity: 68.4807%\n",
      "total_backward_count 420970 real_backward_count 108925  25.875%\n",
      "lif layer 2 self.abs_max_v: 3425.5\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.601571/  1.772806, val:  75.83%, val_best:  75.83%, tr:  94.59%, tr_best:  95.40%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7868%\n",
      "layer   2  Sparsity: 75.6106%\n",
      "layer   3  Sparsity: 68.2372%\n",
      "total_backward_count 425865 real_backward_count 109962  25.821%\n",
      "lif layer 2 self.abs_max_v: 3427.5\n",
      "lif layer 2 self.abs_max_v: 3445.0\n",
      "lif layer 2 self.abs_max_v: 3482.0\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.597852/  1.762069, val:  65.83%, val_best:  75.83%, tr:  94.48%, tr_best:  95.40%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7544%\n",
      "layer   2  Sparsity: 75.1428%\n",
      "layer   3  Sparsity: 67.7119%\n",
      "total_backward_count 430760 real_backward_count 111029  25.775%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.596813/  1.749574, val:  74.17%, val_best:  75.83%, tr:  94.28%, tr_best:  95.40%, epoch time: 44.43 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7734%\n",
      "layer   2  Sparsity: 75.2457%\n",
      "layer   3  Sparsity: 67.3414%\n",
      "total_backward_count 435655 real_backward_count 112084  25.728%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.595081/  1.761528, val:  65.42%, val_best:  75.83%, tr:  94.08%, tr_best:  95.40%, epoch time: 44.18 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7978%\n",
      "layer   2  Sparsity: 75.3837%\n",
      "layer   3  Sparsity: 67.8077%\n",
      "total_backward_count 440550 real_backward_count 113136  25.681%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.570773/  1.781524, val:  48.75%, val_best:  75.83%, tr:  95.51%, tr_best:  95.51%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7846%\n",
      "layer   2  Sparsity: 75.3845%\n",
      "layer   3  Sparsity: 68.1223%\n",
      "total_backward_count 445445 real_backward_count 114139  25.624%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.567804/  1.719440, val:  57.08%, val_best:  75.83%, tr:  95.71%, tr_best:  95.71%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7560%\n",
      "layer   2  Sparsity: 75.3801%\n",
      "layer   3  Sparsity: 68.0927%\n",
      "total_backward_count 450340 real_backward_count 115139  25.567%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.560030/  1.713823, val:  69.58%, val_best:  75.83%, tr:  93.97%, tr_best:  95.71%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7796%\n",
      "layer   2  Sparsity: 75.1510%\n",
      "layer   3  Sparsity: 67.9396%\n",
      "total_backward_count 455235 real_backward_count 116213  25.528%\n",
      "fc layer 1 self.abs_max_out: 3909.0\n",
      "lif layer 1 self.abs_max_v: 3909.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.551138/  1.731118, val:  61.67%, val_best:  75.83%, tr:  95.61%, tr_best:  95.71%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7713%\n",
      "layer   2  Sparsity: 75.4078%\n",
      "layer   3  Sparsity: 67.6298%\n",
      "total_backward_count 460130 real_backward_count 117231  25.478%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.582861/  1.732380, val:  59.17%, val_best:  75.83%, tr:  95.71%, tr_best:  95.71%, epoch time: 44.71 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.8033%\n",
      "layer   2  Sparsity: 75.5300%\n",
      "layer   3  Sparsity: 68.0736%\n",
      "total_backward_count 465025 real_backward_count 118236  25.426%\n",
      "lif layer 2 self.abs_max_v: 3729.5\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.570084/  1.733593, val:  72.50%, val_best:  75.83%, tr:  95.30%, tr_best:  95.71%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7714%\n",
      "layer   2  Sparsity: 75.3783%\n",
      "layer   3  Sparsity: 67.5550%\n",
      "total_backward_count 469920 real_backward_count 119275  25.382%\n",
      "fc layer 1 self.abs_max_out: 3926.0\n",
      "lif layer 1 self.abs_max_v: 3926.0\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.575348/  1.745474, val:  65.42%, val_best:  75.83%, tr:  94.59%, tr_best:  95.71%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7712%\n",
      "layer   2  Sparsity: 75.4406%\n",
      "layer   3  Sparsity: 67.4656%\n",
      "total_backward_count 474815 real_backward_count 120347  25.346%\n",
      "fc layer 3 self.abs_max_out: 544.0\n",
      "fc layer 3 self.abs_max_out: 577.0\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.565214/  1.724957, val:  62.08%, val_best:  75.83%, tr:  95.40%, tr_best:  95.71%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7921%\n",
      "layer   2  Sparsity: 75.5253%\n",
      "layer   3  Sparsity: 67.7173%\n",
      "total_backward_count 479710 real_backward_count 121377  25.302%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.572596/  1.771381, val:  51.25%, val_best:  75.83%, tr:  96.32%, tr_best:  96.32%, epoch time: 44.98 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.8029%\n",
      "layer   2  Sparsity: 75.5779%\n",
      "layer   3  Sparsity: 67.7622%\n",
      "total_backward_count 484605 real_backward_count 122392  25.256%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.587798/  1.740808, val:  67.50%, val_best:  75.83%, tr:  94.89%, tr_best:  96.32%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7958%\n",
      "layer   2  Sparsity: 75.4395%\n",
      "layer   3  Sparsity: 67.8304%\n",
      "total_backward_count 489500 real_backward_count 123415  25.212%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.569814/  1.731681, val:  71.67%, val_best:  75.83%, tr:  95.30%, tr_best:  96.32%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7805%\n",
      "layer   2  Sparsity: 75.3979%\n",
      "layer   3  Sparsity: 67.7639%\n",
      "total_backward_count 494395 real_backward_count 124410  25.164%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.581694/  1.741057, val:  66.25%, val_best:  75.83%, tr:  95.61%, tr_best:  96.32%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7647%\n",
      "layer   2  Sparsity: 75.2664%\n",
      "layer   3  Sparsity: 67.6756%\n",
      "total_backward_count 499290 real_backward_count 125408  25.117%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.576297/  1.765112, val:  56.25%, val_best:  75.83%, tr:  95.10%, tr_best:  96.32%, epoch time: 44.36 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7608%\n",
      "layer   2  Sparsity: 75.3790%\n",
      "layer   3  Sparsity: 67.9924%\n",
      "total_backward_count 504185 real_backward_count 126424  25.075%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.579382/  1.732793, val:  68.75%, val_best:  75.83%, tr:  95.20%, tr_best:  96.32%, epoch time: 44.19 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7484%\n",
      "layer   2  Sparsity: 75.1082%\n",
      "layer   3  Sparsity: 67.6685%\n",
      "total_backward_count 509080 real_backward_count 127449  25.035%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.581583/  1.744191, val:  66.67%, val_best:  75.83%, tr:  94.89%, tr_best:  96.32%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7499%\n",
      "layer   2  Sparsity: 75.2482%\n",
      "layer   3  Sparsity: 67.7559%\n",
      "total_backward_count 513975 real_backward_count 128470  24.995%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.576154/  1.759265, val:  62.08%, val_best:  75.83%, tr:  95.71%, tr_best:  96.32%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7913%\n",
      "layer   2  Sparsity: 75.4858%\n",
      "layer   3  Sparsity: 67.6677%\n",
      "total_backward_count 518870 real_backward_count 129491  24.956%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.571363/  1.776178, val:  50.42%, val_best:  75.83%, tr:  95.81%, tr_best:  96.32%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7854%\n",
      "layer   2  Sparsity: 75.5440%\n",
      "layer   3  Sparsity: 67.5075%\n",
      "total_backward_count 523765 real_backward_count 130476  24.911%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.553774/  1.718051, val:  65.83%, val_best:  75.83%, tr:  96.22%, tr_best:  96.32%, epoch time: 44.33 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7938%\n",
      "layer   2  Sparsity: 75.3584%\n",
      "layer   3  Sparsity: 67.3144%\n",
      "total_backward_count 528660 real_backward_count 131492  24.873%\n",
      "fc layer 2 self.abs_max_out: 2837.0\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.566482/  1.721957, val:  68.33%, val_best:  75.83%, tr:  95.20%, tr_best:  96.32%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7999%\n",
      "layer   2  Sparsity: 75.3124%\n",
      "layer   3  Sparsity: 67.8385%\n",
      "total_backward_count 533555 real_backward_count 132504  24.834%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.566362/  1.719113, val:  60.42%, val_best:  75.83%, tr:  95.40%, tr_best:  96.32%, epoch time: 43.96 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 96.7741%\n",
      "layer   2  Sparsity: 75.3459%\n",
      "layer   3  Sparsity: 67.8591%\n",
      "total_backward_count 538450 real_backward_count 133519  24.797%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.576107/  1.715168, val:  71.67%, val_best:  75.83%, tr:  95.71%, tr_best:  96.32%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7818%\n",
      "layer   2  Sparsity: 75.1816%\n",
      "layer   3  Sparsity: 67.7212%\n",
      "total_backward_count 543345 real_backward_count 134498  24.754%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.552378/  1.738566, val:  59.58%, val_best:  75.83%, tr:  94.99%, tr_best:  96.32%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7649%\n",
      "layer   2  Sparsity: 74.9877%\n",
      "layer   3  Sparsity: 67.6572%\n",
      "total_backward_count 548240 real_backward_count 135485  24.713%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.549834/  1.704410, val:  70.42%, val_best:  75.83%, tr:  94.99%, tr_best:  96.32%, epoch time: 45.23 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7840%\n",
      "layer   2  Sparsity: 75.2414%\n",
      "layer   3  Sparsity: 67.4812%\n",
      "total_backward_count 553135 real_backward_count 136494  24.676%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.534176/  1.724198, val:  57.08%, val_best:  75.83%, tr:  95.91%, tr_best:  96.32%, epoch time: 45.21 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7624%\n",
      "layer   2  Sparsity: 74.9907%\n",
      "layer   3  Sparsity: 67.1733%\n",
      "total_backward_count 558030 real_backward_count 137439  24.629%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.552669/  1.718617, val:  57.50%, val_best:  75.83%, tr:  95.20%, tr_best:  96.32%, epoch time: 44.91 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7617%\n",
      "layer   2  Sparsity: 75.1653%\n",
      "layer   3  Sparsity: 67.3627%\n",
      "total_backward_count 562925 real_backward_count 138405  24.587%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.558468/  1.727220, val:  63.33%, val_best:  75.83%, tr:  95.40%, tr_best:  96.32%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7815%\n",
      "layer   2  Sparsity: 75.0706%\n",
      "layer   3  Sparsity: 67.2969%\n",
      "total_backward_count 567820 real_backward_count 139417  24.553%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.560983/  1.733898, val:  61.67%, val_best:  75.83%, tr:  95.81%, tr_best:  96.32%, epoch time: 44.54 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7802%\n",
      "layer   2  Sparsity: 75.1120%\n",
      "layer   3  Sparsity: 67.5576%\n",
      "total_backward_count 572715 real_backward_count 140393  24.514%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.571598/  1.727199, val:  60.00%, val_best:  75.83%, tr:  96.42%, tr_best:  96.42%, epoch time: 44.70 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7795%\n",
      "layer   2  Sparsity: 75.0615%\n",
      "layer   3  Sparsity: 67.4943%\n",
      "total_backward_count 577610 real_backward_count 141375  24.476%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.572125/  1.723495, val:  71.25%, val_best:  75.83%, tr:  95.20%, tr_best:  96.42%, epoch time: 44.37 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.8014%\n",
      "layer   2  Sparsity: 75.3438%\n",
      "layer   3  Sparsity: 67.5597%\n",
      "total_backward_count 582505 real_backward_count 142369  24.441%\n",
      "fc layer 2 self.abs_max_out: 2894.0\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.556684/  1.699326, val:  75.00%, val_best:  75.83%, tr:  96.53%, tr_best:  96.53%, epoch time: 44.26 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7781%\n",
      "layer   2  Sparsity: 75.3172%\n",
      "layer   3  Sparsity: 67.5894%\n",
      "total_backward_count 587400 real_backward_count 143334  24.401%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.556152/  1.723868, val:  62.50%, val_best:  75.83%, tr:  95.40%, tr_best:  96.53%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7806%\n",
      "layer   2  Sparsity: 75.2711%\n",
      "layer   3  Sparsity: 67.8579%\n",
      "total_backward_count 592295 real_backward_count 144321  24.366%\n",
      "lif layer 2 self.abs_max_v: 3819.5\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.556509/  1.725168, val:  66.25%, val_best:  75.83%, tr:  95.81%, tr_best:  96.53%, epoch time: 45.30 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 96.7853%\n",
      "layer   2  Sparsity: 75.1491%\n",
      "layer   3  Sparsity: 67.5587%\n",
      "total_backward_count 597190 real_backward_count 145301  24.331%\n",
      "lif layer 2 self.abs_max_v: 3828.0\n",
      "lif layer 1 self.abs_max_v: 3945.5\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.530851/  1.712300, val:  67.08%, val_best:  75.83%, tr:  95.71%, tr_best:  96.53%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7508%\n",
      "layer   2  Sparsity: 75.0280%\n",
      "layer   3  Sparsity: 67.1318%\n",
      "total_backward_count 602085 real_backward_count 146293  24.298%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.531122/  1.685320, val:  70.83%, val_best:  75.83%, tr:  95.81%, tr_best:  96.53%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7818%\n",
      "layer   2  Sparsity: 75.2491%\n",
      "layer   3  Sparsity: 67.6772%\n",
      "total_backward_count 606980 real_backward_count 147280  24.264%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.521374/  1.693768, val:  72.92%, val_best:  75.83%, tr:  95.81%, tr_best:  96.53%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7595%\n",
      "layer   2  Sparsity: 75.0256%\n",
      "layer   3  Sparsity: 67.2090%\n",
      "total_backward_count 611875 real_backward_count 148174  24.216%\n",
      "lif layer 2 self.abs_max_v: 3896.0\n",
      "lif layer 2 self.abs_max_v: 3902.5\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.537467/  1.720943, val:  55.00%, val_best:  75.83%, tr:  95.91%, tr_best:  96.53%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7678%\n",
      "layer   2  Sparsity: 74.8709%\n",
      "layer   3  Sparsity: 67.3349%\n",
      "total_backward_count 616770 real_backward_count 149124  24.178%\n",
      "lif layer 2 self.abs_max_v: 4006.0\n",
      "lif layer 2 self.abs_max_v: 4040.0\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.543418/  1.719508, val:  62.92%, val_best:  75.83%, tr:  95.81%, tr_best:  96.53%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7488%\n",
      "layer   2  Sparsity: 74.7350%\n",
      "layer   3  Sparsity: 67.5013%\n",
      "total_backward_count 621665 real_backward_count 150114  24.147%\n",
      "fc layer 2 self.abs_max_out: 2904.0\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.552156/  1.729451, val:  67.50%, val_best:  75.83%, tr:  95.20%, tr_best:  96.53%, epoch time: 45.10 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7548%\n",
      "layer   2  Sparsity: 74.9482%\n",
      "layer   3  Sparsity: 67.5457%\n",
      "total_backward_count 626560 real_backward_count 151081  24.113%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.564352/  1.721875, val:  70.83%, val_best:  75.83%, tr:  95.81%, tr_best:  96.53%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7664%\n",
      "layer   2  Sparsity: 74.9938%\n",
      "layer   3  Sparsity: 67.3293%\n",
      "total_backward_count 631455 real_backward_count 152043  24.078%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.524722/  1.716802, val:  60.83%, val_best:  75.83%, tr:  94.89%, tr_best:  96.53%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7937%\n",
      "layer   2  Sparsity: 75.0791%\n",
      "layer   3  Sparsity: 67.3345%\n",
      "total_backward_count 636350 real_backward_count 153027  24.048%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.557014/  1.727808, val:  42.92%, val_best:  75.83%, tr:  95.51%, tr_best:  96.53%, epoch time: 44.18 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7872%\n",
      "layer   2  Sparsity: 75.0867%\n",
      "layer   3  Sparsity: 67.2336%\n",
      "total_backward_count 641245 real_backward_count 153967  24.011%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.518564/  1.699480, val:  58.75%, val_best:  75.83%, tr:  96.22%, tr_best:  96.53%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7917%\n",
      "layer   2  Sparsity: 75.1033%\n",
      "layer   3  Sparsity: 67.7832%\n",
      "total_backward_count 646140 real_backward_count 154905  23.974%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.508307/  1.720358, val:  55.42%, val_best:  75.83%, tr:  96.22%, tr_best:  96.53%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7641%\n",
      "layer   2  Sparsity: 75.1654%\n",
      "layer   3  Sparsity: 67.5043%\n",
      "total_backward_count 651035 real_backward_count 155814  23.933%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.523621/  1.712357, val:  69.17%, val_best:  75.83%, tr:  95.71%, tr_best:  96.53%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7442%\n",
      "layer   2  Sparsity: 74.9833%\n",
      "layer   3  Sparsity: 67.4932%\n",
      "total_backward_count 655930 real_backward_count 156801  23.905%\n",
      "fc layer 2 self.abs_max_out: 2955.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.532250/  1.728582, val:  54.17%, val_best:  75.83%, tr:  94.79%, tr_best:  96.53%, epoch time: 44.19 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7849%\n",
      "layer   2  Sparsity: 75.1590%\n",
      "layer   3  Sparsity: 67.5332%\n",
      "total_backward_count 660825 real_backward_count 157804  23.880%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.531868/  1.684407, val:  69.58%, val_best:  75.83%, tr:  96.42%, tr_best:  96.53%, epoch time: 45.02 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7756%\n",
      "layer   2  Sparsity: 74.7974%\n",
      "layer   3  Sparsity: 67.7843%\n",
      "total_backward_count 665720 real_backward_count 158775  23.850%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.517278/  1.650136, val:  69.17%, val_best:  75.83%, tr:  96.02%, tr_best:  96.53%, epoch time: 44.18 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7757%\n",
      "layer   2  Sparsity: 74.9319%\n",
      "layer   3  Sparsity: 67.5695%\n",
      "total_backward_count 670615 real_backward_count 159703  23.814%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.490985/  1.702303, val:  64.17%, val_best:  75.83%, tr:  96.73%, tr_best:  96.73%, epoch time: 44.05 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 96.7564%\n",
      "layer   2  Sparsity: 74.7834%\n",
      "layer   3  Sparsity: 67.2701%\n",
      "total_backward_count 675510 real_backward_count 160594  23.774%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.515129/  1.685251, val:  59.17%, val_best:  75.83%, tr:  97.14%, tr_best:  97.14%, epoch time: 44.29 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7760%\n",
      "layer   2  Sparsity: 74.9856%\n",
      "layer   3  Sparsity: 67.5240%\n",
      "total_backward_count 680405 real_backward_count 161539  23.742%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.502962/  1.711563, val:  60.83%, val_best:  75.83%, tr:  95.91%, tr_best:  97.14%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7678%\n",
      "layer   2  Sparsity: 74.9169%\n",
      "layer   3  Sparsity: 66.9055%\n",
      "total_backward_count 685300 real_backward_count 162514  23.714%\n",
      "fc layer 2 self.abs_max_out: 3071.0\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.510820/  1.668301, val:  67.08%, val_best:  75.83%, tr:  95.71%, tr_best:  97.14%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7689%\n",
      "layer   2  Sparsity: 74.7207%\n",
      "layer   3  Sparsity: 67.0283%\n",
      "total_backward_count 690195 real_backward_count 163458  23.683%\n",
      "lif layer 1 self.abs_max_v: 3978.5\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.503450/  1.697983, val:  55.00%, val_best:  75.83%, tr:  95.61%, tr_best:  97.14%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7773%\n",
      "layer   2  Sparsity: 74.8609%\n",
      "layer   3  Sparsity: 67.6250%\n",
      "total_backward_count 695090 real_backward_count 164412  23.653%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.503014/  1.656111, val:  60.42%, val_best:  75.83%, tr:  96.63%, tr_best:  97.14%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7868%\n",
      "layer   2  Sparsity: 74.8221%\n",
      "layer   3  Sparsity: 67.7516%\n",
      "total_backward_count 699985 real_backward_count 165369  23.625%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.490609/  1.664259, val:  67.08%, val_best:  75.83%, tr:  95.40%, tr_best:  97.14%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7678%\n",
      "layer   2  Sparsity: 74.6954%\n",
      "layer   3  Sparsity: 67.8497%\n",
      "total_backward_count 704880 real_backward_count 166312  23.594%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.517618/  1.678675, val:  68.33%, val_best:  75.83%, tr:  96.02%, tr_best:  97.14%, epoch time: 44.21 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7749%\n",
      "layer   2  Sparsity: 74.8511%\n",
      "layer   3  Sparsity: 67.3476%\n",
      "total_backward_count 709775 real_backward_count 167239  23.562%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.513601/  1.685248, val:  72.08%, val_best:  75.83%, tr:  95.81%, tr_best:  97.14%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7833%\n",
      "layer   2  Sparsity: 74.7332%\n",
      "layer   3  Sparsity: 67.6719%\n",
      "total_backward_count 714670 real_backward_count 168171  23.531%\n",
      "lif layer 1 self.abs_max_v: 3987.0\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.527946/  1.724273, val:  57.50%, val_best:  75.83%, tr:  96.53%, tr_best:  97.14%, epoch time: 44.16 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.8144%\n",
      "layer   2  Sparsity: 74.8132%\n",
      "layer   3  Sparsity: 68.0466%\n",
      "total_backward_count 719565 real_backward_count 169137  23.505%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.514640/  1.726424, val:  48.75%, val_best:  75.83%, tr:  96.53%, tr_best:  97.14%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7834%\n",
      "layer   2  Sparsity: 74.7648%\n",
      "layer   3  Sparsity: 68.2593%\n",
      "total_backward_count 724460 real_backward_count 169996  23.465%\n",
      "lif layer 1 self.abs_max_v: 4012.5\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.515581/  1.679064, val:  72.08%, val_best:  75.83%, tr:  96.42%, tr_best:  97.14%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7479%\n",
      "layer   2  Sparsity: 74.5817%\n",
      "layer   3  Sparsity: 67.7627%\n",
      "total_backward_count 729355 real_backward_count 170942  23.437%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.542333/  1.713435, val:  52.50%, val_best:  75.83%, tr:  96.12%, tr_best:  97.14%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7991%\n",
      "layer   2  Sparsity: 74.8746%\n",
      "layer   3  Sparsity: 66.9004%\n",
      "total_backward_count 734250 real_backward_count 171916  23.414%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.522181/  1.664405, val:  68.33%, val_best:  75.83%, tr:  96.12%, tr_best:  97.14%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7637%\n",
      "layer   2  Sparsity: 74.6672%\n",
      "layer   3  Sparsity: 66.7861%\n",
      "total_backward_count 739145 real_backward_count 172809  23.380%\n",
      "fc layer 1 self.abs_max_out: 4121.0\n",
      "lif layer 1 self.abs_max_v: 4121.0\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.529366/  1.681041, val:  72.50%, val_best:  75.83%, tr:  96.63%, tr_best:  97.14%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7874%\n",
      "layer   2  Sparsity: 74.7960%\n",
      "layer   3  Sparsity: 67.1723%\n",
      "total_backward_count 744040 real_backward_count 173765  23.354%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.531849/  1.671838, val:  72.92%, val_best:  75.83%, tr:  95.40%, tr_best:  97.14%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7882%\n",
      "layer   2  Sparsity: 74.6687%\n",
      "layer   3  Sparsity: 67.7940%\n",
      "total_backward_count 748935 real_backward_count 174688  23.325%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.525933/  1.670212, val:  72.08%, val_best:  75.83%, tr:  96.02%, tr_best:  97.14%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7907%\n",
      "layer   2  Sparsity: 74.4925%\n",
      "layer   3  Sparsity: 67.4386%\n",
      "total_backward_count 753830 real_backward_count 175665  23.303%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.507005/  1.648616, val:  78.75%, val_best:  78.75%, tr:  96.02%, tr_best:  97.14%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7762%\n",
      "layer   2  Sparsity: 74.6361%\n",
      "layer   3  Sparsity: 67.1072%\n",
      "total_backward_count 758725 real_backward_count 176632  23.280%\n",
      "fc layer 1 self.abs_max_out: 4211.0\n",
      "lif layer 1 self.abs_max_v: 4211.0\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.488948/  1.672308, val:  65.83%, val_best:  78.75%, tr:  95.51%, tr_best:  97.14%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7612%\n",
      "layer   2  Sparsity: 74.7944%\n",
      "layer   3  Sparsity: 67.5206%\n",
      "total_backward_count 763620 real_backward_count 177544  23.250%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.499717/  1.697767, val:  59.58%, val_best:  78.75%, tr:  95.91%, tr_best:  97.14%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7727%\n",
      "layer   2  Sparsity: 74.5966%\n",
      "layer   3  Sparsity: 67.5551%\n",
      "total_backward_count 768515 real_backward_count 178458  23.221%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.512201/  1.701020, val:  64.17%, val_best:  78.75%, tr:  96.22%, tr_best:  97.14%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7979%\n",
      "layer   2  Sparsity: 74.7392%\n",
      "layer   3  Sparsity: 67.2473%\n",
      "total_backward_count 773410 real_backward_count 179383  23.194%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.519002/  1.679712, val:  71.25%, val_best:  78.75%, tr:  95.51%, tr_best:  97.14%, epoch time: 44.37 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7838%\n",
      "layer   2  Sparsity: 74.5495%\n",
      "layer   3  Sparsity: 66.7598%\n",
      "total_backward_count 778305 real_backward_count 180364  23.174%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.526023/  1.687188, val:  67.08%, val_best:  78.75%, tr:  97.04%, tr_best:  97.14%, epoch time: 44.16 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7652%\n",
      "layer   2  Sparsity: 74.6028%\n",
      "layer   3  Sparsity: 67.0048%\n",
      "total_backward_count 783200 real_backward_count 181244  23.141%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.513406/  1.674631, val:  72.08%, val_best:  78.75%, tr:  96.02%, tr_best:  97.14%, epoch time: 44.26 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7740%\n",
      "layer   2  Sparsity: 74.5895%\n",
      "layer   3  Sparsity: 66.8454%\n",
      "total_backward_count 788095 real_backward_count 182146  23.112%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.499700/  1.663651, val:  65.83%, val_best:  78.75%, tr:  96.53%, tr_best:  97.14%, epoch time: 43.90 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 96.7706%\n",
      "layer   2  Sparsity: 74.4265%\n",
      "layer   3  Sparsity: 66.9204%\n",
      "total_backward_count 792990 real_backward_count 183107  23.091%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.482529/  1.677143, val:  62.08%, val_best:  78.75%, tr:  95.51%, tr_best:  97.14%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7738%\n",
      "layer   2  Sparsity: 74.6025%\n",
      "layer   3  Sparsity: 66.6975%\n",
      "total_backward_count 797885 real_backward_count 184041  23.066%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.491197/  1.716550, val:  69.17%, val_best:  78.75%, tr:  96.63%, tr_best:  97.14%, epoch time: 44.04 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 96.7630%\n",
      "layer   2  Sparsity: 74.4866%\n",
      "layer   3  Sparsity: 66.5331%\n",
      "total_backward_count 802780 real_backward_count 184943  23.038%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.516867/  1.690034, val:  61.67%, val_best:  78.75%, tr:  95.30%, tr_best:  97.14%, epoch time: 45.23 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7772%\n",
      "layer   2  Sparsity: 74.5628%\n",
      "layer   3  Sparsity: 66.7757%\n",
      "total_backward_count 807675 real_backward_count 185867  23.013%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.511128/  1.714827, val:  63.33%, val_best:  78.75%, tr:  96.53%, tr_best:  97.14%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7596%\n",
      "layer   2  Sparsity: 74.6313%\n",
      "layer   3  Sparsity: 66.8849%\n",
      "total_backward_count 812570 real_backward_count 186814  22.991%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.511498/  1.672569, val:  65.00%, val_best:  78.75%, tr:  96.53%, tr_best:  97.14%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7996%\n",
      "layer   2  Sparsity: 74.7264%\n",
      "layer   3  Sparsity: 67.1958%\n",
      "total_backward_count 817465 real_backward_count 187680  22.959%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.495558/  1.679820, val:  57.50%, val_best:  78.75%, tr:  96.22%, tr_best:  97.14%, epoch time: 44.10 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7475%\n",
      "layer   2  Sparsity: 74.5150%\n",
      "layer   3  Sparsity: 66.9789%\n",
      "total_backward_count 822360 real_backward_count 188587  22.932%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.504731/  1.684186, val:  72.08%, val_best:  78.75%, tr:  94.99%, tr_best:  97.14%, epoch time: 45.00 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7866%\n",
      "layer   2  Sparsity: 74.6639%\n",
      "layer   3  Sparsity: 67.5281%\n",
      "total_backward_count 827255 real_backward_count 189533  22.911%\n",
      "lif layer 1 self.abs_max_v: 4232.5\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.514918/  1.686610, val:  69.17%, val_best:  78.75%, tr:  96.83%, tr_best:  97.14%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7498%\n",
      "layer   2  Sparsity: 74.4952%\n",
      "layer   3  Sparsity: 67.5785%\n",
      "total_backward_count 832150 real_backward_count 190475  22.890%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.517534/  1.713041, val:  63.75%, val_best:  78.75%, tr:  95.91%, tr_best:  97.14%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7856%\n",
      "layer   2  Sparsity: 74.6243%\n",
      "layer   3  Sparsity: 67.6711%\n",
      "total_backward_count 837045 real_backward_count 191357  22.861%\n",
      "lif layer 1 self.abs_max_v: 4305.5\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.520369/  1.710208, val:  61.67%, val_best:  78.75%, tr:  96.32%, tr_best:  97.14%, epoch time: 44.31 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7753%\n",
      "layer   2  Sparsity: 74.5122%\n",
      "layer   3  Sparsity: 67.2902%\n",
      "total_backward_count 841940 real_backward_count 192291  22.839%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.539273/  1.722762, val:  65.42%, val_best:  78.75%, tr:  95.61%, tr_best:  97.14%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7739%\n",
      "layer   2  Sparsity: 74.5930%\n",
      "layer   3  Sparsity: 67.0893%\n",
      "total_backward_count 846835 real_backward_count 193229  22.818%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.513574/  1.680918, val:  56.67%, val_best:  78.75%, tr:  97.04%, tr_best:  97.14%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7892%\n",
      "layer   2  Sparsity: 74.4567%\n",
      "layer   3  Sparsity: 66.5719%\n",
      "total_backward_count 851730 real_backward_count 194071  22.786%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.508551/  1.683158, val:  64.58%, val_best:  78.75%, tr:  96.83%, tr_best:  97.14%, epoch time: 45.09 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7547%\n",
      "layer   2  Sparsity: 74.6329%\n",
      "layer   3  Sparsity: 67.2291%\n",
      "total_backward_count 856625 real_backward_count 194970  22.760%\n",
      "fc layer 1 self.abs_max_out: 4442.0\n",
      "lif layer 1 self.abs_max_v: 4442.0\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.495299/  1.677626, val:  59.58%, val_best:  78.75%, tr:  96.73%, tr_best:  97.14%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7615%\n",
      "layer   2  Sparsity: 74.5132%\n",
      "layer   3  Sparsity: 67.1909%\n",
      "total_backward_count 861520 real_backward_count 195861  22.734%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.494221/  1.655745, val:  74.58%, val_best:  78.75%, tr:  97.65%, tr_best:  97.65%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7906%\n",
      "layer   2  Sparsity: 74.5320%\n",
      "layer   3  Sparsity: 67.3883%\n",
      "total_backward_count 866415 real_backward_count 196732  22.706%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.518570/  1.680449, val:  74.58%, val_best:  78.75%, tr:  96.32%, tr_best:  97.65%, epoch time: 45.53 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 96.7512%\n",
      "layer   2  Sparsity: 74.4837%\n",
      "layer   3  Sparsity: 67.7541%\n",
      "total_backward_count 871310 real_backward_count 197656  22.685%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.506770/  1.661063, val:  69.17%, val_best:  78.75%, tr:  95.51%, tr_best:  97.65%, epoch time: 44.45 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7695%\n",
      "layer   2  Sparsity: 74.4331%\n",
      "layer   3  Sparsity: 67.5756%\n",
      "total_backward_count 876205 real_backward_count 198579  22.664%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.481016/  1.664065, val:  71.25%, val_best:  78.75%, tr:  97.04%, tr_best:  97.65%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7898%\n",
      "layer   2  Sparsity: 74.3681%\n",
      "layer   3  Sparsity: 67.6704%\n",
      "total_backward_count 881100 real_backward_count 199418  22.633%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.496192/  1.653580, val:  70.42%, val_best:  78.75%, tr:  96.02%, tr_best:  97.65%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7629%\n",
      "layer   2  Sparsity: 74.3733%\n",
      "layer   3  Sparsity: 67.9111%\n",
      "total_backward_count 885995 real_backward_count 200337  22.612%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.505591/  1.684450, val:  75.42%, val_best:  78.75%, tr:  95.40%, tr_best:  97.65%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7713%\n",
      "layer   2  Sparsity: 74.5013%\n",
      "layer   3  Sparsity: 68.1451%\n",
      "total_backward_count 890890 real_backward_count 201224  22.587%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.501456/  1.694475, val:  53.33%, val_best:  78.75%, tr:  96.94%, tr_best:  97.65%, epoch time: 45.37 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 96.7548%\n",
      "layer   2  Sparsity: 74.6872%\n",
      "layer   3  Sparsity: 68.1371%\n",
      "total_backward_count 895785 real_backward_count 202082  22.559%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.491603/  1.715243, val:  47.92%, val_best:  78.75%, tr:  96.12%, tr_best:  97.65%, epoch time: 45.08 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7733%\n",
      "layer   2  Sparsity: 74.4477%\n",
      "layer   3  Sparsity: 67.8131%\n",
      "total_backward_count 900680 real_backward_count 202966  22.535%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.492336/  1.669305, val:  68.75%, val_best:  78.75%, tr:  96.02%, tr_best:  97.65%, epoch time: 45.10 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7622%\n",
      "layer   2  Sparsity: 74.4912%\n",
      "layer   3  Sparsity: 66.8660%\n",
      "total_backward_count 905575 real_backward_count 203883  22.514%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.484617/  1.635005, val:  75.00%, val_best:  78.75%, tr:  96.63%, tr_best:  97.65%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7945%\n",
      "layer   2  Sparsity: 74.5623%\n",
      "layer   3  Sparsity: 66.7989%\n",
      "total_backward_count 910470 real_backward_count 204762  22.490%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.496185/  1.661664, val:  61.67%, val_best:  78.75%, tr:  96.53%, tr_best:  97.65%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7615%\n",
      "layer   2  Sparsity: 74.3391%\n",
      "layer   3  Sparsity: 66.8678%\n",
      "total_backward_count 915365 real_backward_count 205632  22.464%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.482412/  1.662823, val:  58.33%, val_best:  78.75%, tr:  96.02%, tr_best:  97.65%, epoch time: 44.35 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7710%\n",
      "layer   2  Sparsity: 74.5495%\n",
      "layer   3  Sparsity: 66.6834%\n",
      "total_backward_count 920260 real_backward_count 206545  22.444%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.492576/  1.697796, val:  62.08%, val_best:  78.75%, tr:  96.22%, tr_best:  97.65%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7583%\n",
      "layer   2  Sparsity: 74.4437%\n",
      "layer   3  Sparsity: 67.0283%\n",
      "total_backward_count 925155 real_backward_count 207477  22.426%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.475050/  1.653052, val:  74.58%, val_best:  78.75%, tr:  96.53%, tr_best:  97.65%, epoch time: 44.04 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 96.7585%\n",
      "layer   2  Sparsity: 74.4119%\n",
      "layer   3  Sparsity: 66.9173%\n",
      "total_backward_count 930050 real_backward_count 208355  22.403%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.491298/  1.657876, val:  77.50%, val_best:  78.75%, tr:  95.20%, tr_best:  97.65%, epoch time: 42.04 seconds, 0.70 minutes\n",
      "layer   1  Sparsity: 96.7839%\n",
      "layer   2  Sparsity: 74.4583%\n",
      "layer   3  Sparsity: 67.0162%\n",
      "total_backward_count 934945 real_backward_count 209260  22.382%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.509886/  1.677848, val:  51.67%, val_best:  78.75%, tr:  96.22%, tr_best:  97.65%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7995%\n",
      "layer   2  Sparsity: 74.5936%\n",
      "layer   3  Sparsity: 67.0283%\n",
      "total_backward_count 939840 real_backward_count 210181  22.363%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.502886/  1.643942, val:  72.92%, val_best:  78.75%, tr:  96.63%, tr_best:  97.65%, epoch time: 44.21 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7580%\n",
      "layer   2  Sparsity: 74.6369%\n",
      "layer   3  Sparsity: 66.9105%\n",
      "total_backward_count 944735 real_backward_count 211093  22.344%\n",
      "fc layer 1 self.abs_max_out: 4606.0\n",
      "lif layer 1 self.abs_max_v: 4606.0\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.468208/  1.709169, val:  55.42%, val_best:  78.75%, tr:  96.83%, tr_best:  97.65%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7456%\n",
      "layer   2  Sparsity: 74.5285%\n",
      "layer   3  Sparsity: 67.1519%\n",
      "total_backward_count 949630 real_backward_count 211952  22.319%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.481136/  1.650497, val:  68.33%, val_best:  78.75%, tr:  96.73%, tr_best:  97.65%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7634%\n",
      "layer   2  Sparsity: 74.4800%\n",
      "layer   3  Sparsity: 67.3559%\n",
      "total_backward_count 954525 real_backward_count 212848  22.299%\n",
      "lif layer 2 self.abs_max_v: 4135.0\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.495685/  1.667398, val:  62.50%, val_best:  78.75%, tr:  96.94%, tr_best:  97.65%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7928%\n",
      "layer   2  Sparsity: 74.5855%\n",
      "layer   3  Sparsity: 67.8285%\n",
      "total_backward_count 959420 real_backward_count 213749  22.279%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.488528/  1.655107, val:  75.00%, val_best:  78.75%, tr:  97.34%, tr_best:  97.65%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7855%\n",
      "layer   2  Sparsity: 74.4402%\n",
      "layer   3  Sparsity: 67.2341%\n",
      "total_backward_count 964315 real_backward_count 214609  22.255%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.481254/  1.634096, val:  68.75%, val_best:  78.75%, tr:  96.42%, tr_best:  97.65%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7706%\n",
      "layer   2  Sparsity: 74.6378%\n",
      "layer   3  Sparsity: 67.1759%\n",
      "total_backward_count 969210 real_backward_count 215451  22.230%\n",
      "fc layer 3 self.abs_max_out: 581.0\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.467415/  1.670222, val:  57.50%, val_best:  78.75%, tr:  97.14%, tr_best:  97.65%, epoch time: 44.84 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7485%\n",
      "layer   2  Sparsity: 74.6459%\n",
      "layer   3  Sparsity: 67.5434%\n",
      "total_backward_count 974105 real_backward_count 216309  22.206%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.474040/  1.644783, val:  62.08%, val_best:  78.75%, tr:  97.34%, tr_best:  97.65%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7792%\n",
      "layer   2  Sparsity: 74.3275%\n",
      "layer   3  Sparsity: 67.4527%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feee08d4da5a48beb5e96e39159768d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÉ‚ñà‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñà‚ñÉ‚ñÖ‚ñÑ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÉ‚ñà‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñà‚ñÉ‚ñÖ‚ñÑ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.97344</td></tr><tr><td>tr_epoch_loss</td><td>1.47404</td></tr><tr><td>val_acc_best</td><td>0.7875</td></tr><tr><td>val_acc_now</td><td>0.62083</td></tr><tr><td>val_loss</td><td>1.64478</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-sweep-102</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/imdb8psw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/imdb8psw</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_111204-imdb8psw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m5ovvwl7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 12000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_134134-m5ovvwl7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m5ovvwl7' target=\"_blank\">distinctive-sweep-105</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m5ovvwl7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m5ovvwl7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_134143_257', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 20, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 12000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 67f09733060e9328908e01cda0ab3532\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=20, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=20, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 592\n",
      "fc layer 1 self.abs_max_out: 119.0\n",
      "lif layer 1 self.abs_max_v: 119.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 121.0\n",
      "lif layer 1 self.abs_max_v: 144.5\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 36.0\n",
      "fc layer 2 self.abs_max_out: 65.0\n",
      "lif layer 2 self.abs_max_v: 69.0\n",
      "smallest_now_T updated: 534\n",
      "fc layer 1 self.abs_max_out: 169.0\n",
      "lif layer 1 self.abs_max_v: 169.0\n",
      "lif layer 1 self.abs_max_v: 210.0\n",
      "fc layer 2 self.abs_max_out: 152.0\n",
      "lif layer 2 self.abs_max_v: 160.0\n",
      "fc layer 3 self.abs_max_out: 33.0\n",
      "fc layer 1 self.abs_max_out: 196.0\n",
      "lif layer 1 self.abs_max_v: 259.0\n",
      "fc layer 2 self.abs_max_out: 194.0\n",
      "lif layer 2 self.abs_max_v: 221.5\n",
      "fc layer 3 self.abs_max_out: 63.0\n",
      "lif layer 1 self.abs_max_v: 272.5\n",
      "lif layer 1 self.abs_max_v: 301.5\n",
      "fc layer 2 self.abs_max_out: 263.0\n",
      "lif layer 2 self.abs_max_v: 232.5\n",
      "smallest_now_T updated: 407\n",
      "fc layer 1 self.abs_max_out: 236.0\n",
      "lif layer 2 self.abs_max_v: 299.0\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "fc layer 1 self.abs_max_out: 282.0\n",
      "lif layer 1 self.abs_max_v: 304.0\n",
      "fc layer 3 self.abs_max_out: 74.0\n",
      "lif layer 1 self.abs_max_v: 340.0\n",
      "fc layer 3 self.abs_max_out: 79.0\n",
      "lif layer 2 self.abs_max_v: 319.5\n",
      "fc layer 3 self.abs_max_out: 84.0\n",
      "fc layer 2 self.abs_max_out: 294.0\n",
      "lif layer 2 self.abs_max_v: 395.5\n",
      "fc layer 3 self.abs_max_out: 89.0\n",
      "lif layer 1 self.abs_max_v: 345.0\n",
      "fc layer 3 self.abs_max_out: 120.0\n",
      "lif layer 1 self.abs_max_v: 374.0\n",
      "fc layer 3 self.abs_max_out: 132.0\n",
      "smallest_now_T updated: 345\n",
      "fc layer 2 self.abs_max_out: 295.0\n",
      "fc layer 1 self.abs_max_out: 291.0\n",
      "fc layer 2 self.abs_max_out: 331.0\n",
      "lif layer 1 self.abs_max_v: 383.0\n",
      "fc layer 1 self.abs_max_out: 374.0\n",
      "lif layer 2 self.abs_max_v: 399.0\n",
      "fc layer 1 self.abs_max_out: 410.0\n",
      "lif layer 1 self.abs_max_v: 410.0\n",
      "fc layer 3 self.abs_max_out: 141.0\n",
      "smallest_now_T updated: 317\n",
      "fc layer 3 self.abs_max_out: 165.0\n",
      "fc layer 2 self.abs_max_out: 351.0\n",
      "lif layer 2 self.abs_max_v: 421.5\n",
      "fc layer 2 self.abs_max_out: 366.0\n",
      "fc layer 2 self.abs_max_out: 398.0\n",
      "lif layer 2 self.abs_max_v: 460.5\n",
      "fc layer 1 self.abs_max_out: 413.0\n",
      "lif layer 1 self.abs_max_v: 413.0\n",
      "lif layer 1 self.abs_max_v: 429.0\n",
      "smallest_now_T updated: 286\n",
      "fc layer 2 self.abs_max_out: 468.0\n",
      "lif layer 2 self.abs_max_v: 531.5\n",
      "lif layer 1 self.abs_max_v: 432.0\n",
      "lif layer 1 self.abs_max_v: 492.5\n",
      "fc layer 1 self.abs_max_out: 435.0\n",
      "fc layer 3 self.abs_max_out: 176.0\n",
      "fc layer 3 self.abs_max_out: 193.0\n",
      "fc layer 1 self.abs_max_out: 448.0\n",
      "fc layer 1 self.abs_max_out: 601.0\n",
      "lif layer 1 self.abs_max_v: 601.0\n",
      "fc layer 1 self.abs_max_out: 684.0\n",
      "lif layer 1 self.abs_max_v: 684.0\n",
      "lif layer 2 self.abs_max_v: 566.5\n",
      "smallest_now_T updated: 247\n",
      "lif layer 2 self.abs_max_v: 626.0\n",
      "smallest_now_T updated: 192\n",
      "fc layer 3 self.abs_max_out: 201.0\n",
      "fc layer 3 self.abs_max_out: 202.0\n",
      "lif layer 2 self.abs_max_v: 687.5\n",
      "fc layer 2 self.abs_max_out: 483.0\n",
      "fc layer 2 self.abs_max_out: 499.0\n",
      "fc layer 3 self.abs_max_out: 223.0\n",
      "fc layer 1 self.abs_max_out: 711.0\n",
      "lif layer 1 self.abs_max_v: 711.0\n",
      "fc layer 2 self.abs_max_out: 537.0\n",
      "fc layer 1 self.abs_max_out: 764.0\n",
      "lif layer 1 self.abs_max_v: 764.0\n",
      "fc layer 2 self.abs_max_out: 562.0\n",
      "fc layer 2 self.abs_max_out: 620.0\n",
      "fc layer 1 self.abs_max_out: 887.0\n",
      "lif layer 1 self.abs_max_v: 887.0\n",
      "lif layer 2 self.abs_max_v: 736.5\n",
      "smallest_now_T_val updated: 552\n",
      "smallest_now_T_val updated: 456\n",
      "smallest_now_T_val updated: 448\n",
      "smallest_now_T_val updated: 440\n",
      "smallest_now_T_val updated: 368\n",
      "smallest_now_T_val updated: 137\n",
      "fc layer 2 self.abs_max_out: 621.0\n",
      "fc layer 2 self.abs_max_out: 648.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.174951/  2.174302, val:  32.92%, val_best:  32.92%, tr:  51.28%, tr_best:  51.28%, epoch time: 44.91 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7628%\n",
      "layer   2  Sparsity: 92.3764%\n",
      "layer   3  Sparsity: 91.4929%\n",
      "total_backward_count 4895 real_backward_count 3079  62.901%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 895.0\n",
      "lif layer 1 self.abs_max_v: 895.0\n",
      "lif layer 2 self.abs_max_v: 777.5\n",
      "fc layer 2 self.abs_max_out: 664.0\n",
      "fc layer 2 self.abs_max_out: 687.0\n",
      "fc layer 2 self.abs_max_out: 706.0\n",
      "fc layer 1 self.abs_max_out: 967.0\n",
      "lif layer 1 self.abs_max_v: 967.0\n",
      "fc layer 1 self.abs_max_out: 1184.0\n",
      "lif layer 1 self.abs_max_v: 1184.0\n",
      "lif layer 2 self.abs_max_v: 781.0\n",
      "lif layer 2 self.abs_max_v: 877.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.110025/  2.162672, val:  37.92%, val_best:  37.92%, tr:  68.23%, tr_best:  68.23%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7794%\n",
      "layer   2  Sparsity: 90.4185%\n",
      "layer   3  Sparsity: 88.8193%\n",
      "total_backward_count 9790 real_backward_count 5422  55.383%\n",
      "fc layer 2 self.abs_max_out: 711.0\n",
      "fc layer 2 self.abs_max_out: 738.0\n",
      "fc layer 3 self.abs_max_out: 236.0\n",
      "fc layer 2 self.abs_max_out: 805.0\n",
      "fc layer 1 self.abs_max_out: 1266.0\n",
      "lif layer 1 self.abs_max_v: 1266.0\n",
      "lif layer 2 self.abs_max_v: 904.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.085915/  2.164992, val:  39.58%, val_best:  39.58%, tr:  72.52%, tr_best:  72.52%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7912%\n",
      "layer   2  Sparsity: 89.6049%\n",
      "layer   3  Sparsity: 87.3231%\n",
      "total_backward_count 14685 real_backward_count 7509  51.134%\n",
      "lif layer 2 self.abs_max_v: 1046.5\n",
      "fc layer 1 self.abs_max_out: 1319.0\n",
      "lif layer 1 self.abs_max_v: 1319.0\n",
      "fc layer 2 self.abs_max_out: 833.0\n",
      "fc layer 1 self.abs_max_out: 1381.0\n",
      "lif layer 1 self.abs_max_v: 1381.0\n",
      "fc layer 2 self.abs_max_out: 839.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.089014/  2.128531, val:  32.50%, val_best:  39.58%, tr:  74.16%, tr_best:  74.16%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7670%\n",
      "layer   2  Sparsity: 88.9460%\n",
      "layer   3  Sparsity: 86.4454%\n",
      "total_backward_count 19580 real_backward_count 9497  48.504%\n",
      "fc layer 2 self.abs_max_out: 917.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.068866/  2.148144, val:  35.42%, val_best:  39.58%, tr:  77.22%, tr_best:  77.22%, epoch time: 44.89 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7764%\n",
      "layer   2  Sparsity: 88.7764%\n",
      "layer   3  Sparsity: 85.7256%\n",
      "total_backward_count 24475 real_backward_count 11429  46.697%\n",
      "fc layer 3 self.abs_max_out: 241.0\n",
      "fc layer 3 self.abs_max_out: 264.0\n",
      "fc layer 1 self.abs_max_out: 1442.0\n",
      "lif layer 1 self.abs_max_v: 1442.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.070185/  2.138430, val:  37.08%, val_best:  39.58%, tr:  77.73%, tr_best:  77.73%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7738%\n",
      "layer   2  Sparsity: 88.4039%\n",
      "layer   3  Sparsity: 85.8739%\n",
      "total_backward_count 29370 real_backward_count 13365  45.506%\n",
      "fc layer 2 self.abs_max_out: 938.0\n",
      "fc layer 1 self.abs_max_out: 1448.0\n",
      "lif layer 1 self.abs_max_v: 1448.0\n",
      "fc layer 1 self.abs_max_out: 1483.0\n",
      "lif layer 1 self.abs_max_v: 1483.0\n",
      "fc layer 2 self.abs_max_out: 995.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.064257/  2.149414, val:  36.67%, val_best:  39.58%, tr:  78.96%, tr_best:  78.96%, epoch time: 44.36 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7942%\n",
      "layer   2  Sparsity: 88.1083%\n",
      "layer   3  Sparsity: 85.6929%\n",
      "total_backward_count 34265 real_backward_count 15190  44.331%\n",
      "lif layer 2 self.abs_max_v: 1103.5\n",
      "fc layer 1 self.abs_max_out: 1566.0\n",
      "lif layer 1 self.abs_max_v: 1566.0\n",
      "fc layer 1 self.abs_max_out: 1652.0\n",
      "lif layer 1 self.abs_max_v: 1652.0\n",
      "fc layer 2 self.abs_max_out: 1013.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  2.054392/  2.123566, val:  38.75%, val_best:  39.58%, tr:  79.98%, tr_best:  79.98%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7838%\n",
      "layer   2  Sparsity: 87.8704%\n",
      "layer   3  Sparsity: 85.2475%\n",
      "total_backward_count 39160 real_backward_count 16969  43.332%\n",
      "fc layer 1 self.abs_max_out: 1757.0\n",
      "lif layer 1 self.abs_max_v: 1757.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  2.061446/  2.117362, val:  45.83%, val_best:  45.83%, tr:  79.47%, tr_best:  79.98%, epoch time: 44.39 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7725%\n",
      "layer   2  Sparsity: 87.8464%\n",
      "layer   3  Sparsity: 85.2947%\n",
      "total_backward_count 44055 real_backward_count 18820  42.719%\n",
      "fc layer 2 self.abs_max_out: 1055.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  2.047997/  2.134754, val:  39.17%, val_best:  45.83%, tr:  81.51%, tr_best:  81.51%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7658%\n",
      "layer   2  Sparsity: 87.7650%\n",
      "layer   3  Sparsity: 84.8074%\n",
      "total_backward_count 48950 real_backward_count 20591  42.065%\n",
      "fc layer 1 self.abs_max_out: 1786.0\n",
      "lif layer 1 self.abs_max_v: 1786.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  2.033751/  2.139431, val:  36.67%, val_best:  45.83%, tr:  81.61%, tr_best:  81.61%, epoch time: 44.42 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7454%\n",
      "layer   2  Sparsity: 87.7944%\n",
      "layer   3  Sparsity: 84.2995%\n",
      "total_backward_count 53845 real_backward_count 22329  41.469%\n",
      "lif layer 2 self.abs_max_v: 1181.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  2.040916/  2.127678, val:  42.50%, val_best:  45.83%, tr:  80.08%, tr_best:  81.61%, epoch time: 44.98 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7900%\n",
      "layer   2  Sparsity: 88.2066%\n",
      "layer   3  Sparsity: 84.2141%\n",
      "total_backward_count 58740 real_backward_count 24060  40.960%\n",
      "fc layer 1 self.abs_max_out: 1831.0\n",
      "lif layer 1 self.abs_max_v: 1831.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  2.026118/  2.110280, val:  38.33%, val_best:  45.83%, tr:  83.55%, tr_best:  83.55%, epoch time: 44.10 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 96.7570%\n",
      "layer   2  Sparsity: 87.9879%\n",
      "layer   3  Sparsity: 83.8842%\n",
      "total_backward_count 63635 real_backward_count 25717  40.413%\n",
      "fc layer 2 self.abs_max_out: 1099.0\n",
      "fc layer 2 self.abs_max_out: 1114.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  2.020771/  2.105637, val:  27.92%, val_best:  45.83%, tr:  84.78%, tr_best:  84.78%, epoch time: 44.45 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7753%\n",
      "layer   2  Sparsity: 87.7864%\n",
      "layer   3  Sparsity: 84.1013%\n",
      "total_backward_count 68530 real_backward_count 27349  39.908%\n",
      "fc layer 1 self.abs_max_out: 1855.0\n",
      "lif layer 1 self.abs_max_v: 1855.0\n",
      "fc layer 1 self.abs_max_out: 1856.0\n",
      "lif layer 1 self.abs_max_v: 1856.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  2.019710/  2.122514, val:  34.58%, val_best:  45.83%, tr:  82.64%, tr_best:  84.78%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7799%\n",
      "layer   2  Sparsity: 87.9684%\n",
      "layer   3  Sparsity: 84.4338%\n",
      "total_backward_count 73425 real_backward_count 28987  39.478%\n",
      "fc layer 1 self.abs_max_out: 1886.0\n",
      "lif layer 1 self.abs_max_v: 1886.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  2.030781/  2.127432, val:  29.58%, val_best:  45.83%, tr:  82.84%, tr_best:  84.78%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7685%\n",
      "layer   2  Sparsity: 87.6878%\n",
      "layer   3  Sparsity: 84.4943%\n",
      "total_backward_count 78320 real_backward_count 30630  39.109%\n",
      "fc layer 1 self.abs_max_out: 1892.0\n",
      "lif layer 1 self.abs_max_v: 1892.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  2.015701/  2.118758, val:  46.25%, val_best:  46.25%, tr:  84.78%, tr_best:  84.78%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7829%\n",
      "layer   2  Sparsity: 87.5394%\n",
      "layer   3  Sparsity: 83.8117%\n",
      "total_backward_count 83215 real_backward_count 32254  38.760%\n",
      "fc layer 1 self.abs_max_out: 1930.0\n",
      "lif layer 1 self.abs_max_v: 1930.0\n",
      "fc layer 2 self.abs_max_out: 1138.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  2.017030/  2.103629, val:  45.83%, val_best:  46.25%, tr:  85.80%, tr_best:  85.80%, epoch time: 45.00 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7783%\n",
      "layer   2  Sparsity: 87.2577%\n",
      "layer   3  Sparsity: 83.5925%\n",
      "total_backward_count 88110 real_backward_count 33862  38.432%\n",
      "fc layer 2 self.abs_max_out: 1139.0\n",
      "fc layer 2 self.abs_max_out: 1154.0\n",
      "fc layer 1 self.abs_max_out: 1950.0\n",
      "lif layer 1 self.abs_max_v: 1950.0\n",
      "fc layer 2 self.abs_max_out: 1165.0\n",
      "fc layer 2 self.abs_max_out: 1179.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  2.014697/  2.101222, val:  40.42%, val_best:  46.25%, tr:  85.70%, tr_best:  85.80%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7750%\n",
      "layer   2  Sparsity: 87.2101%\n",
      "layer   3  Sparsity: 83.5703%\n",
      "total_backward_count 93005 real_backward_count 35511  38.182%\n",
      "fc layer 1 self.abs_max_out: 2039.0\n",
      "lif layer 1 self.abs_max_v: 2039.0\n",
      "fc layer 2 self.abs_max_out: 1189.0\n",
      "lif layer 2 self.abs_max_v: 1189.0\n",
      "fc layer 2 self.abs_max_out: 1267.0\n",
      "lif layer 2 self.abs_max_v: 1267.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  2.007211/  2.094452, val:  38.33%, val_best:  46.25%, tr:  85.39%, tr_best:  85.80%, epoch time: 44.84 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7572%\n",
      "layer   2  Sparsity: 87.1080%\n",
      "layer   3  Sparsity: 83.4507%\n",
      "total_backward_count 97900 real_backward_count 37049  37.844%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  2.010550/  2.093231, val:  40.00%, val_best:  46.25%, tr:  84.88%, tr_best:  85.80%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7784%\n",
      "layer   2  Sparsity: 86.9106%\n",
      "layer   3  Sparsity: 83.1710%\n",
      "total_backward_count 102795 real_backward_count 38625  37.575%\n",
      "fc layer 2 self.abs_max_out: 1292.0\n",
      "lif layer 2 self.abs_max_v: 1292.0\n",
      "fc layer 2 self.abs_max_out: 1308.0\n",
      "lif layer 2 self.abs_max_v: 1308.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  2.007424/  2.107173, val:  41.25%, val_best:  46.25%, tr:  84.17%, tr_best:  85.80%, epoch time: 45.32 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 96.7877%\n",
      "layer   2  Sparsity: 87.2538%\n",
      "layer   3  Sparsity: 83.2448%\n",
      "total_backward_count 107690 real_backward_count 40258  37.383%\n",
      "fc layer 2 self.abs_max_out: 1340.0\n",
      "lif layer 2 self.abs_max_v: 1340.0\n",
      "fc layer 2 self.abs_max_out: 1365.0\n",
      "lif layer 2 self.abs_max_v: 1365.0\n",
      "fc layer 1 self.abs_max_out: 2044.0\n",
      "lif layer 1 self.abs_max_v: 2044.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.993561/  2.079714, val:  45.00%, val_best:  46.25%, tr:  85.29%, tr_best:  85.80%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7900%\n",
      "layer   2  Sparsity: 87.1366%\n",
      "layer   3  Sparsity: 83.0414%\n",
      "total_backward_count 112585 real_backward_count 41843  37.166%\n",
      "fc layer 1 self.abs_max_out: 2075.0\n",
      "lif layer 1 self.abs_max_v: 2075.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.978281/  2.076720, val:  50.42%, val_best:  50.42%, tr:  86.93%, tr_best:  86.93%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7649%\n",
      "layer   2  Sparsity: 87.1135%\n",
      "layer   3  Sparsity: 82.9498%\n",
      "total_backward_count 117480 real_backward_count 43379  36.925%\n",
      "fc layer 3 self.abs_max_out: 268.0\n",
      "fc layer 1 self.abs_max_out: 2117.0\n",
      "lif layer 1 self.abs_max_v: 2117.0\n",
      "fc layer 1 self.abs_max_out: 2233.0\n",
      "lif layer 1 self.abs_max_v: 2233.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.989107/  2.095987, val:  40.42%, val_best:  50.42%, tr:  84.58%, tr_best:  86.93%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7973%\n",
      "layer   2  Sparsity: 87.1854%\n",
      "layer   3  Sparsity: 82.9868%\n",
      "total_backward_count 122375 real_backward_count 44946  36.728%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.986457/  2.075376, val:  55.42%, val_best:  55.42%, tr:  85.90%, tr_best:  86.93%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7806%\n",
      "layer   2  Sparsity: 87.3925%\n",
      "layer   3  Sparsity: 82.8857%\n",
      "total_backward_count 127270 real_backward_count 46566  36.588%\n",
      "fc layer 2 self.abs_max_out: 1374.0\n",
      "lif layer 2 self.abs_max_v: 1374.0\n",
      "fc layer 1 self.abs_max_out: 2293.0\n",
      "lif layer 1 self.abs_max_v: 2293.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.982250/  2.059626, val:  54.17%, val_best:  55.42%, tr:  85.80%, tr_best:  86.93%, epoch time: 45.00 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7734%\n",
      "layer   2  Sparsity: 87.2973%\n",
      "layer   3  Sparsity: 82.8197%\n",
      "total_backward_count 132165 real_backward_count 48140  36.424%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.978159/  2.082605, val:  53.33%, val_best:  55.42%, tr:  87.44%, tr_best:  87.44%, epoch time: 44.33 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7799%\n",
      "layer   2  Sparsity: 87.2902%\n",
      "layer   3  Sparsity: 82.7591%\n",
      "total_backward_count 137060 real_backward_count 49660  36.232%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.970522/  2.059474, val:  51.67%, val_best:  55.42%, tr:  85.90%, tr_best:  87.44%, epoch time: 45.11 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7798%\n",
      "layer   2  Sparsity: 87.1689%\n",
      "layer   3  Sparsity: 82.4568%\n",
      "total_backward_count 141955 real_backward_count 51147  36.030%\n",
      "fc layer 3 self.abs_max_out: 269.0\n",
      "fc layer 3 self.abs_max_out: 279.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.964592/  2.034599, val:  38.75%, val_best:  55.42%, tr:  86.82%, tr_best:  87.44%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7793%\n",
      "layer   2  Sparsity: 87.3576%\n",
      "layer   3  Sparsity: 82.3303%\n",
      "total_backward_count 146850 real_backward_count 52645  35.850%\n",
      "fc layer 2 self.abs_max_out: 1378.0\n",
      "lif layer 2 self.abs_max_v: 1378.0\n",
      "fc layer 2 self.abs_max_out: 1454.0\n",
      "lif layer 2 self.abs_max_v: 1454.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.952713/  2.047827, val:  48.75%, val_best:  55.42%, tr:  86.41%, tr_best:  87.44%, epoch time: 45.53 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 96.7878%\n",
      "layer   2  Sparsity: 87.2273%\n",
      "layer   3  Sparsity: 82.2798%\n",
      "total_backward_count 151745 real_backward_count 54179  35.704%\n",
      "fc layer 2 self.abs_max_out: 1474.0\n",
      "lif layer 2 self.abs_max_v: 1474.0\n",
      "fc layer 2 self.abs_max_out: 1543.0\n",
      "lif layer 2 self.abs_max_v: 1543.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.973204/  2.048073, val:  46.67%, val_best:  55.42%, tr:  88.05%, tr_best:  88.05%, epoch time: 42.90 seconds, 0.71 minutes\n",
      "layer   1  Sparsity: 96.7930%\n",
      "layer   2  Sparsity: 87.0987%\n",
      "layer   3  Sparsity: 82.3257%\n",
      "total_backward_count 156640 real_backward_count 55713  35.568%\n",
      "fc layer 3 self.abs_max_out: 293.0\n",
      "fc layer 3 self.abs_max_out: 295.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.971773/  2.084399, val:  42.50%, val_best:  55.42%, tr:  85.60%, tr_best:  88.05%, epoch time: 43.35 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 96.7537%\n",
      "layer   2  Sparsity: 86.8469%\n",
      "layer   3  Sparsity: 82.5586%\n",
      "total_backward_count 161535 real_backward_count 57188  35.403%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.976671/  2.050189, val:  49.58%, val_best:  55.42%, tr:  86.82%, tr_best:  88.05%, epoch time: 45.11 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7623%\n",
      "layer   2  Sparsity: 86.8802%\n",
      "layer   3  Sparsity: 82.6121%\n",
      "total_backward_count 166430 real_backward_count 58718  35.281%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.962032/  2.054141, val:  45.42%, val_best:  55.42%, tr:  87.33%, tr_best:  88.05%, epoch time: 44.14 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7453%\n",
      "layer   2  Sparsity: 86.7069%\n",
      "layer   3  Sparsity: 82.3550%\n",
      "total_backward_count 171325 real_backward_count 60141  35.103%\n",
      "fc layer 3 self.abs_max_out: 314.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.952818/  2.039312, val:  60.00%, val_best:  60.00%, tr:  87.23%, tr_best:  88.05%, epoch time: 44.28 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7864%\n",
      "layer   2  Sparsity: 86.6507%\n",
      "layer   3  Sparsity: 82.0541%\n",
      "total_backward_count 176220 real_backward_count 61630  34.973%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.942244/  2.032170, val:  53.33%, val_best:  60.00%, tr:  87.95%, tr_best:  88.05%, epoch time: 44.19 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7963%\n",
      "layer   2  Sparsity: 86.8135%\n",
      "layer   3  Sparsity: 81.7425%\n",
      "total_backward_count 181115 real_backward_count 63102  34.841%\n",
      "fc layer 1 self.abs_max_out: 2308.0\n",
      "lif layer 1 self.abs_max_v: 2308.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.937661/  2.043487, val:  42.08%, val_best:  60.00%, tr:  88.46%, tr_best:  88.46%, epoch time: 44.24 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7629%\n",
      "layer   2  Sparsity: 86.8455%\n",
      "layer   3  Sparsity: 81.8781%\n",
      "total_backward_count 186010 real_backward_count 64536  34.695%\n",
      "fc layer 1 self.abs_max_out: 2332.0\n",
      "lif layer 1 self.abs_max_v: 2332.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.940378/  2.023499, val:  55.42%, val_best:  60.00%, tr:  88.97%, tr_best:  88.97%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7891%\n",
      "layer   2  Sparsity: 86.7362%\n",
      "layer   3  Sparsity: 81.5649%\n",
      "total_backward_count 190905 real_backward_count 66028  34.587%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.948016/  2.026376, val:  48.33%, val_best:  60.00%, tr:  88.15%, tr_best:  88.97%, epoch time: 44.36 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7935%\n",
      "layer   2  Sparsity: 86.7587%\n",
      "layer   3  Sparsity: 82.0115%\n",
      "total_backward_count 195800 real_backward_count 67477  34.462%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.926858/  2.013001, val:  47.50%, val_best:  60.00%, tr:  88.97%, tr_best:  88.97%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7436%\n",
      "layer   2  Sparsity: 86.4791%\n",
      "layer   3  Sparsity: 81.9880%\n",
      "total_backward_count 200695 real_backward_count 68902  34.332%\n",
      "fc layer 1 self.abs_max_out: 2346.0\n",
      "lif layer 1 self.abs_max_v: 2346.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.928763/  2.009076, val:  50.83%, val_best:  60.00%, tr:  89.79%, tr_best:  89.79%, epoch time: 44.20 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7795%\n",
      "layer   2  Sparsity: 86.4295%\n",
      "layer   3  Sparsity: 81.9002%\n",
      "total_backward_count 205590 real_backward_count 70327  34.207%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.915616/  2.032444, val:  47.08%, val_best:  60.00%, tr:  89.38%, tr_best:  89.79%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7406%\n",
      "layer   2  Sparsity: 86.6203%\n",
      "layer   3  Sparsity: 81.8779%\n",
      "total_backward_count 210485 real_backward_count 71729  34.078%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.924217/  2.032568, val:  58.75%, val_best:  60.00%, tr:  89.07%, tr_best:  89.79%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7721%\n",
      "layer   2  Sparsity: 87.0030%\n",
      "layer   3  Sparsity: 81.9319%\n",
      "total_backward_count 215380 real_backward_count 73149  33.963%\n",
      "fc layer 1 self.abs_max_out: 2354.0\n",
      "lif layer 1 self.abs_max_v: 2354.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.937795/  2.039493, val:  52.50%, val_best:  60.00%, tr:  89.99%, tr_best:  89.99%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7769%\n",
      "layer   2  Sparsity: 86.9799%\n",
      "layer   3  Sparsity: 82.1091%\n",
      "total_backward_count 220275 real_backward_count 74554  33.846%\n",
      "fc layer 1 self.abs_max_out: 2505.0\n",
      "lif layer 1 self.abs_max_v: 2505.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.945287/  2.039652, val:  35.42%, val_best:  60.00%, tr:  87.95%, tr_best:  89.99%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7648%\n",
      "layer   2  Sparsity: 87.0043%\n",
      "layer   3  Sparsity: 82.2525%\n",
      "total_backward_count 225170 real_backward_count 75949  33.730%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.935482/  2.021704, val:  59.58%, val_best:  60.00%, tr:  89.99%, tr_best:  89.99%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7837%\n",
      "layer   2  Sparsity: 87.0803%\n",
      "layer   3  Sparsity: 81.6319%\n",
      "total_backward_count 230065 real_backward_count 77382  33.635%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.925959/  2.019897, val:  43.75%, val_best:  60.00%, tr:  89.89%, tr_best:  89.99%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.8029%\n",
      "layer   2  Sparsity: 87.1884%\n",
      "layer   3  Sparsity: 81.4235%\n",
      "total_backward_count 234960 real_backward_count 78797  33.536%\n",
      "fc layer 1 self.abs_max_out: 2766.0\n",
      "lif layer 1 self.abs_max_v: 2766.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.919966/  2.020126, val:  55.00%, val_best:  60.00%, tr:  89.89%, tr_best:  89.99%, epoch time: 44.23 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7831%\n",
      "layer   2  Sparsity: 87.1586%\n",
      "layer   3  Sparsity: 81.6454%\n",
      "total_backward_count 239855 real_backward_count 80173  33.426%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.927057/  2.016362, val:  52.08%, val_best:  60.00%, tr:  90.19%, tr_best:  90.19%, epoch time: 44.70 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7586%\n",
      "layer   2  Sparsity: 87.0324%\n",
      "layer   3  Sparsity: 81.7076%\n",
      "total_backward_count 244750 real_backward_count 81601  33.341%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.930363/  2.038218, val:  40.00%, val_best:  60.00%, tr:  89.48%, tr_best:  90.19%, epoch time: 44.36 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7523%\n",
      "layer   2  Sparsity: 87.1011%\n",
      "layer   3  Sparsity: 82.2503%\n",
      "total_backward_count 249645 real_backward_count 83024  33.257%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.928037/  2.000763, val:  46.67%, val_best:  60.00%, tr:  89.48%, tr_best:  90.19%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7488%\n",
      "layer   2  Sparsity: 87.1133%\n",
      "layer   3  Sparsity: 82.3778%\n",
      "total_backward_count 254540 real_backward_count 84409  33.161%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.923480/  2.000496, val:  51.67%, val_best:  60.00%, tr:  89.89%, tr_best:  90.19%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7837%\n",
      "layer   2  Sparsity: 87.1781%\n",
      "layer   3  Sparsity: 82.2388%\n",
      "total_backward_count 259435 real_backward_count 85850  33.091%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.917761/  2.009604, val:  66.25%, val_best:  66.25%, tr:  89.68%, tr_best:  90.19%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7695%\n",
      "layer   2  Sparsity: 87.0068%\n",
      "layer   3  Sparsity: 81.7713%\n",
      "total_backward_count 264330 real_backward_count 87236  33.003%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.913563/  2.033407, val:  55.42%, val_best:  66.25%, tr:  89.48%, tr_best:  90.19%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7720%\n",
      "layer   2  Sparsity: 87.0801%\n",
      "layer   3  Sparsity: 81.7040%\n",
      "total_backward_count 269225 real_backward_count 88610  32.913%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.914361/  2.047129, val:  41.67%, val_best:  66.25%, tr:  89.68%, tr_best:  90.19%, epoch time: 44.84 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7888%\n",
      "layer   2  Sparsity: 87.0505%\n",
      "layer   3  Sparsity: 81.7596%\n",
      "total_backward_count 274120 real_backward_count 90009  32.836%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.917577/  2.002094, val:  50.42%, val_best:  66.25%, tr:  90.30%, tr_best:  90.30%, epoch time: 45.16 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7681%\n",
      "layer   2  Sparsity: 87.0381%\n",
      "layer   3  Sparsity: 81.6872%\n",
      "total_backward_count 279015 real_backward_count 91367  32.746%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.917572/  2.016236, val:  49.58%, val_best:  66.25%, tr:  91.93%, tr_best:  91.93%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.8082%\n",
      "layer   2  Sparsity: 87.1946%\n",
      "layer   3  Sparsity: 81.9699%\n",
      "total_backward_count 283910 real_backward_count 92704  32.653%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.926110/  2.030538, val:  59.58%, val_best:  66.25%, tr:  90.50%, tr_best:  91.93%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7827%\n",
      "layer   2  Sparsity: 87.1915%\n",
      "layer   3  Sparsity: 81.8022%\n",
      "total_backward_count 288805 real_backward_count 94060  32.569%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.925966/  2.029217, val:  50.42%, val_best:  66.25%, tr:  92.34%, tr_best:  92.34%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7941%\n",
      "layer   2  Sparsity: 87.2000%\n",
      "layer   3  Sparsity: 82.0406%\n",
      "total_backward_count 293700 real_backward_count 95393  32.480%\n",
      "fc layer 1 self.abs_max_out: 2903.0\n",
      "lif layer 1 self.abs_max_v: 2903.0\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.916577/  2.000622, val:  50.00%, val_best:  66.25%, tr:  90.60%, tr_best:  92.34%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7679%\n",
      "layer   2  Sparsity: 87.1585%\n",
      "layer   3  Sparsity: 82.1047%\n",
      "total_backward_count 298595 real_backward_count 96762  32.406%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.913525/  2.008004, val:  49.58%, val_best:  66.25%, tr:  91.01%, tr_best:  92.34%, epoch time: 44.15 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7482%\n",
      "layer   2  Sparsity: 87.2227%\n",
      "layer   3  Sparsity: 82.3438%\n",
      "total_backward_count 303490 real_backward_count 98061  32.311%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.911465/  2.008279, val:  56.25%, val_best:  66.25%, tr:  90.91%, tr_best:  92.34%, epoch time: 44.33 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7780%\n",
      "layer   2  Sparsity: 87.1713%\n",
      "layer   3  Sparsity: 82.3718%\n",
      "total_backward_count 308385 real_backward_count 99396  32.231%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.925702/  2.036528, val:  56.67%, val_best:  66.25%, tr:  91.01%, tr_best:  92.34%, epoch time: 44.30 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7922%\n",
      "layer   2  Sparsity: 87.2208%\n",
      "layer   3  Sparsity: 82.1970%\n",
      "total_backward_count 313280 real_backward_count 100648  32.127%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.936012/  2.028760, val:  53.75%, val_best:  66.25%, tr:  91.01%, tr_best:  92.34%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7850%\n",
      "layer   2  Sparsity: 87.0527%\n",
      "layer   3  Sparsity: 82.1670%\n",
      "total_backward_count 318175 real_backward_count 101960  32.045%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.907446/  2.019547, val:  55.00%, val_best:  66.25%, tr:  91.01%, tr_best:  92.34%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7990%\n",
      "layer   2  Sparsity: 87.0619%\n",
      "layer   3  Sparsity: 82.3221%\n",
      "total_backward_count 323070 real_backward_count 103250  31.959%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.911943/  1.994840, val:  62.08%, val_best:  66.25%, tr:  92.85%, tr_best:  92.85%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7880%\n",
      "layer   2  Sparsity: 87.1235%\n",
      "layer   3  Sparsity: 82.2651%\n",
      "total_backward_count 327965 real_backward_count 104564  31.883%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.915781/  2.017381, val:  62.92%, val_best:  66.25%, tr:  90.70%, tr_best:  92.85%, epoch time: 44.59 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7720%\n",
      "layer   2  Sparsity: 87.2073%\n",
      "layer   3  Sparsity: 82.1564%\n",
      "total_backward_count 332860 real_backward_count 105929  31.824%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.919450/  2.009529, val:  61.25%, val_best:  66.25%, tr:  91.32%, tr_best:  92.85%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7762%\n",
      "layer   2  Sparsity: 87.1371%\n",
      "layer   3  Sparsity: 81.8037%\n",
      "total_backward_count 337755 real_backward_count 107231  31.748%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.920035/  2.034716, val:  57.50%, val_best:  66.25%, tr:  89.58%, tr_best:  92.85%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7657%\n",
      "layer   2  Sparsity: 87.2098%\n",
      "layer   3  Sparsity: 81.8044%\n",
      "total_backward_count 342650 real_backward_count 108568  31.685%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.917520/  1.999652, val:  64.58%, val_best:  66.25%, tr:  93.26%, tr_best:  93.26%, epoch time: 44.39 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7661%\n",
      "layer   2  Sparsity: 87.1386%\n",
      "layer   3  Sparsity: 82.1443%\n",
      "total_backward_count 347545 real_backward_count 109880  31.616%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.913895/  2.018496, val:  48.33%, val_best:  66.25%, tr:  92.44%, tr_best:  93.26%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7657%\n",
      "layer   2  Sparsity: 86.9443%\n",
      "layer   3  Sparsity: 82.3091%\n",
      "total_backward_count 352440 real_backward_count 111148  31.537%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.925425/  2.004762, val:  58.75%, val_best:  66.25%, tr:  90.19%, tr_best:  93.26%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7340%\n",
      "layer   2  Sparsity: 86.8633%\n",
      "layer   3  Sparsity: 82.3203%\n",
      "total_backward_count 357335 real_backward_count 112446  31.468%\n",
      "fc layer 1 self.abs_max_out: 2996.0\n",
      "lif layer 1 self.abs_max_v: 2996.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.902677/  2.015487, val:  46.25%, val_best:  66.25%, tr:  92.65%, tr_best:  93.26%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7995%\n",
      "layer   2  Sparsity: 86.8655%\n",
      "layer   3  Sparsity: 81.8706%\n",
      "total_backward_count 362230 real_backward_count 113707  31.391%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.906837/  2.022725, val:  50.42%, val_best:  66.25%, tr:  92.13%, tr_best:  93.26%, epoch time: 44.97 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7412%\n",
      "layer   2  Sparsity: 86.7787%\n",
      "layer   3  Sparsity: 81.4053%\n",
      "total_backward_count 367125 real_backward_count 115025  31.331%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.907787/  1.994866, val:  61.25%, val_best:  66.25%, tr:  91.62%, tr_best:  93.26%, epoch time: 44.00 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 96.7893%\n",
      "layer   2  Sparsity: 86.9377%\n",
      "layer   3  Sparsity: 81.6055%\n",
      "total_backward_count 372020 real_backward_count 116303  31.263%\n",
      "fc layer 2 self.abs_max_out: 1566.0\n",
      "lif layer 2 self.abs_max_v: 1566.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.913363/  2.007077, val:  59.58%, val_best:  66.25%, tr:  91.01%, tr_best:  93.26%, epoch time: 45.03 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7735%\n",
      "layer   2  Sparsity: 86.9428%\n",
      "layer   3  Sparsity: 81.8111%\n",
      "total_backward_count 376915 real_backward_count 117614  31.204%\n",
      "fc layer 3 self.abs_max_out: 323.0\n",
      "fc layer 2 self.abs_max_out: 1572.0\n",
      "lif layer 2 self.abs_max_v: 1572.0\n",
      "fc layer 2 self.abs_max_out: 1589.0\n",
      "lif layer 2 self.abs_max_v: 1589.0\n",
      "fc layer 2 self.abs_max_out: 1712.0\n",
      "lif layer 2 self.abs_max_v: 1712.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.911973/  2.014518, val:  47.08%, val_best:  66.25%, tr:  91.62%, tr_best:  93.26%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7863%\n",
      "layer   2  Sparsity: 86.8852%\n",
      "layer   3  Sparsity: 81.4627%\n",
      "total_backward_count 381810 real_backward_count 118887  31.138%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.909699/  2.003403, val:  60.42%, val_best:  66.25%, tr:  91.22%, tr_best:  93.26%, epoch time: 45.72 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 96.7773%\n",
      "layer   2  Sparsity: 86.9772%\n",
      "layer   3  Sparsity: 81.6829%\n",
      "total_backward_count 386705 real_backward_count 120174  31.076%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.897920/  1.985445, val:  65.83%, val_best:  66.25%, tr:  91.52%, tr_best:  93.26%, epoch time: 44.42 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7832%\n",
      "layer   2  Sparsity: 86.7792%\n",
      "layer   3  Sparsity: 81.7399%\n",
      "total_backward_count 391600 real_backward_count 121421  31.006%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.891962/  1.996120, val:  63.33%, val_best:  66.25%, tr:  92.54%, tr_best:  93.26%, epoch time: 45.14 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7666%\n",
      "layer   2  Sparsity: 86.7482%\n",
      "layer   3  Sparsity: 81.5701%\n",
      "total_backward_count 396495 real_backward_count 122662  30.937%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.905690/  2.012383, val:  54.58%, val_best:  66.25%, tr:  90.81%, tr_best:  93.26%, epoch time: 44.19 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7914%\n",
      "layer   2  Sparsity: 86.8179%\n",
      "layer   3  Sparsity: 81.4191%\n",
      "total_backward_count 401390 real_backward_count 123978  30.887%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.909144/  2.018003, val:  52.08%, val_best:  66.25%, tr:  91.73%, tr_best:  93.26%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7799%\n",
      "layer   2  Sparsity: 86.8375%\n",
      "layer   3  Sparsity: 81.3326%\n",
      "total_backward_count 406285 real_backward_count 125258  30.830%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.891565/  1.988816, val:  60.00%, val_best:  66.25%, tr:  92.13%, tr_best:  93.26%, epoch time: 44.14 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7472%\n",
      "layer   2  Sparsity: 86.9008%\n",
      "layer   3  Sparsity: 81.3667%\n",
      "total_backward_count 411180 real_backward_count 126532  30.773%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.886575/  1.979342, val:  64.17%, val_best:  66.25%, tr:  90.81%, tr_best:  93.26%, epoch time: 44.29 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7654%\n",
      "layer   2  Sparsity: 86.8815%\n",
      "layer   3  Sparsity: 81.3854%\n",
      "total_backward_count 416075 real_backward_count 127825  30.722%\n",
      "fc layer 2 self.abs_max_out: 1749.0\n",
      "lif layer 2 self.abs_max_v: 1749.0\n",
      "fc layer 2 self.abs_max_out: 1778.0\n",
      "lif layer 2 self.abs_max_v: 1778.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.881232/  1.983854, val:  49.58%, val_best:  66.25%, tr:  92.03%, tr_best:  93.26%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7814%\n",
      "layer   2  Sparsity: 86.8176%\n",
      "layer   3  Sparsity: 81.2597%\n",
      "total_backward_count 420970 real_backward_count 129105  30.668%\n",
      "fc layer 1 self.abs_max_out: 3069.0\n",
      "lif layer 1 self.abs_max_v: 3069.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.876980/  1.987165, val:  63.75%, val_best:  66.25%, tr:  91.52%, tr_best:  93.26%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7868%\n",
      "layer   2  Sparsity: 86.7976%\n",
      "layer   3  Sparsity: 81.3870%\n",
      "total_backward_count 425865 real_backward_count 130377  30.615%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.909001/  1.991173, val:  65.00%, val_best:  66.25%, tr:  91.73%, tr_best:  93.26%, epoch time: 44.20 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7544%\n",
      "layer   2  Sparsity: 86.7909%\n",
      "layer   3  Sparsity: 81.4651%\n",
      "total_backward_count 430760 real_backward_count 131671  30.567%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.890197/  1.978888, val:  60.83%, val_best:  66.25%, tr:  91.93%, tr_best:  93.26%, epoch time: 44.98 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7734%\n",
      "layer   2  Sparsity: 86.7793%\n",
      "layer   3  Sparsity: 81.2053%\n",
      "total_backward_count 435655 real_backward_count 132965  30.521%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.891184/  1.979745, val:  54.17%, val_best:  66.25%, tr:  92.34%, tr_best:  93.26%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7978%\n",
      "layer   2  Sparsity: 86.8600%\n",
      "layer   3  Sparsity: 81.0413%\n",
      "total_backward_count 440550 real_backward_count 134228  30.468%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.867717/  1.987828, val:  54.58%, val_best:  66.25%, tr:  92.03%, tr_best:  93.26%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7846%\n",
      "layer   2  Sparsity: 86.6796%\n",
      "layer   3  Sparsity: 80.7057%\n",
      "total_backward_count 445445 real_backward_count 135472  30.413%\n",
      "fc layer 2 self.abs_max_out: 1815.0\n",
      "lif layer 2 self.abs_max_v: 1815.0\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.867557/  1.960747, val:  56.67%, val_best:  66.25%, tr:  92.65%, tr_best:  93.26%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7560%\n",
      "layer   2  Sparsity: 86.6050%\n",
      "layer   3  Sparsity: 81.2684%\n",
      "total_backward_count 450340 real_backward_count 136713  30.358%\n",
      "fc layer 3 self.abs_max_out: 355.0\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.883779/  1.970605, val:  58.33%, val_best:  66.25%, tr:  91.73%, tr_best:  93.26%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7796%\n",
      "layer   2  Sparsity: 86.4377%\n",
      "layer   3  Sparsity: 81.4909%\n",
      "total_backward_count 455235 real_backward_count 137968  30.307%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.871157/  1.992290, val:  62.92%, val_best:  66.25%, tr:  93.26%, tr_best:  93.26%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7713%\n",
      "layer   2  Sparsity: 86.5328%\n",
      "layer   3  Sparsity: 81.3991%\n",
      "total_backward_count 460130 real_backward_count 139197  30.252%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.887910/  1.963472, val:  62.08%, val_best:  66.25%, tr:  92.85%, tr_best:  93.26%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.8033%\n",
      "layer   2  Sparsity: 86.5537%\n",
      "layer   3  Sparsity: 81.2094%\n",
      "total_backward_count 465025 real_backward_count 140455  30.204%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.876431/  2.000253, val:  62.08%, val_best:  66.25%, tr:  93.77%, tr_best:  93.77%, epoch time: 44.78 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7714%\n",
      "layer   2  Sparsity: 86.4260%\n",
      "layer   3  Sparsity: 80.8768%\n",
      "total_backward_count 469920 real_backward_count 141688  30.152%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.882889/  1.966753, val:  60.83%, val_best:  66.25%, tr:  92.24%, tr_best:  93.77%, epoch time: 44.10 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7712%\n",
      "layer   2  Sparsity: 86.4287%\n",
      "layer   3  Sparsity: 81.2348%\n",
      "total_backward_count 474815 real_backward_count 142922  30.101%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.861886/  1.948464, val:  72.50%, val_best:  72.50%, tr:  92.54%, tr_best:  93.77%, epoch time: 45.17 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7921%\n",
      "layer   2  Sparsity: 86.5433%\n",
      "layer   3  Sparsity: 81.4781%\n",
      "total_backward_count 479710 real_backward_count 144150  30.049%\n",
      "fc layer 1 self.abs_max_out: 3179.0\n",
      "lif layer 1 self.abs_max_v: 3179.0\n",
      "fc layer 1 self.abs_max_out: 3339.0\n",
      "lif layer 1 self.abs_max_v: 3339.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.861277/  1.982104, val:  57.08%, val_best:  72.50%, tr:  92.03%, tr_best:  93.77%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.8029%\n",
      "layer   2  Sparsity: 86.5445%\n",
      "layer   3  Sparsity: 81.3797%\n",
      "total_backward_count 484605 real_backward_count 145374  29.998%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.875962/  1.961807, val:  58.75%, val_best:  72.50%, tr:  93.36%, tr_best:  93.77%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7958%\n",
      "layer   2  Sparsity: 86.5763%\n",
      "layer   3  Sparsity: 81.3376%\n",
      "total_backward_count 489500 real_backward_count 146581  29.945%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.862335/  1.967894, val:  65.42%, val_best:  72.50%, tr:  93.05%, tr_best:  93.77%, epoch time: 44.25 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7805%\n",
      "layer   2  Sparsity: 86.6226%\n",
      "layer   3  Sparsity: 81.2606%\n",
      "total_backward_count 494395 real_backward_count 147768  29.889%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.868261/  1.960851, val:  62.08%, val_best:  72.50%, tr:  94.08%, tr_best:  94.08%, epoch time: 45.11 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7647%\n",
      "layer   2  Sparsity: 86.5784%\n",
      "layer   3  Sparsity: 80.9370%\n",
      "total_backward_count 499290 real_backward_count 148959  29.834%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.861282/  1.961888, val:  51.67%, val_best:  72.50%, tr:  93.97%, tr_best:  94.08%, epoch time: 44.09 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 96.7608%\n",
      "layer   2  Sparsity: 86.5910%\n",
      "layer   3  Sparsity: 80.9276%\n",
      "total_backward_count 504185 real_backward_count 150189  29.788%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.868529/  1.971612, val:  61.25%, val_best:  72.50%, tr:  93.16%, tr_best:  94.08%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7484%\n",
      "layer   2  Sparsity: 86.4971%\n",
      "layer   3  Sparsity: 80.5456%\n",
      "total_backward_count 509080 real_backward_count 151409  29.742%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.870299/  1.995025, val:  62.50%, val_best:  72.50%, tr:  92.54%, tr_best:  94.08%, epoch time: 44.16 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7499%\n",
      "layer   2  Sparsity: 86.4871%\n",
      "layer   3  Sparsity: 80.4104%\n",
      "total_backward_count 513975 real_backward_count 152599  29.690%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.885521/  1.970711, val:  59.58%, val_best:  72.50%, tr:  92.34%, tr_best:  94.08%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7913%\n",
      "layer   2  Sparsity: 86.3656%\n",
      "layer   3  Sparsity: 80.5719%\n",
      "total_backward_count 518870 real_backward_count 153799  29.641%\n",
      "fc layer 2 self.abs_max_out: 1836.0\n",
      "lif layer 2 self.abs_max_v: 1836.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.871170/  1.959339, val:  57.50%, val_best:  72.50%, tr:  93.16%, tr_best:  94.08%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7854%\n",
      "layer   2  Sparsity: 86.4811%\n",
      "layer   3  Sparsity: 80.9285%\n",
      "total_backward_count 523765 real_backward_count 154993  29.592%\n",
      "fc layer 2 self.abs_max_out: 2005.0\n",
      "lif layer 2 self.abs_max_v: 2005.0\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.872982/  1.954648, val:  64.58%, val_best:  72.50%, tr:  91.42%, tr_best:  94.08%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7938%\n",
      "layer   2  Sparsity: 86.4244%\n",
      "layer   3  Sparsity: 80.9420%\n",
      "total_backward_count 528660 real_backward_count 156216  29.549%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.876768/  1.968381, val:  64.17%, val_best:  72.50%, tr:  94.08%, tr_best:  94.08%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7999%\n",
      "layer   2  Sparsity: 86.5199%\n",
      "layer   3  Sparsity: 81.2642%\n",
      "total_backward_count 533555 real_backward_count 157419  29.504%\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.873846/  1.963883, val:  57.50%, val_best:  72.50%, tr:  92.44%, tr_best:  94.08%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7741%\n",
      "layer   2  Sparsity: 86.4433%\n",
      "layer   3  Sparsity: 81.3953%\n",
      "total_backward_count 538450 real_backward_count 158596  29.454%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.868925/  1.971079, val:  54.58%, val_best:  72.50%, tr:  94.48%, tr_best:  94.48%, epoch time: 44.37 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7818%\n",
      "layer   2  Sparsity: 86.4759%\n",
      "layer   3  Sparsity: 81.3461%\n",
      "total_backward_count 543345 real_backward_count 159784  29.407%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.859349/  1.953455, val:  58.33%, val_best:  72.50%, tr:  94.28%, tr_best:  94.48%, epoch time: 44.59 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7649%\n",
      "layer   2  Sparsity: 86.2739%\n",
      "layer   3  Sparsity: 80.9363%\n",
      "total_backward_count 548240 real_backward_count 160983  29.364%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.854701/  1.966692, val:  67.08%, val_best:  72.50%, tr:  93.16%, tr_best:  94.48%, epoch time: 44.24 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7840%\n",
      "layer   2  Sparsity: 86.2958%\n",
      "layer   3  Sparsity: 80.8027%\n",
      "total_backward_count 553135 real_backward_count 162155  29.316%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.863361/  1.938671, val:  51.67%, val_best:  72.50%, tr:  93.46%, tr_best:  94.48%, epoch time: 44.94 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7624%\n",
      "layer   2  Sparsity: 86.3305%\n",
      "layer   3  Sparsity: 80.4490%\n",
      "total_backward_count 558030 real_backward_count 163324  29.268%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.856179/  1.943299, val:  58.75%, val_best:  72.50%, tr:  93.05%, tr_best:  94.48%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7617%\n",
      "layer   2  Sparsity: 86.5021%\n",
      "layer   3  Sparsity: 80.3842%\n",
      "total_backward_count 562925 real_backward_count 164490  29.221%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.863420/  1.974477, val:  73.33%, val_best:  73.33%, tr:  93.26%, tr_best:  94.48%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7815%\n",
      "layer   2  Sparsity: 86.4424%\n",
      "layer   3  Sparsity: 80.3485%\n",
      "total_backward_count 567820 real_backward_count 165677  29.178%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.871403/  1.972496, val:  58.33%, val_best:  73.33%, tr:  93.26%, tr_best:  94.48%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7802%\n",
      "layer   2  Sparsity: 86.3848%\n",
      "layer   3  Sparsity: 80.2931%\n",
      "total_backward_count 572715 real_backward_count 166868  29.136%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.871112/  1.961064, val:  60.42%, val_best:  73.33%, tr:  92.54%, tr_best:  94.48%, epoch time: 44.29 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7795%\n",
      "layer   2  Sparsity: 86.4684%\n",
      "layer   3  Sparsity: 80.4405%\n",
      "total_backward_count 577610 real_backward_count 168056  29.095%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.859344/  1.952703, val:  58.75%, val_best:  73.33%, tr:  93.67%, tr_best:  94.48%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.8014%\n",
      "layer   2  Sparsity: 86.3739%\n",
      "layer   3  Sparsity: 80.4961%\n",
      "total_backward_count 582505 real_backward_count 169266  29.058%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.844130/  1.950418, val:  60.42%, val_best:  73.33%, tr:  93.77%, tr_best:  94.48%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7781%\n",
      "layer   2  Sparsity: 86.5182%\n",
      "layer   3  Sparsity: 80.5847%\n",
      "total_backward_count 587400 real_backward_count 170405  29.010%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.865900/  1.972494, val:  65.42%, val_best:  73.33%, tr:  92.95%, tr_best:  94.48%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7806%\n",
      "layer   2  Sparsity: 86.4833%\n",
      "layer   3  Sparsity: 80.5295%\n",
      "total_backward_count 592295 real_backward_count 171600  28.972%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.870744/  1.984876, val:  64.17%, val_best:  73.33%, tr:  93.67%, tr_best:  94.48%, epoch time: 44.24 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7853%\n",
      "layer   2  Sparsity: 86.4332%\n",
      "layer   3  Sparsity: 80.6228%\n",
      "total_backward_count 597190 real_backward_count 172767  28.930%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.872913/  1.982081, val:  55.42%, val_best:  73.33%, tr:  94.08%, tr_best:  94.48%, epoch time: 44.90 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7508%\n",
      "layer   2  Sparsity: 86.3941%\n",
      "layer   3  Sparsity: 80.5313%\n",
      "total_backward_count 602085 real_backward_count 173961  28.893%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.871928/  1.943848, val:  70.42%, val_best:  73.33%, tr:  94.18%, tr_best:  94.48%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7818%\n",
      "layer   2  Sparsity: 86.5652%\n",
      "layer   3  Sparsity: 81.1005%\n",
      "total_backward_count 606980 real_backward_count 175165  28.858%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.858813/  1.939951, val:  67.92%, val_best:  73.33%, tr:  94.38%, tr_best:  94.48%, epoch time: 44.29 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7595%\n",
      "layer   2  Sparsity: 86.3069%\n",
      "layer   3  Sparsity: 80.7946%\n",
      "total_backward_count 611875 real_backward_count 176275  28.809%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.853558/  1.953226, val:  55.00%, val_best:  73.33%, tr:  93.77%, tr_best:  94.48%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7678%\n",
      "layer   2  Sparsity: 86.2752%\n",
      "layer   3  Sparsity: 80.8023%\n",
      "total_backward_count 616770 real_backward_count 177411  28.765%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.854763/  1.956915, val:  60.00%, val_best:  73.33%, tr:  94.08%, tr_best:  94.48%, epoch time: 44.23 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7488%\n",
      "layer   2  Sparsity: 86.1291%\n",
      "layer   3  Sparsity: 80.4993%\n",
      "total_backward_count 621665 real_backward_count 178564  28.724%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.855220/  1.976626, val:  56.25%, val_best:  73.33%, tr:  93.36%, tr_best:  94.48%, epoch time: 44.37 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7548%\n",
      "layer   2  Sparsity: 86.1970%\n",
      "layer   3  Sparsity: 80.5901%\n",
      "total_backward_count 626560 real_backward_count 179674  28.676%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.866485/  1.959957, val:  60.42%, val_best:  73.33%, tr:  93.97%, tr_best:  94.48%, epoch time: 43.94 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 96.7664%\n",
      "layer   2  Sparsity: 86.2461%\n",
      "layer   3  Sparsity: 80.2686%\n",
      "total_backward_count 631455 real_backward_count 180814  28.635%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.849304/  1.956901, val:  57.92%, val_best:  73.33%, tr:  93.46%, tr_best:  94.48%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7937%\n",
      "layer   2  Sparsity: 86.2952%\n",
      "layer   3  Sparsity: 80.2212%\n",
      "total_backward_count 636350 real_backward_count 182017  28.603%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.843525/  1.945521, val:  54.58%, val_best:  73.33%, tr:  93.87%, tr_best:  94.48%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7872%\n",
      "layer   2  Sparsity: 86.2561%\n",
      "layer   3  Sparsity: 80.1733%\n",
      "total_backward_count 641245 real_backward_count 183131  28.559%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.836970/  1.947030, val:  52.92%, val_best:  73.33%, tr:  94.28%, tr_best:  94.48%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7917%\n",
      "layer   2  Sparsity: 86.4401%\n",
      "layer   3  Sparsity: 80.1678%\n",
      "total_backward_count 646140 real_backward_count 184255  28.516%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.832680/  1.918373, val:  62.08%, val_best:  73.33%, tr:  93.97%, tr_best:  94.48%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7641%\n",
      "layer   2  Sparsity: 86.5343%\n",
      "layer   3  Sparsity: 80.0325%\n",
      "total_backward_count 651035 real_backward_count 185385  28.475%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.830961/  1.938143, val:  68.33%, val_best:  73.33%, tr:  93.97%, tr_best:  94.48%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7442%\n",
      "layer   2  Sparsity: 86.3727%\n",
      "layer   3  Sparsity: 79.5460%\n",
      "total_backward_count 655930 real_backward_count 186550  28.441%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.831711/  1.958005, val:  54.17%, val_best:  73.33%, tr:  93.67%, tr_best:  94.48%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7849%\n",
      "layer   2  Sparsity: 86.4960%\n",
      "layer   3  Sparsity: 79.2154%\n",
      "total_backward_count 660825 real_backward_count 187741  28.410%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.824338/  1.922945, val:  59.58%, val_best:  73.33%, tr:  94.18%, tr_best:  94.48%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7756%\n",
      "layer   2  Sparsity: 86.4394%\n",
      "layer   3  Sparsity: 79.1390%\n",
      "total_backward_count 665720 real_backward_count 188907  28.376%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.812602/  1.931376, val:  62.50%, val_best:  73.33%, tr:  94.59%, tr_best:  94.59%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7757%\n",
      "layer   2  Sparsity: 86.4365%\n",
      "layer   3  Sparsity: 79.4293%\n",
      "total_backward_count 670615 real_backward_count 190072  28.343%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.809913/  1.921719, val:  65.00%, val_best:  73.33%, tr:  93.97%, tr_best:  94.59%, epoch time: 44.89 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7564%\n",
      "layer   2  Sparsity: 86.4048%\n",
      "layer   3  Sparsity: 79.5149%\n",
      "total_backward_count 675510 real_backward_count 191196  28.304%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.835891/  1.913226, val:  62.50%, val_best:  73.33%, tr:  94.18%, tr_best:  94.59%, epoch time: 45.20 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7760%\n",
      "layer   2  Sparsity: 86.5155%\n",
      "layer   3  Sparsity: 80.0390%\n",
      "total_backward_count 680405 real_backward_count 192385  28.275%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.835318/  1.954242, val:  59.58%, val_best:  73.33%, tr:  93.36%, tr_best:  94.59%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7678%\n",
      "layer   2  Sparsity: 86.4080%\n",
      "layer   3  Sparsity: 79.8984%\n",
      "total_backward_count 685300 real_backward_count 193490  28.234%\n",
      "fc layer 1 self.abs_max_out: 3410.0\n",
      "lif layer 1 self.abs_max_v: 3410.0\n",
      "fc layer 2 self.abs_max_out: 2032.0\n",
      "lif layer 2 self.abs_max_v: 2032.0\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.836721/  1.959802, val:  61.25%, val_best:  73.33%, tr:  94.48%, tr_best:  94.59%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7689%\n",
      "layer   2  Sparsity: 86.2346%\n",
      "layer   3  Sparsity: 79.5710%\n",
      "total_backward_count 690195 real_backward_count 194599  28.195%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.832422/  1.964900, val:  60.83%, val_best:  73.33%, tr:  93.77%, tr_best:  94.59%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7773%\n",
      "layer   2  Sparsity: 86.1829%\n",
      "layer   3  Sparsity: 79.6626%\n",
      "total_backward_count 695090 real_backward_count 195767  28.164%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.837802/  1.916358, val:  67.08%, val_best:  73.33%, tr:  94.69%, tr_best:  94.69%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7868%\n",
      "layer   2  Sparsity: 86.2843%\n",
      "layer   3  Sparsity: 79.5579%\n",
      "total_backward_count 699985 real_backward_count 196927  28.133%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.833188/  1.923856, val:  63.75%, val_best:  73.33%, tr:  93.46%, tr_best:  94.69%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7678%\n",
      "layer   2  Sparsity: 86.3397%\n",
      "layer   3  Sparsity: 79.7376%\n",
      "total_backward_count 704880 real_backward_count 198013  28.092%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.840937/  1.924020, val:  73.33%, val_best:  73.33%, tr:  93.16%, tr_best:  94.69%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7749%\n",
      "layer   2  Sparsity: 86.4854%\n",
      "layer   3  Sparsity: 80.0657%\n",
      "total_backward_count 709775 real_backward_count 199165  28.060%\n",
      "fc layer 1 self.abs_max_out: 3412.0\n",
      "lif layer 1 self.abs_max_v: 3412.0\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.823099/  1.924148, val:  60.42%, val_best:  73.33%, tr:  94.18%, tr_best:  94.69%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7833%\n",
      "layer   2  Sparsity: 86.3833%\n",
      "layer   3  Sparsity: 79.7472%\n",
      "total_backward_count 714670 real_backward_count 200320  28.030%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.815576/  1.935792, val:  60.83%, val_best:  73.33%, tr:  92.54%, tr_best:  94.69%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.8144%\n",
      "layer   2  Sparsity: 86.4089%\n",
      "layer   3  Sparsity: 79.6744%\n",
      "total_backward_count 719565 real_backward_count 201501  28.003%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.796566/  1.921839, val:  50.83%, val_best:  73.33%, tr:  95.91%, tr_best:  95.91%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7834%\n",
      "layer   2  Sparsity: 86.4158%\n",
      "layer   3  Sparsity: 79.2621%\n",
      "total_backward_count 724460 real_backward_count 202601  27.966%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.803257/  1.918235, val:  64.58%, val_best:  73.33%, tr:  94.59%, tr_best:  95.91%, epoch time: 44.35 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7479%\n",
      "layer   2  Sparsity: 86.4317%\n",
      "layer   3  Sparsity: 79.3615%\n",
      "total_backward_count 729355 real_backward_count 203719  27.931%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.824019/  1.921324, val:  67.92%, val_best:  73.33%, tr:  93.16%, tr_best:  95.91%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7991%\n",
      "layer   2  Sparsity: 86.5926%\n",
      "layer   3  Sparsity: 79.8192%\n",
      "total_backward_count 734250 real_backward_count 204879  27.903%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.815738/  1.938706, val:  63.33%, val_best:  73.33%, tr:  95.10%, tr_best:  95.91%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7637%\n",
      "layer   2  Sparsity: 86.3587%\n",
      "layer   3  Sparsity: 79.6220%\n",
      "total_backward_count 739145 real_backward_count 205970  27.866%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.818788/  1.906940, val:  70.83%, val_best:  73.33%, tr:  94.08%, tr_best:  95.91%, epoch time: 44.35 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7874%\n",
      "layer   2  Sparsity: 86.3963%\n",
      "layer   3  Sparsity: 79.5974%\n",
      "total_backward_count 744040 real_backward_count 207139  27.840%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.818376/  1.918152, val:  75.42%, val_best:  75.42%, tr:  93.87%, tr_best:  95.91%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7882%\n",
      "layer   2  Sparsity: 86.4226%\n",
      "layer   3  Sparsity: 79.8172%\n",
      "total_backward_count 748935 real_backward_count 208294  27.812%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.835056/  1.936561, val:  66.67%, val_best:  75.42%, tr:  93.26%, tr_best:  95.91%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7907%\n",
      "layer   2  Sparsity: 86.3395%\n",
      "layer   3  Sparsity: 80.0011%\n",
      "total_backward_count 753830 real_backward_count 209445  27.784%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.836714/  1.924043, val:  75.42%, val_best:  75.42%, tr:  94.38%, tr_best:  95.91%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7762%\n",
      "layer   2  Sparsity: 86.4540%\n",
      "layer   3  Sparsity: 80.1486%\n",
      "total_backward_count 758725 real_backward_count 210590  27.756%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.806454/  1.922033, val:  59.58%, val_best:  75.42%, tr:  93.97%, tr_best:  95.91%, epoch time: 44.24 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7612%\n",
      "layer   2  Sparsity: 86.4550%\n",
      "layer   3  Sparsity: 79.9422%\n",
      "total_backward_count 763620 real_backward_count 211673  27.720%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.820142/  1.950539, val:  59.17%, val_best:  75.42%, tr:  93.46%, tr_best:  95.91%, epoch time: 45.36 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 96.7727%\n",
      "layer   2  Sparsity: 86.4290%\n",
      "layer   3  Sparsity: 79.9838%\n",
      "total_backward_count 768515 real_backward_count 212799  27.690%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.831737/  1.925827, val:  74.17%, val_best:  75.42%, tr:  93.46%, tr_best:  95.91%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7979%\n",
      "layer   2  Sparsity: 86.4152%\n",
      "layer   3  Sparsity: 79.9090%\n",
      "total_backward_count 773410 real_backward_count 213934  27.661%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.834055/  1.918185, val:  66.25%, val_best:  75.42%, tr:  93.77%, tr_best:  95.91%, epoch time: 45.31 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 96.7838%\n",
      "layer   2  Sparsity: 86.3495%\n",
      "layer   3  Sparsity: 79.8837%\n",
      "total_backward_count 778305 real_backward_count 215058  27.632%\n",
      "fc layer 1 self.abs_max_out: 3467.0\n",
      "lif layer 1 self.abs_max_v: 3467.0\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.827196/  1.918590, val:  67.92%, val_best:  75.42%, tr:  94.69%, tr_best:  95.91%, epoch time: 44.15 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7652%\n",
      "layer   2  Sparsity: 86.3431%\n",
      "layer   3  Sparsity: 79.8409%\n",
      "total_backward_count 783200 real_backward_count 216139  27.597%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.831277/  1.941077, val:  56.67%, val_best:  75.42%, tr:  94.18%, tr_best:  95.91%, epoch time: 44.71 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7740%\n",
      "layer   2  Sparsity: 86.3430%\n",
      "layer   3  Sparsity: 79.8505%\n",
      "total_backward_count 788095 real_backward_count 217250  27.566%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.813136/  1.932404, val:  57.50%, val_best:  75.42%, tr:  93.97%, tr_best:  95.91%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7706%\n",
      "layer   2  Sparsity: 86.2694%\n",
      "layer   3  Sparsity: 79.5272%\n",
      "total_backward_count 792990 real_backward_count 218350  27.535%\n",
      "fc layer 1 self.abs_max_out: 3501.0\n",
      "lif layer 1 self.abs_max_v: 3501.0\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.809028/  1.933773, val:  52.92%, val_best:  75.42%, tr:  93.56%, tr_best:  95.91%, epoch time: 44.79 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7738%\n",
      "layer   2  Sparsity: 86.3728%\n",
      "layer   3  Sparsity: 79.5917%\n",
      "total_backward_count 797885 real_backward_count 219440  27.503%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.812450/  1.939249, val:  62.92%, val_best:  75.42%, tr:  93.87%, tr_best:  95.91%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7630%\n",
      "layer   2  Sparsity: 86.3056%\n",
      "layer   3  Sparsity: 79.5657%\n",
      "total_backward_count 802780 real_backward_count 220540  27.472%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.824409/  1.932821, val:  63.33%, val_best:  75.42%, tr:  93.87%, tr_best:  95.91%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7772%\n",
      "layer   2  Sparsity: 86.3247%\n",
      "layer   3  Sparsity: 79.4277%\n",
      "total_backward_count 807675 real_backward_count 221667  27.445%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.833618/  1.947958, val:  65.42%, val_best:  75.42%, tr:  93.05%, tr_best:  95.91%, epoch time: 44.11 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7596%\n",
      "layer   2  Sparsity: 86.4304%\n",
      "layer   3  Sparsity: 79.6631%\n",
      "total_backward_count 812570 real_backward_count 222816  27.421%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.832017/  1.948694, val:  65.00%, val_best:  75.42%, tr:  94.59%, tr_best:  95.91%, epoch time: 44.26 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7996%\n",
      "layer   2  Sparsity: 86.4539%\n",
      "layer   3  Sparsity: 79.8625%\n",
      "total_backward_count 817465 real_backward_count 223897  27.389%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.822544/  1.926927, val:  55.83%, val_best:  75.42%, tr:  94.59%, tr_best:  95.91%, epoch time: 44.39 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7475%\n",
      "layer   2  Sparsity: 86.4390%\n",
      "layer   3  Sparsity: 79.4478%\n",
      "total_backward_count 822360 real_backward_count 225016  27.362%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.840214/  1.949754, val:  64.58%, val_best:  75.42%, tr:  93.05%, tr_best:  95.91%, epoch time: 44.54 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7866%\n",
      "layer   2  Sparsity: 86.4729%\n",
      "layer   3  Sparsity: 79.9109%\n",
      "total_backward_count 827255 real_backward_count 226138  27.336%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.838182/  1.942071, val:  59.17%, val_best:  75.42%, tr:  94.28%, tr_best:  95.91%, epoch time: 44.23 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7498%\n",
      "layer   2  Sparsity: 86.4665%\n",
      "layer   3  Sparsity: 79.9892%\n",
      "total_backward_count 832150 real_backward_count 227275  27.312%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.825626/  1.947406, val:  52.50%, val_best:  75.42%, tr:  94.48%, tr_best:  95.91%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7856%\n",
      "layer   2  Sparsity: 86.4389%\n",
      "layer   3  Sparsity: 79.8591%\n",
      "total_backward_count 837045 real_backward_count 228384  27.285%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.829804/  1.945439, val:  62.50%, val_best:  75.42%, tr:  94.18%, tr_best:  95.91%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7753%\n",
      "layer   2  Sparsity: 86.3139%\n",
      "layer   3  Sparsity: 79.7592%\n",
      "total_backward_count 841940 real_backward_count 229473  27.255%\n",
      "fc layer 1 self.abs_max_out: 3624.0\n",
      "lif layer 1 self.abs_max_v: 3624.0\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.824625/  1.939120, val:  70.42%, val_best:  75.42%, tr:  92.75%, tr_best:  95.91%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7739%\n",
      "layer   2  Sparsity: 86.4020%\n",
      "layer   3  Sparsity: 79.6023%\n",
      "total_backward_count 846835 real_backward_count 230622  27.233%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.837107/  1.916259, val:  71.67%, val_best:  75.42%, tr:  95.20%, tr_best:  95.91%, epoch time: 44.89 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7892%\n",
      "layer   2  Sparsity: 86.3893%\n",
      "layer   3  Sparsity: 79.8400%\n",
      "total_backward_count 851730 real_backward_count 231666  27.199%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.820801/  1.912569, val:  62.50%, val_best:  75.42%, tr:  94.48%, tr_best:  95.91%, epoch time: 44.70 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7547%\n",
      "layer   2  Sparsity: 86.3797%\n",
      "layer   3  Sparsity: 79.8816%\n",
      "total_backward_count 856625 real_backward_count 232779  27.174%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.811263/  1.921104, val:  56.67%, val_best:  75.42%, tr:  94.69%, tr_best:  95.91%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7615%\n",
      "layer   2  Sparsity: 86.3309%\n",
      "layer   3  Sparsity: 79.4060%\n",
      "total_backward_count 861520 real_backward_count 233870  27.146%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.796665/  1.890003, val:  65.83%, val_best:  75.42%, tr:  94.08%, tr_best:  95.91%, epoch time: 44.08 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 96.7906%\n",
      "layer   2  Sparsity: 86.2640%\n",
      "layer   3  Sparsity: 79.3247%\n",
      "total_backward_count 866415 real_backward_count 234935  27.116%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.811765/  1.934046, val:  66.25%, val_best:  75.42%, tr:  94.08%, tr_best:  95.91%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7512%\n",
      "layer   2  Sparsity: 86.3986%\n",
      "layer   3  Sparsity: 79.2882%\n",
      "total_backward_count 871310 real_backward_count 236020  27.088%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.812641/  1.928375, val:  56.67%, val_best:  75.42%, tr:  95.20%, tr_best:  95.91%, epoch time: 43.97 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 96.7695%\n",
      "layer   2  Sparsity: 86.3576%\n",
      "layer   3  Sparsity: 79.6945%\n",
      "total_backward_count 876205 real_backward_count 237054  27.055%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.806462/  1.938207, val:  59.58%, val_best:  75.42%, tr:  94.69%, tr_best:  95.91%, epoch time: 45.02 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7898%\n",
      "layer   2  Sparsity: 86.2389%\n",
      "layer   3  Sparsity: 79.6289%\n",
      "total_backward_count 881100 real_backward_count 238104  27.023%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.823010/  1.946895, val:  74.17%, val_best:  75.42%, tr:  95.30%, tr_best:  95.91%, epoch time: 44.37 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7629%\n",
      "layer   2  Sparsity: 86.0443%\n",
      "layer   3  Sparsity: 79.7107%\n",
      "total_backward_count 885995 real_backward_count 239200  26.998%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.825068/  1.927899, val:  65.83%, val_best:  75.42%, tr:  94.28%, tr_best:  95.91%, epoch time: 45.11 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7713%\n",
      "layer   2  Sparsity: 86.1762%\n",
      "layer   3  Sparsity: 79.8502%\n",
      "total_backward_count 890890 real_backward_count 240224  26.964%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.825615/  1.921165, val:  61.67%, val_best:  75.42%, tr:  94.99%, tr_best:  95.91%, epoch time: 43.97 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 96.7548%\n",
      "layer   2  Sparsity: 86.3223%\n",
      "layer   3  Sparsity: 79.6244%\n",
      "total_backward_count 895785 real_backward_count 241279  26.935%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.819271/  1.952406, val:  38.33%, val_best:  75.42%, tr:  94.79%, tr_best:  95.91%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7733%\n",
      "layer   2  Sparsity: 86.2879%\n",
      "layer   3  Sparsity: 79.2988%\n",
      "total_backward_count 900680 real_backward_count 242391  26.912%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.819607/  1.922815, val:  57.08%, val_best:  75.42%, tr:  94.69%, tr_best:  95.91%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7622%\n",
      "layer   2  Sparsity: 86.3463%\n",
      "layer   3  Sparsity: 79.1188%\n",
      "total_backward_count 905575 real_backward_count 243500  26.889%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.822988/  1.902700, val:  62.92%, val_best:  75.42%, tr:  94.59%, tr_best:  95.91%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7945%\n",
      "layer   2  Sparsity: 86.5139%\n",
      "layer   3  Sparsity: 79.0661%\n",
      "total_backward_count 910470 real_backward_count 244578  26.863%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.809023/  1.917972, val:  66.67%, val_best:  75.42%, tr:  94.59%, tr_best:  95.91%, epoch time: 44.13 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7615%\n",
      "layer   2  Sparsity: 86.4092%\n",
      "layer   3  Sparsity: 78.6787%\n",
      "total_backward_count 915365 real_backward_count 245683  26.840%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.814839/  1.922514, val:  70.83%, val_best:  75.42%, tr:  93.05%, tr_best:  95.91%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7710%\n",
      "layer   2  Sparsity: 86.3879%\n",
      "layer   3  Sparsity: 78.8583%\n",
      "total_backward_count 920260 real_backward_count 246816  26.820%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.818700/  1.928074, val:  71.67%, val_best:  75.42%, tr:  95.20%, tr_best:  95.91%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7583%\n",
      "layer   2  Sparsity: 86.2190%\n",
      "layer   3  Sparsity: 78.8870%\n",
      "total_backward_count 925155 real_backward_count 247892  26.795%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.800585/  1.907759, val:  67.08%, val_best:  75.42%, tr:  95.10%, tr_best:  95.91%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7585%\n",
      "layer   2  Sparsity: 86.2782%\n",
      "layer   3  Sparsity: 79.0847%\n",
      "total_backward_count 930050 real_backward_count 248934  26.766%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.800182/  1.900616, val:  70.83%, val_best:  75.42%, tr:  93.97%, tr_best:  95.91%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7839%\n",
      "layer   2  Sparsity: 86.2246%\n",
      "layer   3  Sparsity: 79.2624%\n",
      "total_backward_count 934945 real_backward_count 250042  26.744%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.821947/  1.930454, val:  58.33%, val_best:  75.42%, tr:  94.18%, tr_best:  95.91%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 96.7995%\n",
      "layer   2  Sparsity: 86.2687%\n",
      "layer   3  Sparsity: 79.2374%\n",
      "total_backward_count 939840 real_backward_count 251193  26.727%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.819626/  1.945204, val:  66.25%, val_best:  75.42%, tr:  95.20%, tr_best:  95.91%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7580%\n",
      "layer   2  Sparsity: 86.3313%\n",
      "layer   3  Sparsity: 79.4262%\n",
      "total_backward_count 944735 real_backward_count 252265  26.702%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.795725/  1.934182, val:  46.67%, val_best:  75.42%, tr:  96.02%, tr_best:  96.02%, epoch time: 44.19 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7456%\n",
      "layer   2  Sparsity: 86.2004%\n",
      "layer   3  Sparsity: 79.2785%\n",
      "total_backward_count 949630 real_backward_count 253278  26.671%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.790930/  1.890825, val:  66.67%, val_best:  75.42%, tr:  94.99%, tr_best:  96.02%, epoch time: 44.25 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7634%\n",
      "layer   2  Sparsity: 86.2946%\n",
      "layer   3  Sparsity: 79.4895%\n",
      "total_backward_count 954525 real_backward_count 254345  26.646%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.815911/  1.928795, val:  68.75%, val_best:  75.42%, tr:  95.20%, tr_best:  96.02%, epoch time: 44.21 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7928%\n",
      "layer   2  Sparsity: 86.3229%\n",
      "layer   3  Sparsity: 79.7317%\n",
      "total_backward_count 959420 real_backward_count 255429  26.623%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.816399/  1.912732, val:  74.58%, val_best:  75.42%, tr:  94.38%, tr_best:  96.02%, epoch time: 44.26 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7855%\n",
      "layer   2  Sparsity: 86.2567%\n",
      "layer   3  Sparsity: 79.6085%\n",
      "total_backward_count 964315 real_backward_count 256487  26.598%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.818611/  1.913147, val:  60.42%, val_best:  75.42%, tr:  94.69%, tr_best:  96.02%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7706%\n",
      "layer   2  Sparsity: 86.3829%\n",
      "layer   3  Sparsity: 79.7269%\n",
      "total_backward_count 969210 real_backward_count 257571  26.575%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.800571/  1.911743, val:  58.75%, val_best:  75.42%, tr:  94.18%, tr_best:  96.02%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7485%\n",
      "layer   2  Sparsity: 86.3804%\n",
      "layer   3  Sparsity: 79.7094%\n",
      "total_backward_count 974105 real_backward_count 258594  26.547%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.799004/  1.901567, val:  66.67%, val_best:  75.42%, tr:  94.59%, tr_best:  96.02%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 96.7792%\n",
      "layer   2  Sparsity: 86.2607%\n",
      "layer   3  Sparsity: 79.6264%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bb6d406c81420792cc9a0210525cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÑ‚ñá‚ñà‚ñÜ‚ñÖ‚ñà‚ñá‚ñÇ‚ñá‚ñÑ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÑ‚ñá‚ñà‚ñÜ‚ñÖ‚ñà‚ñá‚ñÇ‚ñá‚ñÑ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.94586</td></tr><tr><td>tr_epoch_loss</td><td>1.799</td></tr><tr><td>val_acc_best</td><td>0.75417</td></tr><tr><td>val_acc_now</td><td>0.66667</td></tr><tr><td>val_loss</td><td>1.90157</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">distinctive-sweep-105</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m5ovvwl7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m5ovvwl7</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_134134-m5ovvwl7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6v2y73r5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_161049-6v2y73r5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6v2y73r5' target=\"_blank\">earnest-sweep-108</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/celzwsdu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6v2y73r5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6v2y73r5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_161058_528', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 43471ed3c7f31ff42498182012c432a5\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 282\n",
      "fc layer 1 self.abs_max_out: 166.0\n",
      "lif layer 1 self.abs_max_v: 166.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 185.0\n",
      "fc layer 2 self.abs_max_out: 103.0\n",
      "lif layer 2 self.abs_max_v: 103.0\n",
      "lif layer 1 self.abs_max_v: 186.5\n",
      "fc layer 2 self.abs_max_out: 122.0\n",
      "lif layer 2 self.abs_max_v: 145.5\n",
      "fc layer 3 self.abs_max_out: 18.0\n",
      "lif layer 1 self.abs_max_v: 198.0\n",
      "lif layer 2 self.abs_max_v: 158.0\n",
      "smallest_now_T updated: 254\n",
      "fc layer 1 self.abs_max_out: 168.0\n",
      "lif layer 1 self.abs_max_v: 225.0\n",
      "fc layer 2 self.abs_max_out: 201.0\n",
      "lif layer 2 self.abs_max_v: 224.0\n",
      "fc layer 3 self.abs_max_out: 80.0\n",
      "fc layer 1 self.abs_max_out: 224.0\n",
      "lif layer 1 self.abs_max_v: 258.0\n",
      "fc layer 2 self.abs_max_out: 280.0\n",
      "lif layer 2 self.abs_max_v: 288.0\n",
      "fc layer 3 self.abs_max_out: 95.0\n",
      "fc layer 1 self.abs_max_out: 316.0\n",
      "lif layer 1 self.abs_max_v: 316.0\n",
      "lif layer 2 self.abs_max_v: 305.0\n",
      "fc layer 3 self.abs_max_out: 145.0\n",
      "lif layer 1 self.abs_max_v: 378.0\n",
      "fc layer 2 self.abs_max_out: 296.0\n",
      "lif layer 2 self.abs_max_v: 344.5\n",
      "fc layer 3 self.abs_max_out: 170.0\n",
      "smallest_now_T updated: 193\n",
      "fc layer 1 self.abs_max_out: 339.0\n",
      "lif layer 2 self.abs_max_v: 379.0\n",
      "fc layer 1 self.abs_max_out: 428.0\n",
      "lif layer 1 self.abs_max_v: 428.0\n",
      "fc layer 2 self.abs_max_out: 301.0\n",
      "lif layer 2 self.abs_max_v: 400.5\n",
      "lif layer 1 self.abs_max_v: 470.0\n",
      "fc layer 2 self.abs_max_out: 343.0\n",
      "lif layer 1 self.abs_max_v: 494.0\n",
      "lif layer 2 self.abs_max_v: 423.5\n",
      "lif layer 1 self.abs_max_v: 524.0\n",
      "fc layer 2 self.abs_max_out: 363.0\n",
      "lif layer 2 self.abs_max_v: 462.0\n",
      "fc layer 1 self.abs_max_out: 460.0\n",
      "lif layer 2 self.abs_max_v: 472.0\n",
      "fc layer 2 self.abs_max_out: 379.0\n",
      "lif layer 2 self.abs_max_v: 615.0\n",
      "smallest_now_T updated: 163\n",
      "fc layer 1 self.abs_max_out: 553.0\n",
      "lif layer 1 self.abs_max_v: 553.0\n",
      "fc layer 2 self.abs_max_out: 383.0\n",
      "fc layer 2 self.abs_max_out: 388.0\n",
      "fc layer 2 self.abs_max_out: 394.0\n",
      "smallest_now_T updated: 150\n",
      "fc layer 2 self.abs_max_out: 402.0\n",
      "fc layer 1 self.abs_max_out: 581.0\n",
      "lif layer 1 self.abs_max_v: 581.0\n",
      "lif layer 2 self.abs_max_v: 637.0\n",
      "fc layer 1 self.abs_max_out: 601.0\n",
      "lif layer 1 self.abs_max_v: 601.0\n",
      "fc layer 2 self.abs_max_out: 417.0\n",
      "fc layer 2 self.abs_max_out: 462.0\n",
      "lif layer 2 self.abs_max_v: 734.0\n",
      "fc layer 3 self.abs_max_out: 174.0\n",
      "fc layer 2 self.abs_max_out: 468.0\n",
      "fc layer 3 self.abs_max_out: 250.0\n",
      "fc layer 1 self.abs_max_out: 617.0\n",
      "lif layer 1 self.abs_max_v: 617.0\n",
      "fc layer 2 self.abs_max_out: 500.0\n",
      "smallest_now_T updated: 135\n",
      "fc layer 2 self.abs_max_out: 533.0\n",
      "fc layer 1 self.abs_max_out: 631.0\n",
      "lif layer 1 self.abs_max_v: 631.0\n",
      "fc layer 1 self.abs_max_out: 691.0\n",
      "lif layer 1 self.abs_max_v: 691.0\n",
      "lif layer 2 self.abs_max_v: 749.5\n",
      "fc layer 1 self.abs_max_out: 698.0\n",
      "lif layer 1 self.abs_max_v: 698.0\n",
      "lif layer 1 self.abs_max_v: 702.5\n",
      "fc layer 2 self.abs_max_out: 558.0\n",
      "fc layer 1 self.abs_max_out: 734.0\n",
      "lif layer 1 self.abs_max_v: 734.0\n",
      "fc layer 1 self.abs_max_out: 742.0\n",
      "lif layer 1 self.abs_max_v: 742.0\n",
      "fc layer 1 self.abs_max_out: 937.0\n",
      "lif layer 1 self.abs_max_v: 937.0\n",
      "fc layer 2 self.abs_max_out: 562.0\n",
      "smallest_now_T updated: 116\n",
      "smallest_now_T updated: 90\n",
      "fc layer 2 self.abs_max_out: 570.0\n",
      "lif layer 2 self.abs_max_v: 762.5\n",
      "fc layer 1 self.abs_max_out: 952.0\n",
      "lif layer 1 self.abs_max_v: 952.0\n",
      "fc layer 2 self.abs_max_out: 578.0\n",
      "fc layer 2 self.abs_max_out: 619.0\n",
      "fc layer 2 self.abs_max_out: 657.0\n",
      "fc layer 2 self.abs_max_out: 681.0\n",
      "fc layer 2 self.abs_max_out: 716.0\n",
      "lif layer 2 self.abs_max_v: 794.5\n",
      "fc layer 1 self.abs_max_out: 976.0\n",
      "lif layer 1 self.abs_max_v: 976.0\n",
      "fc layer 1 self.abs_max_out: 1036.0\n",
      "lif layer 1 self.abs_max_v: 1036.0\n",
      "fc layer 1 self.abs_max_out: 1067.0\n",
      "lif layer 1 self.abs_max_v: 1067.0\n",
      "lif layer 2 self.abs_max_v: 892.0\n",
      "lif layer 2 self.abs_max_v: 897.0\n",
      "fc layer 1 self.abs_max_out: 1113.0\n",
      "lif layer 1 self.abs_max_v: 1113.0\n",
      "fc layer 2 self.abs_max_out: 724.0\n",
      "lif layer 2 self.abs_max_v: 993.5\n",
      "fc layer 3 self.abs_max_out: 290.0\n",
      "fc layer 3 self.abs_max_out: 294.0\n",
      "fc layer 1 self.abs_max_out: 1135.0\n",
      "lif layer 1 self.abs_max_v: 1135.0\n",
      "fc layer 1 self.abs_max_out: 1145.0\n",
      "lif layer 1 self.abs_max_v: 1145.0\n",
      "fc layer 1 self.abs_max_out: 1151.0\n",
      "lif layer 1 self.abs_max_v: 1151.0\n",
      "fc layer 1 self.abs_max_out: 1279.0\n",
      "lif layer 1 self.abs_max_v: 1279.0\n",
      "smallest_now_T_val updated: 262\n",
      "smallest_now_T_val updated: 217\n",
      "smallest_now_T_val updated: 213\n",
      "smallest_now_T_val updated: 209\n",
      "smallest_now_T_val updated: 174\n",
      "smallest_now_T_val updated: 63\n",
      "fc layer 2 self.abs_max_out: 746.0\n",
      "fc layer 2 self.abs_max_out: 750.0\n",
      "lif layer 2 self.abs_max_v: 1053.0\n",
      "fc layer 2 self.abs_max_out: 760.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.028032/  2.064596, val:  28.33%, val_best:  28.33%, tr:  68.85%, tr_best:  68.85%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 94.2809%\n",
      "layer   2  Sparsity: 85.5115%\n",
      "layer   3  Sparsity: 83.4186%\n",
      "total_backward_count 4895 real_backward_count 2272  46.415%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 782.0\n",
      "fc layer 1 self.abs_max_out: 1377.0\n",
      "lif layer 1 self.abs_max_v: 1377.0\n",
      "fc layer 2 self.abs_max_out: 850.0\n",
      "fc layer 2 self.abs_max_out: 853.0\n",
      "fc layer 1 self.abs_max_out: 1405.0\n",
      "lif layer 1 self.abs_max_v: 1405.0\n",
      "fc layer 1 self.abs_max_out: 1443.0\n",
      "lif layer 1 self.abs_max_v: 1443.0\n",
      "fc layer 1 self.abs_max_out: 1475.0\n",
      "lif layer 1 self.abs_max_v: 1475.0\n",
      "fc layer 1 self.abs_max_out: 1518.0\n",
      "lif layer 1 self.abs_max_v: 1518.0\n",
      "fc layer 1 self.abs_max_out: 1535.0\n",
      "lif layer 1 self.abs_max_v: 1535.0\n",
      "lif layer 1 self.abs_max_v: 1601.0\n",
      "lif layer 1 self.abs_max_v: 1712.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.936964/  2.045742, val:  45.42%, val_best:  45.42%, tr:  84.07%, tr_best:  84.07%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2786%\n",
      "layer   2  Sparsity: 84.4328%\n",
      "layer   3  Sparsity: 81.0956%\n",
      "total_backward_count 9790 real_backward_count 3935  40.194%\n",
      "fc layer 3 self.abs_max_out: 320.0\n",
      "fc layer 2 self.abs_max_out: 906.0\n",
      "fc layer 2 self.abs_max_out: 922.0\n",
      "fc layer 2 self.abs_max_out: 928.0\n",
      "fc layer 1 self.abs_max_out: 1539.0\n",
      "fc layer 1 self.abs_max_out: 1635.0\n",
      "lif layer 1 self.abs_max_v: 1719.0\n",
      "lif layer 2 self.abs_max_v: 1126.5\n",
      "lif layer 1 self.abs_max_v: 1808.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.932167/  2.051486, val:  37.08%, val_best:  45.42%, tr:  87.64%, tr_best:  87.64%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 94.2669%\n",
      "layer   2  Sparsity: 84.0580%\n",
      "layer   3  Sparsity: 80.1862%\n",
      "total_backward_count 14685 real_backward_count 5383  36.656%\n",
      "fc layer 2 self.abs_max_out: 966.0\n",
      "fc layer 1 self.abs_max_out: 1759.0\n",
      "fc layer 1 self.abs_max_out: 1922.0\n",
      "lif layer 1 self.abs_max_v: 1922.0\n",
      "lif layer 2 self.abs_max_v: 1133.5\n",
      "lif layer 2 self.abs_max_v: 1264.0\n",
      "lif layer 2 self.abs_max_v: 1283.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.924435/  2.038355, val:  38.75%, val_best:  45.42%, tr:  89.07%, tr_best:  89.07%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2770%\n",
      "layer   2  Sparsity: 83.2824%\n",
      "layer   3  Sparsity: 78.9634%\n",
      "total_backward_count 19580 real_backward_count 6780  34.627%\n",
      "fc layer 2 self.abs_max_out: 970.0\n",
      "fc layer 2 self.abs_max_out: 973.0\n",
      "fc layer 2 self.abs_max_out: 984.0\n",
      "fc layer 2 self.abs_max_out: 1040.0\n",
      "lif layer 1 self.abs_max_v: 1983.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.888151/  2.024444, val:  42.50%, val_best:  45.42%, tr:  91.22%, tr_best:  91.22%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2697%\n",
      "layer   2  Sparsity: 82.8490%\n",
      "layer   3  Sparsity: 78.3927%\n",
      "total_backward_count 24475 real_backward_count 8094  33.070%\n",
      "fc layer 2 self.abs_max_out: 1066.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.886892/  2.018198, val:  43.33%, val_best:  45.42%, tr:  89.27%, tr_best:  91.22%, epoch time: 44.16 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2856%\n",
      "layer   2  Sparsity: 82.7926%\n",
      "layer   3  Sparsity: 78.1864%\n",
      "total_backward_count 29370 real_backward_count 9405  32.022%\n",
      "fc layer 3 self.abs_max_out: 325.0\n",
      "lif layer 1 self.abs_max_v: 2101.0\n",
      "fc layer 3 self.abs_max_out: 339.0\n",
      "lif layer 1 self.abs_max_v: 2179.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.869131/  2.004622, val:  51.25%, val_best:  51.25%, tr:  91.32%, tr_best:  91.32%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2497%\n",
      "layer   2  Sparsity: 82.2532%\n",
      "layer   3  Sparsity: 78.1235%\n",
      "total_backward_count 34265 real_backward_count 10657  31.102%\n",
      "lif layer 1 self.abs_max_v: 2184.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.849500/  1.998219, val:  50.83%, val_best:  51.25%, tr:  92.34%, tr_best:  92.34%, epoch time: 44.33 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2696%\n",
      "layer   2  Sparsity: 82.3476%\n",
      "layer   3  Sparsity: 78.5769%\n",
      "total_backward_count 39160 real_backward_count 11890  30.363%\n",
      "fc layer 1 self.abs_max_out: 1924.0\n",
      "fc layer 2 self.abs_max_out: 1070.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.843385/  1.976205, val:  59.58%, val_best:  59.58%, tr:  91.83%, tr_best:  92.34%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2702%\n",
      "layer   2  Sparsity: 82.3308%\n",
      "layer   3  Sparsity: 77.8766%\n",
      "total_backward_count 44055 real_backward_count 13131  29.806%\n",
      "fc layer 2 self.abs_max_out: 1077.0\n",
      "lif layer 2 self.abs_max_v: 1321.5\n",
      "fc layer 2 self.abs_max_out: 1138.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.828560/  1.967112, val:  52.50%, val_best:  59.58%, tr:  93.16%, tr_best:  93.16%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 94.2558%\n",
      "layer   2  Sparsity: 81.9212%\n",
      "layer   3  Sparsity: 77.3945%\n",
      "total_backward_count 48950 real_backward_count 14320  29.254%\n",
      "fc layer 1 self.abs_max_out: 1982.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.821818/  1.989330, val:  49.17%, val_best:  59.58%, tr:  93.05%, tr_best:  93.16%, epoch time: 44.09 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 94.2585%\n",
      "layer   2  Sparsity: 81.7316%\n",
      "layer   3  Sparsity: 77.9050%\n",
      "total_backward_count 53845 real_backward_count 15441  28.677%\n",
      "fc layer 1 self.abs_max_out: 2138.0\n",
      "lif layer 1 self.abs_max_v: 2271.0\n",
      "lif layer 2 self.abs_max_v: 1337.5\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.836384/  1.970876, val:  52.08%, val_best:  59.58%, tr:  93.56%, tr_best:  93.56%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2715%\n",
      "layer   2  Sparsity: 81.4601%\n",
      "layer   3  Sparsity: 78.7087%\n",
      "total_backward_count 58740 real_backward_count 16587  28.238%\n",
      "fc layer 3 self.abs_max_out: 343.0\n",
      "lif layer 2 self.abs_max_v: 1346.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.820576/  1.953986, val:  46.67%, val_best:  59.58%, tr:  94.38%, tr_best:  94.38%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 94.2760%\n",
      "layer   2  Sparsity: 81.5277%\n",
      "layer   3  Sparsity: 78.0325%\n",
      "total_backward_count 63635 real_backward_count 17707  27.826%\n",
      "fc layer 1 self.abs_max_out: 2559.0\n",
      "lif layer 1 self.abs_max_v: 2559.0\n",
      "fc layer 2 self.abs_max_out: 1163.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.806417/  1.947891, val:  48.33%, val_best:  59.58%, tr:  94.08%, tr_best:  94.38%, epoch time: 44.26 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2812%\n",
      "layer   2  Sparsity: 81.4244%\n",
      "layer   3  Sparsity: 78.4293%\n",
      "total_backward_count 68530 real_backward_count 18844  27.497%\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.803407/  1.929447, val:  41.67%, val_best:  59.58%, tr:  94.28%, tr_best:  94.38%, epoch time: 44.94 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 94.2906%\n",
      "layer   2  Sparsity: 81.0655%\n",
      "layer   3  Sparsity: 77.8906%\n",
      "total_backward_count 73425 real_backward_count 19971  27.199%\n",
      "fc layer 3 self.abs_max_out: 365.0\n",
      "lif layer 2 self.abs_max_v: 1374.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.784780/  1.921520, val:  54.58%, val_best:  59.58%, tr:  94.38%, tr_best:  94.38%, epoch time: 43.97 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 94.2768%\n",
      "layer   2  Sparsity: 80.7979%\n",
      "layer   3  Sparsity: 77.5517%\n",
      "total_backward_count 78320 real_backward_count 21026  26.846%\n",
      "fc layer 3 self.abs_max_out: 369.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.775089/  1.894115, val:  64.58%, val_best:  64.58%, tr:  95.51%, tr_best:  95.51%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 94.2632%\n",
      "layer   2  Sparsity: 81.0410%\n",
      "layer   3  Sparsity: 78.3605%\n",
      "total_backward_count 83215 real_backward_count 22054  26.502%\n",
      "fc layer 2 self.abs_max_out: 1208.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.787652/  1.922726, val:  65.42%, val_best:  65.42%, tr:  96.12%, tr_best:  96.12%, epoch time: 44.42 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2620%\n",
      "layer   2  Sparsity: 81.0084%\n",
      "layer   3  Sparsity: 78.4383%\n",
      "total_backward_count 88110 real_backward_count 23058  26.170%\n",
      "lif layer 2 self.abs_max_v: 1417.5\n",
      "lif layer 2 self.abs_max_v: 1450.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.787426/  1.903224, val:  59.17%, val_best:  65.42%, tr:  95.10%, tr_best:  96.12%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2557%\n",
      "layer   2  Sparsity: 80.6771%\n",
      "layer   3  Sparsity: 78.1401%\n",
      "total_backward_count 93005 real_backward_count 24077  25.888%\n",
      "lif layer 2 self.abs_max_v: 1455.5\n",
      "lif layer 2 self.abs_max_v: 1532.0\n",
      "fc layer 1 self.abs_max_out: 2703.0\n",
      "lif layer 1 self.abs_max_v: 2703.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.768212/  1.882423, val:  62.08%, val_best:  65.42%, tr:  95.20%, tr_best:  96.12%, epoch time: 44.18 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2601%\n",
      "layer   2  Sparsity: 80.5624%\n",
      "layer   3  Sparsity: 78.1011%\n",
      "total_backward_count 97900 real_backward_count 25051  25.588%\n",
      "fc layer 3 self.abs_max_out: 371.0\n",
      "fc layer 3 self.abs_max_out: 375.0\n",
      "lif layer 2 self.abs_max_v: 1599.5\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.752727/  1.916785, val:  57.50%, val_best:  65.42%, tr:  95.91%, tr_best:  96.12%, epoch time: 44.00 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 94.2685%\n",
      "layer   2  Sparsity: 80.7514%\n",
      "layer   3  Sparsity: 77.7792%\n",
      "total_backward_count 102795 real_backward_count 26025  25.317%\n",
      "fc layer 3 self.abs_max_out: 427.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.760584/  1.886491, val:  72.50%, val_best:  72.50%, tr:  96.02%, tr_best:  96.12%, epoch time: 44.18 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2663%\n",
      "layer   2  Sparsity: 80.5933%\n",
      "layer   3  Sparsity: 77.1943%\n",
      "total_backward_count 107690 real_backward_count 26981  25.054%\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.743344/  1.861118, val:  65.42%, val_best:  72.50%, tr:  96.42%, tr_best:  96.42%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2467%\n",
      "layer   2  Sparsity: 80.6434%\n",
      "layer   3  Sparsity: 76.7485%\n",
      "total_backward_count 112585 real_backward_count 27914  24.794%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.735081/  1.880638, val:  68.33%, val_best:  72.50%, tr:  95.61%, tr_best:  96.42%, epoch time: 44.12 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2756%\n",
      "layer   2  Sparsity: 80.2991%\n",
      "layer   3  Sparsity: 76.9074%\n",
      "total_backward_count 117480 real_backward_count 28859  24.565%\n",
      "lif layer 2 self.abs_max_v: 1625.5\n",
      "lif layer 2 self.abs_max_v: 1890.5\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.730245/  1.848823, val:  70.00%, val_best:  72.50%, tr:  96.94%, tr_best:  96.94%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2678%\n",
      "layer   2  Sparsity: 80.2116%\n",
      "layer   3  Sparsity: 76.7006%\n",
      "total_backward_count 122375 real_backward_count 29789  24.342%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.706510/  1.836900, val:  77.92%, val_best:  77.92%, tr:  95.91%, tr_best:  96.94%, epoch time: 43.93 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 94.2428%\n",
      "layer   2  Sparsity: 80.0336%\n",
      "layer   3  Sparsity: 76.0676%\n",
      "total_backward_count 127270 real_backward_count 30708  24.128%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.706466/  1.850792, val:  74.17%, val_best:  77.92%, tr:  97.24%, tr_best:  97.24%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 94.2617%\n",
      "layer   2  Sparsity: 80.0433%\n",
      "layer   3  Sparsity: 76.4277%\n",
      "total_backward_count 132165 real_backward_count 31567  23.885%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.716245/  1.863014, val:  71.25%, val_best:  77.92%, tr:  95.61%, tr_best:  97.24%, epoch time: 44.13 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2611%\n",
      "layer   2  Sparsity: 80.3951%\n",
      "layer   3  Sparsity: 76.2861%\n",
      "total_backward_count 137060 real_backward_count 32500  23.712%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.716388/  1.865031, val:  60.83%, val_best:  77.92%, tr:  95.91%, tr_best:  97.24%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2511%\n",
      "layer   2  Sparsity: 80.2612%\n",
      "layer   3  Sparsity: 76.6030%\n",
      "total_backward_count 141955 real_backward_count 33356  23.498%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.717279/  1.854046, val:  52.92%, val_best:  77.92%, tr:  96.73%, tr_best:  97.24%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2757%\n",
      "layer   2  Sparsity: 80.1116%\n",
      "layer   3  Sparsity: 76.4783%\n",
      "total_backward_count 146850 real_backward_count 34214  23.299%\n",
      "lif layer 1 self.abs_max_v: 2765.5\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.708257/  1.847233, val:  70.83%, val_best:  77.92%, tr:  97.85%, tr_best:  97.85%, epoch time: 45.18 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 94.2449%\n",
      "layer   2  Sparsity: 80.0765%\n",
      "layer   3  Sparsity: 76.3833%\n",
      "total_backward_count 151745 real_backward_count 35077  23.116%\n",
      "fc layer 2 self.abs_max_out: 1237.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.699627/  1.848125, val:  70.83%, val_best:  77.92%, tr:  96.73%, tr_best:  97.85%, epoch time: 44.16 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2738%\n",
      "layer   2  Sparsity: 79.5681%\n",
      "layer   3  Sparsity: 76.2000%\n",
      "total_backward_count 156640 real_backward_count 35930  22.938%\n",
      "fc layer 2 self.abs_max_out: 1337.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.701207/  1.850850, val:  72.50%, val_best:  77.92%, tr:  96.94%, tr_best:  97.85%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2982%\n",
      "layer   2  Sparsity: 79.6616%\n",
      "layer   3  Sparsity: 76.5142%\n",
      "total_backward_count 161535 real_backward_count 36776  22.767%\n",
      "lif layer 1 self.abs_max_v: 2875.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.700289/  1.822909, val:  68.33%, val_best:  77.92%, tr:  97.45%, tr_best:  97.85%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2731%\n",
      "layer   2  Sparsity: 79.9662%\n",
      "layer   3  Sparsity: 77.1080%\n",
      "total_backward_count 166430 real_backward_count 37613  22.600%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.698441/  1.835843, val:  66.25%, val_best:  77.92%, tr:  97.04%, tr_best:  97.85%, epoch time: 44.84 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 94.2676%\n",
      "layer   2  Sparsity: 79.8848%\n",
      "layer   3  Sparsity: 76.7980%\n",
      "total_backward_count 171325 real_backward_count 38419  22.425%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.695070/  1.834633, val:  75.83%, val_best:  77.92%, tr:  97.34%, tr_best:  97.85%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2532%\n",
      "layer   2  Sparsity: 79.7813%\n",
      "layer   3  Sparsity: 76.8678%\n",
      "total_backward_count 176220 real_backward_count 39253  22.275%\n",
      "fc layer 1 self.abs_max_out: 2860.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.675645/  1.816642, val:  75.42%, val_best:  77.92%, tr:  97.24%, tr_best:  97.85%, epoch time: 44.22 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2604%\n",
      "layer   2  Sparsity: 79.6806%\n",
      "layer   3  Sparsity: 76.0153%\n",
      "total_backward_count 181115 real_backward_count 40015  22.094%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.659867/  1.831995, val:  62.08%, val_best:  77.92%, tr:  98.06%, tr_best:  98.06%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2433%\n",
      "layer   2  Sparsity: 79.5966%\n",
      "layer   3  Sparsity: 76.1368%\n",
      "total_backward_count 186010 real_backward_count 40758  21.912%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.665299/  1.787918, val:  73.75%, val_best:  77.92%, tr:  98.06%, tr_best:  98.06%, epoch time: 44.35 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2532%\n",
      "layer   2  Sparsity: 79.5326%\n",
      "layer   3  Sparsity: 76.5769%\n",
      "total_backward_count 190905 real_backward_count 41518  21.748%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.659315/  1.795807, val:  72.08%, val_best:  77.92%, tr:  96.73%, tr_best:  98.06%, epoch time: 45.14 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 94.2840%\n",
      "layer   2  Sparsity: 79.6257%\n",
      "layer   3  Sparsity: 76.5940%\n",
      "total_backward_count 195800 real_backward_count 42313  21.610%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.656667/  1.805475, val:  67.50%, val_best:  77.92%, tr:  96.63%, tr_best:  98.06%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 94.2765%\n",
      "layer   2  Sparsity: 79.9559%\n",
      "layer   3  Sparsity: 76.5185%\n",
      "total_backward_count 200695 real_backward_count 43069  21.460%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.651071/  1.796959, val:  75.00%, val_best:  77.92%, tr:  97.85%, tr_best:  98.06%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2742%\n",
      "layer   2  Sparsity: 79.9003%\n",
      "layer   3  Sparsity: 75.9781%\n",
      "total_backward_count 205590 real_backward_count 43807  21.308%\n",
      "lif layer 1 self.abs_max_v: 2927.5\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.661083/  1.812815, val:  67.92%, val_best:  77.92%, tr:  97.24%, tr_best:  98.06%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2941%\n",
      "layer   2  Sparsity: 80.1346%\n",
      "layer   3  Sparsity: 76.4534%\n",
      "total_backward_count 210485 real_backward_count 44592  21.185%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.661890/  1.800673, val:  77.92%, val_best:  77.92%, tr:  98.26%, tr_best:  98.26%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2388%\n",
      "layer   2  Sparsity: 79.8535%\n",
      "layer   3  Sparsity: 76.3920%\n",
      "total_backward_count 215380 real_backward_count 45345  21.053%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.650946/  1.787582, val:  79.17%, val_best:  79.17%, tr:  97.96%, tr_best:  98.26%, epoch time: 44.64 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2633%\n",
      "layer   2  Sparsity: 79.7274%\n",
      "layer   3  Sparsity: 76.8055%\n",
      "total_backward_count 220275 real_backward_count 46107  20.932%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.659114/  1.787776, val:  73.75%, val_best:  79.17%, tr:  97.75%, tr_best:  98.26%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 94.2633%\n",
      "layer   2  Sparsity: 79.6287%\n",
      "layer   3  Sparsity: 76.9376%\n",
      "total_backward_count 225170 real_backward_count 46904  20.830%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.660284/  1.808014, val:  73.33%, val_best:  79.17%, tr:  97.75%, tr_best:  98.26%, epoch time: 44.33 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2833%\n",
      "layer   2  Sparsity: 79.8058%\n",
      "layer   3  Sparsity: 76.4175%\n",
      "total_backward_count 230065 real_backward_count 47640  20.707%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.635170/  1.810183, val:  67.08%, val_best:  79.17%, tr:  97.75%, tr_best:  98.26%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 94.2493%\n",
      "layer   2  Sparsity: 79.7518%\n",
      "layer   3  Sparsity: 76.6618%\n",
      "total_backward_count 234960 real_backward_count 48328  20.569%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.641712/  1.771073, val:  78.33%, val_best:  79.17%, tr:  97.85%, tr_best:  98.26%, epoch time: 44.33 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2513%\n",
      "layer   2  Sparsity: 79.5708%\n",
      "layer   3  Sparsity: 76.1251%\n",
      "total_backward_count 239855 real_backward_count 49090  20.467%\n",
      "fc layer 2 self.abs_max_out: 1372.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.634660/  1.758060, val:  81.25%, val_best:  81.25%, tr:  97.85%, tr_best:  98.26%, epoch time: 44.45 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2253%\n",
      "layer   2  Sparsity: 79.4478%\n",
      "layer   3  Sparsity: 76.0505%\n",
      "total_backward_count 244750 real_backward_count 49807  20.350%\n",
      "lif layer 1 self.abs_max_v: 2999.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.631787/  1.803857, val:  69.17%, val_best:  81.25%, tr:  97.65%, tr_best:  98.26%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2868%\n",
      "layer   2  Sparsity: 79.5322%\n",
      "layer   3  Sparsity: 76.2445%\n",
      "total_backward_count 249645 real_backward_count 50547  20.248%\n",
      "lif layer 1 self.abs_max_v: 3065.5\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.645070/  1.788883, val:  75.00%, val_best:  81.25%, tr:  98.57%, tr_best:  98.57%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2812%\n",
      "layer   2  Sparsity: 79.5129%\n",
      "layer   3  Sparsity: 76.2762%\n",
      "total_backward_count 254540 real_backward_count 51229  20.126%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.623810/  1.751227, val:  77.92%, val_best:  81.25%, tr:  98.06%, tr_best:  98.57%, epoch time: 44.23 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2921%\n",
      "layer   2  Sparsity: 79.6925%\n",
      "layer   3  Sparsity: 76.5098%\n",
      "total_backward_count 259435 real_backward_count 51905  20.007%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.624443/  1.772744, val:  79.58%, val_best:  81.25%, tr:  98.26%, tr_best:  98.57%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 94.2335%\n",
      "layer   2  Sparsity: 79.6554%\n",
      "layer   3  Sparsity: 76.7912%\n",
      "total_backward_count 264330 real_backward_count 52628  19.910%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.630464/  1.774613, val:  71.25%, val_best:  81.25%, tr:  97.75%, tr_best:  98.57%, epoch time: 43.97 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 94.2851%\n",
      "layer   2  Sparsity: 79.7385%\n",
      "layer   3  Sparsity: 76.6374%\n",
      "total_backward_count 269225 real_backward_count 53314  19.803%\n",
      "lif layer 1 self.abs_max_v: 3193.0\n",
      "lif layer 1 self.abs_max_v: 3230.5\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.635579/  1.771716, val:  77.92%, val_best:  81.25%, tr:  98.06%, tr_best:  98.57%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2630%\n",
      "layer   2  Sparsity: 79.8403%\n",
      "layer   3  Sparsity: 76.7179%\n",
      "total_backward_count 274120 real_backward_count 53996  19.698%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.620583/  1.768381, val:  78.75%, val_best:  81.25%, tr:  98.16%, tr_best:  98.57%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2901%\n",
      "layer   2  Sparsity: 79.6309%\n",
      "layer   3  Sparsity: 76.7354%\n",
      "total_backward_count 279015 real_backward_count 54698  19.604%\n",
      "lif layer 2 self.abs_max_v: 1932.5\n",
      "fc layer 3 self.abs_max_out: 428.0\n",
      "fc layer 3 self.abs_max_out: 432.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.622113/  1.765930, val:  72.08%, val_best:  81.25%, tr:  98.16%, tr_best:  98.57%, epoch time: 44.23 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2686%\n",
      "layer   2  Sparsity: 79.4936%\n",
      "layer   3  Sparsity: 76.5294%\n",
      "total_backward_count 283910 real_backward_count 55327  19.488%\n",
      "fc layer 3 self.abs_max_out: 444.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.606127/  1.743287, val:  75.00%, val_best:  81.25%, tr:  98.26%, tr_best:  98.57%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2426%\n",
      "layer   2  Sparsity: 79.4065%\n",
      "layer   3  Sparsity: 76.1180%\n",
      "total_backward_count 288805 real_backward_count 55964  19.378%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.602449/  1.769791, val:  63.75%, val_best:  81.25%, tr:  98.77%, tr_best:  98.77%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 94.2320%\n",
      "layer   2  Sparsity: 79.2326%\n",
      "layer   3  Sparsity: 76.1658%\n",
      "total_backward_count 293700 real_backward_count 56597  19.270%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.602330/  1.750416, val:  73.33%, val_best:  81.25%, tr:  98.88%, tr_best:  98.88%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2716%\n",
      "layer   2  Sparsity: 79.3657%\n",
      "layer   3  Sparsity: 76.0456%\n",
      "total_backward_count 298595 real_backward_count 57253  19.174%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.593618/  1.748576, val:  75.00%, val_best:  81.25%, tr:  99.08%, tr_best:  99.08%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2675%\n",
      "layer   2  Sparsity: 79.4167%\n",
      "layer   3  Sparsity: 76.3051%\n",
      "total_backward_count 303490 real_backward_count 57894  19.076%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.614066/  1.784344, val:  66.67%, val_best:  81.25%, tr:  97.96%, tr_best:  99.08%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2940%\n",
      "layer   2  Sparsity: 79.4604%\n",
      "layer   3  Sparsity: 76.0542%\n",
      "total_backward_count 308385 real_backward_count 58517  18.975%\n",
      "fc layer 3 self.abs_max_out: 455.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.603943/  1.757006, val:  77.50%, val_best:  81.25%, tr:  98.98%, tr_best:  99.08%, epoch time: 44.17 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 94.2627%\n",
      "layer   2  Sparsity: 79.3116%\n",
      "layer   3  Sparsity: 76.1285%\n",
      "total_backward_count 313280 real_backward_count 59123  18.872%\n",
      "fc layer 2 self.abs_max_out: 1387.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.612507/  1.752182, val:  80.00%, val_best:  81.25%, tr:  98.16%, tr_best:  99.08%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 94.2969%\n",
      "layer   2  Sparsity: 79.3356%\n",
      "layer   3  Sparsity: 76.5419%\n",
      "total_backward_count 318175 real_backward_count 59760  18.782%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'grid', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [5, 10, 15]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.25]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [4.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/512]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [5, 10, 15, 20, 25, 30]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [12_000, 25_000, 50_000, 75_000, 100_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"4\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'celzwsdu'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
