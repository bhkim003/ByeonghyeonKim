{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22288/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> 클래스 인덱스\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            # MSE를 계산\n",
    "            ctx.save_for_backward(input, target)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "            return torch.mean((input - target_one_hot) ** 2)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE 스타일의 gradient를 흉내냄\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "            # print('grad_output', grad_output) # 이거 걍 1.0임\n",
    "            return input_one_hot - target_one_hot, None  # target에는 gradient 없음\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    \n",
    "    print('current loss function:', criterion)\n",
    "    ####################################################\n",
    "    \n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb 과거 하이퍼파라미터 가져와서 붙여넣기 (devices unique_name은 니가 할당해라)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250512_222220-5j2f5lq4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5j2f5lq4' target=\"_blank\">playful-microwave-8395</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5j2f5lq4' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5j2f5lq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 17, 'which_data': 'NMNIST_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0.0, 'lif_layer_v_decay': 0.0, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000.0, 'lif_layer_sg_width': 4.0, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_main.pth', 'learning_rate': 0.01, 'epoch_num': 10000, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1.0, 'bias': True, 'last_lif': False, 'temporal_filter': 1, 'initial_pooling': 1, 'temporal_filter_accumulation': False} \n",
      "\n",
      "dataset_hash = 7b0583c2e220caca87b64bcaac63adf4\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 3750 BATCH: 16 train_data_count: 60000\n",
      "len(test_loader): 625 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): Shaker_for_FC()\n",
      "      (2): Sparsity_Checker()\n",
      "      (3): SYNAPSE_FC(in_features=578, out_features=200, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0.0, v_decay=0.0, v_threshold=0.5, v_reset=10000.0, sg_width=4.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.0, TIME=10, sstep=True, trace_on=False)\n",
      "      (5): Sparsity_Checker()\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (8): LIF_layer(v_init=0.0, v_decay=0.0, v_threshold=0.5, v_reset=10000.0, sg_width=4.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.0, TIME=10, sstep=True, trace_on=False)\n",
      "      (9): Sparsity_Checker()\n",
      "      (10): Feedback_Receiver()\n",
      "      (11): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 158,010\n",
      "========================================================\n",
      "\n",
      "current loss function: CrossEntropyLoss()\n",
      "self.perm fc input 처음에 한번 섞기 tensor([575, 370,  61, 417, 461,  13, 521, 431, 548, 388, 477, 324, 235,  75,\n",
      "          4, 257,  81, 220, 328, 313, 274, 382, 334, 558,  35, 518, 482,  79,\n",
      "        133,  21, 171, 510, 389, 112, 159, 246, 507, 150, 437,  85, 287,  20,\n",
      "        418, 529, 394, 142, 503, 368,  32, 443, 156, 376, 110,  58, 128, 564,\n",
      "          5, 190, 309, 113, 407, 260, 547, 264,  94, 367, 141, 327, 341, 188,\n",
      "        253, 255, 364, 291,  25, 230,  89, 530, 285, 557,  82, 381, 138,  30,\n",
      "        252, 233, 353, 129, 254, 148, 210, 192, 100,  98, 321,  49, 377, 472,\n",
      "        345,  84, 225, 224, 245, 532, 490, 419, 447, 320, 372, 244, 184, 556,\n",
      "        359, 476, 214,   9, 237, 401, 467, 475, 474, 272, 132, 378, 549, 319,\n",
      "        135,  39, 436,  83, 312, 217, 271, 115, 293, 294, 373, 512, 536, 303,\n",
      "        131, 329, 535,  95, 404, 124, 123, 152, 109, 365, 533, 164, 116, 470,\n",
      "        339, 420, 178, 433, 351, 179,  40, 400, 390, 374,  72, 519, 356, 360,\n",
      "        538, 333, 411, 495,  41,  31,  69, 180, 465, 523, 207, 487, 396, 120,\n",
      "        391, 157, 173, 403, 386, 452, 511, 101, 406, 137, 182,  54, 163, 570,\n",
      "        338, 122, 145, 554, 185, 460, 405,  18, 261, 162, 302, 308, 453,   0,\n",
      "        144, 170, 198, 295, 161,  53,  73, 416,  46, 429, 169, 238, 402, 296,\n",
      "        380, 542, 540, 277, 514, 323, 301,  86, 565, 304, 577,  14, 143, 183,\n",
      "         63, 107, 212, 481, 213,  67, 573, 500, 286, 165, 290, 488,  88, 263,\n",
      "        218,  55, 493, 130, 197, 517, 205, 454,  59,   7, 392, 236, 262, 223,\n",
      "        240, 397, 119, 196, 114, 219, 318, 527, 459, 444,  93, 395, 408, 498,\n",
      "        151, 340, 464, 506, 509,  78,  70, 572, 221,  16, 562,  50, 479, 438,\n",
      "        208, 516,  36,  57, 200, 355,  91,  77, 243, 149, 485, 496, 568, 242,\n",
      "        469, 344, 106, 247, 471, 410, 413,  87, 352,   3, 393, 421, 231,  60,\n",
      "        189, 449,  38, 428, 357, 571,  19, 204, 227, 450, 520, 480, 504, 102,\n",
      "        435, 307, 335, 256, 222, 160,   1, 300, 515,  37, 288,  71, 306, 187,\n",
      "        125, 127, 531, 325, 385, 158, 379, 134,  48, 502, 166, 362,  90, 346,\n",
      "        154, 281, 473, 282, 167,  96, 349,  51, 269,  92, 422, 528, 425,  12,\n",
      "         42, 215, 267,  45, 276, 489, 491, 525, 427, 140,  26, 463, 442,  74,\n",
      "         64, 193, 441, 383, 455,   8, 458, 492, 387, 248, 409, 350, 358, 155,\n",
      "        546, 366,  97, 551, 423, 239,  52, 228, 118,  15,  47, 560, 117, 424,\n",
      "        567, 194, 168, 203,  66, 147, 172, 146, 501,  10, 279, 111, 268, 559,\n",
      "        175, 305, 211, 466,  62,  29, 299, 251, 297, 315, 311, 229, 484, 298,\n",
      "        177, 258, 539, 412, 348, 289, 426, 513, 216, 226, 555, 524, 398, 432,\n",
      "        552, 576, 434, 314, 105, 440, 574, 266,  33, 363, 316, 202, 280,  80,\n",
      "        522, 181, 278, 273, 139, 136,   6, 292, 354, 526,  17, 445, 462, 326,\n",
      "        250,  44, 486, 553, 569, 369, 561, 259, 494,  43, 439, 265, 195, 176,\n",
      "         68, 201, 284,  24, 121, 347,  65,  99, 343, 283, 241, 566, 322, 317,\n",
      "        104, 545, 103, 457,  27, 478, 199, 505, 483,  34, 234, 499, 275, 126,\n",
      "        310,  22,  56, 361, 537, 430, 249, 543, 451, 191, 534, 174, 497, 544,\n",
      "        186, 206, 508, 337, 108,   2, 541, 384, 371, 330, 375, 456, 414, 550,\n",
      "        336, 331, 232, 209, 270,  76,  11, 342, 415, 446, 332, 448,  28, 399,\n",
      "        563, 153,  23, 468], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0100000'], tr/val_loss:  0.527400/  0.424790, val:  93.49%, val_best:  93.49%, tr:  91.06%, tr_best:  91.06%\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "[module.layers.10] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.316206/  0.266877, val:  96.62%, val_best:  96.62%, tr:  95.66%, tr_best:  95.66%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.276818/  0.240057, val:  97.29%, val_best:  97.29%, tr:  96.47%, tr_best:  96.47%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.252981/  0.298737, val:  96.25%, val_best:  97.29%, tr:  96.81%, tr_best:  96.81%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.235111/  0.245357, val:  97.15%, val_best:  97.29%, tr:  97.25%, tr_best:  97.25%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.220115/  0.248101, val:  97.13%, val_best:  97.29%, tr:  97.48%, tr_best:  97.48%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.209185/  0.240071, val:  97.22%, val_best:  97.29%, tr:  97.66%, tr_best:  97.66%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.203552/  0.234184, val:  97.44%, val_best:  97.44%, tr:  97.84%, tr_best:  97.84%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.193754/  0.228055, val:  97.39%, val_best:  97.44%, tr:  97.93%, tr_best:  97.93%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.184545/  0.222344, val:  97.79%, val_best:  97.79%, tr:  98.23%, tr_best:  98.23%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.177653/  0.236792, val:  97.48%, val_best:  97.79%, tr:  98.25%, tr_best:  98.25%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.174136/  0.237424, val:  97.50%, val_best:  97.79%, tr:  98.33%, tr_best:  98.33%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.170424/  0.262870, val:  97.13%, val_best:  97.79%, tr:  98.40%, tr_best:  98.40%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.164062/  0.240729, val:  97.37%, val_best:  97.79%, tr:  98.53%, tr_best:  98.53%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.159704/  0.231267, val:  97.63%, val_best:  97.79%, tr:  98.62%, tr_best:  98.62%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.154562/  0.221742, val:  97.66%, val_best:  97.79%, tr:  98.72%, tr_best:  98.72%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.152985/  0.266413, val:  97.19%, val_best:  97.79%, tr:  98.69%, tr_best:  98.72%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.147361/  0.256782, val:  97.32%, val_best:  97.79%, tr:  98.79%, tr_best:  98.79%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.145235/  0.226641, val:  97.80%, val_best:  97.80%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.142324/  0.230817, val:  97.70%, val_best:  97.80%, tr:  98.94%, tr_best:  98.94%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.138317/  0.239630, val:  97.67%, val_best:  97.80%, tr:  99.01%, tr_best:  99.01%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.136827/  0.283027, val:  97.00%, val_best:  97.80%, tr:  99.03%, tr_best:  99.03%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.133306/  0.251552, val:  97.50%, val_best:  97.80%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.129599/  0.244878, val:  97.58%, val_best:  97.80%, tr:  99.13%, tr_best:  99.13%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.129850/  0.247877, val:  97.56%, val_best:  97.80%, tr:  99.11%, tr_best:  99.13%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.127235/  0.263521, val:  97.40%, val_best:  97.80%, tr:  99.16%, tr_best:  99.16%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.123331/  0.238449, val:  97.86%, val_best:  97.86%, tr:  99.24%, tr_best:  99.24%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.121348/  0.249963, val:  97.85%, val_best:  97.86%, tr:  99.23%, tr_best:  99.24%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.119779/  0.255363, val:  97.79%, val_best:  97.86%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.118239/  0.250096, val:  98.07%, val_best:  98.07%, tr:  99.29%, tr_best:  99.29%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.118022/  0.238299, val:  97.99%, val_best:  98.07%, tr:  99.30%, tr_best:  99.30%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.116558/  0.238734, val:  97.89%, val_best:  98.07%, tr:  99.30%, tr_best:  99.30%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.113812/  0.257538, val:  97.77%, val_best:  98.07%, tr:  99.35%, tr_best:  99.35%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.110841/  0.267283, val:  97.63%, val_best:  98.07%, tr:  99.36%, tr_best:  99.36%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.111500/  0.264822, val:  97.59%, val_best:  98.07%, tr:  99.38%, tr_best:  99.38%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.110499/  0.242600, val:  98.05%, val_best:  98.07%, tr:  99.38%, tr_best:  99.38%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.108553/  0.264380, val:  97.64%, val_best:  98.07%, tr:  99.44%, tr_best:  99.44%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.108187/  0.239503, val:  98.05%, val_best:  98.07%, tr:  99.43%, tr_best:  99.44%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.104424/  0.256924, val:  97.80%, val_best:  98.07%, tr:  99.50%, tr_best:  99.50%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.102981/  0.255490, val:  97.95%, val_best:  98.07%, tr:  99.50%, tr_best:  99.50%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.101791/  0.273936, val:  97.73%, val_best:  98.07%, tr:  99.53%, tr_best:  99.53%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.101038/  0.256885, val:  98.11%, val_best:  98.11%, tr:  99.54%, tr_best:  99.54%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.099727/  0.255117, val:  98.09%, val_best:  98.11%, tr:  99.53%, tr_best:  99.54%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.098311/  0.261009, val:  97.89%, val_best:  98.11%, tr:  99.54%, tr_best:  99.54%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.098906/  0.253601, val:  98.08%, val_best:  98.11%, tr:  99.57%, tr_best:  99.57%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.096474/  0.267462, val:  97.86%, val_best:  98.11%, tr:  99.61%, tr_best:  99.61%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.094314/  0.262740, val:  98.08%, val_best:  98.11%, tr:  99.61%, tr_best:  99.61%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.094670/  0.264990, val:  98.06%, val_best:  98.11%, tr:  99.60%, tr_best:  99.61%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.092712/  0.257825, val:  98.11%, val_best:  98.11%, tr:  99.66%, tr_best:  99.66%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.091311/  0.260663, val:  98.18%, val_best:  98.18%, tr:  99.64%, tr_best:  99.66%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.092408/  0.261085, val:  97.97%, val_best:  98.18%, tr:  99.66%, tr_best:  99.66%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.090751/  0.271702, val:  98.00%, val_best:  98.18%, tr:  99.66%, tr_best:  99.66%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.089746/  0.271782, val:  97.90%, val_best:  98.18%, tr:  99.66%, tr_best:  99.66%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.088250/  0.271838, val:  98.14%, val_best:  98.18%, tr:  99.65%, tr_best:  99.66%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.087617/  0.288811, val:  97.89%, val_best:  98.18%, tr:  99.67%, tr_best:  99.67%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.086538/  0.281635, val:  97.91%, val_best:  98.18%, tr:  99.71%, tr_best:  99.71%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.086967/  0.296971, val:  97.64%, val_best:  98.18%, tr:  99.67%, tr_best:  99.71%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.085362/  0.270499, val:  98.15%, val_best:  98.18%, tr:  99.71%, tr_best:  99.71%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.084907/  0.282352, val:  98.12%, val_best:  98.18%, tr:  99.70%, tr_best:  99.71%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.083728/  0.281835, val:  98.02%, val_best:  98.18%, tr:  99.73%, tr_best:  99.73%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.082287/  0.292412, val:  97.96%, val_best:  98.18%, tr:  99.74%, tr_best:  99.74%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.083599/  0.280065, val:  97.98%, val_best:  98.18%, tr:  99.73%, tr_best:  99.74%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.081789/  0.308358, val:  97.84%, val_best:  98.18%, tr:  99.73%, tr_best:  99.74%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.080820/  0.289595, val:  98.10%, val_best:  98.18%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.080511/  0.288890, val:  97.93%, val_best:  98.18%, tr:  99.75%, tr_best:  99.77%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.079538/  0.285106, val:  97.95%, val_best:  98.18%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.079204/  0.294397, val:  98.00%, val_best:  98.18%, tr:  99.75%, tr_best:  99.77%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.077895/  0.295137, val:  97.86%, val_best:  98.18%, tr:  99.78%, tr_best:  99.78%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.077430/  0.292004, val:  97.84%, val_best:  98.18%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.076262/  0.312077, val:  97.91%, val_best:  98.18%, tr:  99.81%, tr_best:  99.81%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.076358/  0.292154, val:  98.02%, val_best:  98.18%, tr:  99.83%, tr_best:  99.83%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.075059/  0.308719, val:  97.99%, val_best:  98.18%, tr:  99.79%, tr_best:  99.83%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.075381/  0.304662, val:  98.01%, val_best:  98.18%, tr:  99.81%, tr_best:  99.83%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.073795/  0.298174, val:  98.10%, val_best:  98.18%, tr:  99.81%, tr_best:  99.83%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.072993/  0.294464, val:  98.02%, val_best:  98.18%, tr:  99.81%, tr_best:  99.83%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.074141/  0.297320, val:  98.11%, val_best:  98.18%, tr:  99.79%, tr_best:  99.83%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.072151/  0.300722, val:  98.08%, val_best:  98.18%, tr:  99.84%, tr_best:  99.84%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.071400/  0.301762, val:  98.18%, val_best:  98.18%, tr:  99.83%, tr_best:  99.84%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.071098/  0.311486, val:  98.03%, val_best:  98.18%, tr:  99.83%, tr_best:  99.84%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.070654/  0.336880, val:  97.75%, val_best:  98.18%, tr:  99.84%, tr_best:  99.84%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.070645/  0.293787, val:  98.17%, val_best:  98.18%, tr:  99.84%, tr_best:  99.84%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.070217/  0.298900, val:  98.11%, val_best:  98.18%, tr:  99.85%, tr_best:  99.85%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.069980/  0.302787, val:  98.02%, val_best:  98.18%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.069445/  0.300649, val:  98.16%, val_best:  98.18%, tr:  99.84%, tr_best:  99.86%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.068980/  0.304675, val:  98.06%, val_best:  98.18%, tr:  99.87%, tr_best:  99.87%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.068024/  0.303566, val:  98.14%, val_best:  98.18%, tr:  99.85%, tr_best:  99.87%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.067661/  0.305455, val:  98.01%, val_best:  98.18%, tr:  99.86%, tr_best:  99.87%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.067994/  0.334872, val:  97.77%, val_best:  98.18%, tr:  99.88%, tr_best:  99.88%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.066666/  0.315597, val:  98.08%, val_best:  98.18%, tr:  99.86%, tr_best:  99.88%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.067695/  0.326941, val:  97.90%, val_best:  98.18%, tr:  99.86%, tr_best:  99.88%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.065319/  0.318428, val:  98.05%, val_best:  98.18%, tr:  99.87%, tr_best:  99.88%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.066463/  0.325302, val:  97.97%, val_best:  98.18%, tr:  99.88%, tr_best:  99.88%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.064901/  0.313658, val:  98.11%, val_best:  98.18%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.065481/  0.311777, val:  98.08%, val_best:  98.18%, tr:  99.89%, tr_best:  99.91%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.064093/  0.322189, val:  98.12%, val_best:  98.18%, tr:  99.89%, tr_best:  99.91%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.064168/  0.324488, val:  97.99%, val_best:  98.18%, tr:  99.87%, tr_best:  99.91%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.064481/  0.317446, val:  98.03%, val_best:  98.18%, tr:  99.88%, tr_best:  99.91%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.062749/  0.322184, val:  98.17%, val_best:  98.18%, tr:  99.90%, tr_best:  99.91%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.062630/  0.320732, val:  98.13%, val_best:  98.18%, tr:  99.89%, tr_best:  99.91%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.063193/  0.338933, val:  97.93%, val_best:  98.18%, tr:  99.89%, tr_best:  99.91%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.061380/  0.337154, val:  97.91%, val_best:  98.18%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.060637/  0.334815, val:  97.95%, val_best:  98.18%, tr:  99.90%, tr_best:  99.91%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.060516/  0.321284, val:  98.17%, val_best:  98.18%, tr:  99.92%, tr_best:  99.92%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.060693/  0.349598, val:  97.68%, val_best:  98.18%, tr:  99.90%, tr_best:  99.92%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.061063/  0.338111, val:  98.06%, val_best:  98.18%, tr:  99.90%, tr_best:  99.92%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.060490/  0.333278, val:  98.09%, val_best:  98.18%, tr:  99.91%, tr_best:  99.92%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.059579/  0.339848, val:  98.13%, val_best:  98.18%, tr:  99.91%, tr_best:  99.92%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.058872/  0.341470, val:  97.91%, val_best:  98.18%, tr:  99.91%, tr_best:  99.92%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.059088/  0.337437, val:  98.04%, val_best:  98.18%, tr:  99.90%, tr_best:  99.92%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.057788/  0.332190, val:  98.14%, val_best:  98.18%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.057242/  0.334888, val:  98.12%, val_best:  98.18%, tr:  99.90%, tr_best:  99.93%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.057707/  0.348171, val:  97.96%, val_best:  98.18%, tr:  99.91%, tr_best:  99.93%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.056160/  0.334702, val:  98.10%, val_best:  98.18%, tr:  99.92%, tr_best:  99.93%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.056942/  0.339441, val:  98.01%, val_best:  98.18%, tr:  99.92%, tr_best:  99.93%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.058096/  0.343066, val:  98.04%, val_best:  98.18%, tr:  99.92%, tr_best:  99.93%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.055962/  0.337018, val:  98.09%, val_best:  98.18%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.056116/  0.345482, val:  98.09%, val_best:  98.18%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.055775/  0.357601, val:  97.86%, val_best:  98.18%, tr:  99.92%, tr_best:  99.95%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.056247/  0.349344, val:  97.94%, val_best:  98.18%, tr:  99.94%, tr_best:  99.95%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.055504/  0.350756, val:  98.06%, val_best:  98.18%, tr:  99.94%, tr_best:  99.95%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.056170/  0.357809, val:  97.92%, val_best:  98.18%, tr:  99.92%, tr_best:  99.95%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.055161/  0.361459, val:  97.89%, val_best:  98.18%, tr:  99.94%, tr_best:  99.95%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.055097/  0.345064, val:  98.08%, val_best:  98.18%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.054913/  0.363349, val:  97.88%, val_best:  98.18%, tr:  99.92%, tr_best:  99.95%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.054238/  0.349091, val:  97.97%, val_best:  98.18%, tr:  99.94%, tr_best:  99.95%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.053406/  0.355619, val:  98.05%, val_best:  98.18%, tr:  99.94%, tr_best:  99.95%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.054467/  0.351467, val:  97.97%, val_best:  98.18%, tr:  99.92%, tr_best:  99.95%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.053836/  0.366567, val:  97.90%, val_best:  98.18%, tr:  99.94%, tr_best:  99.95%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.052154/  0.358119, val:  98.02%, val_best:  98.18%, tr:  99.94%, tr_best:  99.95%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.052618/  0.349226, val:  98.11%, val_best:  98.18%, tr:  99.94%, tr_best:  99.95%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.053002/  0.369793, val:  97.88%, val_best:  98.18%, tr:  99.96%, tr_best:  99.96%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.052781/  0.355668, val:  98.03%, val_best:  98.18%, tr:  99.94%, tr_best:  99.96%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.051222/  0.363642, val:  97.93%, val_best:  98.18%, tr:  99.97%, tr_best:  99.97%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.051599/  0.369550, val:  97.82%, val_best:  98.18%, tr:  99.94%, tr_best:  99.97%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.051916/  0.361629, val:  98.05%, val_best:  98.18%, tr:  99.95%, tr_best:  99.97%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.050241/  0.359392, val:  98.15%, val_best:  98.18%, tr:  99.95%, tr_best:  99.97%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.050229/  0.364864, val:  98.05%, val_best:  98.18%, tr:  99.96%, tr_best:  99.97%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.050053/  0.362165, val:  98.07%, val_best:  98.18%, tr:  99.95%, tr_best:  99.97%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.049620/  0.374295, val:  97.86%, val_best:  98.18%, tr:  99.94%, tr_best:  99.97%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.049229/  0.366630, val:  98.02%, val_best:  98.18%, tr:  99.95%, tr_best:  99.97%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.050088/  0.370862, val:  97.89%, val_best:  98.18%, tr:  99.96%, tr_best:  99.97%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.048865/  0.388890, val:  97.86%, val_best:  98.18%, tr:  99.96%, tr_best:  99.97%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.049268/  0.361238, val:  98.09%, val_best:  98.18%, tr:  99.96%, tr_best:  99.97%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.049399/  0.369036, val:  97.99%, val_best:  98.18%, tr:  99.95%, tr_best:  99.97%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.048652/  0.376263, val:  97.97%, val_best:  98.18%, tr:  99.95%, tr_best:  99.97%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.048772/  0.362898, val:  98.05%, val_best:  98.18%, tr:  99.96%, tr_best:  99.97%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.048069/  0.369683, val:  97.92%, val_best:  98.18%, tr:  99.97%, tr_best:  99.97%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.047920/  0.372563, val:  98.03%, val_best:  98.18%, tr:  99.96%, tr_best:  99.97%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.047208/  0.377345, val:  97.96%, val_best:  98.18%, tr:  99.96%, tr_best:  99.97%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.047881/  0.370692, val:  98.06%, val_best:  98.18%, tr:  99.97%, tr_best:  99.97%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.047660/  0.382558, val:  97.98%, val_best:  98.18%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.047958/  0.367572, val:  98.13%, val_best:  98.18%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.047303/  0.369755, val:  98.10%, val_best:  98.18%, tr:  99.96%, tr_best:  99.98%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.047290/  0.383893, val:  97.95%, val_best:  98.18%, tr:  99.96%, tr_best:  99.98%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.046630/  0.377167, val:  98.06%, val_best:  98.18%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.046275/  0.383720, val:  97.88%, val_best:  98.18%, tr:  99.96%, tr_best:  99.98%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.046604/  0.372237, val:  98.11%, val_best:  98.18%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.046312/  0.378648, val:  98.01%, val_best:  98.18%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.045349/  0.384185, val:  98.00%, val_best:  98.18%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.045586/  0.397514, val:  97.96%, val_best:  98.18%, tr:  99.96%, tr_best:  99.98%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.045500/  0.385832, val:  98.00%, val_best:  98.18%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.045333/  0.388426, val:  98.01%, val_best:  98.18%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.045592/  0.381315, val:  98.07%, val_best:  98.18%, tr:  99.96%, tr_best:  99.98%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.045160/  0.390192, val:  98.07%, val_best:  98.18%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.044577/  0.385325, val:  98.07%, val_best:  98.18%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.044638/  0.386050, val:  97.98%, val_best:  98.18%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.045236/  0.385753, val:  98.07%, val_best:  98.18%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.044254/  0.386536, val:  98.03%, val_best:  98.18%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.043276/  0.383612, val:  98.06%, val_best:  98.18%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.044084/  0.389081, val:  97.99%, val_best:  98.18%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.044224/  0.406540, val:  97.86%, val_best:  98.18%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.043880/  0.392648, val:  98.02%, val_best:  98.18%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.044022/  0.398223, val:  97.90%, val_best:  98.18%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.043238/  0.409145, val:  97.96%, val_best:  98.18%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.042925/  0.393380, val:  98.00%, val_best:  98.18%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.043039/  0.392612, val:  98.04%, val_best:  98.18%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.043335/  0.391163, val:  98.05%, val_best:  98.18%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.042069/  0.387802, val:  98.02%, val_best:  98.18%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.041533/  0.403655, val:  98.01%, val_best:  98.18%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.042879/  0.404958, val:  98.03%, val_best:  98.18%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.041931/  0.398864, val:  98.07%, val_best:  98.18%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.042564/  0.387252, val:  98.12%, val_best:  98.18%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.041722/  0.401952, val:  98.12%, val_best:  98.18%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.041212/  0.396847, val:  98.16%, val_best:  98.18%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.041721/  0.399747, val:  98.08%, val_best:  98.18%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.040597/  0.401038, val:  98.08%, val_best:  98.18%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.040907/  0.401655, val:  98.09%, val_best:  98.18%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.041513/  0.403127, val:  98.08%, val_best:  98.18%, tr:  99.97%, tr_best:  99.99%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.039789/  0.402546, val:  98.05%, val_best:  98.18%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.041400/  0.400924, val:  98.15%, val_best:  98.18%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.040647/  0.398618, val:  98.06%, val_best:  98.18%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.039610/  0.398897, val:  98.04%, val_best:  98.18%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.039720/  0.403318, val:  98.05%, val_best:  98.18%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.040286/  0.413498, val:  97.92%, val_best:  98.18%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.039569/  0.407220, val:  98.16%, val_best:  98.18%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.039577/  0.404217, val:  98.15%, val_best:  98.18%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.039872/  0.403042, val:  98.17%, val_best:  98.18%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.038856/  0.407732, val:  98.14%, val_best:  98.18%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.038857/  0.423656, val:  97.96%, val_best:  98.18%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.038634/  0.414689, val:  98.06%, val_best:  98.18%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-200 lr=['0.0100000'], tr/val_loss:  0.038142/  0.418144, val:  97.99%, val_best:  98.18%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-201 lr=['0.0100000'], tr/val_loss:  0.038700/  0.414159, val:  98.00%, val_best:  98.18%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-202 lr=['0.0100000'], tr/val_loss:  0.038884/  0.409472, val:  98.00%, val_best:  98.18%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-203 lr=['0.0100000'], tr/val_loss:  0.038027/  0.410079, val:  98.08%, val_best:  98.18%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-204 lr=['0.0100000'], tr/val_loss:  0.037486/  0.414978, val:  97.93%, val_best:  98.18%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-205 lr=['0.0100000'], tr/val_loss:  0.037825/  0.412927, val:  98.06%, val_best:  98.18%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-206 lr=['0.0100000'], tr/val_loss:  0.038184/  0.410961, val:  98.18%, val_best:  98.18%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-207 lr=['0.0100000'], tr/val_loss:  0.037624/  0.409055, val:  98.15%, val_best:  98.18%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-208 lr=['0.0100000'], tr/val_loss:  0.037259/  0.420598, val:  98.01%, val_best:  98.18%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-209 lr=['0.0100000'], tr/val_loss:  0.038144/  0.419360, val:  98.11%, val_best:  98.18%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-210 lr=['0.0100000'], tr/val_loss:  0.037814/  0.412751, val:  98.22%, val_best:  98.22%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-211 lr=['0.0100000'], tr/val_loss:  0.037397/  0.415409, val:  98.08%, val_best:  98.22%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-212 lr=['0.0100000'], tr/val_loss:  0.037401/  0.415671, val:  97.99%, val_best:  98.22%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-213 lr=['0.0100000'], tr/val_loss:  0.036732/  0.417907, val:  98.03%, val_best:  98.22%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-214 lr=['0.0100000'], tr/val_loss:  0.036315/  0.423609, val:  98.02%, val_best:  98.22%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-215 lr=['0.0100000'], tr/val_loss:  0.036481/  0.412972, val:  98.09%, val_best:  98.22%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-216 lr=['0.0100000'], tr/val_loss:  0.036043/  0.421122, val:  98.00%, val_best:  98.22%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-217 lr=['0.0100000'], tr/val_loss:  0.036214/  0.417051, val:  98.22%, val_best:  98.22%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-218 lr=['0.0100000'], tr/val_loss:  0.035715/  0.426297, val:  98.13%, val_best:  98.22%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-219 lr=['0.0100000'], tr/val_loss:  0.036138/  0.420349, val:  98.17%, val_best:  98.22%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-220 lr=['0.0100000'], tr/val_loss:  0.036322/  0.421839, val:  98.15%, val_best:  98.22%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-221 lr=['0.0100000'], tr/val_loss:  0.035102/  0.426365, val:  98.15%, val_best:  98.22%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-222 lr=['0.0100000'], tr/val_loss:  0.035953/  0.428046, val:  98.04%, val_best:  98.22%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-223 lr=['0.0100000'], tr/val_loss:  0.035363/  0.431777, val:  98.11%, val_best:  98.22%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-224 lr=['0.0100000'], tr/val_loss:  0.035164/  0.427159, val:  98.23%, val_best:  98.23%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-225 lr=['0.0100000'], tr/val_loss:  0.035016/  0.433028, val:  98.09%, val_best:  98.23%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-226 lr=['0.0100000'], tr/val_loss:  0.034564/  0.427407, val:  98.10%, val_best:  98.23%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-227 lr=['0.0100000'], tr/val_loss:  0.034674/  0.424051, val:  98.08%, val_best:  98.23%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-228 lr=['0.0100000'], tr/val_loss:  0.035439/  0.425408, val:  98.11%, val_best:  98.23%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-229 lr=['0.0100000'], tr/val_loss:  0.034764/  0.429199, val:  98.05%, val_best:  98.23%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-230 lr=['0.0100000'], tr/val_loss:  0.034369/  0.427250, val:  98.06%, val_best:  98.23%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-231 lr=['0.0100000'], tr/val_loss:  0.034839/  0.443835, val:  98.11%, val_best:  98.23%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-232 lr=['0.0100000'], tr/val_loss:  0.034607/  0.432610, val:  98.09%, val_best:  98.23%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-233 lr=['0.0100000'], tr/val_loss:  0.034870/  0.441811, val:  98.10%, val_best:  98.23%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-234 lr=['0.0100000'], tr/val_loss:  0.034227/  0.433359, val:  98.17%, val_best:  98.23%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-235 lr=['0.0100000'], tr/val_loss:  0.034195/  0.430874, val:  98.16%, val_best:  98.23%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-236 lr=['0.0100000'], tr/val_loss:  0.035084/  0.434981, val:  97.98%, val_best:  98.23%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-237 lr=['0.0100000'], tr/val_loss:  0.033881/  0.443326, val:  98.07%, val_best:  98.23%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-238 lr=['0.0100000'], tr/val_loss:  0.033999/  0.426445, val:  98.20%, val_best:  98.23%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-239 lr=['0.0100000'], tr/val_loss:  0.034398/  0.439407, val:  98.03%, val_best:  98.23%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-240 lr=['0.0100000'], tr/val_loss:  0.033826/  0.437315, val:  98.18%, val_best:  98.23%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-241 lr=['0.0100000'], tr/val_loss:  0.033630/  0.428730, val:  98.15%, val_best:  98.23%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-242 lr=['0.0100000'], tr/val_loss:  0.033970/  0.439783, val:  98.13%, val_best:  98.23%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-243 lr=['0.0100000'], tr/val_loss:  0.034407/  0.437569, val:  98.07%, val_best:  98.23%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-244 lr=['0.0100000'], tr/val_loss:  0.034324/  0.447266, val:  98.05%, val_best:  98.23%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-245 lr=['0.0100000'], tr/val_loss:  0.033310/  0.441609, val:  98.11%, val_best:  98.23%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-246 lr=['0.0100000'], tr/val_loss:  0.033555/  0.446549, val:  98.04%, val_best:  98.23%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-247 lr=['0.0100000'], tr/val_loss:  0.033273/  0.436408, val:  98.24%, val_best:  98.24%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-248 lr=['0.0100000'], tr/val_loss:  0.032968/  0.436837, val:  98.13%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-249 lr=['0.0100000'], tr/val_loss:  0.033743/  0.447824, val:  98.05%, val_best:  98.24%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-250 lr=['0.0100000'], tr/val_loss:  0.033024/  0.444049, val:  98.04%, val_best:  98.24%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-251 lr=['0.0100000'], tr/val_loss:  0.032900/  0.438622, val:  98.05%, val_best:  98.24%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-252 lr=['0.0100000'], tr/val_loss:  0.032693/  0.435298, val:  98.06%, val_best:  98.24%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-253 lr=['0.0100000'], tr/val_loss:  0.032392/  0.437942, val:  98.16%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-254 lr=['0.0100000'], tr/val_loss:  0.033010/  0.447161, val:  98.04%, val_best:  98.24%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-255 lr=['0.0100000'], tr/val_loss:  0.032217/  0.435912, val:  98.22%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-256 lr=['0.0100000'], tr/val_loss:  0.032445/  0.445433, val:  98.06%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-257 lr=['0.0100000'], tr/val_loss:  0.031791/  0.444261, val:  98.13%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-258 lr=['0.0100000'], tr/val_loss:  0.031835/  0.450110, val:  98.10%, val_best:  98.24%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-259 lr=['0.0100000'], tr/val_loss:  0.031897/  0.457607, val:  98.11%, val_best:  98.24%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-260 lr=['0.0100000'], tr/val_loss:  0.032001/  0.440265, val:  98.21%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-261 lr=['0.0100000'], tr/val_loss:  0.031866/  0.446947, val:  98.23%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-262 lr=['0.0100000'], tr/val_loss:  0.032330/  0.445308, val:  98.14%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-263 lr=['0.0100000'], tr/val_loss:  0.031883/  0.450387, val:  98.12%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-264 lr=['0.0100000'], tr/val_loss:  0.031965/  0.449420, val:  98.14%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-265 lr=['0.0100000'], tr/val_loss:  0.031464/  0.449578, val:  98.02%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-266 lr=['0.0100000'], tr/val_loss:  0.031122/  0.451857, val:  98.13%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-267 lr=['0.0100000'], tr/val_loss:  0.030736/  0.449591, val:  98.18%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-268 lr=['0.0100000'], tr/val_loss:  0.031595/  0.454524, val:  98.01%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-269 lr=['0.0100000'], tr/val_loss:  0.030492/  0.455128, val:  98.00%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-270 lr=['0.0100000'], tr/val_loss:  0.031439/  0.463728, val:  98.04%, val_best:  98.24%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-271 lr=['0.0100000'], tr/val_loss:  0.030838/  0.460300, val:  98.12%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-272 lr=['0.0100000'], tr/val_loss:  0.031809/  0.453492, val:  98.13%, val_best:  98.24%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-273 lr=['0.0100000'], tr/val_loss:  0.031812/  0.450569, val:  98.17%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-274 lr=['0.0100000'], tr/val_loss:  0.031572/  0.450848, val:  98.16%, val_best:  98.24%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-275 lr=['0.0100000'], tr/val_loss:  0.031530/  0.455489, val:  98.15%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-276 lr=['0.0100000'], tr/val_loss:  0.030839/  0.456805, val:  98.17%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-277 lr=['0.0100000'], tr/val_loss:  0.030724/  0.454254, val:  98.14%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-278 lr=['0.0100000'], tr/val_loss:  0.030446/  0.455315, val:  98.06%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-279 lr=['0.0100000'], tr/val_loss:  0.030524/  0.450465, val:  98.07%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-280 lr=['0.0100000'], tr/val_loss:  0.030902/  0.459633, val:  97.95%, val_best:  98.24%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-281 lr=['0.0100000'], tr/val_loss:  0.030239/  0.458490, val:  97.99%, val_best:  98.24%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-282 lr=['0.0100000'], tr/val_loss:  0.031719/  0.457361, val:  98.08%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-283 lr=['0.0100000'], tr/val_loss:  0.030287/  0.460489, val:  98.11%, val_best:  98.24%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-284 lr=['0.0100000'], tr/val_loss:  0.029430/  0.453683, val:  98.15%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-285 lr=['0.0100000'], tr/val_loss:  0.029746/  0.461274, val:  98.06%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-286 lr=['0.0100000'], tr/val_loss:  0.030372/  0.458830, val:  98.16%, val_best:  98.24%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-287 lr=['0.0100000'], tr/val_loss:  0.029904/  0.458684, val:  98.08%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-288 lr=['0.0100000'], tr/val_loss:  0.029444/  0.455854, val:  98.07%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-289 lr=['0.0100000'], tr/val_loss:  0.029636/  0.468017, val:  98.10%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-290 lr=['0.0100000'], tr/val_loss:  0.029257/  0.454183, val:  98.09%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-291 lr=['0.0100000'], tr/val_loss:  0.029013/  0.463317, val:  98.01%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-292 lr=['0.0100000'], tr/val_loss:  0.029727/  0.461404, val:  98.03%, val_best:  98.24%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-293 lr=['0.0100000'], tr/val_loss:  0.029231/  0.471976, val:  97.93%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-294 lr=['0.0100000'], tr/val_loss:  0.029344/  0.465591, val:  98.01%, val_best:  98.24%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    }
   ],
   "source": [
    "### my_snn control board (Gesture) ########################\n",
    "decay = 0.0 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "\n",
    "\n",
    "wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "my_snn_system(  devices = \"5\",\n",
    "                single_step = True, # True # False # DFA_on이랑 같이 가라\n",
    "                unique_name = run_name,\n",
    "                my_seed = 42,\n",
    "                TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "                BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 17, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "                # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = 'NMNIST_TONIC',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000.0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "                lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "\n",
    "                synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "                synapse_trace_const2 = decay, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "                # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "                cfg = [200, 200], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "                # cfg = ['M', 'M', 64], \n",
    "                # cfg = [64, 124, 64, 124],\n",
    "                # cfg = ['M','M',512], \n",
    "                # cfg = [512], \n",
    "                # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                # cfg = ['M','M',512],\n",
    "                # cfg = ['M',200],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = ['M','M',200,200],\n",
    "                # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = ['M',200,200],\n",
    "                # cfg = ['M','M',1024,512,256,128,64],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = [12], #fc\n",
    "                # cfg = [12, 'M', 48, 'M', 12], \n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [],        \n",
    "                \n",
    "                net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "                learning_rate = 0.01, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                epoch_num = 10000,\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "                BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                dvs_clipping = 1, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "                # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "                dvs_duration = 5_000, # 10_000 # 25_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "                # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "                # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "                DFA_on = True, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "                trace_on = False,   # True # False\n",
    "                OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용 # trace_on False면 의미없음.\n",
    "\n",
    "                exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "                extra_train_dataset = 0, \n",
    "\n",
    "                num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "                chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "                pin_memory = True, # True # False \n",
    "\n",
    "                UDA_on = False,  # DECREPATED # uda\n",
    "                alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                bias = True, # True # False \n",
    "\n",
    "                last_lif = False, # True # False \n",
    "\n",
    "                temporal_filter = 1, \n",
    "                initial_pooling = 1,\n",
    "\n",
    "                temporal_filter_accumulation = False, # True # False \n",
    "                ) \n",
    "\n",
    "# num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# num_workers = batch_size / num_GPU\n",
    "# num_workers = batch_size / num_CPU\n",
    "\n",
    "# sigmoid와 BN이 있어야 잘된다.\n",
    "# average pooling  \n",
    "# 이 낫다. \n",
    "\n",
    "# nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# # 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# # wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'bayes', # 'random', 'bayes'\n",
    "#     'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         # \"devices\": {\"values\": [\"1\"]},\n",
    "#         \"single_step\": {\"values\": [True]},\n",
    "#         # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "#         \"my_seed\": {\"values\": [42]},\n",
    "#         \"TIME\": {\"values\": [10]},\n",
    "#         \"BATCH\": {\"values\": [16]},\n",
    "#         \"IMAGE_SIZE\": {\"values\": [128]},\n",
    "#         \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "#         \"data_path\": {\"values\": ['/data2']},\n",
    "#         \"rate_coding\": {\"values\": [False]},\n",
    "#         \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "#         \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "#         \"lif_layer_v_threshold\": {\"values\": [0.25, 0.5, 0.75, 1.0]},\n",
    "#         \"lif_layer_v_reset\": {\"values\": [10000.0, 0.0]},\n",
    "#         \"lif_layer_sg_width\": {\"values\": [1.0,2.0,3.0,4.0,5.0]},\n",
    "\n",
    "#         \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "#         \"synapse_conv_stride\": {\"values\": [1]},\n",
    "#         \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "#         \"synapse_trace_const1\": {\"values\": [1]},\n",
    "#         \"synapse_trace_const2\": {\"values\": [0, 0.5]},\n",
    "\n",
    "#         \"pre_trained\": {\"values\": [False]},\n",
    "#         \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "#         \"cfg\": {\"values\": [['M','M',200,200]]},\n",
    "\n",
    "#         \"net_print\": {\"values\": [True]},\n",
    "\n",
    "#         \"pre_trained_path\": {\"values\": [\"net_save/save_now_net_weights_{unique_name}.pth\"]},\n",
    "#         \"learning_rate\": {\"values\": [0.001,0.01,0.1,0.0001]}, \n",
    "#         \"epoch_num\": {\"values\": [100]}, \n",
    "#         \"tdBN_on\": {\"values\": [False]},\n",
    "#         \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "#         \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"optimizer_what\": {\"values\": ['SGD']},\n",
    "#         \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "#         \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"dvs_clipping\": {\"values\": [5]}, \n",
    "\n",
    "#         \"dvs_duration\": {\"values\": [100_000]}, \n",
    "\n",
    "#         \"DFA_on\": {\"values\": [True, False]},\n",
    "\n",
    "#         \"trace_on\": {\"values\": [True]},\n",
    "#         \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "#         \"merge_polarities\": {\"values\": [False]},\n",
    "#         \"denoise_on\": {\"values\": [True, False]},\n",
    "\n",
    "#         \"extra_train_dataset\": {\"values\": [0]},\n",
    "\n",
    "#         \"num_workers\": {\"values\": [2]},\n",
    "#         \"chaching_on\": {\"values\": [True]},\n",
    "#         \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "#         \"UDA_on\": {\"values\": [False]},\n",
    "#         \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "#         \"bias\": {\"values\": [True]},\n",
    "\n",
    "#         \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "#         \"temporal_filter\": {\"values\": [1]},\n",
    "#         \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "#         \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "#     my_snn_system(  \n",
    "#         devices  =  \"0\",\n",
    "#         single_step  =  wandb.config.single_step,\n",
    "#         unique_name  =  unique_name_hyper,\n",
    "#         my_seed  =  wandb.config.my_seed,\n",
    "#         TIME  =  wandb.config.TIME,\n",
    "#         BATCH  =  wandb.config.BATCH,\n",
    "#         IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "#         which_data  =  wandb.config.which_data,\n",
    "#         data_path  =  wandb.config.data_path,\n",
    "#         rate_coding  =  wandb.config.rate_coding,\n",
    "#         lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "#         lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "#         lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "#         lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "#         lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "#         synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "#         synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "#         synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "#         synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "#         synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "#         pre_trained  =  wandb.config.pre_trained,\n",
    "#         convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "#         cfg  =  wandb.config.cfg,\n",
    "#         net_print  =  wandb.config.net_print,\n",
    "#         pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "#         learning_rate  =  wandb.config.learning_rate,\n",
    "#         epoch_num  =  wandb.config.epoch_num,\n",
    "#         tdBN_on  =  wandb.config.tdBN_on,\n",
    "#         BN_on  =  wandb.config.BN_on,\n",
    "#         surrogate  =  wandb.config.surrogate,\n",
    "#         BPTT_on  =  wandb.config.BPTT_on,\n",
    "#         optimizer_what  =  wandb.config.optimizer_what,\n",
    "#         scheduler_name  =  wandb.config.scheduler_name,\n",
    "#         ddp_on  =  wandb.config.ddp_on,\n",
    "#         dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "#         dvs_duration  =  wandb.config.dvs_duration,\n",
    "#         DFA_on  =  wandb.config.DFA_on,\n",
    "#         trace_on  =  wandb.config.trace_on,\n",
    "#         OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "#         exclude_class  =  wandb.config.exclude_class,\n",
    "#         merge_polarities  =  wandb.config.merge_polarities,\n",
    "#         denoise_on  =  wandb.config.denoise_on,\n",
    "#         extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "#         num_workers  =  wandb.config.num_workers,\n",
    "#         chaching_on  =  wandb.config.chaching_on,\n",
    "#         pin_memory  =  wandb.config.pin_memory,\n",
    "#         UDA_on  =  wandb.config.UDA_on,\n",
    "#         alpha_uda  =  wandb.config.alpha_uda,\n",
    "#         bias  =  wandb.config.bias,\n",
    "#         last_lif  =  wandb.config.last_lif,\n",
    "#         temporal_filter  =  wandb.config.temporal_filter,\n",
    "#         initial_pooling  =  wandb.config.initial_pooling,\n",
    "#         temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "#                         ) \n",
    "#     # sigmoid와 BN이 있어야 잘된다.\n",
    "#     # average pooling\n",
    "#     # 이 낫다. \n",
    "    \n",
    "#     # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# # sweep_id = '6pj3lh8j'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
