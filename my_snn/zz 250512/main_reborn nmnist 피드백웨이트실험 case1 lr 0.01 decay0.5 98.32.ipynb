{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20730/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> 클래스 인덱스\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            # MSE를 계산\n",
    "            ctx.save_for_backward(input, target)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "            return torch.mean((input - target_one_hot) ** 2)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE 스타일의 gradient를 흉내냄\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "            # print('grad_output', grad_output) # 이거 걍 1.0임\n",
    "            return input_one_hot - target_one_hot, None  # target에는 gradient 없음\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    \n",
    "    print('current loss function:', criterion)\n",
    "    ####################################################\n",
    "    \n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb 과거 하이퍼파라미터 가져와서 붙여넣기 (devices unique_name은 니가 할당해라)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250512_222116-tsq9v65j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tsq9v65j' target=\"_blank\">lively-star-8393</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tsq9v65j' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tsq9v65j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 17, 'which_data': 'NMNIST_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0.0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000.0, 'lif_layer_sg_width': 4.0, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_main.pth', 'learning_rate': 0.01, 'epoch_num': 10000, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1.0, 'bias': True, 'last_lif': False, 'temporal_filter': 1, 'initial_pooling': 1, 'temporal_filter_accumulation': False} \n",
      "\n",
      "dataset_hash = 7b0583c2e220caca87b64bcaac63adf4\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 3750 BATCH: 16 train_data_count: 60000\n",
      "len(test_loader): 625 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): Shaker_for_FC()\n",
      "      (2): Sparsity_Checker()\n",
      "      (3): SYNAPSE_FC(in_features=578, out_features=200, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.5, v_reset=10000.0, sg_width=4.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (5): Sparsity_Checker()\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (8): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.5, v_reset=10000.0, sg_width=4.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (9): Sparsity_Checker()\n",
      "      (10): Feedback_Receiver()\n",
      "      (11): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 158,010\n",
      "========================================================\n",
      "\n",
      "current loss function: CrossEntropyLoss()\n",
      "self.perm fc input 처음에 한번 섞기 tensor([575, 370,  61, 417, 461,  13, 521, 431, 548, 388, 477, 324, 235,  75,\n",
      "          4, 257,  81, 220, 328, 313, 274, 382, 334, 558,  35, 518, 482,  79,\n",
      "        133,  21, 171, 510, 389, 112, 159, 246, 507, 150, 437,  85, 287,  20,\n",
      "        418, 529, 394, 142, 503, 368,  32, 443, 156, 376, 110,  58, 128, 564,\n",
      "          5, 190, 309, 113, 407, 260, 547, 264,  94, 367, 141, 327, 341, 188,\n",
      "        253, 255, 364, 291,  25, 230,  89, 530, 285, 557,  82, 381, 138,  30,\n",
      "        252, 233, 353, 129, 254, 148, 210, 192, 100,  98, 321,  49, 377, 472,\n",
      "        345,  84, 225, 224, 245, 532, 490, 419, 447, 320, 372, 244, 184, 556,\n",
      "        359, 476, 214,   9, 237, 401, 467, 475, 474, 272, 132, 378, 549, 319,\n",
      "        135,  39, 436,  83, 312, 217, 271, 115, 293, 294, 373, 512, 536, 303,\n",
      "        131, 329, 535,  95, 404, 124, 123, 152, 109, 365, 533, 164, 116, 470,\n",
      "        339, 420, 178, 433, 351, 179,  40, 400, 390, 374,  72, 519, 356, 360,\n",
      "        538, 333, 411, 495,  41,  31,  69, 180, 465, 523, 207, 487, 396, 120,\n",
      "        391, 157, 173, 403, 386, 452, 511, 101, 406, 137, 182,  54, 163, 570,\n",
      "        338, 122, 145, 554, 185, 460, 405,  18, 261, 162, 302, 308, 453,   0,\n",
      "        144, 170, 198, 295, 161,  53,  73, 416,  46, 429, 169, 238, 402, 296,\n",
      "        380, 542, 540, 277, 514, 323, 301,  86, 565, 304, 577,  14, 143, 183,\n",
      "         63, 107, 212, 481, 213,  67, 573, 500, 286, 165, 290, 488,  88, 263,\n",
      "        218,  55, 493, 130, 197, 517, 205, 454,  59,   7, 392, 236, 262, 223,\n",
      "        240, 397, 119, 196, 114, 219, 318, 527, 459, 444,  93, 395, 408, 498,\n",
      "        151, 340, 464, 506, 509,  78,  70, 572, 221,  16, 562,  50, 479, 438,\n",
      "        208, 516,  36,  57, 200, 355,  91,  77, 243, 149, 485, 496, 568, 242,\n",
      "        469, 344, 106, 247, 471, 410, 413,  87, 352,   3, 393, 421, 231,  60,\n",
      "        189, 449,  38, 428, 357, 571,  19, 204, 227, 450, 520, 480, 504, 102,\n",
      "        435, 307, 335, 256, 222, 160,   1, 300, 515,  37, 288,  71, 306, 187,\n",
      "        125, 127, 531, 325, 385, 158, 379, 134,  48, 502, 166, 362,  90, 346,\n",
      "        154, 281, 473, 282, 167,  96, 349,  51, 269,  92, 422, 528, 425,  12,\n",
      "         42, 215, 267,  45, 276, 489, 491, 525, 427, 140,  26, 463, 442,  74,\n",
      "         64, 193, 441, 383, 455,   8, 458, 492, 387, 248, 409, 350, 358, 155,\n",
      "        546, 366,  97, 551, 423, 239,  52, 228, 118,  15,  47, 560, 117, 424,\n",
      "        567, 194, 168, 203,  66, 147, 172, 146, 501,  10, 279, 111, 268, 559,\n",
      "        175, 305, 211, 466,  62,  29, 299, 251, 297, 315, 311, 229, 484, 298,\n",
      "        177, 258, 539, 412, 348, 289, 426, 513, 216, 226, 555, 524, 398, 432,\n",
      "        552, 576, 434, 314, 105, 440, 574, 266,  33, 363, 316, 202, 280,  80,\n",
      "        522, 181, 278, 273, 139, 136,   6, 292, 354, 526,  17, 445, 462, 326,\n",
      "        250,  44, 486, 553, 569, 369, 561, 259, 494,  43, 439, 265, 195, 176,\n",
      "         68, 201, 284,  24, 121, 347,  65,  99, 343, 283, 241, 566, 322, 317,\n",
      "        104, 545, 103, 457,  27, 478, 199, 505, 483,  34, 234, 499, 275, 126,\n",
      "        310,  22,  56, 361, 537, 430, 249, 543, 451, 191, 534, 174, 497, 544,\n",
      "        186, 206, 508, 337, 108,   2, 541, 384, 371, 330, 375, 456, 414, 550,\n",
      "        336, 331, 232, 209, 270,  76,  11, 342, 415, 446, 332, 448,  28, 399,\n",
      "        563, 153,  23, 468], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0100000'], tr/val_loss:  0.634146/  0.546812, val:  91.44%, val_best:  91.44%, tr:  88.97%, tr_best:  88.97%\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "[module.layers.10] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.408043/  0.336736, val:  95.96%, val_best:  95.96%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.351650/  0.284800, val:  96.63%, val_best:  96.63%, tr:  95.24%, tr_best:  95.24%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.322791/  0.394867, val:  94.89%, val_best:  96.63%, tr:  95.92%, tr_best:  95.92%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.302078/  0.259305, val:  96.96%, val_best:  96.96%, tr:  96.28%, tr_best:  96.28%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.284717/  0.250317, val:  97.23%, val_best:  97.23%, tr:  96.69%, tr_best:  96.69%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.269123/  0.413691, val:  94.25%, val_best:  97.23%, tr:  96.86%, tr_best:  96.86%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.261707/  0.268609, val:  97.07%, val_best:  97.23%, tr:  97.05%, tr_best:  97.05%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.247254/  0.265494, val:  97.10%, val_best:  97.23%, tr:  97.21%, tr_best:  97.21%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.232181/  0.323978, val:  96.49%, val_best:  97.23%, tr:  97.43%, tr_best:  97.43%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.224121/  0.314355, val:  96.36%, val_best:  97.23%, tr:  97.59%, tr_best:  97.59%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.220624/  0.351684, val:  96.18%, val_best:  97.23%, tr:  97.68%, tr_best:  97.68%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.222250/  0.348623, val:  96.14%, val_best:  97.23%, tr:  97.76%, tr_best:  97.76%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.211282/  0.407894, val:  95.50%, val_best:  97.23%, tr:  97.86%, tr_best:  97.86%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.202695/  0.272566, val:  97.34%, val_best:  97.34%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.199571/  0.238734, val:  97.71%, val_best:  97.71%, tr:  98.07%, tr_best:  98.07%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.192168/  0.266557, val:  97.40%, val_best:  97.71%, tr:  98.13%, tr_best:  98.13%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.186666/  0.302661, val:  96.89%, val_best:  97.71%, tr:  98.23%, tr_best:  98.23%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.182866/  0.275925, val:  97.16%, val_best:  97.71%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.181378/  0.286762, val:  97.29%, val_best:  97.71%, tr:  98.41%, tr_best:  98.41%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.180705/  0.296577, val:  97.23%, val_best:  97.71%, tr:  98.41%, tr_best:  98.41%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.170472/  0.373932, val:  95.59%, val_best:  97.71%, tr:  98.51%, tr_best:  98.51%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.166170/  0.252289, val:  97.69%, val_best:  97.71%, tr:  98.61%, tr_best:  98.61%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.157717/  0.314721, val:  96.96%, val_best:  97.71%, tr:  98.74%, tr_best:  98.74%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.161535/  0.311150, val:  96.75%, val_best:  97.71%, tr:  98.64%, tr_best:  98.74%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.154713/  0.319047, val:  96.94%, val_best:  97.71%, tr:  98.76%, tr_best:  98.76%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.153950/  0.230465, val:  98.05%, val_best:  98.05%, tr:  98.81%, tr_best:  98.81%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.152327/  0.328654, val:  96.86%, val_best:  98.05%, tr:  98.82%, tr_best:  98.82%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.147832/  0.289361, val:  97.68%, val_best:  98.05%, tr:  98.87%, tr_best:  98.87%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.143591/  0.345928, val:  96.74%, val_best:  98.05%, tr:  98.93%, tr_best:  98.93%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.148759/  0.323498, val:  97.16%, val_best:  98.05%, tr:  98.87%, tr_best:  98.93%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.142412/  0.261280, val:  97.96%, val_best:  98.05%, tr:  98.97%, tr_best:  98.97%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.138844/  0.270315, val:  97.81%, val_best:  98.05%, tr:  98.95%, tr_best:  98.97%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.134153/  0.284845, val:  97.67%, val_best:  98.05%, tr:  99.04%, tr_best:  99.04%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.130932/  0.301978, val:  97.16%, val_best:  98.05%, tr:  99.09%, tr_best:  99.09%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.132688/  0.284470, val:  97.60%, val_best:  98.05%, tr:  99.09%, tr_best:  99.09%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.129481/  0.274901, val:  97.83%, val_best:  98.05%, tr:  99.10%, tr_best:  99.10%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.126258/  0.273973, val:  97.86%, val_best:  98.05%, tr:  99.12%, tr_best:  99.12%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.122971/  0.274003, val:  97.83%, val_best:  98.05%, tr:  99.21%, tr_best:  99.21%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.116591/  0.301745, val:  97.68%, val_best:  98.05%, tr:  99.31%, tr_best:  99.31%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.120566/  0.295471, val:  97.68%, val_best:  98.05%, tr:  99.23%, tr_best:  99.31%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.120535/  0.312683, val:  97.41%, val_best:  98.05%, tr:  99.28%, tr_best:  99.31%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.118250/  0.275629, val:  97.85%, val_best:  98.05%, tr:  99.26%, tr_best:  99.31%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.114376/  0.290403, val:  97.84%, val_best:  98.05%, tr:  99.33%, tr_best:  99.33%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.116298/  0.294357, val:  97.64%, val_best:  98.05%, tr:  99.31%, tr_best:  99.33%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.111125/  0.291293, val:  97.75%, val_best:  98.05%, tr:  99.36%, tr_best:  99.36%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.108485/  0.277300, val:  97.91%, val_best:  98.05%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.107951/  0.279477, val:  97.96%, val_best:  98.05%, tr:  99.38%, tr_best:  99.39%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.105353/  0.277760, val:  97.84%, val_best:  98.05%, tr:  99.44%, tr_best:  99.44%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.102840/  0.279120, val:  97.92%, val_best:  98.05%, tr:  99.47%, tr_best:  99.47%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.105753/  0.268318, val:  98.11%, val_best:  98.11%, tr:  99.46%, tr_best:  99.47%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.103916/  0.279270, val:  98.05%, val_best:  98.11%, tr:  99.44%, tr_best:  99.47%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.104122/  0.288272, val:  97.97%, val_best:  98.11%, tr:  99.44%, tr_best:  99.47%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.100490/  0.266172, val:  98.10%, val_best:  98.11%, tr:  99.51%, tr_best:  99.51%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.100875/  0.301287, val:  97.84%, val_best:  98.11%, tr:  99.50%, tr_best:  99.51%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.098418/  0.313043, val:  97.51%, val_best:  98.11%, tr:  99.53%, tr_best:  99.53%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.096841/  0.313206, val:  97.34%, val_best:  98.11%, tr:  99.52%, tr_best:  99.53%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.094061/  0.288655, val:  97.99%, val_best:  98.11%, tr:  99.53%, tr_best:  99.53%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.094340/  0.317834, val:  97.60%, val_best:  98.11%, tr:  99.55%, tr_best:  99.55%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.093037/  0.309615, val:  97.78%, val_best:  98.11%, tr:  99.54%, tr_best:  99.55%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.091859/  0.315339, val:  97.72%, val_best:  98.11%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.093806/  0.306225, val:  97.78%, val_best:  98.11%, tr:  99.58%, tr_best:  99.59%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.090893/  0.383650, val:  96.54%, val_best:  98.11%, tr:  99.56%, tr_best:  99.59%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.087959/  0.325914, val:  97.71%, val_best:  98.11%, tr:  99.63%, tr_best:  99.63%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.090081/  0.300646, val:  97.92%, val_best:  98.11%, tr:  99.59%, tr_best:  99.63%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.085300/  0.292808, val:  97.90%, val_best:  98.11%, tr:  99.66%, tr_best:  99.66%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.090304/  0.312891, val:  98.00%, val_best:  98.11%, tr:  99.61%, tr_best:  99.66%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.083007/  0.315008, val:  97.67%, val_best:  98.11%, tr:  99.67%, tr_best:  99.67%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.084134/  0.279856, val:  98.08%, val_best:  98.11%, tr:  99.67%, tr_best:  99.67%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.080875/  0.290434, val:  98.18%, val_best:  98.18%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.082639/  0.315752, val:  97.95%, val_best:  98.18%, tr:  99.72%, tr_best:  99.72%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.082961/  0.315568, val:  97.68%, val_best:  98.18%, tr:  99.68%, tr_best:  99.72%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.081113/  0.391098, val:  97.14%, val_best:  98.18%, tr:  99.69%, tr_best:  99.72%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.077539/  0.312657, val:  97.80%, val_best:  98.18%, tr:  99.73%, tr_best:  99.73%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.078432/  0.294574, val:  98.07%, val_best:  98.18%, tr:  99.71%, tr_best:  99.73%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.079567/  0.296679, val:  98.11%, val_best:  98.18%, tr:  99.71%, tr_best:  99.73%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.073689/  0.336167, val:  97.90%, val_best:  98.18%, tr:  99.79%, tr_best:  99.79%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.077634/  0.316346, val:  97.95%, val_best:  98.18%, tr:  99.74%, tr_best:  99.79%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.076220/  0.322582, val:  97.77%, val_best:  98.18%, tr:  99.74%, tr_best:  99.79%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.076803/  0.329427, val:  97.82%, val_best:  98.18%, tr:  99.72%, tr_best:  99.79%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.075649/  0.300731, val:  98.02%, val_best:  98.18%, tr:  99.74%, tr_best:  99.79%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.073771/  0.343488, val:  97.84%, val_best:  98.18%, tr:  99.77%, tr_best:  99.79%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.073484/  0.311355, val:  97.88%, val_best:  98.18%, tr:  99.78%, tr_best:  99.79%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.072458/  0.313313, val:  98.13%, val_best:  98.18%, tr:  99.77%, tr_best:  99.79%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.073386/  0.340855, val:  97.71%, val_best:  98.18%, tr:  99.77%, tr_best:  99.79%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.074600/  0.305956, val:  98.11%, val_best:  98.18%, tr:  99.76%, tr_best:  99.79%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.070495/  0.313296, val:  98.05%, val_best:  98.18%, tr:  99.81%, tr_best:  99.81%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.070276/  0.397431, val:  96.98%, val_best:  98.18%, tr:  99.82%, tr_best:  99.82%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.070926/  0.318051, val:  98.00%, val_best:  98.18%, tr:  99.80%, tr_best:  99.82%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.071232/  0.315252, val:  98.11%, val_best:  98.18%, tr:  99.77%, tr_best:  99.82%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.068069/  0.314163, val:  98.03%, val_best:  98.18%, tr:  99.81%, tr_best:  99.82%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.069609/  0.317942, val:  98.10%, val_best:  98.18%, tr:  99.82%, tr_best:  99.82%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.068466/  0.321389, val:  98.16%, val_best:  98.18%, tr:  99.82%, tr_best:  99.82%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.065199/  0.337968, val:  97.90%, val_best:  98.18%, tr:  99.84%, tr_best:  99.84%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.066788/  0.318897, val:  98.13%, val_best:  98.18%, tr:  99.82%, tr_best:  99.84%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.067867/  0.351421, val:  97.79%, val_best:  98.18%, tr:  99.80%, tr_best:  99.84%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.067011/  0.311984, val:  98.32%, val_best:  98.32%, tr:  99.82%, tr_best:  99.84%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.065230/  0.321616, val:  98.14%, val_best:  98.32%, tr:  99.82%, tr_best:  99.84%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.065757/  0.319669, val:  98.21%, val_best:  98.32%, tr:  99.82%, tr_best:  99.84%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.064147/  0.343602, val:  97.93%, val_best:  98.32%, tr:  99.85%, tr_best:  99.85%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.062097/  0.348095, val:  97.74%, val_best:  98.32%, tr:  99.88%, tr_best:  99.88%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.062317/  0.322245, val:  98.08%, val_best:  98.32%, tr:  99.88%, tr_best:  99.88%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.061469/  0.337653, val:  98.02%, val_best:  98.32%, tr:  99.87%, tr_best:  99.88%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.060319/  0.323883, val:  98.00%, val_best:  98.32%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.060308/  0.341781, val:  97.95%, val_best:  98.32%, tr:  99.86%, tr_best:  99.89%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.059548/  0.363204, val:  97.92%, val_best:  98.32%, tr:  99.87%, tr_best:  99.89%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.061486/  0.373745, val:  97.63%, val_best:  98.32%, tr:  99.86%, tr_best:  99.89%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.061234/  0.330748, val:  98.05%, val_best:  98.32%, tr:  99.86%, tr_best:  99.89%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.059513/  0.349716, val:  97.81%, val_best:  98.32%, tr:  99.86%, tr_best:  99.89%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.057618/  0.348808, val:  97.92%, val_best:  98.32%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.058635/  0.342469, val:  98.13%, val_best:  98.32%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.058977/  0.328187, val:  98.04%, val_best:  98.32%, tr:  99.88%, tr_best:  99.89%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.057969/  0.345058, val:  98.02%, val_best:  98.32%, tr:  99.87%, tr_best:  99.89%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.058026/  0.334074, val:  98.09%, val_best:  98.32%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.058786/  0.326759, val:  98.20%, val_best:  98.32%, tr:  99.89%, tr_best:  99.90%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.056612/  0.371497, val:  97.58%, val_best:  98.32%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.056125/  0.336040, val:  98.06%, val_best:  98.32%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.054725/  0.405881, val:  97.46%, val_best:  98.32%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.056898/  0.335370, val:  98.06%, val_best:  98.32%, tr:  99.90%, tr_best:  99.91%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.056631/  0.352416, val:  97.97%, val_best:  98.32%, tr:  99.89%, tr_best:  99.91%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.056812/  0.354264, val:  97.88%, val_best:  98.32%, tr:  99.90%, tr_best:  99.91%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.053983/  0.385945, val:  97.78%, val_best:  98.32%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.053737/  0.359545, val:  97.90%, val_best:  98.32%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.057058/  0.350009, val:  97.96%, val_best:  98.32%, tr:  99.88%, tr_best:  99.93%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.054417/  0.334167, val:  98.07%, val_best:  98.32%, tr:  99.91%, tr_best:  99.93%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.051773/  0.362775, val:  97.88%, val_best:  98.32%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.053638/  0.330348, val:  98.30%, val_best:  98.32%, tr:  99.92%, tr_best:  99.93%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.053633/  0.346112, val:  98.05%, val_best:  98.32%, tr:  99.91%, tr_best:  99.93%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.051303/  0.340049, val:  98.26%, val_best:  98.32%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.051571/  0.341003, val:  98.21%, val_best:  98.32%, tr:  99.91%, tr_best:  99.93%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.052575/  0.369410, val:  97.94%, val_best:  98.32%, tr:  99.91%, tr_best:  99.93%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.049539/  0.345393, val:  98.10%, val_best:  98.32%, tr:  99.94%, tr_best:  99.94%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.050375/  0.383209, val:  97.79%, val_best:  98.32%, tr:  99.94%, tr_best:  99.94%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.050484/  0.354322, val:  98.11%, val_best:  98.32%, tr:  99.92%, tr_best:  99.94%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.051038/  0.368051, val:  98.02%, val_best:  98.32%, tr:  99.92%, tr_best:  99.94%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.050804/  0.370675, val:  97.91%, val_best:  98.32%, tr:  99.93%, tr_best:  99.94%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.050050/  0.376175, val:  97.93%, val_best:  98.32%, tr:  99.93%, tr_best:  99.94%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.048779/  0.367459, val:  97.93%, val_best:  98.32%, tr:  99.93%, tr_best:  99.94%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.048899/  0.353822, val:  98.09%, val_best:  98.32%, tr:  99.94%, tr_best:  99.94%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.049226/  0.345385, val:  98.18%, val_best:  98.32%, tr:  99.93%, tr_best:  99.94%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.048089/  0.376571, val:  97.95%, val_best:  98.32%, tr:  99.94%, tr_best:  99.94%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.047900/  0.381938, val:  97.97%, val_best:  98.32%, tr:  99.94%, tr_best:  99.94%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.047533/  0.356533, val:  98.05%, val_best:  98.32%, tr:  99.94%, tr_best:  99.94%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.048237/  0.387152, val:  97.90%, val_best:  98.32%, tr:  99.94%, tr_best:  99.94%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.048532/  0.354776, val:  98.10%, val_best:  98.32%, tr:  99.94%, tr_best:  99.94%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.048285/  0.343132, val:  98.25%, val_best:  98.32%, tr:  99.93%, tr_best:  99.94%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.047112/  0.357597, val:  98.21%, val_best:  98.32%, tr:  99.94%, tr_best:  99.94%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.046828/  0.351445, val:  98.24%, val_best:  98.32%, tr:  99.94%, tr_best:  99.94%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.047536/  0.360546, val:  98.15%, val_best:  98.32%, tr:  99.94%, tr_best:  99.94%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.045559/  0.355292, val:  98.15%, val_best:  98.32%, tr:  99.96%, tr_best:  99.96%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.047108/  0.390270, val:  98.00%, val_best:  98.32%, tr:  99.96%, tr_best:  99.96%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.047135/  0.377626, val:  97.93%, val_best:  98.32%, tr:  99.92%, tr_best:  99.96%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.046084/  0.368075, val:  97.97%, val_best:  98.32%, tr:  99.95%, tr_best:  99.96%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.045089/  0.363507, val:  98.18%, val_best:  98.32%, tr:  99.94%, tr_best:  99.96%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.046068/  0.369605, val:  98.00%, val_best:  98.32%, tr:  99.94%, tr_best:  99.96%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.045090/  0.361383, val:  98.16%, val_best:  98.32%, tr:  99.95%, tr_best:  99.96%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.043398/  0.362729, val:  98.07%, val_best:  98.32%, tr:  99.95%, tr_best:  99.96%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.047149/  0.375183, val:  98.10%, val_best:  98.32%, tr:  99.94%, tr_best:  99.96%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.044776/  0.363096, val:  98.20%, val_best:  98.32%, tr:  99.96%, tr_best:  99.96%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.044897/  0.396077, val:  98.02%, val_best:  98.32%, tr:  99.96%, tr_best:  99.96%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.045314/  0.372134, val:  98.16%, val_best:  98.32%, tr:  99.94%, tr_best:  99.96%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.043765/  0.373394, val:  98.11%, val_best:  98.32%, tr:  99.96%, tr_best:  99.96%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.042737/  0.381709, val:  98.13%, val_best:  98.32%, tr:  99.96%, tr_best:  99.96%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.042034/  0.362205, val:  98.20%, val_best:  98.32%, tr:  99.97%, tr_best:  99.97%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.042671/  0.367012, val:  98.09%, val_best:  98.32%, tr:  99.95%, tr_best:  99.97%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.041123/  0.403108, val:  97.81%, val_best:  98.32%, tr:  99.96%, tr_best:  99.97%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.043205/  0.378612, val:  98.05%, val_best:  98.32%, tr:  99.97%, tr_best:  99.97%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.041468/  0.370983, val:  98.23%, val_best:  98.32%, tr:  99.95%, tr_best:  99.97%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.041180/  0.355431, val:  98.25%, val_best:  98.32%, tr:  99.95%, tr_best:  99.97%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.041192/  0.362827, val:  98.35%, val_best:  98.35%, tr:  99.96%, tr_best:  99.97%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.042037/  0.390437, val:  97.76%, val_best:  98.35%, tr:  99.95%, tr_best:  99.97%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.040848/  0.376034, val:  98.14%, val_best:  98.35%, tr:  99.96%, tr_best:  99.97%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.041074/  0.384718, val:  98.02%, val_best:  98.35%, tr:  99.97%, tr_best:  99.97%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.040615/  0.427851, val:  97.71%, val_best:  98.35%, tr:  99.95%, tr_best:  99.97%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.040335/  0.369937, val:  98.17%, val_best:  98.35%, tr:  99.97%, tr_best:  99.97%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.041027/  0.378392, val:  98.02%, val_best:  98.35%, tr:  99.95%, tr_best:  99.97%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.038936/  0.379278, val:  98.08%, val_best:  98.35%, tr:  99.97%, tr_best:  99.97%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.038789/  0.380505, val:  98.07%, val_best:  98.35%, tr:  99.96%, tr_best:  99.97%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.039616/  0.371079, val:  98.08%, val_best:  98.35%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.039002/  0.400840, val:  97.95%, val_best:  98.35%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.039257/  0.373648, val:  98.17%, val_best:  98.35%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.039449/  0.373130, val:  98.33%, val_best:  98.35%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.038614/  0.394007, val:  98.07%, val_best:  98.35%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.038299/  0.383154, val:  98.19%, val_best:  98.35%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.036750/  0.365463, val:  98.26%, val_best:  98.35%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.037293/  0.400909, val:  97.96%, val_best:  98.35%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.038514/  0.392149, val:  98.06%, val_best:  98.35%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.036598/  0.387920, val:  98.06%, val_best:  98.35%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.036934/  0.401068, val:  98.06%, val_best:  98.35%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.037479/  0.385142, val:  98.21%, val_best:  98.35%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.035572/  0.379422, val:  98.19%, val_best:  98.35%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.037318/  0.393851, val:  98.20%, val_best:  98.35%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.036692/  0.383422, val:  98.17%, val_best:  98.35%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.035675/  0.391820, val:  98.07%, val_best:  98.35%, tr:  99.97%, tr_best:  99.98%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.037124/  0.391775, val:  98.11%, val_best:  98.35%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.035728/  0.390655, val:  98.10%, val_best:  98.35%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.036765/  0.383519, val:  98.16%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.037040/  0.381290, val:  98.27%, val_best:  98.35%, tr:  99.97%, tr_best:  99.99%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.034816/  0.384527, val:  98.29%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.035943/  0.410684, val:  97.92%, val_best:  98.35%, tr:  99.97%, tr_best:  99.99%\n",
      "epoch-200 lr=['0.0100000'], tr/val_loss:  0.035830/  0.383664, val:  98.24%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-201 lr=['0.0100000'], tr/val_loss:  0.036023/  0.385927, val:  98.13%, val_best:  98.35%, tr:  99.97%, tr_best:  99.99%\n",
      "epoch-202 lr=['0.0100000'], tr/val_loss:  0.036406/  0.405850, val:  97.97%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-203 lr=['0.0100000'], tr/val_loss:  0.035456/  0.391061, val:  98.10%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-204 lr=['0.0100000'], tr/val_loss:  0.035567/  0.414213, val:  97.92%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-205 lr=['0.0100000'], tr/val_loss:  0.035038/  0.406250, val:  98.01%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-206 lr=['0.0100000'], tr/val_loss:  0.034257/  0.382986, val:  98.22%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-207 lr=['0.0100000'], tr/val_loss:  0.035673/  0.392643, val:  98.19%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-208 lr=['0.0100000'], tr/val_loss:  0.034116/  0.389114, val:  98.20%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-209 lr=['0.0100000'], tr/val_loss:  0.033767/  0.400006, val:  98.08%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-210 lr=['0.0100000'], tr/val_loss:  0.035203/  0.393988, val:  98.13%, val_best:  98.35%, tr:  99.97%, tr_best:  99.99%\n",
      "epoch-211 lr=['0.0100000'], tr/val_loss:  0.034392/  0.389007, val:  98.15%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-212 lr=['0.0100000'], tr/val_loss:  0.034757/  0.396038, val:  98.13%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-213 lr=['0.0100000'], tr/val_loss:  0.033803/  0.389648, val:  98.22%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-214 lr=['0.0100000'], tr/val_loss:  0.033632/  0.394403, val:  98.19%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-215 lr=['0.0100000'], tr/val_loss:  0.035154/  0.399093, val:  98.22%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-216 lr=['0.0100000'], tr/val_loss:  0.033837/  0.398897, val:  98.11%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-217 lr=['0.0100000'], tr/val_loss:  0.032612/  0.390339, val:  98.31%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-218 lr=['0.0100000'], tr/val_loss:  0.032474/  0.399702, val:  98.12%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-219 lr=['0.0100000'], tr/val_loss:  0.033994/  0.389430, val:  98.22%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-220 lr=['0.0100000'], tr/val_loss:  0.033312/  0.393916, val:  98.21%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-221 lr=['0.0100000'], tr/val_loss:  0.032987/  0.403389, val:  98.12%, val_best:  98.35%, tr:  99.97%, tr_best:  99.99%\n",
      "epoch-222 lr=['0.0100000'], tr/val_loss:  0.032623/  0.400568, val:  98.08%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-223 lr=['0.0100000'], tr/val_loss:  0.031972/  0.402907, val:  98.06%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-224 lr=['0.0100000'], tr/val_loss:  0.033015/  0.399081, val:  98.20%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-225 lr=['0.0100000'], tr/val_loss:  0.032579/  0.420687, val:  97.86%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-226 lr=['0.0100000'], tr/val_loss:  0.031845/  0.397963, val:  98.18%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-227 lr=['0.0100000'], tr/val_loss:  0.032493/  0.407952, val:  98.25%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-228 lr=['0.0100000'], tr/val_loss:  0.033554/  0.391357, val:  98.19%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-229 lr=['0.0100000'], tr/val_loss:  0.033594/  0.404556, val:  98.18%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-230 lr=['0.0100000'], tr/val_loss:  0.032174/  0.402663, val:  98.16%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-231 lr=['0.0100000'], tr/val_loss:  0.033009/  0.427914, val:  97.94%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-232 lr=['0.0100000'], tr/val_loss:  0.031041/  0.399727, val:  98.23%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-233 lr=['0.0100000'], tr/val_loss:  0.032226/  0.423218, val:  98.12%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-234 lr=['0.0100000'], tr/val_loss:  0.031172/  0.402124, val:  98.08%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-235 lr=['0.0100000'], tr/val_loss:  0.031818/  0.396323, val:  98.32%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-236 lr=['0.0100000'], tr/val_loss:  0.032681/  0.411209, val:  98.02%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-237 lr=['0.0100000'], tr/val_loss:  0.031190/  0.404500, val:  98.13%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-238 lr=['0.0100000'], tr/val_loss:  0.031008/  0.400689, val:  98.22%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-239 lr=['0.0100000'], tr/val_loss:  0.030195/  0.444085, val:  97.83%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-240 lr=['0.0100000'], tr/val_loss:  0.031322/  0.401197, val:  98.24%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-241 lr=['0.0100000'], tr/val_loss:  0.030160/  0.402175, val:  98.26%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-242 lr=['0.0100000'], tr/val_loss:  0.031675/  0.402419, val:  98.23%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-243 lr=['0.0100000'], tr/val_loss:  0.030992/  0.407554, val:  98.24%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-244 lr=['0.0100000'], tr/val_loss:  0.030879/  0.417656, val:  98.14%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-245 lr=['0.0100000'], tr/val_loss:  0.031023/  0.424023, val:  98.15%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-246 lr=['0.0100000'], tr/val_loss:  0.030530/  0.415095, val:  98.25%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-247 lr=['0.0100000'], tr/val_loss:  0.030226/  0.416167, val:  98.19%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-248 lr=['0.0100000'], tr/val_loss:  0.030802/  0.417835, val:  98.10%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-249 lr=['0.0100000'], tr/val_loss:  0.031047/  0.409904, val:  98.07%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-250 lr=['0.0100000'], tr/val_loss:  0.029807/  0.409025, val:  98.16%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-251 lr=['0.0100000'], tr/val_loss:  0.030469/  0.401219, val:  98.20%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-252 lr=['0.0100000'], tr/val_loss:  0.029913/  0.417311, val:  98.19%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-253 lr=['0.0100000'], tr/val_loss:  0.031287/  0.408595, val:  98.12%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-254 lr=['0.0100000'], tr/val_loss:  0.030937/  0.418409, val:  98.07%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-255 lr=['0.0100000'], tr/val_loss:  0.029039/  0.403812, val:  98.22%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-256 lr=['0.0100000'], tr/val_loss:  0.029024/  0.413570, val:  98.24%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-257 lr=['0.0100000'], tr/val_loss:  0.028850/  0.423055, val:  98.16%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-258 lr=['0.0100000'], tr/val_loss:  0.028878/  0.411864, val:  98.15%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-259 lr=['0.0100000'], tr/val_loss:  0.029752/  0.419672, val:  98.07%, val_best:  98.35%, tr:  99.98%, tr_best:  99.99%\n",
      "epoch-260 lr=['0.0100000'], tr/val_loss:  0.029495/  0.414698, val:  98.15%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-261 lr=['0.0100000'], tr/val_loss:  0.028356/  0.409770, val:  98.22%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-262 lr=['0.0100000'], tr/val_loss:  0.028216/  0.417010, val:  98.18%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-263 lr=['0.0100000'], tr/val_loss:  0.028579/  0.412241, val:  98.17%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-264 lr=['0.0100000'], tr/val_loss:  0.029118/  0.415065, val:  98.18%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-265 lr=['0.0100000'], tr/val_loss:  0.028248/  0.405596, val:  98.24%, val_best:  98.35%, tr:  99.99%, tr_best:  99.99%\n",
      "epoch-266 lr=['0.0100000'], tr/val_loss:  0.028560/  0.431340, val:  98.04%, val_best:  98.35%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-267 lr=['0.0100000'], tr/val_loss:  0.028106/  0.420543, val:  98.21%, val_best:  98.35%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-268 lr=['0.0100000'], tr/val_loss:  0.028690/  0.413674, val:  98.15%, val_best:  98.35%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-269 lr=['0.0100000'], tr/val_loss:  0.028124/  0.416158, val:  98.20%, val_best:  98.35%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-270 lr=['0.0100000'], tr/val_loss:  0.027569/  0.422117, val:  98.15%, val_best:  98.35%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-271 lr=['0.0100000'], tr/val_loss:  0.027779/  0.422680, val:  98.02%, val_best:  98.35%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-272 lr=['0.0100000'], tr/val_loss:  0.027231/  0.414940, val:  98.21%, val_best:  98.35%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-273 lr=['0.0100000'], tr/val_loss:  0.027496/  0.425651, val:  98.06%, val_best:  98.35%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-274 lr=['0.0100000'], tr/val_loss:  0.026920/  0.419138, val:  98.13%, val_best:  98.35%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-275 lr=['0.0100000'], tr/val_loss:  0.027447/  0.410735, val:  98.17%, val_best:  98.35%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-276 lr=['0.0100000'], tr/val_loss:  0.026540/  0.421331, val:  98.11%, val_best:  98.35%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-277 lr=['0.0100000'], tr/val_loss:  0.027661/  0.415104, val:  98.22%, val_best:  98.35%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-278 lr=['0.0100000'], tr/val_loss:  0.026703/  0.424120, val:  98.14%, val_best:  98.35%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-279 lr=['0.0100000'], tr/val_loss:  0.027890/  0.412596, val:  98.08%, val_best:  98.35%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-280 lr=['0.0100000'], tr/val_loss:  0.027493/  0.416505, val:  98.21%, val_best:  98.35%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-281 lr=['0.0100000'], tr/val_loss:  0.029143/  0.427303, val:  98.08%, val_best:  98.35%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-282 lr=['0.0100000'], tr/val_loss:  0.026676/  0.419999, val:  98.18%, val_best:  98.35%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-283 lr=['0.0100000'], tr/val_loss:  0.027091/  0.422820, val:  98.23%, val_best:  98.35%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-284 lr=['0.0100000'], tr/val_loss:  0.025975/  0.425901, val:  98.10%, val_best:  98.35%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-285 lr=['0.0100000'], tr/val_loss:  0.026568/  0.423571, val:  98.14%, val_best:  98.35%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-286 lr=['0.0100000'], tr/val_loss:  0.025901/  0.421464, val:  98.12%, val_best:  98.35%, tr:  99.99%, tr_best: 100.00%\n",
      "epoch-287 lr=['0.0100000'], tr/val_loss:  0.025147/  0.430192, val:  98.28%, val_best:  98.35%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-288 lr=['0.0100000'], tr/val_loss:  0.027173/  0.427962, val:  98.19%, val_best:  98.35%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-289 lr=['0.0100000'], tr/val_loss:  0.026535/  0.414619, val:  98.18%, val_best:  98.35%, tr:  99.99%, tr_best: 100.00%\n"
     ]
    }
   ],
   "source": [
    "### my_snn control board (Gesture) ########################\n",
    "decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "\n",
    "\n",
    "wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "my_snn_system(  devices = \"1\",\n",
    "                single_step = True, # True # False # DFA_on이랑 같이 가라\n",
    "                unique_name = run_name,\n",
    "                my_seed = 42,\n",
    "                TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "                BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 17, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "                # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = 'NMNIST_TONIC',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000.0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "                lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "\n",
    "                synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "                synapse_trace_const2 = decay, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "                # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "                cfg = [200, 200], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "                # cfg = ['M', 'M', 64], \n",
    "                # cfg = [64, 124, 64, 124],\n",
    "                # cfg = ['M','M',512], \n",
    "                # cfg = [512], \n",
    "                # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                # cfg = ['M','M',512],\n",
    "                # cfg = ['M',200],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = ['M','M',200,200],\n",
    "                # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = ['M',200,200],\n",
    "                # cfg = ['M','M',1024,512,256,128,64],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = [12], #fc\n",
    "                # cfg = [12, 'M', 48, 'M', 12], \n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [],        \n",
    "                \n",
    "                net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "                learning_rate = 0.01, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                epoch_num = 10000,\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "                BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                dvs_clipping = 1, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "                # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "                dvs_duration = 5_000, # 10_000 # 25_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "                # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "                # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "                DFA_on = True, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "                trace_on = False,   # True # False\n",
    "                OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용 # trace_on False면 의미없음.\n",
    "\n",
    "                exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "                extra_train_dataset = 0, \n",
    "\n",
    "                num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "                chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "                pin_memory = True, # True # False \n",
    "\n",
    "                UDA_on = False,  # DECREPATED # uda\n",
    "                alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                bias = True, # True # False \n",
    "\n",
    "                last_lif = False, # True # False \n",
    "\n",
    "                temporal_filter = 1, \n",
    "                initial_pooling = 1,\n",
    "\n",
    "                temporal_filter_accumulation = False, # True # False \n",
    "                ) \n",
    "\n",
    "# num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# num_workers = batch_size / num_GPU\n",
    "# num_workers = batch_size / num_CPU\n",
    "\n",
    "# sigmoid와 BN이 있어야 잘된다.\n",
    "# average pooling  \n",
    "# 이 낫다. \n",
    "\n",
    "# nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# # 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# # wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'bayes', # 'random', 'bayes'\n",
    "#     'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         # \"devices\": {\"values\": [\"1\"]},\n",
    "#         \"single_step\": {\"values\": [True]},\n",
    "#         # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "#         \"my_seed\": {\"values\": [42]},\n",
    "#         \"TIME\": {\"values\": [10]},\n",
    "#         \"BATCH\": {\"values\": [16]},\n",
    "#         \"IMAGE_SIZE\": {\"values\": [128]},\n",
    "#         \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "#         \"data_path\": {\"values\": ['/data2']},\n",
    "#         \"rate_coding\": {\"values\": [False]},\n",
    "#         \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "#         \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "#         \"lif_layer_v_threshold\": {\"values\": [0.25, 0.5, 0.75, 1.0]},\n",
    "#         \"lif_layer_v_reset\": {\"values\": [10000.0, 0.0]},\n",
    "#         \"lif_layer_sg_width\": {\"values\": [1.0,2.0,3.0,4.0,5.0]},\n",
    "\n",
    "#         \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "#         \"synapse_conv_stride\": {\"values\": [1]},\n",
    "#         \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "#         \"synapse_trace_const1\": {\"values\": [1]},\n",
    "#         \"synapse_trace_const2\": {\"values\": [0, 0.5]},\n",
    "\n",
    "#         \"pre_trained\": {\"values\": [False]},\n",
    "#         \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "#         \"cfg\": {\"values\": [['M','M',200,200]]},\n",
    "\n",
    "#         \"net_print\": {\"values\": [True]},\n",
    "\n",
    "#         \"pre_trained_path\": {\"values\": [\"net_save/save_now_net_weights_{unique_name}.pth\"]},\n",
    "#         \"learning_rate\": {\"values\": [0.001,0.01,0.1,0.0001]}, \n",
    "#         \"epoch_num\": {\"values\": [100]}, \n",
    "#         \"tdBN_on\": {\"values\": [False]},\n",
    "#         \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "#         \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"optimizer_what\": {\"values\": ['SGD']},\n",
    "#         \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "#         \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"dvs_clipping\": {\"values\": [5]}, \n",
    "\n",
    "#         \"dvs_duration\": {\"values\": [100_000]}, \n",
    "\n",
    "#         \"DFA_on\": {\"values\": [True, False]},\n",
    "\n",
    "#         \"trace_on\": {\"values\": [True]},\n",
    "#         \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "#         \"merge_polarities\": {\"values\": [False]},\n",
    "#         \"denoise_on\": {\"values\": [True, False]},\n",
    "\n",
    "#         \"extra_train_dataset\": {\"values\": [0]},\n",
    "\n",
    "#         \"num_workers\": {\"values\": [2]},\n",
    "#         \"chaching_on\": {\"values\": [True]},\n",
    "#         \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "#         \"UDA_on\": {\"values\": [False]},\n",
    "#         \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "#         \"bias\": {\"values\": [True]},\n",
    "\n",
    "#         \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "#         \"temporal_filter\": {\"values\": [1]},\n",
    "#         \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "#         \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "#     my_snn_system(  \n",
    "#         devices  =  \"0\",\n",
    "#         single_step  =  wandb.config.single_step,\n",
    "#         unique_name  =  unique_name_hyper,\n",
    "#         my_seed  =  wandb.config.my_seed,\n",
    "#         TIME  =  wandb.config.TIME,\n",
    "#         BATCH  =  wandb.config.BATCH,\n",
    "#         IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "#         which_data  =  wandb.config.which_data,\n",
    "#         data_path  =  wandb.config.data_path,\n",
    "#         rate_coding  =  wandb.config.rate_coding,\n",
    "#         lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "#         lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "#         lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "#         lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "#         lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "#         synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "#         synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "#         synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "#         synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "#         synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "#         pre_trained  =  wandb.config.pre_trained,\n",
    "#         convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "#         cfg  =  wandb.config.cfg,\n",
    "#         net_print  =  wandb.config.net_print,\n",
    "#         pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "#         learning_rate  =  wandb.config.learning_rate,\n",
    "#         epoch_num  =  wandb.config.epoch_num,\n",
    "#         tdBN_on  =  wandb.config.tdBN_on,\n",
    "#         BN_on  =  wandb.config.BN_on,\n",
    "#         surrogate  =  wandb.config.surrogate,\n",
    "#         BPTT_on  =  wandb.config.BPTT_on,\n",
    "#         optimizer_what  =  wandb.config.optimizer_what,\n",
    "#         scheduler_name  =  wandb.config.scheduler_name,\n",
    "#         ddp_on  =  wandb.config.ddp_on,\n",
    "#         dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "#         dvs_duration  =  wandb.config.dvs_duration,\n",
    "#         DFA_on  =  wandb.config.DFA_on,\n",
    "#         trace_on  =  wandb.config.trace_on,\n",
    "#         OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "#         exclude_class  =  wandb.config.exclude_class,\n",
    "#         merge_polarities  =  wandb.config.merge_polarities,\n",
    "#         denoise_on  =  wandb.config.denoise_on,\n",
    "#         extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "#         num_workers  =  wandb.config.num_workers,\n",
    "#         chaching_on  =  wandb.config.chaching_on,\n",
    "#         pin_memory  =  wandb.config.pin_memory,\n",
    "#         UDA_on  =  wandb.config.UDA_on,\n",
    "#         alpha_uda  =  wandb.config.alpha_uda,\n",
    "#         bias  =  wandb.config.bias,\n",
    "#         last_lif  =  wandb.config.last_lif,\n",
    "#         temporal_filter  =  wandb.config.temporal_filter,\n",
    "#         initial_pooling  =  wandb.config.initial_pooling,\n",
    "#         temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "#                         ) \n",
    "#     # sigmoid와 BN이 있어야 잘된다.\n",
    "#     # average pooling\n",
    "#     # 이 낫다. \n",
    "    \n",
    "#     # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# # sweep_id = '6pj3lh8j'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
