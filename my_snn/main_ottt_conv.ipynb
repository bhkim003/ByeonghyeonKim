{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "from spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from spikingjelly.datasets import split_to_train_test_set\n",
    "from spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "# import torchneuromorphic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import PIL\n",
    "import time\n",
    "from pathlib import Path\n",
    "from PIL.Image import Image\n",
    "import math\n",
    "import torch.distributed as dist\n",
    "import collections\n",
    "from spikingjelly.activation_based import neuron, layer, functional\n",
    "from spikingjelly_codes.reference_codes import spiking_vggws_ottt as vggmodel\n",
    "from torchtoolbox.transform import Cutout\n",
    "from copy import deepcopy\n",
    "# /home/bhkim003/anaconda3/envs/aedat2_ottt/lib/python3.8/site-packages/spikingjelly-0.0.0.0.15-py3.8.egg/spikingjelly/activation_based/model\n",
    "import inspect\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "                    synapse_conv_trace_const1 = 1,\n",
    "                    synapse_conv_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "                    synapse_fc_trace_const1 = 1,\n",
    "                    synapse_fc_trace_const2 = 0.6,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    weight_count_print = False, # True # False\n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    verbose_interval = 100, #숫자 크게 하면 꺼짐\n",
    "                    validation_interval = 10, #숫자 크게 하면 꺼짐\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    gradient_verbose = False,\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = True,\n",
    "\n",
    "                    nda_net = False,\n",
    "                    \n",
    "                    domain_il_epoch = 0, # over 0, then domain il mode on\n",
    "\n",
    "                    dvs_clipping = True, \n",
    "                    dvs_duration = 1000000,\n",
    "\n",
    "                    OTTT_sWS_on = True, # True # False\n",
    "                  ):\n",
    "    \n",
    "    if OTTT_sWS_on == True:\n",
    "        assert BPTT_on == False and tdBN_on == False and convTrue_fcFalse == True\n",
    "\n",
    "    # 함수 내 모든 로컬 변수 저장\n",
    "    hyperparameters = locals()\n",
    "    hyperparameters['current epoch'] = 0\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    torch.manual_seed(my_seed)\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME,\n",
    "            dvs_clipping,\n",
    "            dvs_duration)\n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "    ###########################################################################################################################################\n",
    "\n",
    "    \n",
    "    ## parameter number calculator (안 중요함) ##################################################################################################################\n",
    "    params_num = 0\n",
    "    img_size = IMAGE_SIZE \n",
    "    bias_param = 1 # 1 or 0\n",
    "    classifier_making = False\n",
    "    if (convTrue_fcFalse == True):\n",
    "        past_kernel = synapse_conv_in_channels\n",
    "        for kernel in cfg:\n",
    "            if (classifier_making == False):\n",
    "                if (type(kernel) == list):\n",
    "                    for residual_kernel in kernel:\n",
    "                        if (residual_kernel >= 10000 and residual_kernel < 20000): # separable\n",
    "                            residual_kernel -= 10000\n",
    "                            params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                            params_num += (1**2 * past_kernel + bias_param) * residual_kernel\n",
    "                            past_kernel = residual_kernel  \n",
    "                        elif (residual_kernel >= 20000 and residual_kernel < 30000): # depthwise\n",
    "                            residual_kernel -= 20000\n",
    "                            # 'past_kernel' should be same with 'kernel'\n",
    "                            params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                            past_kernel = residual_kernel  \n",
    "                        else:\n",
    "                            params_num += residual_kernel * ((synapse_conv_kernel_size**2) * past_kernel + bias_param)\n",
    "                            past_kernel = residual_kernel\n",
    "                elif (kernel == 'P' or kernel == 'M'):\n",
    "                    img_size = img_size // 2\n",
    "                elif (kernel == 'D'):\n",
    "                    img_size = 1\n",
    "                elif (kernel == 'L'):\n",
    "                    classifier_making = True\n",
    "                    past_kernel = past_kernel * (img_size**2)\n",
    "                else:\n",
    "                    if (kernel >= 10000 and kernel < 20000): # separable\n",
    "                        kernel -= 10000\n",
    "                        params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                        params_num += (1**2 * past_kernel + bias_param) * kernel\n",
    "                        past_kernel = kernel  \n",
    "                    elif (kernel >= 20000 and kernel < 30000): # depthwise\n",
    "                        kernel -= 20000\n",
    "                        # 'past_kernel' should be same with 'kernel'\n",
    "                        params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                        past_kernel = kernel  \n",
    "                    else:\n",
    "                        params_num += kernel * (synapse_conv_kernel_size**2 * past_kernel + bias_param)\n",
    "                        past_kernel = kernel    \n",
    "            else: # classifier making\n",
    "                params_num += (past_kernel + bias_param) * kernel\n",
    "                past_kernel = kernel\n",
    "        \n",
    "        \n",
    "        if classifier_making == False:\n",
    "            past_kernel = past_kernel*img_size*img_size\n",
    "\n",
    "        params_num += (past_kernel + bias_param) * synapse_fc_out_features\n",
    "    else:\n",
    "        past_in_channel = synapse_conv_in_channels*img_size*img_size\n",
    "        for in_channel in cfg:\n",
    "            if (type(in_channel) == list):\n",
    "                for residual_in_channel in in_channel:\n",
    "                    params_num += (past_in_channel + bias_param) * residual_in_channel\n",
    "                    past_in_channel = residual_in_channel\n",
    "            # elif (in_channel == 'M'): #it's a holy FC layer!\n",
    "            #     img_size = img_size // 2\n",
    "            else:\n",
    "                print('past_in_channel', past_in_channel)\n",
    "                print('bias_param', bias_param)\n",
    "                print('in_channel', in_channel)\n",
    "                params_num += (past_in_channel + bias_param) * in_channel\n",
    "                past_in_channel = in_channel\n",
    "        params_num += (past_in_channel + bias_param) * synapse_fc_out_features\n",
    "    ###########################################################################################################################################\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if pre_trained == False:\n",
    "        if (convTrue_fcFalse == False):\n",
    "            net = MY_SNN_FC(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                     synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     tdBN_on,\n",
    "                     BN_on, TIME,\n",
    "                     surrogate,\n",
    "                     BPTT_on).to(device)\n",
    "        else:\n",
    "            net = MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                     synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                     synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                     synapse_conv_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     synapse_fc_out_features, synapse_fc_trace_const1, synapse_fc_trace_const2,\n",
    "                     tdBN_on,\n",
    "                     BN_on, TIME,\n",
    "                     surrogate,\n",
    "                     BPTT_on,\n",
    "                     OTTT_sWS_on).to(device)\n",
    "        \n",
    "        if (nda_net == True):\n",
    "            net = VGG(cfg = cfg, num_classes=10, batch_norm = tdBN_on, in_c = synapse_conv_in_channels, \n",
    "                      lif_layer_v_threshold=lif_layer_v_threshold, lif_layer_v_decay=lif_layer_v_decay, lif_layer_sg_width=lif_layer_sg_width)\n",
    "            net.T = TIME\n",
    "        net = torch.nn.DataParallel(net)\n",
    "    else:\n",
    "        net = torch.load(pre_trained_path)\n",
    "\n",
    "    net = vggmodel.ottt_spiking_vggws(num_classes=10, spiking_neuron=neuron.OTTTLIFNode)\n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)        \n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## param num and memory estimation except BN with MY own calculation some lines above ##########################################\n",
    "    real_param_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    if (weight_count_print == True):\n",
    "        for name, param in net.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f'Layer: {name} | Number of parameters: {param.numel()}')\n",
    "    # Batch norm 있으면 아래 두 개 서로 다를 수 있음.\n",
    "    # assert real_param_num == params_num, f'parameter number is not same. real_param_num: {real_param_num}, params_num: {params_num}'    \n",
    "    print('='*50)\n",
    "    print(f\"My Num of PARAMS: {params_num:,}, system's param_num : {real_param_num:,}\")\n",
    "    memory = params_num / 8 / 1024 / 1024 # MB\n",
    "    precision = 32\n",
    "    memory = memory * precision \n",
    "    print(f\"Memory: {memory:.2f}MiB at {precision}-bit\")\n",
    "    print('='*50)\n",
    "    ##############################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    if (OTTT_sWS_on == True):\n",
    "        # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=100)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    val_acc = 0\n",
    "    val_acc_now = 0\n",
    "    elapsed_time_val = 0\n",
    "    iter_acc_array = np.array([])\n",
    "    tr_acc_array = np.array([])\n",
    "    val_acc_now_array = np.array([])\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        print('EPOCH', epoch)\n",
    "        epoch_start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # if (domain_il_epoch>0 and which_data == 'PMNIST'):\n",
    "        #     k = epoch // domain_il_epoch\n",
    "        #     xtrain=data[k]['train']['x']\n",
    "        #     ytrain=data[k]['train']['y']\n",
    "        #     xtest =data[k]['test']['x']\n",
    "        #     ytest =data[k]['test']['y']\n",
    "\n",
    "        \n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        if (ddp_on == True):\n",
    "            if torch.distributed.get_rank() == 0:   \n",
    "                iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        else:\n",
    "            iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "        \n",
    "        #### validation_interval이 batch size보다 작을 시 validation_interval을 batch size로 맞춰줌#############\n",
    "        validation_interval2 = validation_interval\n",
    "        if (validation_interval > len(iterator)):\n",
    "            validation_interval2 = len(iterator)\n",
    "        ##################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            iter_one_train_time_start = time.time()\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                # print('x_len',x_len)\n",
    "                # mask = padded_sequence_mask(x_len)\n",
    "                # max_time_step = x_len.max()\n",
    "                # min_time_step = x_len.min()\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "                    \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "                \n",
    "            # # DVS에서 time duration으로 잘랐을 때는 timestep 맞춰주자 --> data 가져올 때, 그 함수 안에서 처리함.\n",
    "            # if (dvs_duration > 0): \n",
    "            #     # inputs.size(1)를 TIME으로 맞추기\n",
    "            #     T, *spatial_dims = inputs.shape\n",
    "            #     if T > TIME:\n",
    "            #         inputs = inputs[:TIME]\n",
    "            #     else:\n",
    "            #         inputs = torch.cat([inputs, torch.zeros(TIME - T, *spatial_dims)], dim=0)\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "                \n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH)\n",
    "            # ######################################################################################################\n",
    "\n",
    "\n",
    "            ## device로 보내주기 ######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "\n",
    "\n",
    "            # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "            # inputs: [Batch, Time, Channel, Height, Width] \n",
    "            #################################################################################################\n",
    "\n",
    "            y_all = []\n",
    "            if (inputs.size(0) == BATCH):\n",
    "                for t in range(TIME):\n",
    "                    ### input --> net --> output #####################################################\n",
    "                    outputs = net(inputs[:,t])\n",
    "                    ##################################################################################\n",
    "\n",
    "\n",
    "                    #### batch 어긋남 방지 ###############################################\n",
    "                    batch = BATCH \n",
    "                    if labels.size(0) != BATCH: \n",
    "                        batch = labels.size(0)\n",
    "                    #######################################################################\n",
    "                    \n",
    "\n",
    "\n",
    "                    ## loss, backward ##########################################\n",
    "                    loss = criterion(outputs[0:batch,:], labels)\n",
    "                    loss.backward()\n",
    "                    y_all.append(outputs.detach())\n",
    "                    ############################################################\n",
    "          \n",
    "                outputs = torch.stack(y_all, dim=0)\n",
    "                outputs = outputs.sum(axis=0)\n",
    "\n",
    "                ####### training accruacy save for print ###############################\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total = labels.size(0)\n",
    "                correct = (predicted == labels).sum().item()\n",
    "                tr_total += total\n",
    "                tr_correct += correct\n",
    "                iter_acc = correct / total\n",
    "                if i % verbose_interval == verbose_interval-1:\n",
    "                    print(f'{epoch}-{i} training acc: {100 * iter_acc:.2f}%, lr={[f\"{lr}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}, val_acc: {100 * val_acc_now:.2f}%')\n",
    "                iter_acc_string = f'{epoch}-{i}/{len(train_loader)} iter_acc: {100 * iter_acc:.2f}%, lr={[f\"{lr}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "                ################################################################\n",
    "                    \n",
    "                ### gradinet verbose ##########################################\n",
    "                if (gradient_verbose == True):\n",
    "                    if (i % verbose_interval == verbose_interval-1):\n",
    "                        print('\\n\\nepoch', epoch, 'iter', i)\n",
    "                        for name, param in net.named_parameters():\n",
    "                            if param.requires_grad:\n",
    "                                print('\\n\\n\\n\\n' , name, param.grad)\n",
    "                ################################################################\n",
    "                \n",
    "\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                # print(\"Epoch: {}, Iter: {}, Loss: {}\".format(epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "                iter_one_train_time_end = time.time()\n",
    "                elapsed_time = iter_one_train_time_end - iter_one_train_time_start  # 실행 시간 계산\n",
    "\n",
    "                if (i % verbose_interval == verbose_interval-1):\n",
    "                    print(f\"iter_one_train_time: {elapsed_time} seconds, last one_val_time: {elapsed_time_val} seconds\\n\")\n",
    "\n",
    "                ##### validation ##################################################################################################################################\n",
    "            if i % validation_interval2 == validation_interval2-1:\n",
    "                iter_one_val_time_start = time.time()\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data in test_loader:\n",
    "                        ## data loading & semi-pre-processing ##########################################################\n",
    "                        if len(data) == 2:\n",
    "                            inputs, labels = data\n",
    "                            # 처리 로직 작성\n",
    "                        elif len(data) == 3:\n",
    "                            inputs, labels, x_len = data\n",
    "                            # print('x_len',x_len)\n",
    "                            # mask = padded_sequence_mask(x_len)\n",
    "                            # max_time_step = x_len.max()\n",
    "                            # min_time_step = x_len.min()\n",
    "                            # B, T, *spatial_dims = inputs.shape\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "\n",
    "                        if labels.size(0) == BATCH: \n",
    "                            y_all = []\n",
    "                            for t in range(TIME):\n",
    "                                y_t = net(inputs.permute(1, 0, 2, 3, 4)[:,t])\n",
    "                                y_all.append(y_t.detach())\n",
    "                            y_all = torch.stack(y_all, dim=0)\n",
    "                            outputs = y_all.sum(axis=0)\n",
    "\n",
    "                            _, predicted = torch.max(outputs.data, 1)\n",
    "                            total += labels.size(0)\n",
    "                            batch = BATCH \n",
    "                            if labels.size(0) != BATCH: \n",
    "                                batch = labels.size(0)\n",
    "                            correct += (predicted == labels).sum().item()\n",
    "                            val_loss = criterion(outputs[0:batch,:], labels)\n",
    "\n",
    "                    val_acc_now = correct / total\n",
    "                    # print(f'{epoch}-{i} validation acc: {100 * val_acc_now:.2f}%, lr={[f\"{lr:.10f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}')\n",
    "\n",
    "                iter_one_val_time_end = time.time()\n",
    "                elapsed_time_val = iter_one_val_time_end - iter_one_val_time_start  # 실행 시간 계산\n",
    "                # print(f\"iter_one_val_time: {elapsed_time_val} seconds\")\n",
    "\n",
    "                # network save\n",
    "                if val_acc < val_acc_now:\n",
    "                    val_acc = val_acc_now\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "                    torch.save(net, f\"net_save/save_now_net_{unique_name}.pth\")\n",
    "                    # torch.save(net.module.state_dict(), f\"net_save/save_now_net_weights2_{unique_name}.pth\")\n",
    "                    # torch.save(net.module, f\"net_save/save_now_net2_{unique_name}.pth\")\n",
    "            ####################################################################################################################################################\n",
    "            iterator.set_description(f\"iter_acc: {iter_acc_string}, iter_loss: {loss}, val_acc: {100 * val_acc_now:.2f}%\")  \n",
    "\n",
    "            iter_acc_array = np.append(iter_acc_array, iter_acc)\n",
    "            tr_acc_array = np.append(tr_acc_array, tr_acc)\n",
    "            val_acc_now_array = np.append(val_acc_now_array, val_acc_now)\n",
    "            base_name = f'{current_time}'\n",
    "            iter_acc_file_name_time = f'result_save/{base_name}_iter_acc_array_{unique_name}.npy'\n",
    "            tr_acc_file_name_time = f'result_save/{base_name}_tr_acc_array_{unique_name}.npy'\n",
    "            val_acc_file_name_time = f'result_save/{base_name}_val_acc_now_array_{unique_name}.npy'\n",
    "            hyperparameters_file_name_time = f'result_save/{base_name}_hyperparameters_{unique_name}.json'\n",
    "\n",
    "            hyperparameters['current epoch'] = epoch\n",
    "\n",
    "            # # 덮어쓰기 하기 싫으면 주석 풀어서 사용 (시간마다 새로 쓰기)\n",
    "            # np.save(iter_acc_file_name_time, iter_acc_array)\n",
    "            # np.save(tr_acc_file_name_time, iter_acc_array)\n",
    "            # np.save(val_acc_file_name_time, val_acc_now_array)\n",
    "            # with open(hyperparameters_file_name_time, 'w') as f:\n",
    "            #     json.dump(hyperparameters, f, indent=4)\n",
    "\n",
    "            np.save(f'result_save/iter_acc_array_{unique_name}.npy', iter_acc_array)\n",
    "            np.save(f'result_save/tr_acc_array_{unique_name}.npy', tr_acc_array)\n",
    "            np.save(f'result_save/val_acc_now_array_{unique_name}.npy', val_acc_now_array)\n",
    "            with open(f'result_save/hyperparameters_{unique_name}.json', 'w') as f:\n",
    "                json.dump(hyperparameters, f, indent=4)\n",
    "    ###### ITERATION END ##########################################################################################################\n",
    "                \n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "        # 실행 시간 계산\n",
    "        epoch_time_end = time.time()\n",
    "        print(f\"epoch_time: {epoch_time_end - epoch_start_time} seconds\\n\") \n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cfg: [64, 128, 'A', 256, 256, 'A', 512, 512, 'A', 512, 512]\n",
      "weight_standardization: True\n",
      "num_classes: 10\n",
      "init_weights: True\n",
      "spiking_neuron: <class 'spikingjelly.activation_based.neuron.OTTTLIFNode'>\n",
      "light_classifier: True\n",
      "drop_rate: 0.0\n",
      "Contents of **kwargs: {}\n",
      "self.fc_hw: 1\n",
      "yes ws\n",
      "avepool\n",
      "avepool\n",
      "avepool\n",
      "OTTTSpikingVGG(\n",
      "  (features): OTTTSequential(\n",
      "    (0): WSConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), step_mode=s)\n",
      "    (1): SIZE_CHECK_post_conv()\n",
      "    (2): OTTTLIFNode(\n",
      "      v_threshold=1.0, v_reset=None, detach_reset=True, step_mode=s, backend=torch, tau=2.0\n",
      "      (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "    )\n",
      "    (3): SIZE_CHECK_post_neuron()\n",
      "    (4): Scale()\n",
      "    (5): WSConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), step_mode=s)\n",
      "    (6): SIZE_CHECK_post_conv()\n",
      "    (7): OTTTLIFNode(\n",
      "      v_threshold=1.0, v_reset=None, detach_reset=True, step_mode=s, backend=torch, tau=2.0\n",
      "      (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "    )\n",
      "    (8): SIZE_CHECK_post_neuron()\n",
      "    (9): Scale()\n",
      "    (10): AvgPool2d(kernel_size=2, stride=2, padding=0, step_mode=s)\n",
      "    (11): WSConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), step_mode=s)\n",
      "    (12): SIZE_CHECK_post_conv()\n",
      "    (13): OTTTLIFNode(\n",
      "      v_threshold=1.0, v_reset=None, detach_reset=True, step_mode=s, backend=torch, tau=2.0\n",
      "      (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "    )\n",
      "    (14): SIZE_CHECK_post_neuron()\n",
      "    (15): Scale()\n",
      "    (16): WSConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), step_mode=s)\n",
      "    (17): SIZE_CHECK_post_conv()\n",
      "    (18): OTTTLIFNode(\n",
      "      v_threshold=1.0, v_reset=None, detach_reset=True, step_mode=s, backend=torch, tau=2.0\n",
      "      (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "    )\n",
      "    (19): SIZE_CHECK_post_neuron()\n",
      "    (20): Scale()\n",
      "    (21): AvgPool2d(kernel_size=2, stride=2, padding=0, step_mode=s)\n",
      "    (22): WSConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), step_mode=s)\n",
      "    (23): SIZE_CHECK_post_conv()\n",
      "    (24): OTTTLIFNode(\n",
      "      v_threshold=1.0, v_reset=None, detach_reset=True, step_mode=s, backend=torch, tau=2.0\n",
      "      (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "    )\n",
      "    (25): SIZE_CHECK_post_neuron()\n",
      "    (26): Scale()\n",
      "    (27): WSConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), step_mode=s)\n",
      "    (28): SIZE_CHECK_post_conv()\n",
      "    (29): OTTTLIFNode(\n",
      "      v_threshold=1.0, v_reset=None, detach_reset=True, step_mode=s, backend=torch, tau=2.0\n",
      "      (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "    )\n",
      "    (30): SIZE_CHECK_post_neuron()\n",
      "    (31): Scale()\n",
      "    (32): AvgPool2d(kernel_size=2, stride=2, padding=0, step_mode=s)\n",
      "    (33): WSConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), step_mode=s)\n",
      "    (34): SIZE_CHECK_post_conv()\n",
      "    (35): OTTTLIFNode(\n",
      "      v_threshold=1.0, v_reset=None, detach_reset=True, step_mode=s, backend=torch, tau=2.0\n",
      "      (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "    )\n",
      "    (36): SIZE_CHECK_post_neuron()\n",
      "    (37): Scale()\n",
      "    (38): WSConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), step_mode=s)\n",
      "    (39): SIZE_CHECK_post_conv()\n",
      "    (40): OTTTLIFNode(\n",
      "      v_threshold=1.0, v_reset=None, detach_reset=True, step_mode=s, backend=torch, tau=2.0\n",
      "      (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "    )\n",
      "    (41): SIZE_CHECK_post_neuron()\n",
      "    (42): Scale()\n",
      "  )\n",
      "  (classifier): OTTTSequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1), step_mode=s)\n",
      "    (1): Flatten(start_dim=1, end_dim=-1, step_mode=s)\n",
      "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 9,225,610, system's param_num : 18,446,090\n",
      "Memory: 35.19MiB at 32-bit\n",
      "==================================================\n",
      "EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-389/391 iter_acc: 20.31%, lr=['0.1'], iter_loss: 0.3348345160484314, val_acc: 20.90%: 100%|██████████| 391/391 [02:16<00:00,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_time: 136.5183548927307 seconds\n",
      "\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "iter_acc: 1-327/391 iter_acc: 18.75%, lr=['0.09999725846827562'], iter_loss: 0.3264390826225281, val_acc: 20.90%:  84%|████████▍ | 328/391 [01:44<00:19,  3.15it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nda 0.25 # ottt 0.5\u001b[39;00m\n\u001b[1;32m      5\u001b[0m unique_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain_ottt_conv\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m## 이거 설정하면 새로운 경로에 모두 save\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmy_snn_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2,3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43munique_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43munique_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmy_seed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43mTIME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mBATCH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m#pmnist는 28로 해야 됨. 나머지는 바꿔도 돌아는 감.\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# DVS_CIFAR10 할거면 time 10으로 해라\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwhich_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCIFAR10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;43;03m# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;43;03m# 'DVS_GESTURE','DVS_CIFAR10_2','NMNIST','N_CALTECH101','n_tidigits','heidelberg'\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# CLASS_NUM = 10,\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# YOU NEED TO CHANGE THIS\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrate_coding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_v_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_v_decay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_v_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 10000이상으로 하면 NDA LIF 씀. #nda 0.5  #ottt 1.0\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_v_reset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_sg_width\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# # surrogate sigmoid 쓸 때는 의미없음\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_kernel_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_stride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_padding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_trace_const1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_trace_const2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# lif_layer_v_decay\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# synapse_fc_out_features = CLASS_NUM,\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_fc_trace_const1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_fc_trace_const2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# lif_layer_v_decay\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpre_trained\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconvTrue_fcFalse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64],\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#ottt\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], #ottt\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], # ottt \u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\u001b[39;49;00m\n\u001b[1;32m     53\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [20001,10001], # depthwise, separable\u001b[39;49;00m\n\u001b[1;32m     55\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64,20064,10001], # vanilla conv, depthwise, separable\u001b[39;49;00m\n\u001b[1;32m     56\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\u001b[39;49;00m\n\u001b[1;32m     57\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [], \u001b[39;49;00m\n\u001b[1;32m     58\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnet_print\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweight_count_print\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpre_trained_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnet_save/save_now_net_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43munique_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# default 0.001  # ottt 0.1 0.00001 # nda 0.001 \u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepoch_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                \u001b[49m\u001b[43mverbose_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m999999999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#숫자 크게 하면 꺼짐 #걍 중간중간 iter에서 끊어서 출력\u001b[39;49;00m\n\u001b[1;32m     66\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m999999999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#숫자 크게 하면 에포크 마지막 iter 때 val 함\u001b[39;49;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtdBN_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m                \u001b[49m\u001b[43mBN_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     70\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                \u001b[49m\u001b[43msurrogate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigmoid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 'rectangle' 'sigmoid' 'rough_rectangle'\u001b[39;49;00m\n\u001b[1;32m     72\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgradient_verbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True # False  # weight gradient 각 layer마다 띄워줌\u001b[39;49;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[43m                \u001b[49m\u001b[43mBPTT_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\u001b[39;49;00m\n\u001b[1;32m     76\u001b[0m \u001b[43m                \u001b[49m\u001b[43moptimizer_what\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSGD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 'SGD' 'Adam', 'RMSprop'\u001b[39;49;00m\n\u001b[1;32m     77\u001b[0m \u001b[43m                \u001b[49m\u001b[43mscheduler_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCosineAnnealingLR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\u001b[39;49;00m\n\u001b[1;32m     78\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m                \u001b[49m\u001b[43mddp_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnda_net\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdomain_il_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# over 0, then domain il mode on # pmnist 쓸거면 HLOP 코드보고 더 디벨롭하셈. 지금 개발 hold함.\u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdvs_clipping\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# dvs zero&one  # gesture, cifar-dvs2, nmnist, ncaltech101\u001b[39;49;00m\n\u001b[1;32m     86\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdvs_duration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\u001b[39;49;00m\n\u001b[1;32m     87\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m#있는 데이터들 #gesture 1000000 #nmnist 10000\u001b[39;49;00m\n\u001b[1;32m     88\u001b[0m \n\u001b[1;32m     89\u001b[0m \u001b[43m                \u001b[49m\u001b[43mOTTT_sWS_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False # BPTT끄고, CONV에만 적용됨.\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 388\u001b[0m, in \u001b[0;36mmy_snn_system\u001b[0;34m(devices, unique_name, my_seed, TIME, BATCH, IMAGE_SIZE, which_data, data_path, rate_coding, lif_layer_v_init, lif_layer_v_decay, lif_layer_v_threshold, lif_layer_v_reset, lif_layer_sg_width, synapse_conv_kernel_size, synapse_conv_stride, synapse_conv_padding, synapse_conv_trace_const1, synapse_conv_trace_const2, synapse_fc_trace_const1, synapse_fc_trace_const2, pre_trained, convTrue_fcFalse, cfg, net_print, weight_count_print, pre_trained_path, learning_rate, epoch_num, verbose_interval, validation_interval, tdBN_on, BN_on, surrogate, gradient_verbose, BPTT_on, optimizer_what, scheduler_name, ddp_on, nda_net, domain_il_epoch, dvs_clipping, dvs_duration, OTTT_sWS_on)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m BATCH):\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(TIME):\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;66;03m### input --> net --> output #####################################################\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;66;03m##################################################################################\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \n\u001b[1;32m    392\u001b[0m         \u001b[38;5;66;03m#### batch 어긋남 방지 ###############################################\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         batch \u001b[38;5;241m=\u001b[39m BATCH \n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2_ottt/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/spikingjelly_codes/reference_codes/spiking_vggws_ottt.py:88\u001b[0m, in \u001b[0;36mOTTTSpikingVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 88\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# print('classifier output size:', x.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2_ottt/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2_ottt/lib/python3.8/site-packages/spikingjelly/activation_based/layer.py:2693\u001b[0m, in \u001b[0;36mOTTTSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# e.g., Dropout, AvgPool, etc.\u001b[39;00m\n\u001b[1;32m   2692\u001b[0m             module \u001b[38;5;241m=\u001b[39m SpikeTraceOp(module)\n\u001b[0;32m-> 2693\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2695\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2_ottt/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2_ottt/lib/python3.8/site-packages/spikingjelly/activation_based/layer.py:2622\u001b[0m, in \u001b[0;36mGradwithTrace.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   2619\u001b[0m spike, trace \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   2621\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 2622\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspike\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   2624\u001b[0m in_for_grad \u001b[38;5;241m=\u001b[39m ReplaceforGrad\u001b[38;5;241m.\u001b[39mapply(spike, trace)\n\u001b[1;32m   2625\u001b[0m out_for_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(in_for_grad)\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2_ottt/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2_ottt/lib/python3.8/site-packages/spikingjelly/activation_based/layer.py:2772\u001b[0m, in \u001b[0;36mWSConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   2769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor):\n\u001b[1;32m   2770\u001b[0m     \u001b[38;5;66;03m# print(self.step_mode)\u001b[39;00m\n\u001b[1;32m   2771\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 2772\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#이걸로가네\u001b[39;00m\n\u001b[1;32m   2774\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   2775\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2_ottt/lib/python3.8/site-packages/spikingjelly/activation_based/layer.py:2767\u001b[0m, in \u001b[0;36mWSConv2d._forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   2763\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor):\n\u001b[1;32m   2764\u001b[0m     \u001b[38;5;66;03m# ws_weight = self.get_weight()\u001b[39;00m\n\u001b[1;32m   2765\u001b[0m     \u001b[38;5;66;03m# print('\\n\\n같냐',torch.eq(ws_weight,self.weight_past))\u001b[39;00m\n\u001b[1;32m   2766\u001b[0m     \u001b[38;5;66;03m# self.weight_past = torch.nn.Parameter(ws_weight)\u001b[39;00m\n\u001b[0;32m-> 2767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(x, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2_ottt/lib/python3.8/site-packages/spikingjelly/activation_based/layer.py:2758\u001b[0m, in \u001b[0;36mWSConv2d.get_weight\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2756\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, axis\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2757\u001b[0m var \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, axis\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2758\u001b[0m weight \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfan_in\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgain \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2760\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgain\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### my_snn control board ########################\n",
    "decay = 0.5 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "unique_name = 'main_ottt_conv' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "my_snn_system(  devices = \"2,3\",\n",
    "                unique_name = unique_name,\n",
    "                my_seed = 42,\n",
    "                TIME = 6 , # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "                BATCH = 128, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 32, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "                #pmnist는 28로 해야 됨. 나머지는 바꿔도 돌아는 감.\n",
    "\n",
    "                # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = 'CIFAR10',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# 'DVS_GESTURE','DVS_CIFAR10_2','NMNIST','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 1.0,  # 10000이상으로 하면 NDA LIF 씀. #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "                lif_layer_sg_width = 1.0, # # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "                synapse_conv_trace_const1 = 1,\n",
    "                synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "                synapse_fc_trace_const1 = 1,\n",
    "                synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = True, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "                # cfg = [64],\n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], #ottt\n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], # ottt \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [], \n",
    "                \n",
    "                net_print = True, # True # False\n",
    "                weight_count_print = False, # True # False\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_{unique_name}.pth\",\n",
    "                learning_rate = 0.1, # default 0.001  # ottt 0.1 0.00001 # nda 0.001 \n",
    "                epoch_num = 300,\n",
    "                verbose_interval = 999999999, #숫자 크게 하면 꺼짐 #걍 중간중간 iter에서 끊어서 출력\n",
    "                validation_interval = 999999999, #숫자 크게 하면 에포크 마지막 iter 때 val 함\n",
    "\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'sigmoid', # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "                gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "                BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'CosineAnnealingLR', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False,   # True # False\n",
    "\n",
    "                nda_net = False,   # True # False\n",
    "\n",
    "                domain_il_epoch = 0, # over 0, then domain il mode on # pmnist 쓸거면 HLOP 코드보고 더 디벨롭하셈. 지금 개발 hold함.\n",
    "                \n",
    "                dvs_clipping = True, # dvs zero&one  # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                dvs_duration = 1000000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                #있는 데이터들 #gesture 1000000 #nmnist 10000\n",
    "\n",
    "                OTTT_sWS_on = True, # True # False # BPTT끄고, CONV에만 적용됨.\n",
    "                \n",
    "                ) \n",
    "# sigmoid와 BN이 있어야 잘된다.\n",
    "# average pooling이 낫다. \n",
    "\n",
    "# nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "def pad_array_to_match_length(array1, array2):\n",
    "    if len(array1) > len(array2):\n",
    "        padded_array2 = np.pad(array2, (0, len(array1) - len(array2)), 'constant')\n",
    "        return array1, padded_array2\n",
    "    elif len(array2) > len(array1):\n",
    "        padded_array1 = np.pad(array1, (0, len(array2) - len(array1)), 'constant')\n",
    "        return padded_array1, array2\n",
    "    else:\n",
    "        return array1, array2\n",
    "def load_hyperparameters(filename=f'result_save/hyperparameters_{unique_name}.json'):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "current_time = '20240628_110116'\n",
    "base_name = f'{current_time}'\n",
    "iter_acc_file_name = f'result_save/{base_name}_iter_acc_array_{unique_name}.npy'\n",
    "val_acc_file_name = f'result_save/{base_name}_val_acc_now_array_{unique_name}.npy'\n",
    "hyperparameters_file_name = f'result_save/{base_name}_hyperparameters_{unique_name}.json'\n",
    "\n",
    "### if you want to just see most recent train and val acc###########################\n",
    "iter_acc_file_name = f'result_save/iter_acc_array_{unique_name}.npy'\n",
    "tr_acc_file_name = f'result_save/tr_acc_array_{unique_name}.npy'\n",
    "val_acc_file_name = f'result_save/val_acc_now_array_{unique_name}.npy'\n",
    "hyperparameters_file_name = f'result_save/hyperparameters_{unique_name}.json'\n",
    "\n",
    "loaded_iter_acc_array = np.load(iter_acc_file_name)*100\n",
    "loaded_tr_acc_array = np.load(tr_acc_file_name)*100\n",
    "loaded_val_acc_array = np.load(val_acc_file_name)*100\n",
    "hyperparameters = load_hyperparameters(hyperparameters_file_name)\n",
    "\n",
    "loaded_iter_acc_array, loaded_val_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_val_acc_array)\n",
    "loaded_iter_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_tr_acc_array)\n",
    "loaded_val_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_val_acc_array, loaded_tr_acc_array)\n",
    "\n",
    "top_iter_acc = np.max(loaded_iter_acc_array)\n",
    "top_tr_acc = np.max(loaded_tr_acc_array)\n",
    "top_val_acc = np.max(loaded_val_acc_array)\n",
    "\n",
    "which_data = hyperparameters['which_data']\n",
    "BPTT_on = hyperparameters['BPTT_on']\n",
    "current_epoch = hyperparameters['current epoch']\n",
    "surrogate = hyperparameters['surrogate']\n",
    "cfg = hyperparameters['cfg']\n",
    "tdBN_on = hyperparameters['tdBN_on']\n",
    "BN_on = hyperparameters['BN_on']\n",
    "\n",
    "\n",
    "iterations = np.arange(len(loaded_iter_acc_array))\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(iterations, loaded_iter_acc_array, label='Iter Accuracy', color='g', alpha=0.2)\n",
    "plt.plot(iterations, loaded_tr_acc_array, label='Training Accuracy', color='b')\n",
    "plt.plot(iterations, loaded_val_acc_array, label='Validation Accuracy', color='r')\n",
    "\n",
    "# # 텍스트 추가\n",
    "# plt.text(0.05, 0.95, f'Top Training Accuracy: {100*top_iter_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='blue')\n",
    "# plt.text(0.05, 0.90, f'Top Validation Accuracy: {100*top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='red')\n",
    "# 텍스트 추가\n",
    "plt.text(0.5, 0.10, f'Top Training Accuracy: {top_tr_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='blue')\n",
    "plt.text(0.5, 0.05, f'Top Validation Accuracy: {top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='red')\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy [%]')\n",
    "\n",
    "# 그래프 제목에 하이퍼파라미터 정보 추가\n",
    "title = f'Training and Validation Accuracy over Iterations\\n\\nData: {which_data}, BPTT: {\"On\" if BPTT_on else \"Off\"}, Current Epoch: {current_epoch}, Surrogate: {surrogate},\\nCFG: {cfg}, tdBN: {\"On\" if tdBN_on else \"Off\"}, BN: {\"On\" if BN_on else \"Off\"}'\n",
    "\n",
    "plt.title(title)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim(0)  # x축을 0부터 시작\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
