{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34440/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    lif_layer_sg_width2 = None,\n",
    "                    lif_layer_v_threshold2 = None,\n",
    "                    learning_rate2 = None,\n",
    "                    init_scaling = None,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp,\n",
    "                    ANPI_MODE=False,\n",
    "                    lif_layer_sg_width2=lif_layer_sg_width2,\n",
    "                    lif_layer_v_threshold2=lif_layer_v_threshold2,\n",
    "                    init_scaling=init_scaling).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                # lr = group['lr']\n",
    "\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        lr = learning_rate\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        lr = learning_rate2\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        lr = 1.0\n",
    "\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 8, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[0,0],[0,0],[0,0]], \n",
    "#                 lif_layer_sg_width2 = 4.0,\n",
    "#                 lif_layer_v_threshold2 = 8,\n",
    "#                 learning_rate2 = 8,\n",
    "#                 init_scaling = [1/2,1/2,1/2],\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tslebin1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_125820-tslebin1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tslebin1' target=\"_blank\">frosty-sweep-16</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tslebin1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tslebin1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251214_125827_338', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 64, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 32, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 16, 'lif_layer_v_threshold2': 32, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 32, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 16, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=16, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 2 self.abs_max_out: 6.0\n",
      "lif layer 2 self.abs_max_v: 6.0\n",
      "fc layer 1 self.abs_max_out: 55.0\n",
      "lif layer 1 self.abs_max_v: 81.5\n",
      "fc layer 2 self.abs_max_out: 23.0\n",
      "lif layer 2 self.abs_max_v: 25.0\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 129.5\n",
      "fc layer 2 self.abs_max_out: 25.0\n",
      "lif layer 2 self.abs_max_v: 31.5\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 147.0\n",
      "fc layer 2 self.abs_max_out: 30.0\n",
      "lif layer 2 self.abs_max_v: 37.5\n",
      "fc layer 3 self.abs_max_out: 3.0\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 200.0\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 54.0\n",
      "fc layer 3 self.abs_max_out: 11.0\n",
      "fc layer 1 self.abs_max_out: 212.0\n",
      "lif layer 1 self.abs_max_v: 229.0\n",
      "fc layer 2 self.abs_max_out: 40.0\n",
      "lif layer 2 self.abs_max_v: 64.0\n",
      "fc layer 1 self.abs_max_out: 242.0\n",
      "lif layer 1 self.abs_max_v: 265.5\n",
      "fc layer 2 self.abs_max_out: 56.0\n",
      "lif layer 2 self.abs_max_v: 79.0\n",
      "fc layer 3 self.abs_max_out: 17.0\n",
      "lif layer 2 self.abs_max_v: 88.5\n",
      "fc layer 1 self.abs_max_out: 246.0\n",
      "fc layer 2 self.abs_max_out: 59.0\n",
      "fc layer 2 self.abs_max_out: 68.0\n",
      "lif layer 2 self.abs_max_v: 91.5\n",
      "fc layer 3 self.abs_max_out: 20.0\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "fc layer 1 self.abs_max_out: 312.0\n",
      "lif layer 1 self.abs_max_v: 312.0\n",
      "fc layer 2 self.abs_max_out: 77.0\n",
      "lif layer 2 self.abs_max_v: 110.5\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "fc layer 3 self.abs_max_out: 36.0\n",
      "fc layer 2 self.abs_max_out: 80.0\n",
      "lif layer 2 self.abs_max_v: 116.5\n",
      "fc layer 3 self.abs_max_out: 46.0\n",
      "fc layer 2 self.abs_max_out: 90.0\n",
      "fc layer 3 self.abs_max_out: 54.0\n",
      "fc layer 2 self.abs_max_out: 95.0\n",
      "fc layer 2 self.abs_max_out: 100.0\n",
      "fc layer 2 self.abs_max_out: 114.0\n",
      "lif layer 2 self.abs_max_v: 117.5\n",
      "lif layer 2 self.abs_max_v: 118.0\n",
      "lif layer 2 self.abs_max_v: 133.0\n",
      "lif layer 1 self.abs_max_v: 334.0\n",
      "lif layer 1 self.abs_max_v: 407.0\n",
      "fc layer 1 self.abs_max_out: 336.0\n",
      "fc layer 1 self.abs_max_out: 352.0\n",
      "fc layer 3 self.abs_max_out: 60.0\n",
      "lif layer 2 self.abs_max_v: 139.5\n",
      "lif layer 2 self.abs_max_v: 142.0\n",
      "fc layer 1 self.abs_max_out: 382.0\n",
      "fc layer 1 self.abs_max_out: 437.0\n",
      "lif layer 1 self.abs_max_v: 437.0\n",
      "fc layer 1 self.abs_max_out: 450.0\n",
      "lif layer 1 self.abs_max_v: 450.0\n",
      "lif layer 2 self.abs_max_v: 152.0\n",
      "fc layer 1 self.abs_max_out: 460.0\n",
      "lif layer 1 self.abs_max_v: 460.0\n",
      "lif layer 2 self.abs_max_v: 161.0\n",
      "lif layer 2 self.abs_max_v: 170.5\n",
      "fc layer 2 self.abs_max_out: 117.0\n",
      "fc layer 3 self.abs_max_out: 63.0\n",
      "lif layer 2 self.abs_max_v: 175.5\n",
      "lif layer 2 self.abs_max_v: 180.0\n",
      "lif layer 2 self.abs_max_v: 185.0\n",
      "lif layer 2 self.abs_max_v: 201.5\n",
      "fc layer 2 self.abs_max_out: 130.0\n",
      "fc layer 2 self.abs_max_out: 148.0\n",
      "fc layer 3 self.abs_max_out: 67.0\n",
      "lif layer 2 self.abs_max_v: 207.5\n",
      "fc layer 3 self.abs_max_out: 73.0\n",
      "lif layer 2 self.abs_max_v: 212.5\n",
      "lif layer 2 self.abs_max_v: 213.0\n",
      "fc layer 3 self.abs_max_out: 78.0\n",
      "lif layer 2 self.abs_max_v: 214.0\n",
      "lif layer 2 self.abs_max_v: 227.0\n",
      "fc layer 2 self.abs_max_out: 153.0\n",
      "fc layer 3 self.abs_max_out: 101.0\n",
      "lif layer 2 self.abs_max_v: 232.5\n",
      "lif layer 2 self.abs_max_v: 236.0\n",
      "fc layer 2 self.abs_max_out: 167.0\n",
      "fc layer 1 self.abs_max_out: 467.0\n",
      "lif layer 1 self.abs_max_v: 467.0\n",
      "fc layer 2 self.abs_max_out: 173.0\n",
      "lif layer 2 self.abs_max_v: 243.5\n",
      "lif layer 1 self.abs_max_v: 493.5\n",
      "fc layer 2 self.abs_max_out: 180.0\n",
      "fc layer 3 self.abs_max_out: 102.0\n",
      "fc layer 3 self.abs_max_out: 107.0\n",
      "fc layer 3 self.abs_max_out: 110.0\n",
      "fc layer 3 self.abs_max_out: 111.0\n",
      "fc layer 2 self.abs_max_out: 194.0\n",
      "fc layer 3 self.abs_max_out: 112.0\n",
      "lif layer 2 self.abs_max_v: 255.5\n",
      "lif layer 2 self.abs_max_v: 267.0\n",
      "fc layer 2 self.abs_max_out: 213.0\n",
      "lif layer 2 self.abs_max_v: 267.5\n",
      "lif layer 2 self.abs_max_v: 281.0\n",
      "fc layer 2 self.abs_max_out: 216.0\n",
      "lif layer 2 self.abs_max_v: 308.5\n",
      "fc layer 3 self.abs_max_out: 124.0\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 3 self.abs_max_out: 149.0\n",
      "fc layer 2 self.abs_max_out: 226.0\n",
      "fc layer 2 self.abs_max_out: 229.0\n",
      "fc layer 1 self.abs_max_out: 520.0\n",
      "lif layer 1 self.abs_max_v: 520.0\n",
      "fc layer 1 self.abs_max_out: 523.0\n",
      "lif layer 1 self.abs_max_v: 523.0\n",
      "lif layer 2 self.abs_max_v: 335.0\n",
      "fc layer 2 self.abs_max_out: 251.0\n",
      "lif layer 2 self.abs_max_v: 338.5\n",
      "fc layer 2 self.abs_max_out: 257.0\n",
      "lif layer 2 self.abs_max_v: 350.5\n",
      "lif layer 2 self.abs_max_v: 368.5\n",
      "lif layer 2 self.abs_max_v: 369.0\n",
      "lif layer 2 self.abs_max_v: 376.5\n",
      "lif layer 2 self.abs_max_v: 382.5\n",
      "lif layer 2 self.abs_max_v: 390.5\n",
      "fc layer 3 self.abs_max_out: 158.0\n",
      "fc layer 2 self.abs_max_out: 258.0\n",
      "fc layer 2 self.abs_max_out: 261.0\n",
      "fc layer 3 self.abs_max_out: 159.0\n",
      "lif layer 1 self.abs_max_v: 530.0\n",
      "lif layer 1 self.abs_max_v: 543.0\n",
      "lif layer 1 self.abs_max_v: 623.5\n",
      "lif layer 2 self.abs_max_v: 409.0\n",
      "fc layer 3 self.abs_max_out: 164.0\n",
      "fc layer 3 self.abs_max_out: 172.0\n",
      "fc layer 3 self.abs_max_out: 177.0\n",
      "fc layer 3 self.abs_max_out: 183.0\n",
      "fc layer 3 self.abs_max_out: 189.0\n",
      "lif layer 2 self.abs_max_v: 413.0\n",
      "fc layer 2 self.abs_max_out: 265.0\n",
      "fc layer 2 self.abs_max_out: 266.0\n",
      "fc layer 2 self.abs_max_out: 282.0\n",
      "fc layer 2 self.abs_max_out: 297.0\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "fc layer 3 self.abs_max_out: 217.0\n",
      "lif layer 2 self.abs_max_v: 423.0\n",
      "fc layer 1 self.abs_max_out: 590.0\n",
      "lif layer 2 self.abs_max_v: 436.0\n",
      "lif layer 2 self.abs_max_v: 463.0\n",
      "lif layer 1 self.abs_max_v: 626.5\n",
      "fc layer 3 self.abs_max_out: 233.0\n",
      "fc layer 2 self.abs_max_out: 311.0\n",
      "fc layer 3 self.abs_max_out: 284.0\n",
      "fc layer 2 self.abs_max_out: 312.0\n",
      "fc layer 2 self.abs_max_out: 351.0\n",
      "fc layer 3 self.abs_max_out: 289.0\n",
      "fc layer 2 self.abs_max_out: 354.0\n",
      "lif layer 1 self.abs_max_v: 713.5\n",
      "lif layer 1 self.abs_max_v: 820.0\n",
      "lif layer 1 self.abs_max_v: 838.0\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "lif layer 2 self.abs_max_v: 465.0\n",
      "lif layer 2 self.abs_max_v: 488.5\n",
      "lif layer 2 self.abs_max_v: 497.5\n",
      "fc layer 2 self.abs_max_out: 359.0\n",
      "fc layer 1 self.abs_max_out: 609.0\n",
      "fc layer 1 self.abs_max_out: 674.0\n",
      "fc layer 1 self.abs_max_out: 676.0\n",
      "lif layer 1 self.abs_max_v: 896.0\n",
      "fc layer 1 self.abs_max_out: 688.0\n",
      "lif layer 1 self.abs_max_v: 1014.0\n",
      "fc layer 1 self.abs_max_out: 774.0\n",
      "lif layer 1 self.abs_max_v: 1032.0\n",
      "lif layer 1 self.abs_max_v: 1105.0\n",
      "lif layer 2 self.abs_max_v: 499.5\n",
      "lif layer 2 self.abs_max_v: 504.0\n",
      "lif layer 2 self.abs_max_v: 513.0\n",
      "lif layer 2 self.abs_max_v: 536.5\n",
      "lif layer 2 self.abs_max_v: 553.5\n",
      "fc layer 1 self.abs_max_out: 794.0\n",
      "fc layer 2 self.abs_max_out: 364.0\n",
      "lif layer 1 self.abs_max_v: 1131.0\n",
      "lif layer 1 self.abs_max_v: 1173.5\n",
      "lif layer 1 self.abs_max_v: 1196.5\n",
      "lif layer 1 self.abs_max_v: 1199.0\n",
      "lif layer 1 self.abs_max_v: 1241.5\n",
      "lif layer 1 self.abs_max_v: 1259.0\n",
      "fc layer 1 self.abs_max_out: 837.0\n",
      "fc layer 2 self.abs_max_out: 387.0\n",
      "fc layer 2 self.abs_max_out: 426.0\n",
      "lif layer 1 self.abs_max_v: 1334.0\n",
      "fc layer 1 self.abs_max_out: 871.0\n",
      "fc layer 1 self.abs_max_out: 877.0\n",
      "lif layer 1 self.abs_max_v: 1342.0\n",
      "lif layer 1 self.abs_max_v: 1404.0\n",
      "lif layer 1 self.abs_max_v: 1452.0\n",
      "fc layer 1 self.abs_max_out: 899.0\n",
      "lif layer 1 self.abs_max_v: 1477.5\n",
      "fc layer 1 self.abs_max_out: 924.0\n",
      "lif layer 1 self.abs_max_v: 1518.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  9.125001/ 75.764236, val:  32.08%, val_best:  32.08%, tr:  99.39%, tr_best:  99.39%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.3811%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1715  17.518%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 576.5\n",
      "lif layer 2 self.abs_max_v: 623.5\n",
      "lif layer 2 self.abs_max_v: 643.0\n",
      "fc layer 3 self.abs_max_out: 311.0\n",
      "lif layer 2 self.abs_max_v: 661.0\n",
      "lif layer 2 self.abs_max_v: 681.5\n",
      "lif layer 2 self.abs_max_v: 706.0\n",
      "lif layer 2 self.abs_max_v: 711.0\n",
      "lif layer 2 self.abs_max_v: 758.5\n",
      "lif layer 2 self.abs_max_v: 771.5\n",
      "fc layer 2 self.abs_max_out: 462.0\n",
      "fc layer 2 self.abs_max_out: 476.0\n",
      "fc layer 1 self.abs_max_out: 939.0\n",
      "fc layer 1 self.abs_max_out: 946.0\n",
      "lif layer 1 self.abs_max_v: 1590.5\n",
      "lif layer 1 self.abs_max_v: 1712.5\n",
      "fc layer 2 self.abs_max_out: 478.0\n",
      "fc layer 2 self.abs_max_out: 488.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  8.748620/ 52.336384, val:  44.17%, val_best:  44.17%, tr:  99.08%, tr_best:  99.39%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.1462%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8074%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3194  16.313%\n",
      "fc layer 1 self.abs_max_out: 1001.0\n",
      "fc layer 3 self.abs_max_out: 338.0\n",
      "fc layer 3 self.abs_max_out: 339.0\n",
      "fc layer 3 self.abs_max_out: 343.0\n",
      "lif layer 2 self.abs_max_v: 798.0\n",
      "lif layer 2 self.abs_max_v: 810.0\n",
      "lif layer 2 self.abs_max_v: 821.0\n",
      "lif layer 2 self.abs_max_v: 877.5\n",
      "lif layer 2 self.abs_max_v: 892.0\n",
      "fc layer 1 self.abs_max_out: 1039.0\n",
      "lif layer 1 self.abs_max_v: 1764.5\n",
      "fc layer 1 self.abs_max_out: 1074.0\n",
      "lif layer 1 self.abs_max_v: 1768.5\n",
      "fc layer 1 self.abs_max_out: 1243.0\n",
      "lif layer 1 self.abs_max_v: 2127.5\n",
      "lif layer 1 self.abs_max_v: 2169.0\n",
      "lif layer 1 self.abs_max_v: 2194.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  8.191122/ 39.694778, val:  45.00%, val_best:  45.00%, tr:  99.18%, tr_best:  99.39%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0393%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6545%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4621  15.734%\n",
      "fc layer 2 self.abs_max_out: 504.0\n",
      "lif layer 2 self.abs_max_v: 894.5\n",
      "lif layer 2 self.abs_max_v: 902.0\n",
      "lif layer 2 self.abs_max_v: 912.0\n",
      "fc layer 1 self.abs_max_out: 1274.0\n",
      "lif layer 2 self.abs_max_v: 913.0\n",
      "lif layer 2 self.abs_max_v: 947.5\n",
      "lif layer 2 self.abs_max_v: 977.0\n",
      "fc layer 2 self.abs_max_out: 506.0\n",
      "fc layer 2 self.abs_max_out: 520.0\n",
      "fc layer 2 self.abs_max_out: 551.0\n",
      "fc layer 2 self.abs_max_out: 561.0\n",
      "lif layer 2 self.abs_max_v: 990.0\n",
      "lif layer 2 self.abs_max_v: 1000.0\n",
      "lif layer 1 self.abs_max_v: 2211.5\n",
      "lif layer 1 self.abs_max_v: 2232.5\n",
      "lif layer 1 self.abs_max_v: 2234.5\n",
      "fc layer 2 self.abs_max_out: 579.0\n",
      "lif layer 2 self.abs_max_v: 1025.5\n",
      "lif layer 2 self.abs_max_v: 1026.0\n",
      "lif layer 2 self.abs_max_v: 1039.0\n",
      "lif layer 2 self.abs_max_v: 1060.5\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  7.774102/ 51.262451, val:  40.42%, val_best:  45.00%, tr:  99.08%, tr_best:  99.39%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.5564%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5177%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6011  15.350%\n",
      "fc layer 3 self.abs_max_out: 351.0\n",
      "fc layer 2 self.abs_max_out: 602.0\n",
      "fc layer 3 self.abs_max_out: 355.0\n",
      "lif layer 1 self.abs_max_v: 2285.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  8.249138/ 44.022995, val:  42.50%, val_best:  45.00%, tr:  99.28%, tr_best:  99.39%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.9747%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7409  15.136%\n",
      "fc layer 3 self.abs_max_out: 366.0\n",
      "fc layer 1 self.abs_max_out: 1305.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  7.810047/ 33.331928, val:  47.50%, val_best:  47.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.30 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.8770%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2564%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 8706  14.821%\n",
      "fc layer 3 self.abs_max_out: 369.0\n",
      "fc layer 2 self.abs_max_out: 632.0\n",
      "fc layer 2 self.abs_max_out: 640.0\n",
      "fc layer 2 self.abs_max_out: 671.0\n",
      "lif layer 2 self.abs_max_v: 1095.0\n",
      "fc layer 2 self.abs_max_out: 679.0\n",
      "lif layer 2 self.abs_max_v: 1112.0\n",
      "fc layer 1 self.abs_max_out: 1313.0\n",
      "lif layer 1 self.abs_max_v: 2328.0\n",
      "fc layer 1 self.abs_max_out: 1340.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  7.684622/ 59.911949, val:  40.83%, val_best:  47.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.9666%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5979%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10049  14.664%\n",
      "fc layer 3 self.abs_max_out: 371.0\n",
      "fc layer 3 self.abs_max_out: 377.0\n",
      "fc layer 3 self.abs_max_out: 380.0\n",
      "lif layer 2 self.abs_max_v: 1135.0\n",
      "lif layer 2 self.abs_max_v: 1139.0\n",
      "lif layer 2 self.abs_max_v: 1165.5\n",
      "fc layer 2 self.abs_max_out: 681.0\n",
      "fc layer 3 self.abs_max_out: 391.0\n",
      "fc layer 3 self.abs_max_out: 416.0\n",
      "fc layer 3 self.abs_max_out: 460.0\n",
      "fc layer 3 self.abs_max_out: 483.0\n",
      "fc layer 2 self.abs_max_out: 688.0\n",
      "lif layer 2 self.abs_max_v: 1235.0\n",
      "fc layer 1 self.abs_max_out: 1375.0\n",
      "lif layer 1 self.abs_max_v: 2453.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  7.501394/ 47.145596, val:  52.92%, val_best:  52.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5688%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11343  14.483%\n",
      "lif layer 2 self.abs_max_v: 1257.0\n",
      "lif layer 2 self.abs_max_v: 1257.5\n",
      "fc layer 2 self.abs_max_out: 712.0\n",
      "lif layer 2 self.abs_max_v: 1341.0\n",
      "fc layer 2 self.abs_max_out: 714.0\n",
      "lif layer 2 self.abs_max_v: 1342.0\n",
      "fc layer 2 self.abs_max_out: 746.0\n",
      "fc layer 2 self.abs_max_out: 759.0\n",
      "lif layer 2 self.abs_max_v: 1405.0\n",
      "fc layer 1 self.abs_max_out: 1376.0\n",
      "lif layer 1 self.abs_max_v: 2492.0\n",
      "lif layer 1 self.abs_max_v: 2553.0\n",
      "lif layer 1 self.abs_max_v: 2570.5\n",
      "lif layer 1 self.abs_max_v: 2608.5\n",
      "fc layer 1 self.abs_max_out: 1405.0\n",
      "lif layer 1 self.abs_max_v: 2709.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.091205/ 56.497189, val:  47.08%, val_best:  52.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7296%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3553%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 12653  14.360%\n",
      "fc layer 1 self.abs_max_out: 1435.0\n",
      "lif layer 1 self.abs_max_v: 2738.0\n",
      "fc layer 1 self.abs_max_out: 1463.0\n",
      "lif layer 1 self.abs_max_v: 2832.0\n",
      "fc layer 1 self.abs_max_out: 1530.0\n",
      "lif layer 1 self.abs_max_v: 2946.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  7.607721/ 52.209278, val:  47.92%, val_best:  52.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.70 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8673%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9771%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 13913  14.211%\n",
      "fc layer 1 self.abs_max_out: 1575.0\n",
      "lif layer 1 self.abs_max_v: 3005.5\n",
      "fc layer 1 self.abs_max_out: 1595.0\n",
      "lif layer 1 self.abs_max_v: 3060.5\n",
      "fc layer 1 self.abs_max_out: 1622.0\n",
      "lif layer 1 self.abs_max_v: 3152.5\n",
      "fc layer 1 self.abs_max_out: 1690.0\n",
      "lif layer 1 self.abs_max_v: 3266.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  7.859890/ 51.542355, val:  56.67%, val_best:  56.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3555%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0016%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15139  14.058%\n",
      "fc layer 2 self.abs_max_out: 765.0\n",
      "fc layer 1 self.abs_max_out: 1709.0\n",
      "lif layer 1 self.abs_max_v: 3308.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.921632/ 44.031612, val:  51.67%, val_best:  56.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.4579%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1006%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16348  13.916%\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.200167/ 47.054810, val:  47.08%, val_best:  56.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7391%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9372%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17529  13.773%\n",
      "fc layer 1 self.abs_max_out: 1739.0\n",
      "lif layer 1 self.abs_max_v: 3377.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.615796/ 57.755562, val:  52.08%, val_best:  56.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 74.31 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.2416%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18712  13.652%\n",
      "fc layer 3 self.abs_max_out: 485.0\n",
      "fc layer 2 self.abs_max_out: 767.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.585056/ 47.969692, val:  56.25%, val_best:  56.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.4407%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5504%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 19901  13.552%\n",
      "fc layer 2 self.abs_max_out: 789.0\n",
      "fc layer 3 self.abs_max_out: 493.0\n",
      "fc layer 2 self.abs_max_out: 793.0\n",
      "lif layer 2 self.abs_max_v: 1464.0\n",
      "fc layer 2 self.abs_max_out: 796.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  8.025545/ 59.837345, val:  45.83%, val_best:  56.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.19 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3164%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21186  13.525%\n",
      "fc layer 2 self.abs_max_out: 816.0\n",
      "fc layer 2 self.abs_max_out: 865.0\n",
      "fc layer 3 self.abs_max_out: 495.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.236440/ 51.532715, val:  52.50%, val_best:  56.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.1115%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22364  13.437%\n",
      "fc layer 2 self.abs_max_out: 875.0\n",
      "fc layer 3 self.abs_max_out: 498.0\n",
      "fc layer 3 self.abs_max_out: 515.0\n",
      "fc layer 3 self.abs_max_out: 516.0\n",
      "fc layer 3 self.abs_max_out: 536.0\n",
      "fc layer 1 self.abs_max_out: 1764.0\n",
      "fc layer 1 self.abs_max_out: 1813.0\n",
      "lif layer 1 self.abs_max_v: 3384.5\n",
      "fc layer 1 self.abs_max_out: 1865.0\n",
      "lif layer 1 self.abs_max_v: 3557.5\n",
      "lif layer 1 self.abs_max_v: 3582.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.914594/ 32.877785, val:  58.75%, val_best:  58.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 74.85 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.1590%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23536  13.356%\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.915679/ 46.193371, val:  52.50%, val_best:  58.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.9365%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7179%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24697  13.277%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.653020/ 65.554443, val:  50.83%, val_best:  58.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0671%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5477%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25824  13.189%\n",
      "lif layer 2 self.abs_max_v: 1484.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  7.635895/ 58.514774, val:  52.92%, val_best:  58.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.9183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2120%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26956  13.112%\n",
      "fc layer 2 self.abs_max_out: 881.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  8.068317/ 37.062733, val:  55.83%, val_best:  58.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.6822%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 28152  13.071%\n",
      "fc layer 3 self.abs_max_out: 553.0\n",
      "fc layer 2 self.abs_max_out: 883.0\n",
      "fc layer 2 self.abs_max_out: 893.0\n",
      "fc layer 1 self.abs_max_out: 1884.0\n",
      "lif layer 1 self.abs_max_v: 3646.5\n",
      "lif layer 1 self.abs_max_v: 3685.5\n",
      "fc layer 1 self.abs_max_out: 1900.0\n",
      "lif layer 1 self.abs_max_v: 3743.0\n",
      "fc layer 3 self.abs_max_out: 554.0\n",
      "fc layer 3 self.abs_max_out: 566.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  7.322935/ 54.995289, val:  61.25%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.5528%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 29273  13.000%\n",
      "fc layer 2 self.abs_max_out: 898.0\n",
      "fc layer 2 self.abs_max_out: 915.0\n",
      "fc layer 2 self.abs_max_out: 938.0\n",
      "fc layer 2 self.abs_max_out: 1019.0\n",
      "fc layer 3 self.abs_max_out: 600.0\n",
      "fc layer 1 self.abs_max_out: 1962.0\n",
      "lif layer 1 self.abs_max_v: 3775.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  7.906593/ 57.541759, val:  60.83%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 71.59 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.1055%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2656%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 30395  12.936%\n",
      "lif layer 2 self.abs_max_v: 1540.5\n",
      "fc layer 3 self.abs_max_out: 608.0\n",
      "fc layer 3 self.abs_max_out: 629.0\n",
      "fc layer 2 self.abs_max_out: 1028.0\n",
      "fc layer 2 self.abs_max_out: 1029.0\n",
      "fc layer 1 self.abs_max_out: 1975.0\n",
      "lif layer 1 self.abs_max_v: 3816.5\n",
      "lif layer 1 self.abs_max_v: 3848.5\n",
      "fc layer 1 self.abs_max_out: 1983.0\n",
      "lif layer 1 self.abs_max_v: 3907.5\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  7.261192/ 61.307251, val:  58.75%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 69.34 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0380%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7730%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 31450  12.850%\n",
      "fc layer 2 self.abs_max_out: 1030.0\n",
      "fc layer 2 self.abs_max_out: 1038.0\n",
      "fc layer 1 self.abs_max_out: 2003.0\n",
      "fc layer 1 self.abs_max_out: 2050.0\n",
      "fc layer 1 self.abs_max_out: 2122.0\n",
      "lif layer 1 self.abs_max_v: 4028.5\n",
      "lif layer 1 self.abs_max_v: 4099.5\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  8.110409/ 54.635761, val:  57.50%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2690%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9165%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 32580  12.800%\n",
      "fc layer 1 self.abs_max_out: 2142.0\n",
      "fc layer 1 self.abs_max_out: 2206.0\n",
      "lif layer 1 self.abs_max_v: 4201.5\n",
      "lif layer 1 self.abs_max_v: 4272.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  7.241057/ 46.012871, val:  61.25%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.32 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.4362%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4378%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 33627  12.722%\n",
      "fc layer 1 self.abs_max_out: 2239.0\n",
      "fc layer 1 self.abs_max_out: 2301.0\n",
      "lif layer 1 self.abs_max_v: 4392.5\n",
      "lif layer 1 self.abs_max_v: 4461.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  8.188043/ 30.326630, val:  79.17%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.39 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0941%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7567%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 34710  12.662%\n",
      "fc layer 2 self.abs_max_out: 1059.0\n",
      "fc layer 1 self.abs_max_out: 2341.0\n",
      "lif layer 1 self.abs_max_v: 4469.0\n",
      "lif layer 1 self.abs_max_v: 4541.5\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  7.837646/ 50.146648, val:  59.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5476%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 35754  12.593%\n",
      "fc layer 2 self.abs_max_out: 1063.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  7.755612/ 96.821793, val:  50.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3580%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3706%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 36818  12.536%\n",
      "fc layer 1 self.abs_max_out: 2351.0\n",
      "fc layer 1 self.abs_max_out: 2419.0\n",
      "lif layer 1 self.abs_max_v: 4620.5\n",
      "lif layer 1 self.abs_max_v: 4694.5\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  7.783158/ 40.278809, val:  63.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2305%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8753%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 37834  12.466%\n",
      "fc layer 2 self.abs_max_out: 1085.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  7.962111/ 52.110764, val:  64.58%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.56 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3766%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 38865  12.406%\n",
      "fc layer 3 self.abs_max_out: 636.0\n",
      "fc layer 3 self.abs_max_out: 660.0\n",
      "fc layer 3 self.abs_max_out: 706.0\n",
      "fc layer 1 self.abs_max_out: 2425.0\n",
      "lif layer 1 self.abs_max_v: 4705.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  7.704235/ 58.494595, val:  55.83%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0263%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.0787%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 39869  12.341%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  7.909823/ 45.715862, val:  73.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.4481%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8382%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 40893  12.285%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  7.553574/ 76.344002, val:  51.67%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.4626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2561%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 41873  12.220%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  7.763651/ 68.164047, val:  65.00%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2947%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3790%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 42871  12.164%\n",
      "lif layer 2 self.abs_max_v: 1543.0\n",
      "lif layer 2 self.abs_max_v: 1589.0\n",
      "lif layer 2 self.abs_max_v: 1611.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  7.986112/ 62.174049, val:  69.17%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3002%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.9451%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 43854  12.107%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  7.142325/ 50.750046, val:  60.83%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.95 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.4509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1299%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 44757  12.031%\n",
      "fc layer 3 self.abs_max_out: 725.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  7.599810/ 58.764748, val:  65.00%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0711%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.5703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 45718  11.974%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  7.859523/ 71.604927, val:  46.67%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.9140%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1438%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 46693  11.924%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  7.488774/ 63.673088, val:  62.08%, val_best:  79.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 74.52 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.7680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.0062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 47636  11.868%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  7.232819/ 49.913746, val:  67.92%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3574%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6982%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 48569  11.812%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  7.465343/ 56.952736, val:  65.00%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.30 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7673%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 49494  11.757%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  7.695240/ 41.527100, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3312%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2830%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 50436  11.709%\n",
      "lif layer 2 self.abs_max_v: 1634.5\n",
      "lif layer 2 self.abs_max_v: 1660.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  7.590876/ 49.449585, val:  71.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.44 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2817%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 51358  11.658%\n",
      "lif layer 2 self.abs_max_v: 1700.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  7.334569/ 38.019928, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.53 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0367%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 52272  11.607%\n",
      "fc layer 3 self.abs_max_out: 736.0\n",
      "lif layer 2 self.abs_max_v: 1742.5\n",
      "lif layer 2 self.abs_max_v: 1769.5\n",
      "lif layer 2 self.abs_max_v: 1817.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  7.442239/ 55.677498, val:  67.92%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.36 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3151%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5634%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 53175  11.557%\n",
      "fc layer 3 self.abs_max_out: 785.0\n",
      "fc layer 1 self.abs_max_out: 2442.0\n",
      "lif layer 1 self.abs_max_v: 4737.5\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  7.313590/ 81.287247, val:  53.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.63 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.4895%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3059%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 54082  11.509%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  7.058516/ 41.562485, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.36 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6131%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 54974  11.460%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  7.228672/ 53.610565, val:  69.58%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4206%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 55832  11.406%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  7.110850/ 66.543060, val:  65.00%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.01 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7203%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6423%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 56689  11.354%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  7.887242/ 40.130833, val:  75.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.41 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.4560%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3053%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 57608  11.316%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  7.477658/ 61.298241, val:  66.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0665%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1603%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 58529  11.280%\n",
      "fc layer 3 self.abs_max_out: 795.0\n",
      "fc layer 3 self.abs_max_out: 810.0\n",
      "fc layer 3 self.abs_max_out: 829.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  7.224830/ 76.513748, val:  60.00%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.24 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.9906%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2927%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 59433  11.242%\n",
      "fc layer 3 self.abs_max_out: 835.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  8.153554/ 69.418549, val:  61.67%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.9246%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6486%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 60358  11.210%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  7.842065/ 52.070316, val:  73.75%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.63 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.1852%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7349%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 61253  11.173%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  7.584045/ 64.147125, val:  64.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.46 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2406%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3516%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 62140  11.136%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  7.282558/ 71.263176, val:  66.67%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.32 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3871%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5679%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 62975  11.091%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  7.184959/ 53.412823, val:  75.83%, val_best:  79.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.56 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5804%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7063%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 63795  11.045%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  7.292578/ 70.679115, val:  58.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.52 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5422%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.4867%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 64650  11.006%\n",
      "fc layer 2 self.abs_max_out: 1122.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  7.336529/ 88.575348, val:  54.58%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.45 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.4296%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 65479  10.965%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  7.673365/ 50.598160, val:  72.50%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.8994%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0520%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 66370  10.934%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  7.324155/ 43.813847, val:  75.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0540%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2259%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 67208  10.897%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  7.160114/ 61.395424, val:  64.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2964%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7870%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 68005  10.854%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  7.088398/ 45.468971, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.53 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.1516%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5854%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 68842  10.818%\n",
      "fc layer 3 self.abs_max_out: 866.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  6.915805/ 59.593624, val:  70.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.4591%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0108%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 69650  10.779%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  7.324061/ 49.343601, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.4121%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8480%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 70481  10.745%\n",
      "fc layer 1 self.abs_max_out: 2453.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  7.747392/ 59.245285, val:  73.75%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0774%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5827%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 71331  10.715%\n",
      "fc layer 3 self.abs_max_out: 874.0\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  7.283535/ 56.162510, val:  73.75%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.7731%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9609%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 72140  10.679%\n",
      "fc layer 3 self.abs_max_out: 880.0\n",
      "fc layer 2 self.abs_max_out: 1137.0\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  6.511383/ 84.969543, val:  59.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.9201%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1186%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 72885  10.635%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  7.028170/ 55.079895, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.9389%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6392%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 73648  10.595%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  6.855406/ 67.882233, val:  67.92%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.9694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0282%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 74431  10.559%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  6.759112/ 61.982288, val:  69.17%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.9286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1655%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 75185  10.520%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  6.744637/108.012360, val:  59.17%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.1082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7123%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 75948  10.483%\n",
      "fc layer 3 self.abs_max_out: 913.0\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  6.929185/ 65.535538, val:  73.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.61 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2763%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0420%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 76748  10.453%\n",
      "fc layer 3 self.abs_max_out: 997.0\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  7.161937/ 49.168430, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.4164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 77500  10.416%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  7.791087/ 55.394642, val:  67.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3311%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 78349  10.393%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  6.633249/ 90.006958, val:  57.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3886%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9823%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 79104  10.359%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  6.983331/ 59.107948, val:  74.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.8251%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 79880  10.328%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  7.349762/ 43.804554, val:  82.08%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.9162%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3208%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 80672  10.300%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  6.598896/ 64.106644, val:  69.58%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0651%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8591%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 81419  10.267%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  6.462281/ 68.637184, val:  67.50%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.1205%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7517%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 82153  10.234%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  7.285377/ 62.283092, val:  70.42%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2103%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5745%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 82941  10.207%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  7.018930/ 52.870487, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9923%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 83729  10.182%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  7.599358/ 68.112839, val:  73.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.1390%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 84549  10.160%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  6.862638/ 62.005836, val:  74.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2508%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.9756%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 85319  10.134%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  6.938703/ 63.926754, val:  72.92%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3290%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1836%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 86061  10.104%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  7.228002/ 72.132286, val:  70.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.4998%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3778%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 86836  10.079%\n",
      "fc layer 1 self.abs_max_out: 2486.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  6.711693/ 69.239395, val:  71.25%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.4799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5183%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 87544  10.047%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  6.850381/ 49.517288, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.21 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.9541%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 88261  10.017%\n",
      "fc layer 1 self.abs_max_out: 2509.0\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  7.324566/ 71.776886, val:  68.75%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3966%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 89020   9.992%\n",
      "lif layer 1 self.abs_max_v: 4790.0\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  5.840129/ 53.738419, val:  76.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.1459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8811%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 89695   9.959%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  6.760887/ 87.422150, val:  64.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2576%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4030%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 90409   9.930%\n",
      "fc layer 1 self.abs_max_out: 2524.0\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  6.389301/ 58.531170, val:  74.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2815%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4102%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 91112   9.901%\n",
      "fc layer 2 self.abs_max_out: 1181.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  6.867122/ 50.751705, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4952%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 91840   9.875%\n",
      "fc layer 1 self.abs_max_out: 2551.0\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  6.577170/ 55.616211, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.4646%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 92561   9.849%\n",
      "fc layer 1 self.abs_max_out: 2606.0\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  6.484059/ 63.477242, val:  75.42%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.21 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3367%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1356%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 93269   9.822%\n",
      "fc layer 1 self.abs_max_out: 2673.0\n",
      "fc layer 3 self.abs_max_out: 999.0\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  6.092110/ 57.797127, val:  73.75%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7706%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 93946   9.792%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  6.613065/ 79.224380, val:  63.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3265%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7930%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 94631   9.764%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  6.057276/ 61.971710, val:  73.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0854%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0441%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 95296   9.734%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  6.222082/ 64.229469, val:  72.50%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0370%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4520%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 95979   9.707%\n",
      "fc layer 2 self.abs_max_out: 1250.0\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  6.431836/ 57.621750, val:  78.75%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2409%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 96676   9.681%\n",
      "fc layer 1 self.abs_max_out: 2803.0\n",
      "fc layer 2 self.abs_max_out: 1255.0\n",
      "lif layer 1 self.abs_max_v: 4841.5\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  5.897711/ 49.380707, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.8696%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9275%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 97331   9.652%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  6.171288/ 54.084522, val:  77.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0098%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6079%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 98002   9.625%\n",
      "fc layer 1 self.abs_max_out: 3107.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  6.345324/ 52.483192, val:  81.67%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.8604%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9954%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 98695   9.601%\n",
      "fc layer 3 self.abs_max_out: 1013.0\n",
      "fc layer 1 self.abs_max_out: 3131.0\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  5.778909/ 59.458351, val:  77.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.9225%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2495%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 99374   9.576%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  6.004664/ 66.158516, val:  72.08%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.0372%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3486%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 100043   9.550%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  6.699250/ 75.743614, val:  72.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2037%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 100771   9.531%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  6.178448/ 52.860188, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3657%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 101458   9.508%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  5.653938/ 62.849506, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5419%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6985%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 102104   9.481%\n",
      "fc layer 3 self.abs_max_out: 1026.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  5.895529/ 60.886250, val:  77.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7569%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0444%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 102775   9.458%\n",
      "fc layer 3 self.abs_max_out: 1037.0\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  6.121157/ 67.021370, val:  66.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5722%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2748%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 103422   9.432%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  5.662428/ 56.895390, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3767%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1284%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 104090   9.409%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  5.561559/ 67.246796, val:  71.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2912%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0930%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 104711   9.382%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  5.846401/ 62.679230, val:  72.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3708%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5185%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 105347   9.357%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  5.955269/ 55.138885, val:  78.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3795%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7377%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 105985   9.333%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  6.153361/ 63.326836, val:  75.83%, val_best:  83.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3438%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9779%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 106648   9.311%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  6.329788/ 61.601044, val:  81.25%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3438%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1660%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 107310   9.289%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  6.156822/ 51.344910, val:  79.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5021%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 107953   9.266%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  5.805488/ 52.853241, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5657%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2673%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 108591   9.243%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  5.403469/ 67.783737, val:  76.67%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.4807%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6318%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 109195   9.218%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  5.983007/ 49.187798, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8425%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 109864   9.198%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  5.744258/ 57.050667, val:  80.42%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7331%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2698%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 110530   9.179%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  5.819010/ 46.515030, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.9287%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5223%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 111147   9.156%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  5.549969/ 49.974266, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7702%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4201%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 111763   9.133%\n",
      "fc layer 3 self.abs_max_out: 1047.0\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  5.583252/ 51.273293, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8445%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0819%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 112346   9.108%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  5.762347/ 56.682095, val:  77.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7770%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3544%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 112937   9.083%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  4.968005/ 68.593071, val:  75.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.60 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7444%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9888%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 113506   9.058%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  5.471626/ 61.417702, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5939%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 114118   9.036%\n",
      "lif layer 2 self.abs_max_v: 1829.5\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  5.238472/ 54.038544, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3902%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 114704   9.013%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  5.492926/ 68.765167, val:  71.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5034%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9311%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 115297   8.990%\n",
      "lif layer 2 self.abs_max_v: 1868.5\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  5.947471/ 52.373547, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5173%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8959%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 115923   8.970%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  5.001040/ 64.858459, val:  80.42%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3205%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5253%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 116499   8.947%\n",
      "lif layer 2 self.abs_max_v: 1907.0\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  5.245492/ 64.749451, val:  80.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.3598%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2310%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 117074   8.924%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  5.026417/ 82.412560, val:  74.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.6282%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5042%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 117641   8.901%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  5.020371/ 61.462723, val:  80.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.6221%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2213%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 118206   8.878%\n",
      "lif layer 2 self.abs_max_v: 2002.0\n",
      "fc layer 3 self.abs_max_out: 1055.0\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  5.071970/ 66.274666, val:  80.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7796%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5912%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 118762   8.855%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  5.505430/ 64.035080, val:  75.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2160%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 119362   8.835%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  5.346749/ 60.895203, val:  80.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0348%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4909%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 119961   8.815%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  5.727817/ 47.311577, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9066%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 120583   8.798%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  5.576740/ 62.041656, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.61 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0961%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9201%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 121184   8.779%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  5.602185/ 60.224087, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2688%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 121771   8.759%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  5.687648/ 74.378769, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.9515%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1265%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 122386   8.742%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  5.239086/ 72.707306, val:  70.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.9486%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 122947   8.721%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  5.160769/ 55.982742, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7800%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 123550   8.703%\n",
      "fc layer 3 self.abs_max_out: 1059.0\n",
      "fc layer 3 self.abs_max_out: 1078.0\n",
      "fc layer 3 self.abs_max_out: 1087.0\n",
      "fc layer 3 self.abs_max_out: 1092.0\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  4.949331/ 78.766373, val:  74.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8243%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7680%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 124083   8.681%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  5.398744/ 59.167702, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8512%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6028%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 124670   8.663%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  5.013207/ 54.488304, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.9137%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5476%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 125199   8.641%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  5.772288/ 56.868404, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7832%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6776%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 125816   8.625%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  4.550412/ 77.563446, val:  78.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8603%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 126345   8.604%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  5.307101/ 60.645714, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.9174%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3421%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 126909   8.585%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  5.831987/ 53.191841, val:  83.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8716%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 127513   8.569%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  4.858242/ 72.373146, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7181%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 128024   8.547%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  5.552875/ 54.456665, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8714%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 128605   8.530%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  5.097595/ 48.755466, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0635%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9221%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 129156   8.511%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  4.775527/ 70.438545, val:  77.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.1144%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8338%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 129689   8.492%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  5.148896/ 67.998878, val:  76.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0064%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3770%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 130249   8.474%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  4.340528/ 53.205124, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8007%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5352%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 130752   8.453%\n",
      "fc layer 2 self.abs_max_out: 1259.0\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  5.120182/ 63.098484, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.6004%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9251%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 131330   8.437%\n",
      "fc layer 3 self.abs_max_out: 1094.0\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  5.050771/ 58.320900, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7226%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0244%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 131863   8.418%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  4.852574/ 69.958855, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.00 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7438%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.4739%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 132391   8.399%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  5.157706/ 50.294365, val:  87.08%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2232%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 132918   8.381%\n",
      "fc layer 3 self.abs_max_out: 1131.0\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  4.570822/ 79.877380, val:  77.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.1493%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.7971%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 133417   8.361%\n",
      "fc layer 3 self.abs_max_out: 1141.0\n",
      "fc layer 3 self.abs_max_out: 1168.0\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  5.026594/ 59.740967, val:  78.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0844%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1571%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 133942   8.342%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  5.055928/ 51.887123, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.9093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6505%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 134469   8.324%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  5.128823/ 60.267475, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8471%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.4662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 135017   8.308%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  5.027404/ 67.341682, val:  74.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0180%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.4272%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 135541   8.290%\n",
      "lif layer 2 self.abs_max_v: 2025.0\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  4.888810/ 68.962082, val:  75.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.1874%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6205%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 136065   8.273%\n",
      "lif layer 2 self.abs_max_v: 2049.0\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  4.008757/ 58.512714, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.2872%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 136525   8.252%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  4.359521/ 57.491879, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.1231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 137007   8.232%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  4.109580/ 67.199791, val:  80.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 137473   8.212%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  5.572545/ 57.106945, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.2366%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2679%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 138049   8.198%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  4.156593/ 68.826263, val:  78.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.2696%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5326%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 138539   8.180%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  4.481400/ 65.661888, val:  81.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.2109%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9390%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 139021   8.161%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  4.472166/ 70.969566, val:  77.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0616%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 139519   8.144%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  4.353992/ 57.957973, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0867%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9956%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 139988   8.124%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  4.460481/ 66.260994, val:  80.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8369%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3103%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 140477   8.107%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  4.248857/ 53.629604, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8448%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2543%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 140937   8.088%\n",
      "fc layer 2 self.abs_max_out: 1269.0\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  4.648004/ 69.852814, val:  79.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.9376%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9261%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 141434   8.071%\n",
      "fc layer 3 self.abs_max_out: 1222.0\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  4.976091/ 70.407387, val:  79.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0770%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1710%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 141958   8.056%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  4.516603/ 57.195686, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.9845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4617%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 142450   8.039%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  4.197620/ 54.326183, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8182%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1973%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 142906   8.020%\n",
      "fc layer 2 self.abs_max_out: 1314.0\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  4.103435/ 55.364670, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8616%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3065%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 143361   8.002%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  4.495961/ 64.207970, val:  80.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.9192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4470%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 143830   7.985%\n",
      "lif layer 2 self.abs_max_v: 2075.5\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  4.657953/ 53.337467, val:  81.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0220%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0198%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 144346   7.970%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  4.555851/ 57.624073, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8886%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3861%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 144844   7.954%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  4.717431/ 64.742516, val:  80.00%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.0708%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8723%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 145349   7.939%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  4.220708/ 64.017220, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8127%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2200%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 145843   7.924%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  4.631234/ 72.510788, val:  77.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4336%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 146341   7.909%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  4.145715/ 65.208961, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3197%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 146814   7.893%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  3.929635/ 52.764343, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.6749%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 147232   7.874%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  4.118848/ 85.968536, val:  70.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.6261%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8129%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 147695   7.857%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  4.598157/ 59.597370, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8064%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3629%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 148190   7.843%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  4.307035/ 77.181801, val:  72.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7064%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0387%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 148661   7.827%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  4.194017/ 61.161968, val:  82.92%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.6490%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0156%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 149107   7.811%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  4.279089/ 66.484619, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5915%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1992%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 149574   7.795%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  4.081738/ 66.288666, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5162%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 150008   7.778%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  4.286685/ 68.131264, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7558%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 150465   7.762%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  3.555236/ 58.852802, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1745%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 150861   7.744%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  3.754403/ 61.395397, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.6351%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3391%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6fdc7c70ca40b298bd1440ddc1b4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñÖ‚ñÜ‚ñÉ‚ñÜ‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>3.7544</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.82083</td></tr><tr><td>val_loss</td><td>61.3954</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-sweep-16</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tslebin1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tslebin1</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_125820-tslebin1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: orpo1ut4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_170856-orpo1ut4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/orpo1ut4' target=\"_blank\">gallant-sweep-23</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/orpo1ut4' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/orpo1ut4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251214_170906_413', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 64, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 0.5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 1, 'lif_layer_v_threshold2': 16, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 0.5, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 1, self.v_threshold 16\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=0.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=16, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 2 self.abs_max_out: 6.0\n",
      "lif layer 2 self.abs_max_v: 6.0\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "fc layer 2 self.abs_max_out: 29.0\n",
      "lif layer 2 self.abs_max_v: 29.0\n",
      "fc layer 3 self.abs_max_out: 7.0\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 138.0\n",
      "fc layer 2 self.abs_max_out: 39.0\n",
      "lif layer 2 self.abs_max_v: 43.5\n",
      "fc layer 3 self.abs_max_out: 14.0\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 210.0\n",
      "fc layer 2 self.abs_max_out: 49.0\n",
      "lif layer 2 self.abs_max_v: 59.5\n",
      "fc layer 3 self.abs_max_out: 33.0\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 235.0\n",
      "fc layer 2 self.abs_max_out: 53.0\n",
      "lif layer 2 self.abs_max_v: 80.0\n",
      "fc layer 1 self.abs_max_out: 212.0\n",
      "lif layer 1 self.abs_max_v: 301.5\n",
      "lif layer 2 self.abs_max_v: 88.0\n",
      "fc layer 1 self.abs_max_out: 234.0\n",
      "lif layer 1 self.abs_max_v: 310.0\n",
      "fc layer 2 self.abs_max_out: 64.0\n",
      "lif layer 2 self.abs_max_v: 108.0\n",
      "fc layer 3 self.abs_max_out: 39.0\n",
      "fc layer 1 self.abs_max_out: 255.0\n",
      "lif layer 1 self.abs_max_v: 357.0\n",
      "fc layer 3 self.abs_max_out: 42.0\n",
      "lif layer 1 self.abs_max_v: 363.5\n",
      "fc layer 1 self.abs_max_out: 264.0\n",
      "fc layer 3 self.abs_max_out: 47.0\n",
      "fc layer 2 self.abs_max_out: 74.0\n",
      "lif layer 2 self.abs_max_v: 111.5\n",
      "fc layer 3 self.abs_max_out: 51.0\n",
      "fc layer 2 self.abs_max_out: 82.0\n",
      "fc layer 2 self.abs_max_out: 93.0\n",
      "fc layer 3 self.abs_max_out: 54.0\n",
      "lif layer 2 self.abs_max_v: 128.5\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "lif layer 2 self.abs_max_v: 129.0\n",
      "lif layer 2 self.abs_max_v: 131.5\n",
      "lif layer 1 self.abs_max_v: 401.0\n",
      "lif layer 2 self.abs_max_v: 145.0\n",
      "lif layer 1 self.abs_max_v: 456.5\n",
      "fc layer 2 self.abs_max_out: 94.0\n",
      "lif layer 2 self.abs_max_v: 146.5\n",
      "fc layer 2 self.abs_max_out: 95.0\n",
      "lif layer 2 self.abs_max_v: 152.5\n",
      "lif layer 2 self.abs_max_v: 156.0\n",
      "fc layer 2 self.abs_max_out: 108.0\n",
      "fc layer 1 self.abs_max_out: 269.0\n",
      "fc layer 3 self.abs_max_out: 86.0\n",
      "fc layer 1 self.abs_max_out: 280.0\n",
      "fc layer 1 self.abs_max_out: 426.0\n",
      "fc layer 2 self.abs_max_out: 109.0\n",
      "lif layer 2 self.abs_max_v: 168.5\n",
      "fc layer 2 self.abs_max_out: 117.0\n",
      "lif layer 2 self.abs_max_v: 177.0\n",
      "lif layer 2 self.abs_max_v: 189.0\n",
      "fc layer 2 self.abs_max_out: 140.0\n",
      "lif layer 2 self.abs_max_v: 204.5\n",
      "lif layer 2 self.abs_max_v: 206.0\n",
      "fc layer 1 self.abs_max_out: 459.0\n",
      "lif layer 1 self.abs_max_v: 459.0\n",
      "fc layer 3 self.abs_max_out: 90.0\n",
      "fc layer 3 self.abs_max_out: 102.0\n",
      "fc layer 3 self.abs_max_out: 109.0\n",
      "fc layer 2 self.abs_max_out: 172.0\n",
      "lif layer 2 self.abs_max_v: 212.5\n",
      "fc layer 2 self.abs_max_out: 179.0\n",
      "lif layer 1 self.abs_max_v: 493.0\n",
      "lif layer 1 self.abs_max_v: 495.5\n",
      "lif layer 1 self.abs_max_v: 573.5\n",
      "lif layer 2 self.abs_max_v: 219.5\n",
      "lif layer 2 self.abs_max_v: 227.5\n",
      "lif layer 2 self.abs_max_v: 255.0\n",
      "fc layer 3 self.abs_max_out: 127.0\n",
      "lif layer 2 self.abs_max_v: 255.5\n",
      "fc layer 3 self.abs_max_out: 130.0\n",
      "lif layer 2 self.abs_max_v: 261.5\n",
      "lif layer 2 self.abs_max_v: 276.0\n",
      "fc layer 2 self.abs_max_out: 199.0\n",
      "fc layer 2 self.abs_max_out: 203.0\n",
      "fc layer 2 self.abs_max_out: 205.0\n",
      "fc layer 2 self.abs_max_out: 210.0\n",
      "lif layer 2 self.abs_max_v: 299.5\n",
      "lif layer 1 self.abs_max_v: 644.5\n",
      "lif layer 2 self.abs_max_v: 320.0\n",
      "fc layer 2 self.abs_max_out: 232.0\n",
      "fc layer 2 self.abs_max_out: 244.0\n",
      "lif layer 2 self.abs_max_v: 397.0\n",
      "lif layer 2 self.abs_max_v: 401.5\n",
      "lif layer 2 self.abs_max_v: 431.5\n",
      "lif layer 1 self.abs_max_v: 726.5\n",
      "fc layer 2 self.abs_max_out: 245.0\n",
      "lif layer 2 self.abs_max_v: 461.0\n",
      "fc layer 2 self.abs_max_out: 251.0\n",
      "lif layer 2 self.abs_max_v: 461.5\n",
      "fc layer 2 self.abs_max_out: 261.0\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 3 self.abs_max_out: 139.0\n",
      "fc layer 3 self.abs_max_out: 140.0\n",
      "lif layer 1 self.abs_max_v: 764.5\n",
      "fc layer 2 self.abs_max_out: 263.0\n",
      "fc layer 2 self.abs_max_out: 303.0\n",
      "lif layer 2 self.abs_max_v: 510.5\n",
      "fc layer 3 self.abs_max_out: 148.0\n",
      "fc layer 3 self.abs_max_out: 164.0\n",
      "fc layer 3 self.abs_max_out: 202.0\n",
      "fc layer 3 self.abs_max_out: 214.0\n",
      "fc layer 2 self.abs_max_out: 321.0\n",
      "lif layer 2 self.abs_max_v: 552.5\n",
      "lif layer 2 self.abs_max_v: 553.0\n",
      "lif layer 2 self.abs_max_v: 571.5\n",
      "lif layer 2 self.abs_max_v: 578.5\n",
      "fc layer 2 self.abs_max_out: 352.0\n",
      "lif layer 1 self.abs_max_v: 784.5\n",
      "lif layer 2 self.abs_max_v: 581.0\n",
      "fc layer 1 self.abs_max_out: 519.0\n",
      "lif layer 1 self.abs_max_v: 796.0\n",
      "fc layer 2 self.abs_max_out: 404.0\n",
      "lif layer 2 self.abs_max_v: 679.0\n",
      "lif layer 1 self.abs_max_v: 914.0\n",
      "fc layer 2 self.abs_max_out: 409.0\n",
      "lif layer 2 self.abs_max_v: 748.5\n",
      "fc layer 1 self.abs_max_out: 547.0\n",
      "fc layer 1 self.abs_max_out: 571.0\n",
      "fc layer 1 self.abs_max_out: 626.0\n",
      "lif layer 1 self.abs_max_v: 988.5\n",
      "fc layer 2 self.abs_max_out: 417.0\n",
      "lif layer 2 self.abs_max_v: 752.5\n",
      "fc layer 2 self.abs_max_out: 421.0\n",
      "fc layer 1 self.abs_max_out: 710.0\n",
      "lif layer 1 self.abs_max_v: 1146.0\n",
      "lif layer 1 self.abs_max_v: 1192.0\n",
      "lif layer 2 self.abs_max_v: 780.0\n",
      "fc layer 3 self.abs_max_out: 221.0\n",
      "fc layer 2 self.abs_max_out: 485.0\n",
      "fc layer 2 self.abs_max_out: 490.0\n",
      "fc layer 2 self.abs_max_out: 493.0\n",
      "fc layer 1 self.abs_max_out: 725.0\n",
      "fc layer 1 self.abs_max_out: 761.0\n",
      "fc layer 1 self.abs_max_out: 805.0\n",
      "fc layer 1 self.abs_max_out: 828.0\n",
      "fc layer 1 self.abs_max_out: 928.0\n",
      "fc layer 1 self.abs_max_out: 929.0\n",
      "fc layer 1 self.abs_max_out: 939.0\n",
      "fc layer 1 self.abs_max_out: 960.0\n",
      "fc layer 2 self.abs_max_out: 503.0\n",
      "fc layer 1 self.abs_max_out: 975.0\n",
      "lif layer 1 self.abs_max_v: 1454.5\n",
      "lif layer 1 self.abs_max_v: 1649.5\n",
      "lif layer 2 self.abs_max_v: 808.0\n",
      "fc layer 2 self.abs_max_out: 515.0\n",
      "lif layer 2 self.abs_max_v: 813.5\n",
      "fc layer 1 self.abs_max_out: 987.0\n",
      "fc layer 3 self.abs_max_out: 223.0\n",
      "fc layer 3 self.abs_max_out: 224.0\n",
      "lif layer 2 self.abs_max_v: 820.5\n",
      "fc layer 3 self.abs_max_out: 227.0\n",
      "fc layer 3 self.abs_max_out: 231.0\n",
      "lif layer 2 self.abs_max_v: 870.5\n",
      "fc layer 1 self.abs_max_out: 1068.0\n",
      "fc layer 1 self.abs_max_out: 1079.0\n",
      "fc layer 3 self.abs_max_out: 270.0\n",
      "fc layer 3 self.abs_max_out: 282.0\n",
      "fc layer 3 self.abs_max_out: 287.0\n",
      "fc layer 2 self.abs_max_out: 561.0\n",
      "fc layer 2 self.abs_max_out: 576.0\n",
      "lif layer 2 self.abs_max_v: 875.5\n",
      "lif layer 2 self.abs_max_v: 904.5\n",
      "lif layer 2 self.abs_max_v: 913.5\n",
      "lif layer 2 self.abs_max_v: 942.0\n",
      "lif layer 2 self.abs_max_v: 962.0\n",
      "lif layer 2 self.abs_max_v: 969.0\n",
      "fc layer 1 self.abs_max_out: 1148.0\n",
      "fc layer 1 self.abs_max_out: 1202.0\n",
      "fc layer 1 self.abs_max_out: 1208.0\n",
      "fc layer 2 self.abs_max_out: 592.0\n",
      "fc layer 1 self.abs_max_out: 1247.0\n",
      "fc layer 1 self.abs_max_out: 1351.0\n",
      "fc layer 2 self.abs_max_out: 594.0\n",
      "fc layer 1 self.abs_max_out: 1354.0\n",
      "fc layer 2 self.abs_max_out: 607.0\n",
      "fc layer 2 self.abs_max_out: 641.0\n",
      "lif layer 2 self.abs_max_v: 1045.5\n",
      "lif layer 1 self.abs_max_v: 1652.5\n",
      "fc layer 1 self.abs_max_out: 1360.0\n",
      "fc layer 1 self.abs_max_out: 1414.0\n",
      "lif layer 1 self.abs_max_v: 1852.0\n",
      "fc layer 1 self.abs_max_out: 1468.0\n",
      "fc layer 1 self.abs_max_out: 1494.0\n",
      "fc layer 1 self.abs_max_out: 1617.0\n",
      "fc layer 1 self.abs_max_out: 1782.0\n",
      "fc layer 1 self.abs_max_out: 1800.0\n",
      "fc layer 2 self.abs_max_out: 669.0\n",
      "lif layer 2 self.abs_max_v: 1074.0\n",
      "lif layer 2 self.abs_max_v: 1205.0\n",
      "fc layer 2 self.abs_max_out: 673.0\n",
      "lif layer 2 self.abs_max_v: 1223.5\n",
      "lif layer 1 self.abs_max_v: 1963.5\n",
      "lif layer 2 self.abs_max_v: 1271.0\n",
      "fc layer 3 self.abs_max_out: 293.0\n",
      "fc layer 2 self.abs_max_out: 688.0\n",
      "fc layer 2 self.abs_max_out: 758.0\n",
      "lif layer 2 self.abs_max_v: 1307.5\n",
      "lif layer 2 self.abs_max_v: 1368.0\n",
      "lif layer 2 self.abs_max_v: 1407.5\n",
      "lif layer 1 self.abs_max_v: 2058.5\n",
      "lif layer 1 self.abs_max_v: 2059.5\n",
      "fc layer 3 self.abs_max_out: 315.0\n",
      "fc layer 1 self.abs_max_out: 1855.0\n",
      "fc layer 1 self.abs_max_out: 1941.0\n",
      "fc layer 3 self.abs_max_out: 326.0\n",
      "fc layer 1 self.abs_max_out: 2017.0\n",
      "fc layer 3 self.abs_max_out: 340.0\n",
      "fc layer 2 self.abs_max_out: 806.0\n",
      "lif layer 2 self.abs_max_v: 1483.0\n",
      "fc layer 1 self.abs_max_out: 2089.0\n",
      "lif layer 1 self.abs_max_v: 2089.0\n",
      "fc layer 2 self.abs_max_out: 929.0\n",
      "lif layer 1 self.abs_max_v: 2207.0\n",
      "lif layer 1 self.abs_max_v: 2293.5\n",
      "lif layer 1 self.abs_max_v: 2407.0\n",
      "lif layer 1 self.abs_max_v: 2430.5\n",
      "fc layer 1 self.abs_max_out: 2103.0\n",
      "fc layer 1 self.abs_max_out: 2212.0\n",
      "fc layer 1 self.abs_max_out: 2222.0\n",
      "lif layer 1 self.abs_max_v: 2446.5\n",
      "lif layer 1 self.abs_max_v: 2509.5\n",
      "fc layer 3 self.abs_max_out: 380.0\n",
      "lif layer 2 self.abs_max_v: 1486.0\n",
      "fc layer 2 self.abs_max_out: 931.0\n",
      "lif layer 2 self.abs_max_v: 1529.0\n",
      "lif layer 2 self.abs_max_v: 1595.5\n",
      "lif layer 2 self.abs_max_v: 1623.5\n",
      "lif layer 2 self.abs_max_v: 1706.0\n",
      "lif layer 2 self.abs_max_v: 1747.0\n",
      "fc layer 1 self.abs_max_out: 2299.0\n",
      "fc layer 1 self.abs_max_out: 2358.0\n",
      "fc layer 2 self.abs_max_out: 1000.0\n",
      "fc layer 2 self.abs_max_out: 1020.0\n",
      "fc layer 1 self.abs_max_out: 2369.0\n",
      "fc layer 1 self.abs_max_out: 2377.0\n",
      "lif layer 2 self.abs_max_v: 1858.0\n",
      "lif layer 2 self.abs_max_v: 1866.0\n",
      "lif layer 1 self.abs_max_v: 2712.0\n",
      "lif layer 1 self.abs_max_v: 3034.0\n",
      "fc layer 1 self.abs_max_out: 2401.0\n",
      "fc layer 2 self.abs_max_out: 1053.0\n",
      "fc layer 2 self.abs_max_out: 1074.0\n",
      "lif layer 2 self.abs_max_v: 1915.0\n",
      "fc layer 2 self.abs_max_out: 1108.0\n",
      "lif layer 2 self.abs_max_v: 2065.5\n",
      "fc layer 2 self.abs_max_out: 1156.0\n",
      "fc layer 2 self.abs_max_out: 1200.0\n",
      "lif layer 2 self.abs_max_v: 2173.0\n",
      "fc layer 2 self.abs_max_out: 1218.0\n",
      "lif layer 2 self.abs_max_v: 2293.0\n",
      "fc layer 2 self.abs_max_out: 1230.0\n",
      "lif layer 2 self.abs_max_v: 2372.5\n",
      "fc layer 2 self.abs_max_out: 1232.0\n",
      "lif layer 1 self.abs_max_v: 3112.5\n",
      "lif layer 1 self.abs_max_v: 3244.5\n",
      "lif layer 2 self.abs_max_v: 2377.5\n",
      "lif layer 1 self.abs_max_v: 3299.5\n",
      "fc layer 2 self.abs_max_out: 1252.0\n",
      "fc layer 2 self.abs_max_out: 1261.0\n",
      "fc layer 2 self.abs_max_out: 1302.0\n",
      "lif layer 2 self.abs_max_v: 2455.5\n",
      "lif layer 2 self.abs_max_v: 2486.0\n",
      "fc layer 2 self.abs_max_out: 1314.0\n",
      "fc layer 1 self.abs_max_out: 2426.0\n",
      "lif layer 1 self.abs_max_v: 3677.0\n",
      "lif layer 1 self.abs_max_v: 3847.0\n",
      "fc layer 1 self.abs_max_out: 2430.0\n",
      "lif layer 2 self.abs_max_v: 2499.5\n",
      "fc layer 1 self.abs_max_out: 2525.0\n",
      "fc layer 1 self.abs_max_out: 2580.0\n",
      "fc layer 2 self.abs_max_out: 1327.0\n",
      "fc layer 2 self.abs_max_out: 1447.0\n",
      "lif layer 2 self.abs_max_v: 2576.5\n",
      "lif layer 2 self.abs_max_v: 2656.0\n",
      "lif layer 2 self.abs_max_v: 2708.0\n",
      "fc layer 2 self.abs_max_out: 1490.0\n",
      "fc layer 2 self.abs_max_out: 1493.0\n",
      "fc layer 2 self.abs_max_out: 1528.0\n",
      "fc layer 2 self.abs_max_out: 1681.0\n",
      "lif layer 2 self.abs_max_v: 2853.0\n",
      "lif layer 2 self.abs_max_v: 3083.5\n",
      "lif layer 2 self.abs_max_v: 3122.0\n",
      "lif layer 2 self.abs_max_v: 3141.0\n",
      "lif layer 1 self.abs_max_v: 3913.0\n",
      "fc layer 1 self.abs_max_out: 2979.0\n",
      "fc layer 1 self.abs_max_out: 2990.0\n",
      "fc layer 1 self.abs_max_out: 3065.0\n",
      "fc layer 1 self.abs_max_out: 3226.0\n",
      "lif layer 1 self.abs_max_v: 4117.5\n",
      "lif layer 1 self.abs_max_v: 4206.5\n",
      "lif layer 1 self.abs_max_v: 4284.5\n",
      "fc layer 2 self.abs_max_out: 1704.0\n",
      "lif layer 2 self.abs_max_v: 3179.0\n",
      "lif layer 2 self.abs_max_v: 3240.0\n",
      "lif layer 2 self.abs_max_v: 3271.0\n",
      "lif layer 2 self.abs_max_v: 3286.5\n",
      "fc layer 1 self.abs_max_out: 3252.0\n",
      "fc layer 1 self.abs_max_out: 3266.0\n",
      "fc layer 1 self.abs_max_out: 3308.0\n",
      "fc layer 1 self.abs_max_out: 3474.0\n",
      "fc layer 2 self.abs_max_out: 1822.0\n",
      "fc layer 2 self.abs_max_out: 1848.0\n",
      "lif layer 2 self.abs_max_v: 3450.0\n",
      "fc layer 2 self.abs_max_out: 1895.0\n",
      "lif layer 2 self.abs_max_v: 3620.0\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "lif layer 1 self.abs_max_v: 4434.0\n",
      "lif layer 1 self.abs_max_v: 4459.5\n",
      "fc layer 3 self.abs_max_out: 468.0\n",
      "fc layer 1 self.abs_max_out: 3561.0\n",
      "fc layer 1 self.abs_max_out: 3677.0\n",
      "fc layer 1 self.abs_max_out: 3769.0\n",
      "fc layer 1 self.abs_max_out: 3850.0\n",
      "lif layer 1 self.abs_max_v: 4461.5\n",
      "lif layer 1 self.abs_max_v: 4822.0\n",
      "lif layer 1 self.abs_max_v: 5016.0\n",
      "lif layer 1 self.abs_max_v: 5239.0\n",
      "lif layer 1 self.abs_max_v: 5440.0\n",
      "fc layer 2 self.abs_max_out: 1906.0\n",
      "lif layer 1 self.abs_max_v: 5594.0\n",
      "lif layer 1 self.abs_max_v: 6137.0\n",
      "fc layer 1 self.abs_max_out: 3900.0\n",
      "fc layer 1 self.abs_max_out: 4073.0\n",
      "fc layer 1 self.abs_max_out: 4089.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 12.114419/ 72.124985, val:  20.83%, val_best:  20.83%, tr:  98.47%, tr_best:  98.47%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.4779%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.6770%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1415  14.454%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1914.0\n",
      "fc layer 2 self.abs_max_out: 1921.0\n",
      "fc layer 2 self.abs_max_out: 1928.0\n",
      "fc layer 2 self.abs_max_out: 1941.0\n",
      "lif layer 2 self.abs_max_v: 3724.0\n",
      "fc layer 2 self.abs_max_out: 1988.0\n",
      "lif layer 1 self.abs_max_v: 6338.5\n",
      "lif layer 1 self.abs_max_v: 6568.0\n",
      "fc layer 2 self.abs_max_out: 2050.0\n",
      "fc layer 2 self.abs_max_out: 2222.0\n",
      "fc layer 1 self.abs_max_out: 4200.0\n",
      "fc layer 1 self.abs_max_out: 4287.0\n",
      "fc layer 1 self.abs_max_out: 4363.0\n",
      "fc layer 3 self.abs_max_out: 519.0\n",
      "fc layer 2 self.abs_max_out: 2223.0\n",
      "fc layer 1 self.abs_max_out: 4398.0\n",
      "fc layer 1 self.abs_max_out: 4483.0\n",
      "fc layer 1 self.abs_max_out: 4687.0\n",
      "fc layer 1 self.abs_max_out: 5038.0\n",
      "lif layer 1 self.abs_max_v: 6661.5\n",
      "lif layer 1 self.abs_max_v: 6785.0\n",
      "lif layer 1 self.abs_max_v: 7003.5\n",
      "lif layer 1 self.abs_max_v: 7204.5\n",
      "lif layer 1 self.abs_max_v: 7688.5\n",
      "lif layer 1 self.abs_max_v: 7978.5\n",
      "lif layer 2 self.abs_max_v: 3876.0\n",
      "fc layer 2 self.abs_max_out: 2255.0\n",
      "fc layer 2 self.abs_max_out: 2308.0\n",
      "fc layer 2 self.abs_max_out: 2331.0\n",
      "fc layer 2 self.abs_max_out: 2376.0\n",
      "fc layer 1 self.abs_max_out: 5322.0\n",
      "fc layer 2 self.abs_max_out: 2461.0\n",
      "lif layer 2 self.abs_max_v: 3877.0\n",
      "lif layer 2 self.abs_max_v: 3878.0\n",
      "lif layer 2 self.abs_max_v: 3884.5\n",
      "lif layer 2 self.abs_max_v: 3995.0\n",
      "lif layer 2 self.abs_max_v: 3999.0\n",
      "fc layer 1 self.abs_max_out: 5354.0\n",
      "lif layer 2 self.abs_max_v: 4014.5\n",
      "lif layer 2 self.abs_max_v: 4022.5\n",
      "lif layer 2 self.abs_max_v: 4026.5\n",
      "fc layer 1 self.abs_max_out: 5396.0\n",
      "lif layer 2 self.abs_max_v: 4027.0\n",
      "lif layer 2 self.abs_max_v: 4040.5\n",
      "lif layer 2 self.abs_max_v: 4052.5\n",
      "fc layer 1 self.abs_max_out: 5733.0\n",
      "lif layer 1 self.abs_max_v: 8916.0\n",
      "lif layer 1 self.abs_max_v: 9677.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 13.248274/ 73.517860, val:  34.58%, val_best:  34.58%, tr:  98.57%, tr_best:  98.57%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2891%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.4124%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 2820  14.402%\n",
      "lif layer 2 self.abs_max_v: 4168.5\n",
      "lif layer 2 self.abs_max_v: 4224.0\n",
      "fc layer 2 self.abs_max_out: 2569.0\n",
      "lif layer 2 self.abs_max_v: 4273.0\n",
      "lif layer 2 self.abs_max_v: 4526.5\n",
      "fc layer 2 self.abs_max_out: 2588.0\n",
      "fc layer 1 self.abs_max_out: 6068.0\n",
      "fc layer 1 self.abs_max_out: 6187.0\n",
      "lif layer 1 self.abs_max_v: 9842.5\n",
      "lif layer 1 self.abs_max_v: 10480.0\n",
      "lif layer 1 self.abs_max_v: 11107.0\n",
      "fc layer 1 self.abs_max_out: 6286.0\n",
      "fc layer 2 self.abs_max_out: 2655.0\n",
      "lif layer 2 self.abs_max_v: 4700.5\n",
      "lif layer 2 self.abs_max_v: 5005.5\n",
      "fc layer 1 self.abs_max_out: 6316.0\n",
      "fc layer 1 self.abs_max_out: 6357.0\n",
      "fc layer 1 self.abs_max_out: 6481.0\n",
      "fc layer 1 self.abs_max_out: 6755.0\n",
      "lif layer 1 self.abs_max_v: 11111.0\n",
      "lif layer 1 self.abs_max_v: 12111.5\n",
      "fc layer 1 self.abs_max_out: 6781.0\n",
      "lif layer 1 self.abs_max_v: 12457.0\n",
      "fc layer 1 self.abs_max_out: 6802.0\n",
      "lif layer 1 self.abs_max_v: 13030.5\n",
      "lif layer 1 self.abs_max_v: 13141.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 12.288868/ 72.514832, val:  29.17%, val_best:  34.58%, tr:  98.06%, tr_best:  98.57%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.4502%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3630%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4356  14.831%\n",
      "fc layer 1 self.abs_max_out: 7058.0\n",
      "fc layer 1 self.abs_max_out: 7364.0\n",
      "fc layer 2 self.abs_max_out: 2684.0\n",
      "fc layer 1 self.abs_max_out: 7473.0\n",
      "lif layer 1 self.abs_max_v: 13613.0\n",
      "lif layer 1 self.abs_max_v: 14142.5\n",
      "lif layer 1 self.abs_max_v: 14244.5\n",
      "fc layer 1 self.abs_max_out: 7775.0\n",
      "fc layer 1 self.abs_max_out: 8100.0\n",
      "lif layer 1 self.abs_max_v: 14955.5\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 11.030262/ 60.804321, val:  22.50%, val_best:  34.58%, tr:  98.06%, tr_best:  98.57%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4765%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 5857  14.957%\n",
      "fc layer 2 self.abs_max_out: 2735.0\n",
      "lif layer 2 self.abs_max_v: 5170.5\n",
      "fc layer 2 self.abs_max_out: 2750.0\n",
      "fc layer 2 self.abs_max_out: 2801.0\n",
      "fc layer 2 self.abs_max_out: 2808.0\n",
      "fc layer 2 self.abs_max_out: 2811.0\n",
      "lif layer 2 self.abs_max_v: 5252.0\n",
      "lif layer 2 self.abs_max_v: 5262.5\n",
      "lif layer 2 self.abs_max_v: 5366.5\n",
      "fc layer 2 self.abs_max_out: 2821.0\n",
      "fc layer 2 self.abs_max_out: 2823.0\n",
      "lif layer 2 self.abs_max_v: 5392.5\n",
      "lif layer 2 self.abs_max_v: 5445.5\n",
      "fc layer 1 self.abs_max_out: 8435.0\n",
      "lif layer 1 self.abs_max_v: 15229.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 10.388762/101.900612, val:  17.50%, val_best:  34.58%, tr:  98.26%, tr_best:  98.57%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6941%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6823%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7343  15.001%\n",
      "fc layer 2 self.abs_max_out: 2833.0\n",
      "lif layer 2 self.abs_max_v: 5450.0\n",
      "fc layer 2 self.abs_max_out: 2936.0\n",
      "lif layer 2 self.abs_max_v: 5589.0\n",
      "lif layer 2 self.abs_max_v: 5609.5\n",
      "fc layer 2 self.abs_max_out: 3021.0\n",
      "lif layer 2 self.abs_max_v: 5686.0\n",
      "lif layer 2 self.abs_max_v: 5809.0\n",
      "lif layer 2 self.abs_max_v: 5870.5\n",
      "lif layer 2 self.abs_max_v: 5901.5\n",
      "fc layer 2 self.abs_max_out: 3025.0\n",
      "fc layer 2 self.abs_max_out: 3028.0\n",
      "fc layer 1 self.abs_max_out: 8983.0\n",
      "fc layer 2 self.abs_max_out: 3119.0\n",
      "fc layer 2 self.abs_max_out: 3180.0\n",
      "lif layer 2 self.abs_max_v: 6070.0\n",
      "lif layer 2 self.abs_max_v: 6215.0\n",
      "lif layer 1 self.abs_max_v: 15921.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 10.094964/ 80.162140, val:  27.50%, val_best:  34.58%, tr:  98.57%, tr_best:  98.57%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2755%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2123%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 8854  15.073%\n",
      "lif layer 2 self.abs_max_v: 6243.0\n",
      "fc layer 2 self.abs_max_out: 3234.0\n",
      "fc layer 2 self.abs_max_out: 3250.0\n",
      "lif layer 2 self.abs_max_v: 6308.5\n",
      "fc layer 1 self.abs_max_out: 9039.0\n",
      "lif layer 1 self.abs_max_v: 16484.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  9.204634/ 74.583511, val:  33.33%, val_best:  34.58%, tr:  98.47%, tr_best:  98.57%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5541%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10311  15.046%\n",
      "fc layer 2 self.abs_max_out: 3274.0\n",
      "fc layer 2 self.abs_max_out: 3290.0\n",
      "fc layer 2 self.abs_max_out: 3329.0\n",
      "fc layer 2 self.abs_max_out: 3332.0\n",
      "lif layer 2 self.abs_max_v: 6478.5\n",
      "lif layer 2 self.abs_max_v: 6533.0\n",
      "fc layer 2 self.abs_max_out: 3342.0\n",
      "fc layer 1 self.abs_max_out: 9091.0\n",
      "fc layer 2 self.abs_max_out: 3349.0\n",
      "fc layer 2 self.abs_max_out: 3403.0\n",
      "fc layer 2 self.abs_max_out: 3414.0\n",
      "fc layer 1 self.abs_max_out: 9271.0\n",
      "lif layer 1 self.abs_max_v: 16910.0\n",
      "lif layer 2 self.abs_max_v: 6575.5\n",
      "lif layer 2 self.abs_max_v: 6644.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  9.096318/ 89.138420, val:  27.08%, val_best:  34.58%, tr:  98.16%, tr_best:  98.57%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5328%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9528%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11737  14.986%\n",
      "fc layer 2 self.abs_max_out: 3429.0\n",
      "fc layer 2 self.abs_max_out: 3458.0\n",
      "fc layer 2 self.abs_max_out: 3465.0\n",
      "fc layer 2 self.abs_max_out: 3506.0\n",
      "fc layer 2 self.abs_max_out: 3511.0\n",
      "fc layer 2 self.abs_max_out: 3562.0\n",
      "fc layer 2 self.abs_max_out: 3667.0\n",
      "lif layer 2 self.abs_max_v: 6667.5\n",
      "lif layer 2 self.abs_max_v: 6710.0\n",
      "fc layer 1 self.abs_max_out: 9663.0\n",
      "lif layer 1 self.abs_max_v: 17840.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  9.223401/ 48.940601, val:  40.42%, val_best:  40.42%, tr:  98.37%, tr_best:  98.57%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8207%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9231%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13228  15.013%\n",
      "lif layer 2 self.abs_max_v: 6874.5\n",
      "lif layer 2 self.abs_max_v: 6935.5\n",
      "fc layer 2 self.abs_max_out: 3674.0\n",
      "fc layer 2 self.abs_max_out: 3731.0\n",
      "fc layer 1 self.abs_max_out: 10035.0\n",
      "lif layer 2 self.abs_max_v: 7167.5\n",
      "lif layer 2 self.abs_max_v: 7278.5\n",
      "fc layer 2 self.abs_max_out: 3794.0\n",
      "lif layer 2 self.abs_max_v: 7433.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.894585/ 50.415188, val:  37.08%, val_best:  40.42%, tr:  98.16%, tr_best:  98.57%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9707%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14674  14.989%\n",
      "fc layer 2 self.abs_max_out: 3800.0\n",
      "fc layer 2 self.abs_max_out: 3842.0\n",
      "lif layer 1 self.abs_max_v: 17914.0\n",
      "fc layer 1 self.abs_max_out: 10621.0\n",
      "lif layer 1 self.abs_max_v: 18906.5\n",
      "fc layer 2 self.abs_max_out: 3960.0\n",
      "fc layer 2 self.abs_max_out: 4068.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.860800/ 65.861519, val:  33.75%, val_best:  40.42%, tr:  98.06%, tr_best:  98.57%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0515%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 16136  14.984%\n",
      "lif layer 2 self.abs_max_v: 7468.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  8.328784/ 62.543400, val:  25.83%, val_best:  40.42%, tr:  98.37%, tr_best:  98.57%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9017%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4006%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 17527  14.919%\n",
      "lif layer 2 self.abs_max_v: 7489.0\n",
      "lif layer 2 self.abs_max_v: 7503.5\n",
      "lif layer 2 self.abs_max_v: 7511.0\n",
      "lif layer 2 self.abs_max_v: 7529.0\n",
      "fc layer 2 self.abs_max_out: 4150.0\n",
      "lif layer 1 self.abs_max_v: 19317.5\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  8.363325/ 46.714729, val:  30.00%, val_best:  40.42%, tr:  98.67%, tr_best:  98.67%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 18939  14.881%\n",
      "fc layer 2 self.abs_max_out: 4327.0\n",
      "lif layer 2 self.abs_max_v: 7574.5\n",
      "lif layer 2 self.abs_max_v: 7609.5\n",
      "lif layer 2 self.abs_max_v: 7627.0\n",
      "lif layer 2 self.abs_max_v: 7732.5\n",
      "lif layer 2 self.abs_max_v: 7848.5\n",
      "lif layer 2 self.abs_max_v: 7906.5\n",
      "lif layer 2 self.abs_max_v: 8074.0\n",
      "fc layer 1 self.abs_max_out: 10938.0\n",
      "lif layer 1 self.abs_max_v: 20210.5\n",
      "fc layer 2 self.abs_max_out: 4401.0\n",
      "lif layer 2 self.abs_max_v: 8136.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  8.803503/ 68.730034, val:  20.00%, val_best:  40.42%, tr:  98.26%, tr_best:  98.67%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5699%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0666%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 20431  14.907%\n",
      "fc layer 2 self.abs_max_out: 4407.0\n",
      "fc layer 1 self.abs_max_out: 10983.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  8.316542/ 57.767643, val:  24.58%, val_best:  40.42%, tr:  98.57%, tr_best:  98.67%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2043%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.5494%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 21849  14.878%\n",
      "fc layer 1 self.abs_max_out: 11101.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  8.164111/ 50.675980, val:  38.75%, val_best:  40.42%, tr:  98.06%, tr_best:  98.67%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1102%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0626%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 23244  14.839%\n",
      "lif layer 2 self.abs_max_v: 8188.5\n",
      "lif layer 2 self.abs_max_v: 8223.5\n",
      "lif layer 2 self.abs_max_v: 8241.0\n",
      "lif layer 2 self.abs_max_v: 8330.5\n",
      "lif layer 2 self.abs_max_v: 8439.5\n",
      "lif layer 2 self.abs_max_v: 8446.0\n",
      "fc layer 1 self.abs_max_out: 11121.0\n",
      "lif layer 1 self.abs_max_v: 20357.0\n",
      "fc layer 1 self.abs_max_out: 11533.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  8.381439/ 61.405003, val:  37.50%, val_best:  40.42%, tr:  98.88%, tr_best:  98.88%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2986%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2194%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 24680  14.829%\n",
      "fc layer 2 self.abs_max_out: 4467.0\n",
      "lif layer 1 self.abs_max_v: 20885.5\n",
      "fc layer 1 self.abs_max_out: 11823.0\n",
      "fc layer 2 self.abs_max_out: 4619.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.806810/ 37.368256, val:  45.83%, val_best:  45.83%, tr:  98.67%, tr_best:  98.88%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7253%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.0144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 26067  14.792%\n",
      "lif layer 1 self.abs_max_v: 21308.0\n",
      "fc layer 1 self.abs_max_out: 12178.0\n",
      "lif layer 1 self.abs_max_v: 21759.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  8.226238/ 41.690460, val:  33.75%, val_best:  45.83%, tr:  98.88%, tr_best:  98.88%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8762%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4881%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 27516  14.793%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.757922/ 45.134666, val:  37.92%, val_best:  45.83%, tr:  98.37%, tr_best:  98.88%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9531%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6537%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 28919  14.770%\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  8.050288/ 58.630531, val:  38.33%, val_best:  45.83%, tr:  98.37%, tr_best:  98.88%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5366%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.1277%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 30334  14.755%\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  8.164522/ 58.815083, val:  29.58%, val_best:  45.83%, tr:  98.77%, tr_best:  98.88%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1484%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.9518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 31789  14.759%\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  8.350122/ 31.261364, val:  51.25%, val_best:  51.25%, tr:  99.28%, tr_best:  99.28%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2127%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6116%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 33241  14.763%\n",
      "fc layer 2 self.abs_max_out: 4644.0\n",
      "lif layer 2 self.abs_max_v: 8498.5\n",
      "lif layer 2 self.abs_max_v: 8736.5\n",
      "fc layer 2 self.abs_max_out: 4711.0\n",
      "lif layer 2 self.abs_max_v: 8802.5\n",
      "fc layer 1 self.abs_max_out: 12478.0\n",
      "lif layer 1 self.abs_max_v: 22347.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  8.132838/ 42.580978, val:  42.50%, val_best:  51.25%, tr:  99.08%, tr_best:  99.28%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8916%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6356%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 34669  14.755%\n",
      "lif layer 2 self.abs_max_v: 8842.0\n",
      "lif layer 2 self.abs_max_v: 8898.0\n",
      "lif layer 2 self.abs_max_v: 8950.5\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  8.395067/ 47.671432, val:  38.75%, val_best:  51.25%, tr:  98.77%, tr_best:  99.28%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8507%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2231%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 36089  14.745%\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  8.613639/ 35.180264, val:  51.67%, val_best:  51.67%, tr:  98.77%, tr_best:  99.28%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8849%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2905%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 37591  14.768%\n",
      "fc layer 1 self.abs_max_out: 12605.0\n",
      "lif layer 1 self.abs_max_v: 22458.0\n",
      "lif layer 2 self.abs_max_v: 8974.5\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  8.635771/ 41.409286, val:  45.00%, val_best:  51.67%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1600%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 39033  14.767%\n",
      "lif layer 2 self.abs_max_v: 9052.5\n",
      "lif layer 2 self.abs_max_v: 9080.5\n",
      "lif layer 2 self.abs_max_v: 9130.5\n",
      "lif layer 2 self.abs_max_v: 9155.5\n",
      "fc layer 2 self.abs_max_out: 4786.0\n",
      "lif layer 2 self.abs_max_v: 9169.5\n",
      "fc layer 2 self.abs_max_out: 4824.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  8.636606/ 44.991043, val:  46.25%, val_best:  51.67%, tr:  98.06%, tr_best:  99.28%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0660%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6888%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 40514  14.780%\n",
      "lif layer 2 self.abs_max_v: 9209.5\n",
      "lif layer 2 self.abs_max_v: 9236.5\n",
      "lif layer 2 self.abs_max_v: 9256.5\n",
      "lif layer 2 self.abs_max_v: 9266.5\n",
      "fc layer 2 self.abs_max_out: 4832.0\n",
      "fc layer 2 self.abs_max_out: 4835.0\n",
      "lif layer 2 self.abs_max_v: 9304.0\n",
      "fc layer 2 self.abs_max_out: 4906.0\n",
      "lif layer 2 self.abs_max_v: 9558.0\n",
      "lif layer 2 self.abs_max_v: 9612.0\n",
      "fc layer 1 self.abs_max_out: 13210.0\n",
      "lif layer 1 self.abs_max_v: 23550.5\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  8.448782/ 58.001743, val:  39.17%, val_best:  51.67%, tr:  98.47%, tr_best:  99.28%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5505%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5749%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 41941  14.773%\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  8.391761/ 60.375931, val:  40.00%, val_best:  51.67%, tr:  98.88%, tr_best:  99.28%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3645%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9597%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 43372  14.767%\n",
      "fc layer 2 self.abs_max_out: 5020.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  8.418264/ 67.462868, val:  32.92%, val_best:  51.67%, tr:  99.08%, tr_best:  99.28%, epoch time: 72.23 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7825%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9954%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 44808  14.764%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  8.528228/ 50.694248, val:  39.17%, val_best:  51.67%, tr:  98.37%, tr_best:  99.28%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6394%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1949%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 46275  14.771%\n",
      "fc layer 2 self.abs_max_out: 5743.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  8.611200/ 43.772659, val:  42.50%, val_best:  51.67%, tr:  98.98%, tr_best:  99.28%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7853%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0205%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 47756  14.782%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  8.540358/ 57.243538, val:  37.92%, val_best:  51.67%, tr:  98.37%, tr_best:  99.28%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0073%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0243%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 49217  14.786%\n",
      "fc layer 1 self.abs_max_out: 13563.0\n",
      "lif layer 1 self.abs_max_v: 24403.5\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  8.076097/ 57.349045, val:  37.50%, val_best:  51.67%, tr:  99.28%, tr_best:  99.28%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0214%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.2339%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 50709  14.799%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  8.107899/ 53.677380, val:  45.42%, val_best:  51.67%, tr:  99.28%, tr_best:  99.28%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0519%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.2544%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 52175  14.804%\n",
      "lif layer 2 self.abs_max_v: 9658.0\n",
      "fc layer 1 self.abs_max_out: 13591.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  8.672999/ 53.382233, val:  42.92%, val_best:  51.67%, tr:  98.67%, tr_best:  99.28%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8992%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 53680  14.819%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  8.162534/ 40.233379, val:  40.00%, val_best:  51.67%, tr:  98.88%, tr_best:  99.28%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1707%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.3942%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 55093  14.809%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  8.575762/ 39.458004, val:  47.50%, val_best:  51.67%, tr:  98.88%, tr_best:  99.28%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.3062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 56577  14.818%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  8.205241/ 57.326065, val:  40.83%, val_best:  51.67%, tr:  99.49%, tr_best:  99.49%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9986%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 58006  14.813%\n",
      "fc layer 1 self.abs_max_out: 13612.0\n",
      "lif layer 1 self.abs_max_v: 24445.5\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  8.512878/ 39.673717, val:  49.17%, val_best:  51.67%, tr:  99.28%, tr_best:  99.49%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1430%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8743%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 59427  14.805%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  8.597711/ 77.945312, val:  31.25%, val_best:  51.67%, tr:  99.08%, tr_best:  99.49%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0407%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7119%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 60859  14.801%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  8.351398/ 42.451618, val:  39.58%, val_best:  51.67%, tr:  98.88%, tr_best:  99.49%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9330%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9043%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 62269  14.792%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  8.122600/ 56.450264, val:  37.08%, val_best:  51.67%, tr:  98.77%, tr_best:  99.49%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1290%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2448%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 63702  14.788%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  8.441994/ 49.134323, val:  39.17%, val_best:  51.67%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9519%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8550%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 65157  14.790%\n",
      "fc layer 1 self.abs_max_out: 13928.0\n",
      "lif layer 1 self.abs_max_v: 25085.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  8.582599/ 54.685539, val:  33.33%, val_best:  51.67%, tr:  98.98%, tr_best:  99.49%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4651%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 66598  14.788%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  8.704902/ 35.813782, val:  45.83%, val_best:  51.67%, tr:  99.08%, tr_best:  99.49%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3428%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4785%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 68051  14.790%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  8.359713/ 58.676495, val:  33.33%, val_best:  51.67%, tr:  98.88%, tr_best:  99.49%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2785%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 69453  14.780%\n",
      "lif layer 1 self.abs_max_v: 25235.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  8.320822/ 40.869431, val:  44.17%, val_best:  51.67%, tr:  98.77%, tr_best:  99.49%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 70866  14.773%\n",
      "lif layer 2 self.abs_max_v: 9666.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  8.443526/ 47.676495, val:  44.58%, val_best:  51.67%, tr:  98.47%, tr_best:  99.49%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4091%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 72306  14.771%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  8.701347/ 60.779305, val:  42.08%, val_best:  51.67%, tr:  98.88%, tr_best:  99.49%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1453%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7645%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 73780  14.777%\n",
      "lif layer 2 self.abs_max_v: 9717.5\n",
      "lif layer 2 self.abs_max_v: 9785.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  8.597920/ 47.698490, val:  47.08%, val_best:  51.67%, tr:  98.16%, tr_best:  99.49%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5943%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1554%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 75259  14.783%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  8.711660/ 53.249363, val:  42.08%, val_best:  51.67%, tr:  99.18%, tr_best:  99.49%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5982%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 76715  14.785%\n",
      "fc layer 1 self.abs_max_out: 14032.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  8.998002/ 75.547440, val:  27.08%, val_best:  51.67%, tr:  98.77%, tr_best:  99.49%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0119%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2053%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 78203  14.793%\n",
      "fc layer 1 self.abs_max_out: 14169.0\n",
      "lif layer 1 self.abs_max_v: 25617.5\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  9.304802/ 42.096123, val:  45.83%, val_best:  51.67%, tr:  99.08%, tr_best:  99.49%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6703%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9393%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 79724  14.806%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  8.850267/ 57.702393, val:  38.75%, val_best:  51.67%, tr:  98.67%, tr_best:  99.49%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2909%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5099%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 81141  14.800%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  9.127547/ 56.673763, val:  32.50%, val_best:  51.67%, tr:  98.88%, tr_best:  99.49%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1824%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4287%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 82592  14.801%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  8.350249/ 48.830040, val:  48.33%, val_best:  51.67%, tr:  99.08%, tr_best:  99.49%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4091%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1453%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 83984  14.791%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  8.303367/ 48.441185, val:  30.42%, val_best:  51.67%, tr:  98.47%, tr_best:  99.49%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2587%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3201%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 85364  14.779%\n",
      "fc layer 1 self.abs_max_out: 14182.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  8.810034/ 50.700039, val:  40.83%, val_best:  51.67%, tr:  99.49%, tr_best:  99.49%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9543%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 86797  14.776%\n",
      "fc layer 2 self.abs_max_out: 6215.0\n",
      "lif layer 2 self.abs_max_v: 10045.0\n",
      "lif layer 2 self.abs_max_v: 10371.0\n",
      "fc layer 1 self.abs_max_out: 14275.0\n",
      "lif layer 1 self.abs_max_v: 25786.5\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  8.628728/ 45.593964, val:  43.33%, val_best:  51.67%, tr:  99.18%, tr_best:  99.49%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9019%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9057%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 88214  14.772%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  8.804688/ 58.998016, val:  35.83%, val_best:  51.67%, tr:  98.77%, tr_best:  99.49%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7635%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9402%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 89676  14.774%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  8.856642/ 54.096527, val:  43.33%, val_best:  51.67%, tr:  98.88%, tr_best:  99.49%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4757%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9084%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 91115  14.773%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  8.413081/ 53.210934, val:  40.00%, val_best:  51.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7629%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 92501  14.763%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  8.553041/ 54.463566, val:  39.17%, val_best:  51.67%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9107%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 93903  14.757%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  9.174582/ 58.154762, val:  40.42%, val_best:  51.67%, tr:  99.08%, tr_best:  99.69%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0424%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6909%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 95320  14.752%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  8.672921/ 65.849228, val:  44.17%, val_best:  51.67%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8110%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 96698  14.742%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  8.730256/ 64.416046, val:  44.58%, val_best:  51.67%, tr:  98.57%, tr_best:  99.69%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1464%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9476%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 98093  14.735%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  9.048798/ 34.655796, val:  55.42%, val_best:  55.42%, tr:  98.88%, tr_best:  99.69%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1203%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6795%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 99520  14.733%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  8.906633/ 66.570747, val:  39.58%, val_best:  55.42%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6741%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1207%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 100926  14.727%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  9.215205/ 47.592556, val:  47.50%, val_best:  55.42%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 102351  14.725%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  8.919414/ 49.518967, val:  41.25%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8888%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 103747  14.718%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  9.136415/ 38.986374, val:  50.00%, val_best:  55.42%, tr:  98.88%, tr_best:  99.69%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1523%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4281%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 105142  14.712%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  8.717423/ 71.505295, val:  37.92%, val_best:  55.42%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5736%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4621%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 106530  14.705%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  9.102679/ 64.488533, val:  40.83%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7815%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3471%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 107912  14.697%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  8.521222/ 52.625782, val:  49.17%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2347%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7918%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 109225  14.680%\n",
      "lif layer 2 self.abs_max_v: 10448.5\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  8.999070/ 49.497768, val:  47.92%, val_best:  55.42%, tr:  99.49%, tr_best:  99.69%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8306%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6424%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 110616  14.674%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  9.378923/ 97.685356, val:  27.50%, val_best:  55.42%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5769%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7235%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 111982  14.665%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  9.304349/ 69.857834, val:  41.67%, val_best:  55.42%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4632%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3182%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 113354  14.656%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  9.523644/ 63.228989, val:  47.50%, val_best:  55.42%, tr:  98.67%, tr_best:  99.69%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2275%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7346%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 114735  14.650%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  9.366217/ 50.087292, val:  45.42%, val_best:  55.42%, tr:  99.59%, tr_best:  99.69%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1491%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9945%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 116085  14.639%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  9.143344/ 88.400215, val:  30.83%, val_best:  55.42%, tr:  98.57%, tr_best:  99.69%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3608%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7282%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 117473  14.633%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  9.749834/ 44.599472, val:  41.67%, val_best:  55.42%, tr:  98.67%, tr_best:  99.69%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0210%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3337%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 118930  14.636%\n",
      "fc layer 2 self.abs_max_out: 6557.0\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  9.041028/ 71.996498, val:  39.17%, val_best:  55.42%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3137%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 120295  14.628%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  9.459148/ 71.243217, val:  42.08%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6425%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8566%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 121700  14.625%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  9.939401/ 67.447655, val:  44.17%, val_best:  55.42%, tr:  99.08%, tr_best:  99.69%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0478%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 123116  14.623%\n",
      "fc layer 2 self.abs_max_out: 6895.0\n",
      "lif layer 2 self.abs_max_v: 11259.5\n",
      "lif layer 2 self.abs_max_v: 11989.0\n",
      "lif layer 2 self.abs_max_v: 12889.5\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  9.286768/ 38.753685, val:  50.00%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3248%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6280%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 124478  14.615%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  9.486023/ 64.383453, val:  48.75%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3006%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8206%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 125900  14.614%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  9.564060/ 67.088562, val:  33.75%, val_best:  55.42%, tr:  98.67%, tr_best:  99.69%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6923%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3115%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 127289  14.609%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  9.544842/ 46.006618, val:  47.50%, val_best:  55.42%, tr:  98.77%, tr_best:  99.69%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4200%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8737%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 128676  14.604%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  9.419816/ 62.741646, val:  46.67%, val_best:  55.42%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4079%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5695%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 130079  14.601%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  9.378752/ 58.974674, val:  44.17%, val_best:  55.42%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5124%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5777%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 131477  14.598%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  9.542365/ 75.581245, val:  35.42%, val_best:  55.42%, tr:  98.88%, tr_best:  99.69%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4280%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3356%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 132851  14.591%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  9.578059/ 74.120987, val:  35.42%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5960%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8583%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 134243  14.588%\n",
      "fc layer 2 self.abs_max_out: 7007.0\n",
      "fc layer 2 self.abs_max_out: 7099.0\n",
      "lif layer 2 self.abs_max_v: 13384.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  9.697616/ 57.393688, val:  39.17%, val_best:  55.42%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7300%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2341%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 135610  14.581%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  9.631086/ 54.674980, val:  52.92%, val_best:  55.42%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6562%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 136948  14.571%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  9.947650/ 61.651638, val:  46.25%, val_best:  55.42%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 138278  14.561%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  9.808451/ 38.662746, val:  53.75%, val_best:  55.42%, tr:  99.49%, tr_best:  99.69%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7882%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4467%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 139628  14.553%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  9.957170/ 61.740051, val:  39.58%, val_best:  55.42%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1430%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2420%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 140987  14.547%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss: 10.088937/ 52.304401, val:  40.83%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4331%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8561%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 142333  14.539%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  9.957949/ 71.908485, val:  33.75%, val_best:  55.42%, tr:  99.59%, tr_best:  99.69%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0688%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3982%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 143648  14.528%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss: 10.117867/ 59.142998, val:  45.42%, val_best:  55.42%, tr:  98.57%, tr_best:  99.69%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1635%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 145025  14.523%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  9.843110/ 75.663445, val:  38.33%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6665%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7691%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 146329  14.511%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  9.831017/ 66.288933, val:  44.58%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9172%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 147658  14.502%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss: 10.115189/ 50.732525, val:  42.50%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0952%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5565%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 148992  14.494%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss: 10.479378/ 57.230289, val:  43.33%, val_best:  55.42%, tr:  98.88%, tr_best:  99.69%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1697%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2886%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 150388  14.492%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  9.712444/ 83.781334, val:  36.25%, val_best:  55.42%, tr:  99.08%, tr_best:  99.69%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0315%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 151717  14.483%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss: 10.643795/ 61.428131, val:  46.67%, val_best:  55.42%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2092%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7036%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 153122  14.482%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss: 10.276608/ 43.386089, val:  49.58%, val_best:  55.42%, tr:  98.67%, tr_best:  99.69%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3607%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 154478  14.476%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  9.878821/ 98.139900, val:  32.08%, val_best:  55.42%, tr:  99.08%, tr_best:  99.69%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3522%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3520%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 155787  14.466%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  9.916893/ 59.205330, val:  42.50%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1307%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 157074  14.454%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss: 10.063039/ 55.664631, val:  41.67%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6086%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 158377  14.444%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss: 10.109359/ 49.046574, val:  55.00%, val_best:  55.42%, tr:  98.88%, tr_best:  99.69%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6851%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3331%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 159715  14.437%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss: 10.329851/ 85.157234, val:  40.00%, val_best:  55.42%, tr:  98.67%, tr_best:  99.69%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3103%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 161107  14.435%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  9.984231/ 95.443268, val:  44.17%, val_best:  55.42%, tr:  98.57%, tr_best:  99.69%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5418%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1117%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 162435  14.428%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss: 10.313245/ 67.217133, val:  41.25%, val_best:  55.42%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7918%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 163762  14.420%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  9.928346/ 61.786785, val:  51.67%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8161%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6814%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 165074  14.412%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss: 10.363821/ 54.444290, val:  50.83%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8725%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 166412  14.405%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss: 10.104834/ 61.075977, val:  42.08%, val_best:  55.42%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7722%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2480%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 167721  14.397%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss: 10.391442/ 72.112022, val:  47.92%, val_best:  55.42%, tr:  99.49%, tr_best:  99.69%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9557%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0621%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 169022  14.387%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss: 10.087776/ 69.262085, val:  46.25%, val_best:  55.42%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8007%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0262%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 170296  14.376%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss: 10.383254/ 70.383881, val:  46.25%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9338%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1704%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 171574  14.365%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss: 10.112529/ 73.772064, val:  42.08%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8643%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2072%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 172897  14.358%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss: 10.359784/ 55.401546, val:  49.17%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3640%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9347%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 174244  14.353%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss: 11.022408/ 55.400482, val:  55.00%, val_best:  55.42%, tr:  98.77%, tr_best:  99.69%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3645%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 175634  14.352%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss: 10.463735/ 71.703186, val:  39.58%, val_best:  55.42%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7349%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2961%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 176954  14.345%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss: 10.983934/ 66.343460, val:  45.00%, val_best:  55.42%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5932%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 178325  14.343%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss: 10.276148/ 74.327332, val:  35.83%, val_best:  55.42%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8204%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6620%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 179609  14.333%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss: 10.451213/ 73.626389, val:  37.08%, val_best:  55.42%, tr:  99.49%, tr_best:  99.69%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0494%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9690%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 180902  14.324%\n",
      "fc layer 3 self.abs_max_out: 545.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss: 10.256634/ 61.776287, val:  41.67%, val_best:  55.42%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0743%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0594%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 182193  14.315%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss: 10.989321/120.000648, val:  36.25%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8148%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8513%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 183527  14.310%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss: 10.828683/ 82.396797, val:  44.58%, val_best:  55.42%, tr:  99.59%, tr_best:  99.69%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9337%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 184884  14.307%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss: 11.344856/ 63.239574, val:  47.08%, val_best:  55.42%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7631%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8737%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 186242  14.304%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss: 11.410481/ 76.629395, val:  43.33%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1002%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8015%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 187609  14.301%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss: 11.271209/ 68.098381, val:  44.17%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9283%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1324%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 188988  14.299%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss: 11.621898/ 57.260864, val:  38.75%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0002%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6779%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 190399  14.300%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss: 10.306900/ 62.623074, val:  45.42%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2069%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9736%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 191708  14.293%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss: 10.568563/ 61.951469, val:  45.42%, val_best:  55.42%, tr:  98.88%, tr_best:  99.69%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1708%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 193054  14.289%\n",
      "fc layer 2 self.abs_max_out: 7447.0\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss: 10.523530/ 67.644966, val:  42.50%, val_best:  55.42%, tr:  98.88%, tr_best:  99.69%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2803%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0160%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 194403  14.286%\n",
      "fc layer 2 self.abs_max_out: 7673.0\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss: 10.900034/ 55.417328, val:  42.92%, val_best:  55.42%, tr:  99.08%, tr_best:  99.69%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5882%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9345%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 195763  14.283%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss: 10.629929/ 92.268692, val:  37.08%, val_best:  55.42%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6641%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 197109  14.279%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss: 10.005976/ 65.213959, val:  45.00%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1376%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 198380  14.270%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss: 11.314497/ 91.024445, val:  42.08%, val_best:  55.42%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8443%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7109%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 199796  14.271%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss: 10.963042/ 55.099754, val:  45.42%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2274%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9512%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 201152  14.269%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss: 10.082438/ 68.424950, val:  41.25%, val_best:  55.42%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8290%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7445%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 202476  14.263%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss: 10.293278/ 74.046944, val:  44.17%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3936%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5803%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 203805  14.259%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss: 10.618250/ 67.454399, val:  50.00%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5114%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6313%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 205176  14.257%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss: 10.569242/ 78.571602, val:  36.67%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2982%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8761%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 206498  14.252%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss: 10.624293/ 83.033394, val:  40.42%, val_best:  55.42%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 207859  14.250%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss: 10.709048/ 78.669098, val:  44.17%, val_best:  55.42%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2895%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2592%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 209220  14.247%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss: 10.916483/ 96.176918, val:  32.50%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6706%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 210558  14.243%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss: 11.226327/ 59.647137, val:  52.92%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9427%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8818%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 211938  14.242%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss: 10.678216/ 70.569656, val:  49.17%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7672%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6941%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 213264  14.238%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss: 11.200220/ 74.672920, val:  35.83%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3011%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0231%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 214647  14.237%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss: 10.759830/ 51.211742, val:  50.42%, val_best:  55.42%, tr:  99.08%, tr_best:  99.69%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5493%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9137%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 215980  14.233%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss: 10.987794/ 67.180000, val:  36.67%, val_best:  55.42%, tr:  98.88%, tr_best:  99.69%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7103%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1304%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 217309  14.229%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss: 11.107691/ 81.477882, val:  40.00%, val_best:  55.42%, tr:  98.88%, tr_best:  99.69%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5005%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.7657%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 218686  14.228%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss: 11.182792/ 43.431606, val:  56.25%, val_best:  56.25%, tr:  99.49%, tr_best:  99.69%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4361%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.0781%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 220041  14.225%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss: 11.072468/ 58.417519, val:  45.42%, val_best:  56.25%, tr:  98.88%, tr_best:  99.69%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7546%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 221360  14.221%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss: 10.868027/ 82.696800, val:  45.00%, val_best:  56.25%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1630%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 222693  14.217%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss: 10.792255/ 62.944332, val:  50.42%, val_best:  56.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 73.19 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3053%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0382%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 223958  14.209%\n",
      "fc layer 3 self.abs_max_out: 565.0\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "fc layer 3 self.abs_max_out: 616.0\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss: 10.913002/ 70.239471, val:  45.00%, val_best:  56.25%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9348%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.9227%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 225239  14.202%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss: 11.434818/ 55.296165, val:  50.42%, val_best:  56.25%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8113%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.9631%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 226603  14.200%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss: 10.675447/ 63.967846, val:  50.42%, val_best:  56.25%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8526%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.2590%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 227869  14.192%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss: 11.062709/ 71.685379, val:  48.33%, val_best:  56.25%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0873%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.9315%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 229185  14.188%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss: 11.193286/ 72.708717, val:  42.08%, val_best:  56.25%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7810%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1905%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 230553  14.187%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss: 11.607845/ 77.561913, val:  40.83%, val_best:  56.25%, tr:  99.08%, tr_best:  99.69%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8803%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6160%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 231886  14.183%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss: 11.480254/ 54.694035, val:  51.67%, val_best:  56.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2777%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2253%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 233225  14.180%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss: 10.790514/ 68.188995, val:  45.83%, val_best:  56.25%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0916%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0591%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 234501  14.173%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss: 10.865774/ 62.572266, val:  45.42%, val_best:  56.25%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5854%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6001%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 235771  14.166%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss: 11.196946/105.356339, val:  31.67%, val_best:  56.25%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1820%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7655%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 237056  14.160%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss: 11.013455/ 68.848495, val:  48.75%, val_best:  56.25%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4213%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3016%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 238376  14.156%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss: 11.611220/ 71.931389, val:  41.67%, val_best:  56.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1986%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.9064%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 239729  14.154%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss: 10.373190/ 87.573112, val:  47.08%, val_best:  56.25%, tr:  99.08%, tr_best:  99.69%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7976%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 240946  14.145%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss: 10.798701/ 55.890118, val:  55.42%, val_best:  56.25%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6420%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 242234  14.139%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [256.0,128.0,64.0,32.0,16.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "\n",
    "        \"learning_rate\": {\"values\": [1.0]}, \n",
    "        # \"lr_factor\": {\"values\": [-6, -7, -8, -9]}, \n",
    "        \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [0]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [0]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [0]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "        \"lif_layer_sg_width2\": {\"values\": [0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]},\n",
    "        \"lif_layer_v_threshold2\": {\"values\": [256.0,128.0,64.0,32.0,16.0]},\n",
    "        \"learning_rate2\": {\"values\": [1.0]},\n",
    "        \"init_scaling_0\": {\"values\": [4/128]},\n",
    "        \"init_scaling_1\": {\"values\": [6/128]},\n",
    "        \"init_scaling_2\": {\"values\": [3/128]},\n",
    "        \n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "        lif_layer_sg_width2  =  wandb.config.lif_layer_sg_width2,\n",
    "        lif_layer_v_threshold2  =  wandb.config.lif_layer_v_threshold2,\n",
    "        learning_rate2  =  wandb.config.learning_rate2,\n",
    "        init_scaling = [wandb.config.init_scaling_0,wandb.config.init_scaling_1,wandb.config.init_scaling_2],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'e1m59f1o'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
