{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16916/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAIhCAYAAACfVbSSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA79UlEQVR4nO3deXhU5f3//9ckIROWJKwJQUKIewQ1mLiw+cWFWAqIKxSVRcCCYZGlCClWFCoRtEgrgiKbyGKkgKBSNJUqqFBiRLAuRQVJUGJkkQBCQmbO7w9Kfp8hAZNx5j7MzPNxXee6zJ0z93nPiPr2de65j8OyLEsAAADwuzC7CwAAAAgVNF4AAACG0HgBAAAYQuMFAABgCI0XAACAITReAAAAhtB4AQAAGELjBQAAYAiNFwAAgCE0XoAXFi5cKIfDUXFEREQoISFBv/vd7/TVV1/ZVtdjjz0mh8Nh2/VPl5+fr6FDh+ryyy9XdHS04uPjdfPNN2v9+vWVzu3fv7/HZ1q3bl21bNlSt956qxYsWKDS0tIaX3/06NFyOBzq1q2bL94OAPxqNF7Ar7BgwQJt2rRJ//znPzVs2DCtWbNGHTp00MGDB+0u7ZywbNkybdmyRQMGDNDq1as1d+5cOZ1O3XTTTVq0aFGl82vXrq1NmzZp06ZNeuONNzRp0iTVrVtXDzzwgNLS0rRnz55qX/vEiRNavHixJGndunX67rvvfPa+AMBrFoAaW7BggSXJysvL8xh//PHHLUnW/Pnzbalr4sSJ1rn0j/UPP/xQaay8vNy64oorrAsuuMBjvF+/flbdunWrnOett96yatWqZV177bXVvvby5cstSVbXrl0tSdYTTzxRrdeVlZVZJ06cqPJ3R48erfb1AaAqJF6AD6Wnp0uSfvjhh4qx48ePa8yYMUpNTVVsbKwaNmyotm3bavXq1ZVe73A4NGzYML388stKSUlRnTp1dOWVV+qNN96odO6bb76p1NRUOZ1OJScn6+mnn66ypuPHjysrK0vJycmKjIzUeeedp6FDh+qnn37yOK9ly5bq1q2b3njjDbVp00a1a9dWSkpKxbUXLlyolJQU1a1bV9dcc40++uijX/w84uLiKo2Fh4crLS1NhYWFv/j6UzIyMvTAAw/o3//+tzZs2FCt18ybN0+RkZFasGCBEhMTtWDBAlmW5XHOu+++K4fDoZdfflljxozReeedJ6fTqa+//lr9+/dXvXr19OmnnyojI0PR0dG66aabJEm5ubnq0aOHmjdvrqioKF144YUaPHiw9u3bVzH3xo0b5XA4tGzZskq1LVq0SA6HQ3l5edX+DAAEBxovwId27dolSbr44osrxkpLS3XgwAH94Q9/0GuvvaZly5apQ4cOuuOOO6q83fbmm29q5syZmjRpklasWKGGDRvq9ttv186dOyvOeeedd9SjRw9FR0frlVde0VNPPaVXX31VCxYs8JjLsizddtttevrpp9WnTx+9+eabGj16tF566SXdeOONldZNbdu2TVlZWRo3bpxWrlyp2NhY3XHHHZo4caLmzp2rKVOmaMmSJTp06JC6deumY8eO1fgzKi8v18aNG9WqVasave7WW2+VpGo1Xnv27NHbb7+tHj16qEmTJurXr5++/vrrM742KytLBQUFev755/X6669XNIxlZWW69dZbdeONN2r16tV6/PHHJUnffPON2rZtq9mzZ+vtt9/Wo48+qn//+9/q0KGDTpw4IUnq2LGj2rRpo+eee67S9WbOnKmrr75aV199dY0+AwBBwO7IDQhEp241bt682Tpx4oR1+PBha926dVbTpk2t66+//oy3qizr5K22EydOWAMHDrTatGnj8TtJVnx8vFVSUlIxVlRUZIWFhVnZ2dkVY9dee63VrFkz69ixYxVjJSUlVsOGDT1uNa5bt86SZE2bNs3jOjk5OZYka86cORVjSUlJVu3ata09e/ZUjH3yySeWJCshIcHjNttrr71mSbLWrFlTnY/Lw4QJEyxJ1muvveYxfrZbjZZlWV988YUlyXrwwQd/8RqTJk2yJFnr1q2zLMuydu7caTkcDqtPnz4e5/3rX/+yJFnXX399pTn69etXrdvGbrfbOnHihLV7925LkrV69eqK3536c7J169aKsS1btliSrJdeeukX3weA4EPiBfwK1113nWrVqqXo6Gj95je/UYMGDbR69WpFRER4nLd8+XK1b99e9erVU0REhGrVqqV58+bpiy++qDTnDTfcoOjo6Iqf4+PjFRcXp927d0uSjh49qry8PN1xxx2KioqqOC86Olrdu3f3mOvUtwf79+/vMX733Xerbt26eueddzzGU1NTdd5551X8nJKSIknq1KmT6tSpU2n8VE3VNXfuXD3xxBMaM2aMevToUaPXWqfdJjzbeaduL3bu3FmSlJycrE6dOmnFihUqKSmp9Jo777zzjPNV9bvi4mINGTJEiYmJFX8/k5KSJMnj72nv3r0VFxfnkXo9++yzatKkiXr16lWt9wMguNB4Ab/CokWLlJeXp/Xr12vw4MH64osv1Lt3b49zVq5cqZ49e+q8887T4sWLtWnTJuXl5WnAgAE6fvx4pTkbNWpUaczpdFbc1jt48KDcbreaNm1a6bzTx/bv36+IiAg1adLEY9zhcKhp06bav3+/x3jDhg09fo6MjDzreFX1n8mCBQs0ePBg/f73v9dTTz1V7dedcqrJa9as2VnPW79+vXbt2qW7775bJSUl+umnn/TTTz+pZ8+e+vnnn6tcc5WQkFDlXHXq1FFMTIzHmNvtVkZGhlauXKmHH35Y77zzjrZs2aLNmzdLksftV6fTqcGDB2vp0qX66aef9OOPP+rVV1/VoEGD5HQ6a/T+AQSHiF8+BcCZpKSkVCyov+GGG+RyuTR37lz9/e9/11133SVJWrx4sZKTk5WTk+Oxx5Y3+1JJUoMGDeRwOFRUVFTpd6ePNWrUSOXl5frxxx89mi/LslRUVGRsjdGCBQs0aNAg9evXT88//7xXe42tWbNG0sn07WzmzZsnSZo+fbqmT59e5e8HDx7sMXameqoa/89//qNt27Zp4cKF6tevX8X4119/XeUcDz74oJ588knNnz9fx48fV3l5uYYMGXLW9wAgeJF4AT40bdo0NWjQQI8++qjcbrekk//xjoyM9PiPeFFRUZXfaqyOU98qXLlypUfidPjwYb3++use5576Ft6p/axOWbFihY4ePVrxe39auHChBg0apPvuu09z5871qunKzc3V3Llz1a5dO3Xo0OGM5x08eFCrVq1S+/bt9a9//avSce+99yovL0//+c9/vH4/p+o/PbF64YUXqjw/ISFBd999t2bNmqXnn39e3bt3V4sWLby+PoDARuIF+FCDBg2UlZWlhx9+WEuXLtV9992nbt26aeXKlcrMzNRdd92lwsJCTZ48WQkJCV7vcj958mT95je/UefOnTVmzBi5XC5NnTpVdevW1YEDByrO69y5s2655RaNGzdOJSUlat++vbZv366JEyeqTZs26tOnj6/eepWWL1+ugQMHKjU1VYMHD9aWLVs8ft+mTRuPBsbtdlfcsistLVVBQYH+8Y9/6NVXX1VKSopeffXVs15vyZIlOn78uEaMGFFlMtaoUSMtWbJE8+bN0zPPPOPVe7r00kt1wQUXaPz48bIsSw0bNtTrr7+u3NzcM77moYce0rXXXitJlb55CiDE2Lu2HwhMZ9pA1bIs69ixY1aLFi2siy66yCovL7csy7KefPJJq2XLlpbT6bRSUlKsF198scrNTiVZQ4cOrTRnUlKS1a9fP4+xNWvWWFdccYUVGRlptWjRwnryySernPPYsWPWuHHjrKSkJKtWrVpWQkKC9eCDD1oHDx6sdI2uXbtWunZVNe3atcuSZD311FNn/Iws6///ZuCZjl27dp3x3Nq1a1stWrSwunfvbs2fP98qLS0967Usy7JSU1OtuLi4s5573XXXWY0bN7ZKS0srvtW4fPnyKms/07csP//8c6tz585WdHS01aBBA+vuu++2CgoKLEnWxIkTq3xNy5YtrZSUlF98DwCCm8OyqvlVIQCAV7Zv364rr7xSzz33nDIzM+0uB4CNaLwAwE+++eYb7d69W3/84x9VUFCgr7/+2mNbDgChh8X1AOAnkydPVufOnXXkyBEtX76cpgsAiRcAAIApJF4AAACG0HgBAAAYQuMFAABgSEBvoOp2u/X9998rOjraq92wAQAIJZZl6fDhw2rWrJnCwsxnL8ePH1dZWZlf5o6MjFRUVJRf5valgG68vv/+eyUmJtpdBgAAAaWwsFDNmzc3es3jx48rOameiopdfpm/adOm2rVr1znffAV04xUdHS1JuvT+RxUeeW5/0KcLu2m/3SV4pfHUSLtL8FrYN3vsLsErh6+/2O4SvLJ46ky7S/Da4NT/Z3cJXvlm0hV2l+CV2t8F7qqXS7p599gvu5w4WqZ/3L6k4r+fJpWVlamo2KXd+S0VE+3bv+clh91KSvtWZWVlNF7+dOr2YnhkVOA1XnWcv3zSOSgiPDDrlqQwR2A2jRG1AuvP9inRPv4Xq0kRjlp2l+CVsHP8PzhnEu4M3D8rteoG5r9X7FyeUy/aoXrRvr2+W4Gz3CigGy8AABBYXJZbLh/vIOqy3L6d0I8C938zAAAAAgyJFwAAMMYtS275NvLy9Xz+ROIFAABgCIkXAAAwxi23fL0iy/cz+g+JFwAAgCEkXgAAwBiXZcll+XZNlq/n8ycSLwAAAENIvAAAgDGh/q1GGi8AAGCMW5ZcIdx4casRAADAEBIvAABgTKjfaiTxAgAAMITECwAAGMN2EgAAADCCxAsAABjj/t/h6zkDhe2J16xZs5ScnKyoqCilpaVp48aNdpcEAADgF7Y2Xjk5ORo5cqQmTJigrVu3qmPHjurSpYsKCgrsLAsAAPiJ63/7ePn6CBS2Nl7Tp0/XwIEDNWjQIKWkpGjGjBlKTEzU7Nmz7SwLAAD4icvyzxEobGu8ysrKlJ+fr4yMDI/xjIwMffjhh1W+prS0VCUlJR4HAABAoLCt8dq3b59cLpfi4+M9xuPj41VUVFTla7KzsxUbG1txJCYmmigVAAD4iNtPR6CwfXG9w+Hw+NmyrEpjp2RlZenQoUMVR2FhoYkSAQAAfMK27SQaN26s8PDwSulWcXFxpRTsFKfTKafTaaI8AADgB2455FLVAcuvmTNQ2JZ4RUZGKi0tTbm5uR7jubm5ateunU1VAQAA+I+tG6iOHj1affr0UXp6utq2bas5c+aooKBAQ4YMsbMsAADgJ27r5OHrOQOFrY1Xr169tH//fk2aNEl79+5V69attXbtWiUlJdlZFgAAgF/Y/sigzMxMZWZm2l0GAAAwwOWHNV6+ns+fbG+8AABA6Aj1xsv27SQAAABCBYkXAAAwxm055LZ8vJ2Ej+fzJxIvAAAAQ0i8AACAMazxAgAAgBEkXgAAwBiXwuTyce7j8uls/kXiBQAAYAiJFwAAMMbyw7carQD6ViONFwAAMIbF9QAAADCCxAsAABjjssLksny8uN7y6XR+ReIFAABgCIkXAAAwxi2H3D7OfdwKnMiLxAsAAMCQoEi8DqcdU1idwOl2Jam+3QV4KfzHQ3aX4LWyFTF2l+CVPQXldpfglaUlrewuwWthLc6zuwSvXPLo53aX4JW/bF9ndwleu+epP9hdQo24yo7bXQLfarS7AAAAgFARFIkXAAAIDP75VmPg3PWi8QIAAMacXFzv21uDvp7Pn7jVCAAAYAiJFwAAMMatMLnYTgIAAAD+RuIFAACMCfXF9SReAAAAhpB4AQAAY9wK45FBAAAA8D8SLwAAYIzLcshl+fiRQT6ez59ovAAAgDEuP2wn4eJWIwAAAE5H4gUAAIxxW2Fy+3g7CTfbSQAAAOB0JF4AAMAY1ngBAADACBIvAABgjFu+3/7B7dPZ/IvECwAAwBASLwAAYIx/HhkUODkSjRcAADDGZYXJ5ePtJHw9nz8FTqUAAAABjsQLAAAY45ZDbvl6cX3gPKuRxAsAAMAQEi8AAGAMa7wAAABgBIkXAAAwxj+PDAqcHClwKgUAAAhwJF4AAMAYt+WQ29ePDPLxfP5E4gUAAGAIiRcAADDG7Yc1XjwyCAAAoApuK0xuH2//4Ov5/ClwKgUAAAhwJF4AAMAYlxxy+fgRP76ez59IvAAAAAwh8QIAAMawxgsAAABGkHgBAABjXPL9miyXT2fzLxIvAAAAQ2i8AACAMafWePn68MasWbOUnJysqKgopaWlaePGjWc9f8mSJbryyitVp04dJSQk6P7779f+/ftrdE0aLwAAYIzLCvPLUVM5OTkaOXKkJkyYoK1bt6pjx47q0qWLCgoKqjz//fffV9++fTVw4EB99tlnWr58ufLy8jRo0KAaXZfGCwAAhJzp06dr4MCBGjRokFJSUjRjxgwlJiZq9uzZVZ6/efNmtWzZUiNGjFBycrI6dOigwYMH66OPPqrRdWm8AACAMZYccvv4sP63WL+kpMTjKC0trbKGsrIy5efnKyMjw2M8IyNDH374YZWvadeunfbs2aO1a9fKsiz98MMP+vvf/66uXbvW6P3TeAEAgKCQmJio2NjYiiM7O7vK8/bt2yeXy6X4+HiP8fj4eBUVFVX5mnbt2mnJkiXq1auXIiMj1bRpU9WvX1/PPvtsjWpkOwkAAGCMt2uyfmlOSSosLFRMTEzFuNPpPOvrHA7PbS0sy6o0dsrnn3+uESNG6NFHH9Utt9yivXv3auzYsRoyZIjmzZtX7VppvAAAQFCIiYnxaLzOpHHjxgoPD6+UbhUXF1dKwU7Jzs5W+/btNXbsWEnSFVdcobp166pjx47685//rISEhGrVGBSN13krIhRRK7Deyu4esXaX4JWUpcV2l+C1Dz69yO4SvJLwr3C7S/DK2y0vs7sErx29pLHdJXilTtGPdpfglZEt29ldgtdOjLe7gppxVb3kySi35ZDb8u0GqjWdLzIyUmlpacrNzdXtt99eMZ6bm6sePXpU+Zqff/5ZERGevUZ4+Ml/P1uWVe1rs8YLAACEnNGjR2vu3LmaP3++vvjiC40aNUoFBQUaMmSIJCkrK0t9+/atOL979+5auXKlZs+erZ07d+qDDz7QiBEjdM0116hZs2bVvm5gxUQAACCguRQml49zH2/m69Wrl/bv369JkyZp7969at26tdauXaukpCRJ0t69ez329Orfv78OHz6smTNnasyYMapfv75uvPFGTZ06tUbXpfECAADGnAu3Gk/JzMxUZmZmlb9buHBhpbHhw4dr+PDhXl3rFG41AgAAGELiBQAAjHErTG4f5z6+ns+fAqdSAACAAEfiBQAAjHFZDrl8vMbL1/P5E4kXAACAISReAADAmHPpW412IPECAAAwhMQLAAAYY1lhcvv4IdmWj+fzJxovAABgjEsOueTjxfU+ns+fAqdFBAAACHAkXgAAwBi35fvF8G7Lp9P5FYkXAACAISReAADAGLcfFtf7ej5/CpxKAQAAAhyJFwAAMMYth9w+/hair+fzJ1sTr+zsbF199dWKjo5WXFycbrvtNv33v/+1syQAAAC/sbXxeu+99zR06FBt3rxZubm5Ki8vV0ZGho4ePWpnWQAAwE9OPSTb10egsPVW47p16zx+XrBggeLi4pSfn6/rr7/epqoAAIC/hPri+nNqjdehQ4ckSQ0bNqzy96WlpSotLa34uaSkxEhdAAAAvnDOtIiWZWn06NHq0KGDWrduXeU52dnZio2NrTgSExMNVwkAAH4NtxxyWz4+WFxfc8OGDdP27du1bNmyM56TlZWlQ4cOVRyFhYUGKwQAAPh1zolbjcOHD9eaNWu0YcMGNW/e/IznOZ1OOZ1Og5UBAABfsvywnYQVQImXrY2XZVkaPny4Vq1apXfffVfJycl2lgMAAOBXtjZeQ4cO1dKlS7V69WpFR0erqKhIkhQbG6vatWvbWRoAAPCDU+uyfD1noLB1jdfs2bN16NAhderUSQkJCRVHTk6OnWUBAAD4he23GgEAQOhgHy8AAABDuNUIAAAAI0i8AACAMW4/bCfBBqoAAACohMQLAAAYwxovAAAAGEHiBQAAjCHxAgAAgBEkXgAAwJhQT7xovAAAgDGh3nhxqxEAAMAQEi8AAGCMJd9veBpIT34m8QIAADCExAsAABjDGi8AAAAYQeIFAACMCfXEKygarzpbCxURFml3GTXi7Bdvdwle+eCzi+wuwWv1mx62uwSv1P/XAbtL8MqU7NftLsFrf9zUxe4SvHLg1lZ2l+AVlzNw/qN5uvpfuewuoUbKTwRWvcEoKBovAAAQGEi8AAAADAn1xovF9QAAAIaQeAEAAGMsyyHLxwmVr+fzJxIvAAAAQ0i8AACAMW45fP7IIF/P508kXgAAAIaQeAEAAGP4ViMAAACMIPECAADG8K1GAAAAGEHiBQAAjAn1NV40XgAAwBhuNQIAAMAIEi8AAGCM5YdbjSReAAAAqITECwAAGGNJsizfzxkoSLwAAAAMIfECAADGuOWQg4dkAwAAwN9IvAAAgDGhvo8XjRcAADDGbTnkCOGd67nVCAAAYAiJFwAAMMay/LCdRADtJ0HiBQAAYAiJFwAAMCbUF9eTeAEAABhC4gUAAIwh8QIAAIARJF4AAMCYUN/Hi8YLAAAYw3YSAAAAMILECwAAGHMy8fL14nqfTudXJF4AAACGkHgBAABj2E4CAAAARpB4AQAAY6z/Hb6eM1CQeAEAABhC4gUAAIwJ9TVeNF4AAMCcEL/XyK1GAAAQkmbNmqXk5GRFRUUpLS1NGzduPOv5paWlmjBhgpKSkuR0OnXBBRdo/vz5NbomiRcAADDHD7ca5cV8OTk5GjlypGbNmqX27dvrhRdeUJcuXfT555+rRYsWVb6mZ8+e+uGHHzRv3jxdeOGFKi4uVnl5eY2uS+MFAABCzvTp0zVw4EANGjRIkjRjxgy99dZbmj17trKzsyudv27dOr333nvauXOnGjZsKElq2bJlja/LrUYAAGDMqYdk+/qQpJKSEo+jtLS0yhrKysqUn5+vjIwMj/GMjAx9+OGHVb5mzZo1Sk9P17Rp03Teeefp4osv1h/+8AcdO3asRu+fxAsAAASFxMREj58nTpyoxx57rNJ5+/btk8vlUnx8vMd4fHy8ioqKqpx7586dev/99xUVFaVVq1Zp3759yszM1IEDB2q0zisoGi+raSNZ4U67y6iRxGcD9KN/rOo/kIGgVmZg/RmpcIb/YzvXXR5Zy+4SvFbU8xK7S/CK65af7C7BK8eORdpdgtfea/+c3SXUyOHDbqWstrcGf24nUVhYqJiYmIpxp/Ps/953ODzrsCyr0tgpbrdbDodDS5YsUWxsrKSTtyvvuusuPffcc6pdu3a1auVWIwAACAoxMTEex5kar8aNGys8PLxSulVcXFwpBTslISFB5513XkXTJUkpKSmyLEt79uypdo00XgAAwBzL4Z+jBiIjI5WWlqbc3FyP8dzcXLVr167K17Rv317ff/+9jhw5UjG2Y8cOhYWFqXnz5tW+No0XAAAwxp+L62ti9OjRmjt3rubPn68vvvhCo0aNUkFBgYYMGSJJysrKUt++fSvOv+eee9SoUSPdf//9+vzzz7VhwwaNHTtWAwYMqPZtRilI1ngBAADURK9evbR//35NmjRJe/fuVevWrbV27VolJSVJkvbu3auCgoKK8+vVq6fc3FwNHz5c6enpatSokXr27Kk///nPNboujRcAADDnHHpkUGZmpjIzM6v83cKFCyuNXXrppZVuT9YUtxoBAAAMIfECAADG+HM7iUBA4gUAAGAIiRcAADDL12u8AgiJFwAAgCEkXgAAwJhQX+NF4wUAAMw5h7aTsAO3GgEAAAwh8QIAAAY5/nf4es7AQOIFAABgCIkXAAAwhzVeAAAAMIHECwAAmEPiBQAAABPOmcYrOztbDodDI0eOtLsUAADgL5bDP0eAOCduNebl5WnOnDm64oor7C4FAAD4kWWdPHw9Z6CwPfE6cuSI7r33Xr344otq0KCB3eUAAAD4je2N19ChQ9W1a1fdfPPNv3huaWmpSkpKPA4AABBALD8dAcLWW42vvPKKPv74Y+Xl5VXr/OzsbD3++ON+rgoAAMA/bEu8CgsL9dBDD2nx4sWKioqq1muysrJ06NChiqOwsNDPVQIAAJ9icb098vPzVVxcrLS0tIoxl8ulDRs2aObMmSotLVV4eLjHa5xOp5xOp+lSAQAAfMK2xuumm27Sp59+6jF2//3369JLL9W4ceMqNV0AACDwOayTh6/nDBS2NV7R0dFq3bq1x1jdunXVqFGjSuMAAADBoMZrvF566SW9+eabFT8//PDDql+/vtq1a6fdu3f7tDgAABBkQvxbjTVuvKZMmaLatWtLkjZt2qSZM2dq2rRpaty4sUaNGvWrinn33Xc1Y8aMXzUHAAA4h7G4vmYKCwt14YUXSpJee+013XXXXfr973+v9u3bq1OnTr6uDwAAIGjUOPGqV6+e9u/fL0l6++23KzY+jYqK0rFjx3xbHQAACC4hfquxxolX586dNWjQILVp00Y7duxQ165dJUmfffaZWrZs6ev6AAAAgkaNE6/nnntObdu21Y8//qgVK1aoUaNGkk7uy9W7d2+fFwgAAIIIiVfN1K9fXzNnzqw0zqN8AAAAzq5ajdf27dvVunVrhYWFafv27Wc994orrvBJYQAAIAj5I6EKtsQrNTVVRUVFiouLU2pqqhwOhyzr/3+Xp352OBxyuVx+KxYAACCQVavx2rVrl5o0aVLx1wAAAF7xx75bwbaPV1JSUpV/fbr/m4IBAADAU42/1dinTx8dOXKk0vi3336r66+/3idFAQCA4HTqIdm+PgJFjRuvzz//XJdffrk++OCDirGXXnpJV155peLj431aHAAACDJsJ1Ez//73v/XII4/oxhtv1JgxY/TVV19p3bp1+utf/6oBAwb4o0YAAICgUOPGKyIiQk8++aScTqcmT56siIgIvffee2rbtq0/6gMAAAgaNb7VeOLECY0ZM0ZTp05VVlaW2rZtq9tvv11r1671R30AAABBo8aJV3p6un7++We9++67uu6662RZlqZNm6Y77rhDAwYM0KxZs/xRJwAACAIO+X4xfOBsJuFl4/W3v/1NdevWlXRy89Rx48bplltu0X333efzAqtjz82xCndG2XJtb3W6M9/uErzywYI0u0vw2qBVr9tdglf+9ukNdpfglavy7rW7BK+9+PDf7C7BK3/aebvdJXglPr7E7hK89sQPN9ldQo2UHTkh6VW7ywhpNW685s2bV+V4amqq8vMDs5kAAACGsIGq944dO6YTJ054jDmdzl9VEAAAQLCq8eL6o0ePatiwYYqLi1O9evXUoEEDjwMAAOCMQnwfrxo3Xg8//LDWr1+vWbNmyel0au7cuXr88cfVrFkzLVq0yB81AgCAYBHijVeNbzW+/vrrWrRokTp16qQBAwaoY8eOuvDCC5WUlKQlS5bo3nsDd0EtAACAP9U48Tpw4ICSk5MlSTExMTpw4IAkqUOHDtqwYYNvqwMAAEGFZzXW0Pnnn69vv/1WknTZZZfp1VdPfi319ddfV/369X1ZGwAAQFCpceN1//33a9u2bZKkrKysirVeo0aN0tixY31eIAAACCKs8aqZUaNGVfz1DTfcoC+//FIfffSRLrjgAl155ZU+LQ4AACCY/Kp9vCSpRYsWatGihS9qAQAAwc4fCVUAJV41vtUIAAAA7/zqxAsAAKC6/PEtxKD8VuOePXv8WQcAAAgFp57V6OsjQFS78WrdurVefvllf9YCAAAQ1KrdeE2ZMkVDhw7VnXfeqf379/uzJgAAEKxCfDuJajdemZmZ2rZtmw4ePKhWrVppzZo1/qwLAAAg6NRocX1ycrLWr1+vmTNn6s4771RKSooiIjyn+Pjjj31aIAAACB6hvri+xt9q3L17t1asWKGGDRuqR48elRovAAAAVK1GXdOLL76oMWPG6Oabb9Z//vMfNWnSxF91AQCAYBTiG6hWu/H6zW9+oy1btmjmzJnq27evP2sCAAAIStVuvFwul7Zv367mzZv7sx4AABDM/LDGKygTr9zcXH/WAQAAQkGI32rkWY0AAACG8JVEAABgDokXAAAATCDxAgAAxoT6BqokXgAAAIbQeAEAABhC4wUAAGAIa7wAAIA5If6tRhovAABgDIvrAQAAYASJFwAAMCuAEipfI/ECAAAwhMQLAACYE+KL60m8AAAADCHxAgAAxvCtRgAAABhB4gUAAMwJ8TVeNF4AAMAYbjUCAADACBovAABgjuWnwwuzZs1ScnKyoqKilJaWpo0bN1brdR988IEiIiKUmppa42vSeAEAgJCTk5OjkSNHasKECdq6das6duyoLl26qKCg4KyvO3TokPr27aubbrrJq+vSeAEAAHPOkcRr+vTpGjhwoAYNGqSUlBTNmDFDiYmJmj179llfN3jwYN1zzz1q27ZtzS8qGi8AABAkSkpKPI7S0tIqzysrK1N+fr4yMjI8xjMyMvThhx+ecf4FCxbom2++0cSJE72ukcYLAAAYc+pbjb4+JCkxMVGxsbEVR3Z2dpU17Nu3Ty6XS/Hx8R7j8fHxKioqqvI1X331lcaPH68lS5YoIsL7TSGCYjuJsKt/Ulgdp91l1Minj19pdwleiX/rI7tL8FrO913sLsErEX2O2l2CV+Zf+ZLdJXjtwlouu0vwyi3xn9tdgleG1P/S7hK8dv1jD9ldQo24yo5LetXuMvymsLBQMTExFT87nWfvDRwOh8fPlmVVGpMkl8ule+65R48//rguvvjiX1VjUDReAAAgQPhxA9WYmBiPxutMGjdurPDw8ErpVnFxcaUUTJIOHz6sjz76SFu3btWwYcMkSW63W5ZlKSIiQm+//bZuvPHGapVK4wUAAMw5B3auj4yMVFpamnJzc3X77bdXjOfm5qpHjx6Vzo+JidGnn37qMTZr1iytX79ef//735WcnFzta9N4AQCAkDN69Gj16dNH6enpatu2rebMmaOCggINGTJEkpSVlaXvvvtOixYtUlhYmFq3bu3x+ri4OEVFRVUa/yU0XgAAwJhz5ZFBvXr10v79+zVp0iTt3btXrVu31tq1a5WUlCRJ2rt37y/u6eUNGi8AABCSMjMzlZmZWeXvFi5ceNbXPvbYY3rsscdqfE0aLwAAYM45sMbLTuzjBQAAYAiJFwAAMOZcWeNlFxIvAAAAQ0i8AACAOSG+xovGCwAAmBPijRe3GgEAAAwh8QIAAMY4/nf4es5AQeIFAABgCIkXAAAwhzVeAAAAMIHECwAAGMMGqgAAADDC9sbru+++03333adGjRqpTp06Sk1NVX5+vt1lAQAAf7D8dAQIW281Hjx4UO3bt9cNN9ygf/zjH4qLi9M333yj+vXr21kWAADwpwBqlHzN1sZr6tSpSkxM1IIFCyrGWrZsaV9BAAAAfmTrrcY1a9YoPT1dd999t+Li4tSmTRu9+OKLZzy/tLRUJSUlHgcAAAgcpxbX+/oIFLY2Xjt37tTs2bN10UUX6a233tKQIUM0YsQILVq0qMrzs7OzFRsbW3EkJiYarhgAAMB7tjZebrdbV111laZMmaI2bdpo8ODBeuCBBzR79uwqz8/KytKhQ4cqjsLCQsMVAwCAXyXEF9fb2nglJCTosssu8xhLSUlRQUFBlec7nU7FxMR4HAAAAIHC1sX17du313//+1+PsR07digpKcmmigAAgD+xgaqNRo0apc2bN2vKlCn6+uuvtXTpUs2ZM0dDhw61sywAAAC/sLXxuvrqq7Vq1SotW7ZMrVu31uTJkzVjxgzde++9dpYFAAD8JcTXeNn+rMZu3bqpW7dudpcBAADgd7Y3XgAAIHSE+hovGi8AAGCOP24NBlDjZftDsgEAAEIFiRcAADCHxAsAAAAmkHgBAABjQn1xPYkXAACAISReAADAHNZ4AQAAwAQSLwAAYIzDsuSwfBtR+Xo+f6LxAgAA5nCrEQAAACaQeAEAAGPYTgIAAABGkHgBAABzWOMFAAAAE4Ii8Tq2M0ZhUVF2l1EjtRs47C7BKz+OS7e7BK+1eOuw3SV45WBYAP2v3P/xaWlzu0vw2h973WJ3CV45llDb7hK80mzaQbtL8FrcpgN2l1Aj5a5Su0tgjZfdBQAAAISKoEi8AABAgAjxNV40XgAAwBhuNQIAAMAIEi8AAGBOiN9qJPECAAAwhMQLAAAYFUhrsnyNxAsAAMAQEi8AAGCOZZ08fD1ngCDxAgAAMITECwAAGBPq+3jReAEAAHPYTgIAAAAmkHgBAABjHO6Th6/nDBQkXgAAAIaQeAEAAHNY4wUAAAATSLwAAIAxob6dBIkXAACAISReAADAnBB/ZBCNFwAAMIZbjQAAADCCxAsAAJjDdhIAAAAwgcQLAAAYwxovAAAAGEHiBQAAzAnx7SRIvAAAAAwh8QIAAMaE+hovGi8AAGAO20kAAADABBIvAABgTKjfaiTxAgAAMITECwAAmOO2Th6+njNAkHgBAAAYQuIFAADM4VuNAAAAMIHECwAAGOOQH77V6Nvp/IrGCwAAmMOzGgEAAGACiRcAADCGDVQBAABgBI0XAAAwx/LT4YVZs2YpOTlZUVFRSktL08aNG8947sqVK9W5c2c1adJEMTExatu2rd56660aX5PGCwAAhJycnByNHDlSEyZM0NatW9WxY0d16dJFBQUFVZ6/YcMGde7cWWvXrlV+fr5uuOEGde/eXVu3bq3RdVnjBQAAjHFYlhw+/haiN/NNnz5dAwcO1KBBgyRJM2bM0FtvvaXZs2crOzu70vkzZszw+HnKlClavXq1Xn/9dbVp06ba1w2KxuvxLstVJzrc7jJqZGyte+wuwSuXzP/J7hK8VtS+gd0leOXI3nK7S/DK4z91t7sEr12c/YPdJXjlDy1W2F2CV7YcvcDuErx2/sJv7S6hRsqOnJA62V2F/5SUlHj87HQ65XQ6K51XVlam/Px8jR8/3mM8IyNDH374YbWu5Xa7dfjwYTVs2LBGNXKrEQAAmOP20yEpMTFRsbGxFUdVyZUk7du3Ty6XS/Hx8R7j8fHxKioqqtbb+Mtf/qKjR4+qZ8+e1X3nkoIk8QIAAIHBn7caCwsLFRMTUzFeVdrl8TqH5573lmVVGqvKsmXL9Nhjj2n16tWKi4urUa00XgAAICjExMR4NF5n0rhxY4WHh1dKt4qLiyulYKfLycnRwIEDtXz5ct188801rpFbjQAAwJxzYDuJyMhIpaWlKTc312M8NzdX7dq1O+Prli1bpv79+2vp0qXq2rVrzS76PyReAAAg5IwePVp9+vRRenq62rZtqzlz5qigoEBDhgyRJGVlZem7777TokWLJJ1suvr27au//vWvuu666yrSstq1ays2Nrba16XxAgAA5pwjD8nu1auX9u/fr0mTJmnv3r1q3bq11q5dq6SkJEnS3r17Pfb0euGFF1ReXq6hQ4dq6NChFeP9+vXTwoULq31dGi8AABCSMjMzlZmZWeXvTm+m3n33XZ9ck8YLAAAYw0OyAQAAYASJFwAAMOccWeNlFxIvAAAAQ0i8AACAMQ73ycPXcwYKGi8AAGAOtxoBAABgAokXAAAwx4tH/FRrzgBB4gUAAGAIiRcAADDGYVly+HhNlq/n8ycSLwAAAENIvAAAgDl8q9E+5eXleuSRR5ScnKzatWvr/PPP16RJk+R2B9CGHAAAANVka+I1depUPf/883rppZfUqlUrffTRR7r//vsVGxurhx56yM7SAACAP1iSfJ2vBE7gZW/jtWnTJvXo0UNdu3aVJLVs2VLLli3TRx99VOX5paWlKi0trfi5pKTESJ0AAMA3WFxvow4dOuidd97Rjh07JEnbtm3T+++/r9/+9rdVnp+dna3Y2NiKIzEx0WS5AAAAv4qtide4ceN06NAhXXrppQoPD5fL5dITTzyh3r17V3l+VlaWRo8eXfFzSUkJzRcAAIHEkh8W1/t2On+ytfHKycnR4sWLtXTpUrVq1UqffPKJRo4cqWbNmqlfv36Vznc6nXI6nTZUCgAA8OvZ2niNHTtW48eP1+9+9ztJ0uWXX67du3crOzu7ysYLAAAEOLaTsM/PP/+ssDDPEsLDw9lOAgAABCVbE6/u3bvriSeeUIsWLdSqVStt3bpV06dP14ABA+wsCwAA+ItbksMPcwYIWxuvZ599Vn/605+UmZmp4uJiNWvWTIMHD9ajjz5qZ1kAAAB+YWvjFR0drRkzZmjGjBl2lgEAAAwJ9X28eFYjAAAwh8X1AAAAMIHECwAAmEPiBQAAABNIvAAAgDkkXgAAADCBxAsAAJgT4huokngBAAAYQuIFAACMYQNVAAAAU1hcDwAAABNIvAAAgDluS3L4OKFyk3gBAADgNCReAADAHNZ4AQAAwAQSLwAAYJAfEi8FTuIVFI1XvbBjqhsWbncZNbLz7uftLsErbS++0+4SvFb7pQDa2vj/KIk+YXcJXrEORtpdgtf+X5Ov7C7BK1O++q3dJXilbE0Tu0vwWpO7C+0uoUbKj5baXULIC4rGCwAABIgQX+NF4wUAAMxxW/L5rUG2kwAAAMDpSLwAAIA5lvvk4es5AwSJFwAAgCEkXgAAwJwQX1xP4gUAAGAIiRcAADCHbzUCAADABBIvAABgToiv8aLxAgAA5ljyQ+Pl2+n8iVuNAAAAhpB4AQAAc0L8ViOJFwAAgCEkXgAAwBy3W5KPH/Hj5pFBAAAAOA2JFwAAMIc1XgAAADCBxAsAAJgT4okXjRcAADCHZzUCAADABBIvAABgjGW5ZVm+3f7B1/P5E4kXAACAISReAADAHMvy/ZqsAFpcT+IFAABgCIkXAAAwx/LDtxpJvAAAAHA6Ei8AAGCO2y05fPwtxAD6ViONFwAAMIdbjQAAADCBxAsAABhjud2yfHyrkQ1UAQAAUAmJFwAAMIc1XgAAADCBxAsAAJjjtiQHiRcAAAD8jMQLAACYY1mSfL2BKokXAAAATkPiBQAAjLHcliwfr/GyAijxovECAADmWG75/lYjG6gCAADgNCReAADAmFC/1UjiBQAAYAiJFwAAMCfE13gFdON1Klr8+YjL5kpqrsQVOH9I/i/X0VK7S/DeieN2V+AV98+B+ZlbxwLzz7gkHT9ywu4SvBKo/3y6ygLzn01JKg+wz7z85zJJ9t6aK9cJnz+qsVyB88+swwqkG6On2bNnjxITE+0uAwCAgFJYWKjmzZsbvebx48eVnJysoqIiv8zftGlT7dq1S1FRUX6Z31cCuvFyu936/vvvFR0dLYfD4dO5S0pKlJiYqMLCQsXExPh0blSNz9wsPm+z+LzN4zOvzLIsHT58WM2aNVNYmPll3sePH1dZWZlf5o6MjDznmy4pwG81hoWF+b1jj4mJ4R9Yw/jMzeLzNovP2zw+c0+xsbG2XTsqKiogmiN/4luNAAAAhtB4AQAAGELjdQZOp1MTJ06U0+m0u5SQwWduFp+3WXze5vGZ41wU0IvrAQAAAgmJFwAAgCE0XgAAAIbQeAEAABhC4wUAAGAIjdcZzJo1S8nJyYqKilJaWpo2btxod0lBKTs7W1dffbWio6MVFxen2267Tf/973/tLitkZGdny+FwaOTIkXaXEtS+++473XfffWrUqJHq1Kmj1NRU5efn211WUCovL9cjjzyi5ORk1a5dW+eff74mTZoktztwnx2K4ELjVYWcnByNHDlSEyZM0NatW9WxY0d16dJFBQUFdpcWdN577z0NHTpUmzdvVm5ursrLy5WRkaGjR4/aXVrQy8vL05w5c3TFFVfYXUpQO3jwoNq3b69atWrpH//4hz7//HP95S9/Uf369e0uLShNnTpVzz//vGbOnKkvvvhC06ZN01NPPaVnn33W7tIASWwnUaVrr71WV111lWbPnl0xlpKSottuu03Z2dk2Vhb8fvzxR8XFxem9997T9ddfb3c5QevIkSO66qqrNGvWLP35z39WamqqZsyYYXdZQWn8+PH64IMPSM0N6datm+Lj4zVv3ryKsTvvvFN16tTRyy+/bGNlwEkkXqcpKytTfn6+MjIyPMYzMjL04Ycf2lRV6Dh06JAkqWHDhjZXEtyGDh2qrl276uabb7a7lKC3Zs0apaen6+6771ZcXJzatGmjF1980e6yglaHDh30zjvvaMeOHZKkbdu26f3339dvf/tbmysDTgroh2T7w759++RyuRQfH+8xHh8fr6KiIpuqCg2WZWn06NHq0KGDWrdubXc5QeuVV17Rxx9/rLy8PLtLCQk7d+7U7NmzNXr0aP3xj3/Uli1bNGLECDmdTvXt29fu8oLOuHHjdOjQIV166aUKDw+Xy+XSE088od69e9tdGiCJxuuMHA6Hx8+WZVUag28NGzZM27dv1/vvv293KUGrsLBQDz30kN5++21FRUXZXU5IcLvdSk9P15QpUyRJbdq00WeffabZs2fTePlBTk6OFi9erKVLl6pVq1b65JNPNHLkSDVr1kz9+vWzuzyAxut0jRs3Vnh4eKV0q7i4uFIKBt8ZPny41qxZow0bNqh58+Z2lxO08vPzVVxcrLS0tIoxl8ulDRs2aObMmSotLVV4eLiNFQafhIQEXXbZZR5jKSkpWrFihU0VBbexY8dq/Pjx+t3vfidJuvzyy7V7925lZ2fTeOGcwBqv00RGRiotLU25ubke47m5uWrXrp1NVQUvy7I0bNgwrVy5UuvXr1dycrLdJQW1m266SZ9++qk++eSTiiM9PV333nuvPvnkE5ouP2jfvn2lLVJ27NihpKQkmyoKbj///LPCwjz/0xYeHs52EjhnkHhVYfTo0erTp4/S09PVtm1bzZkzRwUFBRoyZIjdpQWdoUOHaunSpVq9erWio6MrksbY2FjVrl3b5uqCT3R0dKX1c3Xr1lWjRo1YV+cno0aNUrt27TRlyhT17NlTW7Zs0Zw5czRnzhy7SwtK3bt31xNPPKEWLVqoVatW2rp1q6ZPn64BAwbYXRogie0kzmjWrFmaNm2a9u7dq9atW+uZZ55hewM/ONO6uQULFqh///5miwlRnTp1YjsJP3vjjTeUlZWlr776SsnJyRo9erQeeOABu8sKSocPH9af/vQnrVq1SsXFxWrWrJl69+6tRx99VJGRkXaXB9B4AQAAmMIaLwAAAENovAAAAAyh8QIAADCExgsAAMAQGi8AAABDaLwAAAAMofECAAAwhMYLAADAEBovALZzOBx67bXX7C4DAPyOxguAXC6X2rVrpzvvvNNj/NChQ0pMTNQjjzzi1+vv3btXXbp08es1AOBcwCODAEiSvvrqK6WmpmrOnDm69957JUl9+/bVtm3blJeXx3PuAMAHSLwASJIuuugiZWdna/jw4fr++++1evVqvfLKK3rppZfO2nQtXrxY6enpio6OVtOmTXXPPfeouLi44veTJk1Ss2bNtH///oqxW2+9Vddff73cbrckz1uNZWVlGjZsmBISEhQVFaWWLVsqOzvbP28aAAwj8QJQwbIs3XjjjQoPD9enn36q4cOH/+Jtxvnz5yshIUGXXHKJiouLNWrUKDVo0EBr166VdPI2ZseOHRUfH69Vq1bp+eef1/jx47Vt2zYlJSVJOtl4rVq1Srfddpuefvpp/e1vf9OSJUvUokULFRYWqrCwUL179/b7+wcAf6PxAuDhyy+/VEpKii6//HJ9/PHHioiIqNHr8/LydM011+jw4cOqV6+eJGnnzp1KTU1VZmamnn32WY/bmZJn4zVixAh99tln+uc//ymHw+HT9wYAduNWIwAP8+fPV506dbRr1y7t2bPnF8/funWrevTooaSkJEVHR6tTp06SpIKCgopzzj//fD399NOaOnWqunfv7tF0na5///765JNPdMkll2jEiBF6++23f/V7AoBzBY0XgAqbNm3SM888o9WrV6tt27YaOHCgzhaKHz16VBkZGapXr54WL16svLw8rVq1StLJtVr/14YNGxQeHq5vv/1W5eXlZ5zzqquu0q5duzR58mQdO3ZMPXv21F133eWbNwgANqPxAiBJOnbsmPr166fBgwfr5ptv1ty5c5WXl6cXXnjhjK/58ssvtW/fPj355JPq2LGjLr30Uo+F9afk5ORo5cqVevfdd1VYWKjJkyeftZaYmBj16tVLL774onJycrRixQodOHDgV79HALAbjRcASdL48ePldrs1depUSVKLFi30l7/8RWPHjtW3335b5WtatGihyMhIPfvss9q5c6fWrFlTqanas2ePHnzwQU2dOlUdOnTQwoULlZ2drc2bN1c55zPPPKNXXnlFX375pXbs2KHly5eradOmql+/vi/fLgDYgsYLgN577z0999xzWrhwoerWrVsx/sADD6hdu3ZnvOXYpEkTLVy4UMuXL9dll12mJ598Uk8//XTF7y3LUv/+/XXNNddo2LBhkqTOnTtr2LBhuu+++3TkyJFKc9arV09Tp05Venq6rr76an377bdau3atwsL41xWAwMe3GgEAAAzhfyEBAAAMofECAAAwhMYLAADAEBovAAAAQ2i8AAAADKHxAgAAMITGCwAAwBAaLwAAAENovAAAAAyh8QIAADCExgsAAMCQ/w9NVB20G8DWVQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # class CustomCriterion_mseloss(torch.autograd.Function):\n",
    "    #     @staticmethod\n",
    "    #     def forward(ctx, input, target):\n",
    "    #         \"\"\"\n",
    "    #         input: (batch_size, num_classes)\n",
    "    #         target: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "    #         \"\"\"\n",
    "    #         # targetÏùÑ one-hot Î≤°ÌÑ∞Î°ú Î≥ÄÌôò\n",
    "    #         target_one_hot = torch.zeros_like(input)\n",
    "    #         target_one_hot.scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "    #         ctx.save_for_backward(input, target_one_hot)\n",
    "            \n",
    "    #         # MSE Í≥ÑÏÇ∞\n",
    "    #         loss = F.mse_loss(input, target_one_hot, reduction='mean')\n",
    "    #         return loss\n",
    "\n",
    "    #     @staticmethod\n",
    "    #     def backward(ctx, grad_output):\n",
    "    #         input, target_one_hot = ctx.saved_tensors\n",
    "    #         # MSE gradient: 2 * (input - target) / N\n",
    "    #         N = input.numel()\n",
    "    #         grad_input = 2 * (input - target_one_hot) / N\n",
    "    #         # grad_input = grad_input * grad_output  # chain rule\n",
    "    #         return grad_input, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "        \n",
    "    # Wrapper module\n",
    "    class CustomCriterionMSE(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            # return CustomCriterion_mseloss.apply(input, target)\n",
    "\n",
    "            # targetÏùÑ one-hot Î≤°ÌÑ∞Î°ú Î≥ÄÌôò\n",
    "            target_one_hot = torch.zeros_like(input)\n",
    "            target_one_hot.scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # MSE Í≥ÑÏÇ∞\n",
    "            loss = F.mse_loss(input, target_one_hot, reduction='mean')\n",
    "            # print(f'input {input}', f'target_one_hot {target_one_hot}', f'loss {loss}')\n",
    "            return loss\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # criterion = CustomCriterion().to(device)\n",
    "    criterion = CustomCriterionMSE().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for high_seed in [1,2,3]:\n",
    "#     ### my_snn control board (Gesture) ########################\n",
    "#     decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "#     # nda 0.25 # ottt 0.5\n",
    "\n",
    "#     unique_name = 'main'\n",
    "#     run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "#     wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "#     my_snn_system(  devices = \"4\",\n",
    "#                     single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                     unique_name = run_name,\n",
    "#                     my_seed = high_seed,\n",
    "#                     TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                     BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                     IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                     # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                     # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                     which_data = 'DVS_GESTURE_TONIC',\n",
    "#     # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "#     # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                     # CLASS_NUM = 10,\n",
    "#                     data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                     rate_coding = False, # True # False\n",
    "\n",
    "#                     lif_layer_v_init = 0.0,\n",
    "#                     lif_layer_v_decay = decay,\n",
    "#                     lif_layer_v_threshold = 0.25,   #nda 0.5  #ottt 1.0\n",
    "#                     lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                     lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                     # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                     synapse_conv_kernel_size = 3,\n",
    "#                     synapse_conv_stride = 1,\n",
    "#                     synapse_conv_padding = 1,\n",
    "\n",
    "#                     synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                     synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                     # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                     pre_trained = False, # True # False\n",
    "#                     convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                     # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                     # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                     # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                     # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                     # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                     cfg = [200, 200], \n",
    "#                     # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                     # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                     # cfg = ['M', 'M', 64], \n",
    "#                     # cfg = [64, 124, 64, 124],\n",
    "#                     # cfg = ['M','M',512], \n",
    "#                     # cfg = [512], \n",
    "#                     # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                     # cfg = ['M','M',512],\n",
    "#                     # cfg = ['M',200],\n",
    "#                     # cfg = [200,200],\n",
    "#                     # cfg = ['M','M',200,200],\n",
    "#                     # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                     # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                     # cfg = ['M',200,200],\n",
    "#                     # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                     # cfg = [200,200],\n",
    "#                     # cfg = [12], #fc\n",
    "#                     # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                     # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                     # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                     # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                     # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                     # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                     # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                     # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                     # cfg = [20001,10001], # depthwise, separable\n",
    "#                     # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                     # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                     # cfg = [],        \n",
    "                    \n",
    "#                     net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                    \n",
    "#                     pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                     # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                     learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                     epoch_num = 200,\n",
    "#                     tdBN_on = False,  # True # False\n",
    "#                     BN_on = False,  # True # False\n",
    "                    \n",
    "#                     surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                    \n",
    "#                     BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                    \n",
    "#                     optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                     scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                    \n",
    "#                     ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                     dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                     # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                     # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                     dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                     # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                     # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                     # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                     # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                     DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                     trace_on = False,   # True # False\n",
    "#                     OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                     exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                     merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                     denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                     extra_train_dataset = -1, \n",
    "\n",
    "#                     num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                     chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                     pin_memory = True, # True # False \n",
    "\n",
    "#                     UDA_on = False,  # DECREPATED # uda\n",
    "#                     alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                     bias = False, # True # False \n",
    "\n",
    "#                     last_lif = False, # True # False \n",
    "\n",
    "#                     temporal_filter = 5, \n",
    "#                     initial_pooling = 1,\n",
    "\n",
    "#                     temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                     quantize_bit_list=[],\n",
    "#                     scale_exp=[[999,998],[999,999],[999,999]], # [[neuron_quant,feedback weight quant],[],[]]\n",
    "#     # 1w -11~-9\n",
    "#     # 1b -11~ -7\n",
    "#     # 2w -10~-8\n",
    "#     # 2b -10~-8\n",
    "#     # 3w -10\n",
    "#     # 3b -10\n",
    "#                     ) \n",
    "\n",
    "#     # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "#     # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "#     # num_workers = batch_size / num_GPU\n",
    "#     # num_workers = batch_size / num_CPU\n",
    "\n",
    "#     # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "#     # average pooling  \n",
    "#     # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "#     # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "#     wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 4a2lz39l\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uu5s53eb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 30758\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251119_185519-uu5s53eb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/uu5s53eb' target=\"_blank\">cool-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/uu5s53eb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/uu5s53eb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251119_185528_734', 'my_seed': 30758, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.03125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[999, 998], [999, 999], [999, 999]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 0\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 0\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.03125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.03125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([-0.1184,  0.0152, -0.1097,  0.0614, -0.1814,  0.1033, -0.0400,  0.0870,\n",
      "         0.0883,  0.0209,  0.0258,  0.0255,  0.1331, -0.0927,  0.0168, -0.0257,\n",
      "        -0.0723,  0.0892,  0.1011,  0.1592, -0.0666, -0.0071,  0.0797, -0.0063,\n",
      "        -0.0927, -0.0513,  0.0123, -0.0526, -0.0496,  0.0372, -0.0746, -0.1218,\n",
      "        -0.0925, -0.0266,  0.1277,  0.1378, -0.0210,  0.1403, -0.0694, -0.0562,\n",
      "         0.0985,  0.1405, -0.1807,  0.1313,  0.0011, -0.0357,  0.2084, -0.0486,\n",
      "         0.1166,  0.1166, -0.0426, -0.0121,  0.0344,  0.0486, -0.0146, -0.0247,\n",
      "        -0.1620, -0.0884,  0.0435,  0.0557, -0.1239,  0.0399,  0.0808,  0.0216,\n",
      "         0.2101,  0.0405,  0.1222, -0.0750, -0.0129, -0.0337, -0.1712,  0.2017,\n",
      "        -0.1194, -0.1118,  0.1273, -0.1410, -0.0095,  0.0068, -0.0788, -0.0548,\n",
      "        -0.0156, -0.2499,  0.1229,  0.0239,  0.0423,  0.1325, -0.0017, -0.0241,\n",
      "        -0.0058, -0.0003,  0.0701,  0.1339,  0.0400, -0.0743, -0.1013,  0.1726,\n",
      "        -0.0191, -0.1281,  0.0417,  0.1118, -0.0234,  0.1307,  0.1391, -0.0865,\n",
      "        -0.0401, -0.0964,  0.1282,  0.0435, -0.1856,  0.1377, -0.0042, -0.1693,\n",
      "        -0.0005,  0.1067,  0.2037, -0.0444, -0.1330,  0.0251, -0.1530, -0.0666,\n",
      "        -0.0852,  0.0177, -0.0213,  0.0751, -0.0766, -0.1141, -0.0250, -0.0988,\n",
      "         0.0624,  0.1284,  0.0514, -0.0185, -0.0301,  0.1241,  0.1123,  0.0550,\n",
      "         0.0683, -0.0887, -0.0778, -0.1108,  0.0854,  0.0052,  0.1889, -0.0173,\n",
      "        -0.0044,  0.1217, -0.0847,  0.1073,  0.1026, -0.0546, -0.0193,  0.0501,\n",
      "        -0.0154, -0.0341, -0.0381, -0.0113,  0.0889, -0.0226, -0.0065, -0.0286,\n",
      "         0.0292,  0.1774,  0.0462,  0.0574, -0.0016, -0.0462, -0.0822, -0.0954,\n",
      "         0.0373, -0.0741, -0.1753, -0.0183,  0.0388, -0.1012, -0.1068,  0.0952,\n",
      "        -0.0518, -0.0383,  0.0383, -0.0884, -0.0796,  0.0481, -0.0774, -0.1373,\n",
      "         0.1018, -0.0151, -0.0728,  0.0142,  0.0315, -0.1431, -0.0961,  0.0699,\n",
      "         0.0991,  0.0184, -0.0907,  0.1104, -0.0664, -0.0126,  0.1295, -0.1298],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([-2.0831e-02, -4.9342e-02,  1.2348e-01, -3.3352e-02, -1.8192e-02,\n",
      "         1.3411e-01,  2.7760e-02, -1.0326e-01,  3.8517e-02,  1.0528e-01,\n",
      "        -6.6273e-03,  2.7327e-02, -4.7505e-02, -1.2271e-01,  2.3387e-02,\n",
      "        -3.5544e-02,  2.0346e-01,  8.8817e-03, -7.7832e-02,  6.8825e-02,\n",
      "        -1.6553e-01,  1.7015e-01, -1.1771e-02,  6.2901e-02, -1.2091e-01,\n",
      "        -6.5047e-02,  2.5744e-02, -6.1689e-02, -1.4675e-01,  5.8438e-02,\n",
      "         1.8461e-02,  7.0448e-02, -7.1089e-03,  1.1460e-01, -9.2769e-02,\n",
      "        -7.2890e-02, -4.3302e-02, -6.1529e-02,  5.7197e-03,  4.9824e-02,\n",
      "         1.6622e-02,  3.6247e-02, -3.4867e-02,  1.2718e-02,  4.5284e-02,\n",
      "        -1.9946e-01,  7.5211e-02, -2.6743e-02, -4.2310e-02, -3.7355e-02,\n",
      "        -1.5909e-01, -7.5964e-02, -8.7235e-02, -1.4531e-01,  1.0245e-01,\n",
      "         1.0451e-01, -2.4844e-02, -1.2665e-02,  1.6113e-02, -1.0762e-01,\n",
      "         6.1224e-02,  4.4455e-02, -6.2608e-02,  2.0817e-02,  9.6498e-03,\n",
      "        -6.5146e-02, -8.7867e-02,  4.2372e-02, -8.4644e-03,  2.7173e-04,\n",
      "        -8.1360e-02, -8.9382e-02, -1.0401e-02, -5.5326e-02, -7.4111e-02,\n",
      "         6.4078e-02, -1.4744e-01, -5.8387e-02,  3.1698e-02, -7.7345e-02,\n",
      "        -3.5081e-02,  2.6640e-01, -1.4973e-01, -3.4714e-03, -3.6285e-02,\n",
      "         1.4873e-01, -2.2559e-01,  1.1141e-01, -7.1923e-04,  7.8855e-02,\n",
      "         2.9984e-02,  1.3310e-01, -1.0791e-01,  5.5287e-02, -6.5923e-02,\n",
      "         3.4490e-02, -6.8373e-02, -5.9598e-02, -8.2499e-02, -2.7269e-02,\n",
      "        -1.0254e-01,  9.1131e-02,  7.5740e-02, -8.3206e-02,  8.2578e-02,\n",
      "         1.3274e-01, -7.4076e-02, -5.3897e-02, -3.1069e-01,  8.1057e-02,\n",
      "        -4.1452e-02, -4.6157e-02,  6.1689e-02,  5.3516e-02, -7.8378e-02,\n",
      "         3.4092e-02, -9.9914e-02,  8.0011e-02,  1.0654e-01, -8.2368e-03,\n",
      "         4.3832e-02,  2.2840e-01, -2.5297e-02, -1.1827e-01, -5.1365e-02,\n",
      "        -1.4428e-02,  9.9895e-02, -2.6828e-03,  9.6209e-02, -8.8414e-03,\n",
      "        -1.2472e-01, -7.8804e-02,  4.9196e-02,  1.6181e-01, -2.4391e-02,\n",
      "         5.4586e-02, -5.7169e-02,  7.4789e-02,  3.4092e-02, -8.3910e-02,\n",
      "        -9.6605e-02, -1.3551e-01, -1.5749e-01,  5.7673e-02, -9.8732e-02,\n",
      "        -2.3529e-02,  9.3832e-02, -9.1210e-02, -1.1605e-02, -2.6805e-02,\n",
      "         2.0679e-01,  8.5151e-02,  9.2573e-02,  5.2416e-03, -7.3190e-02,\n",
      "        -2.4562e-02, -8.4062e-03, -3.6012e-02,  2.0249e-02,  8.4063e-03,\n",
      "         3.3151e-04, -1.4137e-03, -2.1423e-01, -6.8678e-02,  4.2720e-03,\n",
      "        -1.7514e-01,  6.8211e-02,  5.3063e-02,  6.9158e-02,  2.5833e-02,\n",
      "         5.5996e-02, -1.0560e-01,  4.3905e-02,  1.3651e-01, -1.5003e-02,\n",
      "        -6.9044e-02, -1.9555e-01, -1.1013e-03,  4.5664e-02,  2.8641e-02,\n",
      "         1.4755e-01, -7.3151e-02, -3.1185e-02,  4.8869e-02,  9.7199e-02,\n",
      "         8.2324e-02, -9.5940e-03,  4.2849e-02,  1.4211e-01,  2.4460e-01,\n",
      "        -1.2965e-02,  3.7444e-02,  1.9540e-01,  2.0452e-01, -1.3587e-03,\n",
      "         1.4464e-01,  4.5493e-02, -2.1016e-02,  1.1221e-01, -8.0557e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 0.1482, -0.1993,  0.0258,  0.1731,  0.0544, -0.0446, -0.0008, -0.0971,\n",
      "        -0.0604, -0.0381,  0.0166,  0.0998,  0.0818,  0.1198,  0.0416, -0.2035,\n",
      "         0.0739,  0.0057,  0.0654,  0.0259, -0.0756,  0.0420, -0.1187,  0.0180,\n",
      "         0.0168, -0.0068, -0.0185, -0.1610,  0.0344,  0.1898, -0.0047, -0.0235,\n",
      "         0.0815, -0.0291, -0.0155, -0.1136,  0.1577,  0.0249,  0.0861,  0.0290,\n",
      "         0.1202,  0.0125, -0.0816, -0.0057, -0.0010,  0.0021,  0.0552, -0.1195,\n",
      "        -0.0498,  0.0206, -0.0521,  0.1331, -0.0250,  0.0410,  0.1043,  0.0687,\n",
      "        -0.1572, -0.0202,  0.1673, -0.0148, -0.0925, -0.2229,  0.0401,  0.0490,\n",
      "        -0.0159,  0.0543, -0.0149, -0.1446, -0.1416,  0.0344,  0.1448,  0.1191,\n",
      "         0.0375,  0.1699, -0.0113,  0.2343,  0.0042, -0.1921, -0.0060, -0.0713,\n",
      "        -0.0914,  0.1301,  0.2318,  0.0447, -0.0014, -0.0516,  0.1175, -0.0422,\n",
      "         0.0162, -0.0977,  0.0749,  0.0129,  0.0886, -0.0333,  0.1634, -0.0299,\n",
      "         0.0588,  0.1020, -0.0707,  0.0567, -0.0152, -0.0063,  0.0954, -0.0763,\n",
      "         0.0005, -0.1852,  0.2258, -0.1480,  0.0358, -0.0154,  0.0012, -0.0481,\n",
      "         0.0310,  0.0050,  0.0774, -0.0719,  0.0168,  0.0097,  0.2977,  0.1777,\n",
      "        -0.1428,  0.0488,  0.0446, -0.0563, -0.1025, -0.0041,  0.0043, -0.0620,\n",
      "        -0.0633,  0.1124, -0.0145, -0.1286, -0.0140, -0.2132, -0.1350,  0.0275,\n",
      "        -0.1065,  0.0964,  0.0299,  0.0879,  0.0223,  0.0662, -0.0648, -0.2658,\n",
      "         0.1121, -0.0656, -0.1439, -0.0360,  0.0861,  0.0524,  0.0220,  0.1081,\n",
      "         0.0930,  0.0135,  0.1606, -0.0175, -0.0012,  0.0386, -0.0095, -0.0718,\n",
      "        -0.0360,  0.1513,  0.0003, -0.0154,  0.0236, -0.1124,  0.1872, -0.1109,\n",
      "        -0.1933,  0.1199,  0.1168, -0.1372,  0.0511, -0.1691,  0.0592, -0.0941,\n",
      "         0.0754, -0.0569,  0.0874, -0.0886,  0.0009, -0.0306, -0.0816, -0.1412,\n",
      "        -0.0316, -0.0961,  0.0275,  0.0268,  0.0825,  0.0358, -0.0454, -0.0869,\n",
      "        -0.0289,  0.0013, -0.1732,  0.0225, -0.0389,  0.1467,  0.1575, -0.0197],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([ 6.0225e-02, -1.0968e-01,  1.1086e-01,  8.8982e-02,  2.2672e-02,\n",
      "         8.2571e-02,  5.3883e-02, -9.1131e-02, -1.4277e-01, -2.4574e-02,\n",
      "        -1.1952e-01,  9.9059e-02,  7.8678e-02,  1.3606e-02,  9.4556e-02,\n",
      "         5.8502e-02,  1.6081e-01,  2.4182e-01,  1.3162e-01,  8.6727e-02,\n",
      "        -7.1797e-02, -1.8802e-02,  3.8934e-02,  7.5022e-02, -1.0588e-01,\n",
      "        -3.6679e-02,  5.7308e-02,  5.7297e-02,  3.5845e-02,  1.4817e-01,\n",
      "         8.1440e-02,  2.9746e-02, -1.3422e-01, -1.2034e-01,  8.5687e-02,\n",
      "        -3.9181e-02,  4.6347e-02,  1.1861e-01,  5.3164e-02, -7.7852e-02,\n",
      "        -2.1453e-01, -1.1815e-01, -1.8640e-02, -1.2650e-01, -1.1736e-01,\n",
      "         8.1927e-02, -1.0870e-01, -7.9236e-02,  1.6652e-01, -1.3141e-01,\n",
      "        -7.3054e-02, -6.5814e-02,  5.1707e-02, -9.6167e-02, -3.5650e-02,\n",
      "         5.1156e-02, -7.3773e-02,  5.2305e-02,  1.4563e-01,  4.1648e-02,\n",
      "        -1.5054e-01, -2.0042e-03, -1.6145e-01, -4.2471e-02, -3.5657e-02,\n",
      "         1.2850e-02, -3.1742e-02, -8.0938e-02, -2.8800e-02, -3.9349e-02,\n",
      "         6.1521e-02, -1.2109e-01, -4.6230e-03,  3.1744e-02, -7.6218e-02,\n",
      "         6.7383e-02,  4.3466e-02,  3.7805e-02, -4.9242e-02,  1.8549e-02,\n",
      "         8.7013e-02,  1.7140e-01,  4.2860e-02, -1.9283e-01,  3.6685e-02,\n",
      "         1.0428e-01,  1.8011e-03,  8.5004e-02, -7.7510e-02,  3.7085e-02,\n",
      "        -2.4531e-02, -6.9718e-02, -3.7594e-02, -1.7360e-02,  9.5984e-03,\n",
      "         1.3845e-02,  2.4676e-01,  1.0739e-01, -5.0396e-02,  2.4415e-02,\n",
      "        -1.2999e-02,  3.2614e-02, -1.8142e-01, -3.0425e-03, -6.6137e-02,\n",
      "        -1.1693e-01,  2.2126e-01, -1.1843e-02, -1.6758e-01,  9.7765e-02,\n",
      "         1.3838e-01, -6.7439e-02,  2.1118e-01,  2.4842e-02,  6.5635e-02,\n",
      "         6.4303e-02, -1.2749e-01,  4.0972e-02, -1.7156e-02, -1.1553e-01,\n",
      "        -6.4476e-02, -3.3932e-02,  5.9209e-02, -5.6339e-05, -1.9842e-01,\n",
      "        -5.5876e-02, -9.9350e-02,  1.4641e-01,  1.4176e-02, -1.2277e-01,\n",
      "         9.7143e-02, -4.7584e-02,  8.3464e-02, -1.9511e-02,  1.7224e-01,\n",
      "         1.4616e-02, -1.3348e-02,  2.0268e-02,  1.9603e-02,  1.5439e-01,\n",
      "        -1.4285e-02,  5.2909e-02, -8.4945e-02, -4.0839e-02,  7.8987e-02,\n",
      "         9.9383e-02,  1.4691e-02,  8.9789e-02,  1.0115e-01, -1.3530e-02,\n",
      "         7.5092e-02,  1.2844e-01,  2.2843e-02, -1.3798e-01,  6.4952e-02,\n",
      "         1.8905e-02, -3.6876e-02,  6.9818e-02, -7.7443e-02, -3.6532e-02,\n",
      "         2.2213e-02, -2.2487e-02, -1.3105e-02, -2.7653e-02, -7.1398e-03,\n",
      "         2.0858e-01, -2.6928e-02, -1.3272e-01, -6.3310e-02, -2.0257e-01,\n",
      "        -1.8925e-02, -5.0251e-03,  9.6200e-02, -1.2154e-02,  1.0980e-01,\n",
      "         1.0616e-01, -5.0019e-02, -1.0747e-01,  3.0348e-03, -3.2782e-02,\n",
      "        -9.2721e-02,  2.1311e-01,  3.2507e-03, -2.7959e-02, -5.4783e-03,\n",
      "         1.5061e-01,  8.9745e-02,  7.8272e-02, -1.9491e-01,  1.6048e-01,\n",
      "         3.2481e-02, -2.0396e-01,  1.3854e-01,  1.1765e-02, -6.4248e-02,\n",
      "        -6.6840e-02, -9.1527e-02, -7.5734e-03, -4.4016e-03, -1.7659e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([ 0.0235,  0.0480,  0.0043,  0.1704, -0.0685, -0.0244, -0.0644, -0.1811,\n",
      "        -0.0453, -0.1506,  0.0208,  0.1049,  0.0843,  0.0522, -0.0677,  0.0087,\n",
      "        -0.0747, -0.0557, -0.0261, -0.0191, -0.1445,  0.0888, -0.0696,  0.0909,\n",
      "         0.0713, -0.1340,  0.0492, -0.0843,  0.1218,  0.0669,  0.0814, -0.0942,\n",
      "         0.0035, -0.0232,  0.0366, -0.1724, -0.2220, -0.0978, -0.0331, -0.0775,\n",
      "        -0.0444, -0.0891,  0.0568, -0.0539,  0.0167, -0.0958, -0.0789,  0.0988,\n",
      "        -0.0169, -0.0875,  0.1270,  0.0752,  0.0371, -0.1217, -0.1556,  0.1427,\n",
      "         0.0757,  0.0661,  0.0718,  0.0054,  0.1063,  0.2025, -0.1111,  0.0055,\n",
      "        -0.0090, -0.0915, -0.0230,  0.0788,  0.0946, -0.1248, -0.1134,  0.1522,\n",
      "        -0.0668, -0.0800,  0.1290, -0.0361,  0.0148, -0.0494,  0.0549, -0.0179,\n",
      "        -0.0694,  0.0491,  0.1191, -0.0481,  0.0317, -0.0815, -0.0156,  0.0334,\n",
      "         0.0678,  0.1833, -0.0026,  0.1290,  0.0381,  0.0520, -0.0721, -0.0786,\n",
      "         0.0580, -0.1809,  0.1862, -0.0406,  0.0101,  0.0627,  0.1336,  0.2064,\n",
      "        -0.1370,  0.1340, -0.1104, -0.0562,  0.1721, -0.1012,  0.0604, -0.0446,\n",
      "         0.0541, -0.0637, -0.1543, -0.2707, -0.0767, -0.0873, -0.1002,  0.1849,\n",
      "        -0.0152, -0.0392,  0.0787, -0.0591, -0.1880, -0.0425, -0.1088,  0.0145,\n",
      "         0.0219, -0.0427, -0.0187, -0.1187,  0.0263, -0.0258,  0.0242,  0.1192,\n",
      "        -0.0851,  0.0906, -0.0323, -0.0911,  0.1416,  0.0654,  0.0677, -0.0229,\n",
      "         0.0631, -0.1300, -0.1676, -0.1660,  0.1846,  0.0289,  0.1547, -0.0889,\n",
      "        -0.0651,  0.0303,  0.0752,  0.0743,  0.0061, -0.1435,  0.0205, -0.1554,\n",
      "        -0.2072,  0.1391, -0.0729, -0.1718, -0.0571, -0.0649, -0.0308, -0.0291,\n",
      "        -0.0979,  0.0528, -0.0111, -0.0926, -0.0313, -0.0501, -0.0957, -0.0571,\n",
      "         0.0376, -0.0727, -0.0607, -0.0029, -0.1769,  0.0580,  0.0030, -0.0587,\n",
      "         0.0969, -0.0564,  0.1661,  0.0199,  0.1002, -0.0513, -0.0552,  0.1777,\n",
      "        -0.1635,  0.0849, -0.0713,  0.1090,  0.0354, -0.0392,  0.1078, -0.0697],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 0.1364, -0.0471,  0.0011,  0.0243,  0.0748,  0.0533,  0.0700, -0.0504,\n",
      "         0.2096,  0.1093,  0.0122,  0.0575, -0.2188,  0.1129, -0.0636, -0.0413,\n",
      "         0.0554,  0.1136,  0.0128,  0.0316,  0.0297, -0.0113, -0.0646,  0.0815,\n",
      "         0.0609,  0.0053, -0.1020, -0.0867,  0.0914, -0.0854, -0.0097, -0.1019,\n",
      "        -0.0818,  0.0997, -0.0826, -0.0162,  0.0419, -0.0989, -0.1112,  0.1766,\n",
      "         0.0372, -0.0126, -0.1425,  0.1953, -0.0375,  0.1266,  0.2399, -0.0420,\n",
      "         0.0985, -0.0327, -0.1597, -0.1426,  0.0356, -0.0836, -0.0662,  0.1526,\n",
      "         0.0434, -0.0700, -0.0560, -0.0027, -0.0441,  0.0529,  0.1319, -0.0025,\n",
      "         0.0425,  0.0304,  0.0013, -0.0324,  0.0458,  0.0436, -0.1126, -0.1132,\n",
      "         0.0438, -0.0109, -0.1816,  0.0382, -0.1451,  0.0763, -0.0485, -0.2059,\n",
      "         0.1105, -0.1110,  0.0037,  0.0649,  0.0286, -0.0745,  0.0820, -0.1994,\n",
      "        -0.0157, -0.0016, -0.0105,  0.0220, -0.0800, -0.1382,  0.1055,  0.0062,\n",
      "         0.0392,  0.0508, -0.0488, -0.0931, -0.0179,  0.1106,  0.0616, -0.0841,\n",
      "         0.0265, -0.0954, -0.0571, -0.0919,  0.0189,  0.0321,  0.0156, -0.0954,\n",
      "         0.0817,  0.0922,  0.0941, -0.0011, -0.0682,  0.0369,  0.0893, -0.0747,\n",
      "        -0.0389, -0.0004, -0.0865,  0.0506, -0.1100, -0.0802,  0.0311,  0.0031,\n",
      "         0.0539, -0.0135, -0.1652,  0.0129, -0.0823,  0.0439, -0.0246, -0.2007,\n",
      "         0.0164,  0.0929,  0.1649,  0.0803, -0.0448,  0.0344, -0.0732,  0.0747,\n",
      "        -0.0206, -0.0693,  0.0618,  0.0144,  0.0750,  0.1218,  0.0159, -0.1694,\n",
      "         0.1164, -0.0070,  0.0184,  0.0965,  0.0720, -0.1840, -0.0571, -0.0208,\n",
      "        -0.0104,  0.1711,  0.0586,  0.0562,  0.0943, -0.0242,  0.0880, -0.1370,\n",
      "         0.0090, -0.1410,  0.0022,  0.0762,  0.1032,  0.0127, -0.0054, -0.1778,\n",
      "        -0.1177, -0.0134,  0.0345, -0.0510,  0.1403, -0.1097,  0.0390,  0.0100,\n",
      "         0.0933,  0.0244,  0.0781,  0.0367,  0.0635, -0.0304, -0.1654,  0.0897,\n",
      "        -0.0872, -0.0167, -0.0056,  0.1293, -0.0437, -0.1221,  0.0679,  0.0388],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-0.0518, -0.0910,  0.0272, -0.0856, -0.0360, -0.0478,  0.0157,  0.0103,\n",
      "         0.1057,  0.0064, -0.1513, -0.1431,  0.2396,  0.0964, -0.0845,  0.2088,\n",
      "         0.0111, -0.0758,  0.0085,  0.0302, -0.0413, -0.2274, -0.1420, -0.0618,\n",
      "        -0.0634,  0.2173,  0.0473, -0.1651, -0.0427,  0.0714, -0.0409,  0.1113,\n",
      "        -0.0370, -0.1390,  0.0388, -0.1041, -0.0023,  0.0695,  0.0629, -0.2052,\n",
      "         0.2051, -0.0488,  0.0752,  0.0828,  0.0638,  0.0325,  0.0988, -0.0239,\n",
      "        -0.0455,  0.0122,  0.0219, -0.0499,  0.0993,  0.0095,  0.1025,  0.0072,\n",
      "         0.0466,  0.0452, -0.0778,  0.0155, -0.0812,  0.0423,  0.0176,  0.0120,\n",
      "         0.0291, -0.1022,  0.1165,  0.0366, -0.1030,  0.0247, -0.0553,  0.2111,\n",
      "         0.1779, -0.0340,  0.0275,  0.0062,  0.0182, -0.0738,  0.0155,  0.0447,\n",
      "        -0.0550,  0.0003,  0.1080, -0.0439, -0.0135, -0.1561, -0.0865,  0.0377,\n",
      "         0.1614,  0.0729, -0.0124,  0.0010,  0.1562,  0.0509, -0.0377, -0.0205,\n",
      "         0.2583,  0.0448,  0.1072,  0.0317,  0.0852,  0.0639,  0.0066, -0.0034,\n",
      "         0.0314,  0.0449, -0.0063,  0.1561, -0.0765,  0.1349,  0.0590, -0.0880,\n",
      "        -0.0445,  0.0037, -0.0736,  0.1371, -0.0654, -0.0838, -0.0516,  0.0006,\n",
      "        -0.2049, -0.0752,  0.0446,  0.0525, -0.1948, -0.1186, -0.0489,  0.0090,\n",
      "         0.0944,  0.1247, -0.1089, -0.0686, -0.0691, -0.0564,  0.1424,  0.1187,\n",
      "        -0.1329, -0.0891, -0.0698, -0.0655, -0.0236,  0.0043,  0.0328,  0.0025,\n",
      "        -0.0223, -0.0655, -0.1329,  0.0275, -0.0755, -0.0411,  0.1154, -0.0416,\n",
      "         0.0716,  0.0100, -0.1565, -0.0672, -0.0006, -0.0703,  0.0435,  0.0488,\n",
      "        -0.0669,  0.1930, -0.0613, -0.0635,  0.0087, -0.0239,  0.0424, -0.1819,\n",
      "        -0.0329, -0.0356, -0.0217, -0.1287,  0.0431,  0.0800,  0.1415, -0.0894,\n",
      "        -0.0887,  0.1274,  0.0803,  0.0578, -0.1908,  0.1528,  0.0514, -0.0443,\n",
      "         0.0872,  0.0530, -0.1935, -0.1407, -0.1632, -0.2236,  0.1828,  0.0525,\n",
      "         0.0634,  0.0742, -0.0745, -0.0731, -0.0741,  0.0153, -0.1743, -0.1563],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([-0.0260,  0.0546,  0.0691,  0.1073,  0.0481, -0.1544, -0.0036, -0.1145,\n",
      "         0.1028,  0.1822,  0.0936, -0.0330, -0.0812, -0.0880,  0.1718, -0.0029,\n",
      "        -0.0477, -0.0124,  0.0499,  0.0934, -0.1498, -0.0541,  0.0941, -0.0227,\n",
      "         0.0379, -0.0017, -0.0221, -0.0110,  0.0609,  0.0042,  0.0605, -0.0632,\n",
      "         0.1484, -0.0132,  0.0550, -0.0643, -0.1112, -0.0194,  0.1363, -0.1182,\n",
      "         0.1402,  0.0676, -0.0743,  0.0055, -0.0153, -0.0455,  0.1287,  0.0634,\n",
      "        -0.1280,  0.1512, -0.0967, -0.0485, -0.0994,  0.0519,  0.0487, -0.0531,\n",
      "         0.0526,  0.0656, -0.1653, -0.1091,  0.0452,  0.0526, -0.0628,  0.1174,\n",
      "        -0.1508,  0.0489, -0.0621, -0.0651,  0.0219,  0.0902,  0.0053,  0.0728,\n",
      "         0.0615, -0.0216, -0.0086,  0.1080, -0.0277, -0.0498, -0.0062, -0.1831,\n",
      "         0.0386,  0.0908, -0.0173,  0.1274,  0.1347,  0.0044,  0.1019,  0.1233,\n",
      "        -0.0882,  0.0190,  0.0593, -0.0140, -0.1830, -0.0184, -0.0851, -0.0395,\n",
      "        -0.0895,  0.1320, -0.0179, -0.0290, -0.0337, -0.0194,  0.0057,  0.1355,\n",
      "        -0.2286,  0.0938, -0.0545, -0.0596,  0.0222,  0.1371,  0.0592,  0.1992,\n",
      "        -0.2055, -0.0184, -0.0522, -0.0123, -0.1439,  0.0929, -0.2352,  0.1026,\n",
      "        -0.1056,  0.0911, -0.1024, -0.0671, -0.0958, -0.0633, -0.0453, -0.0324,\n",
      "        -0.0913,  0.0974,  0.1081,  0.0144, -0.0356,  0.1546,  0.1548,  0.0025,\n",
      "         0.0863,  0.1380,  0.0471, -0.1318, -0.1011,  0.0368, -0.0389,  0.2040,\n",
      "         0.1938,  0.1045, -0.0639, -0.1034,  0.2134, -0.1362,  0.1195,  0.0751,\n",
      "        -0.1413, -0.0956, -0.2633, -0.0528, -0.1019, -0.1604,  0.0964,  0.0186,\n",
      "        -0.0080, -0.2647,  0.0447, -0.0917, -0.0855,  0.1267, -0.0242, -0.0685,\n",
      "        -0.0064, -0.0178,  0.0425, -0.0321,  0.0898, -0.0442,  0.0490,  0.1742,\n",
      "         0.0949,  0.0776, -0.0729, -0.0219, -0.0589, -0.0161, -0.0893,  0.1507,\n",
      "        -0.0375,  0.0850,  0.0212,  0.0007,  0.0433, -0.0680,  0.1449, -0.1363,\n",
      "         0.0235, -0.0666,  0.0043, -0.0650, -0.0796, -0.0212, -0.1544, -0.0719],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([-2.6236e-02,  2.1141e-01,  4.7750e-02, -8.6510e-02,  1.3700e-01,\n",
      "        -2.7364e-03, -7.1435e-03, -6.3118e-02, -2.9844e-01, -4.2168e-02,\n",
      "         2.0522e-02, -2.3451e-02, -1.2058e-01, -1.1172e-01,  1.2825e-01,\n",
      "         3.4443e-02, -3.4264e-02, -1.5656e-01,  1.5641e-01,  9.0018e-02,\n",
      "         1.2688e-01,  1.2384e-01,  1.2509e-02, -4.3654e-02,  4.1666e-02,\n",
      "         1.0825e-01,  1.0477e-01, -1.0257e-02, -7.2980e-02,  6.8046e-02,\n",
      "        -9.5946e-03,  5.8983e-02, -1.3180e-01, -6.5653e-02, -4.7349e-02,\n",
      "         6.8906e-02,  1.8952e-02,  4.8765e-02,  2.4355e-01, -5.3119e-02,\n",
      "         5.1588e-02, -9.2416e-02,  5.7827e-02,  2.7166e-01, -9.0079e-02,\n",
      "         1.8573e-01, -1.9548e-01, -1.1169e-01,  1.4145e-02, -1.8672e-01,\n",
      "        -6.6282e-02,  1.4193e-01,  1.8605e-01, -6.8205e-02, -2.4480e-01,\n",
      "        -5.8082e-02, -1.0247e-01,  1.1494e-01,  1.9219e-02, -1.4253e-01,\n",
      "         1.0087e-01,  2.8892e-02, -5.7714e-02,  1.1918e-01, -2.3945e-01,\n",
      "         8.3069e-03, -4.8453e-02, -6.0913e-02,  1.9035e-02, -7.9881e-02,\n",
      "        -5.0028e-04, -4.4074e-02,  4.5220e-02,  5.2802e-03, -1.5476e-01,\n",
      "        -3.2885e-03,  6.2867e-02,  8.2810e-02,  4.1091e-02,  1.5996e-02,\n",
      "        -4.2078e-02, -6.2285e-03, -1.5571e-01, -7.8671e-02, -7.7418e-02,\n",
      "         2.7163e-02, -1.4169e-02, -1.3971e-01, -6.1648e-03,  5.4720e-02,\n",
      "         1.5738e-01,  2.0265e-01, -4.1678e-02,  5.6535e-02,  1.2096e-02,\n",
      "         1.1452e-01,  1.7275e-03,  1.3617e-01, -6.6897e-02, -7.6517e-02,\n",
      "        -9.2137e-02,  2.1969e-02,  2.3860e-02, -8.4943e-02, -2.3229e-02,\n",
      "         1.0576e-01,  4.8938e-02, -4.9163e-02,  2.7344e-02, -3.6529e-02,\n",
      "         1.8552e-02, -6.9353e-02,  8.2956e-02,  6.5692e-02, -2.2564e-03,\n",
      "        -6.8636e-04,  1.0794e-02, -6.1943e-02,  8.5250e-02, -1.2359e-02,\n",
      "         1.0540e-01,  1.8168e-02, -1.0703e-01,  5.4529e-02, -5.6456e-02,\n",
      "         1.4256e-02, -6.2882e-02,  3.2136e-02,  7.5694e-02,  1.6276e-01,\n",
      "         1.4964e-01, -5.1629e-02, -1.4631e-02, -2.2536e-02, -7.9450e-02,\n",
      "        -1.1394e-01,  1.0783e-01,  5.6096e-02, -4.3684e-02,  5.6093e-03,\n",
      "         3.6190e-02, -8.7782e-02, -1.3211e-02,  8.6816e-02,  8.5792e-04,\n",
      "        -1.7480e-04,  1.2079e-01,  1.2488e-01,  1.8155e-02,  1.2860e-02,\n",
      "        -1.4873e-02,  4.1697e-03, -2.7870e-02, -4.0917e-02,  7.3786e-03,\n",
      "        -2.4435e-02, -1.2686e-01,  1.0177e-01, -7.7212e-02,  6.3101e-02,\n",
      "        -8.1702e-02,  2.0939e-01,  1.5675e-02,  4.9042e-02,  2.2042e-02,\n",
      "        -6.3737e-02, -2.5307e-02,  1.3828e-01, -6.8854e-02, -2.9313e-02,\n",
      "        -3.1930e-02,  4.6975e-02,  8.5969e-02, -4.6671e-02, -3.5439e-03,\n",
      "         9.0250e-02,  6.7503e-02,  5.3905e-02, -1.7559e-02, -3.7339e-02,\n",
      "        -6.7976e-02, -7.5767e-03, -4.0681e-02,  1.4824e-03,  2.8057e-02,\n",
      "        -9.6556e-02, -1.2604e-01, -1.4316e-02, -6.8739e-02, -4.6006e-02,\n",
      "        -5.9130e-02, -1.4971e-01, -6.0541e-02,  3.5762e-02,  4.3687e-02,\n",
      "         2.3850e-01, -1.6118e-02, -1.4375e-01,  2.4790e-01,  1.4465e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-0.0774,  0.0719,  0.1115, -0.0043, -0.0261, -0.0047, -0.0470, -0.0187,\n",
      "        -0.1288,  0.0363,  0.0935,  0.0498,  0.1112,  0.0333,  0.1882, -0.1067,\n",
      "         0.0529, -0.0289,  0.1541,  0.0258,  0.0475,  0.0460, -0.0839, -0.1359,\n",
      "         0.1143, -0.2087, -0.0105, -0.0355,  0.0064, -0.0241, -0.0715,  0.2299,\n",
      "         0.0407,  0.0378,  0.0144,  0.0576, -0.1955, -0.0845,  0.0051, -0.0096,\n",
      "         0.0081,  0.0076,  0.0046,  0.0056, -0.0072,  0.1170, -0.1011,  0.2010,\n",
      "        -0.0012,  0.1339, -0.0242, -0.0163, -0.2423, -0.0638,  0.0371, -0.1144,\n",
      "         0.1205,  0.0655,  0.0187,  0.0180, -0.0589,  0.0309,  0.1510,  0.2093,\n",
      "        -0.1137,  0.0411,  0.0941, -0.0956,  0.1574, -0.0639,  0.0060, -0.2186,\n",
      "        -0.1669,  0.0352,  0.2078, -0.1644, -0.1476,  0.0311, -0.0037, -0.1424,\n",
      "         0.0417, -0.0283,  0.0080,  0.0468, -0.2133,  0.0800, -0.0031,  0.0402,\n",
      "         0.0913,  0.0241, -0.0259,  0.1122,  0.0981,  0.0571,  0.0825, -0.0569,\n",
      "        -0.0930,  0.0722,  0.1677, -0.1302, -0.0238, -0.1227,  0.1010,  0.0885,\n",
      "         0.1385, -0.0759, -0.2342, -0.0761,  0.1403,  0.0199, -0.1079,  0.0124,\n",
      "        -0.0071, -0.1024, -0.0872,  0.0263, -0.1809, -0.0348,  0.0169,  0.0366,\n",
      "        -0.0781, -0.0723, -0.0863, -0.1285, -0.1293, -0.1695, -0.1695,  0.1685,\n",
      "         0.0600, -0.0149,  0.0740, -0.0687, -0.1722, -0.0781, -0.0480, -0.0225,\n",
      "        -0.0403,  0.0365,  0.0958,  0.0134,  0.1019,  0.0303,  0.0837,  0.0109,\n",
      "         0.0392, -0.0775,  0.0327,  0.0321, -0.1429,  0.0039, -0.0762,  0.1785,\n",
      "        -0.0730, -0.0441, -0.0200,  0.0971,  0.0250, -0.0442,  0.1374,  0.0823,\n",
      "         0.0722,  0.0076,  0.0496, -0.0969, -0.0753,  0.0563, -0.0634,  0.0229,\n",
      "        -0.1665, -0.2167, -0.0627, -0.0633, -0.0058, -0.0311,  0.0078, -0.0137,\n",
      "         0.1761, -0.0321,  0.0885,  0.0026,  0.0645, -0.3068,  0.0982,  0.1084,\n",
      "        -0.2468, -0.0222,  0.0427,  0.1063, -0.1159, -0.0326,  0.0008,  0.1846,\n",
      "         0.0674, -0.0198,  0.1239,  0.0354, -0.0373,  0.1059, -0.0854,  0.0252],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([-0.1372, -0.0073, -0.1904, -0.0289,  0.1143,  0.0899, -0.0904,  0.1798,\n",
      "         0.0082,  0.0298,  0.0238, -0.0851, -0.1690, -0.0502, -0.0726,  0.0662,\n",
      "        -0.0714, -0.0787, -0.0430, -0.0564, -0.0582, -0.1140, -0.0149,  0.0340,\n",
      "         0.0639,  0.1997, -0.1417,  0.0235, -0.0258, -0.0035,  0.1251, -0.0309,\n",
      "        -0.0700,  0.0794,  0.1855, -0.1163, -0.1765, -0.0169, -0.0924, -0.0455,\n",
      "        -0.0407,  0.1867, -0.0279, -0.0557,  0.0799,  0.1373,  0.0184, -0.1876,\n",
      "         0.3638,  0.0322, -0.0674, -0.0466, -0.0623,  0.1662,  0.0225,  0.1587,\n",
      "         0.0996, -0.0763, -0.0477,  0.0958, -0.0018,  0.0220,  0.0672,  0.1276,\n",
      "         0.0658,  0.0649,  0.0509, -0.0566, -0.0229, -0.0880,  0.0815,  0.0223,\n",
      "        -0.0184,  0.1115,  0.0656, -0.0125, -0.1130,  0.0820, -0.0599,  0.0633,\n",
      "         0.0261,  0.0445, -0.0522, -0.0581, -0.1407,  0.0124,  0.1688, -0.0965,\n",
      "        -0.2236,  0.0205,  0.1640,  0.0234, -0.0599,  0.0788,  0.0252, -0.0117,\n",
      "         0.0244,  0.0576,  0.0924,  0.0484,  0.1108, -0.0558, -0.0692, -0.0567,\n",
      "         0.1656,  0.0662, -0.0898,  0.1011,  0.0089,  0.1734, -0.0987, -0.0782,\n",
      "         0.0018, -0.0083, -0.0970, -0.2469, -0.1670,  0.0500, -0.1843, -0.1148,\n",
      "         0.0986, -0.1216, -0.0287, -0.0165,  0.0340,  0.0567, -0.0937, -0.1817,\n",
      "        -0.0392,  0.0478, -0.0495, -0.3104, -0.0172, -0.1216,  0.0764,  0.1345,\n",
      "        -0.2192, -0.0571, -0.1974,  0.0699,  0.0875,  0.1801, -0.0968, -0.0806,\n",
      "         0.0363, -0.1131,  0.2140,  0.1178, -0.1723, -0.0851,  0.1779,  0.0860,\n",
      "         0.0386, -0.0356, -0.0664,  0.0660,  0.0210, -0.0247,  0.3210, -0.0190,\n",
      "        -0.0330,  0.0582, -0.0800, -0.0720,  0.0495, -0.0416,  0.1125,  0.1290,\n",
      "        -0.0474, -0.0624, -0.1343, -0.1096, -0.1204, -0.1043, -0.1056,  0.0682,\n",
      "        -0.0149, -0.0463,  0.0906, -0.0676,  0.0360, -0.0548, -0.0524,  0.1774,\n",
      "         0.0887,  0.1488, -0.0672, -0.1150, -0.0950,  0.0598, -0.0536, -0.0979,\n",
      "        -0.1873, -0.0527,  0.0439,  0.1050,  0.0268,  0.1861, -0.1036,  0.0950],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 0.0562,  0.0906,  0.0468, -0.0492, -0.0137, -0.0209, -0.0783, -0.1236,\n",
      "        -0.1247,  0.0056,  0.0537, -0.0597,  0.0650, -0.0600, -0.1791, -0.0867,\n",
      "         0.0072, -0.1594,  0.0961, -0.0271, -0.0791,  0.0674,  0.0552,  0.0075,\n",
      "         0.0270, -0.0446, -0.0629,  0.0713,  0.0808, -0.1052, -0.0390,  0.0212,\n",
      "         0.0190,  0.0636,  0.1015,  0.0264, -0.0397, -0.1259,  0.0495,  0.0144,\n",
      "        -0.1181,  0.0285,  0.0425,  0.0968, -0.0729, -0.0537,  0.0723,  0.1280,\n",
      "         0.0973,  0.0067,  0.1209,  0.0250,  0.1253,  0.2388, -0.0022,  0.0744,\n",
      "        -0.0660,  0.0881,  0.0927, -0.1042, -0.0886, -0.0546, -0.0503, -0.0549,\n",
      "        -0.0498,  0.0023, -0.0796, -0.1620,  0.0219, -0.0376,  0.0755,  0.0654,\n",
      "         0.0263, -0.1217,  0.1104, -0.1648,  0.0341, -0.0407,  0.0705,  0.1223,\n",
      "        -0.0730,  0.0183, -0.2178,  0.1369, -0.0746, -0.0881,  0.0227,  0.0479,\n",
      "         0.0948, -0.1547,  0.2114, -0.0280,  0.0053, -0.1538,  0.0812, -0.2060,\n",
      "        -0.0345, -0.0139,  0.0304, -0.0835,  0.0255, -0.1889, -0.0243, -0.0248,\n",
      "        -0.1247,  0.1112, -0.1504, -0.1608,  0.1315, -0.0517, -0.0184, -0.0064,\n",
      "        -0.0149,  0.0797, -0.1523,  0.1705, -0.0252,  0.0887, -0.0088, -0.0909,\n",
      "         0.1201, -0.0143,  0.1217,  0.0176,  0.0356,  0.0438,  0.0637,  0.1362,\n",
      "        -0.1137,  0.0067,  0.0443, -0.1069,  0.0225, -0.0597, -0.0515,  0.0572,\n",
      "        -0.0206, -0.0144, -0.1313, -0.1103,  0.2238,  0.0098, -0.0861,  0.0748,\n",
      "        -0.1198, -0.0205,  0.1441, -0.0121, -0.0215, -0.0166, -0.0167,  0.1166,\n",
      "        -0.0356,  0.0381, -0.2010, -0.1594, -0.0588, -0.1291, -0.0064, -0.2190,\n",
      "        -0.0167, -0.0986, -0.1743, -0.0619,  0.1881,  0.0069, -0.0237, -0.1121,\n",
      "        -0.0358,  0.0763,  0.0709, -0.0868, -0.1545,  0.0103, -0.0841, -0.0614,\n",
      "        -0.0369, -0.0950,  0.0489,  0.0628, -0.0764,  0.0081, -0.0991,  0.1001,\n",
      "        -0.0128, -0.0114, -0.0368, -0.0930, -0.0762,  0.1202, -0.0140,  0.0505,\n",
      "        -0.0576,  0.1586,  0.0490,  0.0474,  0.0148, -0.0298,  0.1035,  0.1921],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([-0.0736, -0.1751,  0.0504, -0.3669,  0.0112,  0.0475, -0.0263,  0.0961,\n",
      "        -0.0112,  0.2255,  0.2069,  0.1897, -0.0819,  0.0992, -0.1862,  0.0590,\n",
      "        -0.1286,  0.0324, -0.0889,  0.1457,  0.0527, -0.0342, -0.1842,  0.0453,\n",
      "        -0.0074, -0.1054,  0.0710, -0.0638,  0.0742, -0.0491, -0.0104, -0.1862,\n",
      "         0.1445, -0.0510, -0.1420,  0.1242, -0.0628, -0.1215,  0.1460, -0.0683,\n",
      "         0.1111, -0.1328, -0.3610, -0.0312, -0.0082, -0.0431, -0.0025,  0.0922,\n",
      "        -0.0042,  0.1258, -0.0616,  0.0505, -0.1458, -0.1470, -0.1136,  0.0638,\n",
      "         0.1450,  0.0504,  0.1554, -0.0752,  0.0481, -0.1849,  0.0429,  0.0123,\n",
      "        -0.0310, -0.0679, -0.0631, -0.0534, -0.0229,  0.1440, -0.1041,  0.1121,\n",
      "        -0.1263,  0.1014,  0.2342,  0.0597, -0.0107, -0.1551,  0.0343,  0.0074,\n",
      "         0.0493,  0.0248,  0.0358,  0.0841, -0.0355,  0.0521, -0.0584, -0.0175,\n",
      "         0.0370,  0.0860,  0.0670, -0.1274, -0.0069,  0.0288,  0.1830,  0.1201,\n",
      "        -0.0190, -0.1839,  0.0640, -0.0177,  0.0022, -0.0061,  0.0126,  0.0668,\n",
      "         0.0880, -0.0806, -0.1142,  0.1095,  0.0341, -0.0293,  0.0906,  0.0533,\n",
      "        -0.0224,  0.0461,  0.1022, -0.0066, -0.0420, -0.1315,  0.0714, -0.1164,\n",
      "        -0.0850, -0.2003, -0.0063, -0.0239, -0.0907, -0.0067, -0.1372,  0.0199,\n",
      "        -0.0577, -0.1584, -0.0546, -0.0006, -0.0524,  0.0294,  0.0837,  0.1034,\n",
      "         0.0880,  0.1885,  0.1615, -0.0307,  0.1587, -0.0101,  0.1470,  0.0685,\n",
      "         0.1076, -0.0109,  0.0286, -0.1251,  0.1179,  0.1020,  0.0500, -0.0050,\n",
      "         0.0953, -0.1838,  0.0204,  0.0264,  0.2457,  0.0224, -0.1186,  0.0432,\n",
      "         0.0799,  0.1651,  0.0228, -0.0053, -0.0353, -0.0629, -0.1765, -0.0541,\n",
      "         0.1183, -0.0574,  0.1070, -0.0800, -0.0705, -0.1152, -0.0789, -0.0380,\n",
      "        -0.0052,  0.0009,  0.0113,  0.0043,  0.0112, -0.0604,  0.0784, -0.1099,\n",
      "        -0.1713, -0.0501, -0.0181,  0.0585,  0.1567, -0.0762,  0.0345, -0.0554,\n",
      "        -0.0552, -0.0741, -0.1066,  0.0381,  0.0927, -0.0143, -0.0727, -0.1532],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0286, -0.0180, -0.0436, -0.0702,  0.0584,  0.0683,  0.0712,  0.1624,\n",
      "         0.1048, -0.0709,  0.0070, -0.1244,  0.0702, -0.2220, -0.1082,  0.0018,\n",
      "        -0.0199,  0.1833, -0.0493, -0.0468,  0.1599, -0.0690, -0.0514,  0.1247,\n",
      "         0.0658,  0.0439, -0.1288,  0.1081, -0.0248,  0.0158,  0.0164, -0.0514,\n",
      "         0.0893, -0.0737, -0.1021, -0.0042,  0.0509,  0.0772,  0.0748,  0.0341,\n",
      "         0.1955, -0.0281, -0.1153, -0.1807, -0.0458, -0.0331,  0.0938,  0.0880,\n",
      "         0.0050,  0.0477, -0.0211,  0.0637, -0.0353,  0.0806,  0.0993, -0.1775,\n",
      "        -0.0348, -0.0638, -0.0187,  0.2133, -0.0639, -0.0146, -0.0499, -0.0369,\n",
      "         0.1405, -0.0695,  0.0156, -0.1673,  0.0761,  0.1384,  0.1227, -0.1147,\n",
      "         0.1057,  0.0262,  0.0453,  0.0686, -0.0169, -0.0800,  0.1168,  0.0081,\n",
      "         0.0259,  0.0611, -0.0496,  0.0996, -0.0704, -0.0740,  0.1037, -0.1479,\n",
      "         0.0526,  0.1374,  0.0412,  0.0363, -0.0147, -0.0239, -0.0100,  0.0309,\n",
      "         0.0558,  0.0784, -0.0259,  0.1196,  0.1135, -0.2428,  0.0194, -0.0609,\n",
      "         0.0728,  0.0395,  0.1401, -0.2334, -0.0158, -0.1055, -0.0706,  0.0373,\n",
      "        -0.0246,  0.1723,  0.0593, -0.0389, -0.0274,  0.0117, -0.0019,  0.0099,\n",
      "        -0.0380, -0.0382, -0.0511, -0.0897,  0.0605,  0.0359, -0.1718,  0.0831,\n",
      "        -0.0579, -0.1540, -0.0611,  0.0129, -0.3191, -0.0403, -0.0307, -0.0428,\n",
      "        -0.0452,  0.1360, -0.0197,  0.0126, -0.0050,  0.0545, -0.0600,  0.1299,\n",
      "         0.0357,  0.0268,  0.1635, -0.0588, -0.1371, -0.0043, -0.1634, -0.0875,\n",
      "         0.1339,  0.1684, -0.1145,  0.0431,  0.0836,  0.0887,  0.0886,  0.0330,\n",
      "        -0.1724, -0.0217,  0.0187, -0.1049,  0.0493, -0.0546, -0.0045,  0.1296,\n",
      "         0.0760, -0.0571, -0.1177,  0.1976, -0.0365,  0.0893, -0.0238, -0.0358,\n",
      "         0.1349, -0.0959, -0.1737,  0.1367, -0.1398, -0.1275,  0.0201, -0.2324,\n",
      "        -0.1526, -0.1053, -0.0428, -0.0039,  0.0301, -0.0453,  0.1588,  0.0880,\n",
      "        -0.0251, -0.0190, -0.0090,  0.1420,  0.0247, -0.0539, -0.0089,  0.2265],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([ 0.0074,  0.0341, -0.3272, -0.0426,  0.0663,  0.0467,  0.1306,  0.0138,\n",
      "        -0.0935,  0.0752, -0.0235, -0.1350, -0.1198,  0.1203, -0.0229, -0.0713,\n",
      "        -0.0047, -0.1318,  0.0584, -0.0567, -0.1366,  0.0170, -0.0658,  0.0678,\n",
      "         0.0842, -0.1548,  0.0200,  0.0053,  0.0144,  0.0681,  0.0946, -0.1074,\n",
      "        -0.1081,  0.1561, -0.1307,  0.1175, -0.0879,  0.0646, -0.0271,  0.0374,\n",
      "        -0.0984,  0.2366, -0.1602,  0.0173, -0.0803, -0.0817,  0.2635, -0.0585,\n",
      "        -0.0172, -0.1780,  0.0304, -0.0454,  0.0261,  0.1376, -0.0232,  0.0310,\n",
      "        -0.0024, -0.1174, -0.0827,  0.0021, -0.1437,  0.0575, -0.0369,  0.1405,\n",
      "        -0.0038, -0.0544,  0.0319, -0.0792,  0.0014, -0.0879,  0.0209,  0.0770,\n",
      "         0.1785,  0.1266,  0.1376, -0.0774,  0.2070, -0.0526,  0.1503, -0.1658,\n",
      "         0.0099, -0.0067,  0.1425, -0.1182,  0.0182, -0.0403, -0.0026,  0.1231,\n",
      "        -0.0219, -0.2756,  0.0398, -0.0790,  0.1487, -0.2117, -0.1306, -0.0935,\n",
      "         0.0179,  0.1454,  0.0700, -0.0466,  0.0976,  0.0025,  0.2358, -0.0958,\n",
      "        -0.0274, -0.1500,  0.0493, -0.1060, -0.1004, -0.1155, -0.1306,  0.0049,\n",
      "         0.1114,  0.0975,  0.0855, -0.1265, -0.0257, -0.0732,  0.0045,  0.1288,\n",
      "        -0.1973,  0.0486,  0.0135, -0.0377,  0.0886, -0.0715, -0.1010,  0.0362,\n",
      "        -0.1540,  0.0366,  0.2315,  0.0141, -0.0593,  0.1370,  0.0240,  0.2134,\n",
      "        -0.1418, -0.1272,  0.1989,  0.0933, -0.0789,  0.0588,  0.1203,  0.1085,\n",
      "        -0.0438,  0.0419,  0.0572,  0.1211,  0.1778,  0.2781, -0.1818,  0.0615,\n",
      "         0.0975, -0.0078,  0.0394, -0.1184,  0.2621,  0.0428, -0.1332,  0.0742,\n",
      "        -0.1602, -0.0287, -0.0833, -0.0422, -0.2185, -0.0140,  0.1806,  0.1232,\n",
      "        -0.0787,  0.0378, -0.1381, -0.0878, -0.0840, -0.0384, -0.0444, -0.0886,\n",
      "        -0.0478,  0.0971,  0.1244,  0.1254,  0.1270,  0.1006,  0.0483,  0.0086,\n",
      "         0.0058, -0.0657,  0.0525, -0.1376, -0.0563, -0.0057, -0.0958,  0.1123,\n",
      "         0.0926, -0.1415,  0.2348, -0.0477, -0.0370, -0.0315,  0.0087,  0.0186],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([-7.6539e-02,  8.9375e-02,  4.5167e-02, -1.1011e-01, -6.6944e-05,\n",
      "         9.0737e-02,  3.2078e-02,  2.9234e-02,  5.9835e-02, -5.6670e-02,\n",
      "         4.9092e-02, -1.9052e-01, -7.1352e-02,  1.9002e-01,  3.9516e-02,\n",
      "        -4.3507e-02,  4.7675e-02, -1.1742e-01,  4.8330e-03, -5.0643e-02,\n",
      "         1.4230e-01,  3.7130e-02, -2.7272e-02,  7.9933e-02, -5.7064e-02,\n",
      "        -1.9653e-03, -6.0195e-02,  1.2416e-01,  1.0056e-01, -3.6958e-02,\n",
      "         5.6951e-02,  1.3598e-01,  5.3718e-02, -2.0220e-01,  5.4297e-02,\n",
      "         5.5800e-02,  4.4814e-02,  9.6623e-02,  7.5859e-02, -2.6031e-03,\n",
      "         9.1036e-02,  3.8098e-02,  9.8066e-03, -3.0016e-02, -1.1122e-01,\n",
      "         8.0076e-02,  8.3838e-02, -8.0762e-03, -9.6153e-03, -1.7864e-02,\n",
      "        -5.4878e-02, -2.6588e-02, -2.9345e-02,  1.6164e-01, -2.8435e-02,\n",
      "        -6.1241e-02,  1.1251e-01,  2.0045e-02,  1.3382e-01, -7.7511e-02,\n",
      "        -5.9225e-02,  1.0812e-01,  4.1126e-02,  5.8063e-02,  7.6879e-02,\n",
      "         4.0860e-02,  4.5325e-03, -5.2741e-02,  1.5864e-02, -3.4476e-02,\n",
      "         4.6778e-02,  1.6621e-01, -1.0392e-01,  6.3843e-02,  3.9443e-02,\n",
      "        -1.2549e-01,  9.0308e-02,  3.9338e-02, -4.5865e-02,  1.1699e-01,\n",
      "        -5.1566e-02,  8.8674e-02,  9.5413e-02,  8.2183e-02,  2.1972e-01,\n",
      "         2.5629e-02,  7.7838e-02, -9.4826e-02, -7.8027e-02, -5.5978e-03,\n",
      "        -1.9067e-02, -1.7290e-01,  2.3711e-02,  3.7545e-02, -1.0917e-01,\n",
      "        -2.5437e-02, -1.4994e-01,  7.2940e-02,  1.5671e-02, -9.6850e-02,\n",
      "        -2.8963e-03, -8.9035e-02, -3.6378e-02,  5.8881e-02,  3.7118e-02,\n",
      "         5.7631e-02,  1.1665e-03, -2.6240e-03,  1.1616e-01,  8.5147e-02,\n",
      "         7.1518e-02, -5.8797e-02, -4.5829e-02, -4.2724e-02, -1.2929e-01,\n",
      "        -3.8012e-02,  1.5200e-01, -2.2551e-02,  1.2845e-01,  8.7969e-02,\n",
      "        -1.1371e-01,  8.3334e-02, -1.1784e-01,  2.7018e-01,  2.4320e-03,\n",
      "        -8.2732e-02, -1.0224e-01,  8.8929e-02,  1.5825e-01, -5.9154e-02,\n",
      "         9.1081e-02,  1.2695e-01,  2.9580e-02, -1.4227e-01, -1.0591e-01,\n",
      "         9.4140e-02,  2.5477e-02, -2.7890e-02, -1.4179e-01, -1.1768e-01,\n",
      "         5.4247e-02,  7.4828e-02, -9.6335e-03,  6.7575e-02, -4.2075e-02,\n",
      "        -1.3918e-01,  9.7839e-02, -5.6634e-02, -1.5985e-01,  8.1351e-02,\n",
      "         1.3626e-01,  2.0249e-02,  1.2457e-02,  3.6336e-02, -9.8185e-02,\n",
      "         1.9148e-01, -1.1091e-01, -2.5379e-02, -1.7101e-02,  2.1836e-01,\n",
      "        -2.3481e-02, -4.1642e-03,  1.0123e-02,  8.1825e-02,  1.0244e-01,\n",
      "         1.5652e-01,  1.5589e-01, -5.5582e-02,  1.4885e-01, -3.6732e-02,\n",
      "         1.5473e-01, -1.2375e-01,  3.6207e-02, -1.3630e-01,  7.4040e-04,\n",
      "        -7.1724e-02, -1.6032e-02, -2.3448e-02,  6.8463e-02,  1.5341e-01,\n",
      "         8.9580e-02, -1.0178e-01, -8.1118e-03,  1.1008e-01, -7.7227e-02,\n",
      "         3.5505e-02,  5.5238e-02, -1.5136e-01, -9.6152e-02,  3.5739e-02,\n",
      "         8.5463e-02,  1.2870e-02, -2.2109e-01,  6.4746e-02,  9.8528e-02,\n",
      "         1.6789e-02, -7.0153e-02, -1.0061e-01, -1.2247e-01,  5.1437e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([ 0.0409, -0.1525,  0.0280,  0.1483,  0.0190,  0.0108,  0.1320,  0.1782,\n",
      "        -0.0159, -0.0107, -0.1418, -0.0270,  0.0842,  0.0319,  0.0736, -0.0059,\n",
      "         0.1445, -0.0158, -0.0974,  0.0052,  0.0975,  0.0907,  0.0500, -0.0291,\n",
      "        -0.0668,  0.0888, -0.0294,  0.0663,  0.0532, -0.0158, -0.0801, -0.1777,\n",
      "        -0.1121,  0.0882, -0.1050,  0.0422,  0.0096,  0.2913,  0.0221,  0.0926,\n",
      "        -0.0032, -0.0132,  0.1693, -0.1017, -0.0788,  0.0233,  0.0279,  0.1240,\n",
      "        -0.0387, -0.0856,  0.0760,  0.0357, -0.0006, -0.1200, -0.0692,  0.0029,\n",
      "        -0.0087,  0.1063,  0.0236, -0.0298, -0.0079,  0.2011, -0.1501, -0.0164,\n",
      "        -0.1568,  0.0210,  0.0379, -0.0063,  0.0621,  0.2111, -0.0120, -0.0046,\n",
      "         0.0354, -0.0239,  0.0314, -0.0016, -0.0181, -0.0712,  0.1483, -0.0523,\n",
      "        -0.0248, -0.1912,  0.1181,  0.1042, -0.1240,  0.0115,  0.0950,  0.0373,\n",
      "         0.0437,  0.0662,  0.0458,  0.1117, -0.0979,  0.0757, -0.0431, -0.0751,\n",
      "        -0.0035,  0.1227,  0.0849, -0.0399, -0.0036,  0.0220,  0.1283,  0.0449,\n",
      "         0.1505,  0.0610,  0.1397,  0.0118, -0.0928, -0.0206, -0.1219, -0.0297,\n",
      "         0.0113,  0.0292,  0.1567,  0.1745,  0.0270,  0.0727,  0.1004, -0.1341,\n",
      "        -0.0503,  0.0600,  0.0636, -0.1163, -0.0759,  0.0807, -0.0568, -0.0229,\n",
      "         0.0526, -0.0366,  0.1676,  0.1218, -0.0509, -0.0561,  0.0414, -0.0885,\n",
      "         0.0481,  0.1338,  0.1321, -0.1371, -0.0006, -0.1312, -0.0123,  0.0651,\n",
      "         0.0250,  0.0606,  0.2550, -0.1394, -0.0880, -0.0645, -0.1327, -0.0886,\n",
      "        -0.0014, -0.0258,  0.0780, -0.0209, -0.0399, -0.0905, -0.1004,  0.1412,\n",
      "        -0.0043,  0.1036,  0.0309, -0.0101,  0.0401,  0.0343, -0.1028, -0.1789,\n",
      "         0.0040, -0.0074, -0.0022,  0.0741, -0.0559, -0.0512, -0.1161,  0.0817,\n",
      "         0.0116, -0.0104,  0.0032, -0.0703,  0.0235, -0.0203,  0.0308,  0.0562,\n",
      "         0.0420, -0.0566,  0.0267, -0.0887, -0.0274,  0.0825, -0.1517, -0.1106,\n",
      "        -0.0270,  0.0041, -0.1137,  0.0893,  0.0071, -0.1274, -0.1174,  0.0126],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([-1.3967e-01,  5.0294e-03, -1.5833e-01, -1.5130e-02,  5.0585e-02,\n",
      "         3.8576e-02,  9.0357e-02, -3.3319e-02, -1.0655e-01, -1.6900e-02,\n",
      "         2.8043e-02,  1.4795e-02, -8.0734e-02,  5.7987e-02, -1.7455e-02,\n",
      "        -7.2776e-02,  6.7639e-02, -4.7220e-02,  8.0859e-02, -1.1931e-01,\n",
      "         9.5415e-02,  5.6945e-02, -9.3385e-03, -5.7611e-02,  1.4350e-01,\n",
      "        -1.7931e-01,  7.5833e-02,  2.3284e-04, -6.1610e-02, -1.8387e-02,\n",
      "         7.8435e-03, -1.2586e-01,  1.2509e-03, -1.4436e-01, -6.8876e-02,\n",
      "         7.2499e-02,  1.5449e-02, -8.5630e-02,  5.9163e-02,  9.7872e-02,\n",
      "        -1.1407e-01,  4.6154e-02, -1.5581e-01,  5.0302e-02, -7.0139e-02,\n",
      "        -9.3647e-03,  1.8626e-01, -1.0567e-01,  7.6448e-03, -8.2711e-02,\n",
      "        -5.7172e-02,  6.2713e-02,  2.2183e-02, -1.6837e-01,  2.3821e-02,\n",
      "        -5.3737e-02, -1.3839e-01, -3.3838e-02,  6.7899e-02, -2.3466e-02,\n",
      "        -6.9602e-02, -1.0679e-01, -1.3308e-01, -5.1809e-03, -8.8803e-02,\n",
      "         1.7050e-01, -1.5739e-02,  1.8219e-02,  9.3258e-02,  9.0563e-02,\n",
      "        -3.1891e-02,  1.1528e-01, -1.2928e-01, -8.1505e-02,  4.1009e-02,\n",
      "         3.1617e-02, -4.8466e-02, -1.3505e-01, -4.9943e-02,  3.9677e-02,\n",
      "        -8.7587e-02,  1.5103e-02,  6.0978e-02,  1.8347e-02, -1.9921e-02,\n",
      "         9.4541e-02,  1.2773e-01, -5.2539e-03, -2.5683e-02, -6.6164e-02,\n",
      "         1.3082e-01, -1.0365e-01,  2.5099e-02,  7.0049e-02, -2.7286e-03,\n",
      "         9.4198e-02, -5.8743e-02,  1.8719e-01,  1.9228e-01,  1.1034e-01,\n",
      "         9.0923e-02, -2.6364e-01,  2.1932e-02, -2.1895e-01,  1.6719e-01,\n",
      "         2.2153e-02,  2.0669e-02,  9.8778e-02,  1.4097e-01, -1.0442e-01,\n",
      "         4.4874e-02,  5.4268e-02,  4.0031e-02, -2.9858e-02, -1.2393e-01,\n",
      "        -1.7435e-01,  1.0520e-01,  7.9599e-02,  4.5168e-02,  1.7054e-01,\n",
      "         6.7191e-02,  8.9720e-02,  2.6135e-02, -1.1241e-01, -1.3832e-01,\n",
      "        -5.4150e-02, -1.7907e-01,  2.0693e-01,  5.8412e-02,  5.3864e-02,\n",
      "         1.3086e-03, -2.8521e-03, -5.1042e-02,  9.5866e-03,  8.9592e-02,\n",
      "         1.8545e-02,  1.2781e-01,  2.3050e-02,  1.1575e-01, -7.2995e-02,\n",
      "        -9.9738e-03,  8.7969e-02,  6.7675e-02, -3.5332e-02,  2.1040e-03,\n",
      "        -6.3421e-02, -3.9105e-03, -9.1298e-02,  1.7021e-01,  2.2981e-01,\n",
      "         1.6931e-02, -2.5500e-01,  6.2651e-02, -7.9860e-02,  9.7155e-03,\n",
      "        -2.1618e-02,  1.0834e-01,  2.4436e-01, -1.9338e-01,  2.8186e-01,\n",
      "         4.3275e-02,  7.8120e-02, -1.2852e-02,  1.6004e-02, -1.7223e-01,\n",
      "        -2.0486e-01, -5.3351e-02, -1.7738e-01, -9.9390e-02,  1.0031e-01,\n",
      "         9.1900e-02, -1.0842e-01,  1.5468e-01,  1.1827e-01, -1.7521e-04,\n",
      "         1.4482e-01, -3.8056e-02,  1.1472e-01,  7.8795e-02, -5.6371e-02,\n",
      "         8.0233e-02,  4.6290e-02, -1.2486e-01,  1.1209e-01,  2.3265e-01,\n",
      "         1.3329e-02,  2.0691e-02,  1.7379e-02,  5.1378e-02,  2.1479e-01,\n",
      "        -1.1075e-01,  4.9458e-02,  6.1310e-02,  1.0604e-01, -8.6042e-02,\n",
      "        -8.8758e-02,  6.0845e-03,  1.7128e-02,  1.2387e-01, -2.4488e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 0.0344,  0.0364, -0.1021,  0.1188,  0.0525, -0.0248,  0.0792,  0.0442,\n",
      "         0.0177, -0.0190, -0.1303, -0.1642, -0.0473, -0.0974,  0.0948, -0.0759,\n",
      "        -0.0365, -0.0993, -0.1029,  0.2479,  0.1170,  0.1738,  0.0653, -0.0954,\n",
      "        -0.1143,  0.0160,  0.0736, -0.1301, -0.0405, -0.0497,  0.0315,  0.0383,\n",
      "         0.1028, -0.0524, -0.0445, -0.1429,  0.1680,  0.1330,  0.2019, -0.0239,\n",
      "        -0.1607,  0.1268,  0.1502,  0.0055,  0.0595, -0.0860, -0.0482,  0.2104,\n",
      "         0.0942,  0.0134, -0.0138,  0.0608,  0.1565, -0.0258,  0.1967,  0.2003,\n",
      "         0.1015,  0.0224, -0.1094,  0.0530, -0.0381,  0.1274,  0.2591, -0.1249,\n",
      "        -0.0482,  0.0358,  0.1012, -0.0922, -0.1121,  0.0619,  0.0035,  0.0442,\n",
      "        -0.0418,  0.0764, -0.0409, -0.0382,  0.0193,  0.0276,  0.0218, -0.0206,\n",
      "        -0.0984, -0.0777,  0.0684, -0.0082,  0.2027, -0.0077, -0.0257,  0.0185,\n",
      "        -0.0589,  0.1196,  0.0474, -0.1016, -0.1036,  0.0204,  0.0086, -0.0076,\n",
      "        -0.1230,  0.0252, -0.0219, -0.1360,  0.0211, -0.0421,  0.0140,  0.0648,\n",
      "        -0.0523,  0.0422, -0.1237,  0.0948, -0.1144, -0.0253, -0.0193, -0.2060,\n",
      "        -0.0632, -0.1563,  0.2045,  0.1043,  0.2121,  0.0060,  0.1694,  0.0025,\n",
      "        -0.3196, -0.1519, -0.1899,  0.1555,  0.0615,  0.0139,  0.1639,  0.1737,\n",
      "        -0.0614,  0.0544,  0.0820,  0.0424, -0.0004,  0.1501, -0.0302, -0.0611,\n",
      "        -0.0329,  0.0247,  0.0966, -0.1194, -0.0796, -0.0428,  0.0792, -0.1501,\n",
      "        -0.0140,  0.1774,  0.0666,  0.0258,  0.1756,  0.0509, -0.3018, -0.1302,\n",
      "         0.1455, -0.0376,  0.1676, -0.0166,  0.0094, -0.1141,  0.2172,  0.0563,\n",
      "        -0.0981,  0.0909, -0.1024,  0.0067,  0.0827,  0.0717,  0.0438, -0.1812,\n",
      "         0.0850,  0.0998,  0.0099,  0.0302,  0.0620, -0.0993,  0.0220,  0.0847,\n",
      "        -0.0264,  0.2811,  0.1381, -0.2007, -0.0452,  0.0138,  0.0012, -0.0674,\n",
      "        -0.1206,  0.0226, -0.0154,  0.0125, -0.0679,  0.0270, -0.0239, -0.0132,\n",
      "         0.1391,  0.0985,  0.0182, -0.1582,  0.1285, -0.0620, -0.0218, -0.0465],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-0.0085, -0.0234, -0.0421, -0.0532, -0.0604,  0.0360,  0.0408,  0.0639,\n",
      "        -0.0355, -0.0011, -0.1086,  0.0262,  0.0431, -0.0705,  0.1524,  0.0433,\n",
      "        -0.0098, -0.0993,  0.2177,  0.0327,  0.0204, -0.0319, -0.0154, -0.0357,\n",
      "        -0.1580,  0.0375, -0.1441, -0.0078, -0.0861,  0.0249, -0.0968, -0.1098,\n",
      "         0.0816, -0.0443,  0.0505, -0.0006,  0.0014, -0.0266,  0.1026, -0.0338,\n",
      "         0.1210,  0.0027,  0.1071, -0.0292, -0.1861, -0.0136, -0.0201,  0.2161,\n",
      "         0.1654, -0.1534, -0.0203, -0.1487,  0.1674, -0.0337, -0.1989,  0.0018,\n",
      "        -0.0472,  0.0284, -0.0060, -0.0302, -0.2214, -0.0116, -0.1020, -0.0271,\n",
      "        -0.0093, -0.0060,  0.0544, -0.0051,  0.0259,  0.0174, -0.0679,  0.0550,\n",
      "         0.0528,  0.0315, -0.2329, -0.0925, -0.0008, -0.1034, -0.1081,  0.0456,\n",
      "         0.0073,  0.1203, -0.0301,  0.0734,  0.0673, -0.0011,  0.1941,  0.0123,\n",
      "        -0.0930, -0.0457,  0.0004, -0.1283, -0.0189,  0.1049,  0.0705,  0.0615,\n",
      "        -0.0590, -0.0643, -0.1822, -0.0306,  0.1414, -0.1711, -0.0563,  0.0790,\n",
      "         0.0414,  0.2343, -0.0737,  0.0239,  0.2059, -0.0566,  0.0873, -0.1441,\n",
      "         0.0953, -0.0185, -0.1988, -0.0035,  0.0780, -0.0443,  0.0805, -0.0808,\n",
      "         0.0203, -0.0053, -0.1655,  0.0756,  0.0730,  0.0804, -0.1544, -0.0184,\n",
      "         0.0580, -0.0679,  0.0551,  0.0071, -0.0519, -0.0335, -0.0562, -0.0044,\n",
      "        -0.0018,  0.0234, -0.0448,  0.0767,  0.1016,  0.2548,  0.1019, -0.2599,\n",
      "         0.2140,  0.2110, -0.0939, -0.1733, -0.0791,  0.0268, -0.0042,  0.0927,\n",
      "         0.0646,  0.0747, -0.0200, -0.1165,  0.1067,  0.2060,  0.0203,  0.0366,\n",
      "         0.0407, -0.0312, -0.0504, -0.0555, -0.0647, -0.0556,  0.1931, -0.1553,\n",
      "        -0.0750,  0.0205,  0.0474, -0.0235,  0.1340,  0.0206,  0.0218,  0.0861,\n",
      "        -0.0412,  0.0329,  0.1155, -0.0956, -0.0077, -0.1364,  0.0787,  0.1446,\n",
      "        -0.1095, -0.1085,  0.1430,  0.1237, -0.0580,  0.0155,  0.0942, -0.0004,\n",
      "         0.1280,  0.1364,  0.0123, -0.0667,  0.0581,  0.0456, -0.0947,  0.1711],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  0.067048/  0.099424, val:  29.17%, val_best:  29.17%, tr:  73.95%, tr_best:  73.95%, epoch time: 70.45 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   2  Sparsity: 56.6345%\n",
      "layer   3  Sparsity: 54.3583%\n",
      "total_backward_count 9790 real_backward_count 3539  36.149%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  0.047396/  0.072842, val:  56.25%, val_best:  56.25%, tr:  86.41%, tr_best:  86.41%, epoch time: 69.15 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   2  Sparsity: 53.6574%\n",
      "layer   3  Sparsity: 51.9856%\n",
      "total_backward_count 19580 real_backward_count 5756  29.397%\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  0.041804/  0.081404, val:  49.58%, val_best:  56.25%, tr:  87.74%, tr_best:  87.74%, epoch time: 68.32 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   2  Sparsity: 52.4842%\n",
      "layer   3  Sparsity: 51.8331%\n",
      "total_backward_count 29370 real_backward_count 7721  26.289%\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  0.039497/  0.061279, val:  62.50%, val_best:  62.50%, tr:  91.42%, tr_best:  91.42%, epoch time: 68.12 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0525%\n",
      "layer   2  Sparsity: 51.8359%\n",
      "layer   3  Sparsity: 51.0813%\n",
      "total_backward_count 39160 real_backward_count 9527  24.328%\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  0.037425/  0.074666, val:  42.92%, val_best:  62.50%, tr:  90.40%, tr_best:  91.42%, epoch time: 68.36 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0974%\n",
      "layer   2  Sparsity: 51.4111%\n",
      "layer   3  Sparsity: 51.2802%\n",
      "total_backward_count 48950 real_backward_count 11277  23.038%\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  0.035945/  0.074370, val:  55.00%, val_best:  62.50%, tr:  92.03%, tr_best:  92.03%, epoch time: 68.95 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   2  Sparsity: 51.4676%\n",
      "layer   3  Sparsity: 51.0089%\n",
      "total_backward_count 58740 real_backward_count 12893  21.949%\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  0.034456/  0.074264, val:  49.58%, val_best:  62.50%, tr:  92.13%, tr_best:  92.13%, epoch time: 69.27 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 51.6273%\n",
      "layer   3  Sparsity: 50.5910%\n",
      "total_backward_count 68530 real_backward_count 14442  21.074%\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  0.033861/  0.065762, val:  61.25%, val_best:  62.50%, tr:  92.95%, tr_best:  92.95%, epoch time: 69.02 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0935%\n",
      "layer   2  Sparsity: 51.8282%\n",
      "layer   3  Sparsity: 50.5974%\n",
      "total_backward_count 78320 real_backward_count 15932  20.342%\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  0.032640/  0.059913, val:  60.83%, val_best:  62.50%, tr:  94.59%, tr_best:  94.59%, epoch time: 68.71 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   2  Sparsity: 51.6637%\n",
      "layer   3  Sparsity: 50.3615%\n",
      "total_backward_count 88110 real_backward_count 17377  19.722%\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  0.032640/  0.066629, val:  58.75%, val_best:  62.50%, tr:  93.67%, tr_best:  94.59%, epoch time: 68.50 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0983%\n",
      "layer   2  Sparsity: 51.8072%\n",
      "layer   3  Sparsity: 51.0729%\n",
      "total_backward_count 97900 real_backward_count 18897  19.302%\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  0.031281/  0.075387, val:  60.83%, val_best:  62.50%, tr:  94.59%, tr_best:  94.59%, epoch time: 68.11 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1170%\n",
      "layer   2  Sparsity: 52.0677%\n",
      "layer   3  Sparsity: 51.1131%\n",
      "total_backward_count 107690 real_backward_count 20321  18.870%\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  0.030751/  0.059443, val:  61.25%, val_best:  62.50%, tr:  94.38%, tr_best:  94.59%, epoch time: 68.95 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   2  Sparsity: 52.1523%\n",
      "layer   3  Sparsity: 51.1740%\n",
      "total_backward_count 117480 real_backward_count 21714  18.483%\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  0.029807/  0.063383, val:  58.75%, val_best:  62.50%, tr:  95.40%, tr_best:  95.40%, epoch time: 68.45 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1078%\n",
      "layer   2  Sparsity: 52.3333%\n",
      "layer   3  Sparsity: 51.2143%\n",
      "total_backward_count 127270 real_backward_count 23042  18.105%\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  0.029476/  0.058506, val:  68.33%, val_best:  68.33%, tr:  95.71%, tr_best:  95.71%, epoch time: 68.72 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0554%\n",
      "layer   2  Sparsity: 52.2384%\n",
      "layer   3  Sparsity: 51.2464%\n",
      "total_backward_count 137060 real_backward_count 24339  17.758%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  0.028822/  0.069827, val:  55.83%, val_best:  68.33%, tr:  96.63%, tr_best:  96.63%, epoch time: 69.42 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   2  Sparsity: 52.3865%\n",
      "layer   3  Sparsity: 51.3344%\n",
      "total_backward_count 146850 real_backward_count 25606  17.437%\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  0.028193/  0.058126, val:  68.75%, val_best:  68.75%, tr:  96.63%, tr_best:  96.63%, epoch time: 68.79 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0477%\n",
      "layer   2  Sparsity: 52.4195%\n",
      "layer   3  Sparsity: 51.1153%\n",
      "total_backward_count 156640 real_backward_count 26857  17.146%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  0.027540/  0.049732, val:  79.58%, val_best:  79.58%, tr:  98.16%, tr_best:  98.16%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   2  Sparsity: 52.7203%\n",
      "layer   3  Sparsity: 51.0371%\n",
      "total_backward_count 166430 real_backward_count 28082  16.873%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  0.027118/  0.059506, val:  66.25%, val_best:  79.58%, tr:  97.75%, tr_best:  98.16%, epoch time: 68.87 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0687%\n",
      "layer   2  Sparsity: 52.8660%\n",
      "layer   3  Sparsity: 51.1551%\n",
      "total_backward_count 176220 real_backward_count 29256  16.602%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  0.025856/  0.058202, val:  69.58%, val_best:  79.58%, tr:  98.26%, tr_best:  98.26%, epoch time: 69.61 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0489%\n",
      "layer   2  Sparsity: 53.0662%\n",
      "layer   3  Sparsity: 51.2906%\n",
      "total_backward_count 186010 real_backward_count 30353  16.318%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  0.025295/  0.058721, val:  71.25%, val_best:  79.58%, tr:  98.47%, tr_best:  98.47%, epoch time: 69.41 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 53.0468%\n",
      "layer   3  Sparsity: 51.4824%\n",
      "total_backward_count 195800 real_backward_count 31427  16.051%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  0.025799/  0.053380, val:  67.92%, val_best:  79.58%, tr:  97.85%, tr_best:  98.47%, epoch time: 68.99 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0756%\n",
      "layer   2  Sparsity: 53.3122%\n",
      "layer   3  Sparsity: 51.3998%\n",
      "total_backward_count 205590 real_backward_count 32543  15.829%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  0.024468/  0.054193, val:  72.50%, val_best:  79.58%, tr:  98.06%, tr_best:  98.47%, epoch time: 69.13 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0719%\n",
      "layer   2  Sparsity: 53.3441%\n",
      "layer   3  Sparsity: 51.5913%\n",
      "total_backward_count 215380 real_backward_count 33572  15.587%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  0.024033/  0.049706, val:  72.08%, val_best:  79.58%, tr:  98.57%, tr_best:  98.57%, epoch time: 65.27 seconds, 1.09 minutes\n",
      "layer   1  Sparsity: 91.0938%\n",
      "layer   2  Sparsity: 53.3576%\n",
      "layer   3  Sparsity: 51.5465%\n",
      "total_backward_count 225170 real_backward_count 34575  15.355%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  0.023646/  0.045938, val:  80.83%, val_best:  80.83%, tr:  98.16%, tr_best:  98.57%, epoch time: 68.36 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 53.3140%\n",
      "layer   3  Sparsity: 51.5107%\n",
      "total_backward_count 234960 real_backward_count 35571  15.139%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  0.023216/  0.056940, val:  74.58%, val_best:  80.83%, tr:  98.88%, tr_best:  98.88%, epoch time: 68.62 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0881%\n",
      "layer   2  Sparsity: 53.3968%\n",
      "layer   3  Sparsity: 51.4332%\n",
      "total_backward_count 244750 real_backward_count 36540  14.930%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  0.022238/  0.046859, val:  80.00%, val_best:  80.83%, tr:  99.08%, tr_best:  99.08%, epoch time: 68.40 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0569%\n",
      "layer   2  Sparsity: 53.4332%\n",
      "layer   3  Sparsity: 51.3031%\n",
      "total_backward_count 254540 real_backward_count 37466  14.719%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  0.021730/  0.051755, val:  76.67%, val_best:  80.83%, tr:  99.49%, tr_best:  99.49%, epoch time: 68.92 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.1037%\n",
      "layer   2  Sparsity: 53.5519%\n",
      "layer   3  Sparsity: 51.5297%\n",
      "total_backward_count 264330 real_backward_count 38375  14.518%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  0.021703/  0.052683, val:  75.42%, val_best:  80.83%, tr:  98.67%, tr_best:  99.49%, epoch time: 69.12 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 53.4844%\n",
      "layer   3  Sparsity: 51.4071%\n",
      "total_backward_count 274120 real_backward_count 39266  14.324%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  0.021132/  0.057169, val:  75.00%, val_best:  80.83%, tr:  99.18%, tr_best:  99.49%, epoch time: 68.90 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0983%\n",
      "layer   2  Sparsity: 53.4731%\n",
      "layer   3  Sparsity: 51.1888%\n",
      "total_backward_count 283910 real_backward_count 40139  14.138%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  0.020771/  0.056473, val:  72.08%, val_best:  80.83%, tr:  99.18%, tr_best:  99.49%, epoch time: 69.36 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0592%\n",
      "layer   2  Sparsity: 53.6008%\n",
      "layer   3  Sparsity: 51.1350%\n",
      "total_backward_count 293700 real_backward_count 40962  13.947%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  0.020913/  0.044525, val:  83.33%, val_best:  83.33%, tr:  98.98%, tr_best:  99.49%, epoch time: 68.41 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1060%\n",
      "layer   2  Sparsity: 53.6200%\n",
      "layer   3  Sparsity: 51.3267%\n",
      "total_backward_count 303490 real_backward_count 41819  13.779%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  0.019878/  0.043835, val:  85.00%, val_best:  85.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 68.83 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0401%\n",
      "layer   2  Sparsity: 53.6124%\n",
      "layer   3  Sparsity: 51.5808%\n",
      "total_backward_count 313280 real_backward_count 42608  13.601%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  0.020424/  0.044463, val:  80.00%, val_best:  85.00%, tr:  99.28%, tr_best:  99.59%, epoch time: 68.82 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0928%\n",
      "layer   2  Sparsity: 53.5998%\n",
      "layer   3  Sparsity: 51.5288%\n",
      "total_backward_count 323070 real_backward_count 43456  13.451%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  0.019522/  0.044370, val:  80.83%, val_best:  85.00%, tr:  99.08%, tr_best:  99.59%, epoch time: 68.66 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1087%\n",
      "layer   2  Sparsity: 53.6970%\n",
      "layer   3  Sparsity: 51.4744%\n",
      "total_backward_count 332860 real_backward_count 44252  13.294%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  0.019391/  0.043489, val:  84.17%, val_best:  85.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 69.20 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0158%\n",
      "layer   2  Sparsity: 53.7548%\n",
      "layer   3  Sparsity: 51.3113%\n",
      "total_backward_count 342650 real_backward_count 45008  13.135%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  0.019251/  0.043673, val:  81.67%, val_best:  85.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 68.98 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0535%\n",
      "layer   2  Sparsity: 53.5549%\n",
      "layer   3  Sparsity: 51.2566%\n",
      "total_backward_count 352440 real_backward_count 45753  12.982%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  0.019465/  0.044190, val:  82.50%, val_best:  85.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 68.49 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0806%\n",
      "layer   2  Sparsity: 53.5848%\n",
      "layer   3  Sparsity: 51.2895%\n",
      "total_backward_count 362230 real_backward_count 46510  12.840%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  0.018862/  0.048357, val:  82.08%, val_best:  85.00%, tr:  99.08%, tr_best:  99.69%, epoch time: 68.77 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   2  Sparsity: 53.7038%\n",
      "layer   3  Sparsity: 51.0406%\n",
      "total_backward_count 372020 real_backward_count 47225  12.694%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  0.019005/  0.045052, val:  80.42%, val_best:  85.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 68.68 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1348%\n",
      "layer   2  Sparsity: 53.6406%\n",
      "layer   3  Sparsity: 51.0468%\n",
      "total_backward_count 381810 real_backward_count 47968  12.563%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  0.018608/  0.047205, val:  82.92%, val_best:  85.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 68.71 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 53.5881%\n",
      "layer   3  Sparsity: 51.0333%\n",
      "total_backward_count 391600 real_backward_count 48688  12.433%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  0.017806/  0.051524, val:  78.33%, val_best:  85.00%, tr:  99.69%, tr_best:  99.80%, epoch time: 68.93 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   2  Sparsity: 53.7220%\n",
      "layer   3  Sparsity: 50.9517%\n",
      "total_backward_count 401390 real_backward_count 49368  12.299%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  0.017842/  0.046252, val:  82.08%, val_best:  85.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 68.55 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0515%\n",
      "layer   2  Sparsity: 53.6930%\n",
      "layer   3  Sparsity: 51.2676%\n",
      "total_backward_count 411180 real_backward_count 50063  12.175%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  0.017603/  0.039737, val:  83.75%, val_best:  85.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 68.53 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0583%\n",
      "layer   2  Sparsity: 53.7786%\n",
      "layer   3  Sparsity: 51.1111%\n",
      "total_backward_count 420970 real_backward_count 50753  12.056%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  0.017641/  0.046155, val:  80.00%, val_best:  85.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 68.53 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   2  Sparsity: 53.8840%\n",
      "layer   3  Sparsity: 51.0510%\n",
      "total_backward_count 430760 real_backward_count 51437  11.941%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  0.017312/  0.046846, val:  79.17%, val_best:  85.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 68.71 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.1024%\n",
      "layer   2  Sparsity: 53.8071%\n",
      "layer   3  Sparsity: 50.9015%\n",
      "total_backward_count 440550 real_backward_count 52084  11.822%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  0.017101/  0.051845, val:  80.00%, val_best:  85.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 68.10 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   2  Sparsity: 53.7790%\n",
      "layer   3  Sparsity: 50.9100%\n",
      "total_backward_count 450340 real_backward_count 52743  11.712%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  0.017194/  0.044336, val:  86.25%, val_best:  86.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 68.83 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 53.6571%\n",
      "layer   3  Sparsity: 50.7552%\n",
      "total_backward_count 460130 real_backward_count 53399  11.605%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  0.016122/  0.039341, val:  87.92%, val_best:  87.92%, tr:  99.69%, tr_best:  99.80%, epoch time: 68.37 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0710%\n",
      "layer   2  Sparsity: 53.6429%\n",
      "layer   3  Sparsity: 50.8126%\n",
      "total_backward_count 469920 real_backward_count 54010  11.493%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  0.016220/  0.041601, val:  85.00%, val_best:  87.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 68.56 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   2  Sparsity: 53.7967%\n",
      "layer   3  Sparsity: 50.8386%\n",
      "total_backward_count 479710 real_backward_count 54628  11.388%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  0.017170/  0.041298, val:  83.75%, val_best:  87.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 68.84 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   2  Sparsity: 53.7888%\n",
      "layer   3  Sparsity: 50.9054%\n",
      "total_backward_count 489500 real_backward_count 55303  11.298%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  0.016392/  0.042827, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.39 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0650%\n",
      "layer   2  Sparsity: 53.6896%\n",
      "layer   3  Sparsity: 50.8452%\n",
      "total_backward_count 499290 real_backward_count 55926  11.201%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  0.016381/  0.039437, val:  84.17%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.65 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0050%\n",
      "layer   2  Sparsity: 53.6704%\n",
      "layer   3  Sparsity: 50.8208%\n",
      "total_backward_count 509080 real_backward_count 56574  11.113%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  0.015765/  0.051210, val:  78.33%, val_best:  87.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 68.56 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1253%\n",
      "layer   2  Sparsity: 53.7447%\n",
      "layer   3  Sparsity: 50.9520%\n",
      "total_backward_count 518870 real_backward_count 57155  11.015%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  0.015891/  0.047066, val:  82.08%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 69.12 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0796%\n",
      "layer   2  Sparsity: 53.8117%\n",
      "layer   3  Sparsity: 50.9626%\n",
      "total_backward_count 528660 real_backward_count 57754  10.925%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  0.015087/  0.039823, val:  85.83%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 68.29 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0416%\n",
      "layer   2  Sparsity: 53.8673%\n",
      "layer   3  Sparsity: 50.9271%\n",
      "total_backward_count 538450 real_backward_count 58333  10.834%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  0.015515/  0.042695, val:  85.83%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.65 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   2  Sparsity: 53.7630%\n",
      "layer   3  Sparsity: 51.0217%\n",
      "total_backward_count 548240 real_backward_count 58918  10.747%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  0.015615/  0.043364, val:  85.42%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.51 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0640%\n",
      "layer   2  Sparsity: 53.7276%\n",
      "layer   3  Sparsity: 51.0671%\n",
      "total_backward_count 558030 real_backward_count 59500  10.663%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  0.015238/  0.040783, val:  85.83%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.45 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0570%\n",
      "layer   2  Sparsity: 53.9131%\n",
      "layer   3  Sparsity: 50.9070%\n",
      "total_backward_count 567820 real_backward_count 60053  10.576%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  0.015448/  0.038776, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.91 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0259%\n",
      "layer   2  Sparsity: 53.8502%\n",
      "layer   3  Sparsity: 50.8829%\n",
      "total_backward_count 577610 real_backward_count 60639  10.498%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  0.015491/  0.043732, val:  80.83%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 68.45 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0704%\n",
      "layer   2  Sparsity: 53.8142%\n",
      "layer   3  Sparsity: 50.9778%\n",
      "total_backward_count 587400 real_backward_count 61239  10.425%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  0.014978/  0.040944, val:  86.67%, val_best:  87.92%, tr:  99.49%, tr_best: 100.00%, epoch time: 68.70 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.1152%\n",
      "layer   2  Sparsity: 53.7722%\n",
      "layer   3  Sparsity: 51.2059%\n",
      "total_backward_count 597190 real_backward_count 61787  10.346%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  0.014731/  0.041976, val:  81.25%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.76 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0430%\n",
      "layer   2  Sparsity: 53.8624%\n",
      "layer   3  Sparsity: 51.0925%\n",
      "total_backward_count 606980 real_backward_count 62345  10.271%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  0.014381/  0.038886, val:  88.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.33 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   2  Sparsity: 53.8478%\n",
      "layer   3  Sparsity: 51.0558%\n",
      "total_backward_count 616770 real_backward_count 62862  10.192%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  0.014529/  0.038009, val:  86.25%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.38 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   2  Sparsity: 53.9013%\n",
      "layer   3  Sparsity: 51.1277%\n",
      "total_backward_count 626560 real_backward_count 63421  10.122%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  0.014383/  0.041552, val:  84.58%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.57 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0586%\n",
      "layer   2  Sparsity: 53.9763%\n",
      "layer   3  Sparsity: 51.1438%\n",
      "total_backward_count 636350 real_backward_count 63987  10.055%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  0.014270/  0.038568, val:  87.08%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.99 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0506%\n",
      "layer   2  Sparsity: 53.8450%\n",
      "layer   3  Sparsity: 50.9325%\n",
      "total_backward_count 646140 real_backward_count 64533   9.987%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  0.013693/  0.037522, val:  86.67%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 69.02 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.1147%\n",
      "layer   2  Sparsity: 53.7934%\n",
      "layer   3  Sparsity: 51.0000%\n",
      "total_backward_count 655930 real_backward_count 65033   9.915%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  0.013685/  0.040639, val:  87.08%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.32 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0549%\n",
      "layer   2  Sparsity: 53.9402%\n",
      "layer   3  Sparsity: 51.0048%\n",
      "total_backward_count 665720 real_backward_count 65520   9.842%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  0.013880/  0.041559, val:  85.42%, val_best:  88.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 69.16 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   2  Sparsity: 53.8705%\n",
      "layer   3  Sparsity: 51.0960%\n",
      "total_backward_count 675510 real_backward_count 66044   9.777%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  0.013859/  0.038808, val:  86.25%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.24 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   2  Sparsity: 53.9003%\n",
      "layer   3  Sparsity: 51.1263%\n",
      "total_backward_count 685300 real_backward_count 66574   9.715%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  0.013465/  0.043398, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.00 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 53.9337%\n",
      "layer   3  Sparsity: 51.1969%\n",
      "total_backward_count 695090 real_backward_count 67081   9.651%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  0.012978/  0.044508, val:  82.92%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.44 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 53.9383%\n",
      "layer   3  Sparsity: 51.1917%\n",
      "total_backward_count 704880 real_backward_count 67556   9.584%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  0.013356/  0.042738, val:  85.83%, val_best:  88.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 68.69 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0783%\n",
      "layer   2  Sparsity: 54.0616%\n",
      "layer   3  Sparsity: 51.2164%\n",
      "total_backward_count 714670 real_backward_count 68067   9.524%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  0.013070/  0.037746, val:  87.50%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.61 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 53.9433%\n",
      "layer   3  Sparsity: 51.1634%\n",
      "total_backward_count 724460 real_backward_count 68568   9.465%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  0.013174/  0.043671, val:  83.75%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.22 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1127%\n",
      "layer   2  Sparsity: 53.9680%\n",
      "layer   3  Sparsity: 51.2014%\n",
      "total_backward_count 734250 real_backward_count 69046   9.404%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  0.013194/  0.041778, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.65 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0721%\n",
      "layer   2  Sparsity: 54.0586%\n",
      "layer   3  Sparsity: 51.2745%\n",
      "total_backward_count 744040 real_backward_count 69533   9.345%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  0.012420/  0.038238, val:  87.50%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.77 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   2  Sparsity: 54.0824%\n",
      "layer   3  Sparsity: 51.4002%\n",
      "total_backward_count 753830 real_backward_count 69990   9.285%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  0.013187/  0.037746, val:  87.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.44 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0969%\n",
      "layer   2  Sparsity: 54.0249%\n",
      "layer   3  Sparsity: 51.3432%\n",
      "total_backward_count 763620 real_backward_count 70501   9.232%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  0.012610/  0.039452, val:  86.67%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.36 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0609%\n",
      "layer   2  Sparsity: 53.9652%\n",
      "layer   3  Sparsity: 51.1543%\n",
      "total_backward_count 773410 real_backward_count 70963   9.175%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  0.012918/  0.044786, val:  83.75%, val_best:  88.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 68.24 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1269%\n",
      "layer   2  Sparsity: 53.9231%\n",
      "layer   3  Sparsity: 51.3726%\n",
      "total_backward_count 783200 real_backward_count 71435   9.121%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  0.012140/  0.038172, val:  86.25%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.44 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   2  Sparsity: 54.0450%\n",
      "layer   3  Sparsity: 51.3355%\n",
      "total_backward_count 792990 real_backward_count 71855   9.061%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  0.012870/  0.039370, val:  85.00%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.40 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 54.0411%\n",
      "layer   3  Sparsity: 51.2137%\n",
      "total_backward_count 802780 real_backward_count 72360   9.014%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  0.012364/  0.038849, val:  86.67%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.61 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   2  Sparsity: 53.9333%\n",
      "layer   3  Sparsity: 51.3061%\n",
      "total_backward_count 812570 real_backward_count 72792   8.958%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  0.011964/  0.037511, val:  87.92%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.72 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0951%\n",
      "layer   2  Sparsity: 54.0050%\n",
      "layer   3  Sparsity: 51.3100%\n",
      "total_backward_count 822360 real_backward_count 73227   8.904%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  0.012129/  0.036187, val:  88.33%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   2  Sparsity: 54.0941%\n",
      "layer   3  Sparsity: 51.3845%\n",
      "total_backward_count 832150 real_backward_count 73686   8.855%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  0.011824/  0.036286, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.65 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0923%\n",
      "layer   2  Sparsity: 54.1110%\n",
      "layer   3  Sparsity: 51.3559%\n",
      "total_backward_count 841940 real_backward_count 74120   8.803%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  0.012131/  0.037194, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.98 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 54.0941%\n",
      "layer   3  Sparsity: 51.2492%\n",
      "total_backward_count 851730 real_backward_count 74545   8.752%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  0.012226/  0.043673, val:  84.58%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.83 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0514%\n",
      "layer   2  Sparsity: 54.1413%\n",
      "layer   3  Sparsity: 51.3635%\n",
      "total_backward_count 861520 real_backward_count 74995   8.705%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  0.012398/  0.036421, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.78 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0605%\n",
      "layer   2  Sparsity: 54.1339%\n",
      "layer   3  Sparsity: 51.4487%\n",
      "total_backward_count 871310 real_backward_count 75453   8.660%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  0.011672/  0.040989, val:  84.17%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.61 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1052%\n",
      "layer   2  Sparsity: 54.1258%\n",
      "layer   3  Sparsity: 51.2376%\n",
      "total_backward_count 881100 real_backward_count 75862   8.610%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  0.012337/  0.036992, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.67 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   2  Sparsity: 54.0476%\n",
      "layer   3  Sparsity: 51.2630%\n",
      "total_backward_count 890890 real_backward_count 76340   8.569%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  0.011993/  0.036227, val:  87.92%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.65 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   2  Sparsity: 54.1439%\n",
      "layer   3  Sparsity: 51.4042%\n",
      "total_backward_count 900680 real_backward_count 76757   8.522%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  0.011608/  0.041289, val:  86.25%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.23 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0927%\n",
      "layer   2  Sparsity: 54.1305%\n",
      "layer   3  Sparsity: 51.3308%\n",
      "total_backward_count 910470 real_backward_count 77171   8.476%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  0.011357/  0.036237, val:  87.92%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.72 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0824%\n",
      "layer   2  Sparsity: 54.0011%\n",
      "layer   3  Sparsity: 51.1507%\n",
      "total_backward_count 920260 real_backward_count 77576   8.430%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  0.011150/  0.038499, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.56 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0703%\n",
      "layer   2  Sparsity: 54.1767%\n",
      "layer   3  Sparsity: 51.2809%\n",
      "total_backward_count 930050 real_backward_count 77941   8.380%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  0.010955/  0.037640, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.77 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0387%\n",
      "layer   2  Sparsity: 54.1847%\n",
      "layer   3  Sparsity: 51.3283%\n",
      "total_backward_count 939840 real_backward_count 78326   8.334%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  0.011149/  0.040430, val:  83.75%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.13 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0941%\n",
      "layer   2  Sparsity: 54.1263%\n",
      "layer   3  Sparsity: 51.2746%\n",
      "total_backward_count 949630 real_backward_count 78709   8.288%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  0.010902/  0.037735, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.89 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0824%\n",
      "layer   2  Sparsity: 54.0918%\n",
      "layer   3  Sparsity: 51.1448%\n",
      "total_backward_count 959420 real_backward_count 79106   8.245%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  0.011165/  0.046840, val:  84.17%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.55 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   2  Sparsity: 54.1324%\n",
      "layer   3  Sparsity: 51.1982%\n",
      "total_backward_count 969210 real_backward_count 79494   8.202%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  0.010965/  0.041108, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.38 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0574%\n",
      "layer   2  Sparsity: 54.1391%\n",
      "layer   3  Sparsity: 51.2272%\n",
      "total_backward_count 979000 real_backward_count 79863   8.158%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  0.010824/  0.038367, val:  85.83%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.33 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0739%\n",
      "layer   2  Sparsity: 54.1990%\n",
      "layer   3  Sparsity: 51.3152%\n",
      "total_backward_count 988790 real_backward_count 80247   8.116%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  0.010705/  0.036837, val:  88.75%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.58 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0859%\n",
      "layer   2  Sparsity: 54.2805%\n",
      "layer   3  Sparsity: 51.4395%\n",
      "total_backward_count 998580 real_backward_count 80633   8.075%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  0.010682/  0.036185, val:  86.67%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.63 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   2  Sparsity: 54.3208%\n",
      "layer   3  Sparsity: 51.3355%\n",
      "total_backward_count 1008370 real_backward_count 80992   8.032%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  0.010359/  0.040944, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.75 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.1449%\n",
      "layer   2  Sparsity: 54.2592%\n",
      "layer   3  Sparsity: 51.4062%\n",
      "total_backward_count 1018160 real_backward_count 81375   7.992%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  0.010718/  0.038599, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.88 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 54.3531%\n",
      "layer   3  Sparsity: 51.2575%\n",
      "total_backward_count 1027950 real_backward_count 81767   7.954%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  0.010853/  0.039299, val:  86.67%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.49 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0502%\n",
      "layer   2  Sparsity: 54.3328%\n",
      "layer   3  Sparsity: 51.4096%\n",
      "total_backward_count 1037740 real_backward_count 82155   7.917%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  0.010219/  0.039078, val:  86.25%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.78 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 54.4158%\n",
      "layer   3  Sparsity: 51.3803%\n",
      "total_backward_count 1047530 real_backward_count 82509   7.877%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  0.009863/  0.048658, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.83 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0573%\n",
      "layer   2  Sparsity: 54.4393%\n",
      "layer   3  Sparsity: 51.3863%\n",
      "total_backward_count 1057320 real_backward_count 82839   7.835%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  0.010639/  0.036467, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.94 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0638%\n",
      "layer   2  Sparsity: 54.3889%\n",
      "layer   3  Sparsity: 51.3568%\n",
      "total_backward_count 1067110 real_backward_count 83214   7.798%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  0.010224/  0.039825, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.65 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0799%\n",
      "layer   2  Sparsity: 54.4491%\n",
      "layer   3  Sparsity: 51.3106%\n",
      "total_backward_count 1076900 real_backward_count 83556   7.759%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  0.010529/  0.037934, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.20 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 54.4723%\n",
      "layer   3  Sparsity: 51.3167%\n",
      "total_backward_count 1086690 real_backward_count 83927   7.723%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  0.009594/  0.036906, val:  86.25%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.68 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   2  Sparsity: 54.3515%\n",
      "layer   3  Sparsity: 51.5577%\n",
      "total_backward_count 1096480 real_backward_count 84251   7.684%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  0.010436/  0.039215, val:  86.25%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.82 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0579%\n",
      "layer   2  Sparsity: 54.3537%\n",
      "layer   3  Sparsity: 51.4223%\n",
      "total_backward_count 1106270 real_backward_count 84637   7.651%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  0.010235/  0.036050, val:  88.33%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.85 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   2  Sparsity: 54.3751%\n",
      "layer   3  Sparsity: 51.5966%\n",
      "total_backward_count 1116060 real_backward_count 84997   7.616%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  0.010067/  0.036606, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.72 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.1043%\n",
      "layer   2  Sparsity: 54.4436%\n",
      "layer   3  Sparsity: 51.5678%\n",
      "total_backward_count 1125850 real_backward_count 85350   7.581%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  0.010059/  0.036357, val:  89.17%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.26 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   2  Sparsity: 54.5675%\n",
      "layer   3  Sparsity: 51.5332%\n",
      "total_backward_count 1135640 real_backward_count 85703   7.547%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  0.009947/  0.036152, val:  87.08%, val_best:  89.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.38 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0824%\n",
      "layer   2  Sparsity: 54.4755%\n",
      "layer   3  Sparsity: 51.5267%\n",
      "total_backward_count 1145430 real_backward_count 86051   7.513%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  0.009767/  0.038395, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.30 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   2  Sparsity: 54.5268%\n",
      "layer   3  Sparsity: 51.6706%\n",
      "total_backward_count 1155220 real_backward_count 86399   7.479%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  0.009687/  0.036936, val:  86.67%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.51 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   2  Sparsity: 54.4594%\n",
      "layer   3  Sparsity: 51.7581%\n",
      "total_backward_count 1165010 real_backward_count 86741   7.446%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  0.009624/  0.035942, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.78 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0596%\n",
      "layer   2  Sparsity: 54.5635%\n",
      "layer   3  Sparsity: 51.6398%\n",
      "total_backward_count 1174800 real_backward_count 87074   7.412%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  0.009630/  0.035664, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.79 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0342%\n",
      "layer   2  Sparsity: 54.5767%\n",
      "layer   3  Sparsity: 51.6098%\n",
      "total_backward_count 1184590 real_backward_count 87431   7.381%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  0.009618/  0.036438, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.43 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1025%\n",
      "layer   2  Sparsity: 54.4789%\n",
      "layer   3  Sparsity: 51.5591%\n",
      "total_backward_count 1194380 real_backward_count 87773   7.349%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  0.010034/  0.037937, val:  87.92%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.06 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   2  Sparsity: 54.4883%\n",
      "layer   3  Sparsity: 51.5594%\n",
      "total_backward_count 1204170 real_backward_count 88135   7.319%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  0.009776/  0.036457, val:  87.50%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.71 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0429%\n",
      "layer   2  Sparsity: 54.4845%\n",
      "layer   3  Sparsity: 51.5861%\n",
      "total_backward_count 1213960 real_backward_count 88463   7.287%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  0.009601/  0.036495, val:  87.08%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.68 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0583%\n",
      "layer   2  Sparsity: 54.4870%\n",
      "layer   3  Sparsity: 51.7105%\n",
      "total_backward_count 1223750 real_backward_count 88810   7.257%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  0.009057/  0.035252, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.84 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0681%\n",
      "layer   2  Sparsity: 54.5434%\n",
      "layer   3  Sparsity: 51.7843%\n",
      "total_backward_count 1233540 real_backward_count 89105   7.224%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  0.009629/  0.037732, val:  87.50%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.50 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 54.4894%\n",
      "layer   3  Sparsity: 51.8600%\n",
      "total_backward_count 1243330 real_backward_count 89435   7.193%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  0.008771/  0.035285, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.04 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1033%\n",
      "layer   2  Sparsity: 54.4957%\n",
      "layer   3  Sparsity: 51.8797%\n",
      "total_backward_count 1253120 real_backward_count 89713   7.159%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  0.009141/  0.036241, val:  87.50%, val_best:  89.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.64 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 54.5098%\n",
      "layer   3  Sparsity: 51.9509%\n",
      "total_backward_count 1262910 real_backward_count 90017   7.128%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  0.009442/  0.036245, val:  87.08%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.76 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.1064%\n",
      "layer   2  Sparsity: 54.5285%\n",
      "layer   3  Sparsity: 51.8908%\n",
      "total_backward_count 1272700 real_backward_count 90347   7.099%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  0.008923/  0.035590, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.78 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0983%\n",
      "layer   2  Sparsity: 54.5528%\n",
      "layer   3  Sparsity: 51.8965%\n",
      "total_backward_count 1282490 real_backward_count 90619   7.066%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  0.009403/  0.035022, val:  87.92%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.86 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 54.5638%\n",
      "layer   3  Sparsity: 51.8816%\n",
      "total_backward_count 1292280 real_backward_count 90946   7.038%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  0.009496/  0.038520, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.91 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0287%\n",
      "layer   2  Sparsity: 54.6006%\n",
      "layer   3  Sparsity: 51.8091%\n",
      "total_backward_count 1302070 real_backward_count 91274   7.010%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  0.009079/  0.036316, val:  87.92%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.55 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0923%\n",
      "layer   2  Sparsity: 54.6821%\n",
      "layer   3  Sparsity: 51.7966%\n",
      "total_backward_count 1311860 real_backward_count 91583   6.981%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  0.008911/  0.036037, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.87 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.1069%\n",
      "layer   2  Sparsity: 54.6534%\n",
      "layer   3  Sparsity: 51.8039%\n",
      "total_backward_count 1321650 real_backward_count 91867   6.951%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  0.008786/  0.035617, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.59 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 54.4926%\n",
      "layer   3  Sparsity: 51.7299%\n",
      "total_backward_count 1331440 real_backward_count 92170   6.923%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  0.009055/  0.035441, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.92 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   2  Sparsity: 54.5420%\n",
      "layer   3  Sparsity: 51.9539%\n",
      "total_backward_count 1341230 real_backward_count 92467   6.894%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  0.009054/  0.036345, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.86 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   2  Sparsity: 54.5508%\n",
      "layer   3  Sparsity: 51.9068%\n",
      "total_backward_count 1351020 real_backward_count 92758   6.866%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  0.009430/  0.037236, val:  87.50%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.56 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   2  Sparsity: 54.4708%\n",
      "layer   3  Sparsity: 51.9278%\n",
      "total_backward_count 1360810 real_backward_count 93082   6.840%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  0.008396/  0.035282, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.48 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0609%\n",
      "layer   2  Sparsity: 54.5319%\n",
      "layer   3  Sparsity: 51.9398%\n",
      "total_backward_count 1370600 real_backward_count 93341   6.810%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  0.008803/  0.036753, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.87 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0420%\n",
      "layer   2  Sparsity: 54.5877%\n",
      "layer   3  Sparsity: 51.9135%\n",
      "total_backward_count 1380390 real_backward_count 93631   6.783%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  0.008720/  0.036302, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.77 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0550%\n",
      "layer   2  Sparsity: 54.6201%\n",
      "layer   3  Sparsity: 51.8593%\n",
      "total_backward_count 1390180 real_backward_count 93907   6.755%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  0.008727/  0.038272, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.65 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0809%\n",
      "layer   2  Sparsity: 54.6522%\n",
      "layer   3  Sparsity: 51.9055%\n",
      "total_backward_count 1399970 real_backward_count 94199   6.729%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  0.008860/  0.037201, val:  87.08%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.58 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   2  Sparsity: 54.6314%\n",
      "layer   3  Sparsity: 51.9598%\n",
      "total_backward_count 1409760 real_backward_count 94508   6.704%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  0.008612/  0.036157, val:  88.33%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.47 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1093%\n",
      "layer   2  Sparsity: 54.5633%\n",
      "layer   3  Sparsity: 51.9003%\n",
      "total_backward_count 1419550 real_backward_count 94806   6.679%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  0.008696/  0.043007, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 64.26 seconds, 1.07 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   2  Sparsity: 54.5339%\n",
      "layer   3  Sparsity: 51.8565%\n",
      "total_backward_count 1429340 real_backward_count 95101   6.653%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  0.008607/  0.037572, val:  86.67%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.82 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   2  Sparsity: 54.6087%\n",
      "layer   3  Sparsity: 51.9898%\n",
      "total_backward_count 1439130 real_backward_count 95399   6.629%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  0.008684/  0.037005, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.75 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0462%\n",
      "layer   2  Sparsity: 54.6309%\n",
      "layer   3  Sparsity: 52.0616%\n",
      "total_backward_count 1448920 real_backward_count 95680   6.604%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  0.008774/  0.036146, val:  87.50%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.63 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   2  Sparsity: 54.5976%\n",
      "layer   3  Sparsity: 52.0411%\n",
      "total_backward_count 1458710 real_backward_count 95983   6.580%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  0.008474/  0.037569, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.29 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   2  Sparsity: 54.6714%\n",
      "layer   3  Sparsity: 52.0137%\n",
      "total_backward_count 1468500 real_backward_count 96276   6.556%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  0.008489/  0.036415, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.47 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0783%\n",
      "layer   2  Sparsity: 54.7949%\n",
      "layer   3  Sparsity: 51.8577%\n",
      "total_backward_count 1478290 real_backward_count 96545   6.531%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  0.008188/  0.038202, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.32 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 54.7776%\n",
      "layer   3  Sparsity: 51.8180%\n",
      "total_backward_count 1488080 real_backward_count 96799   6.505%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  0.008301/  0.035693, val:  91.25%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.42 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0571%\n",
      "layer   2  Sparsity: 54.6922%\n",
      "layer   3  Sparsity: 51.8578%\n",
      "total_backward_count 1497870 real_backward_count 97092   6.482%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  0.008185/  0.035871, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.00 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   2  Sparsity: 54.7703%\n",
      "layer   3  Sparsity: 51.9591%\n",
      "total_backward_count 1507660 real_backward_count 97365   6.458%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  0.008191/  0.042130, val:  87.50%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.73 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   2  Sparsity: 54.7661%\n",
      "layer   3  Sparsity: 51.9844%\n",
      "total_backward_count 1517450 real_backward_count 97629   6.434%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  0.008232/  0.037819, val:  87.50%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.55 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0924%\n",
      "layer   2  Sparsity: 54.7392%\n",
      "layer   3  Sparsity: 52.0203%\n",
      "total_backward_count 1527240 real_backward_count 97900   6.410%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  0.008233/  0.036480, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.84 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0645%\n",
      "layer   2  Sparsity: 54.7340%\n",
      "layer   3  Sparsity: 51.9335%\n",
      "total_backward_count 1537030 real_backward_count 98154   6.386%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  0.007903/  0.036991, val:  88.75%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.66 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   2  Sparsity: 54.7059%\n",
      "layer   3  Sparsity: 51.8966%\n",
      "total_backward_count 1546820 real_backward_count 98417   6.363%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  0.008081/  0.038130, val:  86.67%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.23 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1222%\n",
      "layer   2  Sparsity: 54.6616%\n",
      "layer   3  Sparsity: 52.0176%\n",
      "total_backward_count 1556610 real_backward_count 98699   6.341%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  0.008348/  0.035713, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.87 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0736%\n",
      "layer   2  Sparsity: 54.6642%\n",
      "layer   3  Sparsity: 51.9462%\n",
      "total_backward_count 1566400 real_backward_count 98983   6.319%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  0.007788/  0.035998, val:  88.33%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.02 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1089%\n",
      "layer   2  Sparsity: 54.6688%\n",
      "layer   3  Sparsity: 52.0066%\n",
      "total_backward_count 1576190 real_backward_count 99225   6.295%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  0.008564/  0.035518, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.17 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0624%\n",
      "layer   2  Sparsity: 54.7131%\n",
      "layer   3  Sparsity: 51.9410%\n",
      "total_backward_count 1585980 real_backward_count 99533   6.276%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  0.008162/  0.035173, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.15 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 54.6265%\n",
      "layer   3  Sparsity: 51.9171%\n",
      "total_backward_count 1595770 real_backward_count 99824   6.256%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  0.008165/  0.036530, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.87 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0874%\n",
      "layer   2  Sparsity: 54.6096%\n",
      "layer   3  Sparsity: 51.9476%\n",
      "total_backward_count 1605560 real_backward_count 100100   6.235%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  0.008144/  0.037174, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.49 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0728%\n",
      "layer   2  Sparsity: 54.5957%\n",
      "layer   3  Sparsity: 51.9097%\n",
      "total_backward_count 1615350 real_backward_count 100376   6.214%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  0.007756/  0.037352, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.83 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0607%\n",
      "layer   2  Sparsity: 54.6212%\n",
      "layer   3  Sparsity: 51.9615%\n",
      "total_backward_count 1625140 real_backward_count 100627   6.192%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  0.008124/  0.036215, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.02 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1330%\n",
      "layer   2  Sparsity: 54.7323%\n",
      "layer   3  Sparsity: 52.2855%\n",
      "total_backward_count 1634930 real_backward_count 100920   6.173%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  0.007895/  0.038083, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.29 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   2  Sparsity: 54.6225%\n",
      "layer   3  Sparsity: 52.1440%\n",
      "total_backward_count 1644720 real_backward_count 101161   6.151%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  0.008370/  0.036530, val:  88.33%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.44 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0824%\n",
      "layer   2  Sparsity: 54.6947%\n",
      "layer   3  Sparsity: 52.1063%\n",
      "total_backward_count 1654510 real_backward_count 101445   6.131%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  0.007336/  0.036197, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.05 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1249%\n",
      "layer   2  Sparsity: 54.7613%\n",
      "layer   3  Sparsity: 52.2174%\n",
      "total_backward_count 1664300 real_backward_count 101685   6.110%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  0.007508/  0.035504, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0333%\n",
      "layer   2  Sparsity: 54.7896%\n",
      "layer   3  Sparsity: 52.3231%\n",
      "total_backward_count 1674090 real_backward_count 101920   6.088%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  0.007478/  0.035683, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.51 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0728%\n",
      "layer   2  Sparsity: 54.8434%\n",
      "layer   3  Sparsity: 52.4156%\n",
      "total_backward_count 1683880 real_backward_count 102148   6.066%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  0.007817/  0.035412, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.10 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   2  Sparsity: 54.8646%\n",
      "layer   3  Sparsity: 52.3678%\n",
      "total_backward_count 1693670 real_backward_count 102400   6.046%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  0.007890/  0.036683, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.66 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1098%\n",
      "layer   2  Sparsity: 54.8390%\n",
      "layer   3  Sparsity: 52.3453%\n",
      "total_backward_count 1703460 real_backward_count 102661   6.027%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  0.008199/  0.038089, val:  86.67%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.01 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1013%\n",
      "layer   2  Sparsity: 54.7227%\n",
      "layer   3  Sparsity: 52.3002%\n",
      "total_backward_count 1713250 real_backward_count 102937   6.008%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  0.007434/  0.036272, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.80 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0375%\n",
      "layer   2  Sparsity: 54.7009%\n",
      "layer   3  Sparsity: 52.3316%\n",
      "total_backward_count 1723040 real_backward_count 103177   5.988%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  0.007523/  0.035302, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.12 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0457%\n",
      "layer   2  Sparsity: 54.7118%\n",
      "layer   3  Sparsity: 52.1914%\n",
      "total_backward_count 1732830 real_backward_count 103423   5.968%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  0.007409/  0.037643, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.96 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   2  Sparsity: 54.7340%\n",
      "layer   3  Sparsity: 52.1966%\n",
      "total_backward_count 1742620 real_backward_count 103661   5.949%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  0.007164/  0.035963, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.07 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0851%\n",
      "layer   2  Sparsity: 54.7895%\n",
      "layer   3  Sparsity: 52.2166%\n",
      "total_backward_count 1752410 real_backward_count 103896   5.929%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  0.007409/  0.037129, val:  87.50%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.28 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0957%\n",
      "layer   2  Sparsity: 54.7849%\n",
      "layer   3  Sparsity: 52.1979%\n",
      "total_backward_count 1762200 real_backward_count 104142   5.910%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  0.007315/  0.035829, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.43 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0623%\n",
      "layer   2  Sparsity: 54.7755%\n",
      "layer   3  Sparsity: 52.1627%\n",
      "total_backward_count 1771990 real_backward_count 104374   5.890%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  0.007372/  0.033738, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.75 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   2  Sparsity: 54.7078%\n",
      "layer   3  Sparsity: 52.1283%\n",
      "total_backward_count 1781780 real_backward_count 104619   5.872%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  0.007595/  0.036864, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.53 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0916%\n",
      "layer   2  Sparsity: 54.7916%\n",
      "layer   3  Sparsity: 52.2459%\n",
      "total_backward_count 1791570 real_backward_count 104895   5.855%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  0.007177/  0.038182, val:  85.83%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.52 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0533%\n",
      "layer   2  Sparsity: 54.8479%\n",
      "layer   3  Sparsity: 52.2788%\n",
      "total_backward_count 1801360 real_backward_count 105117   5.835%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  0.007377/  0.035590, val:  88.33%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   2  Sparsity: 54.8509%\n",
      "layer   3  Sparsity: 52.3632%\n",
      "total_backward_count 1811150 real_backward_count 105357   5.817%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  0.007229/  0.036569, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.82 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0399%\n",
      "layer   2  Sparsity: 54.9233%\n",
      "layer   3  Sparsity: 52.3315%\n",
      "total_backward_count 1820940 real_backward_count 105592   5.799%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  0.007414/  0.036294, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.72 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0979%\n",
      "layer   2  Sparsity: 54.9480%\n",
      "layer   3  Sparsity: 52.3628%\n",
      "total_backward_count 1830730 real_backward_count 105831   5.781%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  0.007599/  0.037247, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.99 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0796%\n",
      "layer   2  Sparsity: 55.0094%\n",
      "layer   3  Sparsity: 52.4130%\n",
      "total_backward_count 1840520 real_backward_count 106077   5.763%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  0.007208/  0.037008, val:  88.33%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.77 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0373%\n",
      "layer   2  Sparsity: 54.9220%\n",
      "layer   3  Sparsity: 52.4955%\n",
      "total_backward_count 1850310 real_backward_count 106316   5.746%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  0.007199/  0.035228, val:  87.50%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.50 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   2  Sparsity: 54.7418%\n",
      "layer   3  Sparsity: 52.4015%\n",
      "total_backward_count 1860100 real_backward_count 106545   5.728%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  0.007471/  0.035404, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.19 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   2  Sparsity: 54.7484%\n",
      "layer   3  Sparsity: 52.4801%\n",
      "total_backward_count 1869890 real_backward_count 106787   5.711%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  0.007355/  0.035024, val:  89.58%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.20 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0489%\n",
      "layer   2  Sparsity: 54.7541%\n",
      "layer   3  Sparsity: 52.3360%\n",
      "total_backward_count 1879680 real_backward_count 107021   5.694%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  0.006843/  0.041598, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.80 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0547%\n",
      "layer   2  Sparsity: 54.7726%\n",
      "layer   3  Sparsity: 52.3842%\n",
      "total_backward_count 1889470 real_backward_count 107213   5.674%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  0.006869/  0.037247, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.77 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   2  Sparsity: 54.8950%\n",
      "layer   3  Sparsity: 52.4057%\n",
      "total_backward_count 1899260 real_backward_count 107443   5.657%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  0.006923/  0.035869, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.48 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   2  Sparsity: 54.8473%\n",
      "layer   3  Sparsity: 52.5456%\n",
      "total_backward_count 1909050 real_backward_count 107668   5.640%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  0.007097/  0.036062, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.90 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0763%\n",
      "layer   2  Sparsity: 54.7851%\n",
      "layer   3  Sparsity: 52.6849%\n",
      "total_backward_count 1918840 real_backward_count 107909   5.624%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  0.007046/  0.036262, val:  88.75%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.10 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0983%\n",
      "layer   2  Sparsity: 54.6887%\n",
      "layer   3  Sparsity: 52.6319%\n",
      "total_backward_count 1928630 real_backward_count 108119   5.606%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  0.006754/  0.035659, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.71 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0910%\n",
      "layer   2  Sparsity: 54.7447%\n",
      "layer   3  Sparsity: 52.6397%\n",
      "total_backward_count 1938420 real_backward_count 108318   5.588%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  0.006947/  0.039787, val:  87.08%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.89 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0936%\n",
      "layer   2  Sparsity: 54.6838%\n",
      "layer   3  Sparsity: 52.6486%\n",
      "total_backward_count 1948210 real_backward_count 108545   5.572%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  0.007059/  0.038194, val:  89.58%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.55 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   2  Sparsity: 54.6884%\n",
      "layer   3  Sparsity: 52.7830%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5798f6cc0a5142f9a64030725b8e8553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>0.00706</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.03819</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cool-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/uu5s53eb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/uu5s53eb</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251119_185519-uu5s53eb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ew07sr9s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 19983\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251119_224424-ew07sr9s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ew07sr9s' target=\"_blank\">dashing-sweep-2</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ew07sr9s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ew07sr9s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251119_224433_621', 'my_seed': 19983, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.03125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[999, 998], [999, 999], [999, 999]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 0\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 0\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.03125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.03125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([ 4.3957e-02, -3.4717e-02,  3.9289e-02, -4.4862e-02, -2.8171e-02,\n",
      "         1.1556e-01,  1.7029e-01,  5.7028e-02, -5.3685e-02,  2.0771e-02,\n",
      "        -9.9898e-02,  1.0752e-01,  6.2032e-02,  8.4627e-02, -1.2802e-01,\n",
      "        -1.1433e-02,  1.8513e-02,  3.4939e-03, -1.0750e-01, -5.7251e-02,\n",
      "         1.6802e-01, -1.1995e-02, -1.4168e-01, -1.3399e-01,  1.4956e-01,\n",
      "        -1.1803e-01, -2.1017e-02, -1.2248e-02,  3.6118e-02, -1.3253e-02,\n",
      "         2.5429e-02,  9.6513e-02,  7.5777e-02,  5.6446e-03, -2.0565e-01,\n",
      "         6.2776e-02,  2.0228e-02,  3.3048e-02, -1.0568e-01,  3.3317e-02,\n",
      "         5.8057e-02, -4.4304e-02, -5.5091e-02, -4.4771e-02,  2.5230e-03,\n",
      "        -9.4305e-02,  1.0634e-01,  8.8737e-02,  1.2130e-01, -9.4193e-02,\n",
      "         4.8611e-02, -6.5078e-03,  4.0872e-02, -8.5127e-02,  2.9687e-02,\n",
      "        -9.4299e-03, -1.6650e-01,  7.1891e-02, -4.5058e-02,  6.3516e-03,\n",
      "        -1.7945e-02,  1.2455e-02, -1.0988e-01, -7.8953e-04,  7.1762e-02,\n",
      "         9.8883e-02,  4.3758e-02, -6.8641e-02,  6.6904e-02, -2.0416e-02,\n",
      "        -4.3964e-02,  1.2970e-02,  7.8800e-02, -1.5883e-01, -3.2786e-02,\n",
      "         7.2813e-02,  2.2389e-01,  1.7448e-01,  1.2813e-02, -5.8444e-02,\n",
      "         1.7062e-01,  3.8751e-02,  1.0689e-01, -9.2452e-02, -5.1940e-03,\n",
      "         3.4541e-02, -7.1549e-02, -4.6740e-02, -1.2584e-01,  6.1030e-02,\n",
      "        -1.0389e-01,  7.0303e-03,  1.3621e-01, -1.2988e-01,  3.2899e-03,\n",
      "        -3.1486e-02, -1.4388e-02, -5.0764e-02, -8.0623e-02, -2.0688e-02,\n",
      "         4.5459e-02, -6.9356e-02, -1.7348e-01, -1.6706e-01, -7.2817e-02,\n",
      "        -1.6764e-02, -9.2545e-02,  2.6626e-04,  2.2318e-02, -9.6979e-02,\n",
      "        -1.0703e-01,  5.3821e-02,  6.0286e-03, -1.9342e-02,  1.7719e-01,\n",
      "        -1.5214e-02, -5.3832e-02,  5.9515e-02,  5.2361e-02, -1.1954e-02,\n",
      "         4.2022e-02,  7.8879e-02,  6.4122e-02,  2.3997e-02,  5.2312e-02,\n",
      "        -1.0499e-01, -1.6914e-03,  4.7965e-02,  3.6198e-02, -1.3279e-01,\n",
      "        -6.1923e-02, -4.9388e-02,  1.5087e-02, -6.0131e-02,  8.6126e-02,\n",
      "         2.3506e-01, -2.0897e-02,  3.1682e-02,  1.5147e-01,  3.4941e-02,\n",
      "        -1.4526e-01, -1.2375e-02,  4.8247e-02,  1.7329e-02, -3.5345e-02,\n",
      "         1.8568e-01, -1.2772e-01,  5.9640e-02, -1.2901e-01, -1.1543e-01,\n",
      "        -2.1461e-02, -2.2945e-02, -2.9937e-02, -1.8241e-01, -1.2642e-02,\n",
      "        -6.6820e-02, -2.2115e-02,  6.2182e-02, -4.9034e-02,  1.2465e-01,\n",
      "        -1.0072e-01,  2.1866e-01,  5.4953e-02,  1.2860e-01,  2.5349e-02,\n",
      "         2.1607e-02,  4.5690e-02,  7.7341e-02, -7.8726e-02,  9.8227e-02,\n",
      "        -6.3492e-02,  1.0163e-02,  8.8736e-02, -1.1351e-02,  3.4517e-02,\n",
      "        -1.1968e-01, -1.0755e-02,  5.4635e-02, -3.6650e-02,  3.6211e-02,\n",
      "        -2.3257e-01, -1.5726e-01, -3.6421e-02, -5.6516e-02, -2.1740e-02,\n",
      "        -4.0906e-02, -5.7844e-02, -3.1644e-01,  7.8688e-03,  9.0519e-02,\n",
      "        -1.0441e-01,  2.6594e-02,  1.2654e-01, -4.5173e-03,  1.1911e-01,\n",
      "         1.0906e-01, -7.2109e-02,  1.0072e-02,  1.3376e-01, -6.6374e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([-1.7903e-01,  6.6041e-02, -1.1775e-01,  1.2081e-01, -9.5389e-02,\n",
      "        -1.8849e-01, -1.4939e-01,  4.5734e-02,  2.5202e-02,  2.5286e-02,\n",
      "        -2.2609e-02, -1.2253e-02,  1.1836e-01,  3.4916e-02, -8.4331e-02,\n",
      "         6.7019e-02, -1.1973e-01, -2.0503e-01, -1.0659e-01, -3.8755e-02,\n",
      "         3.0455e-02, -1.1674e-01,  9.0441e-02,  1.2865e-01,  1.5931e-01,\n",
      "         6.0239e-02,  9.0416e-02, -8.9540e-02, -1.1452e-01, -8.3703e-02,\n",
      "         1.1325e-02,  3.5524e-02,  4.6023e-02, -9.1228e-02, -1.0540e-01,\n",
      "         2.4909e-01, -4.7152e-02,  4.2363e-02, -2.8114e-01, -1.0489e-01,\n",
      "         7.3535e-02, -1.3546e-01,  5.6561e-02, -6.5313e-02,  1.3054e-02,\n",
      "        -5.5955e-02, -1.5616e-01,  2.1171e-02, -6.0122e-02, -1.1511e-01,\n",
      "        -3.6053e-02,  4.6994e-02,  2.3886e-01, -8.9810e-02,  5.4342e-02,\n",
      "        -1.2372e-02, -5.0812e-02,  1.3071e-01,  2.5137e-03,  9.2684e-02,\n",
      "        -1.2382e-01,  5.1792e-02, -1.0161e-01,  1.0036e-02,  1.4645e-01,\n",
      "        -1.4241e-05, -1.8779e-01,  5.5549e-02, -3.4692e-02, -9.3533e-02,\n",
      "         2.9456e-02, -4.4524e-02, -6.9227e-02,  1.4674e-01, -3.0008e-02,\n",
      "         2.3036e-02, -3.4291e-02, -7.2953e-02,  1.6075e-02, -3.5261e-02,\n",
      "         6.2648e-02, -1.5890e-02, -1.2650e-01, -1.2875e-01, -1.3981e-01,\n",
      "         1.0359e-01, -5.6278e-02,  4.4788e-02,  7.5215e-02, -1.6653e-01,\n",
      "         1.1132e-01, -2.6244e-01, -9.4577e-02,  8.2393e-02,  7.7532e-02,\n",
      "        -3.6125e-02,  6.4290e-02, -9.0163e-02, -8.1133e-05,  8.7502e-02,\n",
      "        -3.1087e-01, -5.6552e-02,  1.6939e-01, -7.2401e-04, -9.2105e-02,\n",
      "         2.1395e-02, -4.6487e-02,  9.9714e-02, -6.6118e-02, -1.7684e-01,\n",
      "        -3.7511e-02,  3.4562e-02,  1.4098e-01, -4.7894e-02,  1.1235e-01,\n",
      "        -1.9319e-01, -9.9734e-02, -1.7873e-01, -1.9586e-01, -1.3756e-02,\n",
      "        -1.8416e-01, -2.3755e-02,  4.7232e-02, -1.0632e-01, -1.9602e-02,\n",
      "        -4.7673e-03, -6.0335e-02,  2.6433e-01, -2.7361e-02,  1.3530e-01,\n",
      "        -5.1654e-02,  1.2702e-01,  1.2854e-01,  2.3305e-01,  1.1217e-01,\n",
      "         7.0959e-02,  3.1260e-02, -1.3031e-01, -1.3335e-01,  4.2431e-02,\n",
      "        -5.1987e-02, -1.4946e-02,  1.7012e-01, -3.4199e-02,  2.5367e-02,\n",
      "        -4.7556e-03, -1.4328e-01,  4.5756e-02,  2.5739e-02, -1.0933e-01,\n",
      "         1.0785e-02,  3.7914e-02,  6.7589e-02,  5.4514e-02,  2.8029e-01,\n",
      "         1.5859e-04,  5.9004e-03,  1.5558e-01,  2.2530e-02, -1.6582e-01,\n",
      "        -1.6628e-01,  1.2891e-01,  5.7170e-02,  7.9975e-02, -2.3154e-02,\n",
      "        -7.2639e-02,  1.1025e-01,  1.0091e-01,  2.1932e-02,  4.2236e-02,\n",
      "        -8.7150e-02,  1.6115e-01,  1.1531e-02, -4.6473e-02, -1.7884e-01,\n",
      "        -6.5916e-03, -6.0383e-03, -1.2021e-01, -9.4471e-02, -9.8974e-02,\n",
      "        -6.6462e-02, -7.0364e-02, -7.1230e-03, -1.2032e-02, -5.8607e-02,\n",
      "         3.4588e-02,  6.6846e-02,  1.4583e-02,  1.3971e-01,  9.9150e-02,\n",
      "         1.4971e-01, -4.3685e-02, -1.4031e-01,  9.8505e-02, -5.5008e-02,\n",
      "        -5.4416e-03,  3.7470e-02, -4.2826e-02,  4.1744e-02,  3.6995e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 0.0560,  0.0574,  0.0134,  0.0056, -0.0025, -0.0674, -0.0344,  0.0539,\n",
      "        -0.0484,  0.0673,  0.0151,  0.0430, -0.0848,  0.0858, -0.0692, -0.0022,\n",
      "        -0.0295, -0.0604, -0.0359, -0.0549,  0.0184,  0.0534, -0.1482,  0.0245,\n",
      "         0.0446,  0.1040, -0.1278,  0.0346, -0.0569,  0.1528,  0.0336, -0.0661,\n",
      "         0.0155, -0.1372, -0.0500, -0.1943,  0.1066,  0.0199,  0.1952, -0.0088,\n",
      "         0.2373, -0.0286, -0.0451, -0.0285, -0.0066,  0.1500,  0.0361,  0.1687,\n",
      "        -0.1069,  0.1415, -0.1072,  0.0406, -0.0160,  0.1050, -0.0298, -0.1113,\n",
      "         0.0241,  0.0198,  0.1911,  0.1309, -0.0041, -0.1457,  0.0946, -0.1428,\n",
      "         0.0209,  0.0068,  0.1088,  0.1109,  0.0176,  0.0651, -0.0566,  0.1081,\n",
      "        -0.1101,  0.1221, -0.0274, -0.0040,  0.0695, -0.2023, -0.1571,  0.1130,\n",
      "         0.0791, -0.1078, -0.1872, -0.1907, -0.0267,  0.1130, -0.1429,  0.1809,\n",
      "        -0.1969, -0.0517,  0.0127,  0.1460,  0.0493, -0.0330, -0.1368,  0.1379,\n",
      "        -0.0207,  0.0084,  0.1296, -0.0713,  0.1300, -0.1092, -0.0778, -0.1626,\n",
      "         0.1790, -0.0144, -0.0320, -0.0285, -0.0309,  0.1414, -0.0246, -0.0370,\n",
      "         0.1190, -0.0176, -0.1700, -0.0426, -0.0198,  0.0250, -0.1667, -0.0254,\n",
      "         0.0307,  0.1120,  0.0186, -0.0765,  0.2310,  0.0209,  0.0576,  0.0070,\n",
      "        -0.0891, -0.0622, -0.0582,  0.0141,  0.0016, -0.0294, -0.0799,  0.0814,\n",
      "        -0.1197,  0.0645,  0.0184, -0.1148,  0.0735,  0.0089,  0.0018, -0.0636,\n",
      "         0.0665,  0.0583, -0.0278,  0.1801, -0.1264,  0.0489,  0.0447,  0.0383,\n",
      "        -0.0740, -0.0783,  0.1122,  0.1416, -0.0550,  0.1345,  0.1345, -0.1091,\n",
      "         0.1188, -0.0925, -0.0718, -0.0836, -0.0620, -0.0176, -0.0976,  0.0325,\n",
      "        -0.1415, -0.0214,  0.0588,  0.0182, -0.0169,  0.0345, -0.0241,  0.1455,\n",
      "        -0.0091,  0.0658, -0.0258, -0.1580,  0.0037,  0.0514, -0.0333,  0.0078,\n",
      "         0.0277,  0.0696, -0.0764,  0.0649, -0.1229, -0.1062, -0.0124, -0.0399,\n",
      "        -0.0099, -0.0107, -0.0003,  0.0045, -0.0519,  0.0004, -0.0014, -0.0640],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([ 0.0145,  0.0074, -0.0198, -0.0612, -0.0005,  0.0046,  0.0984, -0.0621,\n",
      "        -0.0542,  0.1488,  0.0633,  0.0681, -0.0069,  0.0467,  0.0363, -0.0154,\n",
      "         0.0968,  0.1558, -0.0948, -0.0535,  0.1210, -0.1031,  0.0074, -0.1203,\n",
      "        -0.0094, -0.2064, -0.0516, -0.0075,  0.0045,  0.0220,  0.0409, -0.0427,\n",
      "        -0.1947, -0.0708, -0.0872,  0.0116,  0.0539, -0.0183, -0.1127,  0.0585,\n",
      "         0.1605,  0.0496, -0.3144, -0.0142,  0.1752, -0.0212,  0.0347, -0.0637,\n",
      "        -0.1602, -0.0974,  0.0338, -0.0576,  0.0417, -0.0067,  0.0337, -0.0706,\n",
      "         0.0023, -0.0820, -0.0098,  0.0357,  0.1271,  0.1134,  0.2152,  0.0577,\n",
      "        -0.1000,  0.0811, -0.1370,  0.1717, -0.0149,  0.1344,  0.1135,  0.0091,\n",
      "        -0.1095, -0.1200, -0.1363, -0.0521, -0.1111,  0.1492,  0.0441,  0.1992,\n",
      "        -0.1017,  0.0921,  0.0046,  0.0756,  0.0568, -0.0817,  0.0641, -0.0088,\n",
      "        -0.0493,  0.0456,  0.0476,  0.0903, -0.1599,  0.0406,  0.0035,  0.1475,\n",
      "        -0.1444,  0.0699,  0.0059, -0.0153,  0.2054,  0.0670,  0.0458, -0.1352,\n",
      "        -0.0396, -0.3184, -0.0113, -0.1214,  0.2646,  0.1176, -0.0481,  0.0166,\n",
      "         0.0606,  0.0129,  0.1153, -0.0383, -0.1969, -0.0413,  0.0654, -0.1172,\n",
      "         0.1380,  0.0983,  0.1115, -0.1309,  0.0179,  0.1203,  0.0539,  0.0705,\n",
      "         0.0263,  0.0118,  0.0347, -0.0967,  0.0188, -0.0742,  0.0877,  0.0241,\n",
      "        -0.1166, -0.0306,  0.0425,  0.0672,  0.0568,  0.0990, -0.0521,  0.0201,\n",
      "        -0.2229,  0.1002, -0.0975,  0.0909,  0.0049,  0.0787, -0.1325,  0.0793,\n",
      "        -0.0726, -0.1167,  0.0667,  0.0528,  0.1546,  0.0101, -0.0604, -0.0964,\n",
      "         0.0630,  0.0842,  0.0971, -0.1387, -0.0782,  0.0175, -0.0249, -0.0330,\n",
      "        -0.0524,  0.1046,  0.1355,  0.0691,  0.0827, -0.0162,  0.0718,  0.0396,\n",
      "        -0.0156, -0.1453,  0.0112,  0.1822, -0.1343,  0.0043,  0.0353,  0.0316,\n",
      "         0.0040, -0.0388,  0.0159, -0.0361,  0.0400, -0.0677,  0.0515,  0.1003,\n",
      "         0.1732,  0.0475, -0.1846,  0.0959,  0.1389,  0.0971,  0.0026,  0.0294],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-0.0737,  0.0195, -0.1074,  0.1453, -0.1742, -0.0054,  0.1532, -0.0875,\n",
      "         0.1266,  0.0254, -0.1355,  0.0822,  0.2277,  0.0534, -0.0817, -0.2008,\n",
      "        -0.1125,  0.1986, -0.1562, -0.0490, -0.0506, -0.1185, -0.0403,  0.1404,\n",
      "        -0.0739, -0.0678, -0.0121, -0.0595,  0.0207,  0.1415,  0.0691, -0.2084,\n",
      "        -0.1194, -0.0575,  0.0604, -0.0865,  0.0182, -0.1353,  0.0486,  0.0243,\n",
      "        -0.2116, -0.1299,  0.0563,  0.1976,  0.1044,  0.1951,  0.1788,  0.2079,\n",
      "         0.0052, -0.0343,  0.1315, -0.0503,  0.0060,  0.0592, -0.0195, -0.1291,\n",
      "        -0.0205, -0.1193,  0.0956,  0.0007,  0.0647,  0.1560,  0.1391, -0.1938,\n",
      "        -0.0165, -0.1269,  0.0332,  0.0432,  0.0220, -0.0261,  0.0149,  0.0014,\n",
      "        -0.0342, -0.0212,  0.1073, -0.0803,  0.0396, -0.0657,  0.0777,  0.0022,\n",
      "        -0.0595, -0.0866, -0.0193,  0.0876,  0.0821,  0.0285,  0.0692, -0.1172,\n",
      "        -0.0032, -0.0131,  0.0939,  0.1105,  0.0586,  0.1029, -0.0961, -0.0190,\n",
      "         0.2803,  0.0658, -0.1245, -0.0322,  0.0528, -0.0550,  0.0869, -0.1152,\n",
      "        -0.1674,  0.0665, -0.0316, -0.0390, -0.1373, -0.0209, -0.1249, -0.0219,\n",
      "         0.0631, -0.0568, -0.0612, -0.0479,  0.0608, -0.0314,  0.0852, -0.1338,\n",
      "        -0.0371,  0.0019, -0.2000, -0.0744, -0.1372,  0.0120, -0.0364, -0.0296,\n",
      "         0.1075,  0.0581, -0.0454, -0.0572, -0.0448,  0.0865, -0.0951, -0.1688,\n",
      "        -0.0143, -0.1734, -0.0341, -0.1612, -0.1434, -0.0058,  0.1213, -0.0917,\n",
      "        -0.1079,  0.1173, -0.0273, -0.1925, -0.0010,  0.1456,  0.0651, -0.0560,\n",
      "         0.1051, -0.0780,  0.1507,  0.0797,  0.0989, -0.0629, -0.0434, -0.2337,\n",
      "        -0.0487,  0.0160, -0.0784,  0.0337,  0.0903, -0.0069,  0.0786,  0.0540,\n",
      "        -0.0475, -0.0597, -0.2296, -0.0489, -0.1063, -0.1416, -0.0366,  0.0479,\n",
      "        -0.0895,  0.0306, -0.0697,  0.0180, -0.0983, -0.1099, -0.0029,  0.1080,\n",
      "         0.0948, -0.1625,  0.1966,  0.1028, -0.0833,  0.0557,  0.0014, -0.0042,\n",
      "         0.0883,  0.0759, -0.1784,  0.0206,  0.0879,  0.0245, -0.0462,  0.1093],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([-1.0445e-01,  8.7777e-02,  7.8746e-02,  2.5214e-02,  1.0842e-01,\n",
      "        -5.6750e-02,  1.8885e-01,  7.4259e-02,  4.1122e-02,  9.3642e-02,\n",
      "        -6.9056e-02,  1.5517e-02, -5.5308e-02, -1.2436e-03, -1.4175e-01,\n",
      "         2.3006e-02,  5.0752e-03, -2.1977e-01, -4.8080e-02,  1.9734e-01,\n",
      "         6.0895e-02, -3.7951e-02, -1.2159e-01, -7.6056e-02, -2.5582e-02,\n",
      "         5.4963e-02, -1.0308e-01, -7.3601e-02, -3.6183e-02,  3.0441e-02,\n",
      "         5.2262e-02,  1.2584e-01,  3.1897e-02, -5.7779e-02,  5.0809e-02,\n",
      "         1.1997e-01, -1.1472e-01, -1.5681e-01, -6.6027e-02,  1.8070e-02,\n",
      "        -2.0610e-02,  8.8195e-03,  1.3414e-01, -1.4244e-01, -3.2420e-02,\n",
      "         1.1263e-01,  1.4818e-02, -6.6581e-02, -5.5472e-02,  9.8160e-02,\n",
      "        -6.4826e-03, -6.2018e-02,  7.8159e-03, -8.0994e-03, -1.1510e-01,\n",
      "         1.0074e-02, -8.5542e-02, -5.6166e-02, -1.0707e-01,  8.4524e-02,\n",
      "        -9.9969e-02,  2.2170e-01, -2.0682e-01,  1.6662e-02,  5.6290e-02,\n",
      "        -1.5312e-02,  3.2649e-02,  1.6648e-01, -1.3627e-01,  8.2519e-02,\n",
      "        -4.4697e-02,  6.2363e-02,  6.0104e-03, -5.4213e-02, -2.8672e-02,\n",
      "        -2.4666e-01,  9.0252e-02, -1.8227e-01,  9.8204e-02, -8.3786e-02,\n",
      "        -6.4268e-02, -8.4002e-02,  2.4463e-02,  9.3816e-02, -9.0537e-04,\n",
      "         1.1937e-01,  6.5875e-02,  1.1564e-01, -3.5122e-02,  1.5895e-02,\n",
      "         5.1806e-03,  5.0268e-02,  1.4623e-02,  9.6798e-02, -3.9235e-02,\n",
      "        -2.6291e-02, -1.3812e-01,  1.4692e-01, -2.2966e-02, -1.1336e-01,\n",
      "         1.4991e-01, -4.9115e-02,  1.1990e-01, -1.6942e-01, -1.3987e-02,\n",
      "         2.6666e-01, -1.0324e-01,  1.5141e-01,  8.3241e-02,  5.2030e-02,\n",
      "         3.9972e-02,  1.1106e-01, -6.6916e-02, -1.7548e-01,  1.2763e-04,\n",
      "         1.7577e-02, -1.5377e-01, -8.5578e-02,  1.2244e-02, -2.6354e-03,\n",
      "        -6.5139e-02, -1.7029e-01, -4.8774e-03, -1.4186e-02,  1.2779e-02,\n",
      "         1.3745e-01,  1.0348e-01,  9.9629e-03,  7.5648e-03,  4.1635e-02,\n",
      "        -4.1585e-02, -1.7203e-02,  2.0367e-02, -3.6483e-02, -3.1466e-02,\n",
      "        -2.6119e-03, -1.1894e-01,  7.1916e-03, -1.1645e-01,  8.9883e-02,\n",
      "         2.9037e-02, -1.2632e-01,  4.9479e-02,  1.1624e-01,  5.3079e-03,\n",
      "        -2.9316e-02,  1.5208e-02,  3.6584e-02, -4.8979e-03,  5.5410e-03,\n",
      "        -1.0690e-02, -1.1188e-01, -1.1191e-01,  6.8592e-02, -1.7204e-02,\n",
      "        -4.5646e-02,  6.6074e-02,  1.5238e-01, -5.3202e-02,  2.1720e-02,\n",
      "        -1.6402e-02,  5.4181e-02,  6.6202e-02,  1.6466e-04, -1.3858e-02,\n",
      "         7.2051e-02,  5.7592e-02, -4.1804e-03,  6.8808e-02, -4.6979e-02,\n",
      "        -2.4113e-01, -1.2452e-01,  9.6403e-02, -1.3550e-01, -2.3256e-02,\n",
      "        -6.9924e-02, -1.1376e-01, -1.6449e-01, -4.1568e-02,  6.8743e-02,\n",
      "        -1.3488e-01, -1.3133e-01,  6.9876e-02, -7.6848e-03,  7.0388e-02,\n",
      "        -1.7653e-01, -7.8758e-02, -2.3242e-02,  6.7952e-02, -1.3323e-01,\n",
      "         1.3252e-01, -1.1056e-01, -1.6053e-01, -8.8164e-02, -5.9020e-03,\n",
      "         6.1708e-02,  7.6038e-02,  5.4132e-02, -3.3675e-02,  2.5829e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([ 2.9967e-02,  5.8596e-02, -5.8268e-02,  2.1913e-02, -4.7009e-02,\n",
      "        -1.3524e-01,  2.6180e-02,  4.9762e-02, -4.2991e-02, -1.2345e-01,\n",
      "         7.3445e-02,  2.0710e-01, -1.1636e-01,  2.2250e-01,  9.6059e-02,\n",
      "         1.2624e-01,  1.0856e-01,  1.3696e-01,  3.1424e-02, -2.5183e-02,\n",
      "         1.7882e-01,  1.0355e-01, -8.8651e-02,  1.5198e-01,  1.1089e-02,\n",
      "        -4.4922e-02,  1.1444e-01,  3.0908e-02,  1.7858e-02, -1.7959e-02,\n",
      "        -6.4179e-02,  1.6904e-01,  6.6278e-02,  5.1121e-02, -1.4627e-01,\n",
      "         3.4104e-02,  7.7003e-02,  8.1557e-02, -4.2663e-02,  6.3743e-02,\n",
      "         9.4910e-03,  6.1752e-02,  1.4285e-03,  4.5665e-02, -3.0927e-02,\n",
      "        -5.4170e-02, -6.9523e-02,  5.9895e-02, -8.7788e-03, -8.1675e-02,\n",
      "        -3.9559e-02,  7.2758e-02, -1.4839e-02,  1.6260e-02, -5.2662e-02,\n",
      "         2.2840e-02,  1.3101e-01, -1.5429e-01,  1.5267e-02, -4.5277e-02,\n",
      "        -1.9579e-02, -1.1940e-03,  6.4457e-02, -5.7211e-02, -1.8633e-01,\n",
      "        -1.4960e-03,  1.3306e-01, -6.2926e-02, -1.1537e-01, -7.1420e-03,\n",
      "         5.9511e-03, -7.9219e-02,  5.3386e-02,  2.3926e-01, -3.4874e-03,\n",
      "         5.0888e-02, -3.2248e-02, -3.7136e-02, -7.6102e-02,  1.3093e-01,\n",
      "         5.0809e-03,  3.5346e-02,  1.1525e-01,  6.6567e-02,  8.6581e-02,\n",
      "         6.5027e-03,  1.8728e-02,  1.3966e-01, -8.0355e-02, -1.1380e-02,\n",
      "         5.1239e-02,  2.0873e-01, -7.9310e-02, -1.1703e-02, -5.0304e-02,\n",
      "         2.4405e-01, -8.9317e-02, -3.9650e-02, -1.9855e-01,  7.9921e-02,\n",
      "        -2.5765e-02, -1.5442e-01,  5.4225e-02, -8.6169e-02,  3.6694e-02,\n",
      "         5.7162e-02,  1.0694e-01, -1.4431e-01,  4.7470e-02,  1.2999e-01,\n",
      "         5.4239e-02, -1.2061e-01,  9.8631e-02,  8.9624e-03, -8.5620e-02,\n",
      "        -1.3466e-01, -2.0273e-02,  1.9870e-01,  5.5875e-02, -1.8227e-01,\n",
      "         1.0198e-01, -9.9555e-02,  9.4316e-02,  1.3027e-01, -6.2977e-03,\n",
      "        -9.3711e-02,  2.3144e-02,  9.2687e-02,  2.3574e-01, -4.1606e-02,\n",
      "        -1.7446e-01, -6.6345e-02, -5.5056e-02, -2.2948e-01,  7.3865e-02,\n",
      "        -2.0385e-01, -1.3129e-01,  2.5843e-01, -1.5347e-02, -1.6266e-01,\n",
      "        -2.6304e-01,  1.0993e-02, -1.1387e-01,  1.3319e-01,  2.3479e-04,\n",
      "         1.0880e-01, -9.2241e-02, -7.4537e-02, -6.1224e-02,  1.3455e-02,\n",
      "         1.0095e-01,  2.8227e-02, -6.4782e-02, -4.9374e-02, -2.8085e-01,\n",
      "         1.0699e-03, -2.6293e-02,  1.7857e-01,  9.1173e-02,  1.6361e-01,\n",
      "         1.2262e-01,  1.3326e-02,  9.3021e-02, -2.8646e-02, -5.6758e-02,\n",
      "        -1.1400e-01, -1.8504e-02,  9.3801e-02, -4.3636e-02, -7.4236e-02,\n",
      "        -5.6867e-02,  5.2052e-02,  3.8505e-02,  1.0797e-02, -5.2017e-02,\n",
      "        -6.9781e-02, -2.5134e-02, -1.2842e-01, -7.6041e-02,  6.3961e-02,\n",
      "        -1.2596e-01, -9.7956e-02,  8.4959e-04,  1.0699e-01,  1.6643e-02,\n",
      "        -2.1387e-02, -6.9818e-02,  5.0823e-02,  3.4968e-02, -4.2158e-02,\n",
      "        -8.4857e-02,  4.1026e-02, -1.4640e-02, -8.5827e-02,  5.1464e-02,\n",
      "        -4.6715e-02,  6.0920e-02, -3.6651e-02,  5.8376e-02, -1.5851e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 4.7478e-02, -1.3369e-01, -3.9746e-02,  9.3116e-02, -1.5112e-01,\n",
      "        -5.1118e-02,  1.8977e-01,  1.8979e-01,  1.1574e-01,  1.6570e-02,\n",
      "         1.1563e-01,  6.5276e-02, -6.6223e-02,  1.8662e-01, -1.5407e-03,\n",
      "         7.4714e-02,  4.5494e-02,  8.5127e-02, -1.1417e-01,  1.3486e-01,\n",
      "        -1.2160e-01,  1.2577e-01, -7.8599e-02, -1.8269e-01, -1.2462e-02,\n",
      "        -1.0552e-01,  9.5258e-02,  4.1348e-02,  1.1845e-01,  1.7732e-02,\n",
      "        -5.5330e-02, -6.0825e-02, -6.4573e-02,  5.3791e-02, -1.1493e-01,\n",
      "         8.2235e-02,  6.5384e-02, -1.0591e-01,  5.6807e-02, -3.9331e-02,\n",
      "         8.9214e-02,  9.8720e-02, -2.2429e-01,  1.4135e-01, -1.3480e-01,\n",
      "         1.1485e-02, -1.5580e-01, -8.6881e-02,  1.1905e-01,  5.4146e-03,\n",
      "        -2.2154e-02,  4.4802e-03,  1.2879e-02, -2.2750e-02,  5.8787e-02,\n",
      "         1.2693e-01,  1.8328e-02, -2.6302e-02,  2.5105e-02,  1.7687e-01,\n",
      "        -3.7688e-02, -5.8496e-02, -2.5485e-02, -8.7752e-02, -9.6767e-03,\n",
      "         3.0127e-02,  3.9834e-02, -2.0647e-01,  4.7443e-02,  6.6133e-02,\n",
      "        -6.2642e-02,  1.5881e-02,  3.5311e-02, -5.5046e-02, -6.5308e-02,\n",
      "         4.1903e-02, -1.0209e-01,  2.4309e-02,  1.4352e-01,  5.4920e-02,\n",
      "         1.7393e-04,  8.2634e-02,  8.6577e-02,  4.8788e-02,  4.5671e-02,\n",
      "        -7.8845e-02, -7.7638e-02, -1.5875e-02,  6.6006e-02,  1.0380e-02,\n",
      "        -1.0747e-01,  1.0245e-01, -2.6916e-03,  2.3802e-01, -5.1393e-03,\n",
      "        -4.5470e-03,  1.0547e-02, -1.2531e-01, -8.7939e-02, -8.2840e-02,\n",
      "        -4.9626e-02,  6.6391e-02,  9.0684e-03,  2.7756e-02, -1.1350e-01,\n",
      "         7.0131e-03,  2.1717e-01, -1.2723e-02,  3.5927e-02, -2.1014e-02,\n",
      "        -7.9065e-03, -1.1828e-01, -7.9369e-02, -1.1999e-01, -2.4629e-02,\n",
      "         4.0409e-02, -3.3291e-02,  6.0146e-02,  3.5271e-02, -8.3599e-02,\n",
      "         1.0029e-02, -8.4403e-02,  8.1663e-02, -6.6243e-02,  1.1978e-01,\n",
      "         2.8016e-02,  1.4812e-01,  6.3609e-02, -1.9514e-01, -1.7797e-01,\n",
      "         7.9367e-02,  8.0284e-03, -2.1230e-02, -1.6563e-02,  4.0115e-02,\n",
      "        -1.3374e-01,  2.6254e-02, -3.3861e-03,  1.0904e-01,  3.4983e-03,\n",
      "        -3.0829e-02, -1.2033e-01,  1.4646e-02, -1.1655e-01, -2.3924e-01,\n",
      "        -1.7973e-01,  3.4092e-02,  1.4656e-01,  9.5982e-02,  1.1864e-01,\n",
      "        -5.0741e-02,  8.7684e-02, -1.1907e-01, -1.0325e-01, -9.9234e-02,\n",
      "        -1.1183e-01,  6.6669e-02,  1.0135e-01,  6.0754e-02, -6.8265e-02,\n",
      "         2.0271e-02, -1.8036e-01, -1.0366e-01,  1.5245e-01, -1.1157e-01,\n",
      "        -1.0324e-01,  1.1578e-01, -6.4962e-03,  9.1915e-02,  7.3877e-03,\n",
      "         2.5612e-02, -1.6521e-01,  9.6541e-02, -4.8991e-02, -2.2606e-02,\n",
      "        -1.9695e-01,  5.5161e-02,  3.0593e-02, -3.3240e-02,  8.3956e-02,\n",
      "        -5.3986e-03, -8.5253e-03, -2.4898e-02,  1.0870e-01,  4.4504e-03,\n",
      "         1.6676e-02,  2.6560e-01,  5.7214e-02, -2.8016e-02,  1.4540e-01,\n",
      "        -4.6197e-03, -2.7889e-02,  5.6900e-02, -4.0098e-04,  2.3516e-01,\n",
      "         4.3305e-03,  1.4920e-02,  1.0482e-01, -3.7675e-03,  1.1158e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 0.0099, -0.0189, -0.0937,  0.0049,  0.0619,  0.0796, -0.0202, -0.0155,\n",
      "         0.0032, -0.0507,  0.0720, -0.0216, -0.0207, -0.0733, -0.0838,  0.0269,\n",
      "        -0.1489,  0.1284, -0.1007,  0.1267, -0.0678,  0.0645,  0.0636,  0.1193,\n",
      "         0.0793,  0.0110, -0.0263,  0.0486,  0.0076, -0.1044,  0.0082,  0.0409,\n",
      "         0.1031,  0.0875,  0.0848,  0.1672, -0.0372, -0.0131,  0.1296, -0.0948,\n",
      "        -0.0728, -0.1176,  0.1036,  0.0392,  0.1032,  0.0903, -0.0361, -0.1242,\n",
      "        -0.0938,  0.0077, -0.1278,  0.0182, -0.0343,  0.0025, -0.0158,  0.1628,\n",
      "        -0.0227, -0.0815, -0.1441,  0.0020,  0.0326,  0.0146,  0.0545, -0.0185,\n",
      "        -0.1524, -0.1036, -0.0843, -0.1104,  0.0102,  0.0521,  0.2211,  0.1013,\n",
      "         0.0291,  0.0853, -0.0143,  0.0753, -0.1891,  0.0860, -0.0481,  0.1902,\n",
      "        -0.0653, -0.0273,  0.1043,  0.0290, -0.0206,  0.0885,  0.0508, -0.2333,\n",
      "        -0.1533, -0.0710,  0.1227, -0.0352, -0.0183, -0.0669,  0.0100,  0.1492,\n",
      "        -0.0152, -0.0280, -0.0398, -0.2015, -0.0632,  0.0970,  0.1041, -0.1351,\n",
      "        -0.0760,  0.0946,  0.0029,  0.0683, -0.0818,  0.0107,  0.0189, -0.1350,\n",
      "        -0.0069,  0.1222, -0.1285,  0.0111, -0.0997,  0.0820,  0.1245, -0.1898,\n",
      "        -0.1300, -0.1084,  0.0507,  0.0434,  0.0298,  0.1188, -0.0501, -0.0993,\n",
      "         0.0629,  0.0153,  0.1067, -0.0904,  0.1197, -0.1371,  0.1282, -0.1984,\n",
      "         0.0169,  0.0710,  0.0578,  0.0005,  0.0374, -0.1646,  0.0173,  0.0815,\n",
      "         0.2335,  0.0358,  0.0414, -0.0306, -0.0764, -0.1145, -0.0648,  0.0063,\n",
      "        -0.0456,  0.0244, -0.2386, -0.1147, -0.1758,  0.0350, -0.0349,  0.0129,\n",
      "        -0.0818,  0.0944, -0.0011,  0.1027, -0.0227, -0.1294,  0.1868,  0.0801,\n",
      "        -0.0297, -0.0324, -0.0032, -0.1213,  0.1095,  0.1771,  0.0563, -0.1439,\n",
      "         0.0439,  0.2813,  0.1031,  0.0598,  0.0158,  0.1463,  0.0683, -0.1256,\n",
      "         0.1437,  0.1430,  0.1271,  0.1215,  0.0551,  0.0676,  0.0884,  0.0255,\n",
      "         0.0695, -0.1663,  0.0410, -0.1015,  0.1142,  0.1622,  0.0724, -0.0513],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([ 1.9102e-01, -3.4185e-02, -1.2827e-01, -2.3386e-02, -3.2705e-02,\n",
      "        -5.9174e-04, -1.4777e-01, -1.2156e-01, -8.3105e-02, -5.3097e-02,\n",
      "        -7.5513e-02,  1.5209e-02,  1.2263e-01, -2.4354e-02, -1.2072e-02,\n",
      "        -1.4629e-01,  1.9270e-01, -2.6448e-04,  6.6778e-03, -1.2886e-01,\n",
      "        -1.5215e-02, -9.7241e-02,  3.8612e-02,  1.5631e-01,  1.4104e-01,\n",
      "        -3.3596e-02, -1.5924e-01, -7.9156e-02, -4.3052e-03,  1.2332e-01,\n",
      "         4.5306e-03,  3.3276e-02,  3.4609e-02,  6.8103e-02,  2.4638e-02,\n",
      "         7.1365e-02, -1.1362e-01, -5.2254e-03, -8.6224e-02, -1.2326e-01,\n",
      "        -3.8243e-02,  4.1193e-03,  8.2459e-02, -1.0049e-01,  2.1147e-02,\n",
      "        -1.1932e-01,  1.2192e-01,  7.7694e-02, -6.3686e-02,  1.3154e-02,\n",
      "         3.2057e-02, -2.9309e-01, -2.1439e-02,  1.0416e-01, -1.6166e-02,\n",
      "        -5.8375e-02,  9.0607e-02, -1.2532e-02,  1.6766e-01, -1.1897e-01,\n",
      "         4.0067e-02,  1.0415e-02, -1.5241e-01, -3.6162e-02, -2.0550e-01,\n",
      "        -7.8639e-02,  6.9140e-02, -1.2757e-01,  1.0072e-01,  2.2250e-02,\n",
      "        -4.1543e-02, -1.3908e-01, -1.5596e-01, -4.5363e-02,  2.6580e-02,\n",
      "        -3.8248e-02,  6.1392e-02, -1.1720e-01,  1.5439e-02,  7.3927e-05,\n",
      "        -4.6799e-02,  1.6735e-01, -3.5179e-02, -1.8040e-01,  6.8837e-02,\n",
      "         1.1274e-01, -4.4994e-03,  1.0250e-02,  8.6756e-02, -1.0892e-01,\n",
      "        -1.2271e-01,  1.1058e-03,  3.3981e-02,  6.2148e-02, -9.9113e-02,\n",
      "         7.8371e-02,  3.9875e-02,  5.6477e-02, -1.0715e-01,  1.7729e-01,\n",
      "        -9.8474e-02,  1.2549e-01,  3.0015e-02, -4.9095e-02,  6.5863e-02,\n",
      "        -4.2928e-03,  2.9820e-02,  1.3975e-02, -1.4265e-01, -5.1461e-03,\n",
      "         1.9649e-01, -1.4539e-01, -3.1143e-02,  1.1937e-01,  1.1697e-01,\n",
      "         1.5917e-01,  1.0450e-01,  5.0451e-02,  9.9115e-02,  5.9171e-03,\n",
      "        -1.7212e-01,  5.2592e-02, -1.0048e-01,  7.8043e-02,  7.6848e-02,\n",
      "         7.1245e-03,  6.8260e-02, -4.4136e-02,  3.9696e-02, -3.8033e-02,\n",
      "         3.3725e-02, -1.1872e-01, -5.1797e-02,  2.1351e-02, -1.5295e-02,\n",
      "         6.4181e-02,  1.3098e-01, -1.1038e-01, -7.3618e-02,  1.6755e-01,\n",
      "        -1.9019e-01, -2.7477e-02, -4.2023e-02,  4.0953e-02, -2.1790e-02,\n",
      "        -1.2800e-02, -2.2005e-01,  7.0763e-02, -9.0055e-02,  1.3421e-02,\n",
      "        -6.9474e-02,  4.6129e-02,  7.0755e-02, -1.4517e-01, -9.9930e-02,\n",
      "        -1.5761e-01, -8.9627e-02,  1.0320e-01, -1.4106e-01, -4.5064e-02,\n",
      "        -9.2280e-02, -2.0072e-02, -5.2449e-03,  1.0647e-02,  8.5879e-02,\n",
      "        -2.3304e-01,  4.7248e-02,  2.2648e-02, -8.7284e-02,  6.8034e-02,\n",
      "         4.5256e-03, -1.6641e-02, -5.0116e-02,  9.8047e-02,  2.6726e-03,\n",
      "        -2.7143e-02, -3.9092e-03, -5.1909e-02,  1.2897e-01,  6.0420e-02,\n",
      "         3.4970e-02, -5.1878e-02, -1.5624e-01, -7.0212e-03, -2.3241e-02,\n",
      "        -6.9326e-02, -1.1152e-01,  4.5683e-02,  1.6814e-01,  6.8845e-02,\n",
      "         9.3135e-02, -3.5792e-02, -3.7632e-02,  8.0141e-02,  9.0380e-02,\n",
      "        -3.4630e-02, -2.0447e-03, -6.5112e-03, -1.1540e-01,  5.2056e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([-0.1096, -0.0692,  0.1200, -0.0607,  0.0430, -0.1612,  0.1047, -0.0225,\n",
      "         0.0254, -0.0554, -0.0290, -0.1739, -0.0792,  0.0697, -0.0184, -0.0429,\n",
      "         0.1104,  0.1192, -0.0700,  0.1868, -0.0045,  0.0892, -0.1442, -0.0686,\n",
      "        -0.1164, -0.0170, -0.0324,  0.0037,  0.1439,  0.0528, -0.1669,  0.0210,\n",
      "        -0.0036, -0.0761, -0.1556, -0.0107, -0.0747,  0.0671,  0.0927,  0.2911,\n",
      "         0.0022,  0.0172, -0.1366,  0.0569,  0.0972, -0.0196, -0.2169, -0.1145,\n",
      "        -0.1197, -0.0463,  0.1176,  0.1080, -0.0033,  0.0389,  0.1334,  0.1306,\n",
      "        -0.0131, -0.0914, -0.1080,  0.1080,  0.0809,  0.0432, -0.1037,  0.0878,\n",
      "        -0.0038,  0.0531,  0.1804,  0.0007, -0.0582,  0.0301, -0.1671, -0.0114,\n",
      "        -0.1426,  0.1597,  0.1568, -0.0740, -0.0395, -0.0410,  0.0543,  0.0103,\n",
      "         0.1515,  0.1220,  0.0928,  0.1469,  0.0612, -0.0171,  0.0627,  0.1683,\n",
      "         0.0456, -0.0220,  0.0505,  0.1440,  0.1927, -0.0628, -0.0328, -0.2866,\n",
      "        -0.2143,  0.0746, -0.0087, -0.0197,  0.1065, -0.0745,  0.0614, -0.0211,\n",
      "         0.0556,  0.0358, -0.1773,  0.2151,  0.0099,  0.0830,  0.2125, -0.0450,\n",
      "         0.1137, -0.0583, -0.0972,  0.0533,  0.0761,  0.1382,  0.1589, -0.1613,\n",
      "        -0.0280, -0.0734, -0.1950,  0.0287, -0.0933, -0.0793,  0.0858,  0.0657,\n",
      "         0.1138,  0.1395,  0.0057,  0.1012,  0.0022, -0.0238, -0.1167, -0.0801,\n",
      "         0.2565, -0.1517, -0.1182, -0.0794,  0.1001, -0.0458,  0.0731, -0.0678,\n",
      "        -0.0754,  0.0974, -0.0160, -0.0570, -0.0230, -0.2103, -0.0030,  0.1598,\n",
      "         0.0183, -0.1365, -0.1228, -0.0064, -0.0465, -0.0065,  0.0046,  0.0914,\n",
      "         0.1395,  0.0221, -0.0619, -0.0079, -0.0131,  0.0794,  0.0287,  0.1130,\n",
      "         0.2106, -0.0707,  0.1803, -0.0519,  0.0231, -0.1533,  0.1013,  0.0735,\n",
      "         0.0958, -0.1743, -0.0641,  0.0356, -0.0772, -0.0029, -0.0478, -0.1211,\n",
      "         0.1801,  0.0121,  0.0680, -0.1576, -0.0698,  0.0332,  0.0307, -0.0363,\n",
      "         0.0649,  0.1751,  0.0240, -0.0773, -0.1064, -0.0749,  0.2538, -0.1274],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 0.1244, -0.0470,  0.0623, -0.1467, -0.0554,  0.1203, -0.0690,  0.0097,\n",
      "        -0.0668, -0.1001,  0.1614, -0.0125, -0.1176,  0.0678,  0.0363,  0.1566,\n",
      "        -0.0606,  0.0276, -0.0476, -0.1038,  0.1243, -0.1936,  0.0087,  0.0036,\n",
      "         0.1548, -0.0096,  0.0441, -0.0947, -0.0468, -0.0634, -0.0511,  0.0118,\n",
      "         0.0747,  0.0824,  0.0851,  0.0545, -0.0425,  0.0284,  0.1446, -0.0355,\n",
      "         0.1713, -0.0999,  0.0303, -0.0322, -0.1562,  0.0005, -0.0689,  0.1669,\n",
      "         0.0921,  0.1094, -0.1094, -0.0361, -0.2350, -0.1628,  0.0869,  0.0558,\n",
      "         0.1143,  0.0007, -0.1847, -0.1007,  0.1278,  0.0803,  0.0427,  0.0225,\n",
      "        -0.0625,  0.0554,  0.1046,  0.0175, -0.1339, -0.0081,  0.0418, -0.0036,\n",
      "        -0.1019,  0.0489, -0.0757,  0.0840, -0.2114,  0.2569, -0.0647,  0.0451,\n",
      "        -0.0603, -0.2051, -0.0817, -0.1380, -0.1249,  0.0922,  0.0346,  0.1920,\n",
      "        -0.0736, -0.0194,  0.2920,  0.1482,  0.2456,  0.1651, -0.1341, -0.0378,\n",
      "         0.0283, -0.0717,  0.0125,  0.0942, -0.0097, -0.0906, -0.0369, -0.1455,\n",
      "         0.0401,  0.0671, -0.1842,  0.1369,  0.0368, -0.0929,  0.1059, -0.0217,\n",
      "        -0.0630,  0.1694,  0.2479, -0.0393, -0.1405, -0.1500,  0.0352,  0.0130,\n",
      "        -0.1107,  0.0175,  0.0618, -0.0234, -0.1456, -0.1132,  0.0222, -0.0098,\n",
      "         0.0529, -0.0619,  0.0220,  0.0113, -0.1089, -0.1166, -0.0386,  0.0531,\n",
      "         0.1449,  0.0579, -0.0561, -0.1941, -0.0203,  0.0862,  0.0611,  0.0040,\n",
      "         0.0013, -0.0129,  0.0091,  0.0095, -0.0042,  0.0084, -0.0115, -0.0788,\n",
      "         0.1262, -0.0173, -0.0767, -0.1268, -0.0066, -0.1369,  0.0302,  0.0553,\n",
      "        -0.1058, -0.0757,  0.0775,  0.1756,  0.0939, -0.0617,  0.1413,  0.0897,\n",
      "         0.0180, -0.1048,  0.0722,  0.1757,  0.0293, -0.0234, -0.0939, -0.0192,\n",
      "         0.0805, -0.1476, -0.1034,  0.3079,  0.0125, -0.1528, -0.0531,  0.0480,\n",
      "         0.0626, -0.0177,  0.0463,  0.0772, -0.1033,  0.1109, -0.0429,  0.0828,\n",
      "        -0.1390,  0.0834, -0.2407,  0.0477,  0.0351, -0.1167, -0.0814,  0.0009],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([-0.1216,  0.0440, -0.0640,  0.0506, -0.0343, -0.1240,  0.1705,  0.0299,\n",
      "         0.1176, -0.0573, -0.0643, -0.0322,  0.0197,  0.0544,  0.0512,  0.0742,\n",
      "         0.0856, -0.0212, -0.0593, -0.1497, -0.0632,  0.1049,  0.2118, -0.0853,\n",
      "         0.0864,  0.1411, -0.0771, -0.1339, -0.1604, -0.1838, -0.1501,  0.0415,\n",
      "        -0.0528,  0.1030,  0.0519, -0.0341,  0.0016, -0.0691,  0.0668, -0.0599,\n",
      "        -0.0517, -0.0837,  0.0312, -0.1071, -0.0112,  0.0771, -0.0620, -0.0461,\n",
      "         0.0444, -0.0803,  0.0035, -0.0320,  0.0481,  0.0497, -0.0288, -0.0326,\n",
      "        -0.0287,  0.0408, -0.0639,  0.1075, -0.1165,  0.0068, -0.0959,  0.0365,\n",
      "         0.0389, -0.0010,  0.0831, -0.0893,  0.1539, -0.0122, -0.0428, -0.0426,\n",
      "        -0.0241,  0.0407,  0.0196, -0.0201,  0.1403, -0.0126, -0.1150,  0.0995,\n",
      "        -0.3563,  0.0442,  0.1428,  0.0508, -0.0541, -0.0439, -0.0290,  0.1489,\n",
      "         0.0245,  0.0007, -0.0297,  0.0097,  0.0559,  0.0326, -0.0138, -0.1045,\n",
      "        -0.0078, -0.0136,  0.0769,  0.0240, -0.2466, -0.0533,  0.1872, -0.1725,\n",
      "        -0.1203,  0.0362,  0.0035, -0.1039,  0.0229, -0.0889, -0.1281, -0.0163,\n",
      "        -0.1707, -0.0900,  0.0964,  0.0873, -0.0147, -0.1530, -0.0214,  0.0126,\n",
      "        -0.1108,  0.1147,  0.0070, -0.0983,  0.0523, -0.0782,  0.0475, -0.1989,\n",
      "        -0.0411, -0.0391, -0.0395,  0.2473, -0.0038,  0.0416, -0.0308,  0.0561,\n",
      "         0.0968, -0.1148,  0.1063, -0.0143, -0.0012, -0.0739,  0.0761, -0.0595,\n",
      "        -0.0099, -0.0060, -0.0983, -0.1407,  0.1794, -0.0123,  0.1059,  0.0008,\n",
      "        -0.0898,  0.1125,  0.0004, -0.0008, -0.0223,  0.1108,  0.0608,  0.0770,\n",
      "        -0.0665, -0.1468,  0.0429, -0.1891, -0.0660,  0.1310, -0.1103,  0.0148,\n",
      "        -0.1688,  0.0028, -0.0067,  0.0106, -0.1327, -0.0147, -0.0444,  0.0060,\n",
      "        -0.1084, -0.0504,  0.0657,  0.1528,  0.0086, -0.0827,  0.0666,  0.0209,\n",
      "         0.0061, -0.0251, -0.0811, -0.0273, -0.0674, -0.1136, -0.1519,  0.0239,\n",
      "         0.0960, -0.0767, -0.0339,  0.0921, -0.0014,  0.0402, -0.2123,  0.0229],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0858,  0.0650, -0.0631,  0.0509,  0.1852, -0.0146, -0.0766, -0.0310,\n",
      "         0.2187,  0.0129,  0.1089,  0.0255, -0.0494, -0.0400,  0.1094,  0.0337,\n",
      "        -0.0175,  0.0731, -0.1385, -0.0438,  0.0829, -0.1427, -0.2722,  0.0944,\n",
      "         0.0324, -0.1047,  0.1161,  0.1499,  0.1290,  0.1063, -0.0946,  0.0699,\n",
      "         0.1149,  0.0437, -0.3106,  0.1910, -0.1223, -0.0624,  0.0973,  0.0634,\n",
      "         0.0532, -0.2466,  0.1697, -0.0224, -0.0143,  0.0731,  0.0223,  0.1718,\n",
      "        -0.0627,  0.0335,  0.0204,  0.0150,  0.1247, -0.0798, -0.0757, -0.1486,\n",
      "        -0.0735, -0.0808, -0.0465, -0.1374,  0.0766, -0.0086,  0.0371, -0.2394,\n",
      "        -0.0526,  0.0603, -0.1096,  0.0078,  0.0685, -0.0191,  0.0823, -0.0936,\n",
      "         0.0038,  0.0046, -0.0212, -0.0564,  0.0762, -0.1761, -0.1040,  0.0519,\n",
      "        -0.0731,  0.0324,  0.1035,  0.1378,  0.0907, -0.0824,  0.1081,  0.1361,\n",
      "        -0.1037, -0.0340,  0.0143,  0.0630,  0.0423,  0.0236, -0.0256,  0.0123,\n",
      "        -0.0430, -0.0499, -0.0378, -0.0210, -0.0853, -0.0752,  0.0075,  0.0671,\n",
      "         0.0289,  0.0094, -0.0170, -0.2334,  0.0666, -0.0943,  0.1389,  0.1166,\n",
      "         0.0030,  0.0973,  0.0122, -0.0287,  0.0347, -0.0732,  0.0715,  0.0297,\n",
      "        -0.1198, -0.0903, -0.0830,  0.0308,  0.0331, -0.1125,  0.0639,  0.1349,\n",
      "        -0.1510, -0.1661,  0.0243, -0.0368,  0.0923,  0.1711, -0.0113,  0.0961,\n",
      "        -0.0499, -0.1368,  0.0583,  0.0167, -0.0707, -0.0557,  0.0917,  0.0918,\n",
      "        -0.0700, -0.1263, -0.0747,  0.0399, -0.0785, -0.0135, -0.1733, -0.0112,\n",
      "         0.0841, -0.0189, -0.0518, -0.0788,  0.0599, -0.1636,  0.0239,  0.0143,\n",
      "        -0.0784,  0.0766, -0.0759, -0.1063,  0.0169,  0.0335,  0.1520,  0.0254,\n",
      "         0.1096,  0.1417,  0.0120, -0.0545,  0.0180,  0.0164, -0.0685, -0.0569,\n",
      "         0.0748,  0.0101,  0.1164, -0.0086,  0.0693, -0.0073,  0.0139, -0.0821,\n",
      "         0.0299,  0.1342, -0.0272, -0.1269, -0.0297, -0.0817,  0.0360, -0.0522,\n",
      "        -0.0209, -0.0348, -0.0351,  0.0823, -0.2728, -0.0970,  0.0996,  0.0189],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([ 1.5849e-01,  2.3679e-02,  1.3566e-02,  1.0734e-01,  1.7433e-01,\n",
      "         3.9209e-02,  2.2796e-02, -9.5966e-02,  4.2975e-02, -2.5829e-01,\n",
      "        -3.0352e-02,  2.2381e-01,  5.2197e-02, -3.5444e-02,  1.5874e-01,\n",
      "        -1.2947e-02,  2.6940e-04, -4.0569e-03,  2.0145e-02, -1.5539e-02,\n",
      "        -1.4895e-01,  1.1339e-01,  2.0218e-02,  5.9172e-02,  7.4578e-02,\n",
      "         6.7166e-02, -7.5975e-02,  1.5908e-01, -2.5167e-02,  9.8827e-02,\n",
      "        -9.9862e-03, -2.3284e-01, -6.1667e-02,  7.9751e-02,  2.3723e-02,\n",
      "         9.0911e-02, -1.4512e-01,  7.8680e-02,  1.4969e-01, -6.5051e-02,\n",
      "         3.7054e-03,  3.1934e-02,  8.3728e-03,  1.0174e-01, -9.5373e-02,\n",
      "        -2.0249e-01,  5.8993e-02, -1.3037e-01, -1.2322e-01,  4.7339e-02,\n",
      "        -6.5832e-02,  1.2837e-01,  8.0599e-02, -2.0157e-01, -2.7995e-02,\n",
      "        -8.7224e-03,  9.6848e-02, -2.0270e-02,  1.0397e-01, -7.7413e-02,\n",
      "        -3.6434e-02, -2.6463e-02,  1.5062e-02, -4.9797e-03,  1.2398e-02,\n",
      "         4.0794e-02, -1.5929e-01, -8.8939e-02, -6.0441e-02, -1.4344e-01,\n",
      "         2.9011e-02, -7.5216e-02, -1.5774e-01, -4.5697e-02, -1.5955e-01,\n",
      "         2.0152e-01,  3.6381e-02,  4.5547e-02, -1.6658e-01, -7.4051e-02,\n",
      "        -3.4815e-02,  1.2470e-01,  5.7039e-03,  8.0042e-02,  7.6185e-02,\n",
      "         1.1184e-01,  3.2995e-02,  5.8578e-02, -1.3185e-01, -4.6624e-02,\n",
      "        -1.0898e-03,  1.5356e-02,  4.9326e-02,  2.8004e-02,  6.1321e-02,\n",
      "        -4.4638e-02, -5.4449e-02, -2.2959e-02,  4.5339e-02, -2.8627e-01,\n",
      "        -1.4769e-02, -4.2961e-02,  1.4314e-02,  3.3504e-02, -7.2396e-02,\n",
      "         5.2715e-02,  8.8147e-02,  8.4247e-02,  1.9690e-03, -1.3275e-01,\n",
      "        -2.2973e-01,  1.1064e-01,  1.0212e-01,  1.7895e-02,  1.2994e-02,\n",
      "        -6.5323e-02,  5.6229e-02,  2.7322e-02, -8.4434e-02,  2.0327e-02,\n",
      "        -2.9470e-01, -8.1770e-02, -1.3709e-02,  2.1883e-01,  6.5465e-02,\n",
      "        -1.5719e-01,  1.5258e-01, -5.4416e-02, -1.5961e-01, -1.9263e-01,\n",
      "        -1.0857e-01, -6.5266e-02,  7.4174e-02, -8.8055e-02, -6.9738e-03,\n",
      "        -1.1951e-01,  1.0797e-02,  1.1371e-02,  1.2549e-01,  3.4640e-01,\n",
      "        -1.4038e-01, -7.2220e-02, -3.8635e-02,  1.5386e-01,  1.6621e-01,\n",
      "        -4.0399e-02, -8.7887e-03, -3.5529e-03,  3.4935e-02, -5.5878e-02,\n",
      "        -1.2174e-01,  2.6569e-02, -5.8118e-02, -5.8003e-02, -2.2280e-01,\n",
      "         1.2722e-01, -1.6025e-01,  1.5708e-02,  6.2048e-02,  2.6721e-02,\n",
      "        -8.8051e-03,  1.0600e-01, -1.0399e-02, -1.7242e-01,  1.0091e-02,\n",
      "        -2.5520e-03, -8.0835e-02,  1.3849e-01, -5.2459e-02, -5.1549e-03,\n",
      "         1.5640e-01, -5.6216e-02, -7.8921e-02,  1.4171e-01,  1.0466e-01,\n",
      "         8.6440e-02, -8.0193e-02, -1.3422e-01, -2.8721e-01, -1.2977e-01,\n",
      "        -1.0860e-01, -7.5714e-02, -8.8388e-02, -1.0775e-01,  1.3923e-01,\n",
      "        -4.9953e-02,  1.0752e-01,  4.5349e-03,  1.4797e-01,  1.0044e-01,\n",
      "        -1.8321e-01, -1.2360e-01, -1.3144e-01,  2.5921e-02,  5.0204e-02,\n",
      "         9.6864e-02,  8.3198e-03,  7.8777e-02,  4.5637e-02, -1.6303e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 0.0732,  0.0741,  0.2313,  0.1181, -0.1670,  0.0654, -0.0135, -0.1202,\n",
      "         0.0143, -0.0572,  0.1424, -0.0363, -0.0073, -0.1312,  0.1773, -0.0283,\n",
      "         0.0004,  0.1459, -0.1475, -0.0027,  0.1286,  0.0262,  0.0454, -0.1083,\n",
      "        -0.0302, -0.0479, -0.1865, -0.0472,  0.0563,  0.0325,  0.0268,  0.0343,\n",
      "         0.0457,  0.1011,  0.1495,  0.0300, -0.1236,  0.1504,  0.0046, -0.0647,\n",
      "         0.0114,  0.2303, -0.1182,  0.0087, -0.1243,  0.1406,  0.0845, -0.0724,\n",
      "        -0.1268,  0.0963,  0.0399,  0.0424,  0.0839, -0.0363, -0.0561,  0.1502,\n",
      "        -0.0493, -0.0211, -0.0414,  0.0772,  0.0549, -0.1091, -0.0727,  0.0098,\n",
      "        -0.1017, -0.0860,  0.0155, -0.2393,  0.1163, -0.1148, -0.2275, -0.0043,\n",
      "         0.1408,  0.0807,  0.0252,  0.1343, -0.1415, -0.0166, -0.1040,  0.1677,\n",
      "        -0.0583,  0.1253, -0.0867, -0.0894, -0.1532, -0.0385, -0.1057, -0.0351,\n",
      "        -0.0480,  0.0044,  0.0591, -0.0911,  0.0724, -0.0086,  0.0474, -0.0211,\n",
      "        -0.0425,  0.0698,  0.0788, -0.0989,  0.0471, -0.1375,  0.1450,  0.2644,\n",
      "         0.0156,  0.0698, -0.0372,  0.0510,  0.0025, -0.0426, -0.0516,  0.1266,\n",
      "         0.1484, -0.0417, -0.0085,  0.1378, -0.0571,  0.0292, -0.0511, -0.1388,\n",
      "         0.1328,  0.0223, -0.0696,  0.1528,  0.2419,  0.0478, -0.0180,  0.0722,\n",
      "         0.0266,  0.1482, -0.1067, -0.1125, -0.0447,  0.0258, -0.1845, -0.0473,\n",
      "        -0.1753, -0.1652, -0.0112,  0.0071,  0.0076, -0.0798,  0.0395,  0.1330,\n",
      "         0.2310,  0.1397,  0.0322,  0.1101,  0.0605, -0.1853, -0.1923, -0.0848,\n",
      "        -0.0473, -0.0141,  0.0824,  0.0859, -0.0147, -0.0169,  0.0812, -0.0762,\n",
      "         0.2319,  0.0753, -0.0203,  0.0833,  0.0351,  0.0786,  0.0753,  0.0666,\n",
      "        -0.0090, -0.0088,  0.0101, -0.0039, -0.0125, -0.0108, -0.0920,  0.0187,\n",
      "        -0.0256,  0.0645,  0.0371, -0.0738,  0.0038,  0.0336,  0.0655, -0.0351,\n",
      "         0.0137,  0.1571,  0.0351,  0.1191,  0.0697,  0.0734, -0.0692,  0.0413,\n",
      "        -0.0055, -0.0051, -0.0872, -0.0608,  0.0131,  0.0124, -0.1252, -0.1379],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-0.1076, -0.0393,  0.0406, -0.0410,  0.0170,  0.1271,  0.0755, -0.0003,\n",
      "         0.0770,  0.1312,  0.1424,  0.0635,  0.1147, -0.0696, -0.0255, -0.0834,\n",
      "        -0.0051, -0.0814,  0.0894,  0.0571,  0.0204,  0.2439, -0.1648, -0.0601,\n",
      "        -0.0240, -0.0124,  0.0060,  0.0596, -0.0878, -0.1012,  0.0012, -0.1962,\n",
      "        -0.0462, -0.0618,  0.0049, -0.0301, -0.0448,  0.0490,  0.0822, -0.0214,\n",
      "        -0.0445, -0.1750, -0.0355, -0.0069, -0.1411, -0.0784,  0.0986, -0.1250,\n",
      "         0.1102, -0.2366,  0.1119,  0.0574, -0.1518,  0.1687,  0.0899, -0.1017,\n",
      "         0.0093,  0.0022, -0.0426, -0.0309,  0.0399,  0.1174, -0.1743, -0.1418,\n",
      "         0.0361, -0.1238, -0.1715,  0.0593,  0.0382,  0.0593,  0.0854,  0.0388,\n",
      "         0.0713, -0.0835,  0.0226, -0.1758, -0.0266,  0.1431,  0.0361,  0.0300,\n",
      "        -0.0260,  0.1300,  0.0390,  0.1361, -0.0562,  0.0411, -0.0163, -0.0908,\n",
      "         0.0808,  0.0195,  0.0339,  0.0182, -0.0741,  0.0674,  0.1494,  0.0449,\n",
      "        -0.1511, -0.0079, -0.0038, -0.0358,  0.1907, -0.0543, -0.0096, -0.0083,\n",
      "        -0.1593,  0.1843, -0.0645, -0.0164, -0.1389, -0.1156, -0.1592,  0.2092,\n",
      "         0.0542, -0.0091,  0.0102, -0.1332,  0.0284,  0.0612, -0.0283,  0.0937,\n",
      "        -0.0825, -0.1369, -0.0272, -0.1027,  0.0090, -0.0423, -0.0302, -0.0156,\n",
      "        -0.0394,  0.1877,  0.2744, -0.0499,  0.0442, -0.0712, -0.0695, -0.1172,\n",
      "        -0.0023,  0.0627,  0.2155, -0.0484,  0.0266,  0.3375,  0.0263, -0.1519,\n",
      "         0.1114, -0.1968,  0.1853, -0.0083,  0.0084, -0.0749,  0.0859,  0.0281,\n",
      "        -0.0490, -0.1094, -0.0664,  0.0149,  0.0736, -0.0369, -0.1550,  0.0734,\n",
      "         0.0136,  0.0148, -0.0099,  0.0996,  0.2757,  0.0601,  0.0928,  0.0152,\n",
      "         0.0873,  0.0522, -0.0086,  0.1304, -0.0717,  0.1544, -0.1552,  0.0586,\n",
      "         0.0704, -0.1856,  0.0412,  0.0975, -0.1395, -0.0071, -0.1535, -0.1086,\n",
      "         0.0176,  0.0609, -0.0211, -0.1648,  0.1907, -0.0682,  0.0782,  0.0582,\n",
      "        -0.1244, -0.2924, -0.0619,  0.1024, -0.0823,  0.1635, -0.1174,  0.1210],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 0.1395,  0.0534, -0.0695, -0.0048, -0.0126, -0.2033, -0.0882, -0.1174,\n",
      "        -0.0966, -0.1242,  0.0791,  0.0876,  0.0947, -0.0438,  0.0129,  0.0554,\n",
      "        -0.0158,  0.0738,  0.0360,  0.0235,  0.0222,  0.0914,  0.0894,  0.0840,\n",
      "        -0.0637,  0.0813,  0.0573, -0.0271, -0.1223,  0.1174,  0.0601,  0.0039,\n",
      "        -0.0024, -0.0148, -0.1079, -0.0210,  0.1326,  0.0169,  0.0774, -0.0965,\n",
      "        -0.0712, -0.1399,  0.0230,  0.1138, -0.1341, -0.1221, -0.0462,  0.0363,\n",
      "        -0.0786, -0.0219, -0.0179, -0.0251,  0.0331,  0.0111, -0.0925, -0.0448,\n",
      "         0.0070, -0.0202, -0.1424, -0.1049, -0.0418,  0.0058,  0.0346, -0.1332,\n",
      "        -0.0947, -0.0031, -0.0859,  0.2036,  0.0689,  0.0380,  0.0099, -0.1012,\n",
      "         0.0033, -0.0448, -0.1100, -0.0073, -0.0724, -0.0742,  0.0985, -0.1397,\n",
      "         0.0100,  0.0127,  0.1655,  0.0200, -0.0671, -0.0832, -0.0589, -0.0853,\n",
      "         0.0833,  0.1564, -0.0254, -0.1539,  0.0103, -0.0888, -0.0625,  0.0595,\n",
      "        -0.1104, -0.0320, -0.0478,  0.0244, -0.0303, -0.1726,  0.0501,  0.1089,\n",
      "         0.0843,  0.2630,  0.0354,  0.0736,  0.0807,  0.0690,  0.1129, -0.1625,\n",
      "         0.0933, -0.1523,  0.1505,  0.1213,  0.1134, -0.1654, -0.0974, -0.1524,\n",
      "         0.0239,  0.1596,  0.0981, -0.1779,  0.1113,  0.0110,  0.0435, -0.0040,\n",
      "         0.0741,  0.0451,  0.1607, -0.0297,  0.2117,  0.1259, -0.1905, -0.0354,\n",
      "         0.0372, -0.0601,  0.0486, -0.1098, -0.0291,  0.1837, -0.0640, -0.0475,\n",
      "        -0.0706, -0.0266, -0.0214,  0.0846,  0.0028, -0.0053, -0.1114,  0.0774,\n",
      "        -0.1479, -0.0637, -0.0132, -0.0117, -0.0828, -0.0646,  0.0410,  0.0409,\n",
      "        -0.0293,  0.0712,  0.0670, -0.1369,  0.0057, -0.0035, -0.0860, -0.0834,\n",
      "         0.1047, -0.0894,  0.0488,  0.1428, -0.0408, -0.0601,  0.0268,  0.1470,\n",
      "         0.3016, -0.0513, -0.1273,  0.0172, -0.0376,  0.0214, -0.0434,  0.0294,\n",
      "        -0.1377,  0.0471, -0.0220, -0.0021,  0.0422, -0.1599, -0.1767,  0.0464,\n",
      "         0.0014, -0.0571,  0.0160, -0.0198, -0.1708,  0.1424,  0.0662, -0.0499],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 0.0128, -0.0871,  0.0322,  0.0738, -0.0744, -0.1441,  0.1536, -0.0505,\n",
      "        -0.0984,  0.0439, -0.0929, -0.0601, -0.0410,  0.0248,  0.2237,  0.0117,\n",
      "        -0.0861,  0.0113, -0.0168,  0.0495, -0.0763, -0.0262,  0.1475,  0.0643,\n",
      "        -0.0803, -0.0460, -0.0345, -0.0472, -0.1775, -0.1245,  0.2596,  0.0333,\n",
      "        -0.0802, -0.0416, -0.0192, -0.0787,  0.0404,  0.1954, -0.0557,  0.0364,\n",
      "         0.0072,  0.1443, -0.0795,  0.0835, -0.0102,  0.0224, -0.0385, -0.0260,\n",
      "         0.0116, -0.0695,  0.0958,  0.0909,  0.0041, -0.0392,  0.0353, -0.0629,\n",
      "        -0.0511, -0.1085, -0.0755, -0.1980,  0.0705, -0.0324,  0.1778,  0.1383,\n",
      "        -0.0385,  0.0398, -0.0643, -0.0061, -0.0390,  0.0147, -0.0866,  0.0021,\n",
      "        -0.1006, -0.1404, -0.1611, -0.1366,  0.0606, -0.0252, -0.0507,  0.1269,\n",
      "        -0.0060, -0.0722, -0.1634,  0.0207, -0.0758, -0.1116, -0.1820,  0.0547,\n",
      "        -0.0561, -0.0165,  0.1375,  0.1178,  0.0407,  0.0361, -0.1932, -0.1558,\n",
      "         0.1168,  0.0539, -0.1677,  0.0185, -0.0654, -0.0711, -0.0174,  0.0965,\n",
      "        -0.2019,  0.0635,  0.0202, -0.0792,  0.0610,  0.0126,  0.0303,  0.0280,\n",
      "        -0.0779, -0.0686, -0.0287, -0.1318, -0.1112, -0.0303,  0.0962, -0.0528,\n",
      "        -0.0399,  0.2152, -0.0590, -0.0418, -0.0201, -0.0977, -0.0190, -0.1221,\n",
      "        -0.0577,  0.1767, -0.0143, -0.1003, -0.0096,  0.0980, -0.2058,  0.0942,\n",
      "        -0.0429,  0.0077, -0.0989,  0.0445,  0.1789, -0.0584, -0.1730, -0.1366,\n",
      "         0.0390, -0.0033,  0.0962,  0.0283, -0.0413, -0.1301, -0.0343,  0.0210,\n",
      "        -0.0201, -0.0484,  0.0270, -0.0501, -0.0395,  0.0381, -0.0971, -0.1389,\n",
      "         0.0180, -0.1067, -0.0058,  0.0061,  0.0060, -0.0588, -0.0523,  0.1086,\n",
      "        -0.0586,  0.0191,  0.1526, -0.0229,  0.0749, -0.0125, -0.1595,  0.1054,\n",
      "        -0.0892,  0.0669, -0.0340, -0.0607, -0.0590,  0.2360, -0.0819,  0.1578,\n",
      "        -0.0252,  0.1171, -0.0604,  0.2067,  0.1321,  0.0608,  0.0900,  0.1461,\n",
      "        -0.0400,  0.2929, -0.1245,  0.0752,  0.1406,  0.2489,  0.0185, -0.0159],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([ 6.6888e-02, -6.1418e-03, -2.4568e-02, -9.4828e-02,  4.6516e-02,\n",
      "         5.6757e-02, -1.7947e-01, -9.1283e-02,  4.5237e-02,  2.3114e-02,\n",
      "        -6.0123e-02, -2.1058e-01, -9.0464e-02,  1.8217e-02,  4.4946e-02,\n",
      "         4.4095e-02, -5.1784e-02, -7.8815e-02,  1.0761e-01,  1.0487e-01,\n",
      "         3.0290e-02,  2.2630e-01, -1.3630e-01,  2.3062e-01, -6.3343e-02,\n",
      "         1.4002e-01,  3.4069e-02,  1.7433e-03, -1.8485e-02, -1.0928e-01,\n",
      "         1.2108e-01, -2.9272e-01, -1.7142e-01,  3.7265e-02, -1.1584e-01,\n",
      "        -1.1976e-01, -1.3414e-02,  2.2054e-02,  1.5513e-01, -4.2630e-02,\n",
      "         1.1323e-02, -7.9538e-02,  1.9228e-01, -5.2400e-02,  3.5196e-02,\n",
      "        -5.8046e-02, -2.2247e-02, -1.1166e-01, -3.8577e-02, -3.9360e-02,\n",
      "         3.2670e-02,  5.7935e-03, -6.1064e-02,  4.2191e-02,  8.7134e-02,\n",
      "        -1.5192e-01,  1.0160e-01,  3.0027e-02, -9.5434e-02, -1.1957e-01,\n",
      "        -4.9575e-02,  9.4898e-02,  3.4092e-02, -1.1536e-02, -1.6865e-02,\n",
      "        -1.3919e-01,  7.2858e-02, -1.0815e-01,  3.1637e-02, -5.6318e-03,\n",
      "        -1.6310e-01,  7.0930e-02, -7.3628e-02, -1.4554e-01, -3.4920e-03,\n",
      "         2.1053e-01,  2.1163e-04, -2.5883e-01, -7.0333e-02, -7.7609e-03,\n",
      "        -4.4997e-02,  5.8295e-02,  1.2393e-01,  1.7513e-01, -4.4302e-02,\n",
      "        -4.7895e-02,  7.7313e-03, -1.5118e-01, -6.7367e-02, -4.5166e-03,\n",
      "         3.6162e-02, -8.5863e-02, -1.6763e-01, -1.1016e-02,  2.1437e-02,\n",
      "        -4.9902e-02,  1.9406e-03, -3.3102e-02, -4.2933e-03, -9.3692e-02,\n",
      "        -7.6522e-02,  2.5584e-02, -7.5407e-04,  2.3609e-02,  4.8496e-02,\n",
      "        -7.4784e-02,  1.5813e-01,  7.0761e-02,  4.7037e-02,  1.4785e-02,\n",
      "         1.8875e-01, -1.0310e-01, -1.7205e-03, -1.4258e-01, -1.3152e-02,\n",
      "         1.2133e-01,  8.7368e-02, -4.0124e-02, -7.1978e-02,  3.9199e-02,\n",
      "        -1.3281e-02, -8.4048e-02,  1.4860e-01,  1.7051e-02,  3.8434e-02,\n",
      "         3.2071e-02,  1.1985e-01, -8.1576e-02,  8.8900e-03, -1.0452e-01,\n",
      "        -1.6471e-02, -5.2419e-02,  2.8481e-02, -1.3222e-02, -7.7088e-02,\n",
      "         1.6480e-01,  7.3869e-02, -1.5203e-02, -1.4554e-01, -1.9358e-01,\n",
      "        -9.5834e-02,  2.1470e-02, -4.7028e-02, -1.5307e-01,  9.8116e-02,\n",
      "         1.7550e-01,  1.3024e-01, -1.1570e-01, -2.6860e-02, -2.4644e-02,\n",
      "        -9.3422e-02,  9.3815e-02, -1.5879e-01, -7.7661e-02, -3.7704e-02,\n",
      "        -2.1603e-02, -1.3125e-01,  1.6445e-05, -1.2429e-01,  1.0572e-01,\n",
      "         5.4368e-02,  1.7548e-01, -2.0966e-02, -4.0451e-02, -1.2016e-01,\n",
      "        -9.5806e-02, -1.8581e-01,  1.1379e-01,  4.2148e-02,  1.9837e-01,\n",
      "        -1.0590e-01, -1.2844e-01,  1.3600e-01, -1.0818e-01, -9.9925e-02,\n",
      "        -1.0407e-01, -1.1221e-01, -1.4553e-01, -1.8208e-01, -3.5679e-02,\n",
      "        -1.7863e-01,  1.8451e-02,  1.1091e-01, -1.1660e-02,  1.0457e-01,\n",
      "        -6.1032e-02, -1.8050e-01,  6.0150e-02,  9.5766e-03,  5.1878e-02,\n",
      "        -1.9350e-01, -1.3023e-01, -1.8349e-02, -2.2720e-03, -7.7910e-02,\n",
      "         8.7460e-02,  7.7030e-02,  7.9021e-02, -6.9683e-03,  1.0263e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  0.067108/  0.083278, val:  44.17%, val_best:  44.17%, tr:  75.28%, tr_best:  75.28%, epoch time: 68.04 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0763%\n",
      "layer   2  Sparsity: 53.9977%\n",
      "layer   3  Sparsity: 49.4053%\n",
      "total_backward_count 9790 real_backward_count 3514  35.894%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  0.046400/  0.074089, val:  48.75%, val_best:  48.75%, tr:  88.36%, tr_best:  88.36%, epoch time: 67.74 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0555%\n",
      "layer   2  Sparsity: 50.9271%\n",
      "layer   3  Sparsity: 47.4927%\n",
      "total_backward_count 19580 real_backward_count 5691  29.065%\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  0.040648/  0.070588, val:  51.67%, val_best:  51.67%, tr:  90.40%, tr_best:  90.40%, epoch time: 68.42 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1386%\n",
      "layer   2  Sparsity: 49.8613%\n",
      "layer   3  Sparsity: 47.2036%\n",
      "total_backward_count 29370 real_backward_count 7508  25.564%\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  0.038947/  0.066751, val:  59.58%, val_best:  59.58%, tr:  90.81%, tr_best:  90.81%, epoch time: 68.01 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   2  Sparsity: 49.6940%\n",
      "layer   3  Sparsity: 46.8204%\n",
      "total_backward_count 39160 real_backward_count 9258  23.641%\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  0.036640/  0.067765, val:  54.17%, val_best:  59.58%, tr:  91.52%, tr_best:  91.52%, epoch time: 67.97 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 49.4358%\n",
      "layer   3  Sparsity: 46.7135%\n",
      "total_backward_count 48950 real_backward_count 10899  22.266%\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  0.034943/  0.078558, val:  52.08%, val_best:  59.58%, tr:  93.16%, tr_best:  93.16%, epoch time: 67.96 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0513%\n",
      "layer   2  Sparsity: 48.9452%\n",
      "layer   3  Sparsity: 45.7752%\n",
      "total_backward_count 58740 real_backward_count 12441  21.180%\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  0.034825/  0.061949, val:  60.00%, val_best:  60.00%, tr:  95.10%, tr_best:  95.10%, epoch time: 68.05 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0710%\n",
      "layer   2  Sparsity: 48.7510%\n",
      "layer   3  Sparsity: 45.2328%\n",
      "total_backward_count 68530 real_backward_count 13937  20.337%\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  0.034009/  0.076326, val:  50.42%, val_best:  60.00%, tr:  92.85%, tr_best:  95.10%, epoch time: 67.51 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   2  Sparsity: 48.9405%\n",
      "layer   3  Sparsity: 44.8580%\n",
      "total_backward_count 78320 real_backward_count 15456  19.734%\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  0.032695/  0.080436, val:  52.08%, val_best:  60.00%, tr:  95.10%, tr_best:  95.10%, epoch time: 68.36 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0801%\n",
      "layer   2  Sparsity: 49.0949%\n",
      "layer   3  Sparsity: 44.5550%\n",
      "total_backward_count 88110 real_backward_count 16855  19.129%\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  0.032082/  0.062969, val:  59.58%, val_best:  60.00%, tr:  95.20%, tr_best:  95.20%, epoch time: 62.33 seconds, 1.04 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   2  Sparsity: 48.8171%\n",
      "layer   3  Sparsity: 44.4209%\n",
      "total_backward_count 97900 real_backward_count 18228  18.619%\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  0.031555/  0.056267, val:  68.33%, val_best:  68.33%, tr:  95.61%, tr_best:  95.61%, epoch time: 67.58 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0677%\n",
      "layer   2  Sparsity: 48.8796%\n",
      "layer   3  Sparsity: 44.2248%\n",
      "total_backward_count 107690 real_backward_count 19639  18.237%\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  0.030354/  0.066437, val:  60.42%, val_best:  68.33%, tr:  95.20%, tr_best:  95.61%, epoch time: 67.37 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0733%\n",
      "layer   2  Sparsity: 49.1234%\n",
      "layer   3  Sparsity: 44.2351%\n",
      "total_backward_count 117480 real_backward_count 20958  17.840%\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  0.029561/  0.055436, val:  67.50%, val_best:  68.33%, tr:  97.04%, tr_best:  97.04%, epoch time: 68.35 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   2  Sparsity: 49.1770%\n",
      "layer   3  Sparsity: 44.1047%\n",
      "total_backward_count 127270 real_backward_count 22198  17.442%\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  0.028371/  0.063300, val:  59.17%, val_best:  68.33%, tr:  96.42%, tr_best:  97.04%, epoch time: 68.01 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0554%\n",
      "layer   2  Sparsity: 49.0721%\n",
      "layer   3  Sparsity: 44.0400%\n",
      "total_backward_count 137060 real_backward_count 23383  17.060%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  0.028258/  0.073695, val:  57.92%, val_best:  68.33%, tr:  96.94%, tr_best:  97.04%, epoch time: 67.92 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0780%\n",
      "layer   2  Sparsity: 49.1953%\n",
      "layer   3  Sparsity: 43.8773%\n",
      "total_backward_count 146850 real_backward_count 24549  16.717%\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  0.027935/  0.063703, val:  63.33%, val_best:  68.33%, tr:  97.14%, tr_best:  97.14%, epoch time: 68.01 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0429%\n",
      "layer   2  Sparsity: 49.2469%\n",
      "layer   3  Sparsity: 44.1142%\n",
      "total_backward_count 156640 real_backward_count 25700  16.407%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  0.026872/  0.055265, val:  72.50%, val_best:  72.50%, tr:  98.26%, tr_best:  98.26%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0614%\n",
      "layer   2  Sparsity: 49.1151%\n",
      "layer   3  Sparsity: 44.2549%\n",
      "total_backward_count 166430 real_backward_count 26824  16.117%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  0.026701/  0.053763, val:  71.67%, val_best:  72.50%, tr:  97.75%, tr_best:  98.26%, epoch time: 67.61 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   2  Sparsity: 49.3112%\n",
      "layer   3  Sparsity: 44.2161%\n",
      "total_backward_count 176220 real_backward_count 27938  15.854%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  0.025597/  0.069070, val:  54.17%, val_best:  72.50%, tr:  98.67%, tr_best:  98.67%, epoch time: 67.92 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1055%\n",
      "layer   2  Sparsity: 49.4872%\n",
      "layer   3  Sparsity: 44.0594%\n",
      "total_backward_count 186010 real_backward_count 28969  15.574%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  0.025106/  0.053170, val:  72.08%, val_best:  72.50%, tr:  98.77%, tr_best:  98.77%, epoch time: 67.99 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0790%\n",
      "layer   2  Sparsity: 49.4623%\n",
      "layer   3  Sparsity: 43.9517%\n",
      "total_backward_count 195800 real_backward_count 30031  15.338%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  0.024546/  0.079068, val:  50.00%, val_best:  72.50%, tr:  98.77%, tr_best:  98.77%, epoch time: 67.75 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0990%\n",
      "layer   2  Sparsity: 49.6711%\n",
      "layer   3  Sparsity: 43.7548%\n",
      "total_backward_count 205590 real_backward_count 31008  15.082%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  0.024170/  0.050844, val:  72.92%, val_best:  72.92%, tr:  98.88%, tr_best:  98.88%, epoch time: 67.85 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0891%\n",
      "layer   2  Sparsity: 49.8126%\n",
      "layer   3  Sparsity: 43.3931%\n",
      "total_backward_count 215380 real_backward_count 31972  14.844%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  0.024163/  0.057251, val:  72.50%, val_best:  72.92%, tr:  98.77%, tr_best:  98.88%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0652%\n",
      "layer   2  Sparsity: 49.8273%\n",
      "layer   3  Sparsity: 43.7347%\n",
      "total_backward_count 225170 real_backward_count 32920  14.620%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  0.023622/  0.047585, val:  77.92%, val_best:  77.92%, tr:  98.67%, tr_best:  98.88%, epoch time: 67.92 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   2  Sparsity: 49.6880%\n",
      "layer   3  Sparsity: 43.5595%\n",
      "total_backward_count 234960 real_backward_count 33856  14.409%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  0.022270/  0.055631, val:  72.92%, val_best:  77.92%, tr:  99.08%, tr_best:  99.08%, epoch time: 68.42 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1095%\n",
      "layer   2  Sparsity: 49.7661%\n",
      "layer   3  Sparsity: 43.5744%\n",
      "total_backward_count 244750 real_backward_count 34688  14.173%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  0.022356/  0.049846, val:  80.00%, val_best:  80.00%, tr:  99.28%, tr_best:  99.28%, epoch time: 68.51 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   2  Sparsity: 49.8150%\n",
      "layer   3  Sparsity: 43.3454%\n",
      "total_backward_count 254540 real_backward_count 35561  13.971%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  0.022246/  0.047064, val:  75.83%, val_best:  80.00%, tr:  99.28%, tr_best:  99.28%, epoch time: 67.50 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0618%\n",
      "layer   2  Sparsity: 49.8105%\n",
      "layer   3  Sparsity: 43.3772%\n",
      "total_backward_count 264330 real_backward_count 36431  13.782%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  0.021491/  0.065868, val:  66.25%, val_best:  80.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 67.95 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1068%\n",
      "layer   2  Sparsity: 50.0482%\n",
      "layer   3  Sparsity: 43.3933%\n",
      "total_backward_count 274120 real_backward_count 37255  13.591%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  0.020810/  0.048709, val:  77.50%, val_best:  80.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0475%\n",
      "layer   2  Sparsity: 49.8703%\n",
      "layer   3  Sparsity: 43.1954%\n",
      "total_backward_count 283910 real_backward_count 38065  13.407%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  0.021681/  0.053871, val:  75.42%, val_best:  80.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   2  Sparsity: 49.9645%\n",
      "layer   3  Sparsity: 43.0959%\n",
      "total_backward_count 293700 real_backward_count 38932  13.256%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  0.020594/  0.055651, val:  70.42%, val_best:  80.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 68.16 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0391%\n",
      "layer   2  Sparsity: 50.1844%\n",
      "layer   3  Sparsity: 42.8914%\n",
      "total_backward_count 303490 real_backward_count 39728  13.090%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  0.020838/  0.049586, val:  81.25%, val_best:  81.25%, tr:  99.49%, tr_best:  99.69%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0907%\n",
      "layer   2  Sparsity: 50.1011%\n",
      "layer   3  Sparsity: 42.9265%\n",
      "total_backward_count 313280 real_backward_count 40532  12.938%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  0.020368/  0.044432, val:  82.50%, val_best:  82.50%, tr:  99.49%, tr_best:  99.69%, epoch time: 67.73 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0672%\n",
      "layer   2  Sparsity: 50.2348%\n",
      "layer   3  Sparsity: 42.7767%\n",
      "total_backward_count 323070 real_backward_count 41316  12.789%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  0.020172/  0.046913, val:  82.50%, val_best:  82.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0455%\n",
      "layer   2  Sparsity: 50.0845%\n",
      "layer   3  Sparsity: 42.6954%\n",
      "total_backward_count 332860 real_backward_count 42095  12.646%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  0.019799/  0.045693, val:  80.00%, val_best:  82.50%, tr:  99.49%, tr_best:  99.69%, epoch time: 68.01 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 50.1488%\n",
      "layer   3  Sparsity: 42.4401%\n",
      "total_backward_count 342650 real_backward_count 42867  12.510%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  0.019276/  0.058302, val:  78.33%, val_best:  82.50%, tr:  99.59%, tr_best:  99.69%, epoch time: 67.98 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 50.3037%\n",
      "layer   3  Sparsity: 42.3474%\n",
      "total_backward_count 352440 real_backward_count 43571  12.363%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  0.019123/  0.058681, val:  79.17%, val_best:  82.50%, tr:  99.80%, tr_best:  99.80%, epoch time: 67.86 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 50.2420%\n",
      "layer   3  Sparsity: 42.8182%\n",
      "total_backward_count 362230 real_backward_count 44289  12.227%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  0.018340/  0.051246, val:  80.83%, val_best:  82.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 67.52 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0858%\n",
      "layer   2  Sparsity: 50.3696%\n",
      "layer   3  Sparsity: 42.7670%\n",
      "total_backward_count 372020 real_backward_count 44971  12.088%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  0.018936/  0.043135, val:  83.33%, val_best:  83.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 68.23 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1075%\n",
      "layer   2  Sparsity: 50.2415%\n",
      "layer   3  Sparsity: 42.8066%\n",
      "total_backward_count 381810 real_backward_count 45658  11.958%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  0.018514/  0.052225, val:  80.83%, val_best:  83.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 68.40 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   2  Sparsity: 50.4228%\n",
      "layer   3  Sparsity: 42.6767%\n",
      "total_backward_count 391600 real_backward_count 46353  11.837%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  0.018501/  0.043890, val:  84.17%, val_best:  84.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 67.74 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   2  Sparsity: 50.4041%\n",
      "layer   3  Sparsity: 42.7734%\n",
      "total_backward_count 401390 real_backward_count 47067  11.726%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  0.018105/  0.054345, val:  76.25%, val_best:  84.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 68.15 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0743%\n",
      "layer   2  Sparsity: 50.5522%\n",
      "layer   3  Sparsity: 42.6651%\n",
      "total_backward_count 411180 real_backward_count 47739  11.610%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  0.017762/  0.042411, val:  85.42%, val_best:  85.42%, tr:  99.49%, tr_best:  99.90%, epoch time: 67.73 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0301%\n",
      "layer   2  Sparsity: 50.6396%\n",
      "layer   3  Sparsity: 42.5167%\n",
      "total_backward_count 420970 real_backward_count 48397  11.497%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  0.017505/  0.041257, val:  84.17%, val_best:  85.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 67.53 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   2  Sparsity: 50.7838%\n",
      "layer   3  Sparsity: 42.7124%\n",
      "total_backward_count 430760 real_backward_count 49071  11.392%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  0.017148/  0.048145, val:  80.83%, val_best:  85.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 68.34 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0564%\n",
      "layer   2  Sparsity: 50.6776%\n",
      "layer   3  Sparsity: 42.6610%\n",
      "total_backward_count 440550 real_backward_count 49725  11.287%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  0.017191/  0.043368, val:  84.17%, val_best:  85.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 68.24 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   2  Sparsity: 50.7131%\n",
      "layer   3  Sparsity: 42.5856%\n",
      "total_backward_count 450340 real_backward_count 50386  11.188%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  0.016877/  0.045930, val:  77.08%, val_best:  85.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 68.23 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1119%\n",
      "layer   2  Sparsity: 50.6681%\n",
      "layer   3  Sparsity: 42.6077%\n",
      "total_backward_count 460130 real_backward_count 50997  11.083%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  0.017297/  0.042415, val:  83.75%, val_best:  85.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 68.03 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0737%\n",
      "layer   2  Sparsity: 50.6476%\n",
      "layer   3  Sparsity: 42.6455%\n",
      "total_backward_count 469920 real_backward_count 51649  10.991%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  0.017005/  0.040829, val:  84.17%, val_best:  85.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 67.53 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   2  Sparsity: 50.6484%\n",
      "layer   3  Sparsity: 42.3898%\n",
      "total_backward_count 479710 real_backward_count 52312  10.905%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  0.016574/  0.040916, val:  84.58%, val_best:  85.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 68.12 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   2  Sparsity: 50.8615%\n",
      "layer   3  Sparsity: 42.2827%\n",
      "total_backward_count 489500 real_backward_count 52954  10.818%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  0.016947/  0.048144, val:  81.67%, val_best:  85.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 68.50 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   2  Sparsity: 50.8653%\n",
      "layer   3  Sparsity: 42.4799%\n",
      "total_backward_count 499290 real_backward_count 53614  10.738%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  0.016292/  0.045779, val:  79.58%, val_best:  85.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 67.78 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0450%\n",
      "layer   2  Sparsity: 50.8325%\n",
      "layer   3  Sparsity: 42.6891%\n",
      "total_backward_count 509080 real_backward_count 54231  10.653%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  0.016017/  0.045842, val:  85.83%, val_best:  85.83%, tr:  99.80%, tr_best:  99.90%, epoch time: 67.73 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0339%\n",
      "layer   2  Sparsity: 50.7911%\n",
      "layer   3  Sparsity: 42.7158%\n",
      "total_backward_count 518870 real_backward_count 54813  10.564%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  0.015138/  0.042174, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.52 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   2  Sparsity: 50.8298%\n",
      "layer   3  Sparsity: 42.6678%\n",
      "total_backward_count 528660 real_backward_count 55365  10.473%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  0.015630/  0.042401, val:  82.08%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 50.9721%\n",
      "layer   3  Sparsity: 42.7055%\n",
      "total_backward_count 538450 real_backward_count 55955  10.392%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  0.015674/  0.041847, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.83 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0538%\n",
      "layer   2  Sparsity: 51.0928%\n",
      "layer   3  Sparsity: 42.6106%\n",
      "total_backward_count 548240 real_backward_count 56538  10.313%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  0.015957/  0.048618, val:  78.33%, val_best:  85.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 68.16 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1028%\n",
      "layer   2  Sparsity: 51.1032%\n",
      "layer   3  Sparsity: 42.7463%\n",
      "total_backward_count 558030 real_backward_count 57167  10.244%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  0.015067/  0.048435, val:  81.25%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.29 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 51.2511%\n",
      "layer   3  Sparsity: 42.8027%\n",
      "total_backward_count 567820 real_backward_count 57716  10.164%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  0.014804/  0.039121, val:  85.42%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.39 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0478%\n",
      "layer   2  Sparsity: 51.3394%\n",
      "layer   3  Sparsity: 42.8618%\n",
      "total_backward_count 577610 real_backward_count 58265  10.087%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  0.014579/  0.045719, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.58 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   2  Sparsity: 51.5771%\n",
      "layer   3  Sparsity: 42.7223%\n",
      "total_backward_count 587400 real_backward_count 58789  10.008%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  0.015142/  0.039627, val:  84.17%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.60 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   2  Sparsity: 51.5219%\n",
      "layer   3  Sparsity: 42.7595%\n",
      "total_backward_count 597190 real_backward_count 59370   9.942%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  0.014838/  0.041173, val:  84.58%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 65.11 seconds, 1.09 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 51.2277%\n",
      "layer   3  Sparsity: 42.6842%\n",
      "total_backward_count 606980 real_backward_count 59945   9.876%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  0.014779/  0.040983, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 65.52 seconds, 1.09 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   2  Sparsity: 51.1954%\n",
      "layer   3  Sparsity: 42.4994%\n",
      "total_backward_count 616770 real_backward_count 60505   9.810%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  0.014399/  0.039025, val:  86.67%, val_best:  86.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 67.89 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0516%\n",
      "layer   2  Sparsity: 51.3130%\n",
      "layer   3  Sparsity: 42.6498%\n",
      "total_backward_count 626560 real_backward_count 60997   9.735%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  0.014302/  0.037760, val:  87.50%, val_best:  87.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 68.35 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1069%\n",
      "layer   2  Sparsity: 51.3655%\n",
      "layer   3  Sparsity: 42.7875%\n",
      "total_backward_count 636350 real_backward_count 61541   9.671%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  0.014349/  0.056975, val:  75.00%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.72 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0865%\n",
      "layer   2  Sparsity: 51.3521%\n",
      "layer   3  Sparsity: 42.6892%\n",
      "total_backward_count 646140 real_backward_count 62091   9.610%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  0.014174/  0.040790, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.14 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0704%\n",
      "layer   2  Sparsity: 51.4011%\n",
      "layer   3  Sparsity: 42.5819%\n",
      "total_backward_count 655930 real_backward_count 62618   9.546%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  0.014043/  0.044732, val:  82.92%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.50 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0386%\n",
      "layer   2  Sparsity: 51.5013%\n",
      "layer   3  Sparsity: 42.6008%\n",
      "total_backward_count 665720 real_backward_count 63160   9.487%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  0.014014/  0.047495, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.62 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   2  Sparsity: 51.4624%\n",
      "layer   3  Sparsity: 42.6297%\n",
      "total_backward_count 675510 real_backward_count 63696   9.429%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  0.014339/  0.043102, val:  84.58%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.00 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0473%\n",
      "layer   2  Sparsity: 51.4273%\n",
      "layer   3  Sparsity: 42.5488%\n",
      "total_backward_count 685300 real_backward_count 64260   9.377%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  0.013270/  0.044741, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.24 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0498%\n",
      "layer   2  Sparsity: 51.3963%\n",
      "layer   3  Sparsity: 42.6450%\n",
      "total_backward_count 695090 real_backward_count 64750   9.315%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  0.013706/  0.038956, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.02 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1158%\n",
      "layer   2  Sparsity: 51.5050%\n",
      "layer   3  Sparsity: 42.5604%\n",
      "total_backward_count 704880 real_backward_count 65275   9.260%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  0.013521/  0.038130, val:  85.83%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.97 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   2  Sparsity: 51.5549%\n",
      "layer   3  Sparsity: 42.4646%\n",
      "total_backward_count 714670 real_backward_count 65806   9.208%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  0.013315/  0.041608, val:  84.58%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.53 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   2  Sparsity: 51.5070%\n",
      "layer   3  Sparsity: 42.5521%\n",
      "total_backward_count 724460 real_backward_count 66298   9.151%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  0.013020/  0.039507, val:  87.08%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.94 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 51.5695%\n",
      "layer   3  Sparsity: 42.6218%\n",
      "total_backward_count 734250 real_backward_count 66797   9.097%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  0.013977/  0.038105, val:  87.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.98 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0736%\n",
      "layer   2  Sparsity: 51.6512%\n",
      "layer   3  Sparsity: 42.5769%\n",
      "total_backward_count 744040 real_backward_count 67357   9.053%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  0.013493/  0.037748, val:  86.67%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.16 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0823%\n",
      "layer   2  Sparsity: 51.8069%\n",
      "layer   3  Sparsity: 42.6414%\n",
      "total_backward_count 753830 real_backward_count 67871   9.003%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  0.012947/  0.037078, val:  88.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.70 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0905%\n",
      "layer   2  Sparsity: 51.6675%\n",
      "layer   3  Sparsity: 42.6816%\n",
      "total_backward_count 763620 real_backward_count 68365   8.953%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  0.012935/  0.038522, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.22 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   2  Sparsity: 51.5194%\n",
      "layer   3  Sparsity: 42.9397%\n",
      "total_backward_count 773410 real_backward_count 68856   8.903%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  0.012901/  0.041341, val:  83.75%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.08 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   2  Sparsity: 51.6152%\n",
      "layer   3  Sparsity: 43.0467%\n",
      "total_backward_count 783200 real_backward_count 69327   8.852%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  0.012578/  0.037122, val:  85.42%, val_best:  88.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 68.39 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   2  Sparsity: 51.6651%\n",
      "layer   3  Sparsity: 43.0137%\n",
      "total_backward_count 792990 real_backward_count 69773   8.799%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  0.012536/  0.040312, val:  86.67%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.79 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 51.8177%\n",
      "layer   3  Sparsity: 42.9268%\n",
      "total_backward_count 802780 real_backward_count 70243   8.750%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  0.012715/  0.039491, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.25 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   2  Sparsity: 51.7784%\n",
      "layer   3  Sparsity: 43.0288%\n",
      "total_backward_count 812570 real_backward_count 70719   8.703%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  0.012518/  0.040796, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.97 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   2  Sparsity: 51.6644%\n",
      "layer   3  Sparsity: 43.0642%\n",
      "total_backward_count 822360 real_backward_count 71184   8.656%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  0.012179/  0.035685, val:  87.08%, val_best:  88.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 68.08 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0889%\n",
      "layer   2  Sparsity: 51.7436%\n",
      "layer   3  Sparsity: 43.1475%\n",
      "total_backward_count 832150 real_backward_count 71653   8.611%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  0.012696/  0.036941, val:  87.50%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0490%\n",
      "layer   2  Sparsity: 51.8127%\n",
      "layer   3  Sparsity: 42.8731%\n",
      "total_backward_count 841940 real_backward_count 72138   8.568%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  0.012026/  0.037726, val:  87.92%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.98 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1074%\n",
      "layer   2  Sparsity: 51.8622%\n",
      "layer   3  Sparsity: 42.9864%\n",
      "total_backward_count 851730 real_backward_count 72592   8.523%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  0.012506/  0.039714, val:  85.42%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0676%\n",
      "layer   2  Sparsity: 51.8404%\n",
      "layer   3  Sparsity: 42.8977%\n",
      "total_backward_count 861520 real_backward_count 73056   8.480%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  0.011705/  0.046724, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.15 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   2  Sparsity: 51.9610%\n",
      "layer   3  Sparsity: 42.9920%\n",
      "total_backward_count 871310 real_backward_count 73482   8.434%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  0.012412/  0.042792, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.61 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0355%\n",
      "layer   2  Sparsity: 51.9841%\n",
      "layer   3  Sparsity: 42.8359%\n",
      "total_backward_count 881100 real_backward_count 73954   8.393%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  0.011912/  0.038896, val:  87.08%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.80 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   2  Sparsity: 52.0288%\n",
      "layer   3  Sparsity: 43.0693%\n",
      "total_backward_count 890890 real_backward_count 74418   8.353%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  0.011804/  0.035421, val:  86.25%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.01 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   2  Sparsity: 52.0415%\n",
      "layer   3  Sparsity: 42.9558%\n",
      "total_backward_count 900680 real_backward_count 74859   8.311%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  0.011482/  0.037571, val:  87.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.30 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1064%\n",
      "layer   2  Sparsity: 51.9348%\n",
      "layer   3  Sparsity: 42.8572%\n",
      "total_backward_count 910470 real_backward_count 75284   8.269%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  0.011677/  0.043201, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.13 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0676%\n",
      "layer   2  Sparsity: 51.8144%\n",
      "layer   3  Sparsity: 42.9726%\n",
      "total_backward_count 920260 real_backward_count 75727   8.229%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  0.011327/  0.036902, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.05 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0982%\n",
      "layer   2  Sparsity: 51.8057%\n",
      "layer   3  Sparsity: 42.9986%\n",
      "total_backward_count 930050 real_backward_count 76131   8.186%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  0.011291/  0.036440, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.74 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0483%\n",
      "layer   2  Sparsity: 51.8811%\n",
      "layer   3  Sparsity: 43.0283%\n",
      "total_backward_count 939840 real_backward_count 76543   8.144%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  0.011832/  0.042284, val:  85.42%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.28 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   2  Sparsity: 51.8634%\n",
      "layer   3  Sparsity: 42.9238%\n",
      "total_backward_count 949630 real_backward_count 76988   8.107%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  0.011613/  0.035510, val:  87.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.42 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0694%\n",
      "layer   2  Sparsity: 51.9349%\n",
      "layer   3  Sparsity: 42.9037%\n",
      "total_backward_count 959420 real_backward_count 77405   8.068%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  0.011309/  0.039352, val:  87.92%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.97 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0937%\n",
      "layer   2  Sparsity: 51.9797%\n",
      "layer   3  Sparsity: 42.9612%\n",
      "total_backward_count 969210 real_backward_count 77815   8.029%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  0.011099/  0.038103, val:  87.50%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.12 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0536%\n",
      "layer   2  Sparsity: 51.9929%\n",
      "layer   3  Sparsity: 43.0471%\n",
      "total_backward_count 979000 real_backward_count 78236   7.991%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  0.010627/  0.040730, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.77 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   2  Sparsity: 51.9729%\n",
      "layer   3  Sparsity: 42.9977%\n",
      "total_backward_count 988790 real_backward_count 78620   7.951%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  0.011675/  0.035462, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.18 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0676%\n",
      "layer   2  Sparsity: 51.8913%\n",
      "layer   3  Sparsity: 42.9897%\n",
      "total_backward_count 998580 real_backward_count 79070   7.918%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  0.011331/  0.035507, val:  88.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.37 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0560%\n",
      "layer   2  Sparsity: 51.9012%\n",
      "layer   3  Sparsity: 43.0175%\n",
      "total_backward_count 1008370 real_backward_count 79491   7.883%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  0.011665/  0.036379, val:  87.92%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.87 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1057%\n",
      "layer   2  Sparsity: 51.9186%\n",
      "layer   3  Sparsity: 43.0993%\n",
      "total_backward_count 1018160 real_backward_count 79947   7.852%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  0.010798/  0.040387, val:  85.42%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.64 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0728%\n",
      "layer   2  Sparsity: 51.9457%\n",
      "layer   3  Sparsity: 42.8417%\n",
      "total_backward_count 1027950 real_backward_count 80342   7.816%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  0.010834/  0.040617, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.95 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0978%\n",
      "layer   2  Sparsity: 51.9097%\n",
      "layer   3  Sparsity: 42.9785%\n",
      "total_backward_count 1037740 real_backward_count 80745   7.781%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  0.010891/  0.035315, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0530%\n",
      "layer   2  Sparsity: 51.8516%\n",
      "layer   3  Sparsity: 43.0751%\n",
      "total_backward_count 1047530 real_backward_count 81132   7.745%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  0.011242/  0.035451, val:  88.75%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.18 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0600%\n",
      "layer   2  Sparsity: 51.9831%\n",
      "layer   3  Sparsity: 43.0498%\n",
      "total_backward_count 1057320 real_backward_count 81544   7.712%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  0.010881/  0.037980, val:  88.75%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.19 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   2  Sparsity: 51.9651%\n",
      "layer   3  Sparsity: 42.9133%\n",
      "total_backward_count 1067110 real_backward_count 81937   7.678%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  0.010815/  0.037822, val:  87.92%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.52 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0418%\n",
      "layer   2  Sparsity: 52.0427%\n",
      "layer   3  Sparsity: 42.9708%\n",
      "total_backward_count 1076900 real_backward_count 82346   7.647%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  0.010517/  0.036768, val:  88.33%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.45 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0587%\n",
      "layer   2  Sparsity: 52.1450%\n",
      "layer   3  Sparsity: 42.9905%\n",
      "total_backward_count 1086690 real_backward_count 82720   7.612%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  0.010517/  0.034876, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.23 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0631%\n",
      "layer   2  Sparsity: 52.0754%\n",
      "layer   3  Sparsity: 42.9275%\n",
      "total_backward_count 1096480 real_backward_count 83113   7.580%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  0.010818/  0.036576, val:  86.67%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0921%\n",
      "layer   2  Sparsity: 52.0669%\n",
      "layer   3  Sparsity: 42.8903%\n",
      "total_backward_count 1106270 real_backward_count 83518   7.550%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  0.010748/  0.036129, val:  85.00%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.14 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1248%\n",
      "layer   2  Sparsity: 52.0380%\n",
      "layer   3  Sparsity: 43.1126%\n",
      "total_backward_count 1116060 real_backward_count 83931   7.520%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  0.010099/  0.037868, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.30 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0937%\n",
      "layer   2  Sparsity: 52.0706%\n",
      "layer   3  Sparsity: 43.1740%\n",
      "total_backward_count 1125850 real_backward_count 84305   7.488%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  0.010214/  0.035073, val:  88.33%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.46 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0888%\n",
      "layer   2  Sparsity: 52.1258%\n",
      "layer   3  Sparsity: 43.3257%\n",
      "total_backward_count 1135640 real_backward_count 84655   7.454%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  0.009805/  0.042129, val:  85.42%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.18 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0547%\n",
      "layer   2  Sparsity: 52.0719%\n",
      "layer   3  Sparsity: 43.3110%\n",
      "total_backward_count 1145430 real_backward_count 85003   7.421%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  0.010128/  0.035537, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.80 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0753%\n",
      "layer   2  Sparsity: 51.9808%\n",
      "layer   3  Sparsity: 43.2467%\n",
      "total_backward_count 1155220 real_backward_count 85370   7.390%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  0.010131/  0.036178, val:  86.67%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.95 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0532%\n",
      "layer   2  Sparsity: 52.0522%\n",
      "layer   3  Sparsity: 43.1727%\n",
      "total_backward_count 1165010 real_backward_count 85731   7.359%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  0.010508/  0.035488, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.45 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0756%\n",
      "layer   2  Sparsity: 51.9668%\n",
      "layer   3  Sparsity: 43.1033%\n",
      "total_backward_count 1174800 real_backward_count 86115   7.330%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  0.010145/  0.034848, val:  88.75%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   2  Sparsity: 52.0586%\n",
      "layer   3  Sparsity: 43.1001%\n",
      "total_backward_count 1184590 real_backward_count 86481   7.301%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  0.009709/  0.036415, val:  86.67%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.07 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0982%\n",
      "layer   2  Sparsity: 52.0115%\n",
      "layer   3  Sparsity: 43.2706%\n",
      "total_backward_count 1194380 real_backward_count 86839   7.271%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  0.010058/  0.035012, val:  88.75%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.16 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   2  Sparsity: 51.9869%\n",
      "layer   3  Sparsity: 43.2011%\n",
      "total_backward_count 1204170 real_backward_count 87209   7.242%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  0.010116/  0.036248, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.16 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1227%\n",
      "layer   2  Sparsity: 52.1042%\n",
      "layer   3  Sparsity: 43.1275%\n",
      "total_backward_count 1213960 real_backward_count 87556   7.212%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  0.009823/  0.034702, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.55 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 52.0895%\n",
      "layer   3  Sparsity: 42.9763%\n",
      "total_backward_count 1223750 real_backward_count 87914   7.184%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  0.009677/  0.035002, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.32 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0544%\n",
      "layer   2  Sparsity: 52.1350%\n",
      "layer   3  Sparsity: 43.0538%\n",
      "total_backward_count 1233540 real_backward_count 88245   7.154%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  0.009399/  0.038019, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.94 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0531%\n",
      "layer   2  Sparsity: 52.1233%\n",
      "layer   3  Sparsity: 43.0762%\n",
      "total_backward_count 1243330 real_backward_count 88566   7.123%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  0.009541/  0.034922, val:  88.33%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.98 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0642%\n",
      "layer   2  Sparsity: 52.1132%\n",
      "layer   3  Sparsity: 43.2702%\n",
      "total_backward_count 1253120 real_backward_count 88906   7.095%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  0.009425/  0.035772, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.22 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0536%\n",
      "layer   2  Sparsity: 52.0481%\n",
      "layer   3  Sparsity: 43.2076%\n",
      "total_backward_count 1262910 real_backward_count 89248   7.067%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  0.009645/  0.037329, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.96 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0820%\n",
      "layer   2  Sparsity: 52.2208%\n",
      "layer   3  Sparsity: 43.2749%\n",
      "total_backward_count 1272700 real_backward_count 89598   7.040%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  0.009573/  0.038377, val:  87.50%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.70 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   2  Sparsity: 52.1950%\n",
      "layer   3  Sparsity: 43.1562%\n",
      "total_backward_count 1282490 real_backward_count 89935   7.013%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  0.009322/  0.039647, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0567%\n",
      "layer   2  Sparsity: 52.1338%\n",
      "layer   3  Sparsity: 43.1118%\n",
      "total_backward_count 1292280 real_backward_count 90266   6.985%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  0.009198/  0.036321, val:  87.92%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.77 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1309%\n",
      "layer   2  Sparsity: 52.2589%\n",
      "layer   3  Sparsity: 43.2645%\n",
      "total_backward_count 1302070 real_backward_count 90598   6.958%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  0.008922/  0.034694, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.84 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0731%\n",
      "layer   2  Sparsity: 52.4132%\n",
      "layer   3  Sparsity: 43.3194%\n",
      "total_backward_count 1311860 real_backward_count 90905   6.929%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  0.009303/  0.036863, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.01 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   2  Sparsity: 52.3219%\n",
      "layer   3  Sparsity: 43.2576%\n",
      "total_backward_count 1321650 real_backward_count 91234   6.903%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  0.009281/  0.034018, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.26 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   2  Sparsity: 52.3420%\n",
      "layer   3  Sparsity: 43.3227%\n",
      "total_backward_count 1331440 real_backward_count 91558   6.877%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  0.009083/  0.035545, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.64 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   2  Sparsity: 52.3791%\n",
      "layer   3  Sparsity: 43.3799%\n",
      "total_backward_count 1341230 real_backward_count 91899   6.852%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  0.008879/  0.034695, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.32 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0953%\n",
      "layer   2  Sparsity: 52.3951%\n",
      "layer   3  Sparsity: 43.4792%\n",
      "total_backward_count 1351020 real_backward_count 92202   6.825%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  0.008930/  0.035133, val:  87.50%, val_best:  90.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0489%\n",
      "layer   2  Sparsity: 52.3597%\n",
      "layer   3  Sparsity: 43.5512%\n",
      "total_backward_count 1360810 real_backward_count 92538   6.800%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  0.008831/  0.034872, val:  88.75%, val_best:  90.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.25 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   2  Sparsity: 52.3550%\n",
      "layer   3  Sparsity: 43.4814%\n",
      "total_backward_count 1370600 real_backward_count 92845   6.774%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  0.008907/  0.034624, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   2  Sparsity: 52.3174%\n",
      "layer   3  Sparsity: 43.3419%\n",
      "total_backward_count 1380390 real_backward_count 93147   6.748%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  0.008828/  0.034118, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.36 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0887%\n",
      "layer   2  Sparsity: 52.3522%\n",
      "layer   3  Sparsity: 43.2160%\n",
      "total_backward_count 1390180 real_backward_count 93451   6.722%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  0.008726/  0.035798, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.70 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1041%\n",
      "layer   2  Sparsity: 52.4192%\n",
      "layer   3  Sparsity: 43.1970%\n",
      "total_backward_count 1399970 real_backward_count 93737   6.696%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  0.008823/  0.033720, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.72 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0659%\n",
      "layer   2  Sparsity: 52.3533%\n",
      "layer   3  Sparsity: 43.2100%\n",
      "total_backward_count 1409760 real_backward_count 94060   6.672%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  0.008820/  0.034625, val:  88.33%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.50 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0798%\n",
      "layer   2  Sparsity: 52.3659%\n",
      "layer   3  Sparsity: 43.2516%\n",
      "total_backward_count 1419550 real_backward_count 94376   6.648%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  0.008509/  0.044538, val:  87.92%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.47 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1034%\n",
      "layer   2  Sparsity: 52.4251%\n",
      "layer   3  Sparsity: 43.4058%\n",
      "total_backward_count 1429340 real_backward_count 94656   6.622%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  0.008305/  0.034006, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.98 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   2  Sparsity: 52.3527%\n",
      "layer   3  Sparsity: 43.4453%\n",
      "total_backward_count 1439130 real_backward_count 94935   6.597%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  0.008781/  0.034542, val:  89.17%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.13 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0737%\n",
      "layer   2  Sparsity: 52.3724%\n",
      "layer   3  Sparsity: 43.2466%\n",
      "total_backward_count 1448920 real_backward_count 95252   6.574%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  0.008656/  0.033200, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.05 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   2  Sparsity: 52.4027%\n",
      "layer   3  Sparsity: 43.2575%\n",
      "total_backward_count 1458710 real_backward_count 95546   6.550%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  0.008948/  0.034259, val:  89.17%, val_best:  90.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.08 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1085%\n",
      "layer   2  Sparsity: 52.4737%\n",
      "layer   3  Sparsity: 43.3369%\n",
      "total_backward_count 1468500 real_backward_count 95865   6.528%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  0.008533/  0.035382, val:  87.50%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.71 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0539%\n",
      "layer   2  Sparsity: 52.4432%\n",
      "layer   3  Sparsity: 43.3305%\n",
      "total_backward_count 1478290 real_backward_count 96170   6.505%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  0.008757/  0.035643, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.01 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0599%\n",
      "layer   2  Sparsity: 52.4320%\n",
      "layer   3  Sparsity: 43.2823%\n",
      "total_backward_count 1488080 real_backward_count 96469   6.483%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  0.008357/  0.034809, val:  90.00%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.13 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0712%\n",
      "layer   2  Sparsity: 52.3708%\n",
      "layer   3  Sparsity: 43.3224%\n",
      "total_backward_count 1497870 real_backward_count 96738   6.458%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  0.008314/  0.033484, val:  89.58%, val_best:  90.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.87 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1104%\n",
      "layer   2  Sparsity: 52.4928%\n",
      "layer   3  Sparsity: 43.3469%\n",
      "total_backward_count 1507660 real_backward_count 97018   6.435%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  0.008652/  0.034636, val:  89.17%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.50 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   2  Sparsity: 52.4630%\n",
      "layer   3  Sparsity: 43.3636%\n",
      "total_backward_count 1517450 real_backward_count 97326   6.414%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  0.008578/  0.034173, val:  89.58%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0576%\n",
      "layer   2  Sparsity: 52.4642%\n",
      "layer   3  Sparsity: 43.3578%\n",
      "total_backward_count 1527240 real_backward_count 97629   6.393%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  0.008697/  0.038152, val:  89.58%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.76 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1148%\n",
      "layer   2  Sparsity: 52.5247%\n",
      "layer   3  Sparsity: 43.3932%\n",
      "total_backward_count 1537030 real_backward_count 97941   6.372%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  0.008632/  0.033733, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.70 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0662%\n",
      "layer   2  Sparsity: 52.4367%\n",
      "layer   3  Sparsity: 43.4213%\n",
      "total_backward_count 1546820 real_backward_count 98238   6.351%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  0.008474/  0.035397, val:  90.00%, val_best:  90.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.77 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0265%\n",
      "layer   2  Sparsity: 52.4810%\n",
      "layer   3  Sparsity: 43.4562%\n",
      "total_backward_count 1556610 real_backward_count 98547   6.331%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  0.007941/  0.034705, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.85 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   2  Sparsity: 52.5906%\n",
      "layer   3  Sparsity: 43.4116%\n",
      "total_backward_count 1566400 real_backward_count 98823   6.309%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  0.007897/  0.035805, val:  87.50%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.73 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0842%\n",
      "layer   2  Sparsity: 52.6123%\n",
      "layer   3  Sparsity: 43.4556%\n",
      "total_backward_count 1576190 real_backward_count 99064   6.285%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  0.008365/  0.043885, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.23 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0987%\n",
      "layer   2  Sparsity: 52.4746%\n",
      "layer   3  Sparsity: 43.4731%\n",
      "total_backward_count 1585980 real_backward_count 99366   6.265%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  0.008411/  0.034111, val:  90.42%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.78 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1225%\n",
      "layer   2  Sparsity: 52.5630%\n",
      "layer   3  Sparsity: 43.4967%\n",
      "total_backward_count 1595770 real_backward_count 99656   6.245%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  0.008335/  0.034433, val:  88.75%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.24 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 52.4985%\n",
      "layer   3  Sparsity: 43.5513%\n",
      "total_backward_count 1605560 real_backward_count 99943   6.225%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  0.008131/  0.034247, val:  90.00%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.10 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   2  Sparsity: 52.6044%\n",
      "layer   3  Sparsity: 43.5693%\n",
      "total_backward_count 1615350 real_backward_count 100231   6.205%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  0.008214/  0.036388, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.99 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1133%\n",
      "layer   2  Sparsity: 52.5558%\n",
      "layer   3  Sparsity: 43.4679%\n",
      "total_backward_count 1625140 real_backward_count 100529   6.186%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  0.007940/  0.035103, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.68 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0888%\n",
      "layer   2  Sparsity: 52.6283%\n",
      "layer   3  Sparsity: 43.4666%\n",
      "total_backward_count 1634930 real_backward_count 100806   6.166%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  0.008164/  0.040892, val:  87.92%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.26 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0441%\n",
      "layer   2  Sparsity: 52.6489%\n",
      "layer   3  Sparsity: 43.4524%\n",
      "total_backward_count 1644720 real_backward_count 101072   6.145%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  0.007846/  0.034255, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0389%\n",
      "layer   2  Sparsity: 52.5923%\n",
      "layer   3  Sparsity: 43.3795%\n",
      "total_backward_count 1654510 real_backward_count 101338   6.125%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  0.008361/  0.039083, val:  87.92%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.38 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   2  Sparsity: 52.5293%\n",
      "layer   3  Sparsity: 43.3614%\n",
      "total_backward_count 1664300 real_backward_count 101638   6.107%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  0.008033/  0.034933, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0960%\n",
      "layer   2  Sparsity: 52.4601%\n",
      "layer   3  Sparsity: 43.3501%\n",
      "total_backward_count 1674090 real_backward_count 101911   6.088%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  0.007838/  0.036230, val:  87.50%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.49 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0630%\n",
      "layer   2  Sparsity: 52.4503%\n",
      "layer   3  Sparsity: 43.4667%\n",
      "total_backward_count 1683880 real_backward_count 102166   6.067%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  0.007616/  0.038059, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.55 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   2  Sparsity: 52.5185%\n",
      "layer   3  Sparsity: 43.5851%\n",
      "total_backward_count 1693670 real_backward_count 102424   6.047%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  0.008070/  0.034125, val:  90.83%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.04 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1019%\n",
      "layer   2  Sparsity: 52.6293%\n",
      "layer   3  Sparsity: 43.6110%\n",
      "total_backward_count 1703460 real_backward_count 102708   6.029%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  0.008070/  0.035241, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   2  Sparsity: 52.6575%\n",
      "layer   3  Sparsity: 43.6151%\n",
      "total_backward_count 1713250 real_backward_count 102973   6.010%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  0.007698/  0.042299, val:  88.33%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.22 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1194%\n",
      "layer   2  Sparsity: 52.6781%\n",
      "layer   3  Sparsity: 43.6009%\n",
      "total_backward_count 1723040 real_backward_count 103239   5.992%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  0.007567/  0.035488, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.94 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0466%\n",
      "layer   2  Sparsity: 52.5248%\n",
      "layer   3  Sparsity: 43.5927%\n",
      "total_backward_count 1732830 real_backward_count 103499   5.973%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  0.008221/  0.034668, val:  89.17%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.50 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   2  Sparsity: 52.4780%\n",
      "layer   3  Sparsity: 43.6472%\n",
      "total_backward_count 1742620 real_backward_count 103787   5.956%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  0.007842/  0.036805, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0671%\n",
      "layer   2  Sparsity: 52.6281%\n",
      "layer   3  Sparsity: 43.6365%\n",
      "total_backward_count 1752410 real_backward_count 104035   5.937%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  0.007798/  0.035313, val:  90.42%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   2  Sparsity: 52.6066%\n",
      "layer   3  Sparsity: 43.7107%\n",
      "total_backward_count 1762200 real_backward_count 104293   5.918%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  0.007296/  0.038192, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.86 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   2  Sparsity: 52.6258%\n",
      "layer   3  Sparsity: 43.6731%\n",
      "total_backward_count 1771990 real_backward_count 104517   5.898%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  0.007579/  0.034154, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.69 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0428%\n",
      "layer   2  Sparsity: 52.6326%\n",
      "layer   3  Sparsity: 43.6178%\n",
      "total_backward_count 1781780 real_backward_count 104767   5.880%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  0.007772/  0.036709, val:  87.08%, val_best:  90.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.76 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0923%\n",
      "layer   2  Sparsity: 52.5825%\n",
      "layer   3  Sparsity: 43.5843%\n",
      "total_backward_count 1791570 real_backward_count 105028   5.862%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  0.007538/  0.037759, val:  88.33%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.08 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0982%\n",
      "layer   2  Sparsity: 52.6646%\n",
      "layer   3  Sparsity: 43.6314%\n",
      "total_backward_count 1801360 real_backward_count 105287   5.845%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  0.006941/  0.038675, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.77 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0898%\n",
      "layer   2  Sparsity: 52.7494%\n",
      "layer   3  Sparsity: 43.6434%\n",
      "total_backward_count 1811150 real_backward_count 105513   5.826%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  0.007543/  0.035162, val:  89.58%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.69 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0409%\n",
      "layer   2  Sparsity: 52.6437%\n",
      "layer   3  Sparsity: 43.6682%\n",
      "total_backward_count 1820940 real_backward_count 105771   5.809%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  0.007289/  0.035123, val:  89.17%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.23 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0779%\n",
      "layer   2  Sparsity: 52.6426%\n",
      "layer   3  Sparsity: 43.8162%\n",
      "total_backward_count 1830730 real_backward_count 106023   5.791%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  0.007295/  0.035564, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.90 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0521%\n",
      "layer   2  Sparsity: 52.5430%\n",
      "layer   3  Sparsity: 43.8205%\n",
      "total_backward_count 1840520 real_backward_count 106273   5.774%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  0.007239/  0.034562, val:  89.58%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.46 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0824%\n",
      "layer   2  Sparsity: 52.6079%\n",
      "layer   3  Sparsity: 43.8601%\n",
      "total_backward_count 1850310 real_backward_count 106504   5.756%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  0.007252/  0.034555, val:  90.83%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.81 seconds, 1.06 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   2  Sparsity: 52.6241%\n",
      "layer   3  Sparsity: 43.9643%\n",
      "total_backward_count 1860100 real_backward_count 106740   5.738%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  0.007781/  0.034681, val:  88.75%, val_best:  90.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.93 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   2  Sparsity: 52.6965%\n",
      "layer   3  Sparsity: 44.0795%\n",
      "total_backward_count 1869890 real_backward_count 107019   5.723%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  0.006988/  0.036391, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.30 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   2  Sparsity: 52.6772%\n",
      "layer   3  Sparsity: 43.9419%\n",
      "total_backward_count 1879680 real_backward_count 107229   5.705%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  0.007594/  0.033937, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.54 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0963%\n",
      "layer   2  Sparsity: 52.7074%\n",
      "layer   3  Sparsity: 43.9079%\n",
      "total_backward_count 1889470 real_backward_count 107486   5.689%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  0.007169/  0.034381, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.07 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1033%\n",
      "layer   2  Sparsity: 52.6253%\n",
      "layer   3  Sparsity: 43.8272%\n",
      "total_backward_count 1899260 real_backward_count 107728   5.672%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  0.007308/  0.035494, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.37 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 52.5842%\n",
      "layer   3  Sparsity: 43.7116%\n",
      "total_backward_count 1909050 real_backward_count 107968   5.656%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  0.007414/  0.036769, val:  87.50%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.08 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   2  Sparsity: 52.7162%\n",
      "layer   3  Sparsity: 43.6781%\n",
      "total_backward_count 1918840 real_backward_count 108218   5.640%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  0.007185/  0.037102, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.01 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0506%\n",
      "layer   2  Sparsity: 52.6462%\n",
      "layer   3  Sparsity: 43.8138%\n",
      "total_backward_count 1928630 real_backward_count 108448   5.623%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  0.007089/  0.038038, val:  89.17%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.96 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0925%\n",
      "layer   2  Sparsity: 52.7362%\n",
      "layer   3  Sparsity: 43.7333%\n",
      "total_backward_count 1938420 real_backward_count 108664   5.606%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  0.007524/  0.039188, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.76 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0983%\n",
      "layer   2  Sparsity: 52.6418%\n",
      "layer   3  Sparsity: 43.6617%\n",
      "total_backward_count 1948210 real_backward_count 108924   5.591%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  0.007159/  0.034811, val:  89.58%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.91 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0599%\n",
      "layer   2  Sparsity: 52.6795%\n",
      "layer   3  Sparsity: 43.8194%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b3082bf27a484fa2c45426e18f0b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñá‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>0.00716</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.03481</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dashing-sweep-2</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ew07sr9s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ew07sr9s</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251119_224424-ew07sr9s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ttur70n8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 14657\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251120_023126-ttur70n8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ttur70n8' target=\"_blank\">jolly-sweep-3</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ttur70n8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ttur70n8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251120_023135_594', 'my_seed': 14657, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.03125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[999, 998], [999, 999], [999, 999]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 0\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 0\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.03125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.03125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([-0.1118,  0.0422,  0.0336, -0.0984,  0.0352, -0.0116, -0.0096,  0.1186,\n",
      "         0.1268, -0.0078,  0.1015, -0.0744,  0.0615,  0.0675, -0.1690, -0.0069,\n",
      "        -0.0306, -0.1379, -0.0902, -0.0425,  0.0426, -0.0580, -0.1266,  0.0713,\n",
      "         0.0237, -0.0288, -0.0862, -0.0800, -0.0954, -0.1235,  0.0055, -0.0568,\n",
      "        -0.1074,  0.1075,  0.0656,  0.0021, -0.1022,  0.1356,  0.0441, -0.0600,\n",
      "        -0.0095,  0.0816,  0.0423,  0.0873, -0.1745, -0.0271, -0.0085, -0.1515,\n",
      "         0.0043, -0.1095,  0.0594, -0.0097, -0.0510,  0.0515,  0.0006,  0.1335,\n",
      "        -0.0389,  0.0027,  0.0544,  0.0442,  0.0334,  0.0200,  0.0085, -0.0318,\n",
      "         0.0311, -0.0011, -0.1459, -0.0134,  0.0393, -0.1000, -0.0379,  0.2372,\n",
      "        -0.0249, -0.0873, -0.0314, -0.1977, -0.0997,  0.2174,  0.0014, -0.1508,\n",
      "         0.0394, -0.0717, -0.0234, -0.0760, -0.0880, -0.0996,  0.0820, -0.1253,\n",
      "         0.1736,  0.0521,  0.0227,  0.0805, -0.1393, -0.0117, -0.0180, -0.0449,\n",
      "         0.0613, -0.1442, -0.1303, -0.1706, -0.1862, -0.0506, -0.0180,  0.0415,\n",
      "         0.0099,  0.1061,  0.1278,  0.0440, -0.1402, -0.0337,  0.0685,  0.0562,\n",
      "        -0.0703, -0.0073, -0.0573, -0.1040,  0.0661, -0.0368,  0.0439,  0.1200,\n",
      "        -0.0220,  0.0988, -0.0708,  0.0552,  0.0588, -0.0088,  0.0218, -0.3063,\n",
      "        -0.0056,  0.1087, -0.1425,  0.2913,  0.1147, -0.1048, -0.0261, -0.0290,\n",
      "        -0.2097, -0.0792,  0.0714, -0.0572,  0.1139, -0.0088, -0.1089,  0.1686,\n",
      "         0.0505, -0.0806,  0.0223,  0.0667, -0.2070, -0.0552, -0.0643, -0.2859,\n",
      "        -0.0262,  0.0218,  0.1585, -0.0735,  0.2073,  0.1029,  0.0099, -0.0060,\n",
      "         0.0467,  0.0954,  0.0311,  0.0176,  0.0728,  0.0278,  0.1219,  0.1712,\n",
      "         0.0423, -0.0263,  0.1682, -0.1023,  0.0137,  0.0398,  0.0641, -0.0166,\n",
      "         0.0491, -0.0945,  0.0959,  0.1976,  0.0963, -0.1750, -0.0004, -0.0307,\n",
      "         0.0128, -0.0067, -0.0090,  0.0066,  0.0297,  0.0293,  0.0250, -0.0050,\n",
      "        -0.1984, -0.0815,  0.1524,  0.0993,  0.0259, -0.0966,  0.1055,  0.0803],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([-5.8867e-02, -3.6707e-03,  1.4914e-01,  2.1937e-02,  1.1177e-01,\n",
      "        -1.4519e-02,  7.3808e-02,  4.2794e-02,  8.8677e-02, -1.2586e-01,\n",
      "         1.6123e-01, -1.2451e-01, -1.0034e-01,  7.3757e-02,  4.9361e-02,\n",
      "         7.1631e-02, -4.7939e-02,  1.0456e-01, -2.9016e-02, -3.6375e-02,\n",
      "        -3.9654e-02, -8.9135e-02,  1.1339e-01,  7.4280e-02, -8.8677e-02,\n",
      "        -8.3849e-02, -1.3440e-02,  3.0631e-04, -4.0044e-02, -5.5868e-02,\n",
      "         4.8886e-02, -5.2036e-02,  1.0692e-01, -1.8124e-01, -1.2096e-01,\n",
      "        -1.7999e-01,  3.6817e-02,  1.0716e-01,  1.3215e-01, -8.4488e-02,\n",
      "         6.4237e-02, -1.0559e-01,  7.6168e-02, -1.6611e-03,  4.5854e-03,\n",
      "        -4.2490e-02, -6.3474e-02,  2.6723e-02,  1.2730e-01, -6.4498e-03,\n",
      "         1.4964e-01, -6.7252e-03,  1.5035e-02, -9.0783e-02,  2.0771e-01,\n",
      "         4.0275e-04, -4.2138e-02, -1.5409e-03,  8.6582e-02,  1.7207e-01,\n",
      "        -4.3902e-04, -2.3306e-02,  6.0503e-02,  1.5243e-02,  1.9143e-01,\n",
      "        -5.5517e-02, -6.1882e-02, -2.5288e-03,  3.7771e-02, -6.2004e-02,\n",
      "         5.5867e-02,  1.0010e-02, -3.5165e-02,  2.1146e-02,  1.6914e-01,\n",
      "         6.7519e-03,  7.7714e-02, -1.5684e-01, -1.1767e-01,  1.9664e-01,\n",
      "         5.7790e-02, -1.8264e-02, -1.0597e-01,  3.9178e-02,  1.6562e-02,\n",
      "         7.1985e-03,  3.3440e-02, -1.2200e-01,  1.0304e-01, -6.0160e-02,\n",
      "        -1.0041e-01,  1.8902e-01, -2.3278e-02,  1.2742e-01, -1.3105e-01,\n",
      "        -6.4388e-03, -3.6908e-02, -5.3358e-02,  5.0766e-02, -2.3724e-02,\n",
      "         5.6638e-02,  1.4239e-01, -5.2259e-02, -1.1258e-01,  1.8034e-01,\n",
      "        -1.1541e-01,  3.4533e-02,  1.3185e-02, -1.3839e-01, -5.0722e-02,\n",
      "        -1.3291e-02,  1.1266e-01,  4.9064e-02, -2.2888e-02,  3.7917e-02,\n",
      "         1.3454e-01, -5.2454e-02, -6.2050e-02,  5.3764e-02, -8.7741e-02,\n",
      "        -4.9418e-02, -7.2796e-02, -5.4945e-02, -3.6434e-02, -5.6760e-02,\n",
      "        -1.0506e-01,  1.1068e-01,  1.3669e-02, -1.4543e-01,  2.1287e-02,\n",
      "        -8.4075e-03, -2.3012e-02,  2.8241e-03,  3.2396e-03, -1.6222e-02,\n",
      "         1.6798e-02, -4.0306e-02,  1.1468e-01, -8.2010e-02, -7.2892e-02,\n",
      "        -5.5962e-02,  6.3650e-02, -7.0722e-05,  5.5536e-02,  1.4069e-02,\n",
      "        -4.6144e-02, -4.4674e-02,  5.8033e-02, -5.9680e-02, -1.8741e-01,\n",
      "        -1.7250e-01, -2.7425e-02, -2.3641e-02, -2.5306e-02, -6.6773e-02,\n",
      "         5.9427e-02,  5.5931e-02,  4.4073e-02,  7.3509e-02,  2.3400e-02,\n",
      "        -6.7210e-02, -1.0528e-01,  1.1966e-01,  2.3479e-02,  6.7064e-02,\n",
      "        -8.8723e-02,  6.1089e-02, -3.2113e-02,  2.0837e-01,  1.3105e-01,\n",
      "        -1.7183e-03,  6.0656e-02,  1.1902e-01,  1.2572e-01,  1.1584e-01,\n",
      "         5.8565e-03,  5.7185e-02, -1.3974e-02, -6.0951e-03, -1.0710e-02,\n",
      "        -1.0335e-01,  2.5997e-02,  6.8459e-02,  1.2233e-01,  6.4543e-02,\n",
      "        -9.0761e-02,  1.2059e-01,  1.3123e-01, -1.2142e-01, -7.6961e-02,\n",
      "        -2.1068e-01,  1.5627e-01, -1.8459e-01,  1.8240e-01, -1.0813e-01,\n",
      "        -8.3313e-02,  6.6647e-03, -8.6888e-02, -2.0332e-01, -5.9372e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 0.0933, -0.0409,  0.1174,  0.0482,  0.1324,  0.1052,  0.0263,  0.0494,\n",
      "        -0.1413,  0.0344,  0.0472,  0.0834, -0.1209, -0.1832,  0.1283,  0.0360,\n",
      "        -0.1290, -0.0292,  0.0990,  0.0791, -0.1754, -0.0828, -0.0148, -0.1322,\n",
      "        -0.1114,  0.0007,  0.1490,  0.1261, -0.0015, -0.0584, -0.1883, -0.0847,\n",
      "         0.0164, -0.0043, -0.1382,  0.0781,  0.1328,  0.0393, -0.1470,  0.1462,\n",
      "        -0.0133, -0.1261,  0.0736, -0.0519, -0.0373, -0.0635, -0.0407,  0.0564,\n",
      "        -0.0229, -0.1444,  0.1430,  0.0184,  0.1250, -0.0662,  0.0471,  0.0975,\n",
      "         0.0213, -0.0914,  0.0229,  0.0728,  0.0488, -0.0690,  0.0127,  0.0718,\n",
      "        -0.1303,  0.0008,  0.1941, -0.0655,  0.1407,  0.0248, -0.1018, -0.0608,\n",
      "         0.0395, -0.0050, -0.1592, -0.1515, -0.0039,  0.0989,  0.0106, -0.0457,\n",
      "        -0.0036, -0.0693, -0.0392,  0.0790, -0.1206,  0.0068,  0.0250,  0.0240,\n",
      "         0.1075, -0.0268, -0.1633,  0.0662,  0.0549, -0.0223, -0.0460,  0.0739,\n",
      "         0.0452,  0.0129, -0.0384, -0.0394,  0.0605,  0.1130,  0.1128,  0.1160,\n",
      "        -0.0705,  0.0556, -0.0814,  0.1006,  0.0975,  0.0648,  0.1118, -0.0770,\n",
      "         0.1644, -0.0439, -0.0450, -0.0528,  0.0031,  0.0348, -0.1238, -0.0334,\n",
      "         0.0418, -0.0143,  0.1647, -0.0021, -0.1076,  0.1832, -0.0310, -0.1031,\n",
      "        -0.1120, -0.1739,  0.0769,  0.1347, -0.1963, -0.0792,  0.1163, -0.0431,\n",
      "        -0.0225,  0.1010,  0.0523,  0.0224, -0.1918,  0.0557, -0.0656,  0.0667,\n",
      "        -0.1734,  0.0085, -0.0118, -0.0826,  0.0734,  0.0422,  0.1030,  0.1238,\n",
      "         0.0832,  0.0097,  0.0516, -0.0406, -0.0622, -0.0466,  0.1959, -0.1284,\n",
      "        -0.0312, -0.2251, -0.0018, -0.0463,  0.0125, -0.1894,  0.0142, -0.0309,\n",
      "        -0.1071, -0.1581,  0.0207,  0.0328, -0.1095, -0.0744,  0.0554, -0.0183,\n",
      "         0.2383, -0.0589, -0.0161,  0.1155,  0.1899, -0.0160,  0.0037, -0.0418,\n",
      "         0.0625,  0.0119,  0.0048,  0.0967,  0.0079, -0.0227, -0.0247,  0.0112,\n",
      "        -0.0729, -0.0069, -0.1777, -0.0752,  0.0696,  0.0019, -0.0489,  0.0392],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([ 0.0092, -0.0420,  0.1410, -0.0103, -0.1598, -0.0071, -0.1507,  0.1372,\n",
      "        -0.1533, -0.0897, -0.2143,  0.1065,  0.1654, -0.0576, -0.0745, -0.0033,\n",
      "         0.0968,  0.0257,  0.0659,  0.0264,  0.1513, -0.1969, -0.2036, -0.0105,\n",
      "        -0.1346, -0.0361,  0.0497,  0.2995, -0.0156, -0.1818, -0.0414, -0.1425,\n",
      "         0.0841, -0.0585, -0.0161, -0.1564,  0.0725, -0.0190, -0.0506, -0.2404,\n",
      "        -0.0547, -0.0267, -0.0125,  0.0189, -0.0299,  0.0769,  0.1814,  0.1664,\n",
      "         0.0855, -0.0073, -0.1956,  0.0780, -0.0253,  0.0365,  0.0891, -0.0159,\n",
      "         0.1020, -0.1259,  0.0260,  0.0499,  0.0974,  0.0226,  0.1634,  0.1314,\n",
      "         0.0078, -0.1180,  0.2002,  0.1416,  0.0116,  0.0819,  0.0442,  0.0330,\n",
      "        -0.0861,  0.0354,  0.0324,  0.0779,  0.0004,  0.0606,  0.0310, -0.1118,\n",
      "         0.1234,  0.0963,  0.1193, -0.1690, -0.0450,  0.0060, -0.0307, -0.0005,\n",
      "         0.0575, -0.0703,  0.1118,  0.0076, -0.0131, -0.0687, -0.0764, -0.0495,\n",
      "         0.1051, -0.0338, -0.2178,  0.0399,  0.0842, -0.0123, -0.1331,  0.1030,\n",
      "         0.0182,  0.0699, -0.1249,  0.0978, -0.0531, -0.0914, -0.1165, -0.0342,\n",
      "         0.0833,  0.1415, -0.1042,  0.1383, -0.2250, -0.0332,  0.1139,  0.0608,\n",
      "        -0.0190,  0.1523,  0.1131,  0.0158,  0.0212,  0.0940, -0.0747,  0.0173,\n",
      "         0.2074, -0.0134, -0.0612, -0.1161,  0.0116,  0.0919, -0.0064, -0.1439,\n",
      "        -0.1047, -0.0342,  0.0773, -0.0115,  0.0033, -0.0346, -0.1227,  0.1631,\n",
      "        -0.0417, -0.1082,  0.0523, -0.0661, -0.0322, -0.1006,  0.0485,  0.0114,\n",
      "         0.0297,  0.0377,  0.0767, -0.0161, -0.0288, -0.0952,  0.0135, -0.0196,\n",
      "         0.0279, -0.0530,  0.1007,  0.1104,  0.0127, -0.0091,  0.0117, -0.0863,\n",
      "         0.0760, -0.0900, -0.0861,  0.0121, -0.1695, -0.0199, -0.1260,  0.2071,\n",
      "        -0.1190, -0.0776, -0.1320,  0.1649, -0.0206, -0.0666, -0.0267,  0.1745,\n",
      "         0.0811,  0.0070, -0.0484, -0.1253,  0.0914, -0.0244, -0.1263, -0.0511,\n",
      "         0.0218, -0.0256,  0.0057,  0.0101, -0.0532, -0.1269, -0.1970, -0.0699],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-0.0579, -0.0203, -0.0443, -0.0975,  0.1093,  0.0878, -0.0474,  0.1214,\n",
      "        -0.0194, -0.0725, -0.0768, -0.0287,  0.0902, -0.0117, -0.0438,  0.0100,\n",
      "        -0.0935,  0.0033,  0.2088,  0.0517, -0.0505,  0.0463, -0.0620, -0.0549,\n",
      "        -0.1044, -0.1021,  0.1269,  0.1060,  0.2078,  0.0835,  0.0179,  0.1439,\n",
      "        -0.1320,  0.0560,  0.1230, -0.1192,  0.0601, -0.1040,  0.1215,  0.0341,\n",
      "         0.0658, -0.0522, -0.1678, -0.0348,  0.0843, -0.0673, -0.0850,  0.0723,\n",
      "        -0.0176,  0.0222,  0.0083, -0.1615, -0.0339, -0.0881, -0.0607,  0.1018,\n",
      "         0.0043,  0.0606, -0.0848,  0.1314,  0.1060, -0.0520, -0.0610,  0.0193,\n",
      "        -0.1436, -0.1335, -0.0406, -0.1257,  0.0816,  0.1048, -0.0167,  0.0545,\n",
      "         0.0236,  0.1462,  0.0117,  0.1172,  0.0973,  0.0870, -0.2160, -0.0072,\n",
      "         0.1105,  0.0407,  0.1629,  0.0533, -0.1423, -0.0008, -0.1222,  0.1629,\n",
      "        -0.1592,  0.0181,  0.0352, -0.0485, -0.1674,  0.0399, -0.0247,  0.0111,\n",
      "        -0.0640,  0.1025,  0.1138,  0.1201,  0.0894,  0.0543,  0.0517, -0.0605,\n",
      "        -0.0048,  0.0425, -0.0009, -0.1264,  0.0742, -0.0324,  0.0746,  0.0446,\n",
      "         0.0243,  0.0140,  0.0905,  0.1299, -0.1197,  0.0240, -0.0775,  0.0983,\n",
      "         0.1256, -0.1994, -0.0191,  0.0507,  0.0138, -0.1406,  0.1546, -0.1076,\n",
      "        -0.0755, -0.0560,  0.1632, -0.0322, -0.0591,  0.0172,  0.0672,  0.0712,\n",
      "         0.0030,  0.1373,  0.0052, -0.0538, -0.0084,  0.0186,  0.0553,  0.0745,\n",
      "         0.0798,  0.0893,  0.1301, -0.1852,  0.0990,  0.0576, -0.1603,  0.1194,\n",
      "        -0.0475,  0.1751, -0.0871, -0.1466, -0.0834, -0.0047, -0.0577,  0.1098,\n",
      "         0.0865, -0.0029, -0.0797,  0.0328,  0.1362,  0.1287,  0.0030, -0.1383,\n",
      "        -0.2245,  0.0720, -0.1822, -0.0238, -0.0186,  0.0248, -0.0711, -0.0580,\n",
      "        -0.0332,  0.0724, -0.0149,  0.1311, -0.0994, -0.0060,  0.2587, -0.0859,\n",
      "        -0.0564,  0.0327, -0.1472, -0.0440, -0.1047,  0.0630, -0.0199,  0.0416,\n",
      "        -0.0089, -0.1193, -0.0506,  0.0563, -0.1837,  0.0678, -0.1919, -0.1202],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([-0.0626, -0.1552, -0.0506,  0.1908, -0.0245,  0.0132,  0.1080, -0.1284,\n",
      "         0.1665, -0.1799,  0.0255, -0.1850, -0.0096, -0.1459, -0.0274,  0.0177,\n",
      "         0.1207,  0.0227,  0.0815,  0.1492,  0.2186,  0.1452, -0.0212,  0.0197,\n",
      "         0.0277,  0.0579,  0.1152, -0.0051,  0.1668,  0.0084,  0.0711, -0.0901,\n",
      "         0.0355,  0.0714, -0.1220,  0.0141,  0.0488,  0.0902, -0.1255, -0.0570,\n",
      "         0.1456,  0.0463, -0.2319,  0.0187, -0.0528, -0.1350, -0.0453,  0.0360,\n",
      "         0.1516,  0.1801, -0.0071,  0.0444, -0.2138, -0.0829, -0.0337, -0.0260,\n",
      "        -0.1153, -0.1377,  0.0194, -0.0560, -0.2774,  0.1209, -0.0544, -0.1248,\n",
      "        -0.0217, -0.0336, -0.0973, -0.0547, -0.0478,  0.0254, -0.1652,  0.0676,\n",
      "        -0.0485,  0.1246, -0.0635,  0.1137, -0.0581, -0.0346, -0.0249,  0.0966,\n",
      "         0.0276, -0.1148, -0.1944,  0.1015, -0.0843, -0.1538,  0.1077,  0.0041,\n",
      "         0.2127, -0.0047, -0.0140,  0.0117, -0.0691, -0.0378,  0.0091,  0.1802,\n",
      "         0.0928,  0.0423,  0.0509,  0.1888,  0.2759, -0.1414,  0.0435, -0.0488,\n",
      "        -0.1526, -0.1398, -0.0460,  0.0533,  0.0109, -0.1093,  0.0010, -0.0853,\n",
      "        -0.0150, -0.1375,  0.0124, -0.0241, -0.2042, -0.0884,  0.1237, -0.1413,\n",
      "         0.1184, -0.0267, -0.0669, -0.0051, -0.0239,  0.0053, -0.1812, -0.0059,\n",
      "         0.0055, -0.1042, -0.0003, -0.0373, -0.1168,  0.0904,  0.0864, -0.0640,\n",
      "         0.1127,  0.0478,  0.0834,  0.1048, -0.1677,  0.0560, -0.0225, -0.0960,\n",
      "         0.0936, -0.0473, -0.2031, -0.0026,  0.0692, -0.0313, -0.0965,  0.0585,\n",
      "         0.0934,  0.0023,  0.0617, -0.0343,  0.0662,  0.0095, -0.0688,  0.0800,\n",
      "        -0.1551, -0.0485,  0.1370, -0.0498,  0.0173,  0.1051, -0.0540,  0.0818,\n",
      "         0.0478,  0.0731,  0.0049, -0.0024, -0.1336,  0.0470,  0.0786, -0.1133,\n",
      "        -0.0911, -0.0644,  0.0154,  0.0108, -0.0688, -0.2193,  0.1195,  0.1256,\n",
      "         0.0368,  0.0667,  0.0071, -0.0670,  0.1093,  0.1435, -0.0475, -0.1615,\n",
      "         0.1434,  0.0431,  0.2106, -0.0084, -0.0465, -0.1067,  0.1433,  0.1690],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-1.6495e-01, -6.2161e-02, -1.8283e-02, -8.9612e-03,  4.3735e-02,\n",
      "        -9.2987e-02, -4.4664e-02, -1.0507e-01,  1.7556e-02,  9.4890e-02,\n",
      "        -1.8903e-01,  9.4864e-02,  1.8131e-01,  1.1093e-01, -1.1313e-02,\n",
      "        -1.3356e-01,  1.4273e-01, -2.2980e-01, -4.6004e-02,  9.5294e-02,\n",
      "        -1.6206e-01, -8.9529e-03,  2.3178e-02, -1.6303e-01,  3.2820e-02,\n",
      "         8.7472e-02, -2.1390e-02, -8.8763e-02, -2.5327e-02,  1.2498e-01,\n",
      "         1.2516e-01,  4.0762e-02,  9.6918e-02,  1.0763e-01,  2.6559e-02,\n",
      "         1.4759e-02,  9.3474e-02, -3.1746e-03,  1.1840e-01, -8.2685e-02,\n",
      "        -1.3998e-02, -6.0082e-02,  4.7136e-02,  4.8419e-02, -1.6484e-01,\n",
      "         8.9695e-02,  5.6290e-02, -9.2748e-03, -7.6551e-02, -1.3430e-01,\n",
      "         9.5432e-02, -7.6175e-02, -1.5690e-01, -5.7401e-02, -8.4334e-02,\n",
      "         1.7134e-01, -5.2737e-02, -2.9332e-02,  1.9349e-03,  5.7444e-02,\n",
      "         9.7725e-02, -1.2127e-02,  3.5444e-02, -5.4913e-02,  1.7519e-01,\n",
      "        -3.3296e-02, -7.9227e-02,  4.9992e-03, -4.7836e-04, -5.0787e-02,\n",
      "         1.4171e-02, -6.2294e-02, -7.3520e-02,  5.8178e-02,  2.1409e-02,\n",
      "         1.6214e-01, -1.7035e-01, -3.8012e-02, -4.6107e-02,  3.3243e-02,\n",
      "         5.5265e-02,  5.4565e-02, -1.7329e-01,  6.3672e-02, -1.1738e-01,\n",
      "        -1.3837e-01,  1.1850e-01,  8.8460e-03,  3.0173e-02, -1.2883e-02,\n",
      "        -3.0468e-02, -8.1441e-02,  4.0421e-02,  1.7423e-01, -7.9661e-02,\n",
      "        -9.5936e-02,  5.6159e-03, -1.5126e-01, -6.9508e-03, -1.6069e-01,\n",
      "         1.1731e-01, -1.6265e-02,  1.7659e-01,  1.0009e-01, -6.2480e-02,\n",
      "        -1.2555e-01, -6.5225e-02, -5.4347e-02,  1.1263e-02,  1.3460e-01,\n",
      "        -9.0898e-02,  1.6374e-02,  5.0982e-02,  1.0662e-01,  2.0244e-01,\n",
      "         1.5653e-01,  1.0870e-01, -1.2834e-01,  6.0832e-02, -8.7726e-02,\n",
      "         1.4269e-01, -2.7943e-02, -1.3514e-01, -1.0345e-01, -2.6856e-02,\n",
      "         7.1597e-02, -7.7560e-02, -1.3609e-02, -8.1024e-02, -1.0863e-01,\n",
      "        -1.4472e-01,  1.5852e-01,  3.7548e-02, -9.7922e-02,  1.5102e-01,\n",
      "        -6.8388e-02,  2.7711e-02, -5.3026e-02,  6.5470e-02,  9.5738e-02,\n",
      "        -3.0181e-02, -7.5236e-02, -4.5027e-02,  1.8105e-02,  1.7878e-02,\n",
      "         5.2165e-02,  9.8734e-02,  2.1298e-02, -1.1793e-01,  5.6085e-02,\n",
      "        -2.2800e-03,  2.3246e-01, -2.6633e-01, -1.1077e-01, -1.1381e-01,\n",
      "         3.7217e-02,  3.3814e-03,  3.4377e-02, -9.3873e-02, -1.2114e-01,\n",
      "         5.2203e-02, -2.1358e-01,  2.0343e-01, -1.5346e-01,  1.1698e-01,\n",
      "        -1.5633e-01,  1.1069e-01, -8.6262e-03, -5.1559e-02, -1.5467e-01,\n",
      "        -9.3792e-02,  2.1786e-01,  8.6515e-02,  1.9106e-02,  4.1549e-02,\n",
      "         6.1220e-02, -3.7655e-02,  2.6949e-06,  9.5721e-03,  1.4210e-02,\n",
      "        -1.9331e-02, -4.9282e-03, -6.7432e-02,  8.4911e-02,  2.7236e-02,\n",
      "        -3.1847e-02, -1.3150e-01,  1.0162e-01, -1.3402e-01, -1.3128e-01,\n",
      "         1.1159e-01, -6.6660e-03,  4.6864e-02,  1.1989e-01, -2.7070e-02,\n",
      "        -1.0520e-01,  1.4910e-01,  1.3600e-01, -1.6556e-01,  1.1532e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([-0.0092, -0.0352, -0.0445, -0.0438, -0.1055, -0.0505,  0.0533,  0.0140,\n",
      "         0.0922,  0.0736,  0.1091, -0.1516, -0.0507, -0.0763, -0.0852, -0.1231,\n",
      "         0.0960, -0.0514, -0.1397,  0.0009,  0.1884, -0.1216, -0.0837,  0.0191,\n",
      "        -0.0055,  0.1373, -0.1053, -0.0836,  0.0429, -0.0250, -0.0076, -0.0346,\n",
      "        -0.1427,  0.0943, -0.0907,  0.0357, -0.0062, -0.0635, -0.2086,  0.0344,\n",
      "        -0.1222,  0.1374, -0.2879,  0.0936,  0.1344,  0.1145,  0.0884,  0.1551,\n",
      "         0.0626, -0.0293, -0.0711, -0.0902, -0.1592, -0.0452, -0.0885, -0.0741,\n",
      "        -0.0522, -0.0143, -0.0257, -0.1187, -0.1783,  0.0555,  0.1531, -0.1472,\n",
      "        -0.2936,  0.1571, -0.0806,  0.0035,  0.0005,  0.1337, -0.0321,  0.0387,\n",
      "        -0.1745,  0.0495,  0.0173, -0.0108, -0.0284,  0.0746, -0.0539,  0.0026,\n",
      "         0.0082, -0.1775, -0.0341,  0.1747,  0.0377,  0.0163, -0.1046, -0.1423,\n",
      "         0.0468, -0.0530,  0.0839, -0.0452,  0.0316, -0.1332,  0.0600, -0.0110,\n",
      "         0.1097,  0.2050, -0.0790, -0.0168,  0.0440, -0.0149, -0.0485,  0.0556,\n",
      "         0.1537, -0.0608, -0.1723, -0.0494, -0.0374,  0.0163,  0.0749,  0.1003,\n",
      "         0.1170,  0.0893, -0.1721,  0.0715, -0.0493, -0.0375, -0.1694, -0.3280,\n",
      "         0.0073, -0.1516,  0.0801, -0.0127, -0.0649,  0.0991,  0.0400, -0.0602,\n",
      "        -0.0472,  0.0313, -0.1423, -0.0419,  0.0430, -0.0747,  0.1990,  0.0341,\n",
      "         0.0716,  0.0811,  0.0573, -0.1071,  0.1013, -0.1917,  0.1015, -0.1037,\n",
      "         0.0887, -0.1209, -0.0564,  0.0556,  0.0006, -0.0220,  0.2561,  0.0057,\n",
      "        -0.0858, -0.0863, -0.0751, -0.1237, -0.0566, -0.0737,  0.0089, -0.1633,\n",
      "        -0.0582, -0.0409, -0.0540,  0.0063, -0.0429, -0.2352,  0.1052, -0.1744,\n",
      "         0.1925,  0.0513, -0.0322, -0.0166,  0.0811, -0.1280, -0.0816, -0.1880,\n",
      "        -0.0850, -0.2162,  0.0840, -0.1938, -0.0520, -0.0240,  0.0635, -0.0698,\n",
      "        -0.2011, -0.1322, -0.0526,  0.0874, -0.0388,  0.1796, -0.1972,  0.1057,\n",
      "         0.0272, -0.0045, -0.1702, -0.0542,  0.1539, -0.1507,  0.0160,  0.2153],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 0.0780, -0.0839,  0.0124, -0.0327, -0.0529,  0.0486, -0.1424, -0.0342,\n",
      "        -0.0213, -0.0257, -0.1249, -0.0565, -0.0203, -0.0209, -0.1420,  0.0429,\n",
      "         0.0103,  0.0797,  0.1810, -0.0076,  0.1030,  0.0424, -0.0071, -0.1531,\n",
      "         0.0584,  0.1649,  0.0656,  0.1450,  0.0838,  0.0300, -0.1305,  0.1888,\n",
      "         0.1232, -0.0949,  0.0861,  0.0703,  0.0241,  0.0410,  0.0035,  0.0297,\n",
      "        -0.1519, -0.0824,  0.1046,  0.1984,  0.0674, -0.0176, -0.0447, -0.1422,\n",
      "         0.0735,  0.0451, -0.1112,  0.0393,  0.0294, -0.1405,  0.0234, -0.1253,\n",
      "        -0.1276, -0.0513, -0.0150,  0.0637,  0.0116, -0.0257, -0.0369,  0.1240,\n",
      "         0.0563, -0.1244,  0.0340,  0.2321, -0.0501, -0.1057,  0.0503,  0.0778,\n",
      "         0.1700,  0.0487,  0.0876,  0.0327, -0.0904,  0.1488, -0.1432,  0.0292,\n",
      "         0.0652, -0.1193,  0.1469,  0.1012, -0.0433,  0.0400, -0.0492,  0.0475,\n",
      "         0.1613,  0.1438,  0.0895, -0.1382,  0.1841,  0.0519,  0.0310,  0.0857,\n",
      "         0.1610,  0.0167, -0.1100,  0.1103,  0.2112,  0.0546, -0.0018, -0.0948,\n",
      "         0.2012,  0.1122, -0.0116,  0.0440, -0.0529,  0.2253,  0.0029, -0.1972,\n",
      "         0.0033, -0.0177, -0.0080, -0.0247,  0.0348, -0.0781,  0.1881, -0.1570,\n",
      "        -0.0506,  0.1049, -0.1211, -0.1017,  0.2226,  0.0600,  0.0329, -0.0806,\n",
      "         0.0609, -0.1177,  0.1016, -0.0044, -0.0225,  0.0084, -0.0072,  0.0332,\n",
      "         0.0020, -0.1413, -0.0488, -0.3335, -0.0158,  0.1308, -0.0271, -0.0739,\n",
      "        -0.1157,  0.0099, -0.0023,  0.1700, -0.0363, -0.0160,  0.0149, -0.0461,\n",
      "         0.1339, -0.0855,  0.1412, -0.1244, -0.0696, -0.1208, -0.0076,  0.0016,\n",
      "        -0.0306,  0.0602,  0.0207, -0.0541,  0.1257,  0.1393, -0.1153,  0.1943,\n",
      "         0.0910,  0.0995, -0.0310,  0.0337,  0.0611, -0.0377, -0.0630,  0.1129,\n",
      "         0.1048, -0.0631,  0.1003, -0.1771, -0.1489,  0.1159, -0.0320, -0.0004,\n",
      "        -0.0193,  0.1079,  0.0577,  0.0059,  0.0090, -0.0738, -0.0493, -0.1558,\n",
      "         0.0073,  0.0445,  0.1471,  0.1139, -0.0081,  0.0025,  0.0228, -0.0350],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([ 0.1202, -0.0262, -0.1234,  0.1714,  0.0636, -0.2032, -0.1317,  0.1271,\n",
      "         0.1377, -0.0712,  0.0840,  0.1106, -0.0630, -0.0168,  0.0964, -0.0047,\n",
      "        -0.0017,  0.0755, -0.0672, -0.0417, -0.0888,  0.0342,  0.1422,  0.1110,\n",
      "        -0.0317, -0.0077, -0.0213, -0.0299, -0.0300, -0.1266, -0.0490,  0.1801,\n",
      "         0.0117,  0.0934, -0.0921, -0.0279,  0.0119,  0.1484,  0.0173,  0.0752,\n",
      "         0.1152, -0.0616,  0.0879,  0.1969,  0.0088,  0.1290,  0.0142,  0.0142,\n",
      "         0.0278,  0.0659, -0.0678, -0.0568, -0.2071, -0.1860,  0.0268,  0.0467,\n",
      "        -0.2326, -0.0314,  0.0904,  0.2557, -0.0185, -0.1776, -0.0030,  0.0113,\n",
      "        -0.0862,  0.2236, -0.0075,  0.1238,  0.0691,  0.1204,  0.0679, -0.0401,\n",
      "        -0.0495, -0.0798, -0.0669, -0.0732, -0.0545,  0.2063,  0.0867, -0.0178,\n",
      "         0.0173, -0.1354,  0.1847,  0.0712,  0.0957, -0.1035,  0.0473,  0.0785,\n",
      "        -0.0400, -0.2318,  0.0089,  0.0546,  0.0476, -0.0414,  0.0203,  0.1352,\n",
      "         0.0394,  0.1145,  0.0683, -0.1472,  0.0248,  0.0618, -0.0010, -0.0077,\n",
      "        -0.0218, -0.0124, -0.1354,  0.0240,  0.0953, -0.0124, -0.1173, -0.1482,\n",
      "         0.1119, -0.0900, -0.0693, -0.0244, -0.0948, -0.0515, -0.0500, -0.0689,\n",
      "         0.0163,  0.0906, -0.0912,  0.1164,  0.0113,  0.0841, -0.2335,  0.2392,\n",
      "        -0.0127,  0.0231,  0.0250, -0.1323, -0.0593,  0.1133,  0.1379,  0.0061,\n",
      "        -0.0992, -0.0403, -0.1855,  0.1803, -0.0190, -0.0535, -0.0473, -0.1457,\n",
      "         0.1243, -0.1146,  0.0511, -0.0506, -0.0135, -0.1252,  0.1024,  0.0341,\n",
      "         0.0027, -0.1835, -0.1998,  0.0039,  0.0659,  0.0370, -0.1119,  0.0181,\n",
      "        -0.0610, -0.0410,  0.0242, -0.0028,  0.1475, -0.0108, -0.0654,  0.2243,\n",
      "        -0.1577, -0.1348, -0.1211,  0.1322, -0.1063,  0.1385,  0.0114, -0.0366,\n",
      "         0.2199, -0.1698,  0.1488,  0.0338, -0.0328, -0.0886, -0.0561, -0.1190,\n",
      "        -0.2324,  0.0089,  0.1224,  0.1789, -0.0683, -0.0776,  0.0248, -0.0586,\n",
      "        -0.1880,  0.0431, -0.0457,  0.0558, -0.0631, -0.1631, -0.0045, -0.1390],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([-0.0877, -0.0956,  0.0217,  0.0797,  0.1188,  0.0516,  0.2102,  0.0108,\n",
      "        -0.0138, -0.1284, -0.0245,  0.0360, -0.0010,  0.1398,  0.0116,  0.1320,\n",
      "        -0.1619,  0.0047, -0.1057, -0.1802,  0.0122,  0.1104,  0.0104, -0.0402,\n",
      "        -0.0100, -0.0249,  0.1534,  0.0483,  0.1323,  0.0404,  0.0739, -0.1094,\n",
      "         0.0196,  0.1187, -0.1146, -0.0648,  0.0146, -0.0169, -0.1295,  0.0348,\n",
      "        -0.0755, -0.0760, -0.0127,  0.2390, -0.2794, -0.0463, -0.1205, -0.0060,\n",
      "         0.1269,  0.1357, -0.0133, -0.0809, -0.0568,  0.0284, -0.1006,  0.0387,\n",
      "         0.0450, -0.0380, -0.0468, -0.0353, -0.0469,  0.0093,  0.0152, -0.0014,\n",
      "        -0.0430,  0.1377, -0.0110,  0.0832,  0.0056, -0.0605,  0.1316,  0.0811,\n",
      "        -0.0735,  0.0874,  0.2685, -0.1147, -0.1893,  0.1228,  0.1156, -0.0954,\n",
      "        -0.0050,  0.1870,  0.1134, -0.0041, -0.1009,  0.1746, -0.0251,  0.0118,\n",
      "        -0.2279, -0.1434,  0.1223, -0.0733, -0.1727,  0.0534,  0.0410, -0.0118,\n",
      "         0.1162,  0.0876, -0.0825, -0.0829, -0.0485,  0.0544, -0.0723, -0.1210,\n",
      "        -0.0820,  0.0099, -0.0742,  0.0692, -0.0065, -0.0765, -0.0620, -0.0988,\n",
      "         0.0427, -0.0395, -0.0273, -0.0219, -0.0244,  0.0726, -0.0218,  0.0231,\n",
      "        -0.0393, -0.0358, -0.0544,  0.1659,  0.0649,  0.0658,  0.0377,  0.0246,\n",
      "         0.0639, -0.0453,  0.1143, -0.0087,  0.1164,  0.0337, -0.0511, -0.0127,\n",
      "        -0.1086,  0.0789,  0.0582,  0.1190,  0.0881,  0.0430, -0.0642, -0.0046,\n",
      "         0.1433, -0.0732, -0.0898,  0.0138, -0.0436,  0.1579,  0.0627, -0.0104,\n",
      "         0.1532,  0.0109,  0.0003, -0.2786, -0.0083,  0.0275,  0.0979,  0.0258,\n",
      "        -0.0268,  0.0860,  0.1199,  0.1257, -0.1193,  0.0512,  0.0103, -0.2201,\n",
      "         0.0724, -0.1317,  0.0547,  0.0282, -0.0735,  0.1430,  0.1602, -0.1160,\n",
      "         0.1665,  0.1585,  0.1910, -0.1532,  0.0827,  0.0509, -0.1487, -0.0844,\n",
      "        -0.0785, -0.0381,  0.0872,  0.0271,  0.0052, -0.0278, -0.0016,  0.2164,\n",
      "         0.1517, -0.0722,  0.1760, -0.0871, -0.1985, -0.0941,  0.1755, -0.0495],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([-0.0458,  0.0977, -0.0134, -0.0835,  0.0448,  0.0171, -0.0576, -0.0397,\n",
      "        -0.0067, -0.1340,  0.1006, -0.0585, -0.2075, -0.0664, -0.1161,  0.0732,\n",
      "        -0.0115,  0.0431, -0.0146,  0.0344,  0.0796, -0.0201,  0.0468,  0.2381,\n",
      "        -0.0774, -0.0332,  0.0355, -0.1269,  0.0069,  0.0507,  0.0382,  0.2128,\n",
      "        -0.0956,  0.0315,  0.2413, -0.1003, -0.2762,  0.0390,  0.0243,  0.0598,\n",
      "        -0.0965,  0.0358, -0.0816,  0.0638,  0.1237, -0.2630, -0.0128,  0.0618,\n",
      "         0.0247,  0.0789,  0.0859, -0.1383, -0.2039, -0.0247, -0.1102, -0.0273,\n",
      "         0.1882, -0.0598,  0.1086,  0.0446, -0.0646, -0.0147, -0.0441, -0.0348,\n",
      "         0.0463, -0.0123, -0.1985, -0.0469, -0.0145, -0.0174, -0.0556,  0.0893,\n",
      "        -0.0437, -0.0930, -0.0747,  0.1090,  0.1191, -0.1687,  0.0925,  0.1003,\n",
      "         0.0098,  0.0242,  0.0638, -0.0252,  0.1562, -0.1205, -0.1023,  0.0004,\n",
      "        -0.0498, -0.2207, -0.1201, -0.0070, -0.1073, -0.0246, -0.0045, -0.0108,\n",
      "        -0.2376, -0.1772,  0.0659, -0.0912, -0.0016,  0.0116, -0.0275,  0.0190,\n",
      "        -0.1949, -0.0008,  0.0605,  0.0476,  0.2083, -0.3634, -0.1401,  0.1284,\n",
      "        -0.0455, -0.0337, -0.0402,  0.1309,  0.0241,  0.0700,  0.0864, -0.1219,\n",
      "         0.1051, -0.0141,  0.1375,  0.0273,  0.2369,  0.1161, -0.0271,  0.0296,\n",
      "         0.2289, -0.0017, -0.1342, -0.1046,  0.2519,  0.0148, -0.0885,  0.1634,\n",
      "         0.0185, -0.0758,  0.1049,  0.0520,  0.0518,  0.1442, -0.1941, -0.0830,\n",
      "         0.1274,  0.1316,  0.0009,  0.1112, -0.1924, -0.0968, -0.0974,  0.0881,\n",
      "        -0.0221, -0.0484, -0.0251,  0.0704, -0.0506,  0.0741,  0.0869,  0.0330,\n",
      "        -0.0466, -0.0537, -0.0250,  0.0555,  0.0666, -0.0657, -0.0021,  0.0308,\n",
      "         0.0849,  0.2331, -0.0225, -0.0522, -0.1208,  0.0729,  0.0501, -0.1728,\n",
      "         0.1213, -0.0319, -0.0316, -0.0309, -0.0081, -0.0184, -0.1813, -0.1157,\n",
      "         0.1358,  0.2464,  0.0998,  0.2454,  0.1000, -0.1569, -0.0818, -0.1620,\n",
      "        -0.0423,  0.0733, -0.0286,  0.0940,  0.0523, -0.0857, -0.0942,  0.0292],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 0.0920,  0.0025,  0.0684,  0.0209, -0.0565, -0.1118, -0.1577,  0.1564,\n",
      "         0.0243,  0.0178,  0.0190, -0.1554,  0.0702,  0.0878,  0.0074,  0.0046,\n",
      "        -0.0495,  0.0075,  0.0314, -0.1426,  0.0347, -0.0335, -0.0080,  0.0207,\n",
      "         0.1908,  0.0548,  0.2066,  0.1970,  0.0827,  0.2131,  0.1992,  0.0040,\n",
      "         0.0645, -0.1709,  0.0119, -0.1041,  0.0316, -0.0105,  0.0361,  0.0075,\n",
      "        -0.0018,  0.0916,  0.0401,  0.0687, -0.0738, -0.1741, -0.0363,  0.0150,\n",
      "         0.1108, -0.0535, -0.0036, -0.0131, -0.0927, -0.1613, -0.1176, -0.0818,\n",
      "        -0.0558,  0.0025,  0.0029,  0.2091,  0.1036, -0.1004,  0.1227, -0.0869,\n",
      "         0.1515, -0.0967, -0.0728,  0.0427, -0.0188, -0.0928,  0.1226, -0.0883,\n",
      "         0.1344, -0.0230,  0.0637,  0.1815,  0.0494, -0.0993, -0.0091, -0.0710,\n",
      "         0.1100, -0.0426, -0.0733, -0.0602, -0.0812,  0.1111,  0.1143, -0.0374,\n",
      "        -0.2008,  0.1128, -0.1432,  0.0433, -0.0631, -0.1863,  0.1906,  0.1531,\n",
      "        -0.0132, -0.1980, -0.1186, -0.0020, -0.0579, -0.1058,  0.0227, -0.0654,\n",
      "         0.0597, -0.0494, -0.2735, -0.0122, -0.0715, -0.0986, -0.0118, -0.1919,\n",
      "        -0.2056,  0.0703, -0.1008,  0.0935,  0.0219, -0.0270, -0.0905,  0.1249,\n",
      "         0.0651, -0.2144, -0.1069,  0.1039, -0.0102,  0.1204, -0.0357,  0.0302,\n",
      "         0.0095,  0.1435,  0.1327,  0.1290, -0.0106,  0.0044, -0.1308,  0.0560,\n",
      "        -0.0741,  0.0600,  0.0712,  0.1139,  0.0295,  0.0337,  0.1186, -0.1614,\n",
      "         0.0319, -0.0348,  0.0508,  0.0129, -0.1496, -0.0576,  0.1401,  0.0410,\n",
      "        -0.0254,  0.0047, -0.0563, -0.0093, -0.0020,  0.0051, -0.1738,  0.0334,\n",
      "        -0.0338,  0.2395,  0.0806,  0.0724,  0.1495, -0.0145, -0.0305,  0.0186,\n",
      "         0.0880, -0.0871, -0.1194, -0.0178, -0.1582,  0.0997,  0.0575,  0.0144,\n",
      "        -0.0654,  0.2522, -0.0783, -0.0796,  0.0523, -0.2514,  0.0456, -0.0742,\n",
      "         0.0047,  0.0301,  0.0216,  0.0501, -0.0097,  0.0769,  0.1078,  0.0563,\n",
      "         0.0369, -0.0443,  0.0710,  0.1165,  0.2539,  0.0293, -0.0378, -0.1127],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([ 0.0428,  0.0321,  0.0731, -0.0146, -0.0239,  0.0978,  0.0727, -0.0371,\n",
      "        -0.0292, -0.0166, -0.1008,  0.0509, -0.1777,  0.0600,  0.0075,  0.1038,\n",
      "        -0.0010, -0.0806,  0.0871,  0.0417, -0.0036,  0.0720, -0.0548,  0.0572,\n",
      "        -0.0479, -0.0059,  0.0147,  0.1250,  0.0194, -0.0106, -0.0614,  0.0434,\n",
      "         0.0713, -0.2075,  0.0036,  0.1023, -0.0389,  0.1255,  0.0307,  0.0435,\n",
      "        -0.0934,  0.0456,  0.0871,  0.0125,  0.1250, -0.0069,  0.1423,  0.0488,\n",
      "         0.0305,  0.1593,  0.0192,  0.0074,  0.0231, -0.1334, -0.0226,  0.0091,\n",
      "        -0.2154, -0.0160, -0.0414,  0.0672,  0.1752, -0.0678,  0.0895,  0.1190,\n",
      "        -0.0351, -0.0715, -0.0103,  0.0097,  0.0869,  0.0240,  0.0793, -0.0767,\n",
      "        -0.0717, -0.0940,  0.0476, -0.2152, -0.0321, -0.0488, -0.0640, -0.0513,\n",
      "        -0.0607, -0.0273,  0.0188, -0.0473,  0.0738, -0.0857,  0.0061,  0.0312,\n",
      "         0.0646, -0.1357, -0.0441, -0.1284, -0.0481,  0.0165, -0.0211,  0.2287,\n",
      "        -0.0271,  0.0152,  0.1242, -0.0533,  0.0187, -0.1091, -0.0617, -0.0662,\n",
      "         0.0227,  0.0505,  0.0962, -0.0345, -0.0240,  0.0080,  0.0202, -0.0835,\n",
      "        -0.0084,  0.0769,  0.2155,  0.1674,  0.1109,  0.0182,  0.0306,  0.0982,\n",
      "         0.0933, -0.0356,  0.0207, -0.0930,  0.0580,  0.0909,  0.0189, -0.0050,\n",
      "         0.1042,  0.0229, -0.0483, -0.0293, -0.0561,  0.0925, -0.1809,  0.1287,\n",
      "        -0.0537,  0.1121,  0.2145, -0.1615, -0.1570, -0.1141,  0.1038, -0.1590,\n",
      "        -0.0571,  0.0479, -0.0813, -0.0180, -0.1263, -0.1126, -0.1739,  0.0779,\n",
      "         0.1343,  0.0210,  0.0778, -0.0032,  0.0438,  0.0255,  0.0740, -0.1069,\n",
      "         0.0560, -0.0313, -0.1275, -0.0635,  0.1251,  0.1513,  0.1299, -0.0706,\n",
      "         0.0473,  0.1493, -0.0339,  0.0252, -0.0442, -0.0416,  0.0829, -0.0210,\n",
      "        -0.1637,  0.0797, -0.0501,  0.0141,  0.1111,  0.0798,  0.1180, -0.0887,\n",
      "        -0.0206,  0.0826, -0.0156, -0.0193,  0.0154, -0.0477,  0.0109, -0.1011,\n",
      "         0.1835, -0.0764, -0.0814, -0.0565,  0.0595, -0.0793, -0.1365,  0.0854],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([ 0.2786, -0.0757,  0.1882,  0.0986, -0.0052, -0.0171,  0.0706,  0.0785,\n",
      "         0.0404, -0.0391, -0.2622, -0.0266, -0.0915, -0.0102, -0.1763, -0.0005,\n",
      "         0.0244, -0.0047, -0.1756,  0.0713,  0.0378,  0.0199, -0.0946, -0.0559,\n",
      "        -0.0006, -0.0575, -0.0134, -0.0547, -0.0265,  0.0573, -0.0855,  0.1161,\n",
      "         0.0019,  0.2446,  0.1314,  0.0081, -0.0187,  0.0311,  0.0440, -0.2296,\n",
      "         0.1232, -0.0735,  0.0655,  0.0614, -0.0057, -0.0544,  0.0744, -0.1517,\n",
      "         0.0216, -0.0729,  0.1657,  0.0011, -0.0487, -0.0614,  0.0782,  0.1953,\n",
      "         0.1306, -0.1252, -0.1087, -0.1332, -0.0280, -0.0858,  0.1140, -0.1461,\n",
      "         0.1185,  0.1491, -0.0037, -0.1928, -0.1212, -0.0950,  0.1770,  0.1110,\n",
      "         0.0038, -0.1364, -0.1562,  0.0473, -0.0993,  0.0195, -0.0412, -0.0313,\n",
      "         0.0800, -0.0122, -0.0489,  0.1224,  0.0790, -0.1303, -0.1176,  0.0733,\n",
      "         0.0580,  0.0163,  0.1199, -0.0615, -0.0266,  0.1104,  0.1625,  0.0142,\n",
      "         0.0859,  0.0706,  0.1991,  0.0637, -0.0936,  0.1338,  0.0970, -0.0677,\n",
      "        -0.0826,  0.0278,  0.0120, -0.0403,  0.1137, -0.0658,  0.1766,  0.1704,\n",
      "        -0.0169,  0.0278, -0.0852,  0.0756,  0.1510,  0.0474,  0.0343,  0.0027,\n",
      "        -0.0547, -0.1135, -0.1153, -0.0456,  0.1936, -0.1598,  0.1217, -0.0154,\n",
      "         0.0888, -0.0265,  0.0447, -0.0987, -0.2350,  0.0377,  0.1118,  0.0101,\n",
      "         0.0257,  0.0181, -0.2676,  0.1026, -0.0412, -0.0949,  0.0210,  0.1016,\n",
      "        -0.1592,  0.1207,  0.0408, -0.0480, -0.0706, -0.1984,  0.0275, -0.0377,\n",
      "         0.0436,  0.0340, -0.0025, -0.1128,  0.1231, -0.1365, -0.1003, -0.0966,\n",
      "         0.0721, -0.0783,  0.0169,  0.0071,  0.0626,  0.1496,  0.0952, -0.0582,\n",
      "         0.1090,  0.0342, -0.0420,  0.1266,  0.0218,  0.0619, -0.1035, -0.0291,\n",
      "         0.1604, -0.1378,  0.1649,  0.0057, -0.1854,  0.0592, -0.0232, -0.0433,\n",
      "         0.0032,  0.1055, -0.0925, -0.1225,  0.0158,  0.0123,  0.0416,  0.0820,\n",
      "         0.0525, -0.0926, -0.0598,  0.0754,  0.1008, -0.0699, -0.0674,  0.1337],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([-1.2305e-01,  3.0136e-02,  1.5853e-01,  1.5404e-02, -1.6246e-02,\n",
      "         1.3518e-01,  1.0854e-01, -6.3399e-02, -6.9404e-02,  1.0519e-01,\n",
      "        -1.4980e-01,  1.1268e-01,  4.9027e-03, -1.4273e-02,  4.6280e-03,\n",
      "        -1.8926e-01, -4.3102e-02, -5.3637e-02, -6.0012e-02, -1.1975e-04,\n",
      "         2.1273e-02,  1.8493e-01, -5.2006e-02, -2.6848e-02, -6.1031e-02,\n",
      "         1.1437e-01, -1.2980e-01,  1.0080e-02, -2.4530e-01,  2.8950e-02,\n",
      "         6.1567e-02,  2.0725e-01, -7.8279e-02, -8.9078e-02, -3.2790e-02,\n",
      "         4.6077e-02, -9.3232e-02, -1.1386e-01,  3.3371e-02, -1.7951e-02,\n",
      "         7.6172e-03, -4.9147e-02, -1.5830e-01, -5.8619e-02,  4.7290e-02,\n",
      "        -4.5728e-02, -1.6242e-02, -1.3271e-01, -1.3476e-01, -4.2014e-02,\n",
      "        -7.0191e-02, -1.3324e-02, -1.0671e-01,  3.3337e-02,  2.2103e-01,\n",
      "         2.6584e-02, -7.5356e-02,  6.6316e-02,  1.7570e-02,  1.3682e-02,\n",
      "        -2.6255e-02, -1.3132e-01, -1.7064e-01,  5.9237e-02,  2.1569e-02,\n",
      "        -2.1649e-02, -1.7018e-01, -2.8342e-02, -2.8326e-02,  1.5299e-01,\n",
      "         6.1362e-02, -5.5245e-02,  3.9658e-02, -5.6887e-02, -3.0726e-01,\n",
      "        -3.9574e-02, -3.4013e-02,  9.9787e-02,  4.0274e-03,  2.5049e-02,\n",
      "        -2.5854e-02, -2.8713e-02, -4.2997e-03,  1.1144e-01, -1.1110e-01,\n",
      "         4.2754e-02,  3.4742e-02, -4.2712e-02,  3.9584e-02, -6.8652e-02,\n",
      "         7.0330e-02, -1.0272e-01,  1.0495e-01,  5.6734e-02,  1.5904e-01,\n",
      "         6.4183e-02, -2.4289e-01, -1.3081e-01, -8.6380e-02,  1.9109e-02,\n",
      "        -1.4522e-01, -1.9701e-04, -1.1509e-02,  3.5057e-02,  1.3077e-01,\n",
      "         6.0310e-02, -1.0871e-01,  5.5452e-02,  2.6526e-02, -6.6922e-02,\n",
      "        -1.3575e-03, -8.2874e-02, -4.5241e-02,  1.3346e-01, -1.0910e-01,\n",
      "        -1.6564e-02,  9.3061e-04, -8.0978e-02, -1.2551e-01, -7.8407e-02,\n",
      "         5.7077e-02, -8.4239e-02,  2.2719e-02, -1.2838e-01, -6.9659e-02,\n",
      "        -5.4203e-02,  4.4498e-02,  4.3950e-02, -1.0655e-01, -9.8321e-02,\n",
      "         1.2683e-01,  1.9014e-02, -1.7623e-01,  2.1006e-01,  7.5540e-02,\n",
      "        -6.0512e-02,  5.6371e-02,  1.9379e-02, -1.0386e-01, -1.8014e-01,\n",
      "         3.6757e-02, -2.7142e-03, -7.6189e-02, -4.9528e-02,  1.4515e-02,\n",
      "        -1.1158e-01,  2.7404e-02,  1.2337e-01, -2.1400e-02,  5.2244e-02,\n",
      "        -1.8202e-01, -1.1256e-01,  1.6741e-02, -1.9378e-02, -5.0425e-02,\n",
      "         6.7579e-03,  5.7549e-02,  4.4756e-02, -2.7086e-02, -1.5972e-01,\n",
      "        -4.8556e-02, -4.6010e-02, -4.9463e-02, -5.8081e-02,  1.1600e-01,\n",
      "        -5.4176e-02, -7.6162e-02,  1.1570e-01, -9.8523e-02,  7.5677e-02,\n",
      "        -1.2649e-01,  1.3097e-02, -1.7000e-02,  1.5598e-01,  2.5393e-01,\n",
      "        -1.3320e-01,  4.3941e-02,  8.0138e-02,  2.0836e-02,  1.0476e-01,\n",
      "         5.5617e-02, -2.7643e-02,  8.9693e-02,  8.5703e-02,  2.0490e-02,\n",
      "        -4.8608e-02, -8.7908e-02, -1.0581e-01, -1.3121e-01, -4.6681e-02,\n",
      "        -6.4641e-02, -1.5862e-01, -7.2798e-02,  5.2644e-02, -1.1139e-02,\n",
      "         9.1557e-03, -7.7487e-02, -1.2601e-01,  1.0296e-01,  1.5958e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([ 0.1046, -0.1296,  0.0851,  0.1008, -0.0445, -0.2260,  0.0434, -0.0966,\n",
      "         0.0908, -0.0407, -0.0833, -0.0598,  0.1949, -0.0273, -0.1071,  0.0570,\n",
      "        -0.1047, -0.2197, -0.1416,  0.0508,  0.0768, -0.1382, -0.0315,  0.0650,\n",
      "         0.0491, -0.2319, -0.0672, -0.0696, -0.0297,  0.0234,  0.0662,  0.1247,\n",
      "        -0.1294,  0.1002,  0.0196, -0.1016, -0.0519, -0.0953,  0.1196, -0.0909,\n",
      "         0.1631,  0.0077, -0.0264,  0.0395, -0.0620,  0.1327, -0.1579,  0.0297,\n",
      "         0.0534, -0.0624,  0.2540, -0.1247, -0.1141, -0.0598,  0.0715,  0.0334,\n",
      "        -0.1894,  0.0597,  0.1659, -0.0759, -0.0910,  0.1353, -0.1211,  0.0824,\n",
      "        -0.0755,  0.0280, -0.0388, -0.0393, -0.0794,  0.1164,  0.0213, -0.0763,\n",
      "        -0.0620,  0.1006,  0.0208, -0.1318,  0.0012, -0.0217, -0.0149, -0.0522,\n",
      "         0.1341, -0.0181, -0.0536,  0.0776,  0.0670, -0.0746, -0.0536,  0.0968,\n",
      "        -0.0370, -0.2170, -0.0052, -0.0247,  0.1515,  0.0281, -0.0342, -0.1555,\n",
      "         0.0719,  0.0087, -0.1259,  0.0364,  0.0674,  0.0160,  0.0020,  0.1713,\n",
      "         0.1345,  0.0562,  0.1615, -0.1444, -0.0080,  0.0321, -0.0166, -0.0145,\n",
      "         0.0113, -0.0378, -0.0423, -0.0445, -0.1164,  0.0951,  0.0053,  0.0475,\n",
      "        -0.0824, -0.1811, -0.0433,  0.0460,  0.0116, -0.0285,  0.0125, -0.0151,\n",
      "         0.0586, -0.1786,  0.0982,  0.0384,  0.0271,  0.0506,  0.0712,  0.0267,\n",
      "        -0.0302, -0.0346,  0.0818,  0.0495,  0.1015, -0.0602,  0.1778,  0.0185,\n",
      "        -0.0196, -0.0521,  0.0738,  0.0711,  0.2094, -0.0504,  0.1377,  0.0974,\n",
      "        -0.0041, -0.0240, -0.0461,  0.0217, -0.0702, -0.0715, -0.2164,  0.0198,\n",
      "         0.1167, -0.0538, -0.1068,  0.0773,  0.0278, -0.0413,  0.0567, -0.0498,\n",
      "         0.1943,  0.0321,  0.1823, -0.0207, -0.0376, -0.1079,  0.0542,  0.0448,\n",
      "         0.1184,  0.0111,  0.0254, -0.0179,  0.0862,  0.0099,  0.0165,  0.0469,\n",
      "         0.0900, -0.0843, -0.0308, -0.0408,  0.0105, -0.0622,  0.1835,  0.0337,\n",
      "        -0.0047,  0.1204, -0.0350,  0.0329,  0.0027, -0.1062,  0.1013, -0.0358],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 0.0113, -0.1464,  0.0336, -0.0976,  0.1180, -0.0842,  0.1367,  0.0304,\n",
      "        -0.0521,  0.0066, -0.1273,  0.0346,  0.0552,  0.0803, -0.0217, -0.0179,\n",
      "        -0.0330, -0.0246,  0.0857,  0.0033, -0.1905, -0.0250, -0.0524,  0.0935,\n",
      "        -0.0149, -0.0106,  0.1604,  0.0027, -0.0431, -0.0098,  0.0437,  0.0943,\n",
      "        -0.2055, -0.0959, -0.0587, -0.0288, -0.0149, -0.1560, -0.0167, -0.0745,\n",
      "        -0.0090, -0.0920,  0.0945,  0.1806,  0.0654,  0.1203, -0.0853,  0.0941,\n",
      "        -0.0520,  0.1196,  0.0529, -0.0573,  0.0748, -0.0540, -0.1416, -0.1071,\n",
      "        -0.0883, -0.0462, -0.1626,  0.0223,  0.0787, -0.0975, -0.0128,  0.0025,\n",
      "        -0.1879,  0.0409,  0.0063,  0.1621,  0.0653,  0.0229,  0.0951,  0.0788,\n",
      "         0.1621, -0.1995, -0.0661, -0.0941,  0.0019, -0.0649, -0.0415,  0.0692,\n",
      "        -0.1435,  0.0607,  0.1574,  0.0712,  0.0745, -0.0763, -0.0769,  0.0912,\n",
      "         0.1068,  0.0413,  0.0301,  0.1277, -0.0690, -0.1551,  0.0374,  0.0205,\n",
      "         0.0547,  0.0360,  0.0548,  0.1058, -0.1230,  0.1627, -0.0823, -0.0775,\n",
      "         0.0422,  0.1258,  0.0162, -0.0995,  0.0840,  0.0638,  0.0359, -0.0756,\n",
      "        -0.1296,  0.0322, -0.0050,  0.0942, -0.0200, -0.1290, -0.1166, -0.0827,\n",
      "        -0.1648, -0.2358, -0.0503,  0.0803,  0.0036,  0.0143,  0.2058, -0.0852,\n",
      "        -0.1067, -0.0396,  0.1123, -0.1626,  0.1050,  0.0835, -0.0293, -0.1010,\n",
      "        -0.1191,  0.0671,  0.0624,  0.0006, -0.1674, -0.0823,  0.0453, -0.1432,\n",
      "         0.0570,  0.0376,  0.1267,  0.0659, -0.0382,  0.0879,  0.1162, -0.1884,\n",
      "        -0.2018,  0.1853, -0.0386,  0.0438, -0.0003, -0.0225,  0.0584, -0.2601,\n",
      "         0.0943, -0.0634,  0.0747,  0.0919, -0.1000,  0.0549,  0.1506, -0.1381,\n",
      "        -0.1029, -0.0155, -0.1133, -0.0336, -0.1067,  0.1414,  0.0097,  0.0796,\n",
      "         0.2063, -0.0028,  0.0603,  0.0544,  0.1021,  0.1003,  0.2182,  0.0620,\n",
      "        -0.0307,  0.0949, -0.0713, -0.0341,  0.1618, -0.1352,  0.0673,  0.1393,\n",
      "         0.1873,  0.0476,  0.1035, -0.1010, -0.0398, -0.0147, -0.0786,  0.1403],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([-6.6747e-03,  6.5729e-02,  1.4470e-01, -6.5672e-02, -1.2252e-02,\n",
      "        -7.4418e-02,  1.3075e-01, -6.4146e-02, -1.5933e-02, -6.0287e-03,\n",
      "         9.1294e-02, -6.2643e-02,  1.5361e-01, -6.2739e-02, -3.8257e-02,\n",
      "        -4.2962e-02, -1.6994e-01,  3.9135e-02,  5.1353e-02,  2.9522e-02,\n",
      "         1.2316e-03,  1.3677e-01,  7.0008e-02,  1.7099e-01,  7.5715e-02,\n",
      "        -1.6870e-02, -4.6371e-02, -1.0763e-01, -2.3548e-02, -5.1952e-02,\n",
      "         1.1021e-02,  1.6576e-01, -9.5629e-03, -9.6102e-02,  1.8124e-01,\n",
      "        -5.5464e-02, -4.2968e-02, -2.2076e-02,  2.3075e-02, -5.1740e-02,\n",
      "        -1.1652e-02,  1.0094e-01,  3.9017e-02,  2.6598e-02,  5.6376e-03,\n",
      "         3.9753e-02,  6.9002e-02, -5.7450e-02, -4.4164e-02,  5.9480e-02,\n",
      "         7.2323e-02,  4.7404e-02, -5.5110e-02, -3.0929e-02, -1.8829e-02,\n",
      "        -3.1004e-02, -6.0127e-02, -1.2910e-01,  5.0327e-03, -1.4981e-01,\n",
      "         4.7995e-02, -1.3479e-01, -2.4164e-02,  2.9881e-02, -8.5115e-02,\n",
      "         1.0751e-01, -7.1635e-02, -1.8901e-01, -1.4803e-01,  1.5322e-01,\n",
      "        -4.3896e-02,  2.1957e-02, -1.4097e-02, -7.5003e-02,  8.5139e-02,\n",
      "        -6.5075e-02, -9.7562e-02,  3.3544e-02, -4.1478e-02, -4.8194e-02,\n",
      "         1.3985e-01, -5.9462e-02,  8.0097e-02,  2.7175e-02,  1.7028e-01,\n",
      "        -3.8302e-02,  2.2231e-02, -3.7208e-02, -3.3830e-02,  2.1785e-01,\n",
      "         1.2512e-01,  2.8557e-02,  1.9138e-01, -3.8588e-02, -1.6664e-01,\n",
      "        -2.2087e-03, -1.0151e-01,  7.4267e-02, -1.0656e-01, -7.7187e-02,\n",
      "        -1.6417e-02, -4.6595e-02,  8.7150e-02,  3.3077e-02, -2.0743e-02,\n",
      "         1.4287e-01, -4.9385e-02, -1.6863e-01,  1.1981e-02, -3.2485e-02,\n",
      "        -1.3748e-01, -1.2643e-02, -8.6585e-02, -9.6918e-02, -2.0467e-01,\n",
      "        -1.4199e-01,  4.9248e-04, -5.3718e-02,  1.6193e-01, -8.8271e-02,\n",
      "        -1.0275e-01,  9.4363e-04,  1.0356e-01, -1.1083e-01, -6.2134e-02,\n",
      "         2.0787e-01,  2.4308e-02, -9.2020e-02,  1.5944e-02, -2.2919e-02,\n",
      "         7.0502e-02, -1.2623e-02, -7.9829e-02, -1.0071e-01,  1.2569e-01,\n",
      "         3.2891e-02, -1.8486e-04,  2.6054e-02,  1.1473e-01, -4.0075e-02,\n",
      "         8.2251e-02,  1.3118e-01,  3.8453e-02,  5.8488e-02, -1.8027e-01,\n",
      "         1.1527e-02, -1.7192e-01, -1.4059e-01,  7.9757e-02, -1.4086e-01,\n",
      "        -6.9015e-02, -1.0096e-01,  8.5088e-04, -1.0197e-01,  8.5525e-03,\n",
      "         1.1546e-02,  3.8997e-02,  1.3018e-01, -6.3487e-04, -2.6409e-02,\n",
      "        -4.4687e-02,  1.8515e-02, -2.2615e-02,  8.9653e-03,  8.7111e-02,\n",
      "        -7.4819e-02, -1.8283e-01,  7.0836e-02,  2.9415e-02,  5.0966e-02,\n",
      "         1.3239e-01, -4.2729e-02,  9.2796e-02, -2.1039e-01,  1.3215e-01,\n",
      "         5.3917e-03,  4.3357e-02,  9.8219e-04, -1.0359e-01, -4.8274e-02,\n",
      "        -8.3598e-02,  7.1841e-02, -1.8416e-01,  6.0103e-02,  2.7801e-02,\n",
      "        -1.5943e-01, -3.0680e-02, -6.1969e-02, -2.0058e-01, -7.6666e-03,\n",
      "         2.6542e-02, -2.5344e-02,  1.5768e-01, -1.9714e-01, -2.0688e-01,\n",
      "         1.9393e-02, -6.3379e-02, -1.8550e-01, -8.7640e-02, -3.5060e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-0.1167,  0.0263,  0.0746, -0.0116,  0.1331,  0.2317,  0.0889, -0.1986,\n",
      "         0.0694,  0.0159,  0.0027,  0.0162,  0.1141,  0.0772,  0.1666,  0.1879,\n",
      "        -0.0916,  0.0409, -0.0014, -0.0194,  0.0015,  0.0653,  0.0764,  0.0640,\n",
      "        -0.0265,  0.0851,  0.0369,  0.0315,  0.0609, -0.0593,  0.1240, -0.0070,\n",
      "        -0.1399, -0.0889,  0.1121, -0.1002,  0.0397, -0.0258, -0.0436,  0.0929,\n",
      "         0.0278,  0.0038,  0.0569, -0.0163,  0.0715,  0.0639, -0.0248, -0.0952,\n",
      "        -0.0286, -0.0169, -0.0360,  0.0317, -0.0099,  0.1990,  0.1750, -0.2304,\n",
      "        -0.0312,  0.1387,  0.0210, -0.0895, -0.1009, -0.1085, -0.0756, -0.0479,\n",
      "         0.1207,  0.0718, -0.1723, -0.0978, -0.0751, -0.0208,  0.1632, -0.0492,\n",
      "         0.0399,  0.0205, -0.0003,  0.1035, -0.0439,  0.0232,  0.0976,  0.0104,\n",
      "         0.0079,  0.1034, -0.0757,  0.0093,  0.0206,  0.0194,  0.0185, -0.0524,\n",
      "         0.1030, -0.0205,  0.0358,  0.0449,  0.1456,  0.0205, -0.0378, -0.0438,\n",
      "        -0.0230,  0.0574, -0.0864, -0.0055,  0.2229, -0.1766,  0.0729,  0.0846,\n",
      "        -0.0929, -0.0235,  0.1088,  0.0235, -0.0131, -0.0163,  0.0556,  0.1112,\n",
      "        -0.0815, -0.0013,  0.1019,  0.1028,  0.1623,  0.0363, -0.1706,  0.0680,\n",
      "        -0.0314,  0.0211,  0.0200, -0.0538, -0.0339,  0.0701, -0.0126, -0.0115,\n",
      "        -0.0739, -0.0386, -0.0136,  0.1118, -0.1125,  0.2340,  0.0462,  0.1383,\n",
      "         0.1026,  0.0914,  0.0067,  0.0007, -0.1423,  0.1220,  0.0465,  0.0952,\n",
      "         0.1026,  0.0023, -0.2052,  0.0964,  0.0302, -0.0303,  0.0150,  0.2041,\n",
      "         0.2416, -0.0872, -0.0081, -0.0253, -0.0041, -0.0063,  0.1450, -0.0404,\n",
      "         0.0523, -0.0593, -0.1151,  0.0554, -0.0563, -0.0658,  0.0560, -0.0704,\n",
      "        -0.0262, -0.0290,  0.1284,  0.0668,  0.0445,  0.0102,  0.0852,  0.1136,\n",
      "         0.0889,  0.1067, -0.1227,  0.0124,  0.0955, -0.1507, -0.1301,  0.0557,\n",
      "         0.0401,  0.1535,  0.1513,  0.1238, -0.0431,  0.1100,  0.1386,  0.0631,\n",
      "        -0.0874, -0.0331, -0.0788,  0.0109,  0.0136, -0.1153,  0.0209,  0.1210],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  0.067153/  0.083232, val:  40.42%, val_best:  40.42%, tr:  74.77%, tr_best:  74.77%, epoch time: 68.36 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0573%\n",
      "layer   2  Sparsity: 56.0590%\n",
      "layer   3  Sparsity: 56.0144%\n",
      "total_backward_count 9790 real_backward_count 3509  35.843%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  0.046740/  0.077665, val:  49.17%, val_best:  49.17%, tr:  86.41%, tr_best:  86.41%, epoch time: 67.94 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0522%\n",
      "layer   2  Sparsity: 53.7165%\n",
      "layer   3  Sparsity: 54.4223%\n",
      "total_backward_count 19580 real_backward_count 5778  29.510%\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  0.041416/  0.075563, val:  51.25%, val_best:  51.25%, tr:  89.48%, tr_best:  89.48%, epoch time: 68.30 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   2  Sparsity: 52.9672%\n",
      "layer   3  Sparsity: 53.6372%\n",
      "total_backward_count 29370 real_backward_count 7711  26.255%\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  0.039442/  0.070609, val:  52.50%, val_best:  52.50%, tr:  89.27%, tr_best:  89.48%, epoch time: 68.16 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0845%\n",
      "layer   2  Sparsity: 52.6351%\n",
      "layer   3  Sparsity: 52.6408%\n",
      "total_backward_count 39160 real_backward_count 9536  24.351%\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  0.038181/  0.070232, val:  57.92%, val_best:  57.92%, tr:  90.09%, tr_best:  90.09%, epoch time: 67.85 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0741%\n",
      "layer   2  Sparsity: 52.4968%\n",
      "layer   3  Sparsity: 52.3135%\n",
      "total_backward_count 48950 real_backward_count 11378  23.244%\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  0.036457/  0.074001, val:  57.08%, val_best:  57.92%, tr:  90.70%, tr_best:  90.70%, epoch time: 68.53 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0636%\n",
      "layer   2  Sparsity: 52.5790%\n",
      "layer   3  Sparsity: 52.0629%\n",
      "total_backward_count 58740 real_backward_count 13038  22.196%\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  0.035239/  0.087888, val:  43.75%, val_best:  57.92%, tr:  93.26%, tr_best:  93.26%, epoch time: 67.80 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0229%\n",
      "layer   2  Sparsity: 52.5888%\n",
      "layer   3  Sparsity: 51.6272%\n",
      "total_backward_count 68530 real_backward_count 14613  21.324%\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  0.034455/  0.060162, val:  60.42%, val_best:  60.42%, tr:  93.05%, tr_best:  93.26%, epoch time: 67.87 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0715%\n",
      "layer   2  Sparsity: 52.6571%\n",
      "layer   3  Sparsity: 51.2525%\n",
      "total_backward_count 78320 real_backward_count 16184  20.664%\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  0.033266/  0.061196, val:  57.50%, val_best:  60.42%, tr:  93.26%, tr_best:  93.26%, epoch time: 67.74 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 52.6575%\n",
      "layer   3  Sparsity: 50.7263%\n",
      "total_backward_count 88110 real_backward_count 17677  20.062%\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  0.033190/  0.068736, val:  57.08%, val_best:  60.42%, tr:  94.38%, tr_best:  94.38%, epoch time: 67.33 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0774%\n",
      "layer   2  Sparsity: 52.8826%\n",
      "layer   3  Sparsity: 50.4370%\n",
      "total_backward_count 97900 real_backward_count 19180  19.591%\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  0.031516/  0.071350, val:  61.25%, val_best:  61.25%, tr:  94.08%, tr_best:  94.38%, epoch time: 68.05 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0625%\n",
      "layer   2  Sparsity: 52.9630%\n",
      "layer   3  Sparsity: 50.3319%\n",
      "total_backward_count 107690 real_backward_count 20555  19.087%\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  0.030839/  0.064026, val:  62.92%, val_best:  62.92%, tr:  94.79%, tr_best:  94.79%, epoch time: 68.47 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   2  Sparsity: 52.9435%\n",
      "layer   3  Sparsity: 50.3479%\n",
      "total_backward_count 117480 real_backward_count 21917  18.656%\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  0.030016/  0.068032, val:  60.42%, val_best:  62.92%, tr:  96.12%, tr_best:  96.12%, epoch time: 67.93 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0910%\n",
      "layer   2  Sparsity: 53.0143%\n",
      "layer   3  Sparsity: 50.3539%\n",
      "total_backward_count 127270 real_backward_count 23208  18.235%\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  0.029068/  0.061525, val:  64.17%, val_best:  64.17%, tr:  95.71%, tr_best:  96.12%, epoch time: 67.61 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0888%\n",
      "layer   2  Sparsity: 53.0457%\n",
      "layer   3  Sparsity: 50.2297%\n",
      "total_backward_count 137060 real_backward_count 24492  17.870%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  0.028975/  0.068178, val:  57.50%, val_best:  64.17%, tr:  96.42%, tr_best:  96.42%, epoch time: 67.99 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   2  Sparsity: 53.1107%\n",
      "layer   3  Sparsity: 49.9611%\n",
      "total_backward_count 146850 real_backward_count 25739  17.527%\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  0.028357/  0.061804, val:  62.92%, val_best:  64.17%, tr:  97.04%, tr_best:  97.04%, epoch time: 68.40 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1249%\n",
      "layer   2  Sparsity: 53.1428%\n",
      "layer   3  Sparsity: 49.8788%\n",
      "total_backward_count 156640 real_backward_count 26965  17.215%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  0.027301/  0.058593, val:  69.17%, val_best:  69.17%, tr:  97.45%, tr_best:  97.45%, epoch time: 68.54 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   2  Sparsity: 53.1955%\n",
      "layer   3  Sparsity: 49.3412%\n",
      "total_backward_count 166430 real_backward_count 28133  16.904%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  0.026211/  0.061786, val:  67.08%, val_best:  69.17%, tr:  97.65%, tr_best:  97.65%, epoch time: 68.26 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   2  Sparsity: 53.2002%\n",
      "layer   3  Sparsity: 49.4980%\n",
      "total_backward_count 176220 real_backward_count 29211  16.576%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  0.026198/  0.068385, val:  60.83%, val_best:  69.17%, tr:  97.85%, tr_best:  97.85%, epoch time: 68.24 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0983%\n",
      "layer   2  Sparsity: 53.3196%\n",
      "layer   3  Sparsity: 49.4153%\n",
      "total_backward_count 186010 real_backward_count 30293  16.286%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  0.024707/  0.056144, val:  69.58%, val_best:  69.58%, tr:  99.39%, tr_best:  99.39%, epoch time: 68.19 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0569%\n",
      "layer   2  Sparsity: 53.2664%\n",
      "layer   3  Sparsity: 49.4197%\n",
      "total_backward_count 195800 real_backward_count 31259  15.965%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  0.024245/  0.056952, val:  78.33%, val_best:  78.33%, tr:  98.98%, tr_best:  99.39%, epoch time: 67.77 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0836%\n",
      "layer   2  Sparsity: 53.5130%\n",
      "layer   3  Sparsity: 49.2584%\n",
      "total_backward_count 205590 real_backward_count 32267  15.695%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  0.024415/  0.053403, val:  75.42%, val_best:  78.33%, tr:  98.98%, tr_best:  99.39%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0506%\n",
      "layer   2  Sparsity: 53.6342%\n",
      "layer   3  Sparsity: 49.2430%\n",
      "total_backward_count 215380 real_backward_count 33271  15.448%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  0.023291/  0.056803, val:  72.50%, val_best:  78.33%, tr:  99.18%, tr_best:  99.39%, epoch time: 67.91 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0955%\n",
      "layer   2  Sparsity: 53.4507%\n",
      "layer   3  Sparsity: 49.3371%\n",
      "total_backward_count 225170 real_backward_count 34213  15.194%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  0.023458/  0.048953, val:  79.17%, val_best:  79.17%, tr:  99.39%, tr_best:  99.39%, epoch time: 67.37 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   2  Sparsity: 53.5979%\n",
      "layer   3  Sparsity: 49.3326%\n",
      "total_backward_count 234960 real_backward_count 35195  14.979%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  0.022227/  0.052680, val:  78.33%, val_best:  79.17%, tr:  99.39%, tr_best:  99.39%, epoch time: 68.13 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0702%\n",
      "layer   2  Sparsity: 53.7470%\n",
      "layer   3  Sparsity: 49.2475%\n",
      "total_backward_count 244750 real_backward_count 36044  14.727%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  0.022761/  0.047581, val:  82.92%, val_best:  82.92%, tr:  98.88%, tr_best:  99.39%, epoch time: 68.13 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0541%\n",
      "layer   2  Sparsity: 53.7986%\n",
      "layer   3  Sparsity: 49.3920%\n",
      "total_backward_count 254540 real_backward_count 36954  14.518%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  0.022220/  0.053142, val:  77.08%, val_best:  82.92%, tr:  99.28%, tr_best:  99.39%, epoch time: 68.01 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0729%\n",
      "layer   2  Sparsity: 53.8749%\n",
      "layer   3  Sparsity: 49.5435%\n",
      "total_backward_count 264330 real_backward_count 37818  14.307%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  0.021192/  0.052419, val:  78.33%, val_best:  82.92%, tr:  99.08%, tr_best:  99.39%, epoch time: 68.20 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0956%\n",
      "layer   2  Sparsity: 53.7938%\n",
      "layer   3  Sparsity: 49.3012%\n",
      "total_backward_count 274120 real_backward_count 38648  14.099%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  0.021462/  0.044426, val:  82.08%, val_best:  82.92%, tr:  99.28%, tr_best:  99.39%, epoch time: 68.04 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0977%\n",
      "layer   2  Sparsity: 53.8214%\n",
      "layer   3  Sparsity: 49.1767%\n",
      "total_backward_count 283910 real_backward_count 39483  13.907%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  0.020768/  0.043655, val:  83.75%, val_best:  83.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0680%\n",
      "layer   2  Sparsity: 53.9013%\n",
      "layer   3  Sparsity: 49.2781%\n",
      "total_backward_count 293700 real_backward_count 40230  13.698%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  0.020085/  0.045807, val:  84.58%, val_best:  84.58%, tr:  99.28%, tr_best:  99.59%, epoch time: 67.67 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1065%\n",
      "layer   2  Sparsity: 54.1839%\n",
      "layer   3  Sparsity: 49.3233%\n",
      "total_backward_count 303490 real_backward_count 40998  13.509%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  0.019990/  0.047344, val:  78.33%, val_best:  84.58%, tr:  99.49%, tr_best:  99.59%, epoch time: 68.19 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0732%\n",
      "layer   2  Sparsity: 54.1784%\n",
      "layer   3  Sparsity: 49.2388%\n",
      "total_backward_count 313280 real_backward_count 41773  13.334%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  0.020200/  0.046209, val:  79.17%, val_best:  84.58%, tr:  99.49%, tr_best:  99.59%, epoch time: 68.16 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0621%\n",
      "layer   2  Sparsity: 54.0244%\n",
      "layer   3  Sparsity: 49.3124%\n",
      "total_backward_count 323070 real_backward_count 42603  13.187%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  0.019770/  0.048896, val:  77.50%, val_best:  84.58%, tr:  99.49%, tr_best:  99.59%, epoch time: 68.10 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0718%\n",
      "layer   2  Sparsity: 54.0877%\n",
      "layer   3  Sparsity: 49.5348%\n",
      "total_backward_count 332860 real_backward_count 43362  13.027%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  0.019794/  0.049319, val:  77.08%, val_best:  84.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 67.78 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0733%\n",
      "layer   2  Sparsity: 54.1599%\n",
      "layer   3  Sparsity: 49.3469%\n",
      "total_backward_count 342650 real_backward_count 44123  12.877%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  0.018822/  0.044347, val:  81.25%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 68.69 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 54.1305%\n",
      "layer   3  Sparsity: 49.1224%\n",
      "total_backward_count 352440 real_backward_count 44821  12.717%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  0.018133/  0.053235, val:  73.75%, val_best:  84.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 68.21 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0909%\n",
      "layer   2  Sparsity: 54.2361%\n",
      "layer   3  Sparsity: 49.3062%\n",
      "total_backward_count 362230 real_backward_count 45497  12.560%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  0.018843/  0.042782, val:  83.33%, val_best:  84.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 67.69 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0688%\n",
      "layer   2  Sparsity: 54.2186%\n",
      "layer   3  Sparsity: 49.5033%\n",
      "total_backward_count 372020 real_backward_count 46194  12.417%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  0.018058/  0.044287, val:  79.17%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 68.38 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1078%\n",
      "layer   2  Sparsity: 54.2225%\n",
      "layer   3  Sparsity: 49.4393%\n",
      "total_backward_count 381810 real_backward_count 46880  12.278%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  0.018287/  0.048781, val:  80.00%, val_best:  84.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 67.62 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   2  Sparsity: 54.2775%\n",
      "layer   3  Sparsity: 49.3204%\n",
      "total_backward_count 391600 real_backward_count 47591  12.153%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  0.017824/  0.044548, val:  82.92%, val_best:  84.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 67.58 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0374%\n",
      "layer   2  Sparsity: 54.2349%\n",
      "layer   3  Sparsity: 49.5356%\n",
      "total_backward_count 401390 real_backward_count 48278  12.028%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  0.017541/  0.046257, val:  79.17%, val_best:  84.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 68.10 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   2  Sparsity: 54.3608%\n",
      "layer   3  Sparsity: 49.5979%\n",
      "total_backward_count 411180 real_backward_count 48929  11.900%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  0.017304/  0.041908, val:  84.58%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 68.16 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   2  Sparsity: 54.4468%\n",
      "layer   3  Sparsity: 49.7261%\n",
      "total_backward_count 420970 real_backward_count 49568  11.775%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  0.017217/  0.042648, val:  84.58%, val_best:  84.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1399%\n",
      "layer   2  Sparsity: 54.4657%\n",
      "layer   3  Sparsity: 49.7940%\n",
      "total_backward_count 430760 real_backward_count 50203  11.655%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  0.017450/  0.045641, val:  80.42%, val_best:  84.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 68.35 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0891%\n",
      "layer   2  Sparsity: 54.5269%\n",
      "layer   3  Sparsity: 49.5207%\n",
      "total_backward_count 440550 real_backward_count 50859  11.544%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  0.016521/  0.039568, val:  87.08%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 67.70 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0962%\n",
      "layer   2  Sparsity: 54.4419%\n",
      "layer   3  Sparsity: 49.6105%\n",
      "total_backward_count 450340 real_backward_count 51489  11.433%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  0.017179/  0.045385, val:  80.42%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 67.87 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0691%\n",
      "layer   2  Sparsity: 54.6067%\n",
      "layer   3  Sparsity: 49.5128%\n",
      "total_backward_count 460130 real_backward_count 52116  11.326%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  0.016988/  0.040969, val:  85.83%, val_best:  87.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 68.13 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1213%\n",
      "layer   2  Sparsity: 54.6914%\n",
      "layer   3  Sparsity: 49.3232%\n",
      "total_backward_count 469920 real_backward_count 52751  11.226%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  0.015853/  0.041789, val:  85.42%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 68.11 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0375%\n",
      "layer   2  Sparsity: 54.4815%\n",
      "layer   3  Sparsity: 49.3636%\n",
      "total_backward_count 479710 real_backward_count 53286  11.108%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  0.016116/  0.041112, val:  86.67%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 68.32 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0879%\n",
      "layer   2  Sparsity: 54.5967%\n",
      "layer   3  Sparsity: 49.4837%\n",
      "total_backward_count 489500 real_backward_count 53866  11.004%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  0.015837/  0.050018, val:  76.67%, val_best:  87.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 67.91 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   2  Sparsity: 54.7406%\n",
      "layer   3  Sparsity: 49.6007%\n",
      "total_backward_count 499290 real_backward_count 54428  10.901%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  0.016287/  0.042117, val:  85.00%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0486%\n",
      "layer   2  Sparsity: 54.6250%\n",
      "layer   3  Sparsity: 49.6432%\n",
      "total_backward_count 509080 real_backward_count 55055  10.815%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  0.015934/  0.040509, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.00 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0907%\n",
      "layer   2  Sparsity: 54.6985%\n",
      "layer   3  Sparsity: 49.4989%\n",
      "total_backward_count 518870 real_backward_count 55666  10.728%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  0.015512/  0.042867, val:  85.00%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 68.02 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0801%\n",
      "layer   2  Sparsity: 54.7087%\n",
      "layer   3  Sparsity: 49.4490%\n",
      "total_backward_count 528660 real_backward_count 56233  10.637%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  0.015985/  0.051293, val:  82.50%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.33 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0820%\n",
      "layer   2  Sparsity: 54.7132%\n",
      "layer   3  Sparsity: 49.4358%\n",
      "total_backward_count 538450 real_backward_count 56845  10.557%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  0.015387/  0.047995, val:  82.08%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 68.59 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0719%\n",
      "layer   2  Sparsity: 54.8453%\n",
      "layer   3  Sparsity: 49.7205%\n",
      "total_backward_count 548240 real_backward_count 57407  10.471%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  0.015155/  0.041424, val:  82.08%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.32 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1148%\n",
      "layer   2  Sparsity: 54.7494%\n",
      "layer   3  Sparsity: 49.6122%\n",
      "total_backward_count 558030 real_backward_count 57965  10.387%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  0.015092/  0.044637, val:  85.42%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 68.63 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0686%\n",
      "layer   2  Sparsity: 54.9382%\n",
      "layer   3  Sparsity: 49.4928%\n",
      "total_backward_count 567820 real_backward_count 58523  10.307%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  0.015202/  0.039469, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.71 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0517%\n",
      "layer   2  Sparsity: 55.0198%\n",
      "layer   3  Sparsity: 49.5556%\n",
      "total_backward_count 577610 real_backward_count 59057  10.224%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  0.014692/  0.041424, val:  86.25%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.23 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0311%\n",
      "layer   2  Sparsity: 54.8596%\n",
      "layer   3  Sparsity: 49.6252%\n",
      "total_backward_count 587400 real_backward_count 59590  10.145%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  0.014529/  0.042178, val:  84.58%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 64.95 seconds, 1.08 minutes\n",
      "layer   1  Sparsity: 91.1041%\n",
      "layer   2  Sparsity: 54.7858%\n",
      "layer   3  Sparsity: 49.5993%\n",
      "total_backward_count 597190 real_backward_count 60119  10.067%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  0.014378/  0.037590, val:  88.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 65.54 seconds, 1.09 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   2  Sparsity: 54.7950%\n",
      "layer   3  Sparsity: 49.4653%\n",
      "total_backward_count 606980 real_backward_count 60658   9.993%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  0.014652/  0.044403, val:  84.17%, val_best:  88.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 67.79 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0988%\n",
      "layer   2  Sparsity: 54.8875%\n",
      "layer   3  Sparsity: 49.3686%\n",
      "total_backward_count 616770 real_backward_count 61204   9.923%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  0.014111/  0.039493, val:  87.50%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.07 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 55.0277%\n",
      "layer   3  Sparsity: 49.3244%\n",
      "total_backward_count 626560 real_backward_count 61725   9.851%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  0.013603/  0.041601, val:  84.58%, val_best:  88.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 68.12 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0539%\n",
      "layer   2  Sparsity: 54.9847%\n",
      "layer   3  Sparsity: 49.3067%\n",
      "total_backward_count 636350 real_backward_count 62208   9.776%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  0.013956/  0.046134, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.62 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1055%\n",
      "layer   2  Sparsity: 55.0885%\n",
      "layer   3  Sparsity: 49.1879%\n",
      "total_backward_count 646140 real_backward_count 62728   9.708%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  0.013837/  0.037161, val:  87.08%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.28 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   2  Sparsity: 55.1032%\n",
      "layer   3  Sparsity: 49.3299%\n",
      "total_backward_count 655930 real_backward_count 63231   9.640%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  0.013586/  0.043916, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.99 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1000%\n",
      "layer   2  Sparsity: 55.1452%\n",
      "layer   3  Sparsity: 49.3406%\n",
      "total_backward_count 665720 real_backward_count 63739   9.574%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  0.014105/  0.039510, val:  85.42%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.55 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   2  Sparsity: 55.2345%\n",
      "layer   3  Sparsity: 49.4000%\n",
      "total_backward_count 675510 real_backward_count 64252   9.512%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  0.013377/  0.037967, val:  87.50%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.92 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   2  Sparsity: 55.2015%\n",
      "layer   3  Sparsity: 49.6098%\n",
      "total_backward_count 685300 real_backward_count 64728   9.445%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  0.013097/  0.038958, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.40 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   2  Sparsity: 55.2002%\n",
      "layer   3  Sparsity: 49.5820%\n",
      "total_backward_count 695090 real_backward_count 65185   9.378%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  0.013567/  0.038253, val:  88.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.92 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0682%\n",
      "layer   2  Sparsity: 55.2864%\n",
      "layer   3  Sparsity: 49.5620%\n",
      "total_backward_count 704880 real_backward_count 65676   9.317%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  0.013159/  0.038446, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.96 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0631%\n",
      "layer   2  Sparsity: 55.2493%\n",
      "layer   3  Sparsity: 49.4409%\n",
      "total_backward_count 714670 real_backward_count 66106   9.250%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  0.013010/  0.063393, val:  70.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.77 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   2  Sparsity: 55.1929%\n",
      "layer   3  Sparsity: 49.5347%\n",
      "total_backward_count 724460 real_backward_count 66569   9.189%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  0.013171/  0.039998, val:  86.67%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.36 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0728%\n",
      "layer   2  Sparsity: 55.3255%\n",
      "layer   3  Sparsity: 49.4177%\n",
      "total_backward_count 734250 real_backward_count 67047   9.131%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  0.012984/  0.037949, val:  89.58%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.65 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0783%\n",
      "layer   2  Sparsity: 55.4612%\n",
      "layer   3  Sparsity: 49.4910%\n",
      "total_backward_count 744040 real_backward_count 67512   9.074%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  0.012840/  0.038971, val:  86.25%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.85 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0371%\n",
      "layer   2  Sparsity: 55.4028%\n",
      "layer   3  Sparsity: 49.5453%\n",
      "total_backward_count 753830 real_backward_count 67974   9.017%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  0.012707/  0.036664, val:  88.33%, val_best:  89.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.42 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1082%\n",
      "layer   2  Sparsity: 55.3909%\n",
      "layer   3  Sparsity: 49.5815%\n",
      "total_backward_count 763620 real_backward_count 68430   8.961%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  0.013006/  0.038493, val:  87.50%, val_best:  89.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.19 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   2  Sparsity: 55.3897%\n",
      "layer   3  Sparsity: 49.5919%\n",
      "total_backward_count 773410 real_backward_count 68915   8.911%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  0.012942/  0.041406, val:  85.00%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.11 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0809%\n",
      "layer   2  Sparsity: 55.4315%\n",
      "layer   3  Sparsity: 49.5607%\n",
      "total_backward_count 783200 real_backward_count 69391   8.860%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  0.012960/  0.038262, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.92 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0497%\n",
      "layer   2  Sparsity: 55.3932%\n",
      "layer   3  Sparsity: 49.4817%\n",
      "total_backward_count 792990 real_backward_count 69870   8.811%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  0.012695/  0.037567, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.10 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0392%\n",
      "layer   2  Sparsity: 55.3970%\n",
      "layer   3  Sparsity: 49.3326%\n",
      "total_backward_count 802780 real_backward_count 70318   8.759%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  0.012832/  0.036390, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.01 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   2  Sparsity: 55.5053%\n",
      "layer   3  Sparsity: 49.3919%\n",
      "total_backward_count 812570 real_backward_count 70793   8.712%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  0.012708/  0.038634, val:  85.83%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.20 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0752%\n",
      "layer   2  Sparsity: 55.5429%\n",
      "layer   3  Sparsity: 49.5892%\n",
      "total_backward_count 822360 real_backward_count 71262   8.666%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  0.012377/  0.036132, val:  86.25%, val_best:  89.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.23 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 55.7576%\n",
      "layer   3  Sparsity: 49.5395%\n",
      "total_backward_count 832150 real_backward_count 71716   8.618%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  0.012365/  0.045530, val:  81.67%, val_best:  89.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.16 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1036%\n",
      "layer   2  Sparsity: 55.8122%\n",
      "layer   3  Sparsity: 49.4113%\n",
      "total_backward_count 841940 real_backward_count 72172   8.572%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  0.012321/  0.038790, val:  84.58%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.99 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 55.7585%\n",
      "layer   3  Sparsity: 49.3765%\n",
      "total_backward_count 851730 real_backward_count 72600   8.524%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  0.012279/  0.036737, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.05 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0740%\n",
      "layer   2  Sparsity: 55.7244%\n",
      "layer   3  Sparsity: 49.5051%\n",
      "total_backward_count 861520 real_backward_count 73069   8.481%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  0.012025/  0.035825, val:  89.17%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.22 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0872%\n",
      "layer   2  Sparsity: 55.7164%\n",
      "layer   3  Sparsity: 49.5364%\n",
      "total_backward_count 871310 real_backward_count 73512   8.437%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  0.011481/  0.039261, val:  86.67%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.19 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0605%\n",
      "layer   2  Sparsity: 55.7041%\n",
      "layer   3  Sparsity: 49.5195%\n",
      "total_backward_count 881100 real_backward_count 73890   8.386%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  0.011966/  0.039175, val:  85.83%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.13 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0821%\n",
      "layer   2  Sparsity: 55.7888%\n",
      "layer   3  Sparsity: 49.3954%\n",
      "total_backward_count 890890 real_backward_count 74326   8.343%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  0.011696/  0.036870, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.85 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   2  Sparsity: 55.6733%\n",
      "layer   3  Sparsity: 49.4802%\n",
      "total_backward_count 900680 real_backward_count 74747   8.299%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  0.011854/  0.037436, val:  86.67%, val_best:  89.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.73 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0997%\n",
      "layer   2  Sparsity: 55.7331%\n",
      "layer   3  Sparsity: 49.3415%\n",
      "total_backward_count 910470 real_backward_count 75182   8.257%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  0.011526/  0.037859, val:  87.50%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.48 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0485%\n",
      "layer   2  Sparsity: 55.6591%\n",
      "layer   3  Sparsity: 49.2030%\n",
      "total_backward_count 920260 real_backward_count 75568   8.212%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  0.011555/  0.036788, val:  87.50%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.21 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0493%\n",
      "layer   2  Sparsity: 55.6553%\n",
      "layer   3  Sparsity: 49.2042%\n",
      "total_backward_count 930050 real_backward_count 75977   8.169%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  0.011605/  0.035865, val:  90.42%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.96 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0381%\n",
      "layer   2  Sparsity: 55.6267%\n",
      "layer   3  Sparsity: 49.2578%\n",
      "total_backward_count 939840 real_backward_count 76400   8.129%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  0.011365/  0.037733, val:  86.67%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.24 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0687%\n",
      "layer   2  Sparsity: 55.6629%\n",
      "layer   3  Sparsity: 49.4612%\n",
      "total_backward_count 949630 real_backward_count 76812   8.089%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  0.011308/  0.036920, val:  90.42%, val_best:  90.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 68.15 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1001%\n",
      "layer   2  Sparsity: 55.8431%\n",
      "layer   3  Sparsity: 49.4893%\n",
      "total_backward_count 959420 real_backward_count 77196   8.046%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  0.011493/  0.036367, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.41 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0486%\n",
      "layer   2  Sparsity: 55.7655%\n",
      "layer   3  Sparsity: 49.5386%\n",
      "total_backward_count 969210 real_backward_count 77594   8.006%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  0.010852/  0.035278, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.76 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0585%\n",
      "layer   2  Sparsity: 55.7325%\n",
      "layer   3  Sparsity: 49.4652%\n",
      "total_backward_count 979000 real_backward_count 77960   7.963%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  0.010617/  0.042434, val:  82.92%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.52 seconds, 1.06 minutes\n",
      "layer   1  Sparsity: 91.0558%\n",
      "layer   2  Sparsity: 55.7846%\n",
      "layer   3  Sparsity: 49.4370%\n",
      "total_backward_count 988790 real_backward_count 78306   7.919%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  0.011006/  0.045546, val:  85.00%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.78 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0836%\n",
      "layer   2  Sparsity: 55.9000%\n",
      "layer   3  Sparsity: 49.6824%\n",
      "total_backward_count 998580 real_backward_count 78697   7.881%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  0.010695/  0.039305, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.79 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   2  Sparsity: 55.8495%\n",
      "layer   3  Sparsity: 49.7054%\n",
      "total_backward_count 1008370 real_backward_count 79062   7.841%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  0.010917/  0.036108, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.63 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0788%\n",
      "layer   2  Sparsity: 55.8857%\n",
      "layer   3  Sparsity: 49.5520%\n",
      "total_backward_count 1018160 real_backward_count 79435   7.802%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  0.010908/  0.037177, val:  86.25%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.29 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0592%\n",
      "layer   2  Sparsity: 55.9731%\n",
      "layer   3  Sparsity: 49.6486%\n",
      "total_backward_count 1027950 real_backward_count 79840   7.767%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  0.010644/  0.035509, val:  87.08%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.89 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0558%\n",
      "layer   2  Sparsity: 55.9471%\n",
      "layer   3  Sparsity: 49.5828%\n",
      "total_backward_count 1037740 real_backward_count 80193   7.728%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  0.010142/  0.036062, val:  86.67%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.65 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0532%\n",
      "layer   2  Sparsity: 56.0026%\n",
      "layer   3  Sparsity: 49.5367%\n",
      "total_backward_count 1047530 real_backward_count 80547   7.689%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  0.010236/  0.036212, val:  86.25%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.61 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0627%\n",
      "layer   2  Sparsity: 55.9750%\n",
      "layer   3  Sparsity: 49.4979%\n",
      "total_backward_count 1057320 real_backward_count 80881   7.650%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  0.011198/  0.037479, val:  86.25%, val_best:  90.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.99 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0168%\n",
      "layer   2  Sparsity: 56.0954%\n",
      "layer   3  Sparsity: 49.5391%\n",
      "total_backward_count 1067110 real_backward_count 81289   7.618%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  0.010715/  0.038000, val:  87.50%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.23 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0954%\n",
      "layer   2  Sparsity: 56.0302%\n",
      "layer   3  Sparsity: 49.6178%\n",
      "total_backward_count 1076900 real_backward_count 81652   7.582%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  0.010310/  0.034641, val:  88.75%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.42 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0687%\n",
      "layer   2  Sparsity: 56.0489%\n",
      "layer   3  Sparsity: 49.5498%\n",
      "total_backward_count 1086690 real_backward_count 82008   7.547%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  0.010447/  0.038835, val:  85.42%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.74 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0754%\n",
      "layer   2  Sparsity: 56.0902%\n",
      "layer   3  Sparsity: 49.5607%\n",
      "total_backward_count 1096480 real_backward_count 82377   7.513%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  0.010632/  0.036589, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.73 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   2  Sparsity: 56.1205%\n",
      "layer   3  Sparsity: 49.5885%\n",
      "total_backward_count 1106270 real_backward_count 82767   7.482%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  0.010610/  0.035063, val:  87.08%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.26 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1007%\n",
      "layer   2  Sparsity: 56.1668%\n",
      "layer   3  Sparsity: 49.5995%\n",
      "total_backward_count 1116060 real_backward_count 83137   7.449%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  0.009912/  0.033890, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.94 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0863%\n",
      "layer   2  Sparsity: 56.2632%\n",
      "layer   3  Sparsity: 49.7825%\n",
      "total_backward_count 1125850 real_backward_count 83477   7.415%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  0.009963/  0.036145, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0976%\n",
      "layer   2  Sparsity: 56.3522%\n",
      "layer   3  Sparsity: 49.7910%\n",
      "total_backward_count 1135640 real_backward_count 83835   7.382%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  0.009980/  0.035341, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.64 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   2  Sparsity: 56.2627%\n",
      "layer   3  Sparsity: 49.7701%\n",
      "total_backward_count 1145430 real_backward_count 84191   7.350%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  0.010048/  0.034473, val:  90.42%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.89 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0662%\n",
      "layer   2  Sparsity: 56.2521%\n",
      "layer   3  Sparsity: 49.8111%\n",
      "total_backward_count 1155220 real_backward_count 84532   7.317%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  0.010220/  0.035154, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.26 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0665%\n",
      "layer   2  Sparsity: 56.2402%\n",
      "layer   3  Sparsity: 49.7247%\n",
      "total_backward_count 1165010 real_backward_count 84895   7.287%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  0.009585/  0.034468, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.59 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.1079%\n",
      "layer   2  Sparsity: 56.2662%\n",
      "layer   3  Sparsity: 49.6589%\n",
      "total_backward_count 1174800 real_backward_count 85224   7.254%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  0.010370/  0.035337, val:  87.08%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 69.11 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 56.2964%\n",
      "layer   3  Sparsity: 49.5771%\n",
      "total_backward_count 1184590 real_backward_count 85603   7.226%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  0.009855/  0.034420, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.48 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   2  Sparsity: 56.2386%\n",
      "layer   3  Sparsity: 49.6831%\n",
      "total_backward_count 1194380 real_backward_count 85954   7.197%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  0.009801/  0.036558, val:  87.50%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.62 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1061%\n",
      "layer   2  Sparsity: 56.2715%\n",
      "layer   3  Sparsity: 49.6548%\n",
      "total_backward_count 1204170 real_backward_count 86277   7.165%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  0.009676/  0.034694, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.90 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0732%\n",
      "layer   2  Sparsity: 56.2656%\n",
      "layer   3  Sparsity: 49.7583%\n",
      "total_backward_count 1213960 real_backward_count 86592   7.133%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  0.009636/  0.035727, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.41 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   2  Sparsity: 56.2430%\n",
      "layer   3  Sparsity: 49.7879%\n",
      "total_backward_count 1223750 real_backward_count 86905   7.102%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  0.009668/  0.034456, val:  89.17%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.02 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   2  Sparsity: 56.0809%\n",
      "layer   3  Sparsity: 49.7397%\n",
      "total_backward_count 1233540 real_backward_count 87245   7.073%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  0.009592/  0.036254, val:  86.67%, val_best:  90.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.49 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0509%\n",
      "layer   2  Sparsity: 56.1764%\n",
      "layer   3  Sparsity: 49.6136%\n",
      "total_backward_count 1243330 real_backward_count 87577   7.044%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  0.009662/  0.039662, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.91 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0665%\n",
      "layer   2  Sparsity: 56.1581%\n",
      "layer   3  Sparsity: 49.7449%\n",
      "total_backward_count 1253120 real_backward_count 87923   7.016%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  0.009076/  0.034976, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.27 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0958%\n",
      "layer   2  Sparsity: 56.1775%\n",
      "layer   3  Sparsity: 49.7852%\n",
      "total_backward_count 1262910 real_backward_count 88222   6.986%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  0.009666/  0.036068, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.14 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   2  Sparsity: 56.1637%\n",
      "layer   3  Sparsity: 49.7571%\n",
      "total_backward_count 1272700 real_backward_count 88559   6.958%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  0.009436/  0.036545, val:  90.00%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.95 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0618%\n",
      "layer   2  Sparsity: 56.2153%\n",
      "layer   3  Sparsity: 49.7030%\n",
      "total_backward_count 1282490 real_backward_count 88906   6.932%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  0.009156/  0.036393, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.02 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   2  Sparsity: 56.2333%\n",
      "layer   3  Sparsity: 49.5170%\n",
      "total_backward_count 1292280 real_backward_count 89205   6.903%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  0.009289/  0.036044, val:  88.33%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.38 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1024%\n",
      "layer   2  Sparsity: 56.3060%\n",
      "layer   3  Sparsity: 49.6349%\n",
      "total_backward_count 1302070 real_backward_count 89511   6.875%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  0.009453/  0.036291, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.13 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1001%\n",
      "layer   2  Sparsity: 56.2385%\n",
      "layer   3  Sparsity: 49.9343%\n",
      "total_backward_count 1311860 real_backward_count 89868   6.850%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  0.009046/  0.035717, val:  87.50%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.30 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0877%\n",
      "layer   2  Sparsity: 56.2919%\n",
      "layer   3  Sparsity: 49.8551%\n",
      "total_backward_count 1321650 real_backward_count 90159   6.822%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  0.009021/  0.037959, val:  86.25%, val_best:  90.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.80 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0466%\n",
      "layer   2  Sparsity: 56.3429%\n",
      "layer   3  Sparsity: 49.7102%\n",
      "total_backward_count 1331440 real_backward_count 90467   6.795%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  0.009023/  0.035465, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.58 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0962%\n",
      "layer   2  Sparsity: 56.4157%\n",
      "layer   3  Sparsity: 49.7066%\n",
      "total_backward_count 1341230 real_backward_count 90774   6.768%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  0.008889/  0.035231, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   2  Sparsity: 56.2771%\n",
      "layer   3  Sparsity: 49.7740%\n",
      "total_backward_count 1351020 real_backward_count 91083   6.742%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  0.009229/  0.042118, val:  85.00%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.94 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0603%\n",
      "layer   2  Sparsity: 56.2870%\n",
      "layer   3  Sparsity: 49.7973%\n",
      "total_backward_count 1360810 real_backward_count 91403   6.717%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  0.008804/  0.035794, val:  87.92%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.55 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0820%\n",
      "layer   2  Sparsity: 56.2717%\n",
      "layer   3  Sparsity: 49.7304%\n",
      "total_backward_count 1370600 real_backward_count 91699   6.690%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  0.008656/  0.036468, val:  87.92%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.43 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0850%\n",
      "layer   2  Sparsity: 56.2759%\n",
      "layer   3  Sparsity: 49.6711%\n",
      "total_backward_count 1380390 real_backward_count 91990   6.664%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  0.008930/  0.034733, val:  87.08%, val_best:  90.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.86 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0324%\n",
      "layer   2  Sparsity: 56.2569%\n",
      "layer   3  Sparsity: 49.6741%\n",
      "total_backward_count 1390180 real_backward_count 92312   6.640%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  0.008795/  0.034676, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   2  Sparsity: 56.3247%\n",
      "layer   3  Sparsity: 49.6669%\n",
      "total_backward_count 1399970 real_backward_count 92611   6.615%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  0.009149/  0.034236, val:  88.33%, val_best:  90.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.05 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   2  Sparsity: 56.3935%\n",
      "layer   3  Sparsity: 49.7080%\n",
      "total_backward_count 1409760 real_backward_count 92945   6.593%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  0.008733/  0.033725, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.06 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0710%\n",
      "layer   2  Sparsity: 56.4704%\n",
      "layer   3  Sparsity: 49.7008%\n",
      "total_backward_count 1419550 real_backward_count 93227   6.567%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  0.008616/  0.036069, val:  88.33%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.94 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0897%\n",
      "layer   2  Sparsity: 56.4941%\n",
      "layer   3  Sparsity: 49.8609%\n",
      "total_backward_count 1429340 real_backward_count 93513   6.542%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  0.008520/  0.035037, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.44 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   2  Sparsity: 56.4184%\n",
      "layer   3  Sparsity: 49.8699%\n",
      "total_backward_count 1439130 real_backward_count 93810   6.519%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  0.008558/  0.036124, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   2  Sparsity: 56.4646%\n",
      "layer   3  Sparsity: 49.7904%\n",
      "total_backward_count 1448920 real_backward_count 94096   6.494%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  0.008338/  0.034243, val:  88.33%, val_best:  90.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.30 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1094%\n",
      "layer   2  Sparsity: 56.5013%\n",
      "layer   3  Sparsity: 49.9179%\n",
      "total_backward_count 1458710 real_backward_count 94385   6.470%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  0.008502/  0.035530, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0419%\n",
      "layer   2  Sparsity: 56.3740%\n",
      "layer   3  Sparsity: 49.8224%\n",
      "total_backward_count 1468500 real_backward_count 94665   6.446%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  0.008335/  0.035064, val:  89.17%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.67 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0677%\n",
      "layer   2  Sparsity: 56.4071%\n",
      "layer   3  Sparsity: 49.8877%\n",
      "total_backward_count 1478290 real_backward_count 94951   6.423%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  0.008641/  0.034380, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.08 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0773%\n",
      "layer   2  Sparsity: 56.4292%\n",
      "layer   3  Sparsity: 49.9509%\n",
      "total_backward_count 1488080 real_backward_count 95248   6.401%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  0.008651/  0.034812, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.80 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   2  Sparsity: 56.4475%\n",
      "layer   3  Sparsity: 49.8747%\n",
      "total_backward_count 1497870 real_backward_count 95545   6.379%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  0.008262/  0.034146, val:  89.17%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.49 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1121%\n",
      "layer   2  Sparsity: 56.3509%\n",
      "layer   3  Sparsity: 49.8632%\n",
      "total_backward_count 1507660 real_backward_count 95815   6.355%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  0.008314/  0.036831, val:  87.92%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.08 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   2  Sparsity: 56.3663%\n",
      "layer   3  Sparsity: 49.8775%\n",
      "total_backward_count 1517450 real_backward_count 96071   6.331%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  0.008255/  0.034275, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.91 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0991%\n",
      "layer   2  Sparsity: 56.4219%\n",
      "layer   3  Sparsity: 49.8122%\n",
      "total_backward_count 1527240 real_backward_count 96338   6.308%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  0.008135/  0.034859, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.38 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0997%\n",
      "layer   2  Sparsity: 56.4658%\n",
      "layer   3  Sparsity: 49.7739%\n",
      "total_backward_count 1537030 real_backward_count 96604   6.285%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  0.008482/  0.041768, val:  83.75%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.47 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0799%\n",
      "layer   2  Sparsity: 56.4639%\n",
      "layer   3  Sparsity: 49.8120%\n",
      "total_backward_count 1546820 real_backward_count 96913   6.265%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  0.008032/  0.039821, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.02 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0677%\n",
      "layer   2  Sparsity: 56.4676%\n",
      "layer   3  Sparsity: 49.9421%\n",
      "total_backward_count 1556610 real_backward_count 97184   6.243%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  0.008024/  0.039918, val:  83.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.12 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0662%\n",
      "layer   2  Sparsity: 56.4613%\n",
      "layer   3  Sparsity: 49.9762%\n",
      "total_backward_count 1566400 real_backward_count 97468   6.222%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  0.007783/  0.035008, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.58 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0623%\n",
      "layer   2  Sparsity: 56.4096%\n",
      "layer   3  Sparsity: 50.0037%\n",
      "total_backward_count 1576190 real_backward_count 97708   6.199%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  0.008656/  0.036540, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.23 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   2  Sparsity: 56.4057%\n",
      "layer   3  Sparsity: 49.8552%\n",
      "total_backward_count 1585980 real_backward_count 98008   6.180%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  0.008057/  0.035864, val:  86.25%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.66 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 56.4429%\n",
      "layer   3  Sparsity: 49.8793%\n",
      "total_backward_count 1595770 real_backward_count 98281   6.159%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  0.008060/  0.036900, val:  87.08%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.05 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1025%\n",
      "layer   2  Sparsity: 56.3181%\n",
      "layer   3  Sparsity: 50.0177%\n",
      "total_backward_count 1605560 real_backward_count 98555   6.138%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  0.008071/  0.034005, val:  89.17%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.66 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0506%\n",
      "layer   2  Sparsity: 56.3255%\n",
      "layer   3  Sparsity: 49.9315%\n",
      "total_backward_count 1615350 real_backward_count 98828   6.118%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  0.007720/  0.035418, val:  89.17%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.99 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0951%\n",
      "layer   2  Sparsity: 56.3661%\n",
      "layer   3  Sparsity: 49.9751%\n",
      "total_backward_count 1625140 real_backward_count 99079   6.097%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  0.007885/  0.035646, val:  86.25%, val_best:  90.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.06 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   2  Sparsity: 56.4419%\n",
      "layer   3  Sparsity: 50.0029%\n",
      "total_backward_count 1634930 real_backward_count 99343   6.076%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  0.007916/  0.038286, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.95 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1058%\n",
      "layer   2  Sparsity: 56.3445%\n",
      "layer   3  Sparsity: 49.9534%\n",
      "total_backward_count 1644720 real_backward_count 99596   6.055%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  0.007794/  0.034729, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.35 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   2  Sparsity: 56.4447%\n",
      "layer   3  Sparsity: 49.8971%\n",
      "total_backward_count 1654510 real_backward_count 99862   6.036%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  0.007877/  0.034403, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.10 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0680%\n",
      "layer   2  Sparsity: 56.4530%\n",
      "layer   3  Sparsity: 49.9537%\n",
      "total_backward_count 1664300 real_backward_count 100138   6.017%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  0.007608/  0.033713, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.42 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0682%\n",
      "layer   2  Sparsity: 56.4586%\n",
      "layer   3  Sparsity: 49.9990%\n",
      "total_backward_count 1674090 real_backward_count 100389   5.997%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  0.007795/  0.034188, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0636%\n",
      "layer   2  Sparsity: 56.5037%\n",
      "layer   3  Sparsity: 50.0669%\n",
      "total_backward_count 1683880 real_backward_count 100625   5.976%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  0.006960/  0.034773, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.00 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0531%\n",
      "layer   2  Sparsity: 56.6003%\n",
      "layer   3  Sparsity: 50.1789%\n",
      "total_backward_count 1693670 real_backward_count 100844   5.954%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  0.008131/  0.034774, val:  87.92%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.80 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   2  Sparsity: 56.5956%\n",
      "layer   3  Sparsity: 50.1147%\n",
      "total_backward_count 1703460 real_backward_count 101111   5.936%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  0.007148/  0.034244, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.54 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0712%\n",
      "layer   2  Sparsity: 56.5328%\n",
      "layer   3  Sparsity: 50.0487%\n",
      "total_backward_count 1713250 real_backward_count 101341   5.915%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  0.007411/  0.034178, val:  88.75%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.64 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   2  Sparsity: 56.3630%\n",
      "layer   3  Sparsity: 50.0338%\n",
      "total_backward_count 1723040 real_backward_count 101588   5.896%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  0.007462/  0.035563, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.03 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0703%\n",
      "layer   2  Sparsity: 56.4562%\n",
      "layer   3  Sparsity: 50.1292%\n",
      "total_backward_count 1732830 real_backward_count 101838   5.877%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  0.007872/  0.034131, val:  88.33%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.72 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   2  Sparsity: 56.5802%\n",
      "layer   3  Sparsity: 50.2661%\n",
      "total_backward_count 1742620 real_backward_count 102100   5.859%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  0.007128/  0.034131, val:  87.92%, val_best:  90.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0482%\n",
      "layer   2  Sparsity: 56.6851%\n",
      "layer   3  Sparsity: 50.2273%\n",
      "total_backward_count 1752410 real_backward_count 102320   5.839%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  0.007423/  0.034373, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.10 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0913%\n",
      "layer   2  Sparsity: 56.6142%\n",
      "layer   3  Sparsity: 50.0748%\n",
      "total_backward_count 1762200 real_backward_count 102560   5.820%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  0.007380/  0.034363, val:  88.33%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.94 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   2  Sparsity: 56.5436%\n",
      "layer   3  Sparsity: 50.0932%\n",
      "total_backward_count 1771990 real_backward_count 102810   5.802%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  0.007637/  0.034780, val:  89.17%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.14 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0905%\n",
      "layer   2  Sparsity: 56.5191%\n",
      "layer   3  Sparsity: 50.0429%\n",
      "total_backward_count 1781780 real_backward_count 103078   5.785%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  0.007311/  0.034480, val:  88.75%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.08 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0410%\n",
      "layer   2  Sparsity: 56.6062%\n",
      "layer   3  Sparsity: 50.0304%\n",
      "total_backward_count 1791570 real_backward_count 103325   5.767%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  0.007713/  0.034761, val:  87.08%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.85 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0776%\n",
      "layer   2  Sparsity: 56.5614%\n",
      "layer   3  Sparsity: 50.0414%\n",
      "total_backward_count 1801360 real_backward_count 103583   5.750%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  0.007590/  0.034365, val:  89.17%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.65 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0718%\n",
      "layer   2  Sparsity: 56.5141%\n",
      "layer   3  Sparsity: 50.0801%\n",
      "total_backward_count 1811150 real_backward_count 103833   5.733%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  0.007454/  0.034755, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.34 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0516%\n",
      "layer   2  Sparsity: 56.5594%\n",
      "layer   3  Sparsity: 50.1880%\n",
      "total_backward_count 1820940 real_backward_count 104086   5.716%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  0.007051/  0.033966, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.78 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1160%\n",
      "layer   2  Sparsity: 56.5662%\n",
      "layer   3  Sparsity: 50.2385%\n",
      "total_backward_count 1830730 real_backward_count 104330   5.699%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  0.006917/  0.034987, val:  87.08%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.00 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0987%\n",
      "layer   2  Sparsity: 56.5882%\n",
      "layer   3  Sparsity: 50.2963%\n",
      "total_backward_count 1840520 real_backward_count 104549   5.680%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  0.007611/  0.034501, val:  88.33%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.22 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0615%\n",
      "layer   2  Sparsity: 56.5798%\n",
      "layer   3  Sparsity: 50.2574%\n",
      "total_backward_count 1850310 real_backward_count 104810   5.664%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  0.007153/  0.033658, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.19 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0473%\n",
      "layer   2  Sparsity: 56.5472%\n",
      "layer   3  Sparsity: 50.3979%\n",
      "total_backward_count 1860100 real_backward_count 105023   5.646%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  0.007180/  0.034391, val:  88.75%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.06 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0131%\n",
      "layer   2  Sparsity: 56.5307%\n",
      "layer   3  Sparsity: 50.4932%\n",
      "total_backward_count 1869890 real_backward_count 105255   5.629%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  0.007193/  0.034474, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.59 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   2  Sparsity: 56.5899%\n",
      "layer   3  Sparsity: 50.3672%\n",
      "total_backward_count 1879680 real_backward_count 105491   5.612%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  0.007263/  0.034144, val:  89.17%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.12 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   2  Sparsity: 56.5621%\n",
      "layer   3  Sparsity: 50.3848%\n",
      "total_backward_count 1889470 real_backward_count 105729   5.596%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  0.006965/  0.034861, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.22 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0203%\n",
      "layer   2  Sparsity: 56.5715%\n",
      "layer   3  Sparsity: 50.4382%\n",
      "total_backward_count 1899260 real_backward_count 105943   5.578%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  0.007197/  0.036520, val:  87.50%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.87 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0499%\n",
      "layer   2  Sparsity: 56.5766%\n",
      "layer   3  Sparsity: 50.3865%\n",
      "total_backward_count 1909050 real_backward_count 106169   5.561%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  0.006975/  0.034406, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.06 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0927%\n",
      "layer   2  Sparsity: 56.6408%\n",
      "layer   3  Sparsity: 50.4252%\n",
      "total_backward_count 1918840 real_backward_count 106382   5.544%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  0.007110/  0.038198, val:  84.17%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.31 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0699%\n",
      "layer   2  Sparsity: 56.6532%\n",
      "layer   3  Sparsity: 50.4130%\n",
      "total_backward_count 1928630 real_backward_count 106605   5.527%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  0.006795/  0.032953, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.14 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   2  Sparsity: 56.6788%\n",
      "layer   3  Sparsity: 50.4779%\n",
      "total_backward_count 1938420 real_backward_count 106809   5.510%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  0.006927/  0.034389, val:  87.50%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.07 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   2  Sparsity: 56.7187%\n",
      "layer   3  Sparsity: 50.3381%\n",
      "total_backward_count 1948210 real_backward_count 107052   5.495%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  0.007151/  0.034820, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.19 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0405%\n",
      "layer   2  Sparsity: 56.7829%\n",
      "layer   3  Sparsity: 50.5718%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b5cc6f42e9494680e9e5b4147627fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00715</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>0.03482</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jolly-sweep-3</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ttur70n8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ttur70n8</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251120_023126-ttur70n8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: exl3cvfq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 31390\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251120_061859-exl3cvfq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/exl3cvfq' target=\"_blank\">pleasant-sweep-4</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/exl3cvfq' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/exl3cvfq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251120_061909_143', 'my_seed': 31390, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.03125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[999, 998], [999, 999], [999, 999]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 0\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 0\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.03125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.03125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([ 0.0050,  0.0687,  0.0345, -0.0514,  0.1268, -0.1071, -0.0293, -0.0134,\n",
      "        -0.2772,  0.1185, -0.0420, -0.2481, -0.0263,  0.1073,  0.0244, -0.0589,\n",
      "         0.0778,  0.0233, -0.0349, -0.0993,  0.1143, -0.1454, -0.0665, -0.1050,\n",
      "        -0.0099, -0.0915, -0.0832, -0.0225, -0.0613, -0.0659, -0.0505,  0.0386,\n",
      "         0.1248,  0.0849,  0.0179, -0.0698,  0.0445,  0.1076,  0.0204,  0.0068,\n",
      "         0.0520,  0.0642, -0.0835,  0.1057, -0.1124,  0.1203,  0.0247, -0.0888,\n",
      "        -0.1221, -0.1893, -0.0210,  0.0438,  0.0330,  0.0052, -0.0893, -0.0402,\n",
      "         0.0201,  0.0416, -0.0107,  0.0055, -0.0801, -0.1248, -0.0696, -0.0622,\n",
      "        -0.0074,  0.1199, -0.0608, -0.0363, -0.0963,  0.0223, -0.0265, -0.0538,\n",
      "        -0.0641,  0.0703, -0.0877, -0.1090, -0.0072,  0.0685, -0.0342, -0.0116,\n",
      "        -0.0690,  0.2203, -0.0535, -0.0090, -0.0697,  0.0628, -0.0477,  0.0626,\n",
      "         0.1334, -0.0591, -0.0487,  0.0151, -0.0834,  0.0154,  0.0012, -0.1726,\n",
      "        -0.0949, -0.0791, -0.0262, -0.0964,  0.0010, -0.0312, -0.0744,  0.0835,\n",
      "        -0.1279, -0.0990, -0.0370, -0.0471, -0.0128,  0.0473, -0.0784,  0.0358,\n",
      "        -0.1484,  0.0429,  0.1330,  0.0423, -0.0676,  0.0865, -0.1955, -0.0327,\n",
      "         0.1629, -0.0493,  0.0786, -0.0573,  0.0115,  0.0485, -0.1036,  0.0936,\n",
      "        -0.0867, -0.2233, -0.0911,  0.1002, -0.0381, -0.0644,  0.0902,  0.0751,\n",
      "        -0.0814,  0.1925,  0.0522, -0.0018,  0.0953, -0.0930,  0.0011, -0.2097,\n",
      "        -0.0757,  0.0903,  0.0766,  0.0783, -0.0741,  0.0311,  0.0391, -0.1532,\n",
      "         0.0064,  0.0068, -0.2052, -0.1030,  0.0991,  0.0164,  0.0159,  0.0731,\n",
      "        -0.0434, -0.0971,  0.0048, -0.0088, -0.0705, -0.1174,  0.0108, -0.0873,\n",
      "        -0.0326, -0.1328, -0.0619,  0.0392, -0.1392, -0.1182,  0.1461,  0.0334,\n",
      "         0.0913,  0.0558, -0.1334,  0.1992, -0.1183,  0.0728, -0.0016, -0.0533,\n",
      "        -0.1626, -0.0120,  0.0455,  0.0829,  0.0643, -0.0097,  0.2189, -0.1246,\n",
      "        -0.1854, -0.0315, -0.0352,  0.0418,  0.1239, -0.0282,  0.0480, -0.0320],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([-1.1912e-01,  1.4782e-01, -1.0370e-02, -3.7531e-02,  1.8561e-02,\n",
      "        -8.6557e-02,  2.8931e-02,  1.8060e-01, -8.1775e-02,  1.9827e-02,\n",
      "        -1.7431e-01,  5.5794e-02,  8.3195e-02, -6.6575e-02,  1.0321e-01,\n",
      "        -7.8754e-02,  5.7183e-02,  2.3209e-02,  1.2107e-01,  4.5994e-02,\n",
      "         1.8886e-02, -6.4203e-02,  1.9633e-01,  1.0029e-01,  1.2300e-02,\n",
      "        -5.6422e-02,  1.2186e-01, -2.2459e-01, -2.3427e-01,  1.3326e-01,\n",
      "        -1.4120e-01, -2.7583e-02, -2.8105e-02,  2.1045e-01,  5.4953e-02,\n",
      "        -3.4088e-02, -1.2097e-01, -2.6313e-02,  4.5924e-02, -1.6030e-01,\n",
      "        -8.6012e-02,  1.1102e-01, -1.4386e-01,  1.8644e-02, -8.9707e-02,\n",
      "         9.2619e-03, -1.9776e-02, -1.0714e-02,  1.6171e-02,  4.2991e-02,\n",
      "        -1.1058e-03, -2.6692e-02,  5.2364e-02, -1.9943e-01,  3.7700e-02,\n",
      "         9.6545e-02,  1.0774e-02, -1.4975e-01, -2.0050e-03,  4.6337e-03,\n",
      "        -1.6586e-01, -1.7481e-01,  9.4037e-02, -4.2394e-02,  5.4852e-02,\n",
      "        -4.9673e-03, -9.3803e-02,  2.9581e-02,  9.4231e-02,  2.8329e-01,\n",
      "         1.2998e-01, -7.3532e-02,  4.6185e-03,  2.4646e-02,  1.4495e-02,\n",
      "        -4.0984e-02, -1.3048e-02, -6.4372e-02, -1.5205e-01,  9.9091e-02,\n",
      "        -1.9210e-01,  1.0041e-02,  1.8271e-01,  2.4088e-02,  2.0065e-02,\n",
      "        -1.3504e-02, -9.7311e-02, -5.2010e-02, -1.8864e-01,  1.2531e-02,\n",
      "        -4.9484e-02,  7.8041e-02,  1.1089e-01,  3.5958e-02,  1.0805e-02,\n",
      "         1.3120e-01, -4.6982e-02, -5.5440e-02, -1.3189e-01, -8.4443e-03,\n",
      "         5.7912e-02, -5.7344e-02,  4.6270e-02,  8.3543e-02,  1.2652e-01,\n",
      "         2.1172e-01,  3.6109e-02, -8.7152e-02, -3.1817e-02, -9.8071e-02,\n",
      "         8.3047e-02,  8.0894e-02,  1.0748e-01, -7.3535e-02,  6.0407e-02,\n",
      "        -1.6208e-01,  2.8324e-02,  1.3515e-01, -7.9773e-02,  6.5632e-02,\n",
      "        -6.5828e-04,  1.4906e-01,  1.0563e-01,  9.8950e-02, -1.5298e-01,\n",
      "        -1.0472e-01, -2.7288e-01, -3.6181e-02,  6.5383e-02, -8.0862e-02,\n",
      "         7.3849e-03,  2.3973e-02, -1.0668e-01,  1.4086e-01, -6.3783e-02,\n",
      "        -1.6983e-01, -1.8721e-01,  1.4271e-02, -1.1380e-01, -1.9731e-01,\n",
      "        -3.5595e-02, -3.9263e-05, -5.6694e-02,  1.4839e-01,  1.5855e-01,\n",
      "        -1.5198e-01,  3.1588e-02, -3.7481e-04,  2.3429e-02,  1.3533e-01,\n",
      "        -1.9644e-02,  2.5988e-02, -1.7913e-02,  1.1774e-01,  1.3305e-01,\n",
      "         5.6417e-02, -5.7641e-02, -1.2723e-01, -9.6298e-03, -4.3593e-02,\n",
      "         6.9609e-02, -2.7192e-03, -8.6725e-02,  1.1915e-01,  4.0032e-02,\n",
      "         1.4115e-01,  5.3638e-02, -1.0138e-01, -6.9127e-02,  3.1714e-02,\n",
      "         1.1853e-01, -2.4898e-01, -1.1930e-01, -3.5668e-03,  1.6714e-01,\n",
      "         1.6736e-02, -1.4280e-01,  1.1406e-02,  4.8046e-02, -5.0156e-03,\n",
      "         7.8280e-02, -1.6698e-01,  3.7855e-02, -8.2310e-02, -2.8489e-02,\n",
      "         3.8044e-02,  1.8573e-02,  7.1235e-02, -2.4086e-03,  8.3866e-02,\n",
      "         8.8099e-02,  4.3459e-02,  7.6006e-02, -8.6063e-02, -1.5168e-02,\n",
      "        -7.6073e-02,  1.0748e-01,  9.9282e-02, -8.3063e-02,  2.8906e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 4.7890e-02, -1.2684e-01, -4.7497e-02, -3.1328e-03, -3.3685e-02,\n",
      "         5.7809e-02, -8.7423e-02,  8.9807e-02, -2.8992e-02, -5.3025e-02,\n",
      "        -1.9223e-01,  1.5654e-01,  4.2724e-02, -1.3940e-02,  1.4239e-02,\n",
      "         1.3959e-01,  5.2108e-02,  2.4344e-02,  1.1165e-01, -6.7764e-02,\n",
      "        -2.8852e-01,  1.2040e-01, -4.1725e-02, -7.6136e-02, -8.9476e-02,\n",
      "         4.7021e-02,  4.6794e-02,  1.5494e-01, -9.4673e-02,  1.4700e-01,\n",
      "        -4.7747e-02,  1.0722e-01,  1.1566e-01,  8.7119e-02, -2.3728e-02,\n",
      "        -9.8229e-02, -1.1373e-01,  5.5148e-02, -9.5477e-02,  2.0223e-02,\n",
      "         7.0746e-02,  1.1120e-01,  3.2814e-03,  1.8268e-02, -7.8511e-03,\n",
      "        -8.4134e-03, -1.1307e-02,  3.4016e-03,  1.4124e-01,  7.2895e-02,\n",
      "         1.2632e-04,  8.3132e-02, -3.1812e-02,  8.8174e-02, -6.7868e-02,\n",
      "        -2.7215e-02,  8.4514e-02, -2.8098e-02, -2.6120e-02,  1.5586e-01,\n",
      "         9.9090e-02, -4.2667e-02,  6.6858e-03,  1.9179e-02,  2.0775e-01,\n",
      "        -7.7474e-02,  1.0463e-01,  1.3857e-01, -8.5505e-02, -9.1735e-02,\n",
      "         1.0620e-02, -8.7594e-02,  8.7028e-02, -7.5078e-03,  1.4361e-01,\n",
      "        -7.6755e-02, -5.1204e-02, -2.1259e-02,  2.5862e-02, -6.7264e-02,\n",
      "        -1.0046e-01, -8.1770e-02, -1.0393e-01, -9.8291e-02, -6.5160e-02,\n",
      "        -1.3858e-01, -9.2945e-02,  9.1809e-02,  1.9972e-02,  8.3482e-02,\n",
      "         9.7636e-02,  5.9828e-03, -1.9667e-02,  6.8925e-03,  1.5503e-02,\n",
      "        -9.2768e-03,  1.1521e-01, -3.2382e-02,  3.8170e-02, -4.7234e-02,\n",
      "        -4.7527e-02,  5.2296e-02, -4.7485e-03, -6.9405e-02,  5.2670e-02,\n",
      "         1.9709e-01,  2.1510e-02,  9.3492e-03,  2.0919e-03,  7.3995e-02,\n",
      "         4.4107e-02,  6.1837e-02,  1.1354e-01,  1.0879e-01, -6.7767e-02,\n",
      "        -1.1278e-02,  7.0022e-02, -9.4662e-02,  4.0500e-02, -7.9955e-02,\n",
      "         5.8297e-02,  1.3300e-01, -3.8609e-02, -9.3447e-03, -2.3087e-01,\n",
      "         7.7674e-02,  2.9131e-02, -3.2032e-02,  2.6083e-01, -4.6917e-02,\n",
      "        -1.8861e-01,  1.9240e-01,  8.2554e-02, -1.0068e-01,  3.5227e-02,\n",
      "        -7.5444e-03, -1.9621e-01, -5.5667e-02, -9.6269e-02, -3.3612e-03,\n",
      "        -1.4405e-01,  4.5860e-02, -1.3281e-01, -4.9923e-02, -1.4233e-01,\n",
      "        -1.0526e-01,  6.2237e-02, -1.7605e-01, -6.7687e-03,  8.9830e-02,\n",
      "         1.9589e-01,  3.5446e-02, -8.7839e-02,  4.7919e-03, -9.8425e-02,\n",
      "        -1.2322e-01, -1.9849e-01, -2.1164e-01, -2.9481e-02, -1.7930e-02,\n",
      "         2.9935e-02,  4.3493e-02,  2.7397e-02, -9.5040e-02, -1.3956e-01,\n",
      "         4.7892e-02, -3.9906e-02, -1.0265e-01, -4.3322e-03,  1.8668e-02,\n",
      "        -3.8174e-02, -7.4404e-02,  9.1334e-03,  1.1941e-01,  3.8209e-02,\n",
      "        -4.6939e-02, -3.9051e-02,  4.9836e-02,  8.9750e-02,  6.3645e-02,\n",
      "         2.2320e-02, -5.6253e-02, -2.4716e-01, -2.4355e-02, -9.9583e-02,\n",
      "        -2.5260e-02, -4.7508e-02,  3.2015e-03, -5.7892e-03,  9.2848e-02,\n",
      "        -1.1625e-01,  9.2092e-03,  9.5045e-02, -3.3438e-02,  6.2251e-02,\n",
      "         6.9145e-02,  5.3517e-02,  4.9233e-02,  2.5730e-01, -1.8256e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([ 0.0076, -0.0311,  0.0233, -0.0327, -0.1519,  0.0417,  0.0746,  0.0896,\n",
      "        -0.2469,  0.0976,  0.1394, -0.1295, -0.0062, -0.0077, -0.0711, -0.0430,\n",
      "         0.0269,  0.0221,  0.0383,  0.0210, -0.0967,  0.0126,  0.1454, -0.0935,\n",
      "        -0.0644, -0.0250, -0.0649, -0.1842,  0.1495,  0.0249,  0.0936, -0.0690,\n",
      "        -0.0532,  0.0877, -0.1237, -0.0204,  0.1548,  0.0245, -0.0537,  0.0861,\n",
      "        -0.0571, -0.1399, -0.0431,  0.2233,  0.2135,  0.1143,  0.0369,  0.1845,\n",
      "        -0.0335, -0.1829, -0.0734, -0.0733,  0.0578, -0.0233, -0.0180, -0.0425,\n",
      "         0.0717, -0.0139,  0.0874,  0.0128,  0.0324,  0.0015, -0.0808,  0.0013,\n",
      "         0.0218,  0.1323,  0.0868,  0.0325, -0.0076, -0.0052, -0.1378,  0.0459,\n",
      "         0.0432,  0.0065, -0.0994, -0.0990, -0.0125,  0.1948, -0.0076,  0.0866,\n",
      "         0.1425,  0.0223,  0.0312,  0.1108, -0.0409, -0.0254, -0.1022, -0.0370,\n",
      "        -0.0243,  0.0568, -0.0392,  0.0960,  0.0310, -0.1125,  0.0562,  0.1834,\n",
      "        -0.0109,  0.1272,  0.0964, -0.0738,  0.0021,  0.0179,  0.0085,  0.1578,\n",
      "        -0.0186,  0.0360,  0.1554,  0.0409,  0.0370,  0.1538, -0.1188,  0.1045,\n",
      "         0.1287,  0.1814, -0.0808,  0.0165, -0.1425,  0.0372,  0.0726,  0.0346,\n",
      "        -0.0378,  0.0903, -0.0266,  0.0273, -0.0823, -0.0816,  0.1101, -0.0826,\n",
      "        -0.1862,  0.0403,  0.1600, -0.1023,  0.0783, -0.0611,  0.2031, -0.1344,\n",
      "         0.1148, -0.0686,  0.0665, -0.0412,  0.0150,  0.2172,  0.1722, -0.1232,\n",
      "         0.0114, -0.1579, -0.0104, -0.0565, -0.0151,  0.0095,  0.1363, -0.0353,\n",
      "         0.0466, -0.0690, -0.0457,  0.0198, -0.0212, -0.0151, -0.0357,  0.1224,\n",
      "        -0.0365,  0.0555,  0.0374, -0.0734,  0.0873,  0.1590,  0.0705, -0.1287,\n",
      "         0.2270,  0.1142,  0.1865, -0.0662,  0.0009,  0.2208,  0.0072,  0.0485,\n",
      "        -0.1533, -0.0116,  0.0727,  0.0771,  0.0014,  0.0334, -0.0409,  0.0057,\n",
      "        -0.0943, -0.2022,  0.2338, -0.0246,  0.0847, -0.0488, -0.0079,  0.1031,\n",
      "        -0.1668, -0.0209, -0.0560,  0.0357,  0.1590,  0.0291,  0.0133, -0.0714],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-0.0398, -0.0813, -0.0370, -0.0855,  0.1545,  0.0222,  0.0074, -0.3230,\n",
      "        -0.2041,  0.0048,  0.0711,  0.0314,  0.0012, -0.0141,  0.0740, -0.0460,\n",
      "         0.0430,  0.0362, -0.0517, -0.0297,  0.1996,  0.0305,  0.1117,  0.0309,\n",
      "         0.0941,  0.0325, -0.0569,  0.0157,  0.0982,  0.0305, -0.0386, -0.0131,\n",
      "        -0.0634, -0.1423,  0.0058,  0.1421, -0.0018,  0.0552,  0.0683, -0.1156,\n",
      "        -0.0005, -0.0014,  0.1583, -0.0038, -0.0896,  0.1749, -0.0909,  0.0266,\n",
      "        -0.1446,  0.0730, -0.0488,  0.0434,  0.0822, -0.0108, -0.0761,  0.0628,\n",
      "         0.0339,  0.1227, -0.0809, -0.1193, -0.2325, -0.0745, -0.0027, -0.0763,\n",
      "         0.1010, -0.0043,  0.0321,  0.0521,  0.0957, -0.1694,  0.0850, -0.3132,\n",
      "        -0.0769, -0.0467, -0.1133,  0.1835,  0.2415, -0.1107, -0.0060,  0.1776,\n",
      "         0.1405,  0.0566,  0.1992,  0.2476, -0.0644, -0.0220,  0.0369, -0.1899,\n",
      "        -0.0598,  0.0786, -0.0166,  0.0762,  0.0694, -0.1897,  0.0805,  0.0230,\n",
      "        -0.1653,  0.0007,  0.0526, -0.2016,  0.1838,  0.0875, -0.0413, -0.0167,\n",
      "        -0.0073, -0.0689, -0.1289, -0.0703,  0.0157, -0.0604,  0.0945,  0.0807,\n",
      "        -0.0047, -0.1727, -0.1795,  0.0910,  0.1000,  0.0668,  0.1964, -0.0461,\n",
      "        -0.0223,  0.0684, -0.0744, -0.0954,  0.0912,  0.1549, -0.0386, -0.1439,\n",
      "        -0.0424,  0.0160, -0.0370, -0.0524, -0.1206, -0.2108,  0.0835, -0.1122,\n",
      "        -0.0527, -0.0474,  0.1940,  0.0315, -0.0767, -0.0262, -0.0480,  0.0482,\n",
      "        -0.0058, -0.0156, -0.0881, -0.0946, -0.0386, -0.0292, -0.0950,  0.1316,\n",
      "         0.0224, -0.0755, -0.0598, -0.1844, -0.0995,  0.0639, -0.1143, -0.1096,\n",
      "        -0.0463,  0.0807,  0.0193, -0.0682, -0.0498,  0.0724,  0.1036,  0.0494,\n",
      "         0.0049,  0.0340,  0.1378,  0.0057,  0.1265, -0.0267,  0.1076,  0.0375,\n",
      "        -0.1285,  0.1105,  0.0771, -0.0154,  0.2266,  0.2496,  0.1757, -0.0746,\n",
      "         0.2095, -0.1758, -0.1918,  0.0168,  0.0066,  0.1625, -0.0540,  0.0481,\n",
      "         0.1463, -0.0293, -0.0478,  0.0262, -0.0036,  0.0665,  0.0503, -0.0169],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([-2.0134e-01,  5.0451e-02,  5.1095e-02,  7.3023e-02,  8.6020e-02,\n",
      "        -2.0061e-02,  3.0240e-02, -4.6835e-02, -4.4545e-02, -5.5386e-02,\n",
      "         1.2172e-01, -1.3698e-01, -4.0342e-02, -1.9584e-01,  2.0364e-02,\n",
      "        -1.6693e-02,  1.4916e-01,  4.0721e-02, -7.5134e-02,  1.5245e-02,\n",
      "         5.6795e-02,  6.0099e-02,  2.9132e-02, -1.3396e-01, -1.6363e-01,\n",
      "         1.0453e-01, -9.5735e-02, -4.7288e-03, -3.8247e-02,  1.9938e-02,\n",
      "         7.5701e-03,  3.3852e-02, -2.0613e-02,  1.0013e-01,  1.1651e-01,\n",
      "        -1.3659e-01,  1.1022e-02,  1.6751e-01,  1.3206e-01,  9.3956e-02,\n",
      "        -5.3031e-02, -2.5378e-02,  3.6267e-02,  7.3838e-02,  8.3049e-02,\n",
      "         8.3636e-02,  5.7305e-02, -1.4131e-01,  1.2950e-01,  2.1687e-02,\n",
      "        -5.7187e-02, -2.6154e-02,  1.6384e-01,  1.5342e-02, -7.9047e-02,\n",
      "        -1.3285e-02, -2.0936e-01, -3.6984e-02,  5.4643e-02,  1.3568e-01,\n",
      "         8.5087e-02, -7.7699e-02,  5.1038e-02, -1.8871e-02,  5.2625e-02,\n",
      "         4.2242e-02,  1.6477e-01,  2.9961e-02,  1.2033e-01, -2.9307e-02,\n",
      "        -2.2160e-01, -4.2077e-03, -6.8402e-02, -1.0848e-01, -1.2930e-01,\n",
      "         8.5655e-02, -1.4484e-01, -1.3272e-01,  5.7944e-02,  7.8488e-02,\n",
      "         5.3810e-02, -5.0170e-02, -2.5175e-01, -9.0627e-02,  2.1767e-02,\n",
      "        -2.4390e-02, -4.3854e-02,  1.2931e-01, -3.6989e-02, -1.8798e-01,\n",
      "         5.8898e-02,  7.6593e-03, -7.6133e-03, -2.5909e-02,  4.9347e-02,\n",
      "        -7.6878e-02, -4.9791e-02, -5.3447e-02, -1.1170e-01,  5.6077e-02,\n",
      "        -9.2353e-02,  1.6498e-01,  2.0084e-01,  1.7656e-01,  9.7376e-05,\n",
      "         4.0115e-02, -1.0268e-01, -1.5433e-01, -5.0241e-02, -6.0142e-02,\n",
      "         1.3009e-02, -3.1711e-02,  1.3059e-01,  1.9643e-02,  2.6163e-02,\n",
      "        -1.7581e-01, -6.0444e-02,  2.4492e-02,  8.3418e-02,  7.4291e-02,\n",
      "        -6.9018e-02,  3.0846e-02,  2.8242e-02, -4.5378e-02,  2.5619e-03,\n",
      "        -7.5716e-02, -1.1830e-01,  2.3432e-02,  5.9632e-02,  1.4211e-01,\n",
      "         8.8834e-02, -1.0295e-01, -1.0756e-01, -5.6154e-02, -1.8082e-02,\n",
      "         1.7543e-01, -1.1897e-02, -9.0745e-02,  4.7787e-02,  3.9258e-02,\n",
      "         5.2532e-02,  5.3901e-02,  4.0687e-03, -1.0242e-01, -6.0049e-02,\n",
      "         1.5469e-01,  9.7354e-02, -8.0952e-02, -7.8869e-03,  1.1565e-01,\n",
      "         1.1434e-02, -7.6323e-02, -4.2650e-02,  6.3109e-02, -1.1246e-01,\n",
      "         2.0123e-02, -1.0990e-03, -5.3767e-02,  1.7313e-01, -1.0910e-01,\n",
      "         4.1250e-02, -2.7646e-01,  3.4151e-02, -1.2443e-02,  5.6676e-02,\n",
      "         3.2908e-02,  1.6299e-01, -6.0301e-02,  8.2150e-02, -6.2534e-02,\n",
      "         3.3060e-02,  2.7058e-02, -1.2682e-02,  6.8825e-02, -3.3422e-02,\n",
      "        -4.1298e-02, -7.3488e-03,  1.0494e-01,  1.7340e-01, -6.7581e-02,\n",
      "         3.2109e-02, -1.2450e-01, -8.5200e-02,  3.2730e-02, -8.0235e-02,\n",
      "         5.5149e-03,  1.2331e-01,  1.7187e-01, -2.9789e-02, -1.3385e-01,\n",
      "        -2.0576e-01, -1.6018e-01,  1.2163e-01, -4.0556e-02, -3.5144e-02,\n",
      "        -5.8685e-02, -1.0303e-01,  1.2202e-01, -4.6597e-02, -7.3774e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([ 1.3537e-01,  1.4633e-01,  1.0663e-01,  4.1657e-02, -8.3922e-02,\n",
      "        -8.5685e-02,  1.5088e-01, -7.4727e-02,  8.2790e-03,  9.5663e-03,\n",
      "        -1.5184e-01, -4.9096e-02,  7.1676e-02, -1.7815e-02, -5.1060e-02,\n",
      "         7.0278e-02,  4.4428e-02,  5.2068e-02, -9.3864e-02,  1.1840e-03,\n",
      "         9.3585e-02, -2.4737e-02,  9.1732e-02, -4.4314e-02,  6.6797e-02,\n",
      "         3.5242e-02, -6.7127e-03,  9.0819e-02,  8.4664e-02,  8.1896e-02,\n",
      "         6.7131e-02, -9.9986e-02,  5.7420e-03, -7.2111e-03,  1.5900e-02,\n",
      "        -1.1017e-01,  1.0350e-01, -1.0012e-01, -8.1095e-05, -6.3131e-02,\n",
      "        -1.6561e-01, -4.2531e-02,  1.1431e-01,  1.1489e-01, -7.5527e-02,\n",
      "        -1.3797e-01, -8.2909e-02,  1.3067e-03,  1.1575e-01,  2.2066e-02,\n",
      "         2.5511e-02,  7.2643e-02, -8.1918e-02, -8.9355e-02,  1.3065e-01,\n",
      "         1.3325e-01, -4.8609e-03,  8.0771e-02,  7.9296e-02, -1.5715e-01,\n",
      "         4.6855e-02, -4.2432e-03, -2.9399e-01,  5.6638e-02,  1.2514e-01,\n",
      "         8.6167e-02, -4.6832e-02, -3.3895e-02,  2.1724e-01, -4.5440e-03,\n",
      "        -2.6536e-02,  7.3383e-02,  1.8810e-02, -2.2760e-02, -4.1976e-01,\n",
      "         7.4105e-02,  9.7562e-02, -3.9331e-02,  8.4970e-02, -5.3426e-02,\n",
      "         8.0707e-02, -5.5468e-02, -1.8248e-02, -1.1934e-01,  2.6937e-02,\n",
      "        -4.1174e-02,  1.1471e-01,  1.2314e-01, -5.0717e-02, -2.4135e-02,\n",
      "        -2.7457e-02, -2.0278e-02, -4.9927e-02, -1.0603e-01, -8.1424e-02,\n",
      "         4.1531e-02,  2.8605e-02, -2.3975e-01,  4.6214e-02,  1.2258e-01,\n",
      "        -6.9613e-03,  1.6277e-01,  1.3088e-02,  8.7230e-02,  8.4799e-02,\n",
      "         9.8520e-02, -1.1873e-02,  1.5050e-01, -1.6180e-02, -7.5633e-02,\n",
      "        -2.0368e-02,  7.0540e-02, -2.2000e-01,  4.8001e-02, -1.3383e-01,\n",
      "        -6.0608e-04, -2.2352e-01, -8.2063e-02, -1.4583e-01,  1.7984e-02,\n",
      "         1.4340e-01,  1.5033e-02, -8.3679e-02, -5.3094e-02, -8.9682e-02,\n",
      "        -9.1052e-02,  7.3474e-02,  1.5087e-01,  1.0310e-01, -4.9944e-04,\n",
      "         9.8648e-02, -1.1499e-01, -2.0085e-01, -3.6878e-02,  1.7449e-02,\n",
      "        -4.0706e-03,  2.5202e-02, -2.8781e-02,  4.9761e-03, -1.5582e-01,\n",
      "        -2.7032e-02, -9.8935e-02, -9.5699e-02,  1.2185e-01, -1.4174e-01,\n",
      "         6.4296e-02, -3.1875e-02,  7.3099e-02, -7.3983e-02,  5.2571e-02,\n",
      "        -2.5360e-02,  1.0356e-01,  2.9201e-02,  5.7441e-02,  9.0328e-02,\n",
      "        -1.9900e-02,  1.2818e-02,  7.8660e-02, -5.7562e-02, -6.4479e-02,\n",
      "         2.1058e-01, -2.2270e-01,  5.1638e-02,  9.7133e-02, -8.8709e-02,\n",
      "         1.8090e-01,  5.1819e-02,  1.3157e-02,  2.0659e-01,  1.1360e-01,\n",
      "         2.0905e-01, -3.1379e-02,  9.2288e-02,  8.2716e-02, -3.2123e-02,\n",
      "         1.0537e-01,  1.5588e-02,  1.4094e-01, -7.2253e-02,  2.1421e-02,\n",
      "         7.0949e-02, -5.3120e-02,  2.3151e-01, -6.3250e-02,  1.2919e-02,\n",
      "         1.5202e-01,  1.1668e-01, -1.3557e-02, -1.7624e-02,  7.8310e-02,\n",
      "        -1.5233e-01, -2.3334e-02, -6.3920e-03, -2.0284e-01,  9.0633e-02,\n",
      "         7.4327e-02, -1.5193e-01,  4.7708e-02,  6.2985e-02, -1.2788e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([-0.0296,  0.0156,  0.0176, -0.1983,  0.0845,  0.0744,  0.0143,  0.1836,\n",
      "         0.1356,  0.0233,  0.0679,  0.0272,  0.0751,  0.1173, -0.1802,  0.0371,\n",
      "        -0.0220, -0.0955,  0.0333,  0.1186, -0.0201, -0.2439, -0.1412, -0.1459,\n",
      "         0.0122, -0.0997, -0.0144, -0.0726,  0.0583,  0.0777,  0.0390, -0.0738,\n",
      "         0.1257, -0.0418,  0.1016,  0.0757, -0.0767, -0.0251, -0.0689, -0.0597,\n",
      "         0.0212,  0.0412, -0.0882, -0.0781, -0.0289,  0.0361, -0.0242,  0.0486,\n",
      "        -0.1530, -0.0586, -0.0174, -0.0991,  0.0305, -0.0065, -0.0540,  0.0009,\n",
      "        -0.0253,  0.0478,  0.0643,  0.0481,  0.2011, -0.2066,  0.0637,  0.0695,\n",
      "         0.0815, -0.0048,  0.0716, -0.1468,  0.1713, -0.0330,  0.0166, -0.0974,\n",
      "        -0.0604,  0.1747,  0.0649, -0.0704,  0.0150,  0.0298,  0.0277, -0.0404,\n",
      "         0.0723, -0.0111, -0.0681, -0.0548,  0.0048, -0.0738,  0.0187, -0.0669,\n",
      "         0.0358, -0.0776, -0.0010, -0.0689,  0.0308, -0.0487,  0.0947, -0.2440,\n",
      "        -0.0137, -0.0806,  0.0928, -0.0108,  0.1173, -0.0373, -0.1273, -0.0698,\n",
      "         0.0618, -0.0676, -0.1244,  0.0096,  0.0068, -0.1522,  0.0074, -0.0454,\n",
      "        -0.0117, -0.1637,  0.0191, -0.0537, -0.0063,  0.0402, -0.0827, -0.2178,\n",
      "         0.0594,  0.0025,  0.0167,  0.0330, -0.2475, -0.0940,  0.0236, -0.0800,\n",
      "         0.0248, -0.0266,  0.1884, -0.0669,  0.0991,  0.1105, -0.0330, -0.0299,\n",
      "        -0.0961,  0.0314, -0.1010,  0.0531, -0.0919,  0.0204, -0.0824,  0.1742,\n",
      "        -0.2604,  0.1867,  0.1857,  0.1895,  0.0035,  0.1063,  0.1039, -0.0228,\n",
      "         0.0395,  0.0572,  0.1300,  0.1646,  0.1777, -0.0665,  0.0810, -0.0535,\n",
      "        -0.1680, -0.0122,  0.0024,  0.0531,  0.1262,  0.2108,  0.0190,  0.1512,\n",
      "        -0.0866,  0.2016, -0.1316, -0.0433, -0.0391,  0.0063, -0.2669,  0.1696,\n",
      "        -0.1156,  0.0221,  0.0116, -0.0665, -0.0153, -0.2960, -0.1542, -0.0992,\n",
      "        -0.1044, -0.1461, -0.0379, -0.0549,  0.0553, -0.2044, -0.0053,  0.0343,\n",
      "         0.1101, -0.0440,  0.0536, -0.1829, -0.2515, -0.2035, -0.0388, -0.0888],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([-0.0807,  0.0381,  0.0817, -0.1153,  0.0753,  0.0909, -0.0185,  0.0436,\n",
      "        -0.0583, -0.1638, -0.0220, -0.1149, -0.0350, -0.0737, -0.0320, -0.1075,\n",
      "         0.1114,  0.1078,  0.1663, -0.0546, -0.0518, -0.0624, -0.0275,  0.0550,\n",
      "         0.0081, -0.0919, -0.0424, -0.0503, -0.0045, -0.0756,  0.0187, -0.0918,\n",
      "         0.0100,  0.0337, -0.1791, -0.1838, -0.1042,  0.0903, -0.1057,  0.0526,\n",
      "         0.0048, -0.1067,  0.0724, -0.0300,  0.1447, -0.1207, -0.0807, -0.1181,\n",
      "         0.0082,  0.0362, -0.1336,  0.0087,  0.0333,  0.0116,  0.1143, -0.0474,\n",
      "         0.0265,  0.0411, -0.0183,  0.1127, -0.1556,  0.0298,  0.0140, -0.0196,\n",
      "         0.0969, -0.0914,  0.0535, -0.1140,  0.0420, -0.0860, -0.0352, -0.1477,\n",
      "         0.0727,  0.0948, -0.1099, -0.0588,  0.0555, -0.0314, -0.0550, -0.0489,\n",
      "         0.0375, -0.0218,  0.0332,  0.1729,  0.1785, -0.0593, -0.0386,  0.0489,\n",
      "        -0.0422, -0.0839,  0.1460,  0.0658,  0.1460,  0.2833, -0.0410,  0.0205,\n",
      "         0.0181,  0.0243, -0.0512, -0.0158, -0.1089, -0.0622, -0.0664,  0.0636,\n",
      "        -0.0755, -0.1286, -0.0655, -0.0457, -0.0415,  0.1765,  0.0454, -0.0082,\n",
      "        -0.0597, -0.1341, -0.0279, -0.1350, -0.0298, -0.1048,  0.0103, -0.0612,\n",
      "        -0.1482,  0.1465,  0.0306, -0.0373, -0.1484,  0.1390,  0.0264, -0.0555,\n",
      "        -0.1697, -0.1369,  0.0829,  0.0475,  0.0695, -0.2041,  0.0325, -0.0981,\n",
      "         0.1024, -0.1126, -0.0097, -0.0397, -0.0211,  0.0103, -0.0071,  0.0065,\n",
      "        -0.0524, -0.0133,  0.0381, -0.1070, -0.0861,  0.0553, -0.1368,  0.0090,\n",
      "         0.0983,  0.0652, -0.0080,  0.1055, -0.0582,  0.0708,  0.0909, -0.0486,\n",
      "         0.0761, -0.0730, -0.0953,  0.0128, -0.1891, -0.0549, -0.0280,  0.0166,\n",
      "         0.0107, -0.0650, -0.0256,  0.0864,  0.0861,  0.0186,  0.0283,  0.1009,\n",
      "        -0.0878, -0.1024, -0.0609, -0.0753,  0.0274, -0.1329,  0.0441, -0.0125,\n",
      "         0.0923, -0.2016, -0.0262,  0.0743,  0.1225,  0.0540,  0.0388,  0.0446,\n",
      "        -0.0990, -0.0507,  0.1330, -0.0660, -0.1191, -0.0590, -0.0722,  0.1140],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([ 0.0865,  0.0885,  0.0677, -0.1546,  0.0403, -0.0110,  0.0431,  0.1760,\n",
      "        -0.1441,  0.1443, -0.0384,  0.0136, -0.0627, -0.1802,  0.0933, -0.0467,\n",
      "        -0.1481, -0.0791,  0.0392,  0.0369,  0.0472, -0.0057,  0.1188, -0.1330,\n",
      "        -0.1474, -0.1573,  0.0626, -0.0714, -0.0480, -0.1152, -0.0640, -0.0533,\n",
      "        -0.0700,  0.0888,  0.1660, -0.0665, -0.0411, -0.0433, -0.0582,  0.0396,\n",
      "         0.0458,  0.2453, -0.0700,  0.0332,  0.0429,  0.0540, -0.0185, -0.1296,\n",
      "        -0.0754, -0.0180, -0.0394,  0.0104,  0.2015,  0.1500,  0.1597, -0.1673,\n",
      "         0.0476, -0.0460, -0.0650,  0.1267, -0.0409,  0.0097, -0.0271, -0.0639,\n",
      "        -0.0768, -0.0435,  0.0658, -0.0348,  0.0372, -0.0407,  0.1872,  0.1614,\n",
      "        -0.2143,  0.1262, -0.0434, -0.0526, -0.0218,  0.0657, -0.1008,  0.1637,\n",
      "         0.0354, -0.0242,  0.0866,  0.0308,  0.1028,  0.0735,  0.1419, -0.0455,\n",
      "        -0.0144,  0.0474,  0.0803,  0.1229, -0.0239,  0.1341,  0.0241, -0.1974,\n",
      "         0.0497, -0.0281,  0.0611,  0.1364, -0.1034,  0.0470,  0.1161,  0.0041,\n",
      "         0.0618,  0.1429, -0.2028,  0.0318, -0.0244, -0.2343, -0.0652, -0.0942,\n",
      "         0.0096, -0.0954, -0.1314, -0.0710, -0.0546, -0.0015,  0.1346,  0.0061,\n",
      "        -0.1032,  0.0084,  0.0050, -0.0337, -0.0549, -0.0388, -0.0565,  0.0825,\n",
      "        -0.1158, -0.0465,  0.0213,  0.0410,  0.0577,  0.0898, -0.0765, -0.0384,\n",
      "        -0.0884, -0.1131,  0.0641, -0.1299, -0.0253,  0.0280,  0.0080,  0.0176,\n",
      "        -0.0042,  0.0755,  0.1582, -0.1306,  0.1097, -0.0684, -0.0159,  0.0785,\n",
      "         0.1332, -0.0613, -0.0364,  0.0826,  0.0209, -0.0748, -0.0164,  0.1013,\n",
      "         0.0552,  0.0096,  0.0871,  0.2379,  0.0479, -0.0372,  0.2446,  0.0150,\n",
      "        -0.1225, -0.0558,  0.0402,  0.1086,  0.1502, -0.1546, -0.1263,  0.0912,\n",
      "        -0.0815,  0.1426, -0.0932,  0.0424,  0.1505,  0.0481,  0.1001, -0.0538,\n",
      "         0.1921, -0.0880, -0.0285, -0.0354,  0.2137,  0.0384, -0.0153, -0.0080,\n",
      "        -0.0378,  0.0035, -0.0064, -0.1547, -0.0380, -0.0311, -0.0305, -0.0837],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([ 5.9985e-02,  6.0546e-02,  3.2498e-02,  1.4448e-01,  1.1016e-01,\n",
      "        -1.2859e-01,  6.6069e-02,  7.2838e-02,  8.6594e-03, -1.1793e-01,\n",
      "        -1.6633e-01, -6.9648e-03,  1.1809e-01, -1.3479e-01,  1.5246e-01,\n",
      "         1.5250e-01,  2.0821e-02,  5.6991e-02,  8.6265e-02,  3.1675e-01,\n",
      "        -6.8042e-02,  5.3957e-02, -2.6866e-02, -1.3046e-02, -4.9850e-02,\n",
      "         9.3010e-04,  1.3948e-03, -2.3566e-01,  2.5725e-02, -6.8385e-02,\n",
      "        -1.8336e-02,  3.2387e-01, -4.8040e-02, -9.6432e-02, -8.4761e-03,\n",
      "        -1.3678e-01,  6.8848e-02,  1.9089e-03, -3.0893e-02, -7.3455e-02,\n",
      "         8.2418e-02,  6.1903e-02,  7.3204e-03,  6.8016e-02, -1.3765e-02,\n",
      "         8.9963e-02, -2.1923e-02, -2.8618e-02, -7.1162e-02, -6.7392e-02,\n",
      "        -2.7087e-02,  5.0067e-02,  5.4392e-02, -1.4950e-01,  5.3924e-02,\n",
      "         1.6973e-02,  1.5316e-01,  1.4311e-01,  1.0138e-01,  2.3316e-02,\n",
      "        -4.6404e-03, -7.6826e-02, -2.7548e-02, -1.9386e-02, -1.3122e-02,\n",
      "        -1.3597e-01, -1.0832e-01,  1.0388e-01,  1.0059e-01, -3.0276e-02,\n",
      "        -5.1055e-02,  4.0848e-02, -1.3916e-01,  1.2256e-01,  9.4754e-02,\n",
      "        -7.6198e-02, -4.8113e-02,  1.8046e-01, -1.2707e-01,  1.2330e-01,\n",
      "         2.9640e-03,  7.2778e-02, -2.3330e-02,  1.6324e-02, -7.3909e-02,\n",
      "         6.5942e-02,  1.9442e-02,  2.4227e-02,  7.1986e-02,  4.1470e-02,\n",
      "         5.9344e-03, -6.3366e-02, -1.5476e-02, -6.3271e-02,  1.5283e-01,\n",
      "         5.0552e-02, -1.5161e-02, -9.7722e-02,  6.0074e-02,  7.9730e-02,\n",
      "         1.7577e-04,  4.1399e-02, -6.6983e-02, -5.3061e-02, -1.5364e-01,\n",
      "         5.0566e-02,  2.4436e-02, -1.8914e-03,  4.8278e-02, -7.5048e-02,\n",
      "         3.1920e-02, -1.0180e-01,  1.0204e-01, -2.3598e-02,  8.5378e-02,\n",
      "         2.8261e-02,  4.9302e-03, -2.0762e-02, -7.5648e-02,  1.0288e-01,\n",
      "         8.5872e-02, -2.4292e-02,  7.2435e-02, -1.7478e-02,  2.5559e-02,\n",
      "        -1.3968e-02, -4.6899e-02, -2.8273e-02,  2.0502e-02, -8.8241e-03,\n",
      "        -5.6078e-02,  1.1323e-01,  4.8869e-02,  1.4042e-01, -8.0119e-03,\n",
      "         6.3440e-02,  4.5723e-02, -1.3631e-01,  6.8446e-02, -5.2805e-02,\n",
      "        -4.5338e-02,  1.3572e-01, -1.4737e-02, -8.5974e-02, -2.6797e-02,\n",
      "        -5.7194e-03,  9.1092e-02, -1.3511e-01,  1.5073e-01, -1.2716e-01,\n",
      "        -1.8581e-01,  3.6142e-02, -2.1310e-01,  8.8794e-03,  4.7589e-02,\n",
      "         5.6875e-02, -2.3076e-02, -2.1436e-02,  7.7529e-03, -1.2912e-01,\n",
      "        -5.9369e-02,  2.1598e-02,  5.5822e-02,  5.7052e-02, -1.7454e-02,\n",
      "         6.4907e-03,  1.9373e-02,  1.3179e-01, -8.9564e-02,  7.0355e-02,\n",
      "         1.2109e-01, -2.0767e-02, -5.1572e-03, -7.6115e-03, -6.9572e-03,\n",
      "        -3.5173e-02, -3.4093e-02, -7.9622e-03, -5.1510e-02, -1.2248e-01,\n",
      "         1.5207e-02, -6.6810e-03, -6.8740e-04,  5.1674e-02, -8.2475e-02,\n",
      "        -1.1297e-01,  4.7711e-03,  2.0625e-01,  4.2534e-02,  2.6298e-01,\n",
      "        -1.2380e-01, -7.6229e-02,  5.1833e-02,  5.3781e-02, -9.3642e-02,\n",
      "        -9.8689e-02, -2.1298e-02,  1.2522e-01, -7.0378e-02, -1.7036e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([-6.3053e-02, -1.3429e-01,  4.5801e-02, -1.0661e-02, -6.9911e-02,\n",
      "        -4.2503e-02,  4.3062e-03, -1.4555e-01,  2.7643e-02, -1.8195e-02,\n",
      "        -2.5101e-01, -1.4796e-01, -7.7368e-02, -7.0195e-02, -2.9671e-02,\n",
      "        -1.5063e-01, -4.5620e-02,  1.5115e-01,  9.4700e-03,  4.4011e-02,\n",
      "         6.3091e-02, -3.5857e-02, -9.2055e-02,  2.9935e-02, -1.3592e-01,\n",
      "         8.6237e-02, -4.0964e-02,  8.2688e-02,  1.9220e-01,  6.8674e-02,\n",
      "        -5.6365e-02, -1.6061e-01, -3.9915e-02,  3.6045e-03, -4.7826e-02,\n",
      "         4.4569e-03, -1.9974e-02,  1.0093e-01, -1.8704e-01, -1.0994e-02,\n",
      "         7.0534e-02,  1.1138e-01,  3.6936e-02,  9.7578e-02,  6.5070e-02,\n",
      "         2.1201e-02, -6.9343e-02, -7.8444e-02, -3.4545e-02, -5.3508e-02,\n",
      "         1.1794e-01, -1.1562e-01, -1.6149e-02,  2.4537e-01, -1.3629e-01,\n",
      "         7.6322e-03,  3.8527e-02,  8.1831e-02, -1.0585e-02,  1.3595e-01,\n",
      "         3.1834e-02, -7.7378e-02, -9.7962e-02,  6.7092e-02,  1.1419e-01,\n",
      "        -1.0845e-01,  2.2962e-02,  7.1572e-02, -1.6354e-01,  3.1968e-03,\n",
      "         5.2831e-02,  9.7305e-02, -1.4708e-02,  2.5873e-02, -4.0712e-02,\n",
      "        -4.9356e-02,  1.0949e-01,  1.2311e-01,  1.4036e-01,  5.6591e-02,\n",
      "        -1.0976e-01,  4.7765e-02,  1.2959e-02, -3.2855e-03, -5.1159e-03,\n",
      "         2.5695e-02,  7.3526e-02, -1.6887e-01, -1.8120e-01,  1.1401e-01,\n",
      "        -1.5709e-02,  1.5234e-01, -8.1791e-03, -9.1216e-02, -1.3994e-01,\n",
      "         9.7774e-02, -5.9876e-02, -6.2817e-02, -2.4366e-02,  5.6810e-02,\n",
      "        -3.6337e-02, -1.1835e-01,  4.5553e-02,  9.5760e-02, -1.9929e-02,\n",
      "         1.4825e-01, -3.7816e-03, -3.4409e-02, -4.7860e-02,  7.3784e-03,\n",
      "         3.4120e-02,  1.4495e-01, -8.2743e-02, -1.5987e-01,  5.7466e-02,\n",
      "         4.5705e-02,  1.6505e-02, -7.1967e-02,  2.2482e-02,  4.8832e-02,\n",
      "         3.5782e-02,  4.9083e-02, -8.7408e-02, -7.8742e-03, -1.2417e-01,\n",
      "        -8.7158e-02,  1.1497e-02,  1.8229e-02, -7.3372e-02,  4.7477e-03,\n",
      "         8.4974e-02, -2.4599e-01,  3.4239e-02,  1.9203e-02,  6.7924e-02,\n",
      "         4.8233e-02, -1.3047e-01,  1.2301e-02,  9.3413e-02,  9.5999e-02,\n",
      "        -1.2358e-01,  1.0729e-02, -6.1465e-02, -4.7172e-02,  5.6470e-02,\n",
      "         1.9215e-01,  9.6344e-02, -7.3999e-03, -3.3514e-03,  2.0935e-02,\n",
      "         1.3133e-01,  3.6559e-02, -7.8193e-02,  9.5111e-02,  1.6755e-01,\n",
      "         1.4996e-01,  8.4205e-02, -9.6464e-02,  1.4256e-02, -1.0569e-01,\n",
      "        -5.9696e-02, -1.6163e-01,  5.4413e-02,  1.6252e-01, -7.4769e-02,\n",
      "        -1.5448e-02, -1.9210e-01, -1.2441e-01,  9.3001e-02,  1.9944e-01,\n",
      "         7.4671e-02,  5.5545e-02,  7.9681e-02,  1.2523e-02,  8.8206e-02,\n",
      "         1.3284e-01,  1.2762e-01, -6.5372e-02,  1.6382e-01, -2.3824e-02,\n",
      "         2.2033e-01, -7.0413e-02,  1.0129e-01, -6.2431e-02, -7.9334e-02,\n",
      "         5.3123e-02, -1.3399e-04, -3.4999e-01, -7.2726e-02, -2.7334e-01,\n",
      "         8.1838e-02, -1.0253e-01,  2.0016e-02,  1.4216e-01,  6.2358e-02,\n",
      "        -1.2627e-01,  4.1266e-02,  2.0480e-01,  7.3929e-02,  4.8009e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 1.2138e-02, -6.7404e-02,  2.3881e-01, -8.9232e-02,  1.6335e-01,\n",
      "         1.5406e-01, -2.3922e-03, -2.8029e-02,  5.6048e-02, -1.3082e-01,\n",
      "         5.6656e-02, -2.6662e-02,  2.6805e-02, -1.3995e-01,  1.5835e-01,\n",
      "        -8.8857e-02, -6.6463e-02,  3.9777e-02,  1.7348e-01, -2.8451e-02,\n",
      "         8.3979e-02,  5.9293e-02, -3.0586e-02,  3.5587e-02, -3.5538e-02,\n",
      "        -2.6357e-02,  1.2500e-01,  7.9086e-02, -1.1199e-03,  9.5240e-02,\n",
      "         2.5295e-02,  1.4313e-01, -1.4176e-01,  3.3999e-02, -1.4283e-01,\n",
      "         6.9084e-02, -2.7149e-02,  1.5553e-02,  3.2805e-02,  1.5945e-01,\n",
      "        -4.3031e-02,  5.6299e-02, -9.7737e-02,  3.0746e-02,  6.4296e-03,\n",
      "        -2.0113e-01,  1.3731e-01, -1.1804e-02, -1.9516e-01,  1.3251e-01,\n",
      "        -1.8206e-02, -1.7912e-02,  5.8127e-02, -2.1548e-01, -1.8607e-02,\n",
      "         8.9767e-02, -2.7692e-02, -3.5375e-02, -5.6778e-02, -9.8239e-02,\n",
      "         2.9694e-04, -7.0236e-02, -3.1047e-01,  1.1472e-02,  2.1009e-01,\n",
      "         9.9362e-02, -1.1405e-01,  4.9716e-03, -4.3550e-02,  1.8276e-01,\n",
      "         7.2520e-02,  2.8954e-02,  1.5730e-03,  3.8235e-02,  1.0724e-01,\n",
      "         9.3459e-02, -2.3109e-03,  8.5620e-03,  1.7336e-01, -9.0264e-02,\n",
      "         5.6055e-03, -1.9108e-02,  1.2140e-01, -5.7767e-02,  2.3894e-01,\n",
      "         6.4335e-02,  6.9567e-02, -4.7254e-02,  2.1669e-02, -1.2745e-01,\n",
      "        -2.0178e-01,  2.2710e-02,  2.2083e-02,  1.5992e-01,  3.8963e-02,\n",
      "         4.7677e-02,  3.3849e-02,  4.7524e-02,  1.9372e-02, -8.2693e-03,\n",
      "        -8.6432e-02,  8.8587e-02, -5.9636e-02, -2.0804e-01,  5.8715e-02,\n",
      "         5.9271e-02,  1.9483e-01,  1.4231e-01, -7.5283e-02, -1.7639e-01,\n",
      "        -1.3299e-01,  5.4530e-02, -7.8195e-02, -2.6663e-02, -1.0691e-01,\n",
      "        -1.7036e-01,  8.5334e-02,  3.0523e-01,  1.4351e-01, -1.4972e-02,\n",
      "         5.6456e-02,  3.9853e-02,  1.6437e-01, -9.0641e-02, -2.9172e-01,\n",
      "         2.2857e-02, -4.8000e-03,  1.2019e-01,  5.9698e-02, -4.9678e-02,\n",
      "         4.2876e-02,  3.6439e-03,  1.0135e-01,  5.7355e-02, -4.3875e-02,\n",
      "        -9.4214e-02,  2.5493e-01,  2.0282e-01, -9.2805e-02, -6.4862e-02,\n",
      "        -7.0648e-02,  5.1572e-02,  9.4573e-02, -1.0649e-01,  1.2928e-01,\n",
      "        -3.7673e-03,  3.9048e-02,  1.8717e-01,  3.4677e-02,  1.3609e-01,\n",
      "        -2.1789e-02,  6.4068e-02,  4.3570e-02,  4.9007e-02, -2.7763e-02,\n",
      "         2.0732e-02, -5.4319e-02, -6.1937e-02, -3.2321e-02, -2.0377e-01,\n",
      "         1.0931e-01,  1.3939e-01, -4.4551e-02,  6.5040e-02, -5.8596e-02,\n",
      "         1.3178e-01, -1.1175e-02,  3.8866e-02,  2.5317e-01,  9.1687e-02,\n",
      "         2.4662e-01,  2.1602e-02, -6.9715e-02, -1.2600e-01, -6.3251e-02,\n",
      "        -4.0498e-02, -6.5652e-03,  4.6523e-02, -4.3062e-02,  3.9489e-02,\n",
      "         7.6265e-02,  7.3246e-02,  8.6537e-02,  1.2143e-01, -1.4383e-02,\n",
      "         1.0592e-01, -4.7705e-02, -1.1224e-01,  8.9563e-02,  5.5742e-02,\n",
      "        -1.7922e-02, -2.4093e-02,  1.0701e-01, -1.5004e-01, -1.0864e-01,\n",
      "        -4.0878e-02, -1.6665e-02,  3.1819e-02,  1.1281e-01, -7.1712e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([ 0.0175,  0.1614, -0.1448, -0.0077,  0.0869, -0.0852,  0.0287,  0.0542,\n",
      "        -0.0696, -0.1672, -0.1688, -0.0024,  0.0029, -0.0653,  0.1490, -0.0445,\n",
      "        -0.0857,  0.2054,  0.0355,  0.0287, -0.1415,  0.0198,  0.0048, -0.0429,\n",
      "        -0.2812,  0.2327, -0.0045,  0.2416,  0.0115, -0.0512,  0.0135, -0.0380,\n",
      "         0.0259,  0.1499,  0.0768, -0.0959,  0.0271,  0.1465, -0.1640, -0.0742,\n",
      "        -0.1611, -0.0698,  0.0629,  0.1702,  0.0313,  0.0744,  0.1350,  0.0010,\n",
      "         0.0644, -0.0437, -0.0171, -0.0471,  0.0410,  0.0115, -0.0006,  0.0630,\n",
      "        -0.1160,  0.0855, -0.1205,  0.0016, -0.0519, -0.0220, -0.0319,  0.0466,\n",
      "         0.0680, -0.0154,  0.0561,  0.1201, -0.0173, -0.0161, -0.1673,  0.0189,\n",
      "        -0.0046, -0.0258,  0.0211,  0.1321, -0.0772,  0.1842,  0.0487, -0.1905,\n",
      "        -0.0588, -0.0282, -0.0295, -0.1252, -0.0319,  0.0128, -0.0269,  0.1353,\n",
      "         0.0167,  0.1250,  0.0135, -0.1297,  0.1255, -0.0360,  0.0455,  0.1141,\n",
      "        -0.0534,  0.1068, -0.0588, -0.0234, -0.0596, -0.1515, -0.0543, -0.1368,\n",
      "        -0.0546,  0.0152, -0.1027,  0.0297, -0.0989,  0.1755, -0.1170, -0.0665,\n",
      "        -0.1401,  0.1578,  0.0242, -0.0521,  0.1819,  0.0409, -0.1116, -0.0122,\n",
      "         0.0158, -0.1836, -0.1524, -0.1325, -0.1401,  0.2177,  0.0101, -0.0277,\n",
      "         0.0208, -0.0739,  0.0953, -0.0271, -0.0785, -0.0258,  0.0660, -0.0109,\n",
      "         0.1084, -0.0425, -0.1729,  0.0721, -0.0020,  0.0701,  0.1704,  0.1572,\n",
      "        -0.0152,  0.1567,  0.1130,  0.0265, -0.0861,  0.0741,  0.1254, -0.0518,\n",
      "        -0.1059, -0.0641,  0.0938, -0.0178, -0.1627,  0.0459, -0.1751,  0.0099,\n",
      "         0.0755,  0.0050, -0.0597,  0.0156,  0.0567,  0.0460,  0.0734, -0.1391,\n",
      "         0.0288,  0.0057, -0.1109,  0.0118, -0.0029, -0.0130, -0.1901, -0.0314,\n",
      "        -0.0768,  0.1162,  0.1251,  0.1201,  0.0144,  0.1231,  0.0662, -0.1048,\n",
      "         0.1400, -0.3071,  0.1765, -0.0910, -0.1005, -0.0018,  0.1095,  0.0020,\n",
      "         0.0787,  0.0131,  0.1201,  0.0418,  0.0472,  0.1315, -0.0554,  0.1565],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([ 0.0184, -0.0088, -0.0315, -0.0117, -0.0426, -0.0222,  0.1537,  0.0525,\n",
      "        -0.1388,  0.0984,  0.1017,  0.0276, -0.0112, -0.0061, -0.1844,  0.1784,\n",
      "        -0.0474, -0.1151,  0.1430, -0.1533, -0.0252,  0.0076,  0.1620, -0.0500,\n",
      "        -0.0556,  0.0991,  0.1144,  0.0688,  0.0468,  0.1258,  0.2040,  0.0833,\n",
      "        -0.1080, -0.0048,  0.1196, -0.0005, -0.0961, -0.0246, -0.1745, -0.1590,\n",
      "        -0.0211,  0.1115, -0.0832, -0.1015, -0.0223, -0.0689,  0.1203, -0.0558,\n",
      "        -0.1633,  0.0088, -0.0792,  0.0558, -0.0138, -0.1033, -0.0730, -0.1756,\n",
      "         0.1086,  0.0713,  0.0371,  0.1012,  0.0412,  0.0290,  0.0030, -0.0706,\n",
      "         0.0260, -0.0272,  0.0658,  0.0904, -0.0680, -0.0560, -0.0297,  0.0056,\n",
      "         0.1005,  0.0189,  0.0137,  0.1002,  0.2035,  0.0637, -0.0824, -0.0156,\n",
      "        -0.1509, -0.0760,  0.2121, -0.0021,  0.1187, -0.0570, -0.0690, -0.1016,\n",
      "         0.1435,  0.0257,  0.0368, -0.0761,  0.0727,  0.1543,  0.0407, -0.0779,\n",
      "        -0.1200, -0.1115,  0.0176, -0.1047, -0.1186,  0.0534, -0.1163, -0.1164,\n",
      "         0.0509,  0.0464, -0.2081,  0.0078, -0.1050, -0.0005,  0.1788,  0.0088,\n",
      "        -0.2006,  0.0053,  0.0609, -0.0760, -0.1634, -0.1181,  0.2116,  0.0309,\n",
      "        -0.1207, -0.0431,  0.1068,  0.1347, -0.1095,  0.0134, -0.1132, -0.0555,\n",
      "        -0.0454, -0.1208,  0.0452,  0.0325,  0.0308,  0.0491, -0.2035, -0.0406,\n",
      "         0.2856, -0.0417,  0.3366,  0.0647, -0.0216, -0.1628,  0.0776,  0.2152,\n",
      "        -0.0509,  0.0532, -0.1332, -0.0707, -0.1421, -0.0410, -0.2082,  0.0613,\n",
      "        -0.0614, -0.0735,  0.1098, -0.0653,  0.1102,  0.0250, -0.0026, -0.0484,\n",
      "         0.1043,  0.1114, -0.0104, -0.0604, -0.0696,  0.0013,  0.1156, -0.0241,\n",
      "        -0.0229, -0.0329,  0.0297, -0.2451, -0.0587,  0.1555, -0.0312, -0.1930,\n",
      "         0.0905, -0.0533, -0.1425, -0.0279, -0.1063,  0.0705,  0.1437,  0.0832,\n",
      "        -0.0184,  0.0093, -0.1096, -0.0298, -0.0286,  0.0534,  0.0199,  0.0396,\n",
      "        -0.1178, -0.1646,  0.1422,  0.0675, -0.1516,  0.0126,  0.0755,  0.0108],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 1.9555e-01, -1.9154e-02,  4.0533e-03,  8.8630e-03,  1.6104e-01,\n",
      "         1.3460e-01,  1.2374e-01,  3.4513e-03,  8.6277e-02, -1.3101e-01,\n",
      "        -2.6375e-02,  1.4244e-01,  1.0913e-02,  6.5643e-02, -2.5559e-02,\n",
      "         2.9884e-03,  1.7270e-01, -1.5933e-01, -9.9392e-02, -6.0529e-02,\n",
      "        -4.2657e-03,  1.0041e-01,  1.3490e-02,  9.1326e-02,  2.3769e-02,\n",
      "        -5.4231e-02,  1.0970e-01,  6.4820e-02,  1.4108e-02, -3.0276e-03,\n",
      "        -1.2247e-01, -7.6619e-02,  1.0927e-03, -4.3134e-02, -3.9999e-03,\n",
      "        -5.4683e-02, -1.5693e-02, -7.5501e-02, -1.4086e-02,  7.0981e-02,\n",
      "        -4.7059e-02, -4.2485e-02,  2.8684e-02,  1.3341e-01, -9.4604e-02,\n",
      "        -3.6968e-02,  7.7719e-02, -4.4067e-02, -1.7084e-01,  5.3462e-02,\n",
      "         8.1834e-02,  1.1980e-01, -3.6397e-02,  8.6537e-02,  1.0283e-01,\n",
      "         6.8020e-02,  1.7204e-01, -1.2850e-01,  2.5840e-02, -1.3316e-02,\n",
      "        -5.4572e-03, -1.8675e-01, -5.8890e-02, -1.6224e-01,  8.8007e-02,\n",
      "         1.7546e-01,  1.9230e-01, -1.3620e-05, -1.7047e-01,  1.5737e-01,\n",
      "         2.6283e-01,  1.2367e-01,  1.5266e-01, -2.7793e-01,  1.2686e-01,\n",
      "        -9.6419e-02,  4.8974e-02,  2.3446e-02,  1.2625e-01, -1.5248e-02,\n",
      "         3.2221e-02, -5.5803e-02, -3.5182e-02,  5.8487e-02, -1.3663e-01,\n",
      "        -4.4660e-02,  5.8716e-02,  1.4102e-01,  4.2186e-02,  3.5307e-02,\n",
      "         1.8279e-02,  1.2175e-01, -2.6692e-02,  8.7681e-02,  3.8775e-03,\n",
      "        -1.8001e-01, -1.1323e-01,  5.7724e-03, -7.7076e-02,  8.9472e-02,\n",
      "        -1.6701e-01, -3.5957e-02,  1.0007e-01,  2.4982e-03,  6.5645e-02,\n",
      "        -8.5674e-02,  1.0456e-01, -8.5842e-02, -1.1916e-02, -1.4279e-01,\n",
      "        -1.1803e-01,  1.6489e-01, -6.2879e-02, -6.4684e-03,  9.6635e-02,\n",
      "        -2.2088e-02,  8.5004e-02, -1.3388e-01, -1.0352e-01, -2.1556e-01,\n",
      "        -7.9274e-02, -7.2926e-03,  7.0340e-03, -5.0937e-02,  7.2631e-02,\n",
      "         1.6095e-01,  5.8866e-02, -1.2703e-01,  8.6117e-02,  1.0394e-01,\n",
      "        -2.4048e-03,  3.0912e-02, -8.3566e-02,  9.0983e-03, -5.8384e-02,\n",
      "         3.4889e-02, -1.0094e-01, -2.3982e-03, -1.0242e-01,  3.0576e-02,\n",
      "         5.5387e-02,  1.6437e-01, -1.5853e-01,  1.1908e-01,  1.8835e-02,\n",
      "         4.2811e-02,  1.6680e-01,  1.1387e-01, -6.2410e-02, -9.1973e-02,\n",
      "        -1.6068e-01, -5.2537e-02,  6.5882e-02,  1.1197e-01, -2.5650e-02,\n",
      "         6.2430e-02,  1.6734e-02, -3.6224e-02, -1.4471e-01, -1.5786e-02,\n",
      "         5.3479e-02, -3.2779e-03,  8.2666e-04, -3.4600e-02, -1.0568e-01,\n",
      "         3.9182e-02, -6.1157e-02,  8.5515e-02, -3.6284e-02, -2.0502e-01,\n",
      "         3.2665e-04,  5.0568e-03,  2.1327e-02,  1.4197e-02,  2.4138e-02,\n",
      "         1.9499e-01, -9.3468e-03, -6.9753e-02,  3.3963e-02, -7.2028e-02,\n",
      "        -3.3002e-02, -5.8156e-02, -4.1266e-02,  2.2453e-01,  5.3100e-02,\n",
      "        -1.5663e-01,  3.7128e-02, -4.9145e-02,  7.5591e-03,  8.0658e-02,\n",
      "        -1.2499e-01, -7.5940e-02, -5.6618e-02,  8.6870e-02, -1.1000e-02,\n",
      "         1.0171e-01,  1.5123e-01, -1.5422e-01, -7.1690e-02, -1.4925e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([ 0.0379,  0.0410, -0.0077, -0.1673,  0.0169, -0.1029, -0.0060,  0.1088,\n",
      "        -0.0377, -0.0367,  0.0376, -0.2465,  0.0889, -0.1333,  0.1366,  0.1698,\n",
      "        -0.0077, -0.0586,  0.0176, -0.1812, -0.1799, -0.1866, -0.1571,  0.0202,\n",
      "         0.0423, -0.1791,  0.1273,  0.0747,  0.0456,  0.0806,  0.0327,  0.0314,\n",
      "         0.0410,  0.0215, -0.0275,  0.1745, -0.1158, -0.0512, -0.0039,  0.0283,\n",
      "         0.0150,  0.0627, -0.0281, -0.0777, -0.1003,  0.0734, -0.0343, -0.0710,\n",
      "        -0.2037,  0.0760, -0.0565, -0.0586, -0.1308, -0.0600, -0.0205, -0.1515,\n",
      "        -0.0214, -0.1356, -0.0402, -0.0773, -0.0163,  0.0972, -0.0225, -0.0059,\n",
      "        -0.1179, -0.0069,  0.1238,  0.1539, -0.0538, -0.0351, -0.0535,  0.0963,\n",
      "        -0.0163,  0.1104,  0.0867,  0.1198, -0.1905, -0.2251, -0.0442,  0.0245,\n",
      "         0.0204, -0.0192,  0.0177, -0.1366, -0.1205,  0.0707,  0.0503,  0.0081,\n",
      "        -0.0992, -0.0265,  0.0075, -0.0313,  0.1484, -0.1196,  0.0782,  0.0601,\n",
      "         0.0410, -0.0516,  0.0617,  0.0547, -0.0398,  0.0004, -0.1971, -0.0036,\n",
      "        -0.1703,  0.0152,  0.0020, -0.1062,  0.1671, -0.0711, -0.0804, -0.0742,\n",
      "        -0.1240,  0.0507,  0.2575, -0.0438, -0.0201, -0.1368,  0.0849,  0.0513,\n",
      "         0.0148,  0.0251, -0.0265,  0.0395, -0.1313,  0.0346, -0.0057, -0.0497,\n",
      "        -0.0779,  0.2452, -0.0845, -0.1213,  0.0227, -0.0438, -0.1431, -0.2550,\n",
      "         0.3106, -0.0703, -0.0569, -0.0820,  0.2085,  0.0255, -0.0709,  0.1401,\n",
      "         0.0934, -0.0050,  0.0286, -0.1154,  0.0033,  0.0402, -0.0981,  0.0807,\n",
      "        -0.1366, -0.0645, -0.0214, -0.1102,  0.0177,  0.0255, -0.2325,  0.1133,\n",
      "        -0.0558, -0.0530,  0.0158, -0.1177, -0.2947, -0.0356,  0.0964,  0.1721,\n",
      "         0.0948,  0.1188, -0.1148,  0.1388, -0.0629,  0.0829, -0.0249, -0.0344,\n",
      "         0.1639, -0.0356, -0.0070,  0.0981, -0.0177, -0.0848, -0.0240,  0.0900,\n",
      "        -0.0472,  0.1569, -0.0295,  0.0374,  0.0996, -0.3266,  0.0190,  0.0009,\n",
      "        -0.1126,  0.0276, -0.0216, -0.1835,  0.0165, -0.1299,  0.0847,  0.0919],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 0.1904, -0.1626, -0.0901, -0.1233,  0.0629,  0.0545, -0.0003,  0.2393,\n",
      "         0.1488, -0.1123,  0.1040, -0.2451, -0.0848, -0.0957,  0.0316, -0.1060,\n",
      "         0.0104,  0.0650,  0.0427, -0.0330, -0.1224,  0.0669,  0.0074,  0.0277,\n",
      "         0.1278,  0.0631, -0.0137,  0.0522,  0.1078,  0.0875, -0.0121,  0.0527,\n",
      "         0.0626, -0.0146, -0.1497, -0.0154,  0.2076, -0.1679, -0.0388, -0.0487,\n",
      "         0.0033,  0.0033, -0.0435,  0.1483,  0.0640, -0.0198,  0.0209, -0.0521,\n",
      "        -0.0334, -0.0221,  0.0247,  0.1132,  0.0135,  0.0179,  0.0160, -0.1622,\n",
      "         0.0569,  0.0460, -0.1437,  0.1257, -0.0985, -0.0404,  0.0536, -0.0807,\n",
      "         0.1038,  0.1548,  0.2168, -0.1753,  0.0481, -0.0181,  0.0157,  0.0509,\n",
      "         0.1315, -0.0840,  0.0323, -0.0523, -0.0871, -0.0523,  0.0784,  0.1449,\n",
      "         0.0337,  0.0289, -0.0625, -0.0469, -0.1177, -0.1476, -0.1388,  0.0321,\n",
      "         0.0909,  0.0231, -0.0428,  0.0389, -0.0958,  0.0471, -0.0579,  0.0447,\n",
      "         0.0550,  0.0444, -0.1868, -0.0244, -0.0150, -0.0142, -0.0893,  0.0036,\n",
      "        -0.0437,  0.0629, -0.0566, -0.0211,  0.0057,  0.1275, -0.1458,  0.1470,\n",
      "         0.0088, -0.0710,  0.0171,  0.0528,  0.1020,  0.1438, -0.1695,  0.1398,\n",
      "        -0.1008,  0.2057,  0.0355,  0.0578, -0.0564,  0.0219, -0.0592,  0.0573,\n",
      "         0.0303, -0.0716, -0.0702, -0.1491, -0.0610,  0.1158,  0.0525,  0.0211,\n",
      "         0.0069,  0.0068, -0.1205, -0.0316,  0.0085,  0.1599,  0.0324, -0.0728,\n",
      "        -0.0987, -0.0477,  0.0021, -0.0482, -0.0053, -0.1071, -0.1143,  0.1128,\n",
      "         0.0476, -0.1080, -0.0545, -0.0695, -0.0236,  0.1151, -0.1131,  0.0786,\n",
      "        -0.0572, -0.1303,  0.2422,  0.0497,  0.0975, -0.0356, -0.0613, -0.1402,\n",
      "        -0.0100,  0.0166,  0.1481, -0.0313,  0.0904,  0.0349, -0.0335,  0.0472,\n",
      "        -0.1384, -0.1755, -0.0279,  0.0963, -0.0552, -0.1468,  0.1075,  0.0158,\n",
      "        -0.0101,  0.0175,  0.1268, -0.0104, -0.2286,  0.0104, -0.0109,  0.1966,\n",
      "         0.0183,  0.1103, -0.0354, -0.1124, -0.0884, -0.0358, -0.0310,  0.0341],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 9.6258e-02, -2.1508e-01,  6.8629e-02, -2.4834e-02, -1.7979e-01,\n",
      "         8.5534e-02,  2.2617e-01, -9.2144e-03,  2.3605e-02, -3.2127e-02,\n",
      "         5.7421e-02,  2.7539e-02,  2.3443e-02,  1.4535e-01,  1.1506e-01,\n",
      "        -1.0219e-01, -1.3391e-02,  8.5387e-02,  9.4357e-04,  2.0701e-02,\n",
      "        -5.4870e-02,  5.8772e-02,  8.9327e-02, -3.5904e-02, -9.6165e-02,\n",
      "         6.9461e-02, -7.2430e-02,  1.5335e-02, -7.6729e-02, -6.7611e-02,\n",
      "         2.2931e-01,  7.0916e-02, -4.0495e-02, -5.0390e-02, -1.2655e-01,\n",
      "        -4.0760e-02,  2.1024e-01,  2.0152e-04, -9.2654e-02,  1.8429e-01,\n",
      "        -9.5624e-02,  6.3968e-02, -9.3181e-02, -3.0189e-02,  2.6055e-02,\n",
      "        -1.0770e-01,  5.6201e-02, -1.1784e-01, -2.0952e-01, -1.3779e-01,\n",
      "        -9.9823e-02, -1.2550e-02, -6.7969e-02, -5.0696e-02,  2.3376e-04,\n",
      "        -3.7046e-02, -4.2878e-02,  2.7069e-02, -9.5510e-02, -9.7365e-02,\n",
      "         6.0529e-03, -7.3791e-02, -4.4721e-02,  1.6460e-02,  6.9008e-02,\n",
      "         9.3039e-02, -9.6108e-02,  1.2111e-01, -1.0624e-01, -1.9796e-02,\n",
      "        -8.3274e-02, -1.0999e-01, -8.7895e-02,  9.1494e-03, -2.0119e-01,\n",
      "        -2.2669e-02,  1.0493e-01,  1.6535e-01,  3.5481e-02, -1.2333e-02,\n",
      "        -5.2606e-02,  1.1774e-01,  7.5314e-02,  1.0149e-01, -4.3562e-03,\n",
      "         3.3850e-02, -1.1122e-01, -1.0174e-02, -1.5621e-02, -1.1048e-02,\n",
      "         6.4945e-02,  1.1964e-02, -1.3193e-02, -9.9668e-02, -2.2904e-02,\n",
      "        -1.7255e-03,  2.8983e-02, -8.4993e-02, -4.7439e-02,  1.8318e-01,\n",
      "         1.4683e-01,  2.8446e-02, -1.2522e-01,  6.8532e-02, -1.0948e-01,\n",
      "        -4.6557e-02, -1.8353e-02, -5.5972e-02,  1.0633e-01,  5.3485e-03,\n",
      "         4.0922e-02, -8.0141e-02,  2.5502e-02, -5.0942e-02,  1.9637e-02,\n",
      "        -8.2472e-02, -8.0561e-02,  3.4663e-02, -8.0706e-02,  1.3223e-01,\n",
      "         9.5705e-02, -3.4508e-02, -8.2318e-02, -2.8594e-02, -1.8709e-02,\n",
      "         8.2884e-02, -1.3155e-02, -2.1714e-01,  1.0165e-01, -8.9749e-02,\n",
      "         1.1360e-01, -2.3420e-01, -7.8970e-02, -7.2658e-02,  6.3994e-02,\n",
      "         1.2081e-01, -1.6663e-01, -1.7321e-02,  1.3837e-01, -1.1058e-02,\n",
      "        -8.1203e-02, -2.9578e-02,  2.6000e-02, -1.5707e-01, -4.4026e-02,\n",
      "         1.1599e-01, -7.9545e-02,  9.8230e-02, -1.8443e-01, -9.1308e-03,\n",
      "        -4.4471e-02,  4.7459e-03, -1.4219e-01, -1.4681e-02,  1.8738e-01,\n",
      "         5.8502e-02,  3.7652e-02, -2.0954e-02, -8.2572e-02,  7.2402e-02,\n",
      "         1.1786e-01, -2.0909e-01, -7.7258e-02,  3.3060e-02, -9.3772e-02,\n",
      "         1.5998e-01,  2.3606e-02, -8.3187e-02,  6.8119e-02, -1.0863e-01,\n",
      "         2.9657e-02,  1.0015e-01, -5.6864e-02,  2.1930e-02,  4.2841e-02,\n",
      "        -2.9274e-02,  4.5595e-02,  1.3323e-02, -1.1289e-01,  1.6550e-01,\n",
      "         3.2507e-02,  1.0522e-01,  3.6010e-02,  1.0785e-02,  1.5137e-02,\n",
      "         7.1953e-02,  1.3204e-02, -1.2634e-01, -6.7418e-02,  2.1902e-01,\n",
      "         5.6100e-03,  9.5498e-02,  2.7252e-02,  1.8824e-01, -4.5046e-02,\n",
      "         8.4698e-03,  1.0092e-01, -7.4647e-02, -1.2552e-01, -2.0398e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-2.0205e-02, -1.1013e-01,  5.8022e-02, -2.5768e-02,  2.4979e-02,\n",
      "        -1.9292e-01,  1.5215e-01,  1.5879e-01,  1.6166e-01, -2.1876e-02,\n",
      "         1.6827e-01, -1.1644e-01, -1.0294e-01, -2.8063e-01, -1.7714e-02,\n",
      "        -1.1082e-01, -1.7296e-02, -6.9864e-02, -9.8872e-03, -9.0847e-02,\n",
      "         3.6029e-03,  1.9835e-02, -7.7576e-02, -5.4699e-02, -1.1383e-01,\n",
      "         6.5865e-02, -2.1916e-02, -2.1030e-03, -2.7866e-02,  5.7071e-02,\n",
      "         1.1594e-01,  9.0781e-02, -2.0605e-02, -3.2649e-02, -4.7844e-02,\n",
      "        -1.0099e-01, -1.3139e-01,  2.1791e-03, -2.7328e-02,  6.9691e-02,\n",
      "         8.7039e-02,  2.7937e-03,  1.2302e-02,  9.7618e-02, -4.4253e-02,\n",
      "         5.6436e-02, -3.4447e-02,  9.4440e-02,  1.4314e-01, -2.8476e-02,\n",
      "        -3.0825e-01, -8.5780e-03,  2.8340e-02, -1.5463e-02,  6.3955e-02,\n",
      "         1.8227e-02, -1.0739e-01, -4.7313e-02, -2.8477e-02,  1.3767e-01,\n",
      "         9.9509e-02, -1.0678e-01, -2.5091e-02,  2.5272e-01,  1.8641e-01,\n",
      "         1.0806e-02,  2.3159e-01, -7.7877e-03,  2.7617e-01, -4.3622e-03,\n",
      "        -1.5866e-02,  1.4828e-01,  4.3153e-02,  5.3986e-02, -7.9974e-02,\n",
      "         4.7948e-02, -3.3726e-02,  5.1044e-02, -2.3072e-02,  3.1300e-03,\n",
      "        -6.6946e-02, -9.0948e-02, -2.4262e-02, -7.9788e-02,  4.7120e-02,\n",
      "         7.1624e-02, -1.4178e-01, -5.7686e-02,  1.0957e-01, -2.2785e-02,\n",
      "         1.2695e-01,  1.2088e-01, -8.6939e-02, -8.3716e-02, -1.2764e-01,\n",
      "        -6.2596e-04, -1.4140e-01,  7.8936e-02, -1.8772e-01, -7.3740e-02,\n",
      "        -6.6950e-02,  2.6612e-02, -8.2052e-02, -1.4097e-01, -7.5349e-02,\n",
      "        -5.7891e-04, -7.3327e-02, -2.1039e-02,  1.0284e-01, -3.2119e-02,\n",
      "        -3.9870e-02,  6.5162e-04, -9.5241e-02, -2.8142e-02, -9.2705e-02,\n",
      "        -2.8549e-02, -1.2855e-01,  2.6159e-03,  7.3574e-02, -1.1391e-01,\n",
      "        -1.8793e-01,  4.8057e-02,  6.3253e-02,  2.3326e-01,  4.1880e-02,\n",
      "        -1.0155e-01, -4.7617e-02,  6.7032e-02, -5.8309e-02, -1.5364e-03,\n",
      "        -2.3916e-02, -1.6766e-01,  1.5301e-01,  6.0799e-02,  9.0580e-02,\n",
      "        -9.4674e-02, -8.9547e-03,  7.3856e-02,  2.8584e-02, -7.4241e-02,\n",
      "         1.1411e-01,  9.5076e-02,  9.8011e-02,  1.1818e-01,  1.0348e-03,\n",
      "         1.4200e-01,  1.9963e-01,  9.0306e-02, -8.6846e-02, -2.9934e-02,\n",
      "         1.8275e-02, -6.8610e-02,  7.7196e-03, -1.5499e-01, -1.5363e-04,\n",
      "         1.4205e-01,  8.5517e-02, -1.6759e-01,  1.0055e-01,  3.9591e-04,\n",
      "         1.6994e-01, -3.7381e-02,  1.1618e-01, -4.5691e-02,  4.9932e-02,\n",
      "        -6.1530e-02, -5.1304e-02,  1.2775e-01,  2.6310e-02, -2.1155e-02,\n",
      "        -5.7866e-02, -1.0657e-01,  6.0984e-02, -1.7181e-01, -9.4698e-02,\n",
      "        -8.6974e-03,  3.5473e-02,  1.5183e-02,  6.5360e-02, -5.7783e-02,\n",
      "         2.2428e-02,  1.4298e-01, -7.5094e-02, -1.3085e-01, -1.0930e-01,\n",
      "        -4.8731e-02, -3.0106e-02,  1.2585e-02,  4.8389e-03, -9.4451e-02,\n",
      "        -1.2080e-01, -1.3470e-01, -2.8989e-02, -1.5674e-01,  3.8817e-02,\n",
      "        -9.3956e-03, -7.7273e-02,  8.2180e-02,  1.7978e-01,  1.7426e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  0.066614/  0.081014, val:  48.75%, val_best:  48.75%, tr:  73.85%, tr_best:  73.85%, epoch time: 68.28 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0543%\n",
      "layer   2  Sparsity: 55.7875%\n",
      "layer   3  Sparsity: 54.4872%\n",
      "total_backward_count 9790 real_backward_count 3494  35.689%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  0.046476/  0.070647, val:  50.83%, val_best:  50.83%, tr:  85.60%, tr_best:  85.60%, epoch time: 68.13 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   2  Sparsity: 52.2291%\n",
      "layer   3  Sparsity: 55.4059%\n",
      "total_backward_count 19580 real_backward_count 5698  29.101%\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  0.041986/  0.071668, val:  51.25%, val_best:  51.25%, tr:  87.33%, tr_best:  87.33%, epoch time: 68.46 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0780%\n",
      "layer   2  Sparsity: 50.7762%\n",
      "layer   3  Sparsity: 56.6221%\n",
      "total_backward_count 29370 real_backward_count 7700  26.217%\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  0.039536/  0.073038, val:  50.42%, val_best:  51.25%, tr:  89.07%, tr_best:  89.07%, epoch time: 68.14 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1040%\n",
      "layer   2  Sparsity: 50.2107%\n",
      "layer   3  Sparsity: 56.9794%\n",
      "total_backward_count 39160 real_backward_count 9587  24.482%\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  0.037987/  0.063612, val:  60.00%, val_best:  60.00%, tr:  89.89%, tr_best:  89.89%, epoch time: 68.25 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0467%\n",
      "layer   2  Sparsity: 50.0805%\n",
      "layer   3  Sparsity: 57.1970%\n",
      "total_backward_count 48950 real_backward_count 11400  23.289%\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  0.036542/  0.063843, val:  59.17%, val_best:  60.00%, tr:  91.01%, tr_best:  91.01%, epoch time: 68.02 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0971%\n",
      "layer   2  Sparsity: 50.2363%\n",
      "layer   3  Sparsity: 57.0673%\n",
      "total_backward_count 58740 real_backward_count 13156  22.397%\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  0.035481/  0.075242, val:  53.33%, val_best:  60.00%, tr:  92.13%, tr_best:  92.13%, epoch time: 68.25 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   2  Sparsity: 50.0980%\n",
      "layer   3  Sparsity: 56.8417%\n",
      "total_backward_count 68530 real_backward_count 14849  21.668%\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  0.034708/  0.064243, val:  60.00%, val_best:  60.00%, tr:  92.44%, tr_best:  92.44%, epoch time: 67.89 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0228%\n",
      "layer   2  Sparsity: 50.2790%\n",
      "layer   3  Sparsity: 56.5972%\n",
      "total_backward_count 78320 real_backward_count 16495  21.061%\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  0.032770/  0.060255, val:  61.25%, val_best:  61.25%, tr:  93.05%, tr_best:  93.05%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   2  Sparsity: 50.3745%\n",
      "layer   3  Sparsity: 56.5529%\n",
      "total_backward_count 88110 real_backward_count 18011  20.441%\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  0.032808/  0.057941, val:  61.67%, val_best:  61.67%, tr:  92.13%, tr_best:  93.05%, epoch time: 68.13 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   2  Sparsity: 50.4240%\n",
      "layer   3  Sparsity: 56.6313%\n",
      "total_backward_count 97900 real_backward_count 19588  20.008%\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  0.031780/  0.061139, val:  60.00%, val_best:  61.67%, tr:  92.65%, tr_best:  93.05%, epoch time: 67.35 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   2  Sparsity: 50.2655%\n",
      "layer   3  Sparsity: 56.6493%\n",
      "total_backward_count 107690 real_backward_count 21044  19.541%\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  0.031468/  0.067607, val:  60.00%, val_best:  61.67%, tr:  93.46%, tr_best:  93.46%, epoch time: 67.97 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   2  Sparsity: 50.3173%\n",
      "layer   3  Sparsity: 56.5396%\n",
      "total_backward_count 117480 real_backward_count 22540  19.186%\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  0.029881/  0.058519, val:  63.33%, val_best:  63.33%, tr:  94.38%, tr_best:  94.38%, epoch time: 67.85 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0466%\n",
      "layer   2  Sparsity: 50.4276%\n",
      "layer   3  Sparsity: 56.3367%\n",
      "total_backward_count 127270 real_backward_count 23944  18.814%\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  0.028942/  0.055285, val:  69.58%, val_best:  69.58%, tr:  94.69%, tr_best:  94.69%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1119%\n",
      "layer   2  Sparsity: 50.8226%\n",
      "layer   3  Sparsity: 56.4077%\n",
      "total_backward_count 137060 real_backward_count 25280  18.444%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  0.028347/  0.053594, val:  69.17%, val_best:  69.58%, tr:  96.32%, tr_best:  96.32%, epoch time: 68.45 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0737%\n",
      "layer   2  Sparsity: 50.9220%\n",
      "layer   3  Sparsity: 56.1247%\n",
      "total_backward_count 146850 real_backward_count 26571  18.094%\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  0.028014/  0.058113, val:  68.33%, val_best:  69.58%, tr:  96.53%, tr_best:  96.53%, epoch time: 68.53 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1102%\n",
      "layer   2  Sparsity: 51.0429%\n",
      "layer   3  Sparsity: 55.9797%\n",
      "total_backward_count 156640 real_backward_count 27835  17.770%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  0.027269/  0.062332, val:  57.08%, val_best:  69.58%, tr:  97.04%, tr_best:  97.04%, epoch time: 68.16 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   2  Sparsity: 51.3424%\n",
      "layer   3  Sparsity: 55.9369%\n",
      "total_backward_count 166430 real_backward_count 29079  17.472%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  0.026608/  0.052731, val:  71.25%, val_best:  71.25%, tr:  97.45%, tr_best:  97.45%, epoch time: 68.35 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0863%\n",
      "layer   2  Sparsity: 51.0302%\n",
      "layer   3  Sparsity: 55.8953%\n",
      "total_backward_count 176220 real_backward_count 30271  17.178%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  0.025872/  0.052455, val:  71.67%, val_best:  71.67%, tr:  97.85%, tr_best:  97.85%, epoch time: 68.11 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0364%\n",
      "layer   2  Sparsity: 51.0062%\n",
      "layer   3  Sparsity: 55.8783%\n",
      "total_backward_count 186010 real_backward_count 31380  16.870%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  0.025514/  0.051915, val:  78.33%, val_best:  78.33%, tr:  97.55%, tr_best:  97.85%, epoch time: 67.95 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   2  Sparsity: 51.2402%\n",
      "layer   3  Sparsity: 55.7706%\n",
      "total_backward_count 195800 real_backward_count 32509  16.603%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  0.024236/  0.053707, val:  71.25%, val_best:  78.33%, tr:  98.06%, tr_best:  98.06%, epoch time: 67.79 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   2  Sparsity: 51.2519%\n",
      "layer   3  Sparsity: 55.7146%\n",
      "total_backward_count 205590 real_backward_count 33541  16.315%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  0.024328/  0.048420, val:  82.08%, val_best:  82.08%, tr:  98.06%, tr_best:  98.06%, epoch time: 67.59 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 51.3622%\n",
      "layer   3  Sparsity: 55.3642%\n",
      "total_backward_count 215380 real_backward_count 34567  16.049%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  0.023644/  0.056740, val:  70.00%, val_best:  82.08%, tr:  98.26%, tr_best:  98.26%, epoch time: 68.07 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   2  Sparsity: 51.3406%\n",
      "layer   3  Sparsity: 55.2066%\n",
      "total_backward_count 225170 real_backward_count 35600  15.810%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  0.022756/  0.050407, val:  81.67%, val_best:  82.08%, tr:  98.98%, tr_best:  98.98%, epoch time: 68.81 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 51.4767%\n",
      "layer   3  Sparsity: 55.2136%\n",
      "total_backward_count 234960 real_backward_count 36576  15.567%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  0.021819/  0.056983, val:  68.75%, val_best:  82.08%, tr:  98.88%, tr_best:  98.98%, epoch time: 68.29 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0856%\n",
      "layer   2  Sparsity: 51.3742%\n",
      "layer   3  Sparsity: 55.1575%\n",
      "total_backward_count 244750 real_backward_count 37492  15.318%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  0.022547/  0.050520, val:  79.17%, val_best:  82.08%, tr:  98.88%, tr_best:  98.98%, epoch time: 68.06 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0622%\n",
      "layer   2  Sparsity: 51.3356%\n",
      "layer   3  Sparsity: 54.9220%\n",
      "total_backward_count 254540 real_backward_count 38469  15.113%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  0.021677/  0.050944, val:  80.83%, val_best:  82.08%, tr:  98.77%, tr_best:  98.98%, epoch time: 68.16 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   2  Sparsity: 51.3378%\n",
      "layer   3  Sparsity: 55.1182%\n",
      "total_backward_count 264330 real_backward_count 39364  14.892%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  0.021601/  0.049423, val:  80.00%, val_best:  82.08%, tr:  98.88%, tr_best:  98.98%, epoch time: 67.94 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 51.2816%\n",
      "layer   3  Sparsity: 55.0006%\n",
      "total_backward_count 274120 real_backward_count 40291  14.698%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  0.020737/  0.059944, val:  68.33%, val_best:  82.08%, tr:  99.28%, tr_best:  99.28%, epoch time: 68.14 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0986%\n",
      "layer   2  Sparsity: 51.2200%\n",
      "layer   3  Sparsity: 55.3121%\n",
      "total_backward_count 283910 real_backward_count 41151  14.494%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  0.021123/  0.046301, val:  81.25%, val_best:  82.08%, tr:  99.28%, tr_best:  99.28%, epoch time: 68.39 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0801%\n",
      "layer   2  Sparsity: 51.2318%\n",
      "layer   3  Sparsity: 55.2671%\n",
      "total_backward_count 293700 real_backward_count 42031  14.311%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  0.020566/  0.055051, val:  72.50%, val_best:  82.08%, tr:  98.98%, tr_best:  99.28%, epoch time: 67.79 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0970%\n",
      "layer   2  Sparsity: 51.2776%\n",
      "layer   3  Sparsity: 55.0242%\n",
      "total_backward_count 303490 real_backward_count 42902  14.136%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  0.020687/  0.045932, val:  84.58%, val_best:  84.58%, tr:  99.39%, tr_best:  99.39%, epoch time: 67.74 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   2  Sparsity: 51.2443%\n",
      "layer   3  Sparsity: 54.7884%\n",
      "total_backward_count 313280 real_backward_count 43796  13.980%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  0.019809/  0.047726, val:  75.83%, val_best:  84.58%, tr:  98.77%, tr_best:  99.39%, epoch time: 62.96 seconds, 1.05 minutes\n",
      "layer   1  Sparsity: 91.0607%\n",
      "layer   2  Sparsity: 51.2385%\n",
      "layer   3  Sparsity: 54.7495%\n",
      "total_backward_count 323070 real_backward_count 44594  13.803%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  0.020024/  0.045254, val:  81.67%, val_best:  84.58%, tr:  98.98%, tr_best:  99.39%, epoch time: 67.24 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   2  Sparsity: 51.2902%\n",
      "layer   3  Sparsity: 54.9626%\n",
      "total_backward_count 332860 real_backward_count 45468  13.660%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  0.020001/  0.044208, val:  82.50%, val_best:  84.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 67.41 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   2  Sparsity: 51.2984%\n",
      "layer   3  Sparsity: 54.9375%\n",
      "total_backward_count 342650 real_backward_count 46328  13.521%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  0.019037/  0.048492, val:  80.83%, val_best:  84.58%, tr:  99.39%, tr_best:  99.49%, epoch time: 67.42 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   2  Sparsity: 51.4361%\n",
      "layer   3  Sparsity: 54.7842%\n",
      "total_backward_count 352440 real_backward_count 47089  13.361%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  0.019388/  0.049154, val:  80.42%, val_best:  84.58%, tr:  99.39%, tr_best:  99.49%, epoch time: 67.66 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0715%\n",
      "layer   2  Sparsity: 51.3526%\n",
      "layer   3  Sparsity: 54.8084%\n",
      "total_backward_count 362230 real_backward_count 47903  13.224%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  0.018740/  0.043522, val:  84.17%, val_best:  84.58%, tr:  99.28%, tr_best:  99.49%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   2  Sparsity: 51.2791%\n",
      "layer   3  Sparsity: 54.7662%\n",
      "total_backward_count 372020 real_backward_count 48668  13.082%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  0.018492/  0.046307, val:  82.92%, val_best:  84.58%, tr:  99.28%, tr_best:  99.49%, epoch time: 67.36 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1116%\n",
      "layer   2  Sparsity: 51.4932%\n",
      "layer   3  Sparsity: 54.7229%\n",
      "total_backward_count 381810 real_backward_count 49404  12.939%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  0.018040/  0.044315, val:  80.83%, val_best:  84.58%, tr:  99.18%, tr_best:  99.49%, epoch time: 67.29 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0712%\n",
      "layer   2  Sparsity: 51.4626%\n",
      "layer   3  Sparsity: 54.7208%\n",
      "total_backward_count 391600 real_backward_count 50098  12.793%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  0.017643/  0.045930, val:  80.00%, val_best:  84.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 67.80 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0574%\n",
      "layer   2  Sparsity: 51.5787%\n",
      "layer   3  Sparsity: 54.5820%\n",
      "total_backward_count 401390 real_backward_count 50763  12.647%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  0.017910/  0.048150, val:  82.08%, val_best:  84.58%, tr:  99.39%, tr_best:  99.49%, epoch time: 67.55 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   2  Sparsity: 51.6365%\n",
      "layer   3  Sparsity: 54.4959%\n",
      "total_backward_count 411180 real_backward_count 51471  12.518%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  0.017776/  0.050749, val:  80.00%, val_best:  84.58%, tr:  99.39%, tr_best:  99.49%, epoch time: 68.23 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0910%\n",
      "layer   2  Sparsity: 51.6280%\n",
      "layer   3  Sparsity: 54.5174%\n",
      "total_backward_count 420970 real_backward_count 52151  12.388%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  0.017577/  0.050340, val:  75.42%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 67.87 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0676%\n",
      "layer   2  Sparsity: 51.7112%\n",
      "layer   3  Sparsity: 54.5587%\n",
      "total_backward_count 430760 real_backward_count 52844  12.268%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  0.017331/  0.045306, val:  81.25%, val_best:  84.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 67.64 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   2  Sparsity: 51.7836%\n",
      "layer   3  Sparsity: 54.5877%\n",
      "total_backward_count 440550 real_backward_count 53527  12.150%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  0.016625/  0.044677, val:  83.33%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 68.03 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0620%\n",
      "layer   2  Sparsity: 51.7872%\n",
      "layer   3  Sparsity: 54.6437%\n",
      "total_backward_count 450340 real_backward_count 54165  12.028%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  0.016593/  0.043042, val:  82.92%, val_best:  84.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 67.73 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0458%\n",
      "layer   2  Sparsity: 51.8107%\n",
      "layer   3  Sparsity: 54.6951%\n",
      "total_backward_count 460130 real_backward_count 54802  11.910%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  0.016691/  0.041566, val:  86.25%, val_best:  86.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 67.72 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0743%\n",
      "layer   2  Sparsity: 51.7055%\n",
      "layer   3  Sparsity: 54.5951%\n",
      "total_backward_count 469920 real_backward_count 55458  11.802%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  0.016522/  0.044833, val:  80.42%, val_best:  86.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 67.89 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 51.5866%\n",
      "layer   3  Sparsity: 54.6236%\n",
      "total_backward_count 479710 real_backward_count 56090  11.692%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  0.016901/  0.041903, val:  83.75%, val_best:  86.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 67.30 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   2  Sparsity: 51.7127%\n",
      "layer   3  Sparsity: 54.5315%\n",
      "total_backward_count 489500 real_backward_count 56780  11.600%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  0.016384/  0.045746, val:  79.58%, val_best:  86.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 67.75 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0524%\n",
      "layer   2  Sparsity: 51.6905%\n",
      "layer   3  Sparsity: 54.5823%\n",
      "total_backward_count 499290 real_backward_count 57411  11.499%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  0.016781/  0.047995, val:  76.67%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 68.01 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0631%\n",
      "layer   2  Sparsity: 51.7813%\n",
      "layer   3  Sparsity: 54.3264%\n",
      "total_backward_count 509080 real_backward_count 58085  11.410%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  0.015673/  0.041664, val:  82.08%, val_best:  86.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 67.58 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   2  Sparsity: 51.8260%\n",
      "layer   3  Sparsity: 54.3197%\n",
      "total_backward_count 518870 real_backward_count 58662  11.306%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  0.015805/  0.042838, val:  82.92%, val_best:  86.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 68.11 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1004%\n",
      "layer   2  Sparsity: 51.7892%\n",
      "layer   3  Sparsity: 54.3897%\n",
      "total_backward_count 528660 real_backward_count 59246  11.207%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  0.015357/  0.050090, val:  82.92%, val_best:  86.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 68.22 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   2  Sparsity: 51.7557%\n",
      "layer   3  Sparsity: 54.3966%\n",
      "total_backward_count 538450 real_backward_count 59820  11.110%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  0.015495/  0.040675, val:  87.50%, val_best:  87.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 67.64 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0743%\n",
      "layer   2  Sparsity: 51.8307%\n",
      "layer   3  Sparsity: 54.6570%\n",
      "total_backward_count 548240 real_backward_count 60409  11.019%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  0.015694/  0.042010, val:  87.92%, val_best:  87.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 67.30 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0780%\n",
      "layer   2  Sparsity: 51.7460%\n",
      "layer   3  Sparsity: 54.5053%\n",
      "total_backward_count 558030 real_backward_count 61000  10.931%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  0.014924/  0.045388, val:  84.17%, val_best:  87.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0622%\n",
      "layer   2  Sparsity: 51.8594%\n",
      "layer   3  Sparsity: 54.6038%\n",
      "total_backward_count 567820 real_backward_count 61560  10.841%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  0.015392/  0.042428, val:  84.17%, val_best:  87.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 68.15 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0568%\n",
      "layer   2  Sparsity: 51.8939%\n",
      "layer   3  Sparsity: 54.7322%\n",
      "total_backward_count 577610 real_backward_count 62185  10.766%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  0.015233/  0.040459, val:  85.00%, val_best:  87.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 68.13 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0519%\n",
      "layer   2  Sparsity: 51.6992%\n",
      "layer   3  Sparsity: 54.7094%\n",
      "total_backward_count 587400 real_backward_count 62776  10.687%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  0.014964/  0.041139, val:  83.33%, val_best:  87.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 68.38 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 51.7510%\n",
      "layer   3  Sparsity: 54.7193%\n",
      "total_backward_count 597190 real_backward_count 63393  10.615%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  0.014477/  0.049042, val:  84.58%, val_best:  87.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 67.75 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   2  Sparsity: 51.8694%\n",
      "layer   3  Sparsity: 54.6843%\n",
      "total_backward_count 606980 real_backward_count 63939  10.534%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  0.014092/  0.043893, val:  81.25%, val_best:  87.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 68.00 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0664%\n",
      "layer   2  Sparsity: 51.8959%\n",
      "layer   3  Sparsity: 54.5909%\n",
      "total_backward_count 616770 real_backward_count 64458  10.451%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  0.014418/  0.041667, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.07 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0854%\n",
      "layer   2  Sparsity: 51.9948%\n",
      "layer   3  Sparsity: 54.5501%\n",
      "total_backward_count 626560 real_backward_count 65012  10.376%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  0.014908/  0.039403, val:  87.92%, val_best:  87.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 67.30 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0938%\n",
      "layer   2  Sparsity: 51.8128%\n",
      "layer   3  Sparsity: 54.4414%\n",
      "total_backward_count 636350 real_backward_count 65603  10.309%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  0.014381/  0.040813, val:  85.00%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 67.75 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1065%\n",
      "layer   2  Sparsity: 51.8883%\n",
      "layer   3  Sparsity: 54.5702%\n",
      "total_backward_count 646140 real_backward_count 66153  10.238%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  0.013424/  0.041664, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0691%\n",
      "layer   2  Sparsity: 51.8779%\n",
      "layer   3  Sparsity: 54.3687%\n",
      "total_backward_count 655930 real_backward_count 66643  10.160%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  0.014127/  0.042973, val:  82.92%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.66 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   2  Sparsity: 51.8535%\n",
      "layer   3  Sparsity: 54.2696%\n",
      "total_backward_count 665720 real_backward_count 67173  10.090%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  0.013859/  0.039259, val:  87.92%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 67.66 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0652%\n",
      "layer   2  Sparsity: 51.9280%\n",
      "layer   3  Sparsity: 54.3051%\n",
      "total_backward_count 675510 real_backward_count 67710  10.024%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  0.013778/  0.040413, val:  85.83%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.32 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1106%\n",
      "layer   2  Sparsity: 51.9801%\n",
      "layer   3  Sparsity: 54.3609%\n",
      "total_backward_count 685300 real_backward_count 68242   9.958%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  0.013191/  0.039703, val:  85.00%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.30 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 51.9586%\n",
      "layer   3  Sparsity: 54.3404%\n",
      "total_backward_count 695090 real_backward_count 68707   9.885%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  0.013107/  0.041893, val:  87.08%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   2  Sparsity: 52.0594%\n",
      "layer   3  Sparsity: 54.4060%\n",
      "total_backward_count 704880 real_backward_count 69185   9.815%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  0.013385/  0.042230, val:  81.25%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.58 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0460%\n",
      "layer   2  Sparsity: 52.1115%\n",
      "layer   3  Sparsity: 54.5447%\n",
      "total_backward_count 714670 real_backward_count 69718   9.755%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  0.013247/  0.044560, val:  79.17%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.44 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   2  Sparsity: 52.3225%\n",
      "layer   3  Sparsity: 54.4560%\n",
      "total_backward_count 724460 real_backward_count 70238   9.695%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  0.013396/  0.039989, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.77 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   2  Sparsity: 52.2216%\n",
      "layer   3  Sparsity: 54.5285%\n",
      "total_backward_count 734250 real_backward_count 70757   9.637%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  0.013226/  0.038427, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.77 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   2  Sparsity: 52.2041%\n",
      "layer   3  Sparsity: 54.5525%\n",
      "total_backward_count 744040 real_backward_count 71262   9.578%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  0.013209/  0.038554, val:  85.42%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 68.35 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   2  Sparsity: 52.3137%\n",
      "layer   3  Sparsity: 54.5642%\n",
      "total_backward_count 753830 real_backward_count 71774   9.521%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  0.012870/  0.039098, val:  87.08%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.56 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 52.3354%\n",
      "layer   3  Sparsity: 54.5366%\n",
      "total_backward_count 763620 real_backward_count 72271   9.464%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  0.012695/  0.043163, val:  84.17%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.27 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   2  Sparsity: 52.2959%\n",
      "layer   3  Sparsity: 54.4998%\n",
      "total_backward_count 773410 real_backward_count 72754   9.407%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  0.012365/  0.041497, val:  84.58%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.54 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0855%\n",
      "layer   2  Sparsity: 52.3016%\n",
      "layer   3  Sparsity: 54.3694%\n",
      "total_backward_count 783200 real_backward_count 73234   9.351%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  0.012458/  0.038772, val:  85.00%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 67.31 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   2  Sparsity: 52.3461%\n",
      "layer   3  Sparsity: 54.5297%\n",
      "total_backward_count 792990 real_backward_count 73706   9.295%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  0.012574/  0.042795, val:  82.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.98 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0962%\n",
      "layer   2  Sparsity: 52.2795%\n",
      "layer   3  Sparsity: 54.4591%\n",
      "total_backward_count 802780 real_backward_count 74213   9.245%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  0.012416/  0.037301, val:  87.50%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.31 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1073%\n",
      "layer   2  Sparsity: 52.1961%\n",
      "layer   3  Sparsity: 54.5303%\n",
      "total_backward_count 812570 real_backward_count 74678   9.190%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  0.012424/  0.042013, val:  86.25%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.72 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.1197%\n",
      "layer   2  Sparsity: 52.2915%\n",
      "layer   3  Sparsity: 54.3171%\n",
      "total_backward_count 822360 real_backward_count 75160   9.140%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  0.012184/  0.039574, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.56 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0741%\n",
      "layer   2  Sparsity: 52.3695%\n",
      "layer   3  Sparsity: 54.3217%\n",
      "total_backward_count 832150 real_backward_count 75617   9.087%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  0.012879/  0.039593, val:  87.92%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 67.60 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0982%\n",
      "layer   2  Sparsity: 52.4258%\n",
      "layer   3  Sparsity: 54.5260%\n",
      "total_backward_count 841940 real_backward_count 76131   9.042%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  0.012076/  0.037359, val:  86.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.12 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1171%\n",
      "layer   2  Sparsity: 52.5288%\n",
      "layer   3  Sparsity: 54.6092%\n",
      "total_backward_count 851730 real_backward_count 76586   8.992%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  0.011989/  0.040106, val:  85.42%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.60 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0591%\n",
      "layer   2  Sparsity: 52.4790%\n",
      "layer   3  Sparsity: 54.7104%\n",
      "total_backward_count 861520 real_backward_count 77026   8.941%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  0.012046/  0.038011, val:  86.67%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.86 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   2  Sparsity: 52.3156%\n",
      "layer   3  Sparsity: 54.6562%\n",
      "total_backward_count 871310 real_backward_count 77486   8.893%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  0.011723/  0.039108, val:  85.42%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.70 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0915%\n",
      "layer   2  Sparsity: 52.3979%\n",
      "layer   3  Sparsity: 54.7368%\n",
      "total_backward_count 881100 real_backward_count 77921   8.844%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  0.011461/  0.039756, val:  87.50%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0773%\n",
      "layer   2  Sparsity: 52.4050%\n",
      "layer   3  Sparsity: 54.7156%\n",
      "total_backward_count 890890 real_backward_count 78342   8.794%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  0.011757/  0.042093, val:  85.00%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.55 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1260%\n",
      "layer   2  Sparsity: 52.2988%\n",
      "layer   3  Sparsity: 54.8189%\n",
      "total_backward_count 900680 real_backward_count 78799   8.749%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  0.011376/  0.038985, val:  86.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.95 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   2  Sparsity: 52.3386%\n",
      "layer   3  Sparsity: 54.7669%\n",
      "total_backward_count 910470 real_backward_count 79232   8.702%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  0.011210/  0.038456, val:  88.75%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.51 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0997%\n",
      "layer   2  Sparsity: 52.3370%\n",
      "layer   3  Sparsity: 54.7986%\n",
      "total_backward_count 920260 real_backward_count 79634   8.653%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  0.011373/  0.039073, val:  87.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   2  Sparsity: 52.3315%\n",
      "layer   3  Sparsity: 54.7566%\n",
      "total_backward_count 930050 real_backward_count 80063   8.608%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  0.011904/  0.039667, val:  86.67%, val_best:  88.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 67.22 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   2  Sparsity: 52.3710%\n",
      "layer   3  Sparsity: 54.9032%\n",
      "total_backward_count 939840 real_backward_count 80531   8.569%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  0.011570/  0.038435, val:  87.92%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.22 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 52.3685%\n",
      "layer   3  Sparsity: 54.8632%\n",
      "total_backward_count 949630 real_backward_count 80986   8.528%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  0.011198/  0.039108, val:  87.08%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.97 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0508%\n",
      "layer   2  Sparsity: 52.3691%\n",
      "layer   3  Sparsity: 54.9883%\n",
      "total_backward_count 959420 real_backward_count 81409   8.485%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  0.011477/  0.043343, val:  84.58%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.04 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0363%\n",
      "layer   2  Sparsity: 52.3497%\n",
      "layer   3  Sparsity: 55.1134%\n",
      "total_backward_count 969210 real_backward_count 81857   8.446%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  0.011483/  0.040318, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.90 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   2  Sparsity: 52.4080%\n",
      "layer   3  Sparsity: 55.0625%\n",
      "total_backward_count 979000 real_backward_count 82292   8.406%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  0.010894/  0.037195, val:  87.92%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.28 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   2  Sparsity: 52.6055%\n",
      "layer   3  Sparsity: 55.0951%\n",
      "total_backward_count 988790 real_backward_count 82695   8.363%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  0.010907/  0.038560, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.91 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1120%\n",
      "layer   2  Sparsity: 52.4999%\n",
      "layer   3  Sparsity: 54.8422%\n",
      "total_backward_count 998580 real_backward_count 83095   8.321%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  0.010795/  0.039812, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.86 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   2  Sparsity: 52.5116%\n",
      "layer   3  Sparsity: 54.8310%\n",
      "total_backward_count 1008370 real_backward_count 83491   8.280%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  0.011111/  0.044820, val:  83.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.08 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   2  Sparsity: 52.4079%\n",
      "layer   3  Sparsity: 54.8500%\n",
      "total_backward_count 1018160 real_backward_count 83914   8.242%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  0.010762/  0.036845, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   2  Sparsity: 52.4587%\n",
      "layer   3  Sparsity: 54.8722%\n",
      "total_backward_count 1027950 real_backward_count 84316   8.202%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  0.010862/  0.035904, val:  86.67%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.91 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1046%\n",
      "layer   2  Sparsity: 52.4913%\n",
      "layer   3  Sparsity: 54.8213%\n",
      "total_backward_count 1037740 real_backward_count 84716   8.164%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  0.010758/  0.039460, val:  85.00%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0959%\n",
      "layer   2  Sparsity: 52.6095%\n",
      "layer   3  Sparsity: 54.8756%\n",
      "total_backward_count 1047530 real_backward_count 85120   8.126%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  0.010782/  0.037715, val:  87.50%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.90 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   2  Sparsity: 52.5471%\n",
      "layer   3  Sparsity: 54.9965%\n",
      "total_backward_count 1057320 real_backward_count 85557   8.092%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  0.010346/  0.038886, val:  86.25%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.21 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0625%\n",
      "layer   2  Sparsity: 52.5532%\n",
      "layer   3  Sparsity: 55.0199%\n",
      "total_backward_count 1067110 real_backward_count 85943   8.054%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  0.010636/  0.037416, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.54 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   2  Sparsity: 52.5581%\n",
      "layer   3  Sparsity: 54.8450%\n",
      "total_backward_count 1076900 real_backward_count 86334   8.017%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  0.010315/  0.039023, val:  85.83%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0851%\n",
      "layer   2  Sparsity: 52.4771%\n",
      "layer   3  Sparsity: 54.9117%\n",
      "total_backward_count 1086690 real_backward_count 86733   7.981%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  0.010649/  0.041011, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.10 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   2  Sparsity: 52.5413%\n",
      "layer   3  Sparsity: 54.9112%\n",
      "total_backward_count 1096480 real_backward_count 87135   7.947%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  0.009797/  0.036891, val:  86.25%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.45 seconds, 1.06 minutes\n",
      "layer   1  Sparsity: 91.1212%\n",
      "layer   2  Sparsity: 52.5272%\n",
      "layer   3  Sparsity: 55.0048%\n",
      "total_backward_count 1106270 real_backward_count 87475   7.907%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  0.010433/  0.040212, val:  85.83%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0768%\n",
      "layer   2  Sparsity: 52.4026%\n",
      "layer   3  Sparsity: 55.0840%\n",
      "total_backward_count 1116060 real_backward_count 87868   7.873%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  0.010018/  0.037077, val:  87.08%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.71 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   2  Sparsity: 52.3866%\n",
      "layer   3  Sparsity: 54.9024%\n",
      "total_backward_count 1125850 real_backward_count 88252   7.839%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  0.009899/  0.036553, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.76 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1033%\n",
      "layer   2  Sparsity: 52.4600%\n",
      "layer   3  Sparsity: 54.9697%\n",
      "total_backward_count 1135640 real_backward_count 88622   7.804%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  0.009953/  0.037787, val:  87.50%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.06 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0671%\n",
      "layer   2  Sparsity: 52.5567%\n",
      "layer   3  Sparsity: 54.9767%\n",
      "total_backward_count 1145430 real_backward_count 88984   7.769%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  0.010291/  0.039364, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.25 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1092%\n",
      "layer   2  Sparsity: 52.5417%\n",
      "layer   3  Sparsity: 54.8457%\n",
      "total_backward_count 1155220 real_backward_count 89385   7.737%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  0.010319/  0.039936, val:  87.08%, val_best:  88.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 67.78 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0876%\n",
      "layer   2  Sparsity: 52.5836%\n",
      "layer   3  Sparsity: 54.9820%\n",
      "total_backward_count 1165010 real_backward_count 89816   7.709%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  0.009890/  0.036914, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.80 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0389%\n",
      "layer   2  Sparsity: 52.7060%\n",
      "layer   3  Sparsity: 54.9433%\n",
      "total_backward_count 1174800 real_backward_count 90181   7.676%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  0.009905/  0.037768, val:  87.50%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.85 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1117%\n",
      "layer   2  Sparsity: 52.6385%\n",
      "layer   3  Sparsity: 54.8097%\n",
      "total_backward_count 1184590 real_backward_count 90569   7.646%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  0.009876/  0.040460, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.05 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0740%\n",
      "layer   2  Sparsity: 52.5730%\n",
      "layer   3  Sparsity: 54.9270%\n",
      "total_backward_count 1194380 real_backward_count 90958   7.615%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  0.010152/  0.038682, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.60 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1043%\n",
      "layer   2  Sparsity: 52.6989%\n",
      "layer   3  Sparsity: 54.9922%\n",
      "total_backward_count 1204170 real_backward_count 91347   7.586%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  0.009698/  0.036931, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.82 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   2  Sparsity: 52.5947%\n",
      "layer   3  Sparsity: 55.0416%\n",
      "total_backward_count 1213960 real_backward_count 91697   7.554%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  0.009612/  0.038179, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.65 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0997%\n",
      "layer   2  Sparsity: 52.5848%\n",
      "layer   3  Sparsity: 55.0549%\n",
      "total_backward_count 1223750 real_backward_count 92047   7.522%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  0.009593/  0.037737, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.47 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1086%\n",
      "layer   2  Sparsity: 52.6551%\n",
      "layer   3  Sparsity: 54.9608%\n",
      "total_backward_count 1233540 real_backward_count 92422   7.492%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  0.009728/  0.039075, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.78 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 52.6192%\n",
      "layer   3  Sparsity: 54.9324%\n",
      "total_backward_count 1243330 real_backward_count 92788   7.463%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  0.009214/  0.037417, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.53 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0522%\n",
      "layer   2  Sparsity: 52.6559%\n",
      "layer   3  Sparsity: 55.1356%\n",
      "total_backward_count 1253120 real_backward_count 93114   7.431%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  0.009951/  0.036032, val:  89.58%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.53 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0904%\n",
      "layer   2  Sparsity: 52.7057%\n",
      "layer   3  Sparsity: 55.0338%\n",
      "total_backward_count 1262910 real_backward_count 93494   7.403%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  0.009482/  0.037211, val:  86.25%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.83 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1081%\n",
      "layer   2  Sparsity: 52.6742%\n",
      "layer   3  Sparsity: 54.9925%\n",
      "total_backward_count 1272700 real_backward_count 93863   7.375%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  0.009654/  0.035998, val:  86.67%, val_best:  89.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 67.72 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0891%\n",
      "layer   2  Sparsity: 52.6524%\n",
      "layer   3  Sparsity: 55.0252%\n",
      "total_backward_count 1282490 real_backward_count 94233   7.348%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  0.009253/  0.038958, val:  86.25%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.12 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0627%\n",
      "layer   2  Sparsity: 52.8050%\n",
      "layer   3  Sparsity: 55.0032%\n",
      "total_backward_count 1292280 real_backward_count 94570   7.318%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  0.009461/  0.037310, val:  87.92%, val_best:  89.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.97 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0686%\n",
      "layer   2  Sparsity: 52.8441%\n",
      "layer   3  Sparsity: 55.1280%\n",
      "total_backward_count 1302070 real_backward_count 94918   7.290%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  0.010096/  0.037852, val:  86.67%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.69 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0753%\n",
      "layer   2  Sparsity: 52.9340%\n",
      "layer   3  Sparsity: 54.9868%\n",
      "total_backward_count 1311860 real_backward_count 95311   7.265%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  0.009252/  0.038116, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.00 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0481%\n",
      "layer   2  Sparsity: 52.8804%\n",
      "layer   3  Sparsity: 54.9935%\n",
      "total_backward_count 1321650 real_backward_count 95655   7.238%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  0.009113/  0.036974, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.99 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   2  Sparsity: 52.9745%\n",
      "layer   3  Sparsity: 55.1167%\n",
      "total_backward_count 1331440 real_backward_count 95990   7.209%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  0.009324/  0.037718, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.13 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0733%\n",
      "layer   2  Sparsity: 52.9090%\n",
      "layer   3  Sparsity: 55.1304%\n",
      "total_backward_count 1341230 real_backward_count 96348   7.184%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  0.009380/  0.037666, val:  87.92%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.60 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0966%\n",
      "layer   2  Sparsity: 52.8582%\n",
      "layer   3  Sparsity: 55.2884%\n",
      "total_backward_count 1351020 real_backward_count 96710   7.158%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  0.008973/  0.038906, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.65 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0788%\n",
      "layer   2  Sparsity: 52.8962%\n",
      "layer   3  Sparsity: 55.3000%\n",
      "total_backward_count 1360810 real_backward_count 97044   7.131%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  0.008876/  0.036445, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 64.24 seconds, 1.07 minutes\n",
      "layer   1  Sparsity: 91.0579%\n",
      "layer   2  Sparsity: 52.9665%\n",
      "layer   3  Sparsity: 55.2539%\n",
      "total_backward_count 1370600 real_backward_count 97359   7.103%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  0.009023/  0.038087, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.35 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0637%\n",
      "layer   2  Sparsity: 52.9363%\n",
      "layer   3  Sparsity: 55.1454%\n",
      "total_backward_count 1380390 real_backward_count 97693   7.077%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  0.008933/  0.036610, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.54 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   2  Sparsity: 52.9538%\n",
      "layer   3  Sparsity: 55.0183%\n",
      "total_backward_count 1390180 real_backward_count 98036   7.052%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  0.009208/  0.038291, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.89 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   2  Sparsity: 52.8478%\n",
      "layer   3  Sparsity: 55.0114%\n",
      "total_backward_count 1399970 real_backward_count 98355   7.026%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  0.009143/  0.036985, val:  87.08%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.19 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0462%\n",
      "layer   2  Sparsity: 52.8892%\n",
      "layer   3  Sparsity: 55.0781%\n",
      "total_backward_count 1409760 real_backward_count 98692   7.001%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  0.008872/  0.037618, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.10 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0858%\n",
      "layer   2  Sparsity: 52.8814%\n",
      "layer   3  Sparsity: 55.3610%\n",
      "total_backward_count 1419550 real_backward_count 99015   6.975%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  0.008501/  0.038185, val:  87.50%, val_best:  89.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.85 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   2  Sparsity: 52.9094%\n",
      "layer   3  Sparsity: 55.3426%\n",
      "total_backward_count 1429340 real_backward_count 99323   6.949%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  0.008580/  0.036532, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.51 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1115%\n",
      "layer   2  Sparsity: 53.0060%\n",
      "layer   3  Sparsity: 55.3903%\n",
      "total_backward_count 1439130 real_backward_count 99626   6.923%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  0.008695/  0.038505, val:  86.67%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.91 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 53.0207%\n",
      "layer   3  Sparsity: 55.3808%\n",
      "total_backward_count 1448920 real_backward_count 99958   6.899%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  0.009141/  0.036240, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.61 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0554%\n",
      "layer   2  Sparsity: 52.9391%\n",
      "layer   3  Sparsity: 55.4091%\n",
      "total_backward_count 1458710 real_backward_count 100307   6.876%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  0.009316/  0.046146, val:  82.92%, val_best:  89.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.05 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   2  Sparsity: 52.9865%\n",
      "layer   3  Sparsity: 55.3984%\n",
      "total_backward_count 1468500 real_backward_count 100654   6.854%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  0.008714/  0.036068, val:  88.33%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.29 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1094%\n",
      "layer   2  Sparsity: 52.9774%\n",
      "layer   3  Sparsity: 55.3450%\n",
      "total_backward_count 1478290 real_backward_count 100975   6.831%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  0.008546/  0.038576, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.06 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0820%\n",
      "layer   2  Sparsity: 52.9739%\n",
      "layer   3  Sparsity: 55.3084%\n",
      "total_backward_count 1488080 real_backward_count 101290   6.807%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  0.008359/  0.037466, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.55 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0337%\n",
      "layer   2  Sparsity: 52.9486%\n",
      "layer   3  Sparsity: 55.4587%\n",
      "total_backward_count 1497870 real_backward_count 101586   6.782%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  0.008597/  0.036873, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.92 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0823%\n",
      "layer   2  Sparsity: 52.9324%\n",
      "layer   3  Sparsity: 55.5167%\n",
      "total_backward_count 1507660 real_backward_count 101916   6.760%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  0.008173/  0.036552, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.17 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0733%\n",
      "layer   2  Sparsity: 52.9409%\n",
      "layer   3  Sparsity: 55.4539%\n",
      "total_backward_count 1517450 real_backward_count 102203   6.735%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  0.008667/  0.037519, val:  87.50%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.98 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1235%\n",
      "layer   2  Sparsity: 52.9495%\n",
      "layer   3  Sparsity: 55.3735%\n",
      "total_backward_count 1527240 real_backward_count 102516   6.713%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  0.008464/  0.036883, val:  87.92%, val_best:  89.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.73 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   2  Sparsity: 52.8911%\n",
      "layer   3  Sparsity: 55.3648%\n",
      "total_backward_count 1537030 real_backward_count 102817   6.689%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  0.008478/  0.037696, val:  87.92%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.86 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   2  Sparsity: 52.9951%\n",
      "layer   3  Sparsity: 55.5267%\n",
      "total_backward_count 1546820 real_backward_count 103139   6.668%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  0.008221/  0.037126, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.30 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0897%\n",
      "layer   2  Sparsity: 52.9237%\n",
      "layer   3  Sparsity: 55.6037%\n",
      "total_backward_count 1556610 real_backward_count 103438   6.645%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  0.008093/  0.037135, val:  88.75%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 66.27 seconds, 1.10 minutes\n",
      "layer   1  Sparsity: 91.0672%\n",
      "layer   2  Sparsity: 52.9324%\n",
      "layer   3  Sparsity: 55.6679%\n",
      "total_backward_count 1566400 real_backward_count 103744   6.623%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  0.008298/  0.037844, val:  87.50%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 64.45 seconds, 1.07 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   2  Sparsity: 52.9422%\n",
      "layer   3  Sparsity: 55.5128%\n",
      "total_backward_count 1576190 real_backward_count 104028   6.600%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  0.008337/  0.037453, val:  87.08%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.92 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0612%\n",
      "layer   2  Sparsity: 52.8733%\n",
      "layer   3  Sparsity: 55.3823%\n",
      "total_backward_count 1585980 real_backward_count 104334   6.579%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  0.008044/  0.036023, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.29 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0905%\n",
      "layer   2  Sparsity: 52.9356%\n",
      "layer   3  Sparsity: 55.4010%\n",
      "total_backward_count 1595770 real_backward_count 104617   6.556%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  0.008147/  0.036209, val:  90.00%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.26 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0492%\n",
      "layer   2  Sparsity: 52.9733%\n",
      "layer   3  Sparsity: 55.3034%\n",
      "total_backward_count 1605560 real_backward_count 104913   6.534%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  0.007943/  0.037819, val:  86.67%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.96 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0806%\n",
      "layer   2  Sparsity: 52.9403%\n",
      "layer   3  Sparsity: 55.5966%\n",
      "total_backward_count 1615350 real_backward_count 105192   6.512%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  0.007771/  0.036292, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.89 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.1023%\n",
      "layer   2  Sparsity: 52.9979%\n",
      "layer   3  Sparsity: 55.4520%\n",
      "total_backward_count 1625140 real_backward_count 105460   6.489%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  0.008382/  0.037547, val:  87.50%, val_best:  90.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0957%\n",
      "layer   2  Sparsity: 52.9553%\n",
      "layer   3  Sparsity: 55.4378%\n",
      "total_backward_count 1634930 real_backward_count 105788   6.470%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  0.007828/  0.035782, val:  87.92%, val_best:  90.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0689%\n",
      "layer   2  Sparsity: 52.8687%\n",
      "layer   3  Sparsity: 55.5219%\n",
      "total_backward_count 1644720 real_backward_count 106074   6.449%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  0.007962/  0.038450, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.70 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.1188%\n",
      "layer   2  Sparsity: 52.9023%\n",
      "layer   3  Sparsity: 55.5039%\n",
      "total_backward_count 1654510 real_backward_count 106351   6.428%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  0.007828/  0.038377, val:  87.08%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.48 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0355%\n",
      "layer   2  Sparsity: 52.9803%\n",
      "layer   3  Sparsity: 55.4886%\n",
      "total_backward_count 1664300 real_backward_count 106627   6.407%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  0.007893/  0.036989, val:  87.50%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.79 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0988%\n",
      "layer   2  Sparsity: 52.9607%\n",
      "layer   3  Sparsity: 55.3659%\n",
      "total_backward_count 1674090 real_backward_count 106912   6.386%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  0.008395/  0.035699, val:  88.33%, val_best:  90.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.71 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0863%\n",
      "layer   2  Sparsity: 52.9716%\n",
      "layer   3  Sparsity: 55.4262%\n",
      "total_backward_count 1683880 real_backward_count 107237   6.368%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  0.008024/  0.037073, val:  87.92%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.77 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0934%\n",
      "layer   2  Sparsity: 53.0502%\n",
      "layer   3  Sparsity: 55.3687%\n",
      "total_backward_count 1693670 real_backward_count 107519   6.348%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  0.007714/  0.038027, val:  85.83%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.29 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0545%\n",
      "layer   2  Sparsity: 53.1267%\n",
      "layer   3  Sparsity: 55.3388%\n",
      "total_backward_count 1703460 real_backward_count 107806   6.329%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  0.007965/  0.037038, val:  87.92%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   2  Sparsity: 53.0318%\n",
      "layer   3  Sparsity: 55.3400%\n",
      "total_backward_count 1713250 real_backward_count 108103   6.310%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  0.007959/  0.035826, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.64 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   2  Sparsity: 53.0705%\n",
      "layer   3  Sparsity: 55.2556%\n",
      "total_backward_count 1723040 real_backward_count 108406   6.292%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  0.008038/  0.035106, val:  88.75%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.72 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0699%\n",
      "layer   2  Sparsity: 53.0651%\n",
      "layer   3  Sparsity: 55.2572%\n",
      "total_backward_count 1732830 real_backward_count 108716   6.274%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  0.007798/  0.036911, val:  87.92%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.00 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1012%\n",
      "layer   2  Sparsity: 53.0292%\n",
      "layer   3  Sparsity: 55.3365%\n",
      "total_backward_count 1742620 real_backward_count 109016   6.256%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  0.007623/  0.037985, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.97 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0789%\n",
      "layer   2  Sparsity: 52.9465%\n",
      "layer   3  Sparsity: 55.5002%\n",
      "total_backward_count 1752410 real_backward_count 109292   6.237%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  0.007427/  0.036123, val:  90.42%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.83 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0768%\n",
      "layer   2  Sparsity: 52.9886%\n",
      "layer   3  Sparsity: 55.4465%\n",
      "total_backward_count 1762200 real_backward_count 109555   6.217%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  0.007340/  0.036517, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.28 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   2  Sparsity: 52.9891%\n",
      "layer   3  Sparsity: 55.5388%\n",
      "total_backward_count 1771990 real_backward_count 109818   6.197%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  0.007240/  0.035983, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.85 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1028%\n",
      "layer   2  Sparsity: 53.1007%\n",
      "layer   3  Sparsity: 55.4791%\n",
      "total_backward_count 1781780 real_backward_count 110066   6.177%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  0.007503/  0.037691, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.87 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0652%\n",
      "layer   2  Sparsity: 53.1135%\n",
      "layer   3  Sparsity: 55.3559%\n",
      "total_backward_count 1791570 real_backward_count 110315   6.157%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  0.007525/  0.038259, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.96 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0583%\n",
      "layer   2  Sparsity: 53.0432%\n",
      "layer   3  Sparsity: 55.3998%\n",
      "total_backward_count 1801360 real_backward_count 110573   6.138%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  0.007913/  0.038531, val:  88.33%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.79 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   2  Sparsity: 53.0673%\n",
      "layer   3  Sparsity: 55.2516%\n",
      "total_backward_count 1811150 real_backward_count 110876   6.122%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  0.007649/  0.037532, val:  87.50%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.17 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   2  Sparsity: 53.0253%\n",
      "layer   3  Sparsity: 55.3555%\n",
      "total_backward_count 1820940 real_backward_count 111151   6.104%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  0.007078/  0.037098, val:  86.67%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.21 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   2  Sparsity: 53.0134%\n",
      "layer   3  Sparsity: 55.4165%\n",
      "total_backward_count 1830730 real_backward_count 111386   6.084%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  0.007397/  0.037720, val:  87.08%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.75 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0881%\n",
      "layer   2  Sparsity: 53.0733%\n",
      "layer   3  Sparsity: 55.4813%\n",
      "total_backward_count 1840520 real_backward_count 111639   6.066%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  0.007423/  0.037124, val:  87.08%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.64 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0991%\n",
      "layer   2  Sparsity: 53.1603%\n",
      "layer   3  Sparsity: 55.6081%\n",
      "total_backward_count 1850310 real_backward_count 111914   6.048%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  0.007277/  0.037313, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.54 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   2  Sparsity: 53.1674%\n",
      "layer   3  Sparsity: 55.6546%\n",
      "total_backward_count 1860100 real_backward_count 112180   6.031%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  0.007199/  0.036248, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.06 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0859%\n",
      "layer   2  Sparsity: 53.0336%\n",
      "layer   3  Sparsity: 55.6000%\n",
      "total_backward_count 1869890 real_backward_count 112448   6.014%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  0.007279/  0.036386, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.62 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0687%\n",
      "layer   2  Sparsity: 53.1781%\n",
      "layer   3  Sparsity: 55.5983%\n",
      "total_backward_count 1879680 real_backward_count 112683   5.995%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  0.007428/  0.036570, val:  89.17%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0609%\n",
      "layer   2  Sparsity: 53.1026%\n",
      "layer   3  Sparsity: 55.6935%\n",
      "total_backward_count 1889470 real_backward_count 112933   5.977%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  0.007127/  0.035505, val:  89.58%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.17 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1185%\n",
      "layer   2  Sparsity: 53.1371%\n",
      "layer   3  Sparsity: 55.5914%\n",
      "total_backward_count 1899260 real_backward_count 113177   5.959%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  0.007082/  0.036020, val:  87.08%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.06 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   2  Sparsity: 53.2038%\n",
      "layer   3  Sparsity: 55.6186%\n",
      "total_backward_count 1909050 real_backward_count 113416   5.941%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  0.006767/  0.036250, val:  86.67%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0479%\n",
      "layer   2  Sparsity: 53.2479%\n",
      "layer   3  Sparsity: 55.7608%\n",
      "total_backward_count 1918840 real_backward_count 113645   5.923%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  0.006862/  0.036531, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.91 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   2  Sparsity: 53.0928%\n",
      "layer   3  Sparsity: 55.7372%\n",
      "total_backward_count 1928630 real_backward_count 113887   5.905%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  0.007389/  0.035442, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.14 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   2  Sparsity: 53.1135%\n",
      "layer   3  Sparsity: 55.7931%\n",
      "total_backward_count 1938420 real_backward_count 114145   5.889%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  0.007201/  0.037931, val:  87.92%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.78 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0343%\n",
      "layer   2  Sparsity: 53.2020%\n",
      "layer   3  Sparsity: 55.8208%\n",
      "total_backward_count 1948210 real_backward_count 114393   5.872%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  0.007273/  0.041475, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.40 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   2  Sparsity: 53.2030%\n",
      "layer   3  Sparsity: 55.8785%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8373f0eab1e47fb88fbac4b6e0fe7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00727</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>0.04148</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pleasant-sweep-4</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/exl3cvfq' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/exl3cvfq</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251120_061859-exl3cvfq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d3tuc05a with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 1746\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251120_100551-d3tuc05a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/d3tuc05a' target=\"_blank\">happy-sweep-5</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4a2lz39l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/d3tuc05a' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/d3tuc05a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251120_100600_907', 'my_seed': 1746, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.03125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[999, 998], [999, 999], [999, 999]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 0\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 0\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.03125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.03125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([-2.2404e-02,  2.0244e-01, -6.7619e-02, -1.9098e-01, -3.8125e-02,\n",
      "         1.3738e-01, -7.3515e-02,  3.3941e-02, -4.8876e-02,  4.4362e-02,\n",
      "        -7.6764e-02, -9.8645e-02, -7.6903e-02,  2.4916e-02,  6.6292e-02,\n",
      "         5.2550e-02,  9.6979e-02, -1.6092e-02,  4.2499e-02,  3.8031e-02,\n",
      "         1.8509e-01, -9.1094e-02,  5.0758e-02, -4.5310e-03, -8.0614e-02,\n",
      "        -4.0030e-02,  2.9792e-02, -1.5674e-01,  2.3084e-02, -2.8832e-03,\n",
      "         1.6870e-01, -6.2787e-02, -2.1472e-01,  1.7840e-03, -1.3053e-01,\n",
      "        -5.2630e-02,  3.6674e-02,  8.9562e-02, -9.5007e-03, -2.0989e-02,\n",
      "        -7.3936e-03, -8.9574e-02,  4.3193e-02, -1.6486e-02, -1.4371e-02,\n",
      "         6.6395e-02,  9.8776e-02,  1.9283e-02,  4.2022e-02,  2.7079e-02,\n",
      "        -1.0862e-01, -9.1011e-02,  1.3089e-01,  1.3342e-01, -9.3305e-02,\n",
      "        -7.7091e-02,  6.7756e-02,  5.5695e-02, -4.4406e-02, -5.7495e-02,\n",
      "         2.1364e-01, -8.1737e-02, -1.5105e-04, -6.5953e-02, -4.8545e-02,\n",
      "         6.8970e-02,  7.2732e-02,  4.0015e-03, -1.3079e-02, -4.4764e-02,\n",
      "         7.2178e-02,  5.4505e-02, -4.4126e-02,  1.9260e-01, -6.9251e-02,\n",
      "        -5.9183e-04,  5.2954e-02,  1.3421e-01, -5.1050e-02, -4.4575e-03,\n",
      "         1.2418e-01,  6.2230e-02,  3.9165e-02,  4.2212e-02, -3.8115e-02,\n",
      "        -6.0624e-02, -1.2056e-01, -5.0430e-02,  6.8062e-03,  4.5439e-02,\n",
      "         1.5430e-01,  6.0695e-02, -1.2806e-01,  7.2317e-02,  2.5474e-03,\n",
      "        -1.4061e-01, -1.8823e-02,  2.2156e-02,  4.6185e-02, -3.0931e-02,\n",
      "        -5.7631e-02, -1.6301e-01, -4.3309e-02, -2.2505e-02, -1.3924e-01,\n",
      "        -7.4984e-02,  6.4370e-02,  9.6483e-02,  7.0821e-02, -1.4377e-01,\n",
      "         1.0666e-02, -1.0047e-01,  2.8735e-02, -1.0494e-01,  7.1963e-02,\n",
      "        -2.6688e-02,  1.5922e-01,  5.2021e-02, -7.2179e-02, -2.4207e-02,\n",
      "         1.5827e-02,  7.3416e-02,  1.7458e-01,  2.9337e-02,  7.6830e-03,\n",
      "         6.2574e-02,  1.2015e-01, -1.2952e-01, -9.3693e-02, -5.4796e-02,\n",
      "        -1.5117e-01,  4.8317e-02,  6.7133e-02,  1.1368e-02, -9.1307e-02,\n",
      "         1.2454e-01,  1.4158e-01,  4.9993e-02, -1.2281e-01,  3.2438e-04,\n",
      "         2.4275e-02, -3.3205e-02, -1.2370e-01, -6.5484e-02, -8.5008e-02,\n",
      "        -2.0747e-02, -1.0430e-01, -5.6772e-02,  3.4773e-02, -9.8394e-02,\n",
      "        -1.6195e-02,  1.4823e-01, -9.7025e-02,  1.0299e-02,  1.4018e-01,\n",
      "        -5.8690e-02, -1.3207e-01, -9.2883e-03,  5.5905e-02,  7.9450e-02,\n",
      "        -7.5228e-02,  2.1593e-02, -1.1382e-01,  4.5049e-02, -3.1481e-02,\n",
      "        -1.2508e-02, -3.9871e-02, -9.8230e-02, -1.2116e-01, -1.3858e-01,\n",
      "         8.9886e-02, -8.4516e-02, -3.5695e-02, -5.9286e-02,  1.7015e-01,\n",
      "        -2.1306e-01, -2.5390e-02, -1.5156e-01,  2.2491e-01,  5.1449e-02,\n",
      "        -9.5852e-02,  2.7783e-02,  9.5738e-02,  1.5132e-01,  1.2892e-01,\n",
      "        -1.1258e-01, -3.5531e-02,  3.0812e-02, -6.7179e-02, -5.8928e-02,\n",
      "         2.6348e-02, -1.0370e-01, -5.3569e-03, -7.9968e-03,  2.5942e-01,\n",
      "        -7.3922e-02,  9.6443e-02,  1.8950e-02,  2.1931e-03, -8.8906e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([-1.5070e-01, -1.2749e-01,  4.8551e-02, -2.0500e-01,  3.7344e-02,\n",
      "         5.0888e-02, -1.3559e-02,  7.0167e-02, -2.7437e-02, -1.0027e-01,\n",
      "        -1.5885e-01, -1.2313e-01, -7.9282e-02,  1.1951e-02,  2.0151e-01,\n",
      "        -9.8285e-02, -1.2574e-01,  1.8872e-01,  9.2162e-02, -1.1627e-01,\n",
      "        -2.0335e-02, -1.3785e-04,  2.0054e-01,  1.1283e-01,  5.7764e-02,\n",
      "        -3.6252e-02, -1.0774e-01, -3.0673e-02, -1.4100e-01, -1.4167e-01,\n",
      "        -4.2986e-02, -2.0499e-01,  2.3758e-02,  6.0089e-02,  2.0804e-02,\n",
      "         1.0997e-01,  6.1154e-02, -1.1108e-01, -4.5987e-02,  2.3904e-02,\n",
      "         4.8229e-02,  1.6016e-01,  2.9333e-03,  2.9246e-02, -2.1602e-02,\n",
      "        -4.3649e-02, -5.7088e-02, -7.5383e-02,  1.4242e-01, -7.8178e-03,\n",
      "         3.6378e-02, -1.6755e-02, -9.6720e-02, -1.1834e-01,  6.7947e-02,\n",
      "        -9.6792e-02, -4.7662e-02,  5.7966e-02,  7.3248e-02,  1.7072e-01,\n",
      "         7.2229e-02,  2.4438e-01,  1.2034e-03,  9.3715e-02,  2.7613e-01,\n",
      "         1.7642e-02, -6.2297e-02, -7.3372e-02, -1.7978e-01, -9.8254e-02,\n",
      "        -1.6685e-02,  7.3923e-02,  3.4231e-02, -2.0663e-01,  1.0223e-01,\n",
      "         4.3049e-02,  2.0311e-01, -9.3341e-02, -6.1362e-02, -7.0223e-02,\n",
      "         9.9980e-03, -3.0062e-02,  1.3776e-01, -8.8209e-02, -1.7858e-01,\n",
      "        -6.8770e-02,  2.1941e-01, -1.3561e-02,  1.8770e-01, -1.4805e-01,\n",
      "        -8.6544e-02,  1.1169e-01, -1.2999e-01, -1.1129e-01, -2.5337e-02,\n",
      "         9.9106e-02, -7.3734e-03, -1.1899e-01, -6.3619e-03, -1.2223e-01,\n",
      "         8.4410e-03, -5.9544e-02, -1.8580e-01, -2.4267e-01, -4.8565e-02,\n",
      "         8.4845e-02,  7.1427e-02, -2.5224e-03,  4.5902e-02,  3.1899e-02,\n",
      "         1.6216e-01, -1.4358e-01,  1.5419e-01,  1.6793e-02,  7.9712e-02,\n",
      "        -2.5208e-02,  2.7896e-02,  1.0490e-01, -1.0559e-01, -2.0035e-02,\n",
      "        -5.4108e-02,  1.9413e-01, -4.6357e-02, -5.8239e-02, -1.6680e-02,\n",
      "         1.6497e-02,  1.7073e-01,  1.6188e-01,  9.5638e-02, -1.0240e-01,\n",
      "         1.6710e-02, -3.3553e-02,  1.1248e-01,  1.5439e-02,  7.6341e-02,\n",
      "        -6.8034e-02, -5.4949e-02, -1.2053e-01,  4.6441e-02, -8.7037e-02,\n",
      "        -9.0961e-03, -2.5390e-02,  4.6702e-02,  1.0038e-02,  5.9304e-02,\n",
      "        -9.2558e-02, -1.3874e-01, -8.3525e-03, -1.1655e-01, -2.2425e-02,\n",
      "        -5.7477e-02, -1.1540e-01,  3.0423e-02, -1.7557e-01,  8.8842e-02,\n",
      "         2.7958e-02, -2.0249e-02,  5.3653e-02,  1.2734e-02, -1.1996e-01,\n",
      "         2.0413e-01, -1.0414e-01,  2.3132e-02, -3.5125e-02,  5.8514e-02,\n",
      "         2.1591e-02, -1.5848e-01,  3.0267e-02,  5.6360e-02,  6.9497e-02,\n",
      "         2.1875e-01, -1.2049e-01,  4.8976e-02,  8.7127e-02,  1.0833e-02,\n",
      "         6.9003e-02, -5.2584e-02, -2.3947e-02,  8.3482e-02, -1.7141e-02,\n",
      "        -4.9989e-02, -1.3076e-01,  1.4653e-01,  9.9468e-02, -1.9702e-02,\n",
      "         5.0978e-04,  1.2263e-01, -6.3469e-03,  1.1845e-02, -3.9454e-02,\n",
      "         3.5042e-02, -9.8346e-03, -1.0517e-01, -1.5494e-01, -1.6452e-01,\n",
      "        -2.4280e-03, -1.4253e-02,  1.0532e-01, -1.1939e-01,  4.3130e-03],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([-0.0359,  0.0682, -0.2415, -0.0272, -0.0150, -0.0468, -0.0881, -0.1491,\n",
      "         0.1446, -0.1190,  0.0256, -0.0193,  0.0167, -0.0276, -0.0485,  0.0370,\n",
      "         0.0960, -0.1585, -0.0164, -0.1887,  0.0409, -0.0865,  0.0273,  0.0275,\n",
      "         0.1462, -0.0429,  0.1014,  0.0881, -0.0007,  0.0210, -0.0608, -0.1171,\n",
      "        -0.0903,  0.0646, -0.0105, -0.1020,  0.1274,  0.1584,  0.0294, -0.1276,\n",
      "        -0.0379, -0.0377,  0.0518, -0.0018,  0.1059, -0.0075,  0.0281, -0.0768,\n",
      "        -0.2011,  0.0283,  0.0324, -0.0708,  0.0399, -0.0919, -0.1124, -0.2491,\n",
      "        -0.2070,  0.0116, -0.0017,  0.1024,  0.1446,  0.1061,  0.1062, -0.0428,\n",
      "        -0.1991, -0.0480,  0.0830, -0.0092, -0.0647, -0.0705, -0.0053,  0.1322,\n",
      "         0.0949, -0.0986,  0.0892, -0.0577, -0.1622, -0.2022,  0.1674,  0.1190,\n",
      "         0.0840,  0.0037,  0.0962,  0.2339, -0.2033, -0.0888,  0.1464, -0.0256,\n",
      "         0.1740,  0.1754, -0.1682,  0.0189, -0.1067, -0.1318,  0.1116, -0.1016,\n",
      "        -0.1344, -0.0319, -0.0878, -0.0726, -0.1781, -0.0288, -0.0446, -0.1828,\n",
      "         0.0975, -0.0423, -0.0296, -0.0071,  0.1800, -0.0367, -0.1813,  0.0564,\n",
      "        -0.1561, -0.0382, -0.0026, -0.1352, -0.1047,  0.0909, -0.2076, -0.2042,\n",
      "         0.0714, -0.1052, -0.1753, -0.0906, -0.0095,  0.0215,  0.0228, -0.0858,\n",
      "         0.0441, -0.0363,  0.0242,  0.0696,  0.1827,  0.0649, -0.0540,  0.1359,\n",
      "        -0.1861, -0.0624,  0.0248, -0.0330,  0.0831,  0.0007, -0.0162, -0.0445,\n",
      "        -0.1529,  0.0954,  0.0099, -0.0606,  0.0285, -0.1517,  0.1794,  0.1004,\n",
      "         0.1741, -0.0092,  0.0058, -0.0214, -0.1566,  0.0324, -0.1338,  0.1156,\n",
      "        -0.0714,  0.0218,  0.0280,  0.0657, -0.1394,  0.1489,  0.0352, -0.0549,\n",
      "         0.0057,  0.2962, -0.0081, -0.0106, -0.0345, -0.1644,  0.0427,  0.0088,\n",
      "         0.0779, -0.0895,  0.2476,  0.1434,  0.0595,  0.1573, -0.0627, -0.0787,\n",
      "        -0.0221,  0.0469,  0.0708,  0.0011,  0.0297,  0.0463,  0.2099, -0.1160,\n",
      "        -0.0750,  0.1266, -0.0107, -0.0826, -0.1733,  0.0550, -0.0667,  0.0916],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-1.1463e-01,  9.4863e-02, -9.6000e-02, -6.4516e-02,  1.2362e-02,\n",
      "         2.4937e-03, -4.7969e-03, -6.5385e-02, -2.5965e-03,  1.9642e-02,\n",
      "         8.4985e-02,  2.5953e-02,  3.4381e-02,  1.1593e-01, -1.4505e-02,\n",
      "        -2.6338e-02, -4.4207e-02, -7.1735e-03, -1.4008e-01,  1.0528e-01,\n",
      "         1.1026e-02, -1.5864e-01,  1.5561e-01, -8.1779e-02,  1.0638e-01,\n",
      "        -1.7868e-02,  8.4261e-02, -2.2383e-02, -6.6416e-03,  9.3691e-02,\n",
      "        -2.4620e-02, -7.1592e-02,  1.1892e-01,  1.0515e-01, -1.4349e-02,\n",
      "         1.0030e-01,  8.2398e-02, -1.1467e-01,  5.6442e-02, -2.8184e-02,\n",
      "         6.8761e-02, -6.4142e-02,  9.7954e-02,  6.7131e-02, -1.5070e-01,\n",
      "        -3.2318e-02, -1.2902e-01,  5.4427e-02, -9.5549e-02, -1.4953e-02,\n",
      "         1.7248e-01, -3.2370e-02, -9.5155e-02,  1.4565e-01,  1.0717e-01,\n",
      "         7.4807e-02,  8.9543e-02,  1.4474e-01, -7.9061e-02,  3.5996e-02,\n",
      "        -6.3089e-02,  2.0704e-02,  7.0158e-02, -4.7013e-05, -1.4971e-01,\n",
      "         1.0149e-01,  3.7666e-03,  1.0387e-01,  5.1434e-02, -2.1569e-01,\n",
      "         3.4851e-02, -4.6653e-03, -8.1488e-03, -8.2993e-02,  2.3247e-02,\n",
      "         1.3469e-02,  5.8827e-03, -3.7797e-02, -5.8030e-02,  5.7106e-02,\n",
      "         1.1691e-01, -2.6785e-02, -1.3038e-03,  7.2326e-03,  5.6481e-02,\n",
      "         4.3404e-02, -1.4866e-01, -8.2952e-02,  4.2683e-02, -2.8298e-02,\n",
      "         1.6437e-01,  1.3409e-01, -4.5519e-02, -1.1248e-01, -1.8865e-04,\n",
      "        -9.7707e-02, -6.2113e-02, -1.0269e-01,  3.4868e-02, -6.6977e-02,\n",
      "         6.3776e-02,  4.3856e-02, -1.1528e-01, -4.7155e-02, -1.0909e-02,\n",
      "        -7.9484e-03,  2.0950e-02,  8.8458e-03,  1.6536e-01,  1.7245e-01,\n",
      "        -8.3190e-02, -1.4376e-01,  1.3473e-01, -2.0335e-01,  7.1512e-02,\n",
      "        -9.0570e-02, -7.8949e-03,  5.7212e-02,  2.2264e-02, -1.0805e-01,\n",
      "        -2.6872e-02, -1.5961e-01, -1.7126e-02,  6.4517e-02,  1.6080e-01,\n",
      "         9.2804e-02,  9.2394e-02,  6.6680e-02,  4.7681e-03, -7.9464e-02,\n",
      "        -4.0883e-02, -8.7437e-03,  7.2529e-02, -5.7476e-03, -1.6923e-01,\n",
      "         1.3494e-01,  4.4086e-03, -2.6635e-02, -9.1306e-02, -1.6101e-01,\n",
      "        -9.0868e-02, -1.2951e-01,  2.0142e-01,  1.6038e-01, -7.5534e-02,\n",
      "        -7.0624e-02, -6.4315e-02,  4.5626e-02,  5.0597e-02,  1.2234e-01,\n",
      "         3.2224e-02, -2.9204e-02, -1.3893e-04, -8.4911e-02, -1.2546e-01,\n",
      "        -8.0464e-02, -7.2484e-02, -6.5274e-02,  1.3907e-01, -7.3904e-02,\n",
      "         1.8982e-02, -1.2879e-01, -6.9592e-02, -1.7689e-02,  7.6078e-02,\n",
      "        -1.8884e-01,  5.3437e-02, -1.5796e-02,  6.1636e-02,  2.1106e-01,\n",
      "         8.9708e-03,  9.2005e-02,  3.1825e-02,  5.9694e-02, -5.0035e-02,\n",
      "        -2.3229e-02,  1.4812e-01,  3.8936e-02, -1.9375e-02,  1.3881e-01,\n",
      "        -3.4806e-02,  8.9239e-02,  2.7730e-02,  2.6963e-02,  1.1297e-01,\n",
      "        -4.7284e-03,  1.4820e-02, -3.9706e-02, -3.4269e-02, -9.1147e-02,\n",
      "        -2.6727e-02, -1.6366e-02, -2.8936e-02, -9.5033e-02,  2.7897e-01,\n",
      "         2.9954e-02, -1.0818e-02, -1.0304e-01, -4.9462e-02, -1.7020e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([ 0.0394, -0.1491, -0.1779, -0.0710,  0.0670, -0.0121, -0.0127,  0.0460,\n",
      "        -0.0049, -0.0176,  0.1191, -0.0131,  0.1944,  0.1848, -0.0067, -0.0600,\n",
      "        -0.1519,  0.0193,  0.2345, -0.0658, -0.0107, -0.0667, -0.1157, -0.0353,\n",
      "         0.0321,  0.0428,  0.1781, -0.0881,  0.0448,  0.0643, -0.0651, -0.1096,\n",
      "         0.1371,  0.1216, -0.0146, -0.1351,  0.0725,  0.0025,  0.0020,  0.1772,\n",
      "        -0.0363,  0.1354, -0.0826,  0.0282,  0.1499,  0.1100, -0.0217, -0.0323,\n",
      "         0.1064,  0.0777, -0.0630,  0.0593,  0.0141,  0.0396,  0.1578,  0.0239,\n",
      "        -0.0297, -0.1157, -0.1137, -0.0027, -0.0045, -0.1797,  0.1568, -0.0571,\n",
      "        -0.0664, -0.0273,  0.1943,  0.0470,  0.0688,  0.0054, -0.0012, -0.0499,\n",
      "         0.0047, -0.0051,  0.0062, -0.1756,  0.0273, -0.1117,  0.2143, -0.0081,\n",
      "        -0.0495, -0.1734,  0.0334, -0.0496,  0.1457, -0.0439,  0.0839, -0.0505,\n",
      "        -0.0089, -0.0271,  0.2175,  0.0486, -0.0341, -0.2209, -0.0034,  0.0340,\n",
      "        -0.1254,  0.1643,  0.0080,  0.1597,  0.0127, -0.0239, -0.0182,  0.0474,\n",
      "        -0.1676,  0.0536,  0.0220,  0.1013,  0.1180, -0.0132, -0.0829,  0.0447,\n",
      "        -0.1728,  0.0686, -0.0004, -0.0928,  0.0969,  0.0629,  0.1016, -0.0971,\n",
      "        -0.0270,  0.1107,  0.1659, -0.1125, -0.0260,  0.1613, -0.0517,  0.0407,\n",
      "         0.0395, -0.0307, -0.0420, -0.0068,  0.0014, -0.0696, -0.0836, -0.0603,\n",
      "         0.0822,  0.0882, -0.0915,  0.0485,  0.0935, -0.0386,  0.0167,  0.1248,\n",
      "         0.0470,  0.0737, -0.0448, -0.0384, -0.0672,  0.1629, -0.0229,  0.0089,\n",
      "        -0.0837, -0.0335, -0.1178,  0.1588, -0.0630, -0.1195,  0.1356,  0.1092,\n",
      "        -0.1060,  0.1282,  0.0429, -0.1231, -0.0743, -0.0008, -0.0186,  0.0896,\n",
      "        -0.0406,  0.0637, -0.0532,  0.0891, -0.0314, -0.1536,  0.0091,  0.1888,\n",
      "        -0.2381,  0.0209, -0.1416, -0.0098, -0.1755, -0.0025,  0.1336, -0.0373,\n",
      "        -0.2174,  0.0078,  0.1147,  0.1133, -0.0072,  0.1242, -0.2566,  0.0794,\n",
      "         0.0189,  0.0114,  0.1542,  0.1465,  0.2008, -0.0085, -0.0034, -0.0937],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 0.0914,  0.1180, -0.0229, -0.1196, -0.0129, -0.0504,  0.0209, -0.0271,\n",
      "        -0.0564, -0.0012,  0.0187, -0.2119,  0.0741, -0.0674, -0.0044, -0.1172,\n",
      "        -0.0261,  0.0678,  0.0451,  0.0797,  0.0520,  0.1229, -0.1709,  0.0607,\n",
      "         0.0241,  0.0400, -0.0082, -0.0386,  0.0279,  0.0271, -0.1326,  0.0435,\n",
      "        -0.0942,  0.0632, -0.1803,  0.0955, -0.0076, -0.1008, -0.1201, -0.0363,\n",
      "         0.0157, -0.0928,  0.0898,  0.0983,  0.0827, -0.0613, -0.0395,  0.1430,\n",
      "         0.0404,  0.1420,  0.1006, -0.0244, -0.1246, -0.1610, -0.0110, -0.0528,\n",
      "        -0.1338, -0.0357,  0.0690,  0.0576, -0.1393, -0.1406, -0.0230,  0.0667,\n",
      "         0.0182,  0.0524,  0.2160, -0.1131,  0.0468,  0.0163,  0.0410,  0.0983,\n",
      "        -0.0028, -0.0469,  0.0684, -0.2216,  0.1558, -0.1197,  0.1024, -0.1434,\n",
      "         0.0958,  0.1495,  0.0768, -0.0951, -0.0892,  0.0343, -0.0838,  0.1064,\n",
      "        -0.0374,  0.0217,  0.0651, -0.0176, -0.0419,  0.0358,  0.1855,  0.1585,\n",
      "         0.1413, -0.1078, -0.0619,  0.0331, -0.0470,  0.0877, -0.0369,  0.0143,\n",
      "         0.0062, -0.1123,  0.0758, -0.0622, -0.0365,  0.0489,  0.1927,  0.0847,\n",
      "        -0.1015,  0.0630, -0.1133, -0.1256,  0.1360, -0.1332,  0.0872, -0.1599,\n",
      "        -0.0854,  0.1867, -0.0034, -0.2340,  0.0620,  0.1052,  0.0365, -0.1078,\n",
      "         0.1621, -0.1138, -0.0358, -0.0804,  0.0170,  0.0876,  0.0143,  0.0506,\n",
      "        -0.0231,  0.0735,  0.0274, -0.1411,  0.0491,  0.0967, -0.0593,  0.1313,\n",
      "         0.0237,  0.0725, -0.0518, -0.0324, -0.0762, -0.0401, -0.0408, -0.0879,\n",
      "         0.0049,  0.0043, -0.0739, -0.1068,  0.0037, -0.0624, -0.0944,  0.0759,\n",
      "         0.0933, -0.0053, -0.0491,  0.1113, -0.0845,  0.0563,  0.0944, -0.0401,\n",
      "         0.0346, -0.0173, -0.0466,  0.0731,  0.0102, -0.0975,  0.0361,  0.0294,\n",
      "        -0.0175, -0.0081,  0.0336,  0.0465,  0.1808, -0.0285,  0.1025, -0.1088,\n",
      "        -0.1825,  0.1126, -0.0188,  0.0613, -0.0308, -0.0631,  0.0853,  0.1618,\n",
      "         0.0741,  0.0685,  0.0342,  0.0538, -0.0213, -0.0030, -0.1286, -0.0548],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([ 0.0224,  0.0946,  0.0133, -0.1132,  0.0010, -0.0133,  0.0031,  0.1008,\n",
      "         0.0212, -0.0341, -0.0248,  0.0546,  0.1564,  0.0631, -0.1225,  0.0253,\n",
      "         0.1188,  0.0976,  0.0636,  0.0430, -0.0356,  0.0598, -0.1432, -0.0806,\n",
      "        -0.2048, -0.0052, -0.0034, -0.0435, -0.1049, -0.0011, -0.0212,  0.0708,\n",
      "         0.0387,  0.0233, -0.1617, -0.0080,  0.1583, -0.0944, -0.0748, -0.2051,\n",
      "        -0.0875,  0.0059, -0.0167, -0.0623,  0.0084, -0.0419, -0.1504, -0.1279,\n",
      "        -0.1402, -0.0827,  0.0179,  0.0556,  0.0019,  0.1691, -0.1009,  0.0423,\n",
      "        -0.0736, -0.0839,  0.1007, -0.1244,  0.1024, -0.1244,  0.0078, -0.1251,\n",
      "        -0.0607,  0.1569,  0.0334,  0.1768, -0.0943, -0.1213,  0.1264,  0.0953,\n",
      "        -0.0232, -0.1155,  0.1418,  0.0410, -0.0699,  0.0250,  0.0560,  0.1527,\n",
      "         0.0333,  0.0055,  0.1346,  0.0200,  0.0035, -0.0574,  0.1489,  0.0715,\n",
      "         0.0533,  0.1377, -0.0910,  0.0285, -0.0638, -0.0159,  0.0638, -0.1863,\n",
      "         0.0573,  0.1685,  0.0779, -0.1418,  0.1581, -0.0769, -0.0513, -0.0866,\n",
      "         0.0069, -0.0284,  0.1844, -0.0338,  0.0171,  0.0032,  0.1009, -0.0172,\n",
      "        -0.0393, -0.0510,  0.2170,  0.0668, -0.0592, -0.0075,  0.0856, -0.0208,\n",
      "         0.1362, -0.0281,  0.1050, -0.0319, -0.0235,  0.0926,  0.0101, -0.0268,\n",
      "        -0.0712,  0.0277, -0.1447,  0.0509,  0.0547,  0.0275, -0.0105, -0.1502,\n",
      "        -0.0557,  0.0192,  0.0110, -0.1416,  0.0273,  0.1708,  0.2071,  0.0848,\n",
      "        -0.1085, -0.0413,  0.0013,  0.0579,  0.0281, -0.0221,  0.0511,  0.0238,\n",
      "         0.0142, -0.1758, -0.1588, -0.0859, -0.0765,  0.0411, -0.0410,  0.1129,\n",
      "         0.1034,  0.0998, -0.0756,  0.0789, -0.0711,  0.0567, -0.1393,  0.0422,\n",
      "         0.0240,  0.1240, -0.2210,  0.0249,  0.0013,  0.0810, -0.1762, -0.0148,\n",
      "         0.0042,  0.1143, -0.0454,  0.0333, -0.1364,  0.0674,  0.0665,  0.1295,\n",
      "         0.0390, -0.0740, -0.0645,  0.0411,  0.2120, -0.0121, -0.1141,  0.1375,\n",
      "         0.1292, -0.0100, -0.0947, -0.1075, -0.2135,  0.0531, -0.0294,  0.0065],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 0.0761,  0.0363,  0.0765,  0.1569, -0.1549,  0.0788, -0.0826,  0.0583,\n",
      "         0.0720, -0.0684,  0.1210,  0.0773, -0.0622, -0.0360, -0.0905, -0.0851,\n",
      "        -0.0685,  0.0550,  0.0461, -0.0545, -0.2840, -0.1047, -0.1551, -0.0185,\n",
      "        -0.0036,  0.0474, -0.1433, -0.0183, -0.0146,  0.0230,  0.0955,  0.0060,\n",
      "        -0.0939,  0.1065,  0.1948, -0.0738,  0.1698, -0.1753, -0.1793,  0.0361,\n",
      "         0.0989,  0.0126,  0.0073,  0.0530,  0.0868, -0.0163, -0.1141, -0.0896,\n",
      "         0.1388,  0.0668, -0.0387,  0.1380,  0.0028, -0.0443,  0.0235, -0.1613,\n",
      "         0.0921, -0.0417, -0.1747, -0.1208, -0.1769, -0.1959, -0.0301, -0.0091,\n",
      "         0.0336,  0.0046, -0.0653,  0.1069, -0.0190, -0.0579, -0.0315,  0.0315,\n",
      "         0.1956,  0.0097,  0.0956,  0.0993, -0.1356,  0.0601,  0.2180,  0.0761,\n",
      "         0.0569, -0.0826, -0.0515,  0.0341, -0.0648,  0.2135, -0.0679, -0.0691,\n",
      "        -0.0834, -0.1142, -0.1839, -0.1059,  0.0788,  0.0038, -0.1050,  0.0465,\n",
      "        -0.0078, -0.0780, -0.0512,  0.0235, -0.1717, -0.1013, -0.0637, -0.0039,\n",
      "         0.0374, -0.1260,  0.0204, -0.0975, -0.0504,  0.1242, -0.0224,  0.0507,\n",
      "         0.0177, -0.0281, -0.0916,  0.0516,  0.1865,  0.0356,  0.0745,  0.0280,\n",
      "         0.0872, -0.0332, -0.2035, -0.0812, -0.0558, -0.0737, -0.0627,  0.1010,\n",
      "        -0.0163,  0.1689,  0.0303,  0.0964,  0.0016, -0.0391,  0.0279,  0.1393,\n",
      "        -0.0514, -0.0948,  0.0116,  0.0907,  0.0479,  0.0683,  0.2143, -0.0127,\n",
      "         0.0566, -0.0479, -0.1482, -0.0575, -0.1018, -0.0947,  0.0579, -0.0427,\n",
      "         0.0003, -0.0439, -0.0204, -0.1996,  0.1488, -0.0840, -0.1244, -0.0342,\n",
      "        -0.1362, -0.0093,  0.0758,  0.0838,  0.0900,  0.0317,  0.1313,  0.0783,\n",
      "         0.0137,  0.0489,  0.1830, -0.0807, -0.1192, -0.1961, -0.0310,  0.0054,\n",
      "         0.0523,  0.1312,  0.0740, -0.1667, -0.0013, -0.0827,  0.0217,  0.1316,\n",
      "        -0.0434, -0.0051, -0.0854, -0.0079, -0.0867, -0.1077,  0.0245, -0.0742,\n",
      "        -0.0368,  0.1433, -0.0050, -0.0213, -0.0137,  0.0700, -0.0387,  0.0309],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 0.0418, -0.1427, -0.0786,  0.0090, -0.1375,  0.0733, -0.0513, -0.2209,\n",
      "         0.1096,  0.1302,  0.0096,  0.0992,  0.1818, -0.0710, -0.1208, -0.0346,\n",
      "         0.1581, -0.0071,  0.0751,  0.0563, -0.0862, -0.0664,  0.2681,  0.0811,\n",
      "         0.0964,  0.0175,  0.0645,  0.1299, -0.1873,  0.0509, -0.0013, -0.0843,\n",
      "        -0.2109, -0.0282,  0.0438, -0.0337, -0.0080,  0.1596,  0.0313, -0.0784,\n",
      "         0.0079, -0.1232, -0.0713, -0.0713, -0.0344,  0.0253, -0.0923, -0.0097,\n",
      "         0.0537, -0.0423,  0.0012,  0.0375,  0.0571, -0.0896, -0.1030,  0.0697,\n",
      "         0.0899, -0.1212,  0.1010, -0.1651,  0.0638, -0.0481,  0.1672, -0.0963,\n",
      "        -0.0093, -0.0652, -0.1405, -0.1012, -0.0551, -0.0324,  0.2010,  0.0520,\n",
      "         0.0120, -0.0015, -0.0102, -0.0625, -0.0804,  0.0815,  0.0043, -0.0309,\n",
      "        -0.0351,  0.1490,  0.0103, -0.1307,  0.0825, -0.1446, -0.1388, -0.0329,\n",
      "         0.0771,  0.0118, -0.0746,  0.0317, -0.0128, -0.1402,  0.0883,  0.1323,\n",
      "        -0.0303,  0.2018, -0.0369,  0.0532,  0.0917,  0.0213,  0.0717, -0.0748,\n",
      "        -0.0940, -0.0083,  0.1637, -0.0299, -0.0296, -0.1240, -0.0921, -0.0156,\n",
      "        -0.0252, -0.0205, -0.0335, -0.1177, -0.1293,  0.1183, -0.1319, -0.0676,\n",
      "        -0.0510, -0.0645, -0.0653, -0.0412,  0.0922, -0.0188, -0.0279, -0.0361,\n",
      "         0.0091,  0.1835, -0.0501, -0.0552, -0.0708, -0.1437, -0.0203, -0.0632,\n",
      "         0.0899, -0.1007,  0.0548, -0.0545, -0.1270, -0.0172, -0.0347,  0.0376,\n",
      "         0.0103,  0.1392, -0.0735,  0.0191, -0.1096,  0.0067, -0.0961, -0.1288,\n",
      "        -0.1203, -0.0883, -0.0551,  0.0313,  0.0581,  0.1358, -0.1208,  0.1844,\n",
      "         0.0088,  0.0147, -0.1194,  0.1389, -0.1325, -0.2128,  0.1464,  0.0647,\n",
      "        -0.1203, -0.1244, -0.0913, -0.1224, -0.2280,  0.0094, -0.0071, -0.0988,\n",
      "        -0.0503,  0.0256, -0.0542,  0.0631,  0.0720,  0.1691, -0.0240, -0.1324,\n",
      "         0.0297, -0.0484, -0.0052, -0.0689, -0.0503,  0.0638, -0.0101, -0.0961,\n",
      "         0.1894,  0.0035, -0.0606,  0.0400,  0.0388,  0.0993,  0.0349,  0.0457],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-0.0614, -0.0883, -0.0292, -0.0611,  0.2230,  0.0062,  0.1176,  0.1301,\n",
      "        -0.1629,  0.1744, -0.2023,  0.2046, -0.0916, -0.0153, -0.2470,  0.1796,\n",
      "        -0.0867,  0.1400, -0.0894, -0.0020,  0.1421, -0.0161,  0.1619,  0.0466,\n",
      "        -0.1079, -0.1021,  0.0159, -0.0426, -0.1016,  0.0344,  0.0263,  0.2150,\n",
      "         0.2312,  0.0525,  0.1495, -0.1144,  0.0461, -0.0560, -0.1200,  0.0365,\n",
      "        -0.0471,  0.0163, -0.0723,  0.0438,  0.0478, -0.1132, -0.0230, -0.0983,\n",
      "        -0.0323,  0.0800,  0.0433, -0.0669, -0.1449,  0.0220, -0.0273, -0.0416,\n",
      "         0.0856,  0.0682,  0.0444, -0.1039, -0.1063,  0.0524, -0.1043,  0.0919,\n",
      "        -0.1012, -0.0338,  0.1926,  0.0175, -0.1564, -0.1166, -0.1902,  0.0078,\n",
      "        -0.0745,  0.0400,  0.0772, -0.1282,  0.0609,  0.0526, -0.0165, -0.2572,\n",
      "        -0.0364, -0.0586,  0.1399,  0.0100,  0.0819, -0.0988, -0.0550,  0.1288,\n",
      "        -0.0973, -0.0121, -0.0864, -0.0158, -0.0357, -0.0483,  0.0568,  0.1409,\n",
      "        -0.0263, -0.0721, -0.0851,  0.0299,  0.0096, -0.0152,  0.1188, -0.1300,\n",
      "         0.0945,  0.0240,  0.0140,  0.0842,  0.0362, -0.0799,  0.0217,  0.0731,\n",
      "        -0.0419,  0.1068, -0.0263,  0.0680,  0.1223, -0.0257,  0.0668,  0.0288,\n",
      "         0.0051, -0.0563,  0.0051, -0.0322,  0.0527, -0.1893, -0.1052,  0.0487,\n",
      "         0.0996,  0.1326,  0.0585, -0.0908, -0.0950, -0.0639, -0.0567, -0.0784,\n",
      "         0.0214, -0.1503,  0.1719,  0.0605, -0.0557, -0.0135,  0.1927, -0.1813,\n",
      "         0.0242,  0.2757, -0.1072, -0.1679, -0.1884, -0.1569, -0.1154,  0.1498,\n",
      "         0.1629, -0.0478,  0.1633, -0.0332,  0.0787, -0.0044, -0.0497,  0.0388,\n",
      "         0.0105,  0.0249,  0.0889, -0.0495,  0.0843, -0.0764,  0.0123,  0.0560,\n",
      "        -0.0650, -0.2278, -0.0636, -0.0622, -0.2162, -0.0081, -0.0615, -0.0590,\n",
      "        -0.0103, -0.0211,  0.0544,  0.0190, -0.0029, -0.0403,  0.0929, -0.0462,\n",
      "         0.1850,  0.0352,  0.0695,  0.0103,  0.1199, -0.2284, -0.1027, -0.0489,\n",
      "        -0.2000, -0.0009, -0.1689, -0.0801,  0.0045, -0.1080, -0.0921, -0.0948],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([ 0.1529, -0.1358, -0.0257,  0.0197,  0.0354, -0.0370,  0.0382,  0.0205,\n",
      "         0.0158, -0.1470, -0.1171, -0.1012, -0.0712, -0.0660,  0.0189,  0.0083,\n",
      "        -0.1342, -0.1202, -0.0607,  0.0283, -0.0018,  0.1302,  0.0108, -0.0970,\n",
      "        -0.0253,  0.0603,  0.1974,  0.0014,  0.0540,  0.0191,  0.0439,  0.0665,\n",
      "         0.0775,  0.0116, -0.1533, -0.0119,  0.0659,  0.0940, -0.0308, -0.0358,\n",
      "        -0.0765, -0.0870,  0.0418, -0.2382,  0.0356, -0.0277, -0.1081,  0.1207,\n",
      "         0.2101, -0.1443,  0.0360,  0.0086,  0.1419, -0.0718, -0.1613, -0.0420,\n",
      "        -0.1460, -0.0396, -0.0513,  0.0948,  0.0649, -0.0184, -0.1491,  0.0254,\n",
      "         0.0103, -0.0049,  0.1764,  0.0566, -0.1306,  0.0541, -0.0697, -0.0260,\n",
      "        -0.0356, -0.1255, -0.1696, -0.1123, -0.1628,  0.1500,  0.0354, -0.1827,\n",
      "         0.0589,  0.0449, -0.0758,  0.1403,  0.0704, -0.0158,  0.0461,  0.0476,\n",
      "        -0.1881,  0.0819, -0.0160,  0.0941, -0.0722, -0.1285,  0.0347, -0.0441,\n",
      "        -0.1422, -0.0444, -0.0069, -0.0483,  0.0429,  0.1049,  0.0157, -0.1315,\n",
      "        -0.0496, -0.0394,  0.0756, -0.1862, -0.1058, -0.1425,  0.2381,  0.1460,\n",
      "         0.0170,  0.2031,  0.0609, -0.2186,  0.0713, -0.0135,  0.0855,  0.0518,\n",
      "        -0.1321, -0.0709, -0.2485,  0.1017,  0.0285, -0.0036, -0.0929,  0.1149,\n",
      "        -0.0791, -0.1008,  0.1252,  0.0613, -0.0295, -0.0541, -0.0921,  0.0106,\n",
      "        -0.2070, -0.0690, -0.0312,  0.0661,  0.0720,  0.1234, -0.1271, -0.1415,\n",
      "        -0.0545, -0.1851,  0.0426,  0.0283,  0.1062, -0.0781,  0.0427,  0.0272,\n",
      "         0.0755,  0.1669,  0.0689,  0.0782, -0.0288, -0.0730, -0.0101,  0.0992,\n",
      "         0.0455,  0.0520,  0.0170, -0.0673, -0.0788,  0.0492, -0.1352, -0.1346,\n",
      "        -0.0046, -0.0974, -0.1281,  0.0303, -0.0671,  0.1033, -0.0069, -0.0413,\n",
      "         0.0779,  0.0999, -0.0652,  0.1312, -0.0183, -0.0994,  0.0791,  0.0652,\n",
      "         0.0208,  0.1589,  0.1630,  0.0915, -0.0576, -0.1568, -0.0209,  0.0989,\n",
      "        -0.1311, -0.0145,  0.1034,  0.1087, -0.0505,  0.0139,  0.0795,  0.0526],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 0.0817,  0.1028,  0.1210,  0.0520, -0.0845,  0.1181,  0.0410, -0.0808,\n",
      "         0.0731, -0.1759, -0.0729,  0.1865, -0.0181,  0.2634,  0.0981,  0.0500,\n",
      "         0.1830, -0.1611,  0.0712, -0.1600, -0.0227,  0.0155, -0.0450,  0.0384,\n",
      "        -0.1292,  0.0568,  0.0141,  0.0364, -0.0261, -0.0599,  0.0920, -0.2235,\n",
      "        -0.0114,  0.0433,  0.1216, -0.0807, -0.0294, -0.0505,  0.0177,  0.0915,\n",
      "         0.0046, -0.0721,  0.0437, -0.0662,  0.0578,  0.1565,  0.0351, -0.0879,\n",
      "        -0.0273, -0.0240, -0.0188,  0.0364,  0.2077, -0.0811,  0.1587,  0.0178,\n",
      "        -0.0389, -0.0614,  0.2114,  0.0205, -0.0742, -0.0931, -0.0248, -0.1433,\n",
      "        -0.1198,  0.0073, -0.0548, -0.0893,  0.0113, -0.1338, -0.0227, -0.0161,\n",
      "         0.2090, -0.2756,  0.0021,  0.0432, -0.1116,  0.0219, -0.0048,  0.0865,\n",
      "        -0.0864, -0.0232,  0.0875, -0.0300, -0.0468,  0.0395, -0.1653,  0.0549,\n",
      "         0.0663, -0.0898,  0.1307,  0.0551,  0.1385, -0.0796, -0.0302,  0.0010,\n",
      "        -0.0299,  0.0415, -0.0944, -0.0373,  0.0818, -0.0771,  0.0061, -0.0533,\n",
      "        -0.0157,  0.1093, -0.0399, -0.0470, -0.0102, -0.0396,  0.0297,  0.0257,\n",
      "        -0.0749, -0.1446,  0.0886,  0.1396,  0.0980,  0.0994,  0.0878,  0.1077,\n",
      "         0.0885,  0.1468,  0.0804,  0.0439, -0.0753, -0.0242,  0.1044,  0.0312,\n",
      "        -0.0194, -0.0491, -0.0383,  0.0890, -0.1947,  0.0542, -0.0742, -0.0615,\n",
      "         0.0019,  0.0617, -0.0221, -0.0392,  0.0293, -0.0413, -0.1057,  0.1993,\n",
      "         0.0370, -0.2002, -0.0522,  0.0819, -0.2129, -0.1595, -0.0497,  0.1268,\n",
      "         0.0389, -0.0097, -0.0431, -0.0442,  0.0601, -0.1828,  0.0641,  0.0268,\n",
      "         0.0617,  0.0346, -0.1673, -0.1402, -0.1595, -0.0893,  0.0011, -0.0925,\n",
      "         0.0081, -0.1161,  0.0092,  0.0962,  0.0718,  0.3020,  0.0654,  0.1417,\n",
      "         0.0748,  0.0169,  0.0666,  0.0669,  0.1179, -0.0726,  0.0090, -0.1204,\n",
      "        -0.0619, -0.1051,  0.0121,  0.0751, -0.0054,  0.0217,  0.0952,  0.0047,\n",
      "         0.1012, -0.0187,  0.0243, -0.0140,  0.1409,  0.1671, -0.0208,  0.0205],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 0.0126,  0.0206, -0.0445, -0.1550, -0.2153,  0.0503, -0.1225, -0.1447,\n",
      "        -0.0547,  0.0919,  0.0553, -0.1528,  0.0165, -0.0480, -0.0763, -0.0504,\n",
      "        -0.0476, -0.1681,  0.0903,  0.1057,  0.0674, -0.0783, -0.0746,  0.0673,\n",
      "         0.0162,  0.1299, -0.1012,  0.0296,  0.0007,  0.1353,  0.0415,  0.1655,\n",
      "        -0.0564, -0.0575, -0.1147,  0.1858, -0.0088,  0.0188,  0.0524,  0.0459,\n",
      "         0.0507,  0.1135,  0.0323, -0.0629,  0.0578, -0.1092, -0.0546, -0.0368,\n",
      "         0.0794, -0.0457, -0.2819,  0.0166, -0.1350,  0.0370, -0.0618, -0.0046,\n",
      "         0.0957,  0.0229, -0.0949,  0.0444,  0.0384, -0.1413, -0.0118, -0.0863,\n",
      "        -0.0434,  0.0324, -0.1121,  0.0540,  0.3296,  0.0242, -0.0802, -0.0883,\n",
      "        -0.0480, -0.1574,  0.0222, -0.0356,  0.0435, -0.0581, -0.0228,  0.0478,\n",
      "         0.2396,  0.0253, -0.1352,  0.0846,  0.1318,  0.0137, -0.0165,  0.1406,\n",
      "         0.0321, -0.0047, -0.0051,  0.0565,  0.0559,  0.0286,  0.0926,  0.0080,\n",
      "        -0.0730, -0.0694,  0.0887,  0.1606,  0.0161, -0.0790, -0.0652,  0.0731,\n",
      "        -0.0212,  0.0551, -0.0655, -0.1641,  0.0053, -0.0328, -0.0984,  0.0513,\n",
      "        -0.0024, -0.1768, -0.0532,  0.1824,  0.0740, -0.0559, -0.1647,  0.0473,\n",
      "         0.1418, -0.1070,  0.0834,  0.1727, -0.0689,  0.0462,  0.1664, -0.0748,\n",
      "        -0.0542,  0.1381,  0.0024,  0.0828, -0.0747, -0.0368,  0.0803, -0.0380,\n",
      "         0.0808,  0.1047,  0.0476, -0.0048, -0.0018, -0.0153, -0.0676, -0.1352,\n",
      "        -0.1475,  0.0802,  0.0140,  0.0948,  0.0639, -0.0223, -0.0591, -0.0461,\n",
      "        -0.0454,  0.1012,  0.0170, -0.1462,  0.1184,  0.0034, -0.1129, -0.1712,\n",
      "        -0.1241,  0.1622,  0.0478,  0.1992,  0.0425, -0.0305, -0.1320,  0.0207,\n",
      "         0.2156,  0.0600, -0.1770, -0.0956,  0.0641, -0.0395,  0.0556, -0.1521,\n",
      "         0.0942, -0.0960,  0.0551, -0.0231, -0.0022, -0.0416,  0.0397, -0.0214,\n",
      "         0.0263,  0.1561,  0.1168,  0.0152, -0.0922,  0.0769, -0.1372, -0.1035,\n",
      "        -0.1183, -0.2373, -0.0130,  0.3082, -0.0101, -0.0665, -0.0233,  0.0880],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-5.4974e-02, -2.2297e-01, -3.4881e-02, -4.2318e-02, -2.7299e-02,\n",
      "         5.5134e-02,  1.5225e-01,  7.8101e-02,  1.5332e-02, -4.0100e-03,\n",
      "        -1.4449e-01, -3.2753e-02, -1.9791e-02, -1.4651e-01, -4.9901e-02,\n",
      "         9.1330e-02,  3.9200e-02,  1.4399e-01,  1.6924e-03, -1.5800e-01,\n",
      "         6.6664e-02,  7.9082e-02,  1.9514e-01,  8.6151e-02, -1.4055e-01,\n",
      "         4.9156e-02, -1.7042e-02,  8.3202e-02,  3.8395e-02, -7.6573e-02,\n",
      "         1.9280e-02,  9.3700e-02,  3.7776e-02, -3.1756e-03,  9.2474e-02,\n",
      "        -1.3142e-01,  3.5914e-01,  1.6660e-01,  9.9825e-02, -7.4743e-02,\n",
      "         1.0601e-01, -3.1590e-02,  5.6121e-02, -3.1144e-03,  2.2674e-03,\n",
      "         2.3569e-01, -1.1508e-02,  1.5895e-01, -3.3772e-02,  4.4261e-02,\n",
      "        -1.2258e-01, -6.1846e-02,  1.4148e-02,  6.2233e-02,  2.5488e-02,\n",
      "        -7.6385e-02,  7.1666e-02,  3.4484e-02,  1.2316e-01, -2.9820e-02,\n",
      "         5.8162e-02, -1.1326e-01, -2.8134e-02,  2.9367e-02,  1.8751e-01,\n",
      "         1.1523e-01, -1.5254e-01,  5.0529e-02,  1.0538e-01, -7.0036e-02,\n",
      "        -1.6343e-01,  1.0726e-01, -4.2066e-02,  4.3294e-02,  1.1721e-01,\n",
      "        -1.8566e-01,  8.8446e-02, -2.6410e-02,  1.5070e-01, -1.5571e-02,\n",
      "         1.3999e-01, -1.2221e-01,  4.5856e-02, -5.5223e-02,  2.5612e-02,\n",
      "        -1.3341e-01, -7.6279e-02, -4.3621e-02, -6.4357e-03, -1.5981e-01,\n",
      "         3.4849e-02, -8.4653e-03, -9.7544e-02, -4.7455e-02,  6.2057e-02,\n",
      "        -5.4443e-02, -2.1113e-01, -1.1706e-01,  1.1000e-01,  2.5254e-02,\n",
      "         1.3215e-01,  8.5947e-02,  1.3846e-01, -5.3756e-02, -1.0540e-01,\n",
      "         5.5996e-02,  2.6918e-02,  3.8291e-02,  1.3306e-01,  8.8181e-02,\n",
      "        -1.1977e-02,  1.0995e-01,  1.7498e-02, -3.1065e-03, -1.7085e-01,\n",
      "        -1.3999e-01,  1.9463e-01,  3.0805e-02,  5.7261e-03, -1.3954e-01,\n",
      "         2.9100e-02,  8.2958e-02, -1.7611e-02, -3.5195e-02,  1.1764e-01,\n",
      "        -1.3955e-01, -2.2546e-02, -9.6504e-02,  5.3088e-02,  7.1790e-02,\n",
      "        -4.8745e-02,  1.4921e-01,  1.1616e-02, -1.3956e-02,  1.2318e-01,\n",
      "        -4.4337e-02, -3.7895e-02, -3.9958e-03, -1.4850e-01, -5.0798e-02,\n",
      "        -1.8495e-03,  1.6168e-02,  1.4667e-01, -4.4026e-02, -1.2612e-01,\n",
      "         5.2271e-02, -5.5778e-03,  8.4403e-02,  3.4779e-03, -5.2990e-02,\n",
      "        -1.6425e-03, -1.4081e-01, -3.4835e-02, -7.5486e-02,  1.0540e-02,\n",
      "         1.2906e-01,  2.9949e-02, -1.3462e-01, -7.9155e-02, -1.5714e-01,\n",
      "        -7.9140e-02,  3.6304e-02, -6.3104e-02, -1.4539e-01,  7.0543e-02,\n",
      "         1.1354e-01,  1.3244e-01,  1.0528e-02, -5.0785e-02, -3.2326e-02,\n",
      "        -8.8459e-02, -1.2603e-01,  1.3186e-01,  1.2378e-01,  1.8561e-02,\n",
      "         7.4035e-02,  4.7008e-02,  9.2645e-02, -1.6392e-01,  1.0661e-01,\n",
      "        -1.2907e-01, -4.2061e-02,  5.0844e-02, -1.3202e-01,  7.8879e-02,\n",
      "        -5.4763e-03,  3.1804e-02, -5.9242e-02,  1.7029e-01,  1.0181e-02,\n",
      "        -1.3398e-03,  8.6708e-02, -1.1215e-01, -5.1276e-02,  2.9005e-02,\n",
      "        -1.8835e-06, -1.8000e-01,  1.7083e-01, -1.5780e-02,  7.1882e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([ 5.9375e-02, -7.6704e-02, -7.6145e-02,  5.3714e-02,  5.8202e-02,\n",
      "         1.8305e-01,  6.2649e-02,  8.8567e-02, -1.0500e-01, -7.5499e-02,\n",
      "        -1.2603e-02,  2.0544e-03, -8.0558e-02,  3.4244e-02,  2.2745e-03,\n",
      "         6.2256e-02,  1.4463e-01,  1.0340e-01,  3.4346e-02,  5.5836e-02,\n",
      "         1.2487e-01, -6.8228e-02, -8.0623e-02,  6.0462e-02,  5.9717e-02,\n",
      "         4.7349e-02,  4.2886e-03, -3.4406e-02, -1.5241e-03, -2.3107e-02,\n",
      "        -8.4638e-02,  1.3694e-01, -8.6882e-02,  5.4386e-02, -6.9839e-02,\n",
      "         4.5247e-02, -9.4761e-02, -2.0315e-02,  8.1773e-02,  1.4329e-02,\n",
      "         5.4450e-03,  4.5778e-02, -6.6697e-03,  7.6840e-03,  1.4523e-01,\n",
      "        -8.5038e-02, -3.0110e-02,  1.1559e-01, -9.3469e-02,  1.2656e-01,\n",
      "         2.3759e-03, -8.0666e-02, -3.2188e-02,  1.0365e-01,  1.9525e-01,\n",
      "        -6.7806e-02,  1.1556e-01, -1.0471e-01, -9.8658e-02,  6.0892e-02,\n",
      "        -1.2953e-01,  9.7777e-02, -2.2148e-01, -1.5614e-01, -2.0930e-02,\n",
      "        -1.2321e-04, -2.4976e-02, -6.0259e-02,  2.4290e-02, -1.7321e-01,\n",
      "        -3.0781e-02, -1.3012e-01, -2.8965e-02, -1.4532e-01,  9.6810e-02,\n",
      "         2.4889e-01,  9.7137e-02,  9.8451e-02,  2.1175e-02, -5.4465e-03,\n",
      "         9.9152e-03,  4.3832e-02, -6.2350e-02, -1.9248e-01, -1.6211e-01,\n",
      "         5.5161e-02, -4.6796e-03,  1.1272e-02, -6.8783e-04,  6.9743e-02,\n",
      "         5.4847e-02,  4.4464e-02,  1.9774e-01,  6.5964e-02,  8.6758e-02,\n",
      "        -2.9624e-02, -2.3558e-02,  2.8779e-01, -5.1679e-03,  1.7323e-01,\n",
      "        -6.2093e-02, -2.0631e-03,  1.0708e-01, -1.0540e-01, -9.7950e-02,\n",
      "        -7.6850e-02, -6.1583e-03,  1.4622e-02, -1.9832e-02,  9.5625e-03,\n",
      "         4.9432e-02,  7.5333e-02,  3.4438e-01,  5.5603e-02, -3.1572e-02,\n",
      "        -1.6360e-02, -1.7910e-03,  2.3582e-01, -2.1540e-02, -6.2122e-02,\n",
      "         5.6872e-02, -1.5383e-03,  1.1906e-01, -4.5656e-02,  4.3734e-02,\n",
      "         6.7003e-02,  5.0686e-02, -2.8803e-02, -3.8180e-02,  8.0218e-02,\n",
      "         5.9399e-02, -5.6738e-02,  1.2859e-02, -1.7085e-01,  8.6136e-02,\n",
      "        -1.3894e-01, -2.8930e-02, -5.8629e-03, -5.4628e-02,  8.8056e-02,\n",
      "        -5.4015e-02,  4.5942e-02,  1.2060e-01, -3.3442e-02, -6.5402e-03,\n",
      "         1.2162e-01,  3.4350e-02, -3.8756e-02, -7.5196e-02, -3.5302e-01,\n",
      "         7.2767e-02,  4.0185e-02,  3.4465e-02, -4.8637e-02,  1.9247e-01,\n",
      "        -2.4304e-01, -2.6409e-02,  9.8851e-02, -8.6745e-02,  1.7552e-03,\n",
      "         1.3757e-01,  1.2431e-01,  1.9678e-03,  7.2300e-02, -1.0662e-01,\n",
      "         1.8833e-01, -1.9998e-01,  9.2911e-02,  8.1926e-02, -4.9496e-02,\n",
      "        -2.6035e-02, -3.3177e-02, -6.3533e-02, -1.8825e-01, -6.1079e-03,\n",
      "         1.8882e-01,  1.7851e-03, -4.3285e-02, -3.2157e-02, -4.5108e-02,\n",
      "         1.0911e-01,  7.9313e-02, -1.0430e-01,  7.5276e-02,  8.7997e-02,\n",
      "        -2.8471e-02,  3.4550e-02, -6.3389e-03,  6.0502e-02,  3.6258e-02,\n",
      "        -3.5568e-02, -6.8907e-02, -1.2004e-01,  2.9745e-01, -9.5167e-02,\n",
      "        -7.7311e-02,  3.3655e-02, -2.3027e-02, -1.5712e-01, -9.7612e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 0.0311, -0.0007,  0.0629,  0.0317,  0.2798, -0.2489, -0.0790, -0.0293,\n",
      "         0.0722, -0.0106, -0.1173,  0.0087,  0.1277,  0.0938,  0.0036,  0.0399,\n",
      "        -0.0296, -0.0021, -0.1105, -0.0811,  0.0772, -0.0459,  0.0796, -0.0581,\n",
      "         0.1464, -0.1314, -0.1065, -0.0187,  0.1895, -0.0847,  0.0269, -0.1065,\n",
      "        -0.1231,  0.0704, -0.0105, -0.0298,  0.0578, -0.0113,  0.1469, -0.0952,\n",
      "         0.0637, -0.2162,  0.0250, -0.0888,  0.0823,  0.1444,  0.0230, -0.0557,\n",
      "         0.1704,  0.0894,  0.1615,  0.0806,  0.0833,  0.0240, -0.0161, -0.0282,\n",
      "        -0.0792,  0.1578, -0.1625,  0.1500, -0.0318, -0.0282, -0.0260, -0.0035,\n",
      "         0.0784, -0.0239,  0.0770, -0.1865, -0.0571,  0.0870, -0.1007,  0.0281,\n",
      "        -0.0713,  0.0717,  0.1238,  0.1216,  0.1066, -0.0312, -0.0911, -0.0536,\n",
      "         0.1092, -0.1572, -0.0039,  0.0991,  0.1128, -0.2336, -0.0040, -0.1238,\n",
      "        -0.0439,  0.1050, -0.2181,  0.0454,  0.0657,  0.0067, -0.0718,  0.0453,\n",
      "        -0.1125, -0.1131, -0.0911,  0.0247, -0.0146, -0.0078,  0.0960,  0.1138,\n",
      "        -0.0346,  0.0562,  0.1412,  0.0725, -0.0326,  0.0351, -0.0063,  0.0080,\n",
      "         0.0907,  0.2647, -0.1034, -0.1216, -0.1155, -0.0135,  0.0246,  0.1549,\n",
      "         0.0353,  0.0483, -0.0025, -0.1011, -0.0650,  0.1263, -0.2216,  0.0219,\n",
      "        -0.0434,  0.0206,  0.0175, -0.0598, -0.0364, -0.1782,  0.0953, -0.1035,\n",
      "        -0.0742,  0.0837, -0.0265, -0.1922, -0.0087, -0.1640, -0.0518, -0.0344,\n",
      "         0.0124,  0.0532,  0.0723, -0.0831,  0.1732, -0.0448, -0.0103, -0.1354,\n",
      "        -0.0522, -0.0420, -0.0649, -0.1044, -0.0269,  0.0143,  0.0179, -0.0237,\n",
      "        -0.0835, -0.0019,  0.1081,  0.1509,  0.0385, -0.2299,  0.0059,  0.0780,\n",
      "        -0.0943,  0.2166,  0.0656, -0.2295, -0.0140, -0.1267,  0.0348, -0.0188,\n",
      "        -0.1542, -0.1179, -0.1004, -0.0768,  0.0163, -0.1128,  0.0662, -0.0508,\n",
      "         0.1186, -0.1549, -0.0302,  0.1885,  0.1300,  0.1533,  0.1691,  0.1743,\n",
      "         0.0860,  0.1271,  0.0993, -0.0737, -0.0292, -0.2214, -0.0309, -0.0463],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([ 0.0845, -0.2188,  0.1519,  0.0252,  0.0874, -0.0196, -0.1169, -0.0246,\n",
      "         0.0615, -0.0262, -0.0139,  0.0260, -0.1098,  0.0394,  0.1519, -0.0440,\n",
      "         0.0454,  0.0552, -0.0286, -0.0211, -0.0378,  0.0004, -0.1446, -0.0387,\n",
      "        -0.0276, -0.0212,  0.1699, -0.0604,  0.0852,  0.0113,  0.0029, -0.1330,\n",
      "         0.1399, -0.1362,  0.0189,  0.0172,  0.0463, -0.1469,  0.0272, -0.1413,\n",
      "         0.1221,  0.0776,  0.1696,  0.0255, -0.0421, -0.1227,  0.0388,  0.1233,\n",
      "         0.0069, -0.0920,  0.2908,  0.1358,  0.0031, -0.1033, -0.1842,  0.0755,\n",
      "         0.0800,  0.1084,  0.1301,  0.0518,  0.0508,  0.0933, -0.0832,  0.0086,\n",
      "        -0.1125, -0.0629, -0.0327, -0.0013,  0.0636,  0.1860,  0.0360, -0.0853,\n",
      "         0.1112, -0.0483,  0.0338,  0.0224,  0.0933, -0.0503, -0.0517, -0.0354,\n",
      "         0.0892,  0.0901,  0.1553, -0.0352,  0.0527, -0.0738, -0.0549, -0.1618,\n",
      "         0.0052, -0.1052,  0.1081, -0.1450, -0.0310,  0.0111,  0.0439,  0.0547,\n",
      "        -0.0119,  0.1460, -0.1199,  0.0848, -0.1861, -0.1009,  0.0893, -0.0249,\n",
      "        -0.1046,  0.0958,  0.0230, -0.1556, -0.1275, -0.0182, -0.0293, -0.0258,\n",
      "         0.0164, -0.1364, -0.0388, -0.1714, -0.0805, -0.0572,  0.1112,  0.0333,\n",
      "        -0.0708,  0.1675, -0.1562,  0.1674, -0.0183, -0.0984,  0.0799,  0.1549,\n",
      "         0.0659, -0.0590, -0.1070, -0.0401,  0.1094, -0.0894,  0.0575,  0.0378,\n",
      "        -0.1423,  0.2235,  0.0048,  0.0746,  0.0136, -0.1266,  0.0015, -0.0285,\n",
      "        -0.1162, -0.0765,  0.0575,  0.0862, -0.0235,  0.1078,  0.0550, -0.0058,\n",
      "         0.0413,  0.0457, -0.0703, -0.0111, -0.1519, -0.0838,  0.0232, -0.0544,\n",
      "         0.0793, -0.0810, -0.2028, -0.1665, -0.0973,  0.0940,  0.1496, -0.0531,\n",
      "        -0.1684, -0.0910,  0.0042, -0.1452,  0.0906, -0.0320, -0.0259,  0.0587,\n",
      "         0.0848, -0.1101, -0.0715, -0.0879,  0.0854,  0.1500, -0.0082, -0.1002,\n",
      "        -0.0065, -0.1670,  0.0589,  0.0075, -0.1083,  0.0568,  0.0661, -0.0605,\n",
      "         0.0610,  0.1015,  0.0214, -0.0371,  0.0521, -0.0713, -0.1853,  0.0535],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([-3.6876e-02, -4.9669e-02,  9.9005e-02,  1.0860e-01, -1.0871e-01,\n",
      "         2.7956e-01,  6.7832e-02,  6.8072e-02, -7.1654e-02, -3.8881e-02,\n",
      "         2.5751e-02,  1.8875e-01,  6.3718e-02,  1.9753e-02,  8.9828e-02,\n",
      "         4.1788e-02, -4.2429e-02,  8.2923e-02, -2.8140e-02,  2.2479e-01,\n",
      "         1.6036e-01, -6.5913e-02,  1.3137e-01, -2.6496e-02,  6.7746e-02,\n",
      "         9.1310e-02,  2.8175e-03,  2.4631e-02, -4.7669e-03, -2.4984e-02,\n",
      "         2.2374e-03, -1.7278e-01,  1.0126e-01,  9.6983e-02,  3.9686e-02,\n",
      "         5.3383e-02, -7.5880e-02,  1.2616e-01, -5.0181e-02,  9.3613e-02,\n",
      "        -4.8937e-02, -1.0337e-01, -1.4467e-01, -4.7167e-02,  9.4517e-02,\n",
      "        -9.8730e-03, -4.9088e-02, -1.2334e-01, -5.3631e-02, -1.4713e-01,\n",
      "        -9.4510e-02, -1.7027e-01,  8.9835e-02,  1.3652e-02, -3.2634e-02,\n",
      "         3.0937e-02,  1.0024e-01, -2.5995e-02,  3.9273e-02, -2.0102e-03,\n",
      "         1.5887e-02,  3.6823e-02,  8.6731e-02, -1.1192e-01,  1.1318e-01,\n",
      "        -1.1096e-02,  7.6170e-02, -5.1259e-02, -1.8678e-01,  1.1488e-01,\n",
      "         1.2786e-02,  4.1755e-02,  5.4775e-02,  1.9289e-01,  1.5120e-02,\n",
      "         7.9162e-02, -2.7783e-04, -1.6643e-01, -5.5321e-02,  2.0850e-01,\n",
      "         9.2677e-03, -1.3706e-01,  5.9678e-02,  1.5976e-01,  5.2735e-02,\n",
      "        -3.6422e-03, -5.0280e-02, -1.2166e-01,  2.5606e-02, -9.9197e-03,\n",
      "         1.2920e-02,  1.0550e-01, -8.8087e-02,  1.5330e-01,  5.2317e-02,\n",
      "         2.4357e-01, -1.9768e-02,  4.5939e-02, -4.1221e-02,  1.0261e-01,\n",
      "         7.8977e-02, -5.5433e-02, -4.1342e-02,  1.1185e-01,  5.2323e-02,\n",
      "        -2.8969e-02, -2.0906e-01,  5.5857e-04,  2.0851e-01, -8.4756e-03,\n",
      "        -4.0766e-02,  1.6426e-01, -1.2086e-02,  1.0616e-01,  1.4379e-01,\n",
      "         1.4786e-02,  6.3238e-02,  5.9145e-03,  9.1153e-02,  3.4932e-02,\n",
      "         7.9166e-02,  3.8234e-03, -1.8062e-01,  2.3459e-03,  1.1018e-01,\n",
      "        -1.6615e-01,  1.5937e-01,  6.1884e-02, -1.6840e-01, -1.1897e-01,\n",
      "         1.1536e-02,  1.5334e-01, -7.6687e-02, -5.1280e-02, -1.2516e-01,\n",
      "        -5.0220e-02, -6.7492e-02,  2.3567e-02, -5.1054e-02,  2.2035e-01,\n",
      "        -5.0759e-02, -5.0431e-02, -1.8132e-03,  1.0943e-01, -1.6994e-01,\n",
      "         1.6462e-02,  1.0024e-01,  1.1332e-01,  6.9177e-02,  5.1765e-02,\n",
      "        -3.8329e-02, -5.9091e-02, -1.6886e-01,  1.3516e-01, -4.5544e-02,\n",
      "        -1.1622e-01, -1.1263e-01,  3.6675e-02, -1.3451e-01, -3.3924e-02,\n",
      "         6.6867e-02, -2.7770e-02, -7.3864e-02,  1.3600e-02, -8.5701e-02,\n",
      "         1.0013e-01,  1.4625e-01, -8.2268e-02,  5.3335e-02, -9.0384e-02,\n",
      "        -1.5292e-01,  3.2314e-02,  7.8510e-02,  1.0687e-01,  9.9541e-02,\n",
      "         1.7737e-01, -9.5348e-02,  1.1430e-01,  9.6552e-02, -5.4725e-02,\n",
      "         8.1018e-02,  2.2761e-02,  1.2537e-01,  1.1077e-01,  1.4499e-01,\n",
      "        -3.5023e-02,  1.1443e-01,  4.6458e-02, -3.5111e-02, -1.2706e-01,\n",
      "         3.1536e-02, -9.8799e-02, -1.6721e-02,  5.3622e-02,  5.2857e-02,\n",
      "         2.2366e-03, -6.1468e-02, -2.7146e-02,  1.0245e-01,  7.1280e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 0.0878,  0.0080,  0.0486,  0.0576,  0.1490,  0.0370,  0.0408, -0.0412,\n",
      "         0.2490,  0.0098, -0.0293,  0.0131, -0.0857, -0.1304, -0.0678, -0.1464,\n",
      "         0.0379,  0.0224,  0.2812,  0.0123,  0.0617, -0.0463,  0.0487, -0.0123,\n",
      "         0.1162, -0.1322, -0.1128,  0.1577, -0.0004, -0.0785, -0.0079, -0.0251,\n",
      "        -0.0158,  0.0626, -0.0270,  0.1095,  0.0220, -0.0405,  0.1460,  0.0040,\n",
      "        -0.0548,  0.0943,  0.1211, -0.0974,  0.1113,  0.1712, -0.0047,  0.0365,\n",
      "        -0.1497, -0.0672,  0.0003, -0.1382,  0.0658, -0.1403, -0.0328, -0.0707,\n",
      "         0.0963, -0.0124,  0.1958, -0.0891,  0.0583, -0.0414,  0.1090,  0.0058,\n",
      "         0.0074, -0.0780, -0.0096, -0.0742, -0.0254,  0.0048, -0.0247, -0.0476,\n",
      "         0.0619, -0.0123,  0.0307,  0.1183,  0.1790, -0.0870, -0.0201,  0.0335,\n",
      "         0.0626, -0.0112,  0.0479,  0.1285, -0.0744, -0.0896, -0.1035,  0.0280,\n",
      "        -0.1037,  0.0578, -0.0905, -0.1037, -0.0423,  0.0015,  0.0175, -0.0016,\n",
      "         0.0369,  0.0396, -0.0072,  0.0015, -0.0367,  0.0788,  0.0563, -0.0116,\n",
      "        -0.1457, -0.1005,  0.0860, -0.0378, -0.0518,  0.2767, -0.0460,  0.0391,\n",
      "        -0.0947,  0.0926, -0.0978, -0.0128, -0.0015,  0.0169, -0.0403, -0.0317,\n",
      "        -0.1332, -0.0321, -0.0037, -0.0686,  0.0983,  0.0416,  0.0280,  0.0057,\n",
      "         0.1820, -0.1037, -0.1246,  0.0956, -0.0499, -0.1206, -0.0907,  0.0600,\n",
      "         0.1097,  0.1554,  0.0305, -0.0014,  0.0872,  0.0052, -0.1868,  0.3174,\n",
      "         0.0372, -0.1566, -0.0441,  0.2846, -0.0258,  0.0279, -0.0482, -0.1368,\n",
      "        -0.0163, -0.0609,  0.0243,  0.0281,  0.0175,  0.0667,  0.0219, -0.0175,\n",
      "         0.0705,  0.0570,  0.0123, -0.1167,  0.0618,  0.0807,  0.1450,  0.0478,\n",
      "        -0.0692, -0.1466, -0.0294, -0.0475, -0.2270, -0.1301, -0.0108, -0.0009,\n",
      "         0.0079,  0.0933,  0.1781,  0.0778, -0.0338, -0.0770,  0.0654, -0.0231,\n",
      "         0.1622,  0.0149, -0.0691,  0.0771,  0.0109,  0.0945,  0.0854, -0.0051,\n",
      "        -0.1066,  0.1244,  0.2132, -0.0290, -0.0155,  0.0129, -0.0564,  0.1826],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([ 0.1504,  0.0543, -0.0049, -0.0091, -0.0210, -0.1233, -0.1687,  0.1418,\n",
      "        -0.1611,  0.0407, -0.0332,  0.0674,  0.1338,  0.0192,  0.0760, -0.0430,\n",
      "        -0.0699,  0.0593, -0.0371, -0.0384, -0.1696,  0.2141,  0.0967,  0.1100,\n",
      "         0.0676, -0.0848, -0.0806, -0.0269, -0.0808,  0.0501, -0.0979,  0.0434,\n",
      "        -0.1130, -0.1594, -0.0724, -0.1089,  0.0577, -0.0420, -0.1049, -0.0653,\n",
      "         0.0755,  0.2010,  0.2911, -0.0659,  0.0807,  0.1089, -0.2063, -0.1041,\n",
      "         0.1128, -0.0471, -0.0217,  0.0458, -0.2049,  0.0866,  0.0644,  0.1438,\n",
      "         0.1802,  0.0858, -0.0747, -0.2696, -0.0825, -0.0391,  0.0147,  0.0222,\n",
      "        -0.0791, -0.0260, -0.0167, -0.1780, -0.1077,  0.1307,  0.1465, -0.0099,\n",
      "        -0.1666, -0.1526, -0.0303,  0.0825, -0.0327, -0.0523, -0.0285,  0.0664,\n",
      "         0.0259, -0.1096, -0.0994, -0.0096, -0.0313, -0.0295,  0.1027, -0.1113,\n",
      "        -0.1118, -0.1073, -0.0133,  0.1019,  0.0339, -0.0345, -0.0787,  0.0762,\n",
      "        -0.0632, -0.0087, -0.0201, -0.1077, -0.0670,  0.1389,  0.0214, -0.1904,\n",
      "         0.1568, -0.1013,  0.0074,  0.2757,  0.1652,  0.0491, -0.0073,  0.0714,\n",
      "        -0.0301,  0.0554, -0.0656, -0.1587,  0.0911,  0.1352,  0.1663, -0.0645,\n",
      "        -0.0471,  0.0227,  0.0254, -0.1084,  0.0461, -0.0059, -0.1147,  0.0145,\n",
      "        -0.1414,  0.0614,  0.0289, -0.0849, -0.0432,  0.0004,  0.0516, -0.0828,\n",
      "         0.0466, -0.0458,  0.1007,  0.0579,  0.0995,  0.0156,  0.0084,  0.0991,\n",
      "        -0.0240, -0.1409,  0.0639, -0.0567, -0.0343, -0.0499, -0.1733, -0.0360,\n",
      "         0.0808, -0.0689, -0.0332, -0.1080,  0.0620,  0.0315, -0.0585, -0.0323,\n",
      "         0.0953,  0.0063,  0.0963, -0.0897, -0.0263,  0.1351, -0.0651,  0.0971,\n",
      "         0.1391, -0.0822,  0.0673,  0.0513,  0.1306,  0.0644, -0.0500,  0.0574,\n",
      "         0.0591, -0.1212,  0.1017,  0.1220,  0.1028, -0.1886, -0.0013,  0.0784,\n",
      "        -0.1400, -0.0457, -0.0257,  0.0713, -0.0600,  0.0014,  0.0700,  0.0802,\n",
      "        -0.0972,  0.1332, -0.1711, -0.0903, -0.0134, -0.1228, -0.0016,  0.0783],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  0.066951/  0.080835, val:  45.00%, val_best:  45.00%, tr:  75.79%, tr_best:  75.79%, epoch time: 68.70 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0740%\n",
      "layer   2  Sparsity: 55.6637%\n",
      "layer   3  Sparsity: 56.7858%\n",
      "total_backward_count 9790 real_backward_count 3544  36.200%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  0.046306/  0.071521, val:  50.83%, val_best:  50.83%, tr:  86.41%, tr_best:  86.41%, epoch time: 67.27 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1058%\n",
      "layer   2  Sparsity: 52.7957%\n",
      "layer   3  Sparsity: 55.7868%\n",
      "total_backward_count 19580 real_backward_count 5731  29.270%\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  0.041834/  0.069344, val:  54.17%, val_best:  54.17%, tr:  88.05%, tr_best:  88.05%, epoch time: 67.65 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0568%\n",
      "layer   2  Sparsity: 52.1378%\n",
      "layer   3  Sparsity: 55.6859%\n",
      "total_backward_count 29370 real_backward_count 7711  26.255%\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  0.039366/  0.075814, val:  50.83%, val_best:  54.17%, tr:  90.19%, tr_best:  90.19%, epoch time: 67.47 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   2  Sparsity: 51.7271%\n",
      "layer   3  Sparsity: 55.2718%\n",
      "total_backward_count 39160 real_backward_count 9537  24.354%\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  0.038174/  0.069843, val:  57.50%, val_best:  57.50%, tr:  88.66%, tr_best:  90.19%, epoch time: 67.41 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0558%\n",
      "layer   2  Sparsity: 51.5934%\n",
      "layer   3  Sparsity: 55.4183%\n",
      "total_backward_count 48950 real_backward_count 11342  23.171%\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  0.036542/  0.058791, val:  62.50%, val_best:  62.50%, tr:  91.42%, tr_best:  91.42%, epoch time: 68.24 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0489%\n",
      "layer   2  Sparsity: 51.3815%\n",
      "layer   3  Sparsity: 55.8550%\n",
      "total_backward_count 58740 real_backward_count 13028  22.179%\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  0.035304/  0.067376, val:  56.67%, val_best:  62.50%, tr:  91.52%, tr_best:  91.52%, epoch time: 68.72 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0689%\n",
      "layer   2  Sparsity: 51.2038%\n",
      "layer   3  Sparsity: 55.4441%\n",
      "total_backward_count 68530 real_backward_count 14660  21.392%\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  0.034446/  0.074396, val:  53.75%, val_best:  62.50%, tr:  91.83%, tr_best:  91.83%, epoch time: 67.77 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1196%\n",
      "layer   2  Sparsity: 51.2114%\n",
      "layer   3  Sparsity: 55.3314%\n",
      "total_backward_count 78320 real_backward_count 16267  20.770%\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  0.034103/  0.065277, val:  64.58%, val_best:  64.58%, tr:  92.34%, tr_best:  92.34%, epoch time: 67.83 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   2  Sparsity: 51.0510%\n",
      "layer   3  Sparsity: 55.0489%\n",
      "total_backward_count 88110 real_backward_count 17858  20.268%\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  0.032782/  0.056185, val:  64.58%, val_best:  64.58%, tr:  93.05%, tr_best:  93.05%, epoch time: 67.22 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   2  Sparsity: 50.7357%\n",
      "layer   3  Sparsity: 54.9391%\n",
      "total_backward_count 97900 real_backward_count 19438  19.855%\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  0.031788/  0.065215, val:  58.33%, val_best:  64.58%, tr:  93.46%, tr_best:  93.46%, epoch time: 67.95 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1333%\n",
      "layer   2  Sparsity: 50.6715%\n",
      "layer   3  Sparsity: 54.7175%\n",
      "total_backward_count 107690 real_backward_count 20891  19.399%\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  0.031429/  0.058258, val:  65.42%, val_best:  65.42%, tr:  94.79%, tr_best:  94.79%, epoch time: 67.98 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   2  Sparsity: 50.7297%\n",
      "layer   3  Sparsity: 54.6933%\n",
      "total_backward_count 117480 real_backward_count 22298  18.980%\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  0.030800/  0.070064, val:  52.08%, val_best:  65.42%, tr:  94.48%, tr_best:  94.79%, epoch time: 67.73 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0499%\n",
      "layer   2  Sparsity: 50.5872%\n",
      "layer   3  Sparsity: 54.3185%\n",
      "total_backward_count 127270 real_backward_count 23744  18.656%\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  0.029675/  0.052025, val:  78.75%, val_best:  78.75%, tr:  95.71%, tr_best:  95.71%, epoch time: 67.75 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   2  Sparsity: 50.4715%\n",
      "layer   3  Sparsity: 54.5089%\n",
      "total_backward_count 137060 real_backward_count 25064  18.287%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  0.029572/  0.066193, val:  57.92%, val_best:  78.75%, tr:  95.10%, tr_best:  95.71%, epoch time: 68.13 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0308%\n",
      "layer   2  Sparsity: 50.6303%\n",
      "layer   3  Sparsity: 54.4180%\n",
      "total_backward_count 146850 real_backward_count 26408  17.983%\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  0.028785/  0.054925, val:  63.75%, val_best:  78.75%, tr:  96.42%, tr_best:  96.42%, epoch time: 68.60 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   2  Sparsity: 50.5298%\n",
      "layer   3  Sparsity: 54.4171%\n",
      "total_backward_count 156640 real_backward_count 27744  17.712%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  0.027982/  0.054068, val:  75.00%, val_best:  78.75%, tr:  96.42%, tr_best:  96.42%, epoch time: 68.00 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0865%\n",
      "layer   2  Sparsity: 50.5555%\n",
      "layer   3  Sparsity: 54.0908%\n",
      "total_backward_count 166430 real_backward_count 29008  17.430%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  0.027042/  0.057353, val:  68.75%, val_best:  78.75%, tr:  97.04%, tr_best:  97.04%, epoch time: 68.59 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0640%\n",
      "layer   2  Sparsity: 50.4544%\n",
      "layer   3  Sparsity: 53.8514%\n",
      "total_backward_count 176220 real_backward_count 30205  17.141%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  0.026435/  0.069293, val:  67.50%, val_best:  78.75%, tr:  96.83%, tr_best:  97.04%, epoch time: 67.23 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   2  Sparsity: 50.5511%\n",
      "layer   3  Sparsity: 54.1527%\n",
      "total_backward_count 186010 real_backward_count 31355  16.857%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  0.026384/  0.052457, val:  72.92%, val_best:  78.75%, tr:  97.34%, tr_best:  97.34%, epoch time: 68.01 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0458%\n",
      "layer   2  Sparsity: 50.5248%\n",
      "layer   3  Sparsity: 54.0131%\n",
      "total_backward_count 195800 real_backward_count 32534  16.616%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  0.025811/  0.061624, val:  68.75%, val_best:  78.75%, tr:  98.16%, tr_best:  98.16%, epoch time: 68.18 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   2  Sparsity: 50.4234%\n",
      "layer   3  Sparsity: 54.1454%\n",
      "total_backward_count 205590 real_backward_count 33688  16.386%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  0.024544/  0.049586, val:  77.08%, val_best:  78.75%, tr:  98.06%, tr_best:  98.16%, epoch time: 68.04 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0689%\n",
      "layer   2  Sparsity: 50.3297%\n",
      "layer   3  Sparsity: 53.9213%\n",
      "total_backward_count 215380 real_backward_count 34771  16.144%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  0.023833/  0.054718, val:  72.50%, val_best:  78.75%, tr:  98.37%, tr_best:  98.37%, epoch time: 68.51 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0944%\n",
      "layer   2  Sparsity: 50.5620%\n",
      "layer   3  Sparsity: 53.8850%\n",
      "total_backward_count 225170 real_backward_count 35790  15.895%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  0.023400/  0.057236, val:  73.75%, val_best:  78.75%, tr:  98.37%, tr_best:  98.37%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0964%\n",
      "layer   2  Sparsity: 50.4195%\n",
      "layer   3  Sparsity: 54.0317%\n",
      "total_backward_count 234960 real_backward_count 36774  15.651%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  0.023041/  0.054382, val:  68.75%, val_best:  78.75%, tr:  98.88%, tr_best:  98.88%, epoch time: 67.49 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   2  Sparsity: 50.3059%\n",
      "layer   3  Sparsity: 53.8975%\n",
      "total_backward_count 244750 real_backward_count 37774  15.434%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  0.022563/  0.050856, val:  76.67%, val_best:  78.75%, tr:  98.88%, tr_best:  98.88%, epoch time: 67.69 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 50.3069%\n",
      "layer   3  Sparsity: 53.9373%\n",
      "total_backward_count 254540 real_backward_count 38682  15.197%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  0.021661/  0.045372, val:  82.92%, val_best:  82.92%, tr:  98.67%, tr_best:  98.88%, epoch time: 68.50 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1001%\n",
      "layer   2  Sparsity: 50.4717%\n",
      "layer   3  Sparsity: 53.8655%\n",
      "total_backward_count 264330 real_backward_count 39581  14.974%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  0.021684/  0.047314, val:  80.83%, val_best:  82.92%, tr:  98.98%, tr_best:  98.98%, epoch time: 68.38 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0811%\n",
      "layer   2  Sparsity: 50.4322%\n",
      "layer   3  Sparsity: 53.9596%\n",
      "total_backward_count 274120 real_backward_count 40510  14.778%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'random', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        # \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.03125]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [4.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/256]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        # \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        # \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        # \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"4\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [],\n",
    "        scale_exp=[[999,998],[999,999],[999,999]], # [[neuron_quant,feedback weight quant],[],[]]\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# sweep_id = 'r8tqnhdz'\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
