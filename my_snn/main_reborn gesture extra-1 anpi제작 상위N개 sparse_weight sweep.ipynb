{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33396/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tkinter import FALSE\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "\n",
    "                    output_threshold=0.5,\n",
    "                    random_select_ratio = 2,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    # if DFA_on == True:\n",
    "    #     assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "    ANPI_MODE = True\n",
    "    if ANPI_MODE == True:\n",
    "        assert BPTT_on == False\n",
    "        assert single_step == False\n",
    "        assert DFA_on == True\n",
    "        assert BATCH == 1\n",
    "        assert lif_layer_v_reset >= 10000    \n",
    "        hetero_timesteps = True\n",
    "        origin_timesteps = TIME\n",
    "\n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp,\n",
    "                    ANPI_MODE).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target, output_threshold):\n",
    "            ctx.save_for_backward(input, target, torch.tensor(output_threshold, device=input.device))\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            input, target, output_threshold = ctx.saved_tensors\n",
    "            output_threshold = output_threshold.item()\n",
    "            \n",
    "            mask = (input > output_threshold).float()\n",
    "\n",
    "            # input_argmax = input.argmax(dim=1)\n",
    "            # input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "\n",
    "            final_grad = mask - target_one_hot\n",
    "            # print(f'target{target}')\n",
    "            # print(f' mask {mask},\\n target_one_hot {target_one_hot},\\n final_grad {final_grad}')\n",
    "            return final_grad, None, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self, output_threshold):\n",
    "            super().__init__()\n",
    "            self.output_threshold = output_threshold\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target, self.output_threshold)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion(output_threshold=output_threshold).to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        # optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        # print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            # print(f'train input {inputs.shape}')\n",
    "\n",
    "            \n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            #######################################################################################################################\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                assert False\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                assert False\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                assert False\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "                            \n",
    "            ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "            if hetero_timesteps == True:\n",
    "                # this_data_timesteps = inputs.shape[1]\n",
    "                # TIME = this_data_timesteps//temporal_filter\n",
    "                TIME = origin_timesteps\n",
    "                net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "            ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "            \n",
    "\n",
    "            \n",
    "            # print(f\"inputs1 {inputs.shape}\")\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(inputs.shape[0]//temporal_filter):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2)\n",
    "                    slice_concat = slice_concat.reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    # print(slice_concat.shape)\n",
    "                    # torch.Size([1, 2, 14, 70])\n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        for ttt in range(temporal_filter):\n",
    "                            if ttt == 0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] = slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] + slice_concat[..., shape_temp[-1] * (ttt-1) : shape_temp[-1] * (ttt)]\n",
    "                        slice_bucket.append(slice_concat)\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                # if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                #     inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            \n",
    "            # print(f\"inputs2 {inputs.shape}\")\n",
    "            # inputs2 torch.Size([285, 1, 2, 14, 70])\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                this_sample_total_tw = inputs.shape[0]\n",
    "\n",
    "                # Í∞Å timestepÎ≥Ñ Ìï© Í≥ÑÏÇ∞\n",
    "                time_sums = inputs.sum(dim=(1, 2, 3, 4)) \n",
    "                \n",
    "                # time_sums ÏÉÅÏúÑ NÍ∞ú Ïù∏Îç±Ïä§ Ï∂îÏ∂ú (Ïòà: N = TIME * 2)\n",
    "                N = min(this_sample_total_tw, round(TIME * random_select_ratio))\n",
    "                topk_vals, topk_idx = torch.topk(time_sums, k=N)\n",
    "\n",
    "                # ÏÉÅÏúÑ NÍ∞ú Ï§ëÏóêÏÑú TIMEÍ∞ú ÎûúÎç§ ÏÑ†ÌÉù\n",
    "                chosen_idx = topk_idx[torch.randperm(N)[:TIME]]\n",
    "\n",
    "                # ÏÑ†ÌÉùÌïú Ïù∏Îç±Ïä§ Ï†ïÎ†¨ (ÏãúÍ∞Ñ ÏàúÏÑú Ïú†ÏßÄ)\n",
    "                chosen_idx, _ = torch.sort(chosen_idx)\n",
    "\n",
    "                # ÏÑ†ÌÉùÌïú Ïù∏Îç±Ïä§Î°ú Ïä¨ÎùºÏù¥Ïã±\n",
    "                inputs = inputs[chosen_idx]\n",
    "                # print(f'N {N}, this_sample_total_tw {this_sample_total_tw}, inputs.shape after random select: {inputs.shape}')\n",
    "                # print(f'chosen_idx {chosen_idx}, TIME {TIME}, time_sums.shape {time_sums.shape} time_sums {time_sums}')\n",
    "\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "                    \n",
    "            # print(f\"inputs3 {inputs.shape}\")\n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # print(f'TRAIN time {TIME}')\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                outputs = outputs.sum(dim=0)\n",
    "                # outputsshape : torch.Size([1, 10])\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                # print(f'outputs', outputs, outputs.shape)\n",
    "                # print(f'labels', labels, labels.shape)  \n",
    "                # print(f'iter_loss', iter_loss)  \n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "                        # print(f'tinputs_val {inputs_val.shape}')\n",
    "\n",
    "                        \n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                            \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            assert False\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "                        ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "                        if hetero_timesteps == True:\n",
    "                            # print(inputs_val.shape)\n",
    "                            this_data_timesteps = inputs_val.shape[0]\n",
    "                            TIME = this_data_timesteps//temporal_filter\n",
    "                            net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "                        ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "\n",
    "            \n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    for ttt in range(temporal_filter):\n",
    "                                        if ttt == 0:\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] = slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] + slice_concat[..., shape_temp[-1] * (ttt-1) : shape_temp[-1] * (ttt)]\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            # if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                            #     inputs_val = (inputs_val != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        \n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            # now_T = inputs_val.shape[0]\n",
    "                            # now_time_steps = temporal_filter*TIME\n",
    "                            # start_idx = 0\n",
    "                            # inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "                        ###################################################################################################\n",
    "\n",
    "\n",
    "                        \n",
    "                            \n",
    "                        # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "                        # print(f'TEST time {TIME}, inputs_val.shape {inputs_val.shape}')\n",
    "                        # ##############################################################################################\n",
    "                        # dvs_visualization(inputs_val, labels_val, TIME, BATCH, my_seed)\n",
    "                        # # #####################################################################################################\n",
    "\n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            # print(f'inputs_val, {inputs_val.shape}')\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            outputs = outputs.sum(dim=0)\n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "\n",
    "\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"3\",\n",
    "#                 single_step = False, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.1,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 1.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'one', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 25, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 5_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = True, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[],\n",
    "#                 scale_exp=[[-10,-10],[-10,-10],[-9,-9]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 output_threshold = 0.01,\n",
    "#                 random_select_ratio = 2,\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tftl4e1o with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [512]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 5000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: one\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251112_174338-tftl4e1o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tftl4e1o' target=\"_blank\">ethereal-sweep-23</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/1qaukqa6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/1qaukqa6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/1qaukqa6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/1qaukqa6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tftl4e1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tftl4e1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': False, 'unique_name': '20251112_174345_001', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [512], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'one', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'output_threshold': 0.5, 'random_select_ratio': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 7a22c8a0ef5b9b252dbf98632e270efd\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=512, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.1, v_reset=10000, sg_width=10, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=False, ANPI_MODE=True)\n",
      "      (4): SYNAPSE_FC(in_features=512, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (DFA_top): Top_Gradient(single_step=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 506,880\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "inFeed spike.shape torch.Size([10, 512]) self.weight_fb.shape torch.Size([10, 512])\n",
      "self.weight_fb[0] tensor([ 1.2009e-02,  1.3379e-01, -1.0650e-02,  5.2556e-02, -1.1912e-01,\n",
      "         4.0419e-02, -4.0199e-02, -5.0604e-02,  3.2680e-02, -7.8942e-02,\n",
      "        -1.0288e-01, -1.8775e-02, -5.7299e-03,  1.2332e-02, -6.9353e-02,\n",
      "         1.1499e-01, -4.4228e-02,  4.2593e-02,  4.9323e-02, -2.0675e-03,\n",
      "         9.2336e-02, -3.1971e-02, -1.5728e-02,  9.1276e-02, -2.0181e-02,\n",
      "        -7.1800e-02,  1.4578e-01, -4.2861e-02,  1.1373e-02, -7.3257e-02,\n",
      "        -1.1159e-01, -9.7846e-02,  5.1912e-02,  8.7845e-02,  4.0044e-02,\n",
      "         2.6324e-02, -9.8372e-02,  3.8522e-02,  1.0460e-01, -4.1150e-02,\n",
      "         5.8342e-02,  4.8482e-03,  5.2401e-03, -8.7172e-03,  2.0523e-02,\n",
      "        -3.6457e-02, -6.6373e-02,  5.9048e-03, -2.0717e-02, -3.2546e-02,\n",
      "        -5.4324e-02,  2.4378e-02,  1.0149e-02, -1.2236e-02,  6.2543e-02,\n",
      "        -8.3454e-02, -2.1650e-02, -3.9879e-02,  2.7655e-02, -3.3246e-02,\n",
      "         7.6898e-02, -5.0422e-02,  1.5484e-02, -2.6447e-02,  6.8359e-02,\n",
      "        -6.8262e-02,  3.4312e-02, -7.9518e-02, -2.3619e-02,  3.1812e-02,\n",
      "         6.2016e-03,  1.6009e-02,  2.2387e-02,  1.4105e-01,  1.4450e-03,\n",
      "         9.7970e-02, -7.1751e-02,  5.8704e-02, -2.8309e-02,  4.7077e-02,\n",
      "        -3.5820e-02, -4.3640e-02, -4.4777e-02, -3.1386e-02, -2.7226e-02,\n",
      "        -2.5884e-02,  1.0779e-02,  2.7401e-02,  3.1376e-02, -7.5319e-02,\n",
      "        -1.6829e-02,  1.7118e-02, -8.9122e-02, -4.0006e-02,  4.6343e-03,\n",
      "         1.2001e-02,  3.6892e-02,  1.4373e-02,  7.0655e-02, -4.2197e-02,\n",
      "        -1.0233e-01,  3.7360e-04,  8.5512e-02,  7.8637e-02,  1.4384e-03,\n",
      "        -8.0477e-02, -4.6482e-02,  2.3251e-02, -3.3886e-02, -2.4537e-03,\n",
      "        -4.8149e-02, -1.5486e-01,  4.3330e-02, -5.8045e-03, -1.3386e-02,\n",
      "         2.7755e-02, -1.9510e-02,  1.3393e-03,  3.8708e-02,  1.5263e-02,\n",
      "         4.6335e-02, -7.2374e-03, -6.3238e-03, -3.1016e-02, -3.1252e-02,\n",
      "        -7.4723e-02, -1.5088e-02, -4.1994e-02,  1.2212e-02,  6.0550e-02,\n",
      "        -1.7745e-03,  1.0415e-01,  6.7522e-02, -6.1409e-02, -4.1550e-02,\n",
      "         1.0644e-01,  1.5230e-01, -3.8367e-02,  7.8697e-02, -1.7323e-02,\n",
      "         2.6986e-02,  2.6370e-02,  6.5894e-02, -1.2553e-01, -3.9156e-02,\n",
      "         1.3065e-01, -5.8646e-03,  1.4600e-02, -4.5190e-02, -1.0434e-01,\n",
      "         5.6415e-02,  4.8810e-02, -3.8917e-02,  1.3367e-01,  7.2065e-02,\n",
      "        -2.6348e-02,  1.4814e-02, -7.9086e-02, -7.4679e-03, -3.7547e-02,\n",
      "        -4.9995e-02,  1.3292e-04, -1.2034e-02,  4.6384e-02,  5.0249e-02,\n",
      "         5.1038e-02, -3.7747e-02,  8.0393e-02, -6.6428e-02, -1.4425e-03,\n",
      "        -2.2637e-02, -3.0118e-02,  9.2677e-03, -9.3434e-02,  1.9207e-02,\n",
      "        -2.7770e-02, -6.7883e-02, -7.8605e-02, -9.7644e-02, -9.8327e-02,\n",
      "        -4.0612e-02,  4.7043e-02, -3.7591e-02,  1.8712e-02, -8.3181e-02,\n",
      "        -1.9715e-02,  3.6721e-02,  3.5419e-02, -4.6781e-02, -7.8367e-03,\n",
      "        -2.6748e-02, -8.6308e-02,  2.3989e-02, -1.2710e-02,  3.7118e-02,\n",
      "        -6.2088e-02, -2.2962e-04, -4.9640e-02,  2.4384e-02,  1.5691e-01,\n",
      "         1.5421e-02,  5.5528e-02,  4.8312e-02,  5.6640e-02, -2.2735e-02,\n",
      "         5.3113e-03, -5.2211e-02,  2.6325e-02,  6.9295e-02,  2.4738e-02,\n",
      "        -5.3518e-03,  5.2276e-02, -2.4634e-02, -5.3242e-03,  1.2084e-01,\n",
      "        -2.6133e-02,  3.3964e-02,  9.2582e-03, -1.2223e-01, -2.1360e-03,\n",
      "        -7.8244e-02, -1.5748e-02,  1.4439e-03,  1.2431e-01,  6.0634e-02,\n",
      "         8.5934e-02, -6.0989e-02, -2.9897e-02, -1.1970e-03, -1.0762e-01,\n",
      "         1.0423e-02,  1.6176e-02, -1.3812e-02, -5.2755e-02,  1.6920e-02,\n",
      "         6.1367e-02,  9.1813e-02,  2.1540e-02,  7.7856e-03, -4.0828e-02,\n",
      "        -9.7598e-02, -4.1089e-02,  9.0935e-02,  1.8519e-02, -3.4424e-02,\n",
      "         2.8530e-03, -6.6620e-02, -8.9594e-03, -6.7013e-03, -4.6130e-02,\n",
      "        -2.1535e-02,  5.8145e-03,  4.0000e-03, -5.7107e-02,  4.8855e-02,\n",
      "        -1.1148e-01, -1.1978e-01,  6.8131e-02,  1.5512e-03,  3.5912e-02,\n",
      "         3.3328e-02,  3.1726e-02, -8.8611e-02,  1.4725e-01, -9.5569e-02,\n",
      "        -1.0785e-02, -1.3891e-03,  1.3467e-02,  4.0348e-02,  9.6515e-02,\n",
      "         1.6649e-02,  3.0992e-02, -1.5092e-02, -5.3478e-02,  2.6478e-02,\n",
      "        -1.3042e-02, -9.5301e-02, -6.6575e-03, -1.5733e-03, -9.9895e-03,\n",
      "         3.4082e-02,  1.5740e-01, -9.9586e-03, -5.3744e-02,  8.7394e-02,\n",
      "         4.2685e-02,  5.2481e-02,  1.7623e-02,  1.0548e-03,  4.5100e-02,\n",
      "         7.4265e-02, -7.1658e-03, -8.7438e-02, -3.9754e-02,  5.4727e-02,\n",
      "         4.6412e-02,  4.2058e-02, -3.2855e-02, -1.1088e-01, -1.7722e-02,\n",
      "         4.9851e-03, -8.0476e-02,  8.2968e-02, -8.2024e-02,  1.6164e-02,\n",
      "         3.7377e-02, -9.2349e-02, -1.1127e-01,  6.9750e-02,  8.6820e-02,\n",
      "        -2.7057e-02, -2.3069e-02, -7.3103e-02, -1.6484e-01, -2.0014e-02,\n",
      "         6.3153e-03,  7.7782e-02, -8.4823e-02,  2.2121e-02,  1.0625e-01,\n",
      "        -1.4292e-01,  8.1527e-02, -7.1087e-02, -8.0429e-02, -4.0732e-03,\n",
      "         6.4006e-02, -1.4278e-01, -7.9276e-03,  5.2838e-02, -3.7510e-03,\n",
      "        -5.9070e-02, -1.1084e-01, -1.6297e-03,  5.6736e-03, -7.3166e-02,\n",
      "        -6.8036e-02,  1.5117e-01,  1.9150e-02, -9.3975e-02, -4.8127e-02,\n",
      "         4.4899e-02,  5.5049e-02,  6.3477e-02,  5.0466e-02,  1.4346e-01,\n",
      "        -1.4061e-02,  1.8790e-01,  3.4009e-02,  1.4160e-03, -2.5282e-02,\n",
      "        -1.6245e-02,  5.4068e-02, -7.5012e-02, -7.5148e-02, -1.8582e-02,\n",
      "        -2.3466e-02,  1.9578e-02, -6.2413e-02,  1.2314e-01,  1.3701e-02,\n",
      "        -5.7122e-03,  8.9041e-02,  3.7946e-02,  4.1243e-02,  4.7171e-02,\n",
      "         2.7039e-02, -5.9925e-03, -2.8245e-02, -7.2878e-02,  1.4521e-02,\n",
      "         9.9702e-02,  6.4296e-02,  7.4185e-02, -7.1993e-02,  1.4546e-02,\n",
      "         7.7495e-02, -9.2409e-03, -3.8808e-02,  7.1566e-02, -1.4977e-01,\n",
      "         4.2293e-02, -4.2540e-02, -5.6876e-03, -4.4148e-02, -8.0183e-02,\n",
      "         7.5278e-02, -2.9656e-03, -4.9337e-02,  2.6277e-02, -1.1994e-02,\n",
      "        -9.6900e-03, -8.8157e-03, -1.7625e-02, -8.9690e-02, -3.2884e-02,\n",
      "        -5.1021e-03, -1.0199e-01, -1.6831e-02,  1.1726e-01, -3.4447e-02,\n",
      "        -2.8511e-02, -1.9198e-02,  3.6576e-03,  3.2099e-02,  4.5579e-03,\n",
      "         8.7041e-02, -3.0138e-02,  1.8212e-02,  7.4119e-02, -1.3839e-02,\n",
      "         5.3415e-02,  2.2786e-02,  1.0557e-01, -5.6927e-02,  3.3285e-02,\n",
      "         7.3276e-02,  1.0244e-01, -1.4565e-02, -1.0259e-01,  1.2200e-01,\n",
      "         6.1812e-02,  4.8889e-02, -5.6486e-02,  5.1047e-02,  9.3909e-02,\n",
      "        -1.0201e-02,  6.4712e-02, -2.3649e-02,  3.8729e-02,  6.1245e-03,\n",
      "        -4.3430e-02,  6.4039e-03, -8.9212e-02,  1.5119e-01,  7.2071e-02,\n",
      "         1.5732e-02, -2.2774e-02,  5.2327e-02,  2.5401e-02,  2.9843e-02,\n",
      "        -1.1558e-01,  5.9937e-02, -5.8328e-02,  7.1370e-02,  4.9816e-02,\n",
      "         6.5657e-02,  3.2430e-02, -8.6861e-03,  8.5977e-02,  1.9082e-02,\n",
      "         2.7206e-02, -1.9106e-03, -6.5907e-02,  4.0442e-03,  1.7387e-02,\n",
      "         1.3066e-01, -8.5428e-02, -2.6442e-02,  5.6974e-02, -8.7909e-02,\n",
      "         3.4048e-02, -5.8666e-02,  1.8037e-02, -6.2223e-02, -1.8848e-02,\n",
      "         9.5296e-03, -5.1592e-03,  5.1242e-03,  9.5190e-02,  1.1389e-02,\n",
      "        -6.1644e-02,  2.7198e-02,  2.2262e-02, -4.7755e-02,  6.3539e-03,\n",
      "        -2.4203e-02,  1.3476e-02,  5.5816e-02,  3.3884e-02,  5.4144e-02,\n",
      "        -2.0123e-02, -2.5729e-02,  3.2092e-02, -3.4289e-02, -1.2439e-03,\n",
      "         1.8775e-01,  5.8437e-02,  1.8716e-02, -5.8857e-02, -6.8036e-02,\n",
      "        -5.9856e-04,  1.0747e-01, -7.1370e-02,  1.3296e-03, -3.0167e-02,\n",
      "        -5.6810e-02, -1.0447e-01, -8.7226e-03, -3.1270e-03,  1.2601e-02,\n",
      "         1.8155e-02, -9.4597e-02, -4.7340e-02,  2.7440e-02, -3.4883e-02,\n",
      "        -3.2968e-02, -6.2905e-02, -1.2657e-02,  3.2411e-02,  1.2026e-02,\n",
      "         2.2878e-02, -5.3231e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 6.6658e-02, -7.8302e-02, -3.9761e-02, -4.1793e-02,  4.5831e-02,\n",
      "         4.8306e-02, -6.7736e-03,  7.5574e-02, -7.4495e-02, -3.0042e-02,\n",
      "         5.2244e-03, -1.3071e-02, -5.5794e-03, -8.3971e-02, -6.9471e-03,\n",
      "        -2.4258e-02,  1.0854e-01, -6.1369e-02, -1.4674e-01,  1.1226e-01,\n",
      "        -6.0065e-02,  5.3451e-02,  1.1262e-01, -4.9005e-03,  1.5264e-01,\n",
      "         7.8240e-02,  3.1867e-02,  7.0535e-03, -8.8613e-02, -1.6180e-02,\n",
      "         7.1920e-03,  3.6067e-02, -1.8580e-02, -6.9305e-02,  5.7444e-02,\n",
      "        -9.3223e-02,  6.4325e-02, -1.2735e-01, -1.6280e-02, -5.1730e-02,\n",
      "        -1.6762e-02,  1.6986e-01,  2.8526e-02,  7.5887e-02,  4.1897e-03,\n",
      "         5.6685e-02,  4.6633e-02, -3.6862e-02, -3.9126e-02, -2.2331e-02,\n",
      "         9.3762e-02, -1.0613e-02,  1.1766e-01, -3.7826e-02,  6.4190e-02,\n",
      "         2.1247e-02, -9.1414e-03,  9.0567e-02, -1.1170e-01,  1.5015e-02,\n",
      "        -1.6912e-02,  1.8269e-02, -6.4949e-02, -5.4902e-02, -8.6944e-03,\n",
      "         1.3896e-01,  1.1010e-01,  1.0749e-02,  8.7195e-02, -6.8369e-03,\n",
      "        -3.5939e-02,  1.3870e-02,  5.9698e-02, -8.9737e-05,  8.3753e-02,\n",
      "        -4.8358e-03, -3.8847e-02, -1.0107e-01,  7.5683e-02, -1.1180e-01,\n",
      "         3.0140e-02, -4.3089e-02, -2.2418e-02, -3.6128e-02, -1.0527e-01,\n",
      "         2.2898e-02,  4.6009e-02, -7.4225e-03, -5.6874e-02,  8.5350e-02,\n",
      "         5.1923e-03,  2.5627e-02, -8.9285e-03, -5.8058e-02,  7.0525e-02,\n",
      "         3.8854e-02,  2.7697e-02,  1.4393e-01, -4.0282e-02,  2.0928e-02,\n",
      "        -2.4592e-02,  6.1504e-02,  8.4973e-02, -6.5030e-03, -1.1406e-02,\n",
      "        -1.5721e-01, -1.2213e-01, -3.2998e-02, -1.0606e-02,  1.5931e-01,\n",
      "         1.4261e-01,  2.5770e-02, -4.0473e-02, -6.6654e-02,  3.4934e-02,\n",
      "         9.9253e-02, -1.0173e-02, -1.4505e-02,  6.1864e-02,  4.7759e-02,\n",
      "        -1.6578e-02,  3.0713e-02,  1.4806e-02,  8.6155e-02, -1.2338e-02,\n",
      "         7.9021e-02, -7.8331e-02, -6.0098e-02,  7.8730e-02,  2.3303e-02,\n",
      "        -8.3858e-03,  4.4462e-02, -5.4935e-02,  4.2922e-02,  4.7366e-02,\n",
      "        -3.2290e-04,  1.8469e-02, -5.9237e-02,  6.0935e-02,  2.3421e-02,\n",
      "         7.0576e-02, -1.8194e-02,  5.7329e-03,  1.2694e-01, -1.6639e-02,\n",
      "         5.9829e-02, -7.5157e-02, -6.8489e-02, -1.1888e-01, -1.4575e-01,\n",
      "        -6.2740e-03,  8.6623e-02, -1.9370e-03, -1.2883e-01,  4.0742e-02,\n",
      "        -3.1368e-02, -6.8863e-03,  6.7565e-03, -5.5464e-02, -5.8365e-02,\n",
      "        -4.6925e-02, -1.8427e-03, -6.9821e-03, -5.4991e-02,  1.4936e-02,\n",
      "        -6.0094e-02,  2.1199e-02,  1.6101e-03, -6.6419e-02, -1.0129e-01,\n",
      "         3.2519e-04, -9.6969e-02,  2.2424e-02,  8.3956e-02, -1.0915e-01,\n",
      "        -5.2411e-02,  7.9012e-02,  7.7652e-02,  7.2692e-02,  5.3036e-02,\n",
      "         8.0605e-03,  1.2090e-01,  4.4321e-02, -1.3145e-02,  2.7608e-02,\n",
      "        -2.4626e-03, -8.6162e-02, -2.0906e-02, -8.0314e-02,  8.6478e-02,\n",
      "         3.2060e-02, -7.4949e-02, -4.5875e-02, -9.1144e-02,  8.5149e-02,\n",
      "         4.7841e-02, -5.8479e-02,  9.3823e-02, -8.9949e-02, -2.2137e-03,\n",
      "         5.3320e-02,  2.4241e-02,  7.6287e-02, -7.3501e-02,  5.9457e-02,\n",
      "         2.5991e-02, -4.9862e-02,  2.1058e-02,  3.7085e-02,  5.8227e-02,\n",
      "         1.6736e-02,  1.3518e-02, -3.6454e-02,  8.9511e-02, -6.0161e-02,\n",
      "         4.3647e-02,  2.5404e-02,  1.6810e-03, -3.8325e-02,  5.1655e-02,\n",
      "        -6.2435e-03, -7.4342e-02,  1.5280e-02, -3.8896e-02, -4.6945e-02,\n",
      "        -4.9156e-02,  5.0480e-02, -1.1144e-01,  4.6365e-02,  4.1312e-02,\n",
      "         4.3370e-02, -6.4439e-02,  1.4321e-01,  5.6491e-03,  4.6217e-02,\n",
      "        -7.8084e-02,  2.2043e-02,  2.4072e-02, -1.1090e-01, -5.7180e-02,\n",
      "         1.3553e-01,  2.0576e-03, -6.7463e-02, -3.7952e-02,  9.7044e-02,\n",
      "         3.9006e-02,  2.3112e-02,  3.6162e-02, -4.4879e-02, -5.0205e-02,\n",
      "        -6.6276e-02,  6.0393e-02, -1.6587e-02, -4.2223e-02,  4.9360e-02,\n",
      "        -5.2514e-02,  5.3070e-02,  3.0898e-02,  8.4096e-03,  4.2029e-02,\n",
      "         8.3128e-03,  7.7944e-02,  7.4944e-02,  3.7365e-02, -1.7412e-02,\n",
      "        -1.7034e-02, -5.1705e-02, -1.0178e-01,  8.1377e-03, -1.1124e-02,\n",
      "         6.0315e-02, -1.2464e-01, -8.2909e-02, -2.0721e-02,  1.5134e-01,\n",
      "        -7.6029e-03, -5.5703e-02,  1.3161e-01,  1.1009e-01,  8.7843e-02,\n",
      "        -1.1565e-02, -7.0188e-02, -1.7204e-01,  9.7961e-02,  1.4806e-01,\n",
      "        -4.5438e-02, -2.6664e-03, -4.6997e-02, -7.0638e-02, -7.9939e-02,\n",
      "        -7.0988e-02, -1.1400e-01, -7.8130e-03, -8.5862e-02, -3.9800e-02,\n",
      "         7.1482e-03, -1.3455e-01, -2.8474e-02, -8.3467e-02,  6.1789e-02,\n",
      "        -1.2440e-02, -1.4384e-01, -5.4934e-02,  1.7171e-02, -4.3710e-02,\n",
      "         5.2462e-03, -9.8457e-02,  6.4931e-02,  3.0336e-02, -8.2045e-03,\n",
      "        -2.1457e-02,  1.9863e-02, -3.9212e-02,  3.6250e-02, -2.9250e-02,\n",
      "         4.0146e-03,  9.8803e-02, -3.5044e-03, -1.3867e-01,  6.7823e-02,\n",
      "        -1.1386e-02,  4.5815e-02, -4.6995e-02, -6.0331e-02,  8.9048e-02,\n",
      "        -3.3910e-03,  5.5142e-02,  1.0962e-01,  7.8482e-02, -5.7451e-02,\n",
      "         6.7650e-02, -5.0193e-02, -1.0531e-01,  3.0873e-02,  4.0250e-02,\n",
      "         3.5226e-02,  3.5651e-02, -1.3163e-02, -1.5697e-02, -1.3301e-02,\n",
      "        -7.5622e-02,  4.6634e-02, -6.0863e-02,  1.1601e-02,  5.8555e-02,\n",
      "         1.9718e-02,  1.4490e-02,  4.6890e-02,  1.9770e-02,  1.8599e-02,\n",
      "         1.5324e-02,  9.0858e-02, -9.4841e-02,  4.4712e-02,  1.0196e-01,\n",
      "         7.1711e-02,  2.8857e-02, -7.6147e-02,  1.1056e-01,  3.8540e-02,\n",
      "        -7.5464e-02, -1.1109e-01,  1.1038e-02,  7.1191e-02,  3.8999e-02,\n",
      "         8.1577e-02,  1.4265e-01, -2.5305e-02,  7.0406e-02, -2.0950e-01,\n",
      "        -1.0905e-01, -7.9404e-02,  9.4908e-02, -6.2777e-02, -4.6448e-02,\n",
      "         6.7760e-02, -4.1111e-02, -3.0499e-02, -6.7737e-02, -1.6252e-02,\n",
      "         7.7219e-02, -9.5822e-02,  7.5935e-03, -2.3492e-02, -3.9966e-02,\n",
      "         2.2348e-02, -5.5910e-02, -2.2430e-02, -1.2789e-01,  1.1506e-02,\n",
      "        -3.6499e-02, -2.3789e-02,  8.8967e-02,  3.7748e-04,  1.4302e-01,\n",
      "        -3.3631e-02, -3.5510e-02, -1.5043e-01,  7.7718e-02,  1.4879e-01,\n",
      "         6.6394e-02, -1.8917e-02,  1.0423e-02, -4.4962e-03, -2.3098e-02,\n",
      "         8.4583e-02,  1.2187e-01,  2.5955e-02,  2.3483e-02, -1.2860e-01,\n",
      "         2.7167e-02,  3.6408e-02,  8.3306e-02,  1.1587e-01,  6.6651e-02,\n",
      "         5.9024e-02,  1.0206e-01, -6.6102e-02, -1.1416e-02,  6.7382e-02,\n",
      "        -1.8530e-01,  7.1940e-02, -3.7391e-02, -1.0281e-01,  5.0257e-02,\n",
      "         4.7398e-02,  2.7898e-02,  6.5546e-02, -3.5585e-02, -1.5329e-02,\n",
      "        -3.8707e-02, -5.4844e-02, -2.3227e-02,  3.0108e-02, -2.5781e-02,\n",
      "        -2.8408e-02,  3.9738e-03,  9.0303e-02,  8.2566e-03,  2.2979e-02,\n",
      "        -5.5796e-02, -3.8515e-02, -6.0057e-02,  7.1408e-02, -6.8506e-02,\n",
      "        -8.3587e-02, -1.1510e-01,  3.3540e-02, -1.6315e-02, -4.7617e-02,\n",
      "        -1.2741e-01, -2.6345e-02, -6.0932e-02, -2.5297e-02,  1.7280e-03,\n",
      "        -5.4365e-02, -5.7350e-02, -4.4366e-02, -1.8187e-02, -5.9762e-02,\n",
      "         1.8093e-02, -6.1407e-02,  1.3368e-01,  3.7309e-02, -2.3302e-02,\n",
      "        -3.6866e-02,  6.9024e-03,  7.7365e-03,  4.0508e-02, -2.5169e-02,\n",
      "        -8.2504e-02,  1.2014e-01, -6.4195e-02,  6.6726e-02,  1.5957e-02,\n",
      "         1.0247e-01,  9.6323e-02,  5.0310e-02, -7.1386e-02, -6.2054e-03,\n",
      "        -1.6760e-01,  3.7466e-03, -9.4249e-02,  7.7653e-02, -1.2555e-01,\n",
      "        -6.1608e-02, -2.9333e-02,  1.3478e-02, -1.4650e-02, -9.3798e-02,\n",
      "         6.4758e-02,  2.1284e-02,  1.5329e-01, -8.6474e-02, -5.4156e-03,\n",
      "        -2.4129e-02,  1.0983e-01, -2.6136e-02,  1.7877e-02,  7.2377e-02,\n",
      "         2.4865e-02,  5.1694e-02,  5.9210e-02,  1.3274e-01, -4.0805e-02,\n",
      "         2.4143e-02,  6.7355e-02,  6.0903e-02,  6.5552e-02,  1.7681e-01,\n",
      "         4.1771e-02,  1.2728e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 5.0966e-02, -1.4745e-01,  7.7494e-02,  1.4598e-02,  1.1066e-01,\n",
      "        -3.6061e-02, -3.4456e-02,  2.3449e-03,  3.6120e-02, -2.1529e-02,\n",
      "         1.0209e-01,  1.2287e-03, -5.0131e-02,  6.2569e-02, -2.0442e-02,\n",
      "         3.2035e-02,  6.1605e-02, -9.9639e-02,  1.5433e-02,  3.8132e-02,\n",
      "        -6.6866e-02, -6.3091e-02, -6.1747e-02,  6.8062e-02,  8.8035e-02,\n",
      "        -1.0674e-01,  5.1352e-02, -3.5963e-02, -4.7417e-03, -4.0600e-03,\n",
      "        -1.0709e-01, -8.8151e-02,  1.0923e-01, -5.1789e-02, -1.1943e-01,\n",
      "        -3.2427e-02,  8.7168e-02,  1.1600e-01, -3.1433e-02,  2.1007e-02,\n",
      "        -2.0211e-02,  5.1138e-02, -3.1195e-02, -1.7929e-02,  1.6682e-02,\n",
      "        -5.8549e-03, -3.0055e-02, -1.2022e-01,  4.2940e-02,  5.0219e-03,\n",
      "        -7.6352e-02,  1.2055e-02,  1.1379e-02,  7.7296e-02, -3.7195e-02,\n",
      "         6.2380e-02, -9.9886e-02,  1.3775e-02, -3.7782e-02, -8.0343e-03,\n",
      "         1.1148e-02, -1.7144e-02, -8.2952e-02,  6.2111e-02,  1.4023e-02,\n",
      "         9.3064e-02, -1.8222e-02,  8.8978e-02, -9.5613e-02,  5.1005e-02,\n",
      "         6.4407e-02, -1.5327e-02, -1.6592e-02, -4.5361e-02, -3.1602e-02,\n",
      "        -4.6708e-02, -4.0381e-02,  9.3572e-02,  1.4583e-02,  1.5900e-02,\n",
      "         5.2908e-02, -6.2023e-02,  9.5726e-02, -2.2317e-02, -1.0207e-02,\n",
      "        -8.4064e-02, -8.5376e-02,  1.4583e-02,  6.5636e-02,  8.2487e-02,\n",
      "         6.9251e-02, -3.3851e-03,  2.0579e-02, -6.4329e-03, -6.3405e-03,\n",
      "         2.8375e-02, -5.4557e-02,  4.9721e-02, -2.8327e-02,  7.1326e-02,\n",
      "        -2.7338e-02,  7.1745e-02,  2.0902e-02, -1.4693e-02, -6.4021e-03,\n",
      "        -3.6755e-02,  2.3320e-02, -1.8848e-02, -8.2152e-03, -7.3774e-02,\n",
      "        -6.4569e-02, -3.3738e-02,  2.3054e-02, -1.0855e-02,  3.3617e-02,\n",
      "         5.3611e-02, -6.7952e-02, -5.8561e-02, -4.5781e-02,  2.4040e-02,\n",
      "        -8.8937e-02,  3.5465e-02,  5.0535e-02,  2.5044e-02, -4.3513e-03,\n",
      "        -3.2971e-02, -1.3832e-01, -8.0301e-02,  1.5525e-01, -8.0106e-02,\n",
      "         2.0949e-02,  1.1226e-02,  5.7637e-02,  9.5634e-02, -4.6271e-02,\n",
      "         6.2753e-02, -4.8439e-02,  5.5866e-02, -5.6149e-02,  8.9882e-03,\n",
      "        -2.2475e-02,  2.6102e-03, -7.5365e-02, -3.5781e-02,  8.7820e-03,\n",
      "        -2.7019e-02,  5.6331e-02,  1.6614e-03, -3.3956e-02, -6.9785e-02,\n",
      "         1.1633e-01,  5.9738e-02, -8.4658e-02,  3.5563e-02,  1.0341e-01,\n",
      "         7.0607e-05, -4.0593e-02,  3.8467e-02,  1.0799e-01,  1.7658e-02,\n",
      "        -9.0117e-02, -9.2431e-02, -7.4624e-02,  3.1521e-02,  4.0765e-02,\n",
      "        -1.2515e-01,  3.0535e-02,  1.1851e-02, -4.0310e-02,  2.2916e-02,\n",
      "         1.2250e-01,  6.9152e-02, -6.2053e-03,  4.0321e-02,  1.6208e-02,\n",
      "        -6.8822e-02,  2.1849e-02, -3.6987e-02, -4.4603e-02, -1.5947e-01,\n",
      "        -1.6658e-02, -9.6214e-02, -3.7753e-02,  5.4041e-02, -1.7003e-02,\n",
      "         8.1025e-02,  2.4926e-02,  5.5767e-02, -7.9529e-02, -2.1234e-01,\n",
      "        -4.7282e-02, -5.5761e-02,  3.0091e-02,  1.4731e-01, -6.2581e-02,\n",
      "         2.2454e-02, -6.7485e-02,  1.5281e-01,  4.6557e-02,  8.2848e-02,\n",
      "        -9.2783e-03,  7.2040e-02, -9.9636e-02,  6.1564e-02, -5.9368e-02,\n",
      "        -1.9590e-02, -1.0435e-02, -4.1890e-02, -4.7181e-02, -1.2446e-02,\n",
      "        -4.0818e-02,  6.1132e-02, -8.5487e-03,  8.7448e-02,  2.1625e-02,\n",
      "        -1.7572e-02, -9.9109e-02,  3.0057e-02,  7.2901e-02, -1.2618e-02,\n",
      "         3.7349e-02, -2.1917e-02, -6.9758e-02, -1.2695e-03, -1.3122e-02,\n",
      "        -5.0221e-02,  2.3869e-02,  5.0954e-02,  7.0282e-04, -3.3970e-02,\n",
      "        -2.8963e-02, -8.4868e-02, -2.6569e-02, -6.5083e-02,  8.5820e-03,\n",
      "        -4.4336e-03,  5.8201e-03,  2.1587e-02,  7.3191e-03,  4.7043e-03,\n",
      "        -5.8309e-02,  2.1552e-02, -2.5648e-02, -2.2331e-02, -1.0112e-01,\n",
      "        -3.7041e-02, -4.1032e-02, -6.8042e-02,  1.7894e-02, -2.6997e-02,\n",
      "        -2.7584e-02,  1.7612e-02, -1.9444e-03,  5.9923e-02,  6.8182e-02,\n",
      "         2.6522e-02, -6.7600e-02,  3.6002e-02, -1.6933e-02,  9.7652e-03,\n",
      "        -1.0266e-01, -3.6495e-03,  1.1981e-01, -3.1746e-02, -2.1659e-02,\n",
      "        -4.1714e-02,  7.0952e-02, -8.4005e-02,  3.2536e-03, -2.2566e-02,\n",
      "        -3.9273e-02,  3.3117e-03, -8.4515e-02,  5.7761e-02,  9.1372e-02,\n",
      "         9.6171e-03, -1.2380e-01, -8.3872e-04, -1.1604e-02, -2.1467e-02,\n",
      "         3.9992e-02,  8.3243e-04, -5.9930e-03, -2.2868e-02,  2.3452e-02,\n",
      "         1.2934e-02,  1.4610e-01,  6.3666e-04, -4.7834e-02, -1.6290e-02,\n",
      "         6.7797e-02,  3.1905e-02, -6.1453e-02,  4.7708e-02,  4.9836e-02,\n",
      "        -3.2332e-02,  1.4693e-02, -8.0379e-02,  5.6533e-02,  6.9687e-02,\n",
      "         6.2967e-02, -3.5479e-02, -9.2222e-03, -6.3729e-03,  8.0024e-02,\n",
      "         1.0684e-02,  5.5488e-02, -5.7777e-03,  1.2793e-01,  2.4388e-02,\n",
      "         6.8428e-02, -2.1748e-03, -4.4633e-02,  1.3514e-02,  2.4887e-03,\n",
      "        -1.9060e-02, -1.2467e-01, -4.7357e-02, -4.9894e-02,  9.8269e-02,\n",
      "        -6.8453e-03,  3.6830e-02, -3.3399e-02, -4.3410e-02, -9.6036e-02,\n",
      "         8.1545e-02, -3.5613e-02,  6.0910e-02, -5.0575e-02,  6.5858e-03,\n",
      "         5.8657e-02,  2.9649e-02, -5.0301e-02, -1.8220e-02, -7.9198e-02,\n",
      "         4.7839e-02,  3.2613e-02, -9.3417e-02,  6.7337e-02, -8.7942e-03,\n",
      "        -1.6459e-02,  2.7349e-02, -4.9454e-02,  6.1516e-02,  6.7670e-02,\n",
      "         4.5408e-03,  3.2664e-02,  3.3849e-02, -8.3817e-03,  2.9799e-02,\n",
      "        -6.4481e-02,  6.9932e-02,  1.3802e-02, -7.4295e-02,  2.8266e-03,\n",
      "         1.3482e-01,  1.6569e-02, -4.2818e-02,  5.2147e-02,  4.8331e-02,\n",
      "        -2.2739e-02, -1.8746e-02,  2.8624e-02, -8.2209e-02, -4.9650e-02,\n",
      "        -2.9904e-02, -3.1530e-02, -4.7788e-02, -4.7805e-02,  4.2077e-02,\n",
      "        -5.1374e-03,  9.3389e-02,  7.7671e-02, -1.0206e-02, -5.3528e-02,\n",
      "        -6.0535e-03,  2.0553e-02,  2.7381e-02,  8.1292e-03, -6.6471e-02,\n",
      "        -1.9595e-02,  2.1768e-02,  4.5958e-02,  5.7396e-02,  1.7548e-02,\n",
      "        -6.3863e-03, -1.7971e-01,  2.8201e-02,  1.6888e-02, -6.0088e-02,\n",
      "        -4.4732e-02,  5.1204e-04,  5.4047e-02,  1.5042e-02,  8.6862e-02,\n",
      "        -5.6149e-02, -8.0252e-02, -1.7712e-02, -3.3251e-02,  6.7082e-02,\n",
      "         5.7277e-02,  7.4467e-02,  1.3210e-02,  8.0749e-02, -4.9230e-02,\n",
      "         4.0126e-02,  6.4328e-02,  3.2686e-02,  5.5669e-02, -4.5429e-02,\n",
      "        -6.0456e-02,  5.9471e-03, -7.2037e-03, -6.6578e-02,  6.4264e-02,\n",
      "        -3.4567e-02,  1.8057e-01,  9.6095e-02,  1.7282e-02, -5.5573e-03,\n",
      "        -1.5813e-02,  7.3891e-02, -9.6589e-03, -5.6928e-02,  3.5197e-02,\n",
      "        -3.6848e-02,  3.3619e-02, -7.9201e-02, -1.0853e-03, -6.1366e-02,\n",
      "        -4.6373e-02, -2.3210e-02,  2.4530e-02, -2.9117e-02, -2.6862e-02,\n",
      "         2.0443e-02, -1.0311e-02, -4.5818e-02,  3.2928e-02, -1.4177e-01,\n",
      "        -3.3394e-02, -8.0657e-02, -1.1610e-01,  2.7471e-03, -1.1582e-02,\n",
      "         1.8751e-03, -3.5150e-02,  9.0628e-02, -1.1234e-02, -6.3072e-03,\n",
      "        -2.9522e-03, -2.5991e-02,  7.4267e-02,  5.3881e-02, -4.0242e-03,\n",
      "         7.6560e-03,  8.1244e-02, -1.5535e-02, -7.0901e-02,  4.0996e-03,\n",
      "        -1.9212e-02,  1.5392e-02, -4.2169e-02,  1.7310e-02, -7.4863e-02,\n",
      "        -5.8399e-02, -4.7026e-02,  1.1410e-01, -1.0140e-01, -9.5707e-02,\n",
      "         2.0097e-02, -1.0625e-01,  6.2864e-02, -1.0046e-01,  4.0808e-02,\n",
      "        -5.9520e-02, -5.2804e-02,  1.8317e-02, -1.1327e-01, -1.7123e-02,\n",
      "        -2.9642e-03, -1.2108e-02,  4.3250e-02, -6.8001e-02,  2.8993e-02,\n",
      "         2.3379e-03,  6.4308e-03, -5.0257e-02, -2.6099e-02, -9.2139e-03,\n",
      "         1.4326e-01, -3.5042e-02, -5.5747e-03,  1.4443e-01,  6.4646e-02,\n",
      "        -3.6846e-02, -3.1642e-02,  1.8773e-04, -6.0860e-02,  7.3784e-02,\n",
      "         3.4365e-02, -5.6993e-02,  4.9817e-02, -4.8040e-02,  7.2079e-02,\n",
      "         6.0582e-02,  1.5344e-03, -6.8195e-02,  2.4479e-02, -6.7752e-02,\n",
      "        -7.2611e-02, -2.7682e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0768, -0.0110,  0.0261, -0.0717,  0.0138, -0.0634, -0.0912,  0.0113,\n",
      "        -0.0347, -0.0304, -0.0077, -0.0341, -0.0804, -0.0470, -0.0264,  0.0091,\n",
      "         0.0322,  0.0482, -0.0405, -0.0913,  0.0352, -0.0308,  0.0159,  0.0034,\n",
      "         0.0155, -0.0147,  0.0697,  0.0984,  0.0066,  0.0651, -0.1385, -0.0525,\n",
      "        -0.0866,  0.0596, -0.0648,  0.0693,  0.0717,  0.0327, -0.0749,  0.1113,\n",
      "         0.0407,  0.0465,  0.1108,  0.0816, -0.0240,  0.0117,  0.0365, -0.0328,\n",
      "         0.0209, -0.0589,  0.0395, -0.0040,  0.0484,  0.0579,  0.0430,  0.0961,\n",
      "         0.0019, -0.0478, -0.0156,  0.0328, -0.0624,  0.0715,  0.0612, -0.0883,\n",
      "         0.0393, -0.0688, -0.0231, -0.0230, -0.0219,  0.0156, -0.0243, -0.1010,\n",
      "        -0.0313,  0.0016, -0.0020, -0.0170, -0.0236, -0.0161, -0.0517, -0.0867,\n",
      "        -0.0712, -0.0125, -0.0954, -0.0109,  0.1592,  0.0375, -0.0574,  0.0412,\n",
      "        -0.0757,  0.1175,  0.0951, -0.0161, -0.0222, -0.1225,  0.0901,  0.0392,\n",
      "        -0.0461, -0.0242,  0.0155, -0.0975, -0.0425, -0.0112,  0.0040,  0.0077,\n",
      "         0.0669, -0.0678, -0.0185, -0.0830, -0.0124,  0.0362, -0.0285,  0.1085,\n",
      "        -0.0133,  0.0715, -0.0329, -0.0025,  0.0326, -0.0271,  0.0487, -0.0552,\n",
      "        -0.0141,  0.0521, -0.0023, -0.0375, -0.1438,  0.0137,  0.0634, -0.0483,\n",
      "        -0.0128,  0.0103,  0.0111,  0.0511,  0.1563,  0.0164,  0.0060, -0.1368,\n",
      "        -0.1142, -0.0285, -0.0205,  0.0208,  0.0782,  0.0446,  0.0960, -0.0340,\n",
      "        -0.0171,  0.0837,  0.1210,  0.0210, -0.0156, -0.0047,  0.0567,  0.1111,\n",
      "        -0.0234, -0.0498, -0.0705, -0.0082,  0.1107,  0.0074,  0.0705, -0.0538,\n",
      "         0.0613, -0.1379,  0.0155, -0.0276,  0.0236, -0.0070, -0.0942, -0.0741,\n",
      "         0.0344,  0.0320, -0.0537, -0.1111, -0.0324,  0.1613,  0.0198,  0.1086,\n",
      "        -0.0317,  0.0004, -0.0473,  0.0628,  0.0596, -0.0103, -0.0568,  0.0624,\n",
      "        -0.0776, -0.1148, -0.0166,  0.0027,  0.0078, -0.0937, -0.0514, -0.0138,\n",
      "        -0.1482, -0.0669, -0.0712,  0.0135,  0.1173, -0.0033, -0.0064, -0.0263,\n",
      "        -0.0567,  0.0106,  0.0777, -0.0619, -0.0526,  0.0932, -0.0841, -0.0340,\n",
      "        -0.1270,  0.0130,  0.0067, -0.0860,  0.1337, -0.0305, -0.0314, -0.0653,\n",
      "         0.1493, -0.0126, -0.0196, -0.0949, -0.0565,  0.0440, -0.0889,  0.0118,\n",
      "        -0.0558, -0.0214, -0.0157, -0.0387, -0.0158,  0.0084, -0.0396, -0.0521,\n",
      "        -0.0809,  0.0183,  0.0045,  0.0053, -0.0093, -0.0678, -0.1156,  0.0174,\n",
      "         0.1187,  0.0416,  0.0693, -0.0025,  0.0486,  0.0294, -0.0075, -0.0575,\n",
      "         0.1809,  0.0164,  0.0446, -0.0271, -0.0230,  0.0786, -0.0114, -0.0058,\n",
      "         0.0358, -0.0731, -0.0365, -0.0286,  0.1120, -0.0882,  0.0127,  0.0710,\n",
      "         0.0003,  0.0062, -0.0400,  0.0463,  0.0816,  0.0720,  0.0084,  0.0478,\n",
      "         0.0634,  0.0475,  0.0025, -0.0680, -0.0101,  0.0497,  0.0274,  0.0548,\n",
      "         0.0372, -0.0325,  0.1441,  0.0648,  0.0218,  0.0187,  0.0017,  0.0058,\n",
      "         0.0606,  0.0349, -0.0842, -0.0129,  0.1517, -0.0832, -0.0344,  0.0722,\n",
      "         0.0201, -0.0085,  0.0686, -0.0399, -0.1319,  0.0208, -0.0094, -0.0035,\n",
      "         0.0502,  0.0415,  0.0268,  0.0031, -0.0782, -0.0470,  0.0647, -0.0245,\n",
      "        -0.0220,  0.0053, -0.0115,  0.0109,  0.0431,  0.0079, -0.0562, -0.0070,\n",
      "         0.0463, -0.0588,  0.0339,  0.0052, -0.0210,  0.1090,  0.0647, -0.0540,\n",
      "         0.0085,  0.0879, -0.0313,  0.0073,  0.0437,  0.0494,  0.0060,  0.1026,\n",
      "         0.0076,  0.0393, -0.0335, -0.0069, -0.1043,  0.0803, -0.0891,  0.1589,\n",
      "        -0.0709, -0.0418, -0.0459, -0.0026,  0.1630, -0.0228,  0.0362,  0.0665,\n",
      "         0.0199,  0.0311, -0.0793,  0.0584, -0.0846, -0.0298,  0.0471,  0.1816,\n",
      "         0.1290, -0.0308, -0.0354,  0.0684,  0.0022,  0.1397,  0.1273, -0.0121,\n",
      "        -0.0255,  0.1549, -0.1043,  0.0030, -0.0070, -0.0533, -0.1327, -0.0505,\n",
      "        -0.0394, -0.0871, -0.1559, -0.1013, -0.0389,  0.0533, -0.0024,  0.0499,\n",
      "         0.0578, -0.0086, -0.0890, -0.0100,  0.0792, -0.0145, -0.0229, -0.0173,\n",
      "        -0.0718,  0.0246, -0.0108, -0.0746, -0.1079, -0.1119, -0.0225,  0.0620,\n",
      "        -0.0441,  0.0702,  0.1055, -0.0187,  0.0807,  0.0159,  0.0401,  0.0435,\n",
      "        -0.0720, -0.1575, -0.0476, -0.0490, -0.0268,  0.1036,  0.0390,  0.0015,\n",
      "        -0.1407, -0.0818, -0.0521, -0.0193,  0.0634,  0.0762, -0.0572,  0.0335,\n",
      "        -0.0147,  0.0902, -0.0812,  0.0083, -0.1243, -0.0758,  0.1391,  0.0418,\n",
      "         0.0337, -0.0012,  0.0702, -0.0611,  0.0674,  0.0109,  0.0365, -0.0833,\n",
      "        -0.0679, -0.0756,  0.0385, -0.0285,  0.0510, -0.0359,  0.0606,  0.0541,\n",
      "         0.0934, -0.0538, -0.0293,  0.0203, -0.0051,  0.1183, -0.0098,  0.0472,\n",
      "         0.0742, -0.0267, -0.0643, -0.0058,  0.0205,  0.0397, -0.0012,  0.0355,\n",
      "         0.0729,  0.0082,  0.0999,  0.0031,  0.0537,  0.0390,  0.0033,  0.0092,\n",
      "         0.0299, -0.0649,  0.0372,  0.0805,  0.0463, -0.0983, -0.0180, -0.0175,\n",
      "         0.0584, -0.0766,  0.0062, -0.0004,  0.0233, -0.0832,  0.0306,  0.0634,\n",
      "         0.0414, -0.0457,  0.0292, -0.0461,  0.0299,  0.0362,  0.0514,  0.0055,\n",
      "        -0.0551, -0.0026, -0.0381, -0.0229, -0.0396, -0.0021,  0.1161, -0.0633,\n",
      "         0.0352, -0.0886,  0.1244, -0.0195,  0.0971,  0.0900, -0.1717, -0.0553],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-2.8312e-02,  4.9911e-02,  9.7769e-03, -1.7147e-02,  4.0901e-02,\n",
      "        -1.2317e-01, -1.1881e-01,  8.5501e-02,  1.1018e-01,  6.2696e-02,\n",
      "         3.1070e-02, -1.0946e-01,  7.7663e-02,  6.7539e-02, -1.3375e-04,\n",
      "        -1.2912e-02,  5.7624e-02, -7.1261e-02,  9.6846e-04, -4.5915e-03,\n",
      "         6.0058e-02,  2.9872e-02,  4.2197e-02,  3.8850e-02,  5.4885e-02,\n",
      "         4.4528e-02, -8.8942e-02,  1.1722e-01, -4.4009e-02,  3.8589e-02,\n",
      "        -7.9293e-02, -1.1473e-02, -2.3653e-02, -4.3948e-02, -2.1827e-02,\n",
      "        -4.3308e-04,  8.2051e-02,  6.2999e-02,  3.0414e-02,  1.3454e-02,\n",
      "         5.9846e-03,  1.5785e-02, -6.2734e-02,  7.9752e-02, -1.4402e-01,\n",
      "        -5.4157e-02,  8.3404e-02, -5.4182e-02, -3.7938e-02,  1.9626e-03,\n",
      "         6.2376e-02, -9.8665e-02,  1.1238e-01,  8.4942e-02, -5.1376e-02,\n",
      "        -4.4197e-03,  1.0537e-02,  7.6728e-02,  7.0679e-02,  7.5002e-02,\n",
      "         2.3206e-02,  2.2686e-02,  3.7321e-02,  3.3898e-02, -2.2739e-02,\n",
      "        -1.1890e-01,  7.7856e-02,  1.0845e-01,  6.1648e-02, -2.4917e-02,\n",
      "        -5.6272e-02, -2.0143e-04, -6.7984e-02, -5.5723e-02,  1.5601e-03,\n",
      "         9.5723e-02, -1.2334e-01,  2.3138e-02,  1.5915e-03,  1.7391e-02,\n",
      "         1.0060e-03, -5.5752e-02, -7.3283e-03,  7.8786e-02, -8.5108e-02,\n",
      "         5.5049e-02,  1.5016e-01, -3.1859e-02,  4.4934e-03, -5.7109e-02,\n",
      "         8.0624e-03,  1.0309e-01, -3.0260e-03, -1.8075e-02,  1.0297e-01,\n",
      "         1.8190e-02,  8.1257e-02, -1.0586e-01,  4.6859e-02,  8.7545e-03,\n",
      "        -1.8347e-02,  7.8826e-04,  3.4076e-02,  3.4202e-02, -4.6036e-02,\n",
      "         7.8401e-02,  1.2534e-02, -2.9604e-02, -1.4013e-01, -1.2220e-01,\n",
      "        -3.9575e-02,  4.2375e-02,  6.8481e-02, -1.1031e-01,  1.7292e-03,\n",
      "         5.6505e-03, -1.3347e-01,  5.8967e-02,  1.0500e-01,  2.8959e-02,\n",
      "        -1.3579e-01, -3.6767e-02, -6.5603e-03,  5.9650e-02,  3.4714e-02,\n",
      "         3.4603e-02,  6.3472e-02,  8.8572e-02, -3.0379e-02,  1.2246e-02,\n",
      "         3.0892e-02, -1.9900e-02, -2.0532e-02, -9.3364e-02,  2.0879e-02,\n",
      "        -3.1082e-02,  7.4723e-02,  3.4827e-02,  9.9355e-03,  4.0432e-02,\n",
      "         9.0674e-02, -6.2378e-02, -1.7440e-02,  1.5880e-02, -1.3521e-02,\n",
      "         6.1648e-02, -2.5270e-02, -1.0506e-02,  1.8069e-02, -5.2453e-02,\n",
      "         1.3252e-02,  6.9504e-03, -5.8516e-02,  4.6623e-02,  1.4739e-02,\n",
      "         6.7765e-03,  3.7023e-03,  3.7319e-02,  1.9224e-02,  2.6738e-02,\n",
      "         8.2818e-02, -1.2007e-04,  7.7645e-02,  9.2141e-03,  4.3738e-03,\n",
      "        -1.0779e-01,  8.4956e-02,  3.7886e-02, -1.3384e-01, -1.1208e-01,\n",
      "        -5.7828e-02, -9.7238e-02,  1.0206e-02,  6.5645e-03, -2.8718e-02,\n",
      "         1.5325e-02,  6.6613e-02,  2.6445e-02, -2.4962e-02, -4.9788e-02,\n",
      "        -4.3545e-03, -4.5150e-02, -1.4951e-02,  6.1688e-02, -9.0608e-03,\n",
      "        -8.5805e-02, -1.0172e-01, -9.2241e-02, -1.5714e-03, -2.6098e-02,\n",
      "        -2.3720e-02, -4.2816e-03, -4.2465e-02,  4.0990e-03,  5.9952e-02,\n",
      "        -8.0171e-02,  3.4743e-02, -5.9418e-02, -5.0707e-04, -1.7003e-02,\n",
      "        -3.6289e-02,  9.0298e-02, -2.5486e-02,  2.2962e-02,  8.9927e-03,\n",
      "         3.8505e-02,  5.5345e-02, -2.0447e-02, -3.3111e-02,  3.7436e-02,\n",
      "         6.5773e-02, -4.5183e-02,  4.1996e-02, -8.7999e-02, -1.1769e-02,\n",
      "        -4.3234e-02, -6.6346e-02, -3.5659e-02, -5.7530e-03,  3.8261e-02,\n",
      "         6.5813e-02, -2.6030e-02, -7.3186e-03, -6.0748e-02, -5.1565e-02,\n",
      "        -2.2371e-02,  1.2256e-02,  7.5072e-02,  1.9970e-02,  2.4642e-02,\n",
      "        -7.0200e-02,  3.6686e-02,  2.4515e-02,  3.2946e-03,  6.7995e-03,\n",
      "         8.7247e-02, -6.1754e-02,  2.3224e-02,  4.8788e-02, -3.7919e-02,\n",
      "        -4.5916e-02, -6.3038e-03, -6.4867e-02,  9.7451e-03, -2.9809e-02,\n",
      "         1.9220e-02,  4.9873e-02, -8.4751e-02, -3.8756e-02,  2.4613e-03,\n",
      "         1.2979e-02, -1.9546e-02, -1.7456e-03,  6.0348e-02,  3.5478e-02,\n",
      "         8.5359e-02,  4.5793e-02, -2.9652e-02, -1.9533e-02,  2.8801e-02,\n",
      "         2.0128e-02, -1.6773e-02, -2.2567e-02,  8.6599e-02,  7.6258e-02,\n",
      "        -1.3919e-02, -5.2701e-03,  1.5254e-02, -5.6596e-03,  1.2512e-02,\n",
      "        -1.1107e-01, -3.9220e-02, -4.3274e-02, -1.4759e-02,  6.3456e-02,\n",
      "        -3.9313e-02,  6.6304e-02, -2.5031e-02, -8.0906e-02, -9.2574e-02,\n",
      "         7.7114e-03, -3.8525e-02,  2.6354e-02,  6.7656e-02, -3.6397e-02,\n",
      "        -6.6598e-02,  4.9100e-02, -4.5302e-02, -9.6687e-02,  3.2252e-03,\n",
      "        -1.6827e-02,  9.3235e-02, -2.9695e-02,  8.8593e-02,  1.0684e-01,\n",
      "         1.0159e-01,  7.8147e-02, -2.3984e-02,  7.4527e-02,  9.7435e-02,\n",
      "         9.9969e-02,  4.1802e-02,  5.5769e-02,  4.1883e-02,  3.7363e-02,\n",
      "        -1.2641e-02,  3.1162e-02, -5.7425e-04,  5.6984e-02,  2.1873e-03,\n",
      "         3.2089e-02, -7.0392e-02,  2.0635e-02,  9.4762e-03, -1.5822e-02,\n",
      "         5.4450e-02, -2.8916e-02,  1.6877e-02, -7.8206e-03, -1.1922e-01,\n",
      "         2.3058e-02,  6.5806e-02,  9.5983e-03,  4.4597e-02,  1.8453e-02,\n",
      "         4.3058e-02,  6.1493e-02, -6.8039e-02, -3.5424e-02, -3.8730e-02,\n",
      "        -4.6403e-02,  2.2619e-03,  1.3438e-02,  3.6322e-02, -9.0361e-02,\n",
      "         2.3885e-02, -6.8223e-02, -2.8933e-02,  1.0164e-01,  1.5505e-02,\n",
      "        -7.0034e-02,  7.1678e-02, -6.8170e-02,  4.8597e-02,  8.5489e-02,\n",
      "         3.4030e-02, -1.1827e-02,  4.7249e-02, -5.7491e-02,  6.4812e-02,\n",
      "        -3.8081e-02,  3.1269e-02,  4.8112e-02, -2.2889e-02, -1.2078e-01,\n",
      "         8.6875e-03,  2.7524e-03, -5.2020e-02, -1.3657e-02, -3.4252e-02,\n",
      "         1.2507e-01,  6.4650e-02, -4.3744e-02,  2.1554e-02,  7.2027e-02,\n",
      "         4.6084e-02,  1.0100e-01,  7.4042e-02, -5.4211e-02, -1.1455e-01,\n",
      "         5.7521e-02, -4.2710e-02, -7.8814e-02, -1.8124e-02,  4.4737e-02,\n",
      "        -5.1269e-02, -6.7855e-02, -8.3722e-02, -6.4286e-02,  3.4506e-02,\n",
      "         8.8117e-02,  4.1227e-02, -1.0366e-01, -5.4640e-02, -3.3339e-03,\n",
      "         1.3867e-01, -5.8631e-02,  1.0841e-02, -9.4331e-02,  1.0992e-01,\n",
      "        -1.8052e-02,  5.6607e-02, -3.0553e-03, -9.7665e-02,  3.6189e-03,\n",
      "         3.8424e-02, -2.0226e-02, -1.0399e-01,  7.1986e-02, -8.7396e-02,\n",
      "        -2.1321e-02, -3.3681e-02, -4.8806e-02, -9.9724e-03,  3.4821e-02,\n",
      "        -3.6701e-02, -1.0064e-01, -4.4952e-02, -2.9649e-02,  6.7568e-02,\n",
      "         1.0062e-01,  1.5413e-02, -5.2982e-03, -8.1491e-02,  6.9497e-02,\n",
      "         7.5970e-03,  2.6650e-02, -7.8061e-02,  8.9628e-02,  5.9069e-02,\n",
      "        -2.8076e-03,  2.2840e-02,  4.9031e-02, -3.0829e-02, -1.4460e-01,\n",
      "         2.0347e-02,  3.0446e-02,  4.5471e-02,  8.5173e-02, -1.1764e-02,\n",
      "        -1.9823e-02, -1.1526e-02, -1.4037e-02, -5.7210e-03,  3.2612e-02,\n",
      "         8.8098e-02,  2.5476e-02,  5.3235e-02,  9.3301e-02,  6.9620e-02,\n",
      "        -6.3628e-02,  6.8000e-02,  1.4908e-01, -5.6959e-02,  5.9116e-02,\n",
      "         2.2112e-02, -2.4973e-02, -2.7610e-02,  4.1903e-02, -2.0115e-02,\n",
      "         5.7806e-02,  1.3158e-03, -8.3065e-02,  4.6314e-02, -9.3857e-02,\n",
      "        -9.9200e-03,  4.4497e-02, -1.1722e-02, -6.1344e-02, -1.3309e-01,\n",
      "         4.0768e-02, -2.1628e-02, -5.0834e-02,  1.0866e-01,  1.6634e-02,\n",
      "         7.5386e-02,  1.1037e-01, -3.8678e-02,  5.1629e-02,  3.5886e-02,\n",
      "         3.2558e-02,  1.4227e-03,  5.5960e-02,  1.0197e-03, -5.6617e-02,\n",
      "         2.2816e-02, -1.3664e-01,  1.3298e-01, -3.5689e-02,  1.8169e-02,\n",
      "        -3.9363e-02, -4.9693e-02,  8.3050e-02, -1.3196e-02, -4.6567e-02,\n",
      "         3.9041e-02,  2.8396e-02, -2.6041e-02,  6.8008e-02, -1.0233e-01,\n",
      "        -1.5822e-02, -3.0579e-02, -4.8071e-02, -6.4514e-02,  1.8201e-02,\n",
      "        -4.3278e-02, -4.3680e-03, -8.4785e-02, -5.5908e-02, -6.7275e-02,\n",
      "         8.3114e-02,  1.3823e-02,  4.9019e-02,  4.0267e-02, -5.4514e-02,\n",
      "         4.9135e-02, -4.8312e-02, -2.4285e-02, -9.7027e-02,  2.4834e-02,\n",
      "         1.4886e-02,  6.9949e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 3.0289e-02,  3.1503e-02,  3.9986e-02,  1.3083e-01, -5.3132e-02,\n",
      "         2.9113e-02,  4.7187e-03,  5.0454e-02,  1.0700e-01, -2.2314e-02,\n",
      "         2.6524e-02, -1.1840e-02,  5.0855e-03,  7.3779e-04, -1.1865e-03,\n",
      "        -4.7954e-02,  1.0474e-02,  2.8582e-02, -7.9896e-02,  7.6038e-02,\n",
      "         4.5977e-02, -1.4148e-02,  3.9841e-02,  1.8766e-02,  8.0392e-02,\n",
      "         2.6746e-02,  2.9566e-02, -2.5976e-02,  1.6478e-02, -5.0035e-02,\n",
      "         2.4266e-02,  4.7684e-03, -4.6095e-02,  5.4383e-02, -5.5842e-02,\n",
      "        -6.3235e-02,  1.0002e-01, -7.9192e-03,  4.9059e-02, -2.9653e-02,\n",
      "         7.4298e-02,  3.2793e-02,  8.6242e-02,  1.3700e-03,  1.4234e-02,\n",
      "         7.6310e-02,  3.2565e-02, -5.5205e-02, -2.8722e-02, -3.9794e-02,\n",
      "         8.0323e-02, -1.0903e-01, -4.8134e-04,  4.3818e-02, -3.0959e-02,\n",
      "        -5.7084e-02,  4.3061e-02,  4.2138e-02,  7.2363e-02,  4.3792e-02,\n",
      "        -7.2850e-02,  5.2529e-03,  4.6195e-03, -6.2514e-02,  8.1972e-02,\n",
      "        -1.2628e-02,  1.1640e-01, -7.5081e-02,  2.6473e-02, -6.2586e-02,\n",
      "        -6.8327e-02,  5.4805e-03, -8.0045e-02, -1.0655e-02, -7.7074e-03,\n",
      "        -8.1215e-02, -1.6442e-02,  6.8840e-03, -6.9273e-03, -4.1731e-02,\n",
      "        -6.2782e-02,  6.2828e-02, -8.7719e-02,  1.7283e-02, -5.3315e-02,\n",
      "        -9.8364e-02, -9.7457e-02,  8.1505e-02,  2.6662e-02,  5.2712e-02,\n",
      "         5.1618e-02, -3.9540e-02, -1.0101e-01, -2.3273e-02,  1.6070e-02,\n",
      "        -3.2476e-02, -3.7883e-02, -1.9677e-02, -3.3466e-02,  1.7523e-02,\n",
      "        -9.1086e-02, -4.3556e-02,  7.8876e-02, -4.1143e-02, -3.5400e-02,\n",
      "        -1.7865e-02,  1.7630e-01,  1.3965e-01, -5.0848e-02, -3.6669e-02,\n",
      "         2.1116e-02, -1.0324e-01, -1.7145e-02,  6.3624e-02, -7.2753e-02,\n",
      "         8.1110e-04,  7.7122e-02,  6.0167e-02,  9.4302e-02,  3.3645e-02,\n",
      "         5.1997e-02,  9.3938e-03,  1.5380e-02,  3.0624e-02,  1.8364e-02,\n",
      "         9.4459e-02, -5.3204e-02,  5.3909e-02,  8.4368e-02, -2.6575e-02,\n",
      "         5.8741e-03,  1.7135e-01,  3.8734e-02,  1.1533e-01, -3.4991e-02,\n",
      "        -1.3902e-01, -5.0564e-02,  2.5342e-02,  1.9510e-03, -4.5458e-02,\n",
      "        -7.6664e-02,  1.0237e-01,  7.7267e-03,  5.8986e-02, -1.9288e-02,\n",
      "         5.3286e-02,  3.6359e-02,  8.0501e-02, -8.3045e-02,  3.3307e-02,\n",
      "         1.5659e-03,  9.6013e-03, -1.5590e-02, -5.1359e-02, -7.0246e-02,\n",
      "        -1.1975e-02,  2.6491e-02, -3.2005e-02,  6.8249e-02,  4.7669e-02,\n",
      "         4.7641e-02, -2.1512e-02, -6.3295e-02, -4.1788e-02, -1.5279e-02,\n",
      "        -9.7037e-02,  2.2685e-02,  2.0949e-02,  3.3309e-02,  9.4829e-03,\n",
      "         5.6710e-02, -7.6783e-03, -1.3969e-01, -4.1760e-02,  8.8335e-03,\n",
      "         4.3914e-02, -1.1144e-02,  2.1213e-02,  5.0143e-02, -1.7819e-02,\n",
      "        -3.6000e-02, -9.8346e-02,  1.8010e-02,  1.1031e-02, -4.7298e-02,\n",
      "        -2.5419e-02, -4.0803e-02,  3.5511e-02,  9.2070e-03,  6.9367e-03,\n",
      "        -4.2061e-02, -1.0377e-02,  8.0876e-02, -5.6107e-02,  5.7277e-02,\n",
      "         8.7439e-03,  1.8353e-02, -4.1559e-02,  3.4507e-02, -1.0548e-01,\n",
      "        -4.0571e-02, -2.1289e-02,  3.0586e-02,  5.1678e-03,  8.7577e-04,\n",
      "         1.3942e-01, -1.1645e-02,  7.2364e-02,  6.5043e-02,  2.4132e-02,\n",
      "         1.1002e-01,  6.1222e-03,  6.6061e-03, -5.2206e-02, -1.3325e-02,\n",
      "        -8.5573e-03, -2.0275e-03,  1.6365e-03,  2.6494e-02,  7.1705e-02,\n",
      "        -7.1865e-02,  8.4742e-02,  6.0429e-02, -5.9917e-04, -5.1137e-02,\n",
      "        -5.9481e-02, -7.6383e-02,  4.8239e-02, -3.4069e-02, -9.6994e-02,\n",
      "         1.8230e-02,  8.8950e-02,  8.6447e-02, -2.9383e-02, -9.0702e-02,\n",
      "        -3.7237e-02, -3.5979e-02, -4.2816e-02, -7.7253e-02,  7.3348e-03,\n",
      "         4.4436e-02, -1.5954e-01,  1.2394e-01,  1.1889e-02,  1.5041e-02,\n",
      "        -6.7389e-02, -4.5964e-02,  2.0859e-02, -3.0347e-02, -2.0750e-02,\n",
      "         3.9519e-02, -2.8886e-02, -8.1723e-02, -2.2986e-02, -2.3117e-03,\n",
      "         7.9396e-02, -4.6225e-02,  5.9592e-02, -6.6315e-02, -4.8456e-02,\n",
      "        -4.7836e-03, -6.7407e-02,  4.6288e-02,  1.5025e-01,  3.1964e-02,\n",
      "        -1.0685e-01, -3.1458e-02, -4.1457e-02,  7.1839e-02, -9.0231e-02,\n",
      "         3.3797e-02, -2.6273e-02, -6.0258e-02, -3.0063e-02, -9.9684e-02,\n",
      "         8.9154e-02,  4.6204e-02,  1.0030e-02, -2.1860e-02, -9.5296e-03,\n",
      "        -2.6632e-02, -2.0542e-02, -8.8112e-02, -3.1891e-02,  8.1285e-02,\n",
      "         3.4284e-02,  9.3343e-02, -7.2938e-02,  4.2222e-02,  8.5092e-02,\n",
      "        -6.9859e-02, -1.1665e-01, -1.7408e-02, -1.5403e-02,  5.4243e-02,\n",
      "         9.8341e-03, -2.8077e-02, -2.9991e-02,  3.4399e-02,  1.4826e-02,\n",
      "         1.0260e-02,  8.0673e-02,  5.1878e-03, -8.1736e-02,  8.6033e-02,\n",
      "         8.2636e-02,  5.0595e-02, -1.1922e-01,  9.3888e-03,  2.7255e-02,\n",
      "         2.7873e-02,  2.2796e-02,  1.8762e-02,  1.4380e-01, -1.4723e-01,\n",
      "        -1.4255e-02, -3.0604e-02, -3.7668e-03,  1.1167e-02, -8.0839e-02,\n",
      "         1.4414e-02, -2.5007e-02, -2.3666e-02, -2.7692e-02, -1.6474e-02,\n",
      "         5.1326e-02, -6.8901e-03,  2.6673e-02, -1.9049e-02, -4.9653e-02,\n",
      "         1.1313e-01,  8.5847e-02,  1.3205e-01, -4.7806e-02, -9.3220e-02,\n",
      "         4.1846e-02, -4.5715e-02,  2.4093e-02, -3.6066e-02,  5.0121e-02,\n",
      "         2.4745e-02, -9.0033e-02,  5.9747e-02, -5.9992e-02, -2.5795e-02,\n",
      "        -3.5649e-02,  2.3503e-02,  1.4340e-01, -5.7906e-02, -8.6132e-03,\n",
      "        -6.0701e-03,  3.0256e-03, -6.0207e-02,  1.3398e-02, -3.4405e-03,\n",
      "         3.6077e-02, -7.9061e-02, -4.5184e-02, -6.7206e-02,  8.3835e-02,\n",
      "        -1.4701e-02,  2.4760e-02,  1.7550e-02,  5.2360e-02, -1.1143e-01,\n",
      "        -6.0042e-02, -2.1617e-02, -2.3820e-02, -1.9716e-02, -1.1295e-01,\n",
      "        -1.7096e-02, -5.0607e-02,  9.7075e-02,  2.0780e-02, -4.8206e-02,\n",
      "         4.0675e-02, -5.4123e-02,  2.6274e-02, -1.1451e-01,  5.9652e-02,\n",
      "        -2.4965e-02, -2.3823e-02,  5.4150e-03, -2.5337e-03, -5.9982e-02,\n",
      "        -3.6474e-02, -1.8158e-02, -1.5301e-02,  1.1725e-02,  2.3499e-02,\n",
      "         7.4033e-02, -4.0130e-02, -5.1274e-02,  9.0815e-02,  5.4975e-02,\n",
      "        -3.4270e-02,  4.5382e-02, -7.2244e-02, -7.0036e-02, -9.7178e-03,\n",
      "        -3.3955e-02, -3.5253e-02,  8.1896e-02,  7.5562e-03, -7.9211e-02,\n",
      "        -1.0875e-01,  1.2409e-03,  7.7800e-02,  1.0634e-02, -8.2665e-02,\n",
      "         1.3230e-02, -3.4552e-02,  9.1453e-02, -6.4865e-02,  4.5128e-02,\n",
      "        -1.1324e-01, -5.8086e-02,  4.5286e-02, -3.5615e-02,  1.1491e-03,\n",
      "         4.5156e-02,  2.6197e-02, -9.7915e-02, -8.8574e-02,  6.3982e-02,\n",
      "        -7.3688e-02,  3.8706e-02,  8.2396e-02,  7.6938e-02, -2.0139e-02,\n",
      "        -6.2673e-02, -8.2048e-02,  5.6388e-02,  1.7644e-02,  4.3307e-02,\n",
      "         8.2072e-03, -4.8394e-02,  7.1145e-03, -1.4995e-01,  6.3767e-02,\n",
      "        -1.7300e-02, -4.0330e-04,  2.5645e-02,  6.1843e-02, -5.0088e-03,\n",
      "         3.9473e-03,  8.7710e-02,  3.0694e-02, -1.5863e-02,  1.2367e-01,\n",
      "         5.8815e-02,  6.1809e-02,  1.1823e-01,  3.4193e-02, -1.3734e-01,\n",
      "        -8.3475e-03, -1.3101e-02,  1.7372e-01,  3.1849e-02,  5.8699e-02,\n",
      "        -8.2168e-02,  2.9679e-02,  2.9754e-02, -1.9589e-02, -2.3867e-05,\n",
      "         2.9229e-03, -5.9795e-02,  1.0513e-01, -2.3250e-02,  1.5259e-02,\n",
      "        -9.9677e-04,  5.2436e-02,  4.5202e-02, -5.3536e-02, -3.1198e-02,\n",
      "         1.1600e-01,  8.2992e-02, -6.0462e-02, -6.9867e-02, -2.0561e-03,\n",
      "         6.2426e-02,  3.0686e-02,  7.3595e-03, -5.2512e-03, -8.7785e-02,\n",
      "         7.2232e-02, -5.5166e-02,  5.2830e-02, -3.4109e-02, -3.5072e-02,\n",
      "        -7.8913e-02,  3.6241e-02,  4.8680e-02, -2.4749e-02,  9.5748e-02,\n",
      "         1.1784e-01,  6.6303e-02, -3.3105e-02,  3.1397e-02,  4.8392e-02,\n",
      "        -9.6809e-02,  6.1331e-02,  3.0868e-02,  3.2937e-02,  1.4860e-02,\n",
      "        -8.8214e-02, -7.5167e-02, -2.6680e-02, -7.2619e-02, -3.8868e-02,\n",
      "         4.7005e-02, -1.5254e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-3.7034e-02, -4.5888e-02,  8.8781e-03,  2.7156e-02,  5.8858e-02,\n",
      "         1.2498e-03, -2.9473e-02, -2.4259e-02,  2.7695e-02,  4.8506e-02,\n",
      "        -1.3610e-02,  2.4264e-02, -1.0506e-02, -2.2343e-02, -1.2575e-02,\n",
      "        -2.7388e-02,  3.7047e-03, -9.8502e-02, -7.6187e-02, -1.3275e-02,\n",
      "         4.0868e-02,  3.1048e-02,  2.9744e-03, -3.4535e-02,  6.2692e-02,\n",
      "        -1.0555e-01, -1.8775e-03, -6.1323e-02,  1.1437e-02,  6.9841e-02,\n",
      "        -1.2952e-02,  7.9710e-02, -3.6756e-02,  1.2847e-02,  1.0407e-01,\n",
      "        -8.7324e-02, -1.0587e-01, -3.1902e-02, -8.2598e-03, -1.0516e-01,\n",
      "        -9.7262e-02,  1.1731e-02, -1.1542e-02, -1.0035e-01, -8.8628e-02,\n",
      "        -1.6604e-02, -6.6435e-04, -5.5660e-02, -5.8090e-03, -9.9288e-03,\n",
      "         2.7286e-02, -4.0562e-02, -1.3763e-02, -5.6210e-02, -8.2477e-03,\n",
      "         3.0968e-02, -2.2097e-02,  2.6884e-02, -4.4554e-03,  6.1624e-02,\n",
      "         5.7080e-02,  9.1388e-03, -2.2383e-02,  3.1594e-02,  9.0034e-02,\n",
      "         2.9283e-04, -2.4813e-03, -4.8279e-02,  2.9078e-02,  3.5868e-02,\n",
      "         4.1491e-02, -6.3660e-02, -8.4763e-02, -8.1597e-02, -5.1852e-02,\n",
      "         2.2601e-04,  1.0845e-01,  3.0973e-02, -1.5400e-01,  3.3164e-02,\n",
      "         7.9088e-02,  6.5250e-02,  5.1900e-02, -4.2283e-02, -1.1346e-01,\n",
      "        -9.0076e-03,  1.1980e-01, -1.1909e-02,  1.2310e-02,  1.8831e-02,\n",
      "        -4.9647e-02,  7.0969e-02, -2.3682e-02, -8.6618e-02,  5.2677e-02,\n",
      "         7.8079e-03, -1.0115e-01,  7.5915e-02, -4.8108e-02, -1.3128e-01,\n",
      "         6.4873e-02, -7.1029e-03, -1.4379e-01, -2.1432e-02, -5.3666e-02,\n",
      "         3.7874e-02, -8.1764e-02,  1.6618e-01,  7.1652e-02,  4.2189e-02,\n",
      "        -4.8112e-02,  5.0704e-02, -8.4332e-02,  2.3637e-02, -1.1713e-02,\n",
      "        -1.4738e-01, -5.6326e-02, -8.2328e-02, -6.9366e-03,  8.9393e-03,\n",
      "         9.0724e-02, -3.4346e-02, -1.7982e-02, -1.4817e-02, -9.2182e-02,\n",
      "         3.9414e-02, -1.3945e-02, -9.3391e-02,  1.0452e-01,  8.3443e-02,\n",
      "        -8.3101e-03,  5.8458e-02,  3.4724e-02, -9.1750e-02,  2.9846e-02,\n",
      "        -9.8895e-02, -2.4202e-02,  4.6580e-02,  4.4337e-02, -1.2447e-02,\n",
      "        -8.0480e-03, -5.6974e-03, -3.7265e-02,  7.7061e-02,  5.1464e-02,\n",
      "        -7.0224e-02, -4.4164e-02,  2.5564e-02,  1.2461e-02, -2.4537e-02,\n",
      "         2.2466e-02,  6.7765e-03, -2.1143e-02,  1.3173e-02, -4.8422e-02,\n",
      "        -2.4130e-02,  4.0795e-02, -6.9050e-02,  5.2960e-02,  2.9344e-02,\n",
      "         6.1323e-02,  2.6642e-02, -1.5501e-02,  1.1257e-02,  5.2199e-02,\n",
      "        -1.9131e-02, -7.1120e-02,  1.5206e-01, -5.5123e-02,  1.6600e-02,\n",
      "        -1.7471e-02,  5.4039e-02,  7.3465e-02, -1.4534e-02,  3.2988e-02,\n",
      "         1.0805e-01,  2.3235e-03,  2.6146e-02,  5.6207e-02,  2.4650e-02,\n",
      "         1.0190e-02, -4.5924e-03,  4.1432e-02, -4.8620e-02, -2.9034e-02,\n",
      "        -2.9012e-02,  1.4155e-02,  3.5942e-02, -9.4590e-03, -3.9627e-02,\n",
      "        -5.3268e-02,  1.3831e-01, -3.0257e-02, -5.7423e-03,  4.2466e-02,\n",
      "         1.2649e-01, -5.0767e-02, -1.1174e-02, -2.3112e-02,  3.8812e-02,\n",
      "        -6.3522e-02,  9.1453e-02,  2.6309e-02, -1.1686e-01, -3.9759e-02,\n",
      "         2.4578e-02, -4.7622e-03, -5.6869e-02,  9.6072e-02,  1.3556e-02,\n",
      "        -2.8459e-02, -4.5581e-02,  1.2914e-01, -1.1633e-02,  1.1193e-01,\n",
      "        -8.6753e-02, -8.5673e-03, -7.3127e-02, -3.6154e-02, -9.3040e-02,\n",
      "        -3.7462e-02,  1.2344e-01,  8.0146e-02, -1.7490e-02,  1.1924e-01,\n",
      "        -1.0738e-02,  6.7925e-02, -6.9445e-02, -2.5708e-02, -5.6665e-02,\n",
      "        -1.5419e-01,  1.2431e-01, -7.5615e-03, -1.0575e-01,  8.1955e-02,\n",
      "        -3.7937e-02,  8.6439e-02, -3.1533e-03,  1.4085e-01,  3.6980e-02,\n",
      "        -1.3440e-02, -5.1998e-02,  5.9634e-02, -4.4400e-02,  1.6468e-02,\n",
      "         3.7003e-02,  2.0843e-02,  4.8651e-02, -3.7829e-02,  1.0212e-01,\n",
      "        -1.8587e-02,  4.5990e-02, -4.5087e-03, -1.0517e-01, -7.8714e-02,\n",
      "        -2.2157e-02, -5.8386e-02,  7.0721e-02, -1.4240e-02, -1.0749e-01,\n",
      "        -6.8921e-02, -3.1443e-02, -3.2220e-02, -6.4972e-02,  1.1256e-02,\n",
      "         4.3494e-02,  1.8916e-02, -1.8547e-01, -2.1113e-02, -3.5792e-02,\n",
      "        -1.2145e-02,  4.6165e-02, -1.1010e-01,  3.3331e-04,  8.4547e-02,\n",
      "         5.4524e-02,  4.8118e-02, -9.5097e-02, -7.2445e-02, -6.6263e-05,\n",
      "         5.1787e-02,  4.9852e-02, -4.7932e-02, -1.2280e-02, -1.6250e-02,\n",
      "        -1.4342e-02, -1.1116e-01, -5.5778e-02, -7.7247e-03, -8.1662e-02,\n",
      "        -4.3206e-03,  6.6698e-02, -5.0373e-02, -1.2831e-01,  7.0735e-02,\n",
      "        -4.0484e-02, -2.6315e-02, -2.7391e-02, -8.0403e-02, -6.9732e-03,\n",
      "         5.4342e-02,  2.0656e-02,  1.5141e-01,  1.0275e-01,  1.5837e-03,\n",
      "        -1.4563e-01,  8.5911e-05,  4.7454e-03, -7.8300e-02,  4.8858e-02,\n",
      "        -2.1546e-02,  1.4427e-02,  4.6923e-02, -4.1582e-02,  3.4860e-02,\n",
      "         1.6094e-01, -2.8653e-02,  6.8671e-02,  3.9210e-02, -2.7989e-02,\n",
      "         1.2157e-01,  3.4874e-02,  1.0473e-01,  5.0698e-02, -6.6427e-02,\n",
      "        -8.5859e-02,  4.0868e-02, -8.1263e-02,  1.2227e-04, -4.1179e-02,\n",
      "         7.0834e-03,  8.5109e-02, -2.0567e-02,  6.0143e-03, -8.9583e-02,\n",
      "         6.3068e-02, -4.5089e-02,  2.6703e-02,  5.3511e-03,  9.8072e-03,\n",
      "         9.1949e-04,  4.8803e-02, -1.2944e-02, -1.6477e-02,  3.7466e-03,\n",
      "        -7.1968e-02, -6.9599e-02, -1.0072e-01, -7.0090e-02,  3.5817e-02,\n",
      "         6.2147e-02,  8.6350e-02,  8.2676e-02,  6.9734e-03, -1.6660e-01,\n",
      "         3.0636e-02, -7.5360e-02,  8.7070e-02,  4.6590e-02, -1.2240e-02,\n",
      "         4.7421e-02,  1.4499e-01, -3.2117e-02,  6.7256e-03, -9.1146e-03,\n",
      "         5.6627e-02,  3.4365e-02,  3.5674e-02,  1.1961e-03,  9.1195e-03,\n",
      "        -1.0258e-01, -2.6809e-02, -3.6439e-02, -5.3987e-02, -3.7285e-02,\n",
      "        -4.7299e-02,  2.0322e-02, -7.9408e-02, -7.7213e-02, -4.1219e-02,\n",
      "         1.1305e-01, -3.6860e-02,  3.4759e-02,  4.5197e-03, -1.8849e-02,\n",
      "        -1.1627e-02,  7.8283e-02, -5.6437e-02,  3.5024e-02,  6.2222e-02,\n",
      "        -8.2901e-02,  7.1049e-02,  9.9048e-03,  8.3881e-02,  3.7555e-03,\n",
      "         8.8532e-02,  9.2635e-02,  1.6246e-02, -3.0551e-02,  4.0173e-02,\n",
      "         3.9328e-02,  9.8969e-03,  7.2826e-04, -8.5527e-03,  1.9672e-02,\n",
      "         1.0268e-01, -4.0752e-03, -5.5843e-02,  1.5902e-02,  7.0855e-03,\n",
      "        -3.0325e-02,  2.9130e-02, -7.9757e-02,  2.0168e-02,  1.3599e-02,\n",
      "        -2.4822e-02, -8.0696e-03,  7.8805e-03,  3.1998e-04, -3.3752e-02,\n",
      "        -2.3653e-02,  7.4149e-02, -9.0394e-03, -6.5222e-03, -3.0573e-02,\n",
      "         1.1063e-01,  7.5828e-02,  4.1677e-02,  1.3911e-02, -7.0996e-03,\n",
      "         2.3597e-03,  2.6949e-03, -5.3042e-03,  7.1347e-02,  2.7978e-02,\n",
      "         9.5793e-04, -2.3873e-02, -7.2959e-02,  3.1148e-02, -6.5378e-02,\n",
      "         4.4773e-02, -4.6407e-02, -2.7808e-02,  6.0678e-02,  2.2824e-02,\n",
      "         1.2299e-02, -1.2252e-01, -9.4176e-02, -3.1335e-02,  6.1090e-02,\n",
      "        -8.9544e-02, -7.8463e-02, -1.0646e-01,  1.2856e-01,  5.3371e-02,\n",
      "        -3.5043e-02,  4.9204e-02, -2.7718e-02, -1.8169e-03, -3.2086e-02,\n",
      "         7.7823e-03,  6.8141e-03,  9.3693e-02,  1.6695e-02, -7.0995e-03,\n",
      "        -8.1406e-02, -1.0529e-02,  2.3930e-02, -2.4667e-02,  1.4599e-02,\n",
      "         2.2815e-02,  6.4431e-02, -8.6203e-02, -1.9157e-01,  3.7300e-02,\n",
      "        -2.8549e-02, -2.9900e-02,  2.0874e-02, -1.8929e-01,  6.7435e-02,\n",
      "        -4.1862e-02,  4.9628e-04,  7.5833e-03,  8.0471e-02, -1.7851e-02,\n",
      "        -4.5390e-02,  2.1833e-02, -1.6886e-02, -1.0043e-02, -7.4905e-02,\n",
      "        -9.9795e-04, -2.0626e-02,  8.3278e-02, -7.4464e-02,  3.2107e-02,\n",
      "         4.9412e-02, -5.9202e-02, -6.2015e-02,  1.0825e-02,  8.4142e-02,\n",
      "         6.0584e-02,  2.8453e-02, -6.4364e-02,  3.4312e-02, -3.1387e-02,\n",
      "        -1.0054e-02,  6.2364e-02,  9.9319e-02,  4.8268e-02,  3.6428e-02,\n",
      "         4.7602e-02, -2.9711e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 1.7651e-03,  3.1396e-03, -1.4551e-02, -3.5147e-03, -2.1400e-02,\n",
      "         1.1861e-01,  2.7863e-02, -5.9323e-02,  5.4408e-02, -3.9685e-02,\n",
      "        -2.9819e-02,  1.3290e-02, -3.3680e-02, -3.0514e-02, -9.5577e-02,\n",
      "        -2.7275e-02, -3.2411e-02,  1.0115e-01,  1.8278e-02,  5.4167e-02,\n",
      "        -6.5277e-02, -4.9623e-02,  3.8521e-02, -7.7113e-02,  2.3679e-02,\n",
      "        -2.1421e-02, -3.1328e-02, -3.8675e-02,  3.2301e-02,  8.0595e-02,\n",
      "        -1.7031e-01, -3.1086e-02,  7.4435e-02,  4.4086e-02, -5.2982e-02,\n",
      "        -3.7618e-02, -2.1757e-02,  2.6901e-02,  1.2416e-02, -6.1151e-02,\n",
      "        -5.6076e-03, -8.1322e-02,  1.2883e-01,  2.1244e-01,  6.3023e-03,\n",
      "        -6.4486e-02, -6.8903e-02, -5.2496e-02, -8.8419e-03, -4.8330e-03,\n",
      "        -9.8466e-02, -9.1724e-02, -4.6670e-03,  2.2265e-02,  7.4199e-03,\n",
      "        -6.7150e-03,  3.1992e-03, -1.9731e-02,  1.7806e-02, -7.7666e-03,\n",
      "         6.6665e-03, -4.9659e-03,  1.3266e-02, -3.1188e-02,  6.3222e-02,\n",
      "         2.4398e-02,  2.9437e-02, -7.7957e-04,  2.4054e-02,  1.6580e-01,\n",
      "        -7.9211e-02, -2.8934e-02,  3.4830e-02,  3.1386e-02,  1.2014e-03,\n",
      "         9.4098e-02, -5.5012e-03,  4.5756e-02,  3.2991e-02,  1.8693e-02,\n",
      "         4.4928e-02, -2.1605e-02,  4.0092e-02, -6.9511e-02, -8.6237e-02,\n",
      "        -1.2794e-01,  2.8559e-02, -3.4364e-02,  2.3834e-03,  9.2352e-03,\n",
      "        -2.0991e-03,  1.2794e-02,  1.0197e-02, -1.4751e-02, -5.3813e-03,\n",
      "         2.9286e-02,  8.8126e-02,  1.5448e-02, -1.4078e-02, -3.9143e-02,\n",
      "        -5.8560e-02,  5.4407e-02,  3.5490e-02, -7.9659e-02,  3.4453e-02,\n",
      "         2.5864e-02, -3.0899e-02, -1.5625e-02, -4.5447e-02,  4.7464e-02,\n",
      "        -3.1091e-02, -3.4445e-02, -5.4052e-02, -6.4918e-02,  4.4487e-02,\n",
      "         2.5045e-02, -1.1488e-02,  2.5262e-02, -1.2607e-02,  1.3235e-02,\n",
      "         2.8561e-02,  6.9778e-02,  3.5717e-02, -1.3796e-02, -1.6055e-01,\n",
      "        -6.3508e-02,  3.0388e-02,  3.6702e-02,  1.4510e-02,  8.2649e-02,\n",
      "        -9.6217e-03,  2.8959e-02,  3.8684e-02, -8.4300e-02, -1.5368e-01,\n",
      "         9.8709e-02, -7.2473e-02,  3.1997e-02,  1.1817e-01, -2.6140e-02,\n",
      "        -6.1742e-02, -1.6166e-02,  7.0216e-02, -1.2530e-01, -3.3601e-02,\n",
      "         1.8504e-02,  4.9253e-02,  1.5496e-01, -7.7431e-02, -1.4273e-02,\n",
      "        -1.3381e-02,  1.0467e-01, -7.3973e-02, -9.8395e-02, -2.9553e-02,\n",
      "         4.8231e-02,  6.4982e-02, -5.0469e-02,  3.5893e-02,  9.4489e-02,\n",
      "         6.2196e-02, -9.2381e-02, -8.7598e-02,  7.9401e-02, -6.6444e-02,\n",
      "        -1.0009e-02, -3.8275e-02, -2.5270e-02, -1.7952e-01, -9.5267e-03,\n",
      "        -1.3783e-01,  2.1312e-01, -1.1740e-02, -8.2986e-02,  3.5087e-02,\n",
      "        -1.9155e-02, -2.4328e-02, -4.0487e-02,  3.3686e-02, -1.7021e-02,\n",
      "        -5.0354e-02, -1.5596e-01, -1.7125e-03,  5.6674e-02,  6.6230e-03,\n",
      "         6.4058e-03, -3.7337e-03,  1.1259e-02, -2.4012e-02,  8.4532e-02,\n",
      "        -2.1994e-02,  3.6341e-03,  8.1102e-02, -5.8442e-02,  9.7022e-02,\n",
      "        -6.0901e-02,  5.0808e-02,  1.3352e-01,  1.6406e-02,  1.3148e-02,\n",
      "         2.8686e-02, -3.0704e-02, -4.3113e-02,  5.2098e-02, -5.5051e-02,\n",
      "        -1.1791e-01,  5.0002e-02,  2.3706e-03, -6.4074e-02,  5.0139e-02,\n",
      "        -3.7592e-02,  5.3099e-02,  3.9144e-02,  4.3691e-03,  1.4775e-02,\n",
      "        -7.3321e-02, -4.6698e-02,  1.2764e-01, -6.2895e-02, -2.6595e-02,\n",
      "         7.9530e-02,  3.6950e-02, -4.7796e-03,  3.2136e-02, -4.4875e-02,\n",
      "        -3.2131e-02,  8.3086e-02,  8.9513e-02, -6.2051e-03, -1.2118e-01,\n",
      "         2.6485e-02, -3.3139e-02,  4.4756e-02,  7.8008e-04,  7.1055e-02,\n",
      "         3.0050e-02,  8.2575e-03, -2.6538e-02, -3.9907e-02, -2.5800e-02,\n",
      "        -3.3800e-02,  1.8517e-02, -7.0688e-02, -1.3011e-01, -3.3101e-02,\n",
      "        -5.4424e-02,  3.0215e-02, -6.2839e-02,  2.4651e-02, -1.8812e-03,\n",
      "        -1.3442e-01,  1.2847e-02,  7.9453e-02,  8.0802e-02, -9.5993e-02,\n",
      "         3.4160e-02,  2.6102e-02, -8.6553e-03,  5.7268e-02,  8.5350e-02,\n",
      "         1.3918e-02,  1.1504e-02,  2.9779e-03,  1.0623e-02,  5.5536e-02,\n",
      "        -4.1146e-02, -9.3039e-02, -3.3455e-03,  1.5882e-02, -1.5050e-01,\n",
      "         7.5856e-03,  2.2823e-02, -3.8871e-02,  5.5844e-02,  5.4641e-03,\n",
      "        -2.4733e-02, -5.1179e-02, -1.8616e-02,  5.5658e-02, -6.9583e-02,\n",
      "        -6.0925e-02, -8.0161e-02, -1.0143e-01,  4.3837e-02,  1.3554e-01,\n",
      "         8.7156e-02,  2.5922e-02, -7.2726e-02, -1.8920e-02,  9.7482e-02,\n",
      "         2.0591e-02, -6.2224e-02,  5.4904e-02, -1.3960e-01, -7.6254e-02,\n",
      "         8.3799e-02, -3.9226e-02, -4.3723e-02, -3.3469e-02,  9.1810e-03,\n",
      "         4.9622e-02,  6.3080e-02, -2.8480e-02, -1.8700e-02,  6.6885e-02,\n",
      "        -6.8625e-03,  7.1043e-02,  7.1088e-02, -9.2783e-02,  9.1262e-02,\n",
      "         4.6247e-02, -2.9005e-02,  2.8690e-02,  1.9394e-02,  5.7164e-05,\n",
      "         2.2624e-02,  3.3163e-02,  1.7700e-02,  3.4232e-02, -2.9858e-02,\n",
      "        -7.4267e-02,  3.6014e-02, -4.4552e-02,  3.5258e-02, -1.0101e-01,\n",
      "        -6.7129e-03,  1.4119e-02,  2.7532e-02,  1.8333e-02,  1.0998e-01,\n",
      "        -4.3879e-04,  6.3078e-02,  1.9749e-02,  4.5188e-02,  1.7698e-02,\n",
      "        -1.6677e-02,  8.2497e-02, -7.5923e-02,  6.3407e-02,  6.3229e-02,\n",
      "         1.7209e-02,  8.9937e-02, -3.1758e-02,  2.4061e-02, -7.6937e-02,\n",
      "         2.9163e-03, -6.6448e-02, -1.3663e-02, -3.8498e-02, -6.1970e-02,\n",
      "        -5.3004e-02,  2.5560e-02,  1.7372e-01,  1.9347e-02,  7.7611e-02,\n",
      "         1.2019e-01, -1.5177e-01, -1.0369e-02, -3.0696e-02,  6.5096e-02,\n",
      "         1.3015e-02,  5.4550e-02, -5.5283e-02,  7.5891e-03, -2.0863e-02,\n",
      "        -2.2272e-02,  1.8210e-02, -6.6587e-03, -1.3865e-02,  5.7003e-02,\n",
      "        -1.9093e-02,  9.1872e-03,  9.9067e-02,  3.3590e-04,  4.0905e-02,\n",
      "         2.1044e-03, -6.7002e-03,  2.9374e-02, -1.1736e-02,  3.6019e-03,\n",
      "        -2.4367e-02, -3.7626e-02, -1.1231e-01,  1.7375e-02, -6.0035e-03,\n",
      "         5.7686e-02, -2.7193e-02,  1.9783e-02, -6.3263e-02,  2.2237e-02,\n",
      "         7.3779e-03, -2.8759e-03, -1.5603e-02,  5.7662e-02,  8.7457e-03,\n",
      "         1.0018e-02, -5.0072e-02, -3.7638e-02,  2.4585e-02, -9.2793e-02,\n",
      "        -1.1872e-01, -2.2116e-02, -1.0956e-01, -1.0836e-01,  8.6403e-02,\n",
      "         4.3467e-02,  2.0700e-02,  5.1945e-02,  3.0060e-02,  2.7744e-02,\n",
      "        -9.2273e-03,  7.5827e-02, -4.5446e-02,  8.2869e-02, -9.2931e-02,\n",
      "         1.2670e-02, -4.8179e-02, -1.5450e-01, -1.6038e-03, -2.9253e-02,\n",
      "        -2.7980e-02, -1.0475e-02,  2.7516e-02,  1.6998e-01,  2.4017e-02,\n",
      "         8.4535e-02, -2.9163e-04,  3.1187e-02,  5.4309e-02, -3.0479e-02,\n",
      "        -8.0611e-02, -6.6498e-02, -1.4551e-01,  2.1430e-03, -3.7552e-02,\n",
      "         7.6690e-02,  7.4113e-02, -1.0557e-01, -7.4909e-02,  6.7211e-02,\n",
      "        -7.8306e-02,  4.8829e-02, -4.6191e-02,  1.3408e-02,  2.7609e-02,\n",
      "        -7.1487e-03, -3.2693e-03,  6.9174e-02,  1.8630e-01,  8.2350e-02,\n",
      "        -7.1999e-02, -5.7636e-04, -1.0190e-01, -2.1849e-02, -2.4579e-02,\n",
      "         8.7751e-02, -3.5942e-02, -2.4704e-03, -1.1202e-01, -7.7516e-02,\n",
      "        -3.0877e-02,  5.2970e-02, -1.0476e-02,  9.5842e-03, -7.3720e-02,\n",
      "         4.0108e-02, -1.2442e-02,  3.8677e-02, -4.2649e-02,  3.2528e-02,\n",
      "         4.7383e-02, -1.2851e-03, -3.1630e-02,  9.8758e-02, -4.5205e-02,\n",
      "         9.8402e-02, -9.2297e-02, -1.9997e-02, -1.7744e-02, -2.2326e-02,\n",
      "         8.0307e-02, -1.7815e-02,  1.9394e-02, -5.2028e-02, -5.1993e-02,\n",
      "         5.9033e-03,  1.0825e-02, -1.7139e-02, -1.4043e-01,  3.3729e-02,\n",
      "         2.4079e-02,  3.1476e-02, -7.7750e-02,  4.4037e-03, -7.0054e-02,\n",
      "        -1.0412e-02,  8.7667e-03,  6.4475e-02, -6.7967e-02,  4.3379e-02,\n",
      "        -8.0798e-02,  1.3300e-01, -2.5715e-02,  4.1997e-02,  1.5607e-02,\n",
      "        -1.8457e-02, -1.4307e-02, -1.3592e-02, -8.4850e-04,  6.9601e-03,\n",
      "         1.7143e-02, -7.7591e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 7.0232e-02, -3.9177e-02, -5.6618e-02,  2.4083e-02, -1.1208e-01,\n",
      "        -8.1959e-02, -1.0443e-02,  3.7170e-02, -5.0199e-02,  3.4944e-02,\n",
      "        -1.1758e-01,  2.1176e-02, -1.3711e-02,  1.7198e-03,  5.5355e-03,\n",
      "        -2.0753e-02, -4.9122e-02, -1.6634e-02,  8.6761e-04,  5.3412e-02,\n",
      "        -5.2480e-02,  3.0198e-03, -2.5701e-02, -1.2355e-01,  5.1328e-02,\n",
      "        -6.1343e-02, -4.0117e-02,  6.6909e-02,  5.3694e-03,  2.3511e-02,\n",
      "        -8.8978e-03, -1.3987e-02,  5.8384e-02, -7.4027e-02,  7.0214e-03,\n",
      "        -2.3619e-02, -1.1647e-02,  5.0248e-03, -1.0472e-01,  5.0924e-02,\n",
      "         9.6729e-03,  4.4740e-02, -8.4304e-03,  3.1834e-02, -5.6028e-02,\n",
      "         1.2579e-02,  4.1046e-02, -7.6398e-02, -8.2215e-02, -7.4826e-02,\n",
      "        -5.8703e-03,  2.1899e-02, -3.4924e-02, -9.3841e-02,  9.2489e-02,\n",
      "        -3.9724e-02,  6.8778e-02,  6.2910e-02,  1.5035e-01, -8.5688e-02,\n",
      "         5.1889e-02, -9.3605e-02, -7.0402e-02,  4.7219e-02,  5.9798e-02,\n",
      "        -3.6312e-03, -1.3177e-02, -4.6579e-02,  2.6072e-02, -1.8031e-02,\n",
      "        -1.5455e-01,  1.6608e-01, -1.5167e-03, -2.2081e-02, -3.3239e-02,\n",
      "         7.1615e-03,  5.0772e-02,  6.4464e-03, -1.0717e-03,  1.1329e-01,\n",
      "        -4.0795e-03,  7.9883e-02, -4.3044e-02,  1.3580e-01, -1.0705e-02,\n",
      "         7.0666e-03, -7.1443e-03,  9.1426e-02, -1.3554e-03, -9.4658e-02,\n",
      "        -4.0040e-02,  5.9643e-02,  1.8720e-02, -6.1085e-03, -5.1143e-03,\n",
      "         5.2426e-03,  3.9795e-02,  5.7733e-02,  9.3336e-02,  4.6847e-03,\n",
      "         5.8702e-02,  2.5341e-02,  4.2893e-02,  7.5947e-02,  2.8520e-04,\n",
      "         7.1536e-03, -4.3884e-03, -4.4555e-02, -4.4503e-02,  5.6192e-02,\n",
      "        -5.1656e-02, -1.2393e-01,  4.3672e-02,  4.7996e-02, -1.0800e-02,\n",
      "         5.6755e-02, -8.3568e-02, -1.3538e-02, -5.6153e-02, -5.5316e-02,\n",
      "        -2.3975e-02, -1.1282e-01, -2.8566e-02,  7.2766e-02, -3.8624e-02,\n",
      "         7.6615e-02,  3.6164e-02,  1.0354e-01,  4.9160e-02,  1.9378e-02,\n",
      "        -2.2329e-02, -1.2350e-01,  1.2831e-01,  9.7161e-03,  8.3806e-02,\n",
      "        -5.0945e-02, -2.3909e-02, -2.4867e-02,  5.3618e-02,  4.3033e-02,\n",
      "        -8.6281e-03, -3.7764e-02, -1.2432e-01,  1.3901e-02, -8.2746e-02,\n",
      "         1.5292e-02, -1.0102e-01, -2.1163e-03,  2.4047e-02, -3.3842e-02,\n",
      "         1.7279e-01, -2.0493e-03, -1.4493e-02,  5.7667e-02, -2.8942e-02,\n",
      "        -3.2882e-03,  7.1961e-02,  1.5763e-02, -1.0857e-01,  3.1682e-02,\n",
      "        -1.5458e-02,  2.3903e-02, -7.8493e-02,  3.3385e-02, -1.1762e-02,\n",
      "         5.4726e-02, -1.0496e-01, -1.9116e-02,  4.4039e-02, -4.5159e-02,\n",
      "         1.1691e-01, -7.5459e-02, -3.4751e-02, -7.0932e-05, -5.4284e-03,\n",
      "        -3.1645e-02,  7.8052e-02, -1.3927e-02, -3.9138e-02, -6.9432e-02,\n",
      "        -5.6814e-02,  4.7092e-02, -9.7913e-02, -7.1706e-02, -7.4354e-02,\n",
      "         2.9061e-02,  1.2788e-01,  3.2878e-02,  6.8620e-02,  7.8050e-03,\n",
      "        -8.1034e-03,  1.2591e-01, -2.5306e-02,  2.3245e-02,  6.0525e-03,\n",
      "         5.1102e-02,  2.6583e-02, -2.1282e-03, -5.5411e-02,  4.6495e-02,\n",
      "        -2.4725e-02,  2.2852e-02, -1.2736e-02,  1.6637e-01, -5.4719e-02,\n",
      "         8.6107e-02, -5.4407e-02,  6.8237e-02, -6.1891e-02, -5.5849e-02,\n",
      "         7.3760e-03, -3.0345e-02, -3.1600e-02,  3.3583e-02,  2.8570e-02,\n",
      "         8.2200e-02,  2.6655e-02,  2.6249e-02, -1.2001e-02,  7.8356e-02,\n",
      "        -1.6183e-02, -1.4890e-02,  1.2511e-02,  3.7454e-02,  2.5717e-02,\n",
      "         2.4392e-03,  1.9375e-02,  6.4533e-02,  3.3817e-02, -6.6789e-02,\n",
      "        -8.1340e-02, -3.5166e-02, -2.8866e-02, -7.5490e-02,  3.9034e-02,\n",
      "        -4.9257e-02,  1.5981e-02,  1.2176e-02,  2.2973e-02,  1.0207e-02,\n",
      "        -1.0285e-03,  1.7862e-01,  6.4228e-02, -3.5339e-02,  8.1926e-02,\n",
      "         7.1711e-02, -1.0528e-02,  3.6034e-02, -2.3140e-02,  4.6343e-02,\n",
      "        -3.3368e-02, -4.6355e-02,  5.1168e-02,  1.8313e-02, -3.8195e-03,\n",
      "         1.0237e-01, -4.0303e-02,  3.3172e-02, -4.5773e-02, -1.4106e-02,\n",
      "        -3.1364e-02,  5.1665e-02, -3.1724e-02,  3.0433e-02, -3.9412e-02,\n",
      "        -3.3040e-02,  2.1146e-02, -1.1771e-01, -6.6739e-02, -3.3981e-02,\n",
      "        -4.4390e-03,  2.8506e-02,  1.9362e-02,  1.0839e-01, -1.5109e-02,\n",
      "         1.5135e-01, -2.9912e-02,  7.0132e-02, -6.2905e-02, -9.2045e-02,\n",
      "        -1.0811e-01, -6.4596e-02,  1.1569e-01,  4.2324e-02, -5.3588e-02,\n",
      "         2.2440e-02, -2.2649e-02, -7.6581e-02, -6.1811e-02,  6.7117e-02,\n",
      "        -8.8734e-02,  1.1926e-02, -1.0264e-02,  2.1893e-02,  6.1756e-02,\n",
      "         1.1959e-01, -9.6380e-02,  1.3470e-02, -7.0965e-02,  2.2478e-02,\n",
      "         5.0166e-02, -4.6788e-03,  9.3105e-02,  1.2183e-01, -1.0024e-01,\n",
      "         1.9777e-04,  8.1114e-02, -2.6921e-02,  1.0334e-01, -3.6504e-02,\n",
      "         1.0802e-02, -2.5081e-02, -6.5181e-02,  8.6339e-02,  3.7305e-02,\n",
      "        -1.2546e-01, -2.3171e-02, -5.1505e-02,  8.1840e-02,  4.9002e-02,\n",
      "         1.8363e-02,  2.9693e-02, -3.8902e-03, -4.1257e-02, -2.2935e-02,\n",
      "         8.3203e-02,  5.9329e-02,  7.7033e-03,  4.9673e-02, -3.4751e-02,\n",
      "        -3.4831e-03, -7.7208e-03,  1.0457e-01, -2.1170e-02,  6.3125e-02,\n",
      "        -1.2047e-02,  1.4499e-02, -5.0847e-02,  2.7684e-02,  8.1270e-02,\n",
      "        -2.4067e-02,  1.6061e-04,  4.5172e-02,  8.9830e-02,  5.0638e-03,\n",
      "        -2.7056e-02,  1.6215e-02, -1.2409e-01,  2.7129e-02, -3.0758e-02,\n",
      "        -4.1683e-02,  7.6068e-03,  1.4988e-02, -2.3955e-02, -8.0970e-02,\n",
      "        -1.0192e-01,  3.6965e-02, -2.6476e-02,  5.7144e-03,  7.6527e-02,\n",
      "         9.9065e-02,  4.3809e-02,  5.6087e-02, -6.8878e-02,  6.4834e-02,\n",
      "         2.2787e-02,  7.3976e-02,  9.9496e-03, -1.7695e-02,  8.8900e-02,\n",
      "        -9.6980e-02, -7.0818e-02,  4.9335e-02, -5.5873e-02, -6.7333e-03,\n",
      "        -1.0160e-02,  2.7102e-02, -2.2473e-02, -4.0724e-02, -4.3555e-02,\n",
      "        -4.3084e-02, -8.1544e-02, -2.8473e-02,  1.8932e-02,  2.8450e-02,\n",
      "        -8.1699e-02, -8.1030e-02,  4.9583e-02,  3.5871e-02, -1.5891e-02,\n",
      "        -8.8298e-03, -1.2130e-02, -8.1447e-02,  4.5123e-02,  6.6769e-02,\n",
      "         4.5007e-02, -1.0901e-02, -1.5257e-01,  1.2816e-02,  6.9188e-02,\n",
      "        -1.3537e-02,  1.0406e-01,  2.4015e-02,  4.3749e-02, -2.3074e-02,\n",
      "         6.2925e-02, -3.2508e-02, -5.4690e-02,  1.6847e-02, -5.7822e-02,\n",
      "         6.0195e-02, -1.6088e-02, -3.6611e-02,  4.3164e-03, -1.6129e-02,\n",
      "        -2.0027e-02,  2.7187e-02, -5.7546e-02,  1.6556e-02, -4.5320e-04,\n",
      "         2.4637e-02, -5.7647e-02, -4.5837e-02,  1.4810e-02,  1.4818e-02,\n",
      "        -9.2751e-03,  3.1316e-02,  4.6298e-02,  1.5679e-02,  2.5335e-02,\n",
      "         1.5162e-02, -6.5274e-02, -1.1448e-01,  3.5900e-02, -1.1034e-01,\n",
      "        -9.4011e-02,  3.3696e-02, -6.7059e-03,  1.4441e-02,  1.3973e-01,\n",
      "         7.2340e-02, -5.2067e-02, -1.5580e-02,  4.3312e-02, -6.7398e-02,\n",
      "         7.6808e-02, -4.1142e-02,  3.2319e-02,  1.2461e-01,  1.5610e-02,\n",
      "         7.3369e-02, -1.0851e-01, -4.5686e-02, -6.5544e-02,  7.0161e-02,\n",
      "        -4.9590e-03,  4.6399e-02,  4.5816e-02, -7.6833e-02,  5.7388e-02,\n",
      "         5.6216e-02,  1.7794e-02, -1.8920e-02, -4.4150e-02,  2.6347e-02,\n",
      "         8.7239e-02, -2.0536e-02, -1.2006e-02, -5.0354e-03,  3.5649e-02,\n",
      "        -8.1056e-02,  5.1311e-02,  1.9925e-02, -4.3425e-02,  2.6601e-02,\n",
      "        -7.5502e-02, -3.4638e-02, -7.5277e-02, -5.1211e-02, -4.9907e-02,\n",
      "         1.9271e-02,  2.3710e-02,  1.7192e-02, -7.7708e-02,  2.5729e-02,\n",
      "         5.5325e-02,  1.0182e-01, -9.2568e-02, -4.8824e-02,  2.3749e-02,\n",
      "         3.6623e-02, -1.6246e-02, -2.5600e-02, -8.7405e-02,  1.7550e-02,\n",
      "        -6.1699e-03, -4.0138e-02, -3.5954e-02, -6.4890e-02,  4.1684e-03,\n",
      "        -8.0014e-02, -7.6652e-02,  9.0478e-02,  2.2696e-03,  4.3178e-03,\n",
      "         1.3625e-01, -4.3848e-02,  3.4243e-02,  1.0695e-01,  2.2553e-02,\n",
      "        -1.3336e-02, -3.6943e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-6.3037e-02,  4.0266e-02, -4.1982e-02, -1.3677e-01, -2.8125e-02,\n",
      "        -3.5907e-02, -1.2441e-01,  3.9413e-02,  7.0257e-02,  2.5938e-02,\n",
      "         1.6168e-02, -4.9772e-03, -8.1783e-02,  5.8682e-02, -5.5678e-03,\n",
      "        -1.6609e-02, -7.0250e-02,  2.3007e-02, -1.1237e-01, -1.1362e-03,\n",
      "         2.6813e-02,  5.1206e-02, -1.0730e-01, -7.5571e-03,  6.9360e-02,\n",
      "        -3.0615e-02, -1.3997e-01, -1.6990e-02, -6.2863e-03,  3.9499e-02,\n",
      "         1.1053e-01,  7.3368e-02, -4.2755e-03, -6.1965e-02,  2.3791e-02,\n",
      "        -5.2535e-02, -2.8180e-02, -1.5272e-02, -2.4229e-02,  8.8722e-02,\n",
      "        -9.6024e-02,  4.9923e-02, -1.7209e-02,  1.6388e-02, -7.5840e-03,\n",
      "        -8.1901e-02,  6.1073e-02, -4.8348e-02, -1.0459e-02, -6.6470e-02,\n",
      "        -2.4781e-02,  5.3203e-02,  2.4020e-02,  4.9423e-02,  3.7947e-02,\n",
      "         2.0673e-01,  6.0895e-02, -4.0758e-02,  2.0071e-02,  1.0387e-01,\n",
      "         1.6163e-02, -1.5090e-02,  6.2854e-02,  2.6011e-02,  3.8561e-02,\n",
      "        -5.5162e-02, -7.5800e-02,  6.2836e-02,  1.3994e-02,  1.1939e-01,\n",
      "        -1.5908e-02,  4.2999e-02, -2.3383e-02,  1.5983e-02, -1.3671e-02,\n",
      "         9.9919e-02,  6.4056e-02, -7.5082e-02, -1.2190e-02, -1.4694e-02,\n",
      "         5.8069e-02,  5.5209e-02,  1.2079e-02,  5.1134e-02,  4.5578e-02,\n",
      "         5.2929e-02, -6.7412e-03,  9.7755e-02, -4.7786e-02, -1.6850e-02,\n",
      "        -5.9766e-02,  9.2122e-02, -2.9754e-02, -1.1698e-01,  2.3706e-02,\n",
      "        -2.3814e-02,  3.3031e-02, -1.1580e-01,  3.8596e-02, -3.3136e-03,\n",
      "        -1.2250e-02,  3.4611e-02, -8.4193e-02, -7.5750e-02, -3.5521e-02,\n",
      "        -5.1473e-03,  3.9007e-02,  6.4325e-03, -5.9280e-02, -1.3100e-02,\n",
      "        -4.1139e-02,  4.7848e-02,  8.4264e-03, -1.0753e-01, -4.3760e-02,\n",
      "         9.4994e-02, -1.7219e-02,  3.9596e-02, -4.1659e-02,  1.2531e-01,\n",
      "        -4.9070e-02,  1.2569e-02, -3.4510e-02,  5.6004e-02, -2.7773e-02,\n",
      "        -8.0413e-02,  7.7013e-02, -3.7365e-02, -7.8601e-02, -4.4590e-02,\n",
      "         1.6158e-02,  2.7064e-02,  1.0510e-01, -1.2408e-02,  1.6963e-02,\n",
      "        -9.1978e-03,  5.7486e-02, -4.6821e-02, -3.0573e-03, -1.0964e-02,\n",
      "        -8.9452e-02,  4.2682e-02, -1.1941e-02,  2.5132e-02, -3.7705e-02,\n",
      "         5.4186e-02, -7.1975e-02, -4.9173e-02, -6.7192e-02,  2.7494e-02,\n",
      "         2.4167e-03,  3.7371e-02,  4.2284e-02,  3.3118e-02,  5.1909e-02,\n",
      "        -6.6921e-02, -5.8869e-02, -6.1932e-02,  3.1455e-02, -2.2885e-02,\n",
      "        -9.3647e-02, -1.9637e-02,  5.1098e-02,  4.5610e-02, -4.1068e-02,\n",
      "         5.7816e-02, -8.5963e-04,  2.2186e-02, -1.8173e-02,  4.3025e-02,\n",
      "        -3.6500e-02,  4.6611e-02,  1.1417e-01, -6.0109e-02, -6.6532e-02,\n",
      "         9.2543e-02,  1.5739e-02, -7.0260e-03, -4.5298e-02, -4.6085e-02,\n",
      "        -1.7641e-02, -3.4245e-02, -2.9982e-02, -3.3564e-02, -2.3251e-02,\n",
      "        -9.0132e-02, -4.9113e-02, -1.5003e-02, -3.4544e-02, -1.2240e-02,\n",
      "        -6.6013e-02, -1.2225e-01,  2.1974e-02, -7.2869e-02,  7.3213e-02,\n",
      "         7.8171e-02, -1.1407e-02,  1.2900e-02,  1.3423e-02,  6.1885e-02,\n",
      "         8.2777e-02,  5.9639e-03, -2.9608e-02,  1.4335e-02, -3.0911e-02,\n",
      "        -2.6568e-02, -7.7970e-02,  5.7262e-02,  7.5148e-03, -8.3736e-02,\n",
      "         1.1164e-01, -3.6595e-02, -3.5647e-02, -2.2155e-02,  3.7071e-02,\n",
      "         5.2191e-03, -5.0187e-02, -1.0465e-02, -2.7389e-02,  2.4710e-02,\n",
      "         3.4442e-02, -3.3596e-02,  5.7857e-02,  4.2296e-02, -2.8121e-02,\n",
      "         3.7366e-02, -5.9914e-02,  1.6653e-02,  3.8050e-02,  5.3976e-02,\n",
      "         1.6561e-02,  5.0949e-02,  7.7352e-02,  8.3561e-02, -4.3670e-02,\n",
      "        -8.8957e-03,  2.0743e-03,  3.0768e-02, -3.4656e-02,  1.0132e-01,\n",
      "         2.0802e-02, -1.4734e-01, -1.1625e-02, -4.6762e-03,  1.0868e-01,\n",
      "         8.2071e-02, -1.4927e-02, -1.5449e-01, -7.1360e-02,  6.3504e-02,\n",
      "        -1.3678e-02, -3.2650e-02,  8.5200e-02, -4.5086e-02,  2.2611e-02,\n",
      "        -1.0392e-01, -6.0944e-02,  1.4738e-02,  3.9227e-02, -8.7592e-03,\n",
      "        -2.2234e-02, -5.5263e-03, -3.3027e-02,  3.9625e-03,  1.5417e-02,\n",
      "         1.2909e-02,  1.0592e-01, -5.5637e-02,  1.6255e-01, -8.2178e-02,\n",
      "         9.2043e-02,  1.9381e-03,  2.2714e-02,  3.5822e-02, -1.0901e-03,\n",
      "         1.2325e-02, -6.4859e-02, -2.5885e-02,  5.1314e-02, -4.6941e-04,\n",
      "        -2.8895e-03,  1.1293e-02, -1.7513e-02, -6.6949e-02,  6.9416e-02,\n",
      "         7.1142e-03, -1.4641e-03, -3.6779e-02,  1.1385e-01, -4.7641e-02,\n",
      "         1.4738e-02, -6.2718e-02,  8.7415e-02, -5.5629e-03,  2.7129e-02,\n",
      "         6.3722e-03,  3.4799e-02,  2.5760e-02, -7.6286e-02, -6.1321e-02,\n",
      "        -5.3081e-02, -1.3048e-03, -1.7442e-02, -1.6667e-01,  3.7299e-03,\n",
      "         1.3328e-02,  6.2362e-02,  1.6265e-02,  5.9280e-02, -9.6899e-02,\n",
      "        -9.9530e-03,  3.4732e-02,  5.4185e-03, -4.3835e-03,  3.4801e-02,\n",
      "         4.0341e-02, -1.1303e-02, -2.8805e-02,  2.6510e-02, -4.8988e-02,\n",
      "        -1.4906e-02, -8.7503e-02, -3.8591e-03,  3.9093e-02,  2.1345e-02,\n",
      "         4.3803e-02, -4.8825e-02, -3.8691e-02, -7.1864e-02, -5.9994e-02,\n",
      "         2.5898e-02, -4.4769e-02,  8.8324e-02, -7.2772e-02,  1.5155e-02,\n",
      "        -5.5817e-02,  5.3736e-02, -2.9101e-02,  1.5793e-03, -1.7930e-01,\n",
      "        -1.7445e-02, -6.8678e-02, -2.1378e-02, -4.4950e-02, -1.7106e-02,\n",
      "         1.5411e-01,  7.0336e-02,  3.1394e-02,  9.1400e-02, -6.3379e-02,\n",
      "         9.3097e-02,  6.2873e-02, -2.3895e-02,  4.8823e-02,  1.5050e-02,\n",
      "         1.5749e-01,  2.0483e-02,  2.5478e-02,  1.2565e-01,  6.4963e-02,\n",
      "        -3.3720e-02,  3.8453e-02, -6.7775e-02, -1.1753e-01,  6.8093e-02,\n",
      "         5.1249e-02, -1.5064e-01, -6.5369e-02,  4.8224e-02, -8.1458e-03,\n",
      "        -2.7762e-02, -2.5249e-02, -1.0149e-02, -1.9384e-02,  4.1005e-02,\n",
      "        -2.7609e-02, -9.2976e-02,  3.8276e-02,  7.2089e-02, -1.2936e-01,\n",
      "        -1.1778e-01, -6.5505e-02,  1.7166e-02,  1.5751e-02, -1.9162e-02,\n",
      "         5.5185e-03, -1.0558e-01, -2.3025e-02, -1.4394e-01,  1.1885e-01,\n",
      "         9.7875e-03, -9.7859e-02, -3.9622e-02, -4.5969e-02, -4.3369e-02,\n",
      "        -2.5617e-02, -5.2712e-02,  3.9468e-02,  1.0800e-01,  5.3185e-02,\n",
      "        -5.0451e-02,  5.3125e-02,  1.4214e-01,  1.0340e-01, -1.7702e-02,\n",
      "        -6.3901e-02,  3.0720e-02, -7.3908e-02,  9.5226e-02,  6.2002e-03,\n",
      "         5.0914e-02, -6.5561e-02,  5.4568e-02,  5.1027e-02, -4.2785e-02,\n",
      "        -7.9318e-02,  6.1157e-02,  6.2453e-02, -4.5603e-02, -2.7345e-02,\n",
      "        -5.6974e-02,  1.2981e-01,  1.0213e-01,  4.7302e-02, -2.4651e-02,\n",
      "        -3.3669e-02, -4.9926e-02,  7.3012e-02, -3.4709e-02,  1.2907e-01,\n",
      "         6.1702e-02,  3.1375e-02,  1.9113e-02, -9.1100e-02,  7.3931e-03,\n",
      "        -8.0293e-02, -3.6101e-02,  5.1210e-02, -2.9621e-02,  5.9973e-03,\n",
      "         9.6392e-02,  4.1492e-03,  2.3054e-02, -8.5028e-02,  1.3075e-03,\n",
      "        -8.0786e-02,  7.1889e-02, -3.7784e-02,  2.0823e-02, -5.1179e-02,\n",
      "         1.3547e-01,  4.0677e-02,  6.0206e-02, -4.4987e-03, -1.4705e-02,\n",
      "        -4.3924e-05,  2.2686e-02, -3.4385e-02, -3.4656e-02, -1.7687e-01,\n",
      "         4.2150e-02, -3.2622e-03, -4.4221e-02, -5.9327e-02, -1.2178e-01,\n",
      "        -9.8243e-02,  2.9285e-02,  1.0800e-01, -2.9136e-02, -1.2633e-02,\n",
      "        -8.9605e-02,  1.0191e-02,  2.9528e-02, -1.6184e-02, -2.1323e-02,\n",
      "         4.3191e-02, -5.9493e-02, -9.2964e-02, -2.2478e-02,  1.4769e-02,\n",
      "        -2.1768e-02,  4.5379e-02,  7.2459e-02,  6.8969e-02, -3.1864e-02,\n",
      "         9.2427e-03,  1.1675e-01, -1.2651e-02, -8.6167e-02, -7.0927e-02,\n",
      "        -3.2216e-02, -3.6091e-02,  1.1292e-02, -2.3667e-02,  1.0530e-01,\n",
      "        -2.7349e-02,  3.5006e-02, -8.4804e-02,  5.3443e-02, -3.9848e-02,\n",
      "        -1.8628e-02, -9.9607e-02, -1.0862e-01,  3.0266e-03,  6.8604e-02,\n",
      "        -2.6277e-02, -1.4869e-01, -4.6595e-02, -9.2243e-02,  5.9800e-02,\n",
      "         1.6846e-02, -5.4605e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.279471/ 42.121407, val:  46.67%, val_best:  46.67%, tr:  36.26%, tr_best:  36.26%, epoch time: 59.20 seconds, 0.99 minutes\n",
      "[module.layers.3] weight_fb parameter count: 5,120\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  1.658807/ 29.205597, val:  53.75%, val_best:  53.75%, tr:  53.93%, tr_best:  53.93%, epoch time: 57.72 seconds, 0.96 minutes\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  1.504344/ 35.155834, val:  51.25%, val_best:  53.75%, tr:  55.46%, tr_best:  55.46%, epoch time: 53.42 seconds, 0.89 minutes\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  1.390627/ 36.494400, val:  50.83%, val_best:  53.75%, tr:  58.02%, tr_best:  58.02%, epoch time: 54.63 seconds, 0.91 minutes\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  1.320954/ 26.843994, val:  57.92%, val_best:  57.92%, tr:  62.92%, tr_best:  62.92%, epoch time: 53.73 seconds, 0.90 minutes\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  1.301773/ 35.794823, val:  52.92%, val_best:  57.92%, tr:  61.80%, tr_best:  62.92%, epoch time: 55.74 seconds, 0.93 minutes\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  1.209148/ 71.745056, val:  40.83%, val_best:  57.92%, tr:  64.66%, tr_best:  64.66%, epoch time: 54.92 seconds, 0.92 minutes\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  1.107968/ 59.033966, val:  49.17%, val_best:  57.92%, tr:  66.09%, tr_best:  66.09%, epoch time: 53.90 seconds, 0.90 minutes\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  1.131251/ 51.331032, val:  52.92%, val_best:  57.92%, tr:  67.62%, tr_best:  67.62%, epoch time: 54.07 seconds, 0.90 minutes\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  1.042278/ 26.352043, val:  59.58%, val_best:  59.58%, tr:  68.54%, tr_best:  68.54%, epoch time: 56.17 seconds, 0.94 minutes\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.954385/ 63.813667, val:  47.08%, val_best:  59.58%, tr:  71.81%, tr_best:  71.81%, epoch time: 52.44 seconds, 0.87 minutes\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.973396/ 65.639763, val:  56.25%, val_best:  59.58%, tr:  71.50%, tr_best:  71.81%, epoch time: 53.20 seconds, 0.89 minutes\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.823460/ 24.973511, val:  65.42%, val_best:  65.42%, tr:  74.46%, tr_best:  74.46%, epoch time: 54.22 seconds, 0.90 minutes\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.798941/ 53.374874, val:  54.17%, val_best:  65.42%, tr:  76.61%, tr_best:  76.61%, epoch time: 53.67 seconds, 0.89 minutes\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.793283/ 58.193836, val:  50.83%, val_best:  65.42%, tr:  75.08%, tr_best:  76.61%, epoch time: 54.12 seconds, 0.90 minutes\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.758545/ 36.129631, val:  55.83%, val_best:  65.42%, tr:  77.63%, tr_best:  77.63%, epoch time: 53.75 seconds, 0.90 minutes\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.757856/ 50.853764, val:  52.08%, val_best:  65.42%, tr:  78.04%, tr_best:  78.04%, epoch time: 54.58 seconds, 0.91 minutes\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.641800/ 59.697441, val:  53.33%, val_best:  65.42%, tr:  79.88%, tr_best:  79.88%, epoch time: 54.72 seconds, 0.91 minutes\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.673426/ 61.339474, val:  60.00%, val_best:  65.42%, tr:  79.37%, tr_best:  79.88%, epoch time: 55.20 seconds, 0.92 minutes\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.570829/ 36.377548, val:  65.83%, val_best:  65.83%, tr:  81.00%, tr_best:  81.00%, epoch time: 53.29 seconds, 0.89 minutes\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.617445/ 46.207592, val:  62.08%, val_best:  65.83%, tr:  82.02%, tr_best:  82.02%, epoch time: 54.29 seconds, 0.90 minutes\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.543211/ 47.579178, val:  60.00%, val_best:  65.83%, tr:  83.45%, tr_best:  83.45%, epoch time: 54.64 seconds, 0.91 minutes\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.557142/ 41.553246, val:  62.50%, val_best:  65.83%, tr:  84.17%, tr_best:  84.17%, epoch time: 55.72 seconds, 0.93 minutes\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.521403/ 41.025230, val:  65.42%, val_best:  65.83%, tr:  83.66%, tr_best:  84.17%, epoch time: 53.25 seconds, 0.89 minutes\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.491134/ 54.708927, val:  55.83%, val_best:  65.83%, tr:  84.98%, tr_best:  84.98%, epoch time: 53.56 seconds, 0.89 minutes\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.504806/ 68.300285, val:  60.83%, val_best:  65.83%, tr:  83.66%, tr_best:  84.98%, epoch time: 55.00 seconds, 0.92 minutes\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.462218/ 33.344131, val:  69.58%, val_best:  69.58%, tr:  86.41%, tr_best:  86.41%, epoch time: 52.89 seconds, 0.88 minutes\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.418571/ 53.438259, val:  53.75%, val_best:  69.58%, tr:  86.93%, tr_best:  86.93%, epoch time: 53.76 seconds, 0.90 minutes\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.375959/ 34.219860, val:  61.25%, val_best:  69.58%, tr:  88.15%, tr_best:  88.15%, epoch time: 54.81 seconds, 0.91 minutes\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.377065/ 38.292343, val:  62.92%, val_best:  69.58%, tr:  88.15%, tr_best:  88.15%, epoch time: 52.52 seconds, 0.88 minutes\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.350310/ 42.001675, val:  64.58%, val_best:  69.58%, tr:  87.64%, tr_best:  88.15%, epoch time: 52.88 seconds, 0.88 minutes\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.316130/ 48.229847, val:  60.42%, val_best:  69.58%, tr:  89.79%, tr_best:  89.79%, epoch time: 55.66 seconds, 0.93 minutes\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.333923/ 56.318504, val:  57.08%, val_best:  69.58%, tr:  88.87%, tr_best:  89.79%, epoch time: 53.09 seconds, 0.88 minutes\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.269431/ 39.670483, val:  61.67%, val_best:  69.58%, tr:  91.83%, tr_best:  91.83%, epoch time: 53.28 seconds, 0.89 minutes\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.281025/ 42.699528, val:  60.00%, val_best:  69.58%, tr:  89.79%, tr_best:  91.83%, epoch time: 54.37 seconds, 0.91 minutes\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.242357/ 42.546021, val:  64.17%, val_best:  69.58%, tr:  91.83%, tr_best:  91.83%, epoch time: 52.21 seconds, 0.87 minutes\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.255896/ 46.779591, val:  62.50%, val_best:  69.58%, tr:  90.91%, tr_best:  91.83%, epoch time: 53.03 seconds, 0.88 minutes\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.255412/ 63.177608, val:  59.58%, val_best:  69.58%, tr:  91.32%, tr_best:  91.83%, epoch time: 52.56 seconds, 0.88 minutes\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.273519/ 49.586723, val:  59.17%, val_best:  69.58%, tr:  91.52%, tr_best:  91.83%, epoch time: 53.75 seconds, 0.90 minutes\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.197299/ 39.201286, val:  65.83%, val_best:  69.58%, tr:  93.16%, tr_best:  93.16%, epoch time: 53.55 seconds, 0.89 minutes\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.196552/ 61.164051, val:  60.00%, val_best:  69.58%, tr:  93.16%, tr_best:  93.16%, epoch time: 54.19 seconds, 0.90 minutes\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.187141/ 43.018532, val:  62.92%, val_best:  69.58%, tr:  94.18%, tr_best:  94.18%, epoch time: 52.93 seconds, 0.88 minutes\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.153163/ 53.006973, val:  61.25%, val_best:  69.58%, tr:  94.38%, tr_best:  94.38%, epoch time: 54.60 seconds, 0.91 minutes\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.206986/ 49.460667, val:  64.58%, val_best:  69.58%, tr:  92.24%, tr_best:  94.38%, epoch time: 54.90 seconds, 0.92 minutes\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.142582/ 36.603832, val:  65.83%, val_best:  69.58%, tr:  94.38%, tr_best:  94.38%, epoch time: 53.90 seconds, 0.90 minutes\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.161120/ 44.252953, val:  64.58%, val_best:  69.58%, tr:  94.79%, tr_best:  94.79%, epoch time: 53.04 seconds, 0.88 minutes\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.147834/ 43.284946, val:  63.75%, val_best:  69.58%, tr:  94.99%, tr_best:  94.99%, epoch time: 54.45 seconds, 0.91 minutes\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.142231/ 46.224842, val:  62.08%, val_best:  69.58%, tr:  96.32%, tr_best:  96.32%, epoch time: 54.76 seconds, 0.91 minutes\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.138632/ 47.046215, val:  65.83%, val_best:  69.58%, tr:  95.81%, tr_best:  96.32%, epoch time: 53.97 seconds, 0.90 minutes\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.131275/ 42.399193, val:  63.75%, val_best:  69.58%, tr:  94.69%, tr_best:  96.32%, epoch time: 54.52 seconds, 0.91 minutes\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.123764/ 47.134533, val:  65.00%, val_best:  69.58%, tr:  95.91%, tr_best:  96.32%, epoch time: 53.66 seconds, 0.89 minutes\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.117534/ 42.578854, val:  66.67%, val_best:  69.58%, tr:  96.53%, tr_best:  96.53%, epoch time: 53.08 seconds, 0.88 minutes\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.118766/ 53.834915, val:  55.83%, val_best:  69.58%, tr:  95.91%, tr_best:  96.53%, epoch time: 54.31 seconds, 0.91 minutes\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.132710/ 54.371838, val:  60.00%, val_best:  69.58%, tr:  95.81%, tr_best:  96.53%, epoch time: 53.34 seconds, 0.89 minutes\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.103969/ 51.162033, val:  63.75%, val_best:  69.58%, tr:  96.22%, tr_best:  96.53%, epoch time: 54.98 seconds, 0.92 minutes\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.099969/ 46.901302, val:  64.17%, val_best:  69.58%, tr:  96.42%, tr_best:  96.53%, epoch time: 52.14 seconds, 0.87 minutes\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.077755/ 61.931171, val:  60.83%, val_best:  69.58%, tr:  97.14%, tr_best:  97.14%, epoch time: 51.90 seconds, 0.86 minutes\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.081007/ 49.476696, val:  65.42%, val_best:  69.58%, tr:  96.63%, tr_best:  97.14%, epoch time: 54.06 seconds, 0.90 minutes\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.081141/ 41.618698, val:  70.00%, val_best:  70.00%, tr:  97.55%, tr_best:  97.55%, epoch time: 55.75 seconds, 0.93 minutes\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.066083/ 52.879223, val:  64.17%, val_best:  70.00%, tr:  97.65%, tr_best:  97.65%, epoch time: 52.17 seconds, 0.87 minutes\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.081799/ 47.634163, val:  65.42%, val_best:  70.00%, tr:  96.63%, tr_best:  97.65%, epoch time: 54.50 seconds, 0.91 minutes\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.096593/ 49.719727, val:  62.08%, val_best:  70.00%, tr:  96.83%, tr_best:  97.65%, epoch time: 53.73 seconds, 0.90 minutes\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.050365/ 47.546928, val:  64.17%, val_best:  70.00%, tr:  98.37%, tr_best:  98.37%, epoch time: 53.47 seconds, 0.89 minutes\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.057505/ 44.746227, val:  63.33%, val_best:  70.00%, tr:  98.26%, tr_best:  98.37%, epoch time: 53.58 seconds, 0.89 minutes\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.066582/ 48.165501, val:  64.58%, val_best:  70.00%, tr:  97.34%, tr_best:  98.37%, epoch time: 53.65 seconds, 0.89 minutes\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.066873/ 49.660854, val:  64.17%, val_best:  70.00%, tr:  97.55%, tr_best:  98.37%, epoch time: 53.39 seconds, 0.89 minutes\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.051529/ 46.664448, val:  65.83%, val_best:  70.00%, tr:  98.06%, tr_best:  98.37%, epoch time: 53.92 seconds, 0.90 minutes\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.049283/ 51.279594, val:  65.00%, val_best:  70.00%, tr:  97.85%, tr_best:  98.37%, epoch time: 53.44 seconds, 0.89 minutes\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.058854/ 50.473320, val:  62.92%, val_best:  70.00%, tr:  98.26%, tr_best:  98.37%, epoch time: 53.17 seconds, 0.89 minutes\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.046058/ 44.722435, val:  67.92%, val_best:  70.00%, tr:  98.47%, tr_best:  98.47%, epoch time: 56.13 seconds, 0.94 minutes\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.049839/ 58.354187, val:  61.25%, val_best:  70.00%, tr:  98.06%, tr_best:  98.47%, epoch time: 53.37 seconds, 0.89 minutes\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.036128/ 49.558125, val:  66.67%, val_best:  70.00%, tr:  99.08%, tr_best:  99.08%, epoch time: 54.39 seconds, 0.91 minutes\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.026920/ 42.765839, val:  66.25%, val_best:  70.00%, tr:  99.39%, tr_best:  99.39%, epoch time: 53.59 seconds, 0.89 minutes\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.025163/ 51.157589, val:  63.33%, val_best:  70.00%, tr:  99.08%, tr_best:  99.39%, epoch time: 53.53 seconds, 0.89 minutes\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.026713/ 39.697037, val:  67.50%, val_best:  70.00%, tr:  99.18%, tr_best:  99.39%, epoch time: 54.48 seconds, 0.91 minutes\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.023988/ 49.558079, val:  62.08%, val_best:  70.00%, tr:  99.49%, tr_best:  99.49%, epoch time: 54.76 seconds, 0.91 minutes\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.039400/ 39.724102, val:  68.75%, val_best:  70.00%, tr:  99.08%, tr_best:  99.49%, epoch time: 53.48 seconds, 0.89 minutes\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.023214/ 60.235245, val:  59.17%, val_best:  70.00%, tr:  99.39%, tr_best:  99.49%, epoch time: 52.80 seconds, 0.88 minutes\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.026812/ 43.953579, val:  67.08%, val_best:  70.00%, tr:  99.39%, tr_best:  99.49%, epoch time: 54.64 seconds, 0.91 minutes\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.009114/ 44.993793, val:  69.58%, val_best:  70.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 52.30 seconds, 0.87 minutes\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.016390/ 48.384235, val:  64.58%, val_best:  70.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 53.68 seconds, 0.89 minutes\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.011167/ 55.640179, val:  64.17%, val_best:  70.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 54.02 seconds, 0.90 minutes\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.011203/ 43.005051, val:  70.42%, val_best:  70.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 53.65 seconds, 0.89 minutes\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.013247/ 41.769409, val:  69.58%, val_best:  70.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 51.58 seconds, 0.86 minutes\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.013835/ 41.361359, val:  67.50%, val_best:  70.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 53.68 seconds, 0.89 minutes\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.014425/ 43.193295, val:  67.08%, val_best:  70.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 52.46 seconds, 0.87 minutes\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.017744/ 52.214989, val:  66.25%, val_best:  70.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 53.55 seconds, 0.89 minutes\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.020940/ 45.744942, val:  64.17%, val_best:  70.42%, tr:  99.28%, tr_best:  99.90%, epoch time: 54.11 seconds, 0.90 minutes\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.027466/ 54.821873, val:  62.50%, val_best:  70.42%, tr:  99.08%, tr_best:  99.90%, epoch time: 52.98 seconds, 0.88 minutes\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.016664/ 48.952999, val:  66.67%, val_best:  70.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 52.47 seconds, 0.87 minutes\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.014635/ 46.266190, val:  66.67%, val_best:  70.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 52.31 seconds, 0.87 minutes\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.024812/ 48.940628, val:  65.83%, val_best:  70.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 51.33 seconds, 0.86 minutes\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.039942/ 60.081871, val:  58.75%, val_best:  70.42%, tr:  98.67%, tr_best:  99.90%, epoch time: 53.70 seconds, 0.89 minutes\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.028982/ 63.013027, val:  62.08%, val_best:  70.42%, tr:  99.08%, tr_best:  99.90%, epoch time: 52.89 seconds, 0.88 minutes\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.026003/ 47.723141, val:  68.33%, val_best:  70.42%, tr:  99.28%, tr_best:  99.90%, epoch time: 52.25 seconds, 0.87 minutes\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.012817/ 49.990303, val:  66.25%, val_best:  70.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 55.03 seconds, 0.92 minutes\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.007382/ 46.885433, val:  67.08%, val_best:  70.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 52.46 seconds, 0.87 minutes\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.011318/ 49.323784, val:  65.00%, val_best:  70.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 50.67 seconds, 0.84 minutes\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.025585/ 50.470993, val:  66.25%, val_best:  70.42%, tr:  99.28%, tr_best:  99.90%, epoch time: 52.53 seconds, 0.88 minutes\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.012605/ 54.476048, val:  65.42%, val_best:  70.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 50.86 seconds, 0.85 minutes\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  0.030900/ 43.295044, val:  67.08%, val_best:  70.42%, tr:  98.98%, tr_best:  99.90%, epoch time: 53.84 seconds, 0.90 minutes\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  0.019020/ 48.881672, val:  64.17%, val_best:  70.42%, tr:  99.49%, tr_best:  99.90%, epoch time: 52.30 seconds, 0.87 minutes\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  0.017133/ 50.395809, val:  67.92%, val_best:  70.42%, tr:  99.49%, tr_best:  99.90%, epoch time: 51.45 seconds, 0.86 minutes\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  0.026793/ 53.899403, val:  61.67%, val_best:  70.42%, tr:  99.28%, tr_best:  99.90%, epoch time: 51.68 seconds, 0.86 minutes\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  0.023238/ 48.500389, val:  66.67%, val_best:  70.42%, tr:  99.39%, tr_best:  99.90%, epoch time: 52.29 seconds, 0.87 minutes\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  0.018941/ 51.563206, val:  64.17%, val_best:  70.42%, tr:  99.49%, tr_best:  99.90%, epoch time: 52.27 seconds, 0.87 minutes\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  0.021478/ 49.247379, val:  66.25%, val_best:  70.42%, tr:  99.28%, tr_best:  99.90%, epoch time: 50.21 seconds, 0.84 minutes\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  0.019645/ 53.032131, val:  61.25%, val_best:  70.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 53.59 seconds, 0.89 minutes\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  0.019187/ 44.790081, val:  64.17%, val_best:  70.42%, tr:  99.28%, tr_best:  99.90%, epoch time: 52.43 seconds, 0.87 minutes\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  0.030294/ 46.886147, val:  66.25%, val_best:  70.42%, tr:  98.98%, tr_best:  99.90%, epoch time: 52.34 seconds, 0.87 minutes\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  0.011480/ 45.074009, val:  67.50%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 53.06 seconds, 0.88 minutes\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  0.010710/ 46.093693, val:  69.17%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 50.59 seconds, 0.84 minutes\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  0.005782/ 47.035458, val:  67.08%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 53.96 seconds, 0.90 minutes\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  0.009554/ 46.849400, val:  67.92%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 53.42 seconds, 0.89 minutes\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  0.010730/ 45.224247, val:  66.67%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 52.54 seconds, 0.88 minutes\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  0.015476/ 50.562836, val:  64.58%, val_best:  70.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 53.57 seconds, 0.89 minutes\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  0.021021/ 49.836647, val:  67.50%, val_best:  70.42%, tr:  99.39%, tr_best: 100.00%, epoch time: 52.55 seconds, 0.88 minutes\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  0.006616/ 50.867325, val:  67.50%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.80 seconds, 0.86 minutes\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  0.011474/ 53.290489, val:  63.75%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 54.09 seconds, 0.90 minutes\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  0.022090/ 51.957989, val:  63.75%, val_best:  70.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 52.99 seconds, 0.88 minutes\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  0.015422/ 46.023216, val:  67.50%, val_best:  70.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 53.80 seconds, 0.90 minutes\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  0.010505/ 48.494785, val:  66.25%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 51.56 seconds, 0.86 minutes\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  0.010836/ 48.520092, val:  66.67%, val_best:  70.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 51.54 seconds, 0.86 minutes\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  0.007000/ 49.227551, val:  66.25%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 51.63 seconds, 0.86 minutes\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  0.006140/ 53.355949, val:  66.25%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 52.41 seconds, 0.87 minutes\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  0.006282/ 53.523811, val:  66.25%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 50.47 seconds, 0.84 minutes\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  0.006224/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 52.64 seconds, 0.88 minutes\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.58 seconds, 0.86 minutes\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.76 seconds, 0.86 minutes\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 52.47 seconds, 0.87 minutes\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.25 seconds, 0.85 minutes\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.71 seconds, 0.86 minutes\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 49.85 seconds, 0.83 minutes\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.09 seconds, 0.85 minutes\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 52.80 seconds, 0.88 minutes\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.01 seconds, 0.85 minutes\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.08 seconds, 0.83 minutes\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 52.80 seconds, 0.88 minutes\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 52.79 seconds, 0.88 minutes\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.72 seconds, 0.86 minutes\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 52.13 seconds, 0.87 minutes\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.15 seconds, 0.85 minutes\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.84 seconds, 0.86 minutes\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 52.08 seconds, 0.87 minutes\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.03 seconds, 0.83 minutes\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.52 seconds, 0.84 minutes\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 49.99 seconds, 0.83 minutes\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 49.44 seconds, 0.82 minutes\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.34 seconds, 0.84 minutes\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.44 seconds, 0.84 minutes\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 49.05 seconds, 0.82 minutes\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.14 seconds, 0.84 minutes\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 52.00 seconds, 0.87 minutes\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 49.62 seconds, 0.83 minutes\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 49.54 seconds, 0.83 minutes\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 49.26 seconds, 0.82 minutes\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.55 seconds, 0.84 minutes\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.41 seconds, 0.84 minutes\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.11 seconds, 0.85 minutes\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 52.40 seconds, 0.87 minutes\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.04 seconds, 0.83 minutes\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.25 seconds, 0.84 minutes\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 49.59 seconds, 0.83 minutes\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.22 seconds, 0.84 minutes\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.59 seconds, 0.84 minutes\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 49.24 seconds, 0.82 minutes\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 49.63 seconds, 0.83 minutes\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 49.23 seconds, 0.82 minutes\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.96 seconds, 0.85 minutes\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.11 seconds, 0.84 minutes\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.15 seconds, 0.85 minutes\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.84 seconds, 0.85 minutes\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.17 seconds, 0.85 minutes\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.10 seconds, 0.85 minutes\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 49.22 seconds, 0.82 minutes\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.74 seconds, 0.85 minutes\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.07 seconds, 0.83 minutes\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.87 seconds, 0.86 minutes\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.54 seconds, 0.84 minutes\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 52.23 seconds, 0.87 minutes\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.85 seconds, 0.85 minutes\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.26 seconds, 0.85 minutes\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.63 seconds, 0.86 minutes\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.73 seconds, 0.86 minutes\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 52.10 seconds, 0.87 minutes\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 52.63 seconds, 0.88 minutes\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 53.75 seconds, 0.90 minutes\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.13 seconds, 0.85 minutes\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 52.54 seconds, 0.88 minutes\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 52.67 seconds, 0.88 minutes\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 52.53 seconds, 0.88 minutes\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.93 seconds, 0.85 minutes\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.29 seconds, 0.85 minutes\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.95 seconds, 0.87 minutes\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.38 seconds, 0.86 minutes\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.56 seconds, 0.84 minutes\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 49.33 seconds, 0.82 minutes\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 50.24 seconds, 0.84 minutes\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.30 seconds, 0.85 minutes\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  0.004466/ 50.616745, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 51.97 seconds, 0.87 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c64ddb77cf4cb5a1660e3ac5801e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÇ‚ñà‚ñÇ‚ñÑ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00447</td></tr><tr><td>val_acc_best</td><td>0.70417</td></tr><tr><td>val_acc_now</td><td>0.66667</td></tr><tr><td>val_loss</td><td>50.61674</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-sweep-23</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tftl4e1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tftl4e1o</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251112_174338-tftl4e1o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 20g84q0a with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [512]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 5000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_threshold: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: one\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251112_203942-20g84q0a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/20g84q0a' target=\"_blank\">driven-sweep-37</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/1qaukqa6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/1qaukqa6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/1qaukqa6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/1qaukqa6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/20g84q0a' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/20g84q0a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': False, 'unique_name': '20251112_203950_609', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 0.001, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [512], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'one', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'output_threshold': 0.5, 'random_select_ratio': 1.75} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 7a22c8a0ef5b9b252dbf98632e270efd\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=512, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.1, v_reset=10000, sg_width=0.001, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=False, ANPI_MODE=True)\n",
      "      (4): SYNAPSE_FC(in_features=512, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (DFA_top): Top_Gradient(single_step=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 506,880\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "inFeed spike.shape torch.Size([10, 512]) self.weight_fb.shape torch.Size([10, 512])\n",
      "self.weight_fb[0] tensor([ 1.2009e-02,  1.3379e-01, -1.0650e-02,  5.2556e-02, -1.1912e-01,\n",
      "         4.0419e-02, -4.0199e-02, -5.0604e-02,  3.2680e-02, -7.8942e-02,\n",
      "        -1.0288e-01, -1.8775e-02, -5.7299e-03,  1.2332e-02, -6.9353e-02,\n",
      "         1.1499e-01, -4.4228e-02,  4.2593e-02,  4.9323e-02, -2.0675e-03,\n",
      "         9.2336e-02, -3.1971e-02, -1.5728e-02,  9.1276e-02, -2.0181e-02,\n",
      "        -7.1800e-02,  1.4578e-01, -4.2861e-02,  1.1373e-02, -7.3257e-02,\n",
      "        -1.1159e-01, -9.7846e-02,  5.1912e-02,  8.7845e-02,  4.0044e-02,\n",
      "         2.6324e-02, -9.8372e-02,  3.8522e-02,  1.0460e-01, -4.1150e-02,\n",
      "         5.8342e-02,  4.8482e-03,  5.2401e-03, -8.7172e-03,  2.0523e-02,\n",
      "        -3.6457e-02, -6.6373e-02,  5.9048e-03, -2.0717e-02, -3.2546e-02,\n",
      "        -5.4324e-02,  2.4378e-02,  1.0149e-02, -1.2236e-02,  6.2543e-02,\n",
      "        -8.3454e-02, -2.1650e-02, -3.9879e-02,  2.7655e-02, -3.3246e-02,\n",
      "         7.6898e-02, -5.0422e-02,  1.5484e-02, -2.6447e-02,  6.8359e-02,\n",
      "        -6.8262e-02,  3.4312e-02, -7.9518e-02, -2.3619e-02,  3.1812e-02,\n",
      "         6.2016e-03,  1.6009e-02,  2.2387e-02,  1.4105e-01,  1.4450e-03,\n",
      "         9.7970e-02, -7.1751e-02,  5.8704e-02, -2.8309e-02,  4.7077e-02,\n",
      "        -3.5820e-02, -4.3640e-02, -4.4777e-02, -3.1386e-02, -2.7226e-02,\n",
      "        -2.5884e-02,  1.0779e-02,  2.7401e-02,  3.1376e-02, -7.5319e-02,\n",
      "        -1.6829e-02,  1.7118e-02, -8.9122e-02, -4.0006e-02,  4.6343e-03,\n",
      "         1.2001e-02,  3.6892e-02,  1.4373e-02,  7.0655e-02, -4.2197e-02,\n",
      "        -1.0233e-01,  3.7360e-04,  8.5512e-02,  7.8637e-02,  1.4384e-03,\n",
      "        -8.0477e-02, -4.6482e-02,  2.3251e-02, -3.3886e-02, -2.4537e-03,\n",
      "        -4.8149e-02, -1.5486e-01,  4.3330e-02, -5.8045e-03, -1.3386e-02,\n",
      "         2.7755e-02, -1.9510e-02,  1.3393e-03,  3.8708e-02,  1.5263e-02,\n",
      "         4.6335e-02, -7.2374e-03, -6.3238e-03, -3.1016e-02, -3.1252e-02,\n",
      "        -7.4723e-02, -1.5088e-02, -4.1994e-02,  1.2212e-02,  6.0550e-02,\n",
      "        -1.7745e-03,  1.0415e-01,  6.7522e-02, -6.1409e-02, -4.1550e-02,\n",
      "         1.0644e-01,  1.5230e-01, -3.8367e-02,  7.8697e-02, -1.7323e-02,\n",
      "         2.6986e-02,  2.6370e-02,  6.5894e-02, -1.2553e-01, -3.9156e-02,\n",
      "         1.3065e-01, -5.8646e-03,  1.4600e-02, -4.5190e-02, -1.0434e-01,\n",
      "         5.6415e-02,  4.8810e-02, -3.8917e-02,  1.3367e-01,  7.2065e-02,\n",
      "        -2.6348e-02,  1.4814e-02, -7.9086e-02, -7.4679e-03, -3.7547e-02,\n",
      "        -4.9995e-02,  1.3292e-04, -1.2034e-02,  4.6384e-02,  5.0249e-02,\n",
      "         5.1038e-02, -3.7747e-02,  8.0393e-02, -6.6428e-02, -1.4425e-03,\n",
      "        -2.2637e-02, -3.0118e-02,  9.2677e-03, -9.3434e-02,  1.9207e-02,\n",
      "        -2.7770e-02, -6.7883e-02, -7.8605e-02, -9.7644e-02, -9.8327e-02,\n",
      "        -4.0612e-02,  4.7043e-02, -3.7591e-02,  1.8712e-02, -8.3181e-02,\n",
      "        -1.9715e-02,  3.6721e-02,  3.5419e-02, -4.6781e-02, -7.8367e-03,\n",
      "        -2.6748e-02, -8.6308e-02,  2.3989e-02, -1.2710e-02,  3.7118e-02,\n",
      "        -6.2088e-02, -2.2962e-04, -4.9640e-02,  2.4384e-02,  1.5691e-01,\n",
      "         1.5421e-02,  5.5528e-02,  4.8312e-02,  5.6640e-02, -2.2735e-02,\n",
      "         5.3113e-03, -5.2211e-02,  2.6325e-02,  6.9295e-02,  2.4738e-02,\n",
      "        -5.3518e-03,  5.2276e-02, -2.4634e-02, -5.3242e-03,  1.2084e-01,\n",
      "        -2.6133e-02,  3.3964e-02,  9.2582e-03, -1.2223e-01, -2.1360e-03,\n",
      "        -7.8244e-02, -1.5748e-02,  1.4439e-03,  1.2431e-01,  6.0634e-02,\n",
      "         8.5934e-02, -6.0989e-02, -2.9897e-02, -1.1970e-03, -1.0762e-01,\n",
      "         1.0423e-02,  1.6176e-02, -1.3812e-02, -5.2755e-02,  1.6920e-02,\n",
      "         6.1367e-02,  9.1813e-02,  2.1540e-02,  7.7856e-03, -4.0828e-02,\n",
      "        -9.7598e-02, -4.1089e-02,  9.0935e-02,  1.8519e-02, -3.4424e-02,\n",
      "         2.8530e-03, -6.6620e-02, -8.9594e-03, -6.7013e-03, -4.6130e-02,\n",
      "        -2.1535e-02,  5.8145e-03,  4.0000e-03, -5.7107e-02,  4.8855e-02,\n",
      "        -1.1148e-01, -1.1978e-01,  6.8131e-02,  1.5512e-03,  3.5912e-02,\n",
      "         3.3328e-02,  3.1726e-02, -8.8611e-02,  1.4725e-01, -9.5569e-02,\n",
      "        -1.0785e-02, -1.3891e-03,  1.3467e-02,  4.0348e-02,  9.6515e-02,\n",
      "         1.6649e-02,  3.0992e-02, -1.5092e-02, -5.3478e-02,  2.6478e-02,\n",
      "        -1.3042e-02, -9.5301e-02, -6.6575e-03, -1.5733e-03, -9.9895e-03,\n",
      "         3.4082e-02,  1.5740e-01, -9.9586e-03, -5.3744e-02,  8.7394e-02,\n",
      "         4.2685e-02,  5.2481e-02,  1.7623e-02,  1.0548e-03,  4.5100e-02,\n",
      "         7.4265e-02, -7.1658e-03, -8.7438e-02, -3.9754e-02,  5.4727e-02,\n",
      "         4.6412e-02,  4.2058e-02, -3.2855e-02, -1.1088e-01, -1.7722e-02,\n",
      "         4.9851e-03, -8.0476e-02,  8.2968e-02, -8.2024e-02,  1.6164e-02,\n",
      "         3.7377e-02, -9.2349e-02, -1.1127e-01,  6.9750e-02,  8.6820e-02,\n",
      "        -2.7057e-02, -2.3069e-02, -7.3103e-02, -1.6484e-01, -2.0014e-02,\n",
      "         6.3153e-03,  7.7782e-02, -8.4823e-02,  2.2121e-02,  1.0625e-01,\n",
      "        -1.4292e-01,  8.1527e-02, -7.1087e-02, -8.0429e-02, -4.0732e-03,\n",
      "         6.4006e-02, -1.4278e-01, -7.9276e-03,  5.2838e-02, -3.7510e-03,\n",
      "        -5.9070e-02, -1.1084e-01, -1.6297e-03,  5.6736e-03, -7.3166e-02,\n",
      "        -6.8036e-02,  1.5117e-01,  1.9150e-02, -9.3975e-02, -4.8127e-02,\n",
      "         4.4899e-02,  5.5049e-02,  6.3477e-02,  5.0466e-02,  1.4346e-01,\n",
      "        -1.4061e-02,  1.8790e-01,  3.4009e-02,  1.4160e-03, -2.5282e-02,\n",
      "        -1.6245e-02,  5.4068e-02, -7.5012e-02, -7.5148e-02, -1.8582e-02,\n",
      "        -2.3466e-02,  1.9578e-02, -6.2413e-02,  1.2314e-01,  1.3701e-02,\n",
      "        -5.7122e-03,  8.9041e-02,  3.7946e-02,  4.1243e-02,  4.7171e-02,\n",
      "         2.7039e-02, -5.9925e-03, -2.8245e-02, -7.2878e-02,  1.4521e-02,\n",
      "         9.9702e-02,  6.4296e-02,  7.4185e-02, -7.1993e-02,  1.4546e-02,\n",
      "         7.7495e-02, -9.2409e-03, -3.8808e-02,  7.1566e-02, -1.4977e-01,\n",
      "         4.2293e-02, -4.2540e-02, -5.6876e-03, -4.4148e-02, -8.0183e-02,\n",
      "         7.5278e-02, -2.9656e-03, -4.9337e-02,  2.6277e-02, -1.1994e-02,\n",
      "        -9.6900e-03, -8.8157e-03, -1.7625e-02, -8.9690e-02, -3.2884e-02,\n",
      "        -5.1021e-03, -1.0199e-01, -1.6831e-02,  1.1726e-01, -3.4447e-02,\n",
      "        -2.8511e-02, -1.9198e-02,  3.6576e-03,  3.2099e-02,  4.5579e-03,\n",
      "         8.7041e-02, -3.0138e-02,  1.8212e-02,  7.4119e-02, -1.3839e-02,\n",
      "         5.3415e-02,  2.2786e-02,  1.0557e-01, -5.6927e-02,  3.3285e-02,\n",
      "         7.3276e-02,  1.0244e-01, -1.4565e-02, -1.0259e-01,  1.2200e-01,\n",
      "         6.1812e-02,  4.8889e-02, -5.6486e-02,  5.1047e-02,  9.3909e-02,\n",
      "        -1.0201e-02,  6.4712e-02, -2.3649e-02,  3.8729e-02,  6.1245e-03,\n",
      "        -4.3430e-02,  6.4039e-03, -8.9212e-02,  1.5119e-01,  7.2071e-02,\n",
      "         1.5732e-02, -2.2774e-02,  5.2327e-02,  2.5401e-02,  2.9843e-02,\n",
      "        -1.1558e-01,  5.9937e-02, -5.8328e-02,  7.1370e-02,  4.9816e-02,\n",
      "         6.5657e-02,  3.2430e-02, -8.6861e-03,  8.5977e-02,  1.9082e-02,\n",
      "         2.7206e-02, -1.9106e-03, -6.5907e-02,  4.0442e-03,  1.7387e-02,\n",
      "         1.3066e-01, -8.5428e-02, -2.6442e-02,  5.6974e-02, -8.7909e-02,\n",
      "         3.4048e-02, -5.8666e-02,  1.8037e-02, -6.2223e-02, -1.8848e-02,\n",
      "         9.5296e-03, -5.1592e-03,  5.1242e-03,  9.5190e-02,  1.1389e-02,\n",
      "        -6.1644e-02,  2.7198e-02,  2.2262e-02, -4.7755e-02,  6.3539e-03,\n",
      "        -2.4203e-02,  1.3476e-02,  5.5816e-02,  3.3884e-02,  5.4144e-02,\n",
      "        -2.0123e-02, -2.5729e-02,  3.2092e-02, -3.4289e-02, -1.2439e-03,\n",
      "         1.8775e-01,  5.8437e-02,  1.8716e-02, -5.8857e-02, -6.8036e-02,\n",
      "        -5.9856e-04,  1.0747e-01, -7.1370e-02,  1.3296e-03, -3.0167e-02,\n",
      "        -5.6810e-02, -1.0447e-01, -8.7226e-03, -3.1270e-03,  1.2601e-02,\n",
      "         1.8155e-02, -9.4597e-02, -4.7340e-02,  2.7440e-02, -3.4883e-02,\n",
      "        -3.2968e-02, -6.2905e-02, -1.2657e-02,  3.2411e-02,  1.2026e-02,\n",
      "         2.2878e-02, -5.3231e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 6.6658e-02, -7.8302e-02, -3.9761e-02, -4.1793e-02,  4.5831e-02,\n",
      "         4.8306e-02, -6.7736e-03,  7.5574e-02, -7.4495e-02, -3.0042e-02,\n",
      "         5.2244e-03, -1.3071e-02, -5.5794e-03, -8.3971e-02, -6.9471e-03,\n",
      "        -2.4258e-02,  1.0854e-01, -6.1369e-02, -1.4674e-01,  1.1226e-01,\n",
      "        -6.0065e-02,  5.3451e-02,  1.1262e-01, -4.9005e-03,  1.5264e-01,\n",
      "         7.8240e-02,  3.1867e-02,  7.0535e-03, -8.8613e-02, -1.6180e-02,\n",
      "         7.1920e-03,  3.6067e-02, -1.8580e-02, -6.9305e-02,  5.7444e-02,\n",
      "        -9.3223e-02,  6.4325e-02, -1.2735e-01, -1.6280e-02, -5.1730e-02,\n",
      "        -1.6762e-02,  1.6986e-01,  2.8526e-02,  7.5887e-02,  4.1897e-03,\n",
      "         5.6685e-02,  4.6633e-02, -3.6862e-02, -3.9126e-02, -2.2331e-02,\n",
      "         9.3762e-02, -1.0613e-02,  1.1766e-01, -3.7826e-02,  6.4190e-02,\n",
      "         2.1247e-02, -9.1414e-03,  9.0567e-02, -1.1170e-01,  1.5015e-02,\n",
      "        -1.6912e-02,  1.8269e-02, -6.4949e-02, -5.4902e-02, -8.6944e-03,\n",
      "         1.3896e-01,  1.1010e-01,  1.0749e-02,  8.7195e-02, -6.8369e-03,\n",
      "        -3.5939e-02,  1.3870e-02,  5.9698e-02, -8.9737e-05,  8.3753e-02,\n",
      "        -4.8358e-03, -3.8847e-02, -1.0107e-01,  7.5683e-02, -1.1180e-01,\n",
      "         3.0140e-02, -4.3089e-02, -2.2418e-02, -3.6128e-02, -1.0527e-01,\n",
      "         2.2898e-02,  4.6009e-02, -7.4225e-03, -5.6874e-02,  8.5350e-02,\n",
      "         5.1923e-03,  2.5627e-02, -8.9285e-03, -5.8058e-02,  7.0525e-02,\n",
      "         3.8854e-02,  2.7697e-02,  1.4393e-01, -4.0282e-02,  2.0928e-02,\n",
      "        -2.4592e-02,  6.1504e-02,  8.4973e-02, -6.5030e-03, -1.1406e-02,\n",
      "        -1.5721e-01, -1.2213e-01, -3.2998e-02, -1.0606e-02,  1.5931e-01,\n",
      "         1.4261e-01,  2.5770e-02, -4.0473e-02, -6.6654e-02,  3.4934e-02,\n",
      "         9.9253e-02, -1.0173e-02, -1.4505e-02,  6.1864e-02,  4.7759e-02,\n",
      "        -1.6578e-02,  3.0713e-02,  1.4806e-02,  8.6155e-02, -1.2338e-02,\n",
      "         7.9021e-02, -7.8331e-02, -6.0098e-02,  7.8730e-02,  2.3303e-02,\n",
      "        -8.3858e-03,  4.4462e-02, -5.4935e-02,  4.2922e-02,  4.7366e-02,\n",
      "        -3.2290e-04,  1.8469e-02, -5.9237e-02,  6.0935e-02,  2.3421e-02,\n",
      "         7.0576e-02, -1.8194e-02,  5.7329e-03,  1.2694e-01, -1.6639e-02,\n",
      "         5.9829e-02, -7.5157e-02, -6.8489e-02, -1.1888e-01, -1.4575e-01,\n",
      "        -6.2740e-03,  8.6623e-02, -1.9370e-03, -1.2883e-01,  4.0742e-02,\n",
      "        -3.1368e-02, -6.8863e-03,  6.7565e-03, -5.5464e-02, -5.8365e-02,\n",
      "        -4.6925e-02, -1.8427e-03, -6.9821e-03, -5.4991e-02,  1.4936e-02,\n",
      "        -6.0094e-02,  2.1199e-02,  1.6101e-03, -6.6419e-02, -1.0129e-01,\n",
      "         3.2519e-04, -9.6969e-02,  2.2424e-02,  8.3956e-02, -1.0915e-01,\n",
      "        -5.2411e-02,  7.9012e-02,  7.7652e-02,  7.2692e-02,  5.3036e-02,\n",
      "         8.0605e-03,  1.2090e-01,  4.4321e-02, -1.3145e-02,  2.7608e-02,\n",
      "        -2.4626e-03, -8.6162e-02, -2.0906e-02, -8.0314e-02,  8.6478e-02,\n",
      "         3.2060e-02, -7.4949e-02, -4.5875e-02, -9.1144e-02,  8.5149e-02,\n",
      "         4.7841e-02, -5.8479e-02,  9.3823e-02, -8.9949e-02, -2.2137e-03,\n",
      "         5.3320e-02,  2.4241e-02,  7.6287e-02, -7.3501e-02,  5.9457e-02,\n",
      "         2.5991e-02, -4.9862e-02,  2.1058e-02,  3.7085e-02,  5.8227e-02,\n",
      "         1.6736e-02,  1.3518e-02, -3.6454e-02,  8.9511e-02, -6.0161e-02,\n",
      "         4.3647e-02,  2.5404e-02,  1.6810e-03, -3.8325e-02,  5.1655e-02,\n",
      "        -6.2435e-03, -7.4342e-02,  1.5280e-02, -3.8896e-02, -4.6945e-02,\n",
      "        -4.9156e-02,  5.0480e-02, -1.1144e-01,  4.6365e-02,  4.1312e-02,\n",
      "         4.3370e-02, -6.4439e-02,  1.4321e-01,  5.6491e-03,  4.6217e-02,\n",
      "        -7.8084e-02,  2.2043e-02,  2.4072e-02, -1.1090e-01, -5.7180e-02,\n",
      "         1.3553e-01,  2.0576e-03, -6.7463e-02, -3.7952e-02,  9.7044e-02,\n",
      "         3.9006e-02,  2.3112e-02,  3.6162e-02, -4.4879e-02, -5.0205e-02,\n",
      "        -6.6276e-02,  6.0393e-02, -1.6587e-02, -4.2223e-02,  4.9360e-02,\n",
      "        -5.2514e-02,  5.3070e-02,  3.0898e-02,  8.4096e-03,  4.2029e-02,\n",
      "         8.3128e-03,  7.7944e-02,  7.4944e-02,  3.7365e-02, -1.7412e-02,\n",
      "        -1.7034e-02, -5.1705e-02, -1.0178e-01,  8.1377e-03, -1.1124e-02,\n",
      "         6.0315e-02, -1.2464e-01, -8.2909e-02, -2.0721e-02,  1.5134e-01,\n",
      "        -7.6029e-03, -5.5703e-02,  1.3161e-01,  1.1009e-01,  8.7843e-02,\n",
      "        -1.1565e-02, -7.0188e-02, -1.7204e-01,  9.7961e-02,  1.4806e-01,\n",
      "        -4.5438e-02, -2.6664e-03, -4.6997e-02, -7.0638e-02, -7.9939e-02,\n",
      "        -7.0988e-02, -1.1400e-01, -7.8130e-03, -8.5862e-02, -3.9800e-02,\n",
      "         7.1482e-03, -1.3455e-01, -2.8474e-02, -8.3467e-02,  6.1789e-02,\n",
      "        -1.2440e-02, -1.4384e-01, -5.4934e-02,  1.7171e-02, -4.3710e-02,\n",
      "         5.2462e-03, -9.8457e-02,  6.4931e-02,  3.0336e-02, -8.2045e-03,\n",
      "        -2.1457e-02,  1.9863e-02, -3.9212e-02,  3.6250e-02, -2.9250e-02,\n",
      "         4.0146e-03,  9.8803e-02, -3.5044e-03, -1.3867e-01,  6.7823e-02,\n",
      "        -1.1386e-02,  4.5815e-02, -4.6995e-02, -6.0331e-02,  8.9048e-02,\n",
      "        -3.3910e-03,  5.5142e-02,  1.0962e-01,  7.8482e-02, -5.7451e-02,\n",
      "         6.7650e-02, -5.0193e-02, -1.0531e-01,  3.0873e-02,  4.0250e-02,\n",
      "         3.5226e-02,  3.5651e-02, -1.3163e-02, -1.5697e-02, -1.3301e-02,\n",
      "        -7.5622e-02,  4.6634e-02, -6.0863e-02,  1.1601e-02,  5.8555e-02,\n",
      "         1.9718e-02,  1.4490e-02,  4.6890e-02,  1.9770e-02,  1.8599e-02,\n",
      "         1.5324e-02,  9.0858e-02, -9.4841e-02,  4.4712e-02,  1.0196e-01,\n",
      "         7.1711e-02,  2.8857e-02, -7.6147e-02,  1.1056e-01,  3.8540e-02,\n",
      "        -7.5464e-02, -1.1109e-01,  1.1038e-02,  7.1191e-02,  3.8999e-02,\n",
      "         8.1577e-02,  1.4265e-01, -2.5305e-02,  7.0406e-02, -2.0950e-01,\n",
      "        -1.0905e-01, -7.9404e-02,  9.4908e-02, -6.2777e-02, -4.6448e-02,\n",
      "         6.7760e-02, -4.1111e-02, -3.0499e-02, -6.7737e-02, -1.6252e-02,\n",
      "         7.7219e-02, -9.5822e-02,  7.5935e-03, -2.3492e-02, -3.9966e-02,\n",
      "         2.2348e-02, -5.5910e-02, -2.2430e-02, -1.2789e-01,  1.1506e-02,\n",
      "        -3.6499e-02, -2.3789e-02,  8.8967e-02,  3.7748e-04,  1.4302e-01,\n",
      "        -3.3631e-02, -3.5510e-02, -1.5043e-01,  7.7718e-02,  1.4879e-01,\n",
      "         6.6394e-02, -1.8917e-02,  1.0423e-02, -4.4962e-03, -2.3098e-02,\n",
      "         8.4583e-02,  1.2187e-01,  2.5955e-02,  2.3483e-02, -1.2860e-01,\n",
      "         2.7167e-02,  3.6408e-02,  8.3306e-02,  1.1587e-01,  6.6651e-02,\n",
      "         5.9024e-02,  1.0206e-01, -6.6102e-02, -1.1416e-02,  6.7382e-02,\n",
      "        -1.8530e-01,  7.1940e-02, -3.7391e-02, -1.0281e-01,  5.0257e-02,\n",
      "         4.7398e-02,  2.7898e-02,  6.5546e-02, -3.5585e-02, -1.5329e-02,\n",
      "        -3.8707e-02, -5.4844e-02, -2.3227e-02,  3.0108e-02, -2.5781e-02,\n",
      "        -2.8408e-02,  3.9738e-03,  9.0303e-02,  8.2566e-03,  2.2979e-02,\n",
      "        -5.5796e-02, -3.8515e-02, -6.0057e-02,  7.1408e-02, -6.8506e-02,\n",
      "        -8.3587e-02, -1.1510e-01,  3.3540e-02, -1.6315e-02, -4.7617e-02,\n",
      "        -1.2741e-01, -2.6345e-02, -6.0932e-02, -2.5297e-02,  1.7280e-03,\n",
      "        -5.4365e-02, -5.7350e-02, -4.4366e-02, -1.8187e-02, -5.9762e-02,\n",
      "         1.8093e-02, -6.1407e-02,  1.3368e-01,  3.7309e-02, -2.3302e-02,\n",
      "        -3.6866e-02,  6.9024e-03,  7.7365e-03,  4.0508e-02, -2.5169e-02,\n",
      "        -8.2504e-02,  1.2014e-01, -6.4195e-02,  6.6726e-02,  1.5957e-02,\n",
      "         1.0247e-01,  9.6323e-02,  5.0310e-02, -7.1386e-02, -6.2054e-03,\n",
      "        -1.6760e-01,  3.7466e-03, -9.4249e-02,  7.7653e-02, -1.2555e-01,\n",
      "        -6.1608e-02, -2.9333e-02,  1.3478e-02, -1.4650e-02, -9.3798e-02,\n",
      "         6.4758e-02,  2.1284e-02,  1.5329e-01, -8.6474e-02, -5.4156e-03,\n",
      "        -2.4129e-02,  1.0983e-01, -2.6136e-02,  1.7877e-02,  7.2377e-02,\n",
      "         2.4865e-02,  5.1694e-02,  5.9210e-02,  1.3274e-01, -4.0805e-02,\n",
      "         2.4143e-02,  6.7355e-02,  6.0903e-02,  6.5552e-02,  1.7681e-01,\n",
      "         4.1771e-02,  1.2728e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 5.0966e-02, -1.4745e-01,  7.7494e-02,  1.4598e-02,  1.1066e-01,\n",
      "        -3.6061e-02, -3.4456e-02,  2.3449e-03,  3.6120e-02, -2.1529e-02,\n",
      "         1.0209e-01,  1.2287e-03, -5.0131e-02,  6.2569e-02, -2.0442e-02,\n",
      "         3.2035e-02,  6.1605e-02, -9.9639e-02,  1.5433e-02,  3.8132e-02,\n",
      "        -6.6866e-02, -6.3091e-02, -6.1747e-02,  6.8062e-02,  8.8035e-02,\n",
      "        -1.0674e-01,  5.1352e-02, -3.5963e-02, -4.7417e-03, -4.0600e-03,\n",
      "        -1.0709e-01, -8.8151e-02,  1.0923e-01, -5.1789e-02, -1.1943e-01,\n",
      "        -3.2427e-02,  8.7168e-02,  1.1600e-01, -3.1433e-02,  2.1007e-02,\n",
      "        -2.0211e-02,  5.1138e-02, -3.1195e-02, -1.7929e-02,  1.6682e-02,\n",
      "        -5.8549e-03, -3.0055e-02, -1.2022e-01,  4.2940e-02,  5.0219e-03,\n",
      "        -7.6352e-02,  1.2055e-02,  1.1379e-02,  7.7296e-02, -3.7195e-02,\n",
      "         6.2380e-02, -9.9886e-02,  1.3775e-02, -3.7782e-02, -8.0343e-03,\n",
      "         1.1148e-02, -1.7144e-02, -8.2952e-02,  6.2111e-02,  1.4023e-02,\n",
      "         9.3064e-02, -1.8222e-02,  8.8978e-02, -9.5613e-02,  5.1005e-02,\n",
      "         6.4407e-02, -1.5327e-02, -1.6592e-02, -4.5361e-02, -3.1602e-02,\n",
      "        -4.6708e-02, -4.0381e-02,  9.3572e-02,  1.4583e-02,  1.5900e-02,\n",
      "         5.2908e-02, -6.2023e-02,  9.5726e-02, -2.2317e-02, -1.0207e-02,\n",
      "        -8.4064e-02, -8.5376e-02,  1.4583e-02,  6.5636e-02,  8.2487e-02,\n",
      "         6.9251e-02, -3.3851e-03,  2.0579e-02, -6.4329e-03, -6.3405e-03,\n",
      "         2.8375e-02, -5.4557e-02,  4.9721e-02, -2.8327e-02,  7.1326e-02,\n",
      "        -2.7338e-02,  7.1745e-02,  2.0902e-02, -1.4693e-02, -6.4021e-03,\n",
      "        -3.6755e-02,  2.3320e-02, -1.8848e-02, -8.2152e-03, -7.3774e-02,\n",
      "        -6.4569e-02, -3.3738e-02,  2.3054e-02, -1.0855e-02,  3.3617e-02,\n",
      "         5.3611e-02, -6.7952e-02, -5.8561e-02, -4.5781e-02,  2.4040e-02,\n",
      "        -8.8937e-02,  3.5465e-02,  5.0535e-02,  2.5044e-02, -4.3513e-03,\n",
      "        -3.2971e-02, -1.3832e-01, -8.0301e-02,  1.5525e-01, -8.0106e-02,\n",
      "         2.0949e-02,  1.1226e-02,  5.7637e-02,  9.5634e-02, -4.6271e-02,\n",
      "         6.2753e-02, -4.8439e-02,  5.5866e-02, -5.6149e-02,  8.9882e-03,\n",
      "        -2.2475e-02,  2.6102e-03, -7.5365e-02, -3.5781e-02,  8.7820e-03,\n",
      "        -2.7019e-02,  5.6331e-02,  1.6614e-03, -3.3956e-02, -6.9785e-02,\n",
      "         1.1633e-01,  5.9738e-02, -8.4658e-02,  3.5563e-02,  1.0341e-01,\n",
      "         7.0607e-05, -4.0593e-02,  3.8467e-02,  1.0799e-01,  1.7658e-02,\n",
      "        -9.0117e-02, -9.2431e-02, -7.4624e-02,  3.1521e-02,  4.0765e-02,\n",
      "        -1.2515e-01,  3.0535e-02,  1.1851e-02, -4.0310e-02,  2.2916e-02,\n",
      "         1.2250e-01,  6.9152e-02, -6.2053e-03,  4.0321e-02,  1.6208e-02,\n",
      "        -6.8822e-02,  2.1849e-02, -3.6987e-02, -4.4603e-02, -1.5947e-01,\n",
      "        -1.6658e-02, -9.6214e-02, -3.7753e-02,  5.4041e-02, -1.7003e-02,\n",
      "         8.1025e-02,  2.4926e-02,  5.5767e-02, -7.9529e-02, -2.1234e-01,\n",
      "        -4.7282e-02, -5.5761e-02,  3.0091e-02,  1.4731e-01, -6.2581e-02,\n",
      "         2.2454e-02, -6.7485e-02,  1.5281e-01,  4.6557e-02,  8.2848e-02,\n",
      "        -9.2783e-03,  7.2040e-02, -9.9636e-02,  6.1564e-02, -5.9368e-02,\n",
      "        -1.9590e-02, -1.0435e-02, -4.1890e-02, -4.7181e-02, -1.2446e-02,\n",
      "        -4.0818e-02,  6.1132e-02, -8.5487e-03,  8.7448e-02,  2.1625e-02,\n",
      "        -1.7572e-02, -9.9109e-02,  3.0057e-02,  7.2901e-02, -1.2618e-02,\n",
      "         3.7349e-02, -2.1917e-02, -6.9758e-02, -1.2695e-03, -1.3122e-02,\n",
      "        -5.0221e-02,  2.3869e-02,  5.0954e-02,  7.0282e-04, -3.3970e-02,\n",
      "        -2.8963e-02, -8.4868e-02, -2.6569e-02, -6.5083e-02,  8.5820e-03,\n",
      "        -4.4336e-03,  5.8201e-03,  2.1587e-02,  7.3191e-03,  4.7043e-03,\n",
      "        -5.8309e-02,  2.1552e-02, -2.5648e-02, -2.2331e-02, -1.0112e-01,\n",
      "        -3.7041e-02, -4.1032e-02, -6.8042e-02,  1.7894e-02, -2.6997e-02,\n",
      "        -2.7584e-02,  1.7612e-02, -1.9444e-03,  5.9923e-02,  6.8182e-02,\n",
      "         2.6522e-02, -6.7600e-02,  3.6002e-02, -1.6933e-02,  9.7652e-03,\n",
      "        -1.0266e-01, -3.6495e-03,  1.1981e-01, -3.1746e-02, -2.1659e-02,\n",
      "        -4.1714e-02,  7.0952e-02, -8.4005e-02,  3.2536e-03, -2.2566e-02,\n",
      "        -3.9273e-02,  3.3117e-03, -8.4515e-02,  5.7761e-02,  9.1372e-02,\n",
      "         9.6171e-03, -1.2380e-01, -8.3872e-04, -1.1604e-02, -2.1467e-02,\n",
      "         3.9992e-02,  8.3243e-04, -5.9930e-03, -2.2868e-02,  2.3452e-02,\n",
      "         1.2934e-02,  1.4610e-01,  6.3666e-04, -4.7834e-02, -1.6290e-02,\n",
      "         6.7797e-02,  3.1905e-02, -6.1453e-02,  4.7708e-02,  4.9836e-02,\n",
      "        -3.2332e-02,  1.4693e-02, -8.0379e-02,  5.6533e-02,  6.9687e-02,\n",
      "         6.2967e-02, -3.5479e-02, -9.2222e-03, -6.3729e-03,  8.0024e-02,\n",
      "         1.0684e-02,  5.5488e-02, -5.7777e-03,  1.2793e-01,  2.4388e-02,\n",
      "         6.8428e-02, -2.1748e-03, -4.4633e-02,  1.3514e-02,  2.4887e-03,\n",
      "        -1.9060e-02, -1.2467e-01, -4.7357e-02, -4.9894e-02,  9.8269e-02,\n",
      "        -6.8453e-03,  3.6830e-02, -3.3399e-02, -4.3410e-02, -9.6036e-02,\n",
      "         8.1545e-02, -3.5613e-02,  6.0910e-02, -5.0575e-02,  6.5858e-03,\n",
      "         5.8657e-02,  2.9649e-02, -5.0301e-02, -1.8220e-02, -7.9198e-02,\n",
      "         4.7839e-02,  3.2613e-02, -9.3417e-02,  6.7337e-02, -8.7942e-03,\n",
      "        -1.6459e-02,  2.7349e-02, -4.9454e-02,  6.1516e-02,  6.7670e-02,\n",
      "         4.5408e-03,  3.2664e-02,  3.3849e-02, -8.3817e-03,  2.9799e-02,\n",
      "        -6.4481e-02,  6.9932e-02,  1.3802e-02, -7.4295e-02,  2.8266e-03,\n",
      "         1.3482e-01,  1.6569e-02, -4.2818e-02,  5.2147e-02,  4.8331e-02,\n",
      "        -2.2739e-02, -1.8746e-02,  2.8624e-02, -8.2209e-02, -4.9650e-02,\n",
      "        -2.9904e-02, -3.1530e-02, -4.7788e-02, -4.7805e-02,  4.2077e-02,\n",
      "        -5.1374e-03,  9.3389e-02,  7.7671e-02, -1.0206e-02, -5.3528e-02,\n",
      "        -6.0535e-03,  2.0553e-02,  2.7381e-02,  8.1292e-03, -6.6471e-02,\n",
      "        -1.9595e-02,  2.1768e-02,  4.5958e-02,  5.7396e-02,  1.7548e-02,\n",
      "        -6.3863e-03, -1.7971e-01,  2.8201e-02,  1.6888e-02, -6.0088e-02,\n",
      "        -4.4732e-02,  5.1204e-04,  5.4047e-02,  1.5042e-02,  8.6862e-02,\n",
      "        -5.6149e-02, -8.0252e-02, -1.7712e-02, -3.3251e-02,  6.7082e-02,\n",
      "         5.7277e-02,  7.4467e-02,  1.3210e-02,  8.0749e-02, -4.9230e-02,\n",
      "         4.0126e-02,  6.4328e-02,  3.2686e-02,  5.5669e-02, -4.5429e-02,\n",
      "        -6.0456e-02,  5.9471e-03, -7.2037e-03, -6.6578e-02,  6.4264e-02,\n",
      "        -3.4567e-02,  1.8057e-01,  9.6095e-02,  1.7282e-02, -5.5573e-03,\n",
      "        -1.5813e-02,  7.3891e-02, -9.6589e-03, -5.6928e-02,  3.5197e-02,\n",
      "        -3.6848e-02,  3.3619e-02, -7.9201e-02, -1.0853e-03, -6.1366e-02,\n",
      "        -4.6373e-02, -2.3210e-02,  2.4530e-02, -2.9117e-02, -2.6862e-02,\n",
      "         2.0443e-02, -1.0311e-02, -4.5818e-02,  3.2928e-02, -1.4177e-01,\n",
      "        -3.3394e-02, -8.0657e-02, -1.1610e-01,  2.7471e-03, -1.1582e-02,\n",
      "         1.8751e-03, -3.5150e-02,  9.0628e-02, -1.1234e-02, -6.3072e-03,\n",
      "        -2.9522e-03, -2.5991e-02,  7.4267e-02,  5.3881e-02, -4.0242e-03,\n",
      "         7.6560e-03,  8.1244e-02, -1.5535e-02, -7.0901e-02,  4.0996e-03,\n",
      "        -1.9212e-02,  1.5392e-02, -4.2169e-02,  1.7310e-02, -7.4863e-02,\n",
      "        -5.8399e-02, -4.7026e-02,  1.1410e-01, -1.0140e-01, -9.5707e-02,\n",
      "         2.0097e-02, -1.0625e-01,  6.2864e-02, -1.0046e-01,  4.0808e-02,\n",
      "        -5.9520e-02, -5.2804e-02,  1.8317e-02, -1.1327e-01, -1.7123e-02,\n",
      "        -2.9642e-03, -1.2108e-02,  4.3250e-02, -6.8001e-02,  2.8993e-02,\n",
      "         2.3379e-03,  6.4308e-03, -5.0257e-02, -2.6099e-02, -9.2139e-03,\n",
      "         1.4326e-01, -3.5042e-02, -5.5747e-03,  1.4443e-01,  6.4646e-02,\n",
      "        -3.6846e-02, -3.1642e-02,  1.8773e-04, -6.0860e-02,  7.3784e-02,\n",
      "         3.4365e-02, -5.6993e-02,  4.9817e-02, -4.8040e-02,  7.2079e-02,\n",
      "         6.0582e-02,  1.5344e-03, -6.8195e-02,  2.4479e-02, -6.7752e-02,\n",
      "        -7.2611e-02, -2.7682e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0768, -0.0110,  0.0261, -0.0717,  0.0138, -0.0634, -0.0912,  0.0113,\n",
      "        -0.0347, -0.0304, -0.0077, -0.0341, -0.0804, -0.0470, -0.0264,  0.0091,\n",
      "         0.0322,  0.0482, -0.0405, -0.0913,  0.0352, -0.0308,  0.0159,  0.0034,\n",
      "         0.0155, -0.0147,  0.0697,  0.0984,  0.0066,  0.0651, -0.1385, -0.0525,\n",
      "        -0.0866,  0.0596, -0.0648,  0.0693,  0.0717,  0.0327, -0.0749,  0.1113,\n",
      "         0.0407,  0.0465,  0.1108,  0.0816, -0.0240,  0.0117,  0.0365, -0.0328,\n",
      "         0.0209, -0.0589,  0.0395, -0.0040,  0.0484,  0.0579,  0.0430,  0.0961,\n",
      "         0.0019, -0.0478, -0.0156,  0.0328, -0.0624,  0.0715,  0.0612, -0.0883,\n",
      "         0.0393, -0.0688, -0.0231, -0.0230, -0.0219,  0.0156, -0.0243, -0.1010,\n",
      "        -0.0313,  0.0016, -0.0020, -0.0170, -0.0236, -0.0161, -0.0517, -0.0867,\n",
      "        -0.0712, -0.0125, -0.0954, -0.0109,  0.1592,  0.0375, -0.0574,  0.0412,\n",
      "        -0.0757,  0.1175,  0.0951, -0.0161, -0.0222, -0.1225,  0.0901,  0.0392,\n",
      "        -0.0461, -0.0242,  0.0155, -0.0975, -0.0425, -0.0112,  0.0040,  0.0077,\n",
      "         0.0669, -0.0678, -0.0185, -0.0830, -0.0124,  0.0362, -0.0285,  0.1085,\n",
      "        -0.0133,  0.0715, -0.0329, -0.0025,  0.0326, -0.0271,  0.0487, -0.0552,\n",
      "        -0.0141,  0.0521, -0.0023, -0.0375, -0.1438,  0.0137,  0.0634, -0.0483,\n",
      "        -0.0128,  0.0103,  0.0111,  0.0511,  0.1563,  0.0164,  0.0060, -0.1368,\n",
      "        -0.1142, -0.0285, -0.0205,  0.0208,  0.0782,  0.0446,  0.0960, -0.0340,\n",
      "        -0.0171,  0.0837,  0.1210,  0.0210, -0.0156, -0.0047,  0.0567,  0.1111,\n",
      "        -0.0234, -0.0498, -0.0705, -0.0082,  0.1107,  0.0074,  0.0705, -0.0538,\n",
      "         0.0613, -0.1379,  0.0155, -0.0276,  0.0236, -0.0070, -0.0942, -0.0741,\n",
      "         0.0344,  0.0320, -0.0537, -0.1111, -0.0324,  0.1613,  0.0198,  0.1086,\n",
      "        -0.0317,  0.0004, -0.0473,  0.0628,  0.0596, -0.0103, -0.0568,  0.0624,\n",
      "        -0.0776, -0.1148, -0.0166,  0.0027,  0.0078, -0.0937, -0.0514, -0.0138,\n",
      "        -0.1482, -0.0669, -0.0712,  0.0135,  0.1173, -0.0033, -0.0064, -0.0263,\n",
      "        -0.0567,  0.0106,  0.0777, -0.0619, -0.0526,  0.0932, -0.0841, -0.0340,\n",
      "        -0.1270,  0.0130,  0.0067, -0.0860,  0.1337, -0.0305, -0.0314, -0.0653,\n",
      "         0.1493, -0.0126, -0.0196, -0.0949, -0.0565,  0.0440, -0.0889,  0.0118,\n",
      "        -0.0558, -0.0214, -0.0157, -0.0387, -0.0158,  0.0084, -0.0396, -0.0521,\n",
      "        -0.0809,  0.0183,  0.0045,  0.0053, -0.0093, -0.0678, -0.1156,  0.0174,\n",
      "         0.1187,  0.0416,  0.0693, -0.0025,  0.0486,  0.0294, -0.0075, -0.0575,\n",
      "         0.1809,  0.0164,  0.0446, -0.0271, -0.0230,  0.0786, -0.0114, -0.0058,\n",
      "         0.0358, -0.0731, -0.0365, -0.0286,  0.1120, -0.0882,  0.0127,  0.0710,\n",
      "         0.0003,  0.0062, -0.0400,  0.0463,  0.0816,  0.0720,  0.0084,  0.0478,\n",
      "         0.0634,  0.0475,  0.0025, -0.0680, -0.0101,  0.0497,  0.0274,  0.0548,\n",
      "         0.0372, -0.0325,  0.1441,  0.0648,  0.0218,  0.0187,  0.0017,  0.0058,\n",
      "         0.0606,  0.0349, -0.0842, -0.0129,  0.1517, -0.0832, -0.0344,  0.0722,\n",
      "         0.0201, -0.0085,  0.0686, -0.0399, -0.1319,  0.0208, -0.0094, -0.0035,\n",
      "         0.0502,  0.0415,  0.0268,  0.0031, -0.0782, -0.0470,  0.0647, -0.0245,\n",
      "        -0.0220,  0.0053, -0.0115,  0.0109,  0.0431,  0.0079, -0.0562, -0.0070,\n",
      "         0.0463, -0.0588,  0.0339,  0.0052, -0.0210,  0.1090,  0.0647, -0.0540,\n",
      "         0.0085,  0.0879, -0.0313,  0.0073,  0.0437,  0.0494,  0.0060,  0.1026,\n",
      "         0.0076,  0.0393, -0.0335, -0.0069, -0.1043,  0.0803, -0.0891,  0.1589,\n",
      "        -0.0709, -0.0418, -0.0459, -0.0026,  0.1630, -0.0228,  0.0362,  0.0665,\n",
      "         0.0199,  0.0311, -0.0793,  0.0584, -0.0846, -0.0298,  0.0471,  0.1816,\n",
      "         0.1290, -0.0308, -0.0354,  0.0684,  0.0022,  0.1397,  0.1273, -0.0121,\n",
      "        -0.0255,  0.1549, -0.1043,  0.0030, -0.0070, -0.0533, -0.1327, -0.0505,\n",
      "        -0.0394, -0.0871, -0.1559, -0.1013, -0.0389,  0.0533, -0.0024,  0.0499,\n",
      "         0.0578, -0.0086, -0.0890, -0.0100,  0.0792, -0.0145, -0.0229, -0.0173,\n",
      "        -0.0718,  0.0246, -0.0108, -0.0746, -0.1079, -0.1119, -0.0225,  0.0620,\n",
      "        -0.0441,  0.0702,  0.1055, -0.0187,  0.0807,  0.0159,  0.0401,  0.0435,\n",
      "        -0.0720, -0.1575, -0.0476, -0.0490, -0.0268,  0.1036,  0.0390,  0.0015,\n",
      "        -0.1407, -0.0818, -0.0521, -0.0193,  0.0634,  0.0762, -0.0572,  0.0335,\n",
      "        -0.0147,  0.0902, -0.0812,  0.0083, -0.1243, -0.0758,  0.1391,  0.0418,\n",
      "         0.0337, -0.0012,  0.0702, -0.0611,  0.0674,  0.0109,  0.0365, -0.0833,\n",
      "        -0.0679, -0.0756,  0.0385, -0.0285,  0.0510, -0.0359,  0.0606,  0.0541,\n",
      "         0.0934, -0.0538, -0.0293,  0.0203, -0.0051,  0.1183, -0.0098,  0.0472,\n",
      "         0.0742, -0.0267, -0.0643, -0.0058,  0.0205,  0.0397, -0.0012,  0.0355,\n",
      "         0.0729,  0.0082,  0.0999,  0.0031,  0.0537,  0.0390,  0.0033,  0.0092,\n",
      "         0.0299, -0.0649,  0.0372,  0.0805,  0.0463, -0.0983, -0.0180, -0.0175,\n",
      "         0.0584, -0.0766,  0.0062, -0.0004,  0.0233, -0.0832,  0.0306,  0.0634,\n",
      "         0.0414, -0.0457,  0.0292, -0.0461,  0.0299,  0.0362,  0.0514,  0.0055,\n",
      "        -0.0551, -0.0026, -0.0381, -0.0229, -0.0396, -0.0021,  0.1161, -0.0633,\n",
      "         0.0352, -0.0886,  0.1244, -0.0195,  0.0971,  0.0900, -0.1717, -0.0553],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-2.8312e-02,  4.9911e-02,  9.7769e-03, -1.7147e-02,  4.0901e-02,\n",
      "        -1.2317e-01, -1.1881e-01,  8.5501e-02,  1.1018e-01,  6.2696e-02,\n",
      "         3.1070e-02, -1.0946e-01,  7.7663e-02,  6.7539e-02, -1.3375e-04,\n",
      "        -1.2912e-02,  5.7624e-02, -7.1261e-02,  9.6846e-04, -4.5915e-03,\n",
      "         6.0058e-02,  2.9872e-02,  4.2197e-02,  3.8850e-02,  5.4885e-02,\n",
      "         4.4528e-02, -8.8942e-02,  1.1722e-01, -4.4009e-02,  3.8589e-02,\n",
      "        -7.9293e-02, -1.1473e-02, -2.3653e-02, -4.3948e-02, -2.1827e-02,\n",
      "        -4.3308e-04,  8.2051e-02,  6.2999e-02,  3.0414e-02,  1.3454e-02,\n",
      "         5.9846e-03,  1.5785e-02, -6.2734e-02,  7.9752e-02, -1.4402e-01,\n",
      "        -5.4157e-02,  8.3404e-02, -5.4182e-02, -3.7938e-02,  1.9626e-03,\n",
      "         6.2376e-02, -9.8665e-02,  1.1238e-01,  8.4942e-02, -5.1376e-02,\n",
      "        -4.4197e-03,  1.0537e-02,  7.6728e-02,  7.0679e-02,  7.5002e-02,\n",
      "         2.3206e-02,  2.2686e-02,  3.7321e-02,  3.3898e-02, -2.2739e-02,\n",
      "        -1.1890e-01,  7.7856e-02,  1.0845e-01,  6.1648e-02, -2.4917e-02,\n",
      "        -5.6272e-02, -2.0143e-04, -6.7984e-02, -5.5723e-02,  1.5601e-03,\n",
      "         9.5723e-02, -1.2334e-01,  2.3138e-02,  1.5915e-03,  1.7391e-02,\n",
      "         1.0060e-03, -5.5752e-02, -7.3283e-03,  7.8786e-02, -8.5108e-02,\n",
      "         5.5049e-02,  1.5016e-01, -3.1859e-02,  4.4934e-03, -5.7109e-02,\n",
      "         8.0624e-03,  1.0309e-01, -3.0260e-03, -1.8075e-02,  1.0297e-01,\n",
      "         1.8190e-02,  8.1257e-02, -1.0586e-01,  4.6859e-02,  8.7545e-03,\n",
      "        -1.8347e-02,  7.8826e-04,  3.4076e-02,  3.4202e-02, -4.6036e-02,\n",
      "         7.8401e-02,  1.2534e-02, -2.9604e-02, -1.4013e-01, -1.2220e-01,\n",
      "        -3.9575e-02,  4.2375e-02,  6.8481e-02, -1.1031e-01,  1.7292e-03,\n",
      "         5.6505e-03, -1.3347e-01,  5.8967e-02,  1.0500e-01,  2.8959e-02,\n",
      "        -1.3579e-01, -3.6767e-02, -6.5603e-03,  5.9650e-02,  3.4714e-02,\n",
      "         3.4603e-02,  6.3472e-02,  8.8572e-02, -3.0379e-02,  1.2246e-02,\n",
      "         3.0892e-02, -1.9900e-02, -2.0532e-02, -9.3364e-02,  2.0879e-02,\n",
      "        -3.1082e-02,  7.4723e-02,  3.4827e-02,  9.9355e-03,  4.0432e-02,\n",
      "         9.0674e-02, -6.2378e-02, -1.7440e-02,  1.5880e-02, -1.3521e-02,\n",
      "         6.1648e-02, -2.5270e-02, -1.0506e-02,  1.8069e-02, -5.2453e-02,\n",
      "         1.3252e-02,  6.9504e-03, -5.8516e-02,  4.6623e-02,  1.4739e-02,\n",
      "         6.7765e-03,  3.7023e-03,  3.7319e-02,  1.9224e-02,  2.6738e-02,\n",
      "         8.2818e-02, -1.2007e-04,  7.7645e-02,  9.2141e-03,  4.3738e-03,\n",
      "        -1.0779e-01,  8.4956e-02,  3.7886e-02, -1.3384e-01, -1.1208e-01,\n",
      "        -5.7828e-02, -9.7238e-02,  1.0206e-02,  6.5645e-03, -2.8718e-02,\n",
      "         1.5325e-02,  6.6613e-02,  2.6445e-02, -2.4962e-02, -4.9788e-02,\n",
      "        -4.3545e-03, -4.5150e-02, -1.4951e-02,  6.1688e-02, -9.0608e-03,\n",
      "        -8.5805e-02, -1.0172e-01, -9.2241e-02, -1.5714e-03, -2.6098e-02,\n",
      "        -2.3720e-02, -4.2816e-03, -4.2465e-02,  4.0990e-03,  5.9952e-02,\n",
      "        -8.0171e-02,  3.4743e-02, -5.9418e-02, -5.0707e-04, -1.7003e-02,\n",
      "        -3.6289e-02,  9.0298e-02, -2.5486e-02,  2.2962e-02,  8.9927e-03,\n",
      "         3.8505e-02,  5.5345e-02, -2.0447e-02, -3.3111e-02,  3.7436e-02,\n",
      "         6.5773e-02, -4.5183e-02,  4.1996e-02, -8.7999e-02, -1.1769e-02,\n",
      "        -4.3234e-02, -6.6346e-02, -3.5659e-02, -5.7530e-03,  3.8261e-02,\n",
      "         6.5813e-02, -2.6030e-02, -7.3186e-03, -6.0748e-02, -5.1565e-02,\n",
      "        -2.2371e-02,  1.2256e-02,  7.5072e-02,  1.9970e-02,  2.4642e-02,\n",
      "        -7.0200e-02,  3.6686e-02,  2.4515e-02,  3.2946e-03,  6.7995e-03,\n",
      "         8.7247e-02, -6.1754e-02,  2.3224e-02,  4.8788e-02, -3.7919e-02,\n",
      "        -4.5916e-02, -6.3038e-03, -6.4867e-02,  9.7451e-03, -2.9809e-02,\n",
      "         1.9220e-02,  4.9873e-02, -8.4751e-02, -3.8756e-02,  2.4613e-03,\n",
      "         1.2979e-02, -1.9546e-02, -1.7456e-03,  6.0348e-02,  3.5478e-02,\n",
      "         8.5359e-02,  4.5793e-02, -2.9652e-02, -1.9533e-02,  2.8801e-02,\n",
      "         2.0128e-02, -1.6773e-02, -2.2567e-02,  8.6599e-02,  7.6258e-02,\n",
      "        -1.3919e-02, -5.2701e-03,  1.5254e-02, -5.6596e-03,  1.2512e-02,\n",
      "        -1.1107e-01, -3.9220e-02, -4.3274e-02, -1.4759e-02,  6.3456e-02,\n",
      "        -3.9313e-02,  6.6304e-02, -2.5031e-02, -8.0906e-02, -9.2574e-02,\n",
      "         7.7114e-03, -3.8525e-02,  2.6354e-02,  6.7656e-02, -3.6397e-02,\n",
      "        -6.6598e-02,  4.9100e-02, -4.5302e-02, -9.6687e-02,  3.2252e-03,\n",
      "        -1.6827e-02,  9.3235e-02, -2.9695e-02,  8.8593e-02,  1.0684e-01,\n",
      "         1.0159e-01,  7.8147e-02, -2.3984e-02,  7.4527e-02,  9.7435e-02,\n",
      "         9.9969e-02,  4.1802e-02,  5.5769e-02,  4.1883e-02,  3.7363e-02,\n",
      "        -1.2641e-02,  3.1162e-02, -5.7425e-04,  5.6984e-02,  2.1873e-03,\n",
      "         3.2089e-02, -7.0392e-02,  2.0635e-02,  9.4762e-03, -1.5822e-02,\n",
      "         5.4450e-02, -2.8916e-02,  1.6877e-02, -7.8206e-03, -1.1922e-01,\n",
      "         2.3058e-02,  6.5806e-02,  9.5983e-03,  4.4597e-02,  1.8453e-02,\n",
      "         4.3058e-02,  6.1493e-02, -6.8039e-02, -3.5424e-02, -3.8730e-02,\n",
      "        -4.6403e-02,  2.2619e-03,  1.3438e-02,  3.6322e-02, -9.0361e-02,\n",
      "         2.3885e-02, -6.8223e-02, -2.8933e-02,  1.0164e-01,  1.5505e-02,\n",
      "        -7.0034e-02,  7.1678e-02, -6.8170e-02,  4.8597e-02,  8.5489e-02,\n",
      "         3.4030e-02, -1.1827e-02,  4.7249e-02, -5.7491e-02,  6.4812e-02,\n",
      "        -3.8081e-02,  3.1269e-02,  4.8112e-02, -2.2889e-02, -1.2078e-01,\n",
      "         8.6875e-03,  2.7524e-03, -5.2020e-02, -1.3657e-02, -3.4252e-02,\n",
      "         1.2507e-01,  6.4650e-02, -4.3744e-02,  2.1554e-02,  7.2027e-02,\n",
      "         4.6084e-02,  1.0100e-01,  7.4042e-02, -5.4211e-02, -1.1455e-01,\n",
      "         5.7521e-02, -4.2710e-02, -7.8814e-02, -1.8124e-02,  4.4737e-02,\n",
      "        -5.1269e-02, -6.7855e-02, -8.3722e-02, -6.4286e-02,  3.4506e-02,\n",
      "         8.8117e-02,  4.1227e-02, -1.0366e-01, -5.4640e-02, -3.3339e-03,\n",
      "         1.3867e-01, -5.8631e-02,  1.0841e-02, -9.4331e-02,  1.0992e-01,\n",
      "        -1.8052e-02,  5.6607e-02, -3.0553e-03, -9.7665e-02,  3.6189e-03,\n",
      "         3.8424e-02, -2.0226e-02, -1.0399e-01,  7.1986e-02, -8.7396e-02,\n",
      "        -2.1321e-02, -3.3681e-02, -4.8806e-02, -9.9724e-03,  3.4821e-02,\n",
      "        -3.6701e-02, -1.0064e-01, -4.4952e-02, -2.9649e-02,  6.7568e-02,\n",
      "         1.0062e-01,  1.5413e-02, -5.2982e-03, -8.1491e-02,  6.9497e-02,\n",
      "         7.5970e-03,  2.6650e-02, -7.8061e-02,  8.9628e-02,  5.9069e-02,\n",
      "        -2.8076e-03,  2.2840e-02,  4.9031e-02, -3.0829e-02, -1.4460e-01,\n",
      "         2.0347e-02,  3.0446e-02,  4.5471e-02,  8.5173e-02, -1.1764e-02,\n",
      "        -1.9823e-02, -1.1526e-02, -1.4037e-02, -5.7210e-03,  3.2612e-02,\n",
      "         8.8098e-02,  2.5476e-02,  5.3235e-02,  9.3301e-02,  6.9620e-02,\n",
      "        -6.3628e-02,  6.8000e-02,  1.4908e-01, -5.6959e-02,  5.9116e-02,\n",
      "         2.2112e-02, -2.4973e-02, -2.7610e-02,  4.1903e-02, -2.0115e-02,\n",
      "         5.7806e-02,  1.3158e-03, -8.3065e-02,  4.6314e-02, -9.3857e-02,\n",
      "        -9.9200e-03,  4.4497e-02, -1.1722e-02, -6.1344e-02, -1.3309e-01,\n",
      "         4.0768e-02, -2.1628e-02, -5.0834e-02,  1.0866e-01,  1.6634e-02,\n",
      "         7.5386e-02,  1.1037e-01, -3.8678e-02,  5.1629e-02,  3.5886e-02,\n",
      "         3.2558e-02,  1.4227e-03,  5.5960e-02,  1.0197e-03, -5.6617e-02,\n",
      "         2.2816e-02, -1.3664e-01,  1.3298e-01, -3.5689e-02,  1.8169e-02,\n",
      "        -3.9363e-02, -4.9693e-02,  8.3050e-02, -1.3196e-02, -4.6567e-02,\n",
      "         3.9041e-02,  2.8396e-02, -2.6041e-02,  6.8008e-02, -1.0233e-01,\n",
      "        -1.5822e-02, -3.0579e-02, -4.8071e-02, -6.4514e-02,  1.8201e-02,\n",
      "        -4.3278e-02, -4.3680e-03, -8.4785e-02, -5.5908e-02, -6.7275e-02,\n",
      "         8.3114e-02,  1.3823e-02,  4.9019e-02,  4.0267e-02, -5.4514e-02,\n",
      "         4.9135e-02, -4.8312e-02, -2.4285e-02, -9.7027e-02,  2.4834e-02,\n",
      "         1.4886e-02,  6.9949e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 3.0289e-02,  3.1503e-02,  3.9986e-02,  1.3083e-01, -5.3132e-02,\n",
      "         2.9113e-02,  4.7187e-03,  5.0454e-02,  1.0700e-01, -2.2314e-02,\n",
      "         2.6524e-02, -1.1840e-02,  5.0855e-03,  7.3779e-04, -1.1865e-03,\n",
      "        -4.7954e-02,  1.0474e-02,  2.8582e-02, -7.9896e-02,  7.6038e-02,\n",
      "         4.5977e-02, -1.4148e-02,  3.9841e-02,  1.8766e-02,  8.0392e-02,\n",
      "         2.6746e-02,  2.9566e-02, -2.5976e-02,  1.6478e-02, -5.0035e-02,\n",
      "         2.4266e-02,  4.7684e-03, -4.6095e-02,  5.4383e-02, -5.5842e-02,\n",
      "        -6.3235e-02,  1.0002e-01, -7.9192e-03,  4.9059e-02, -2.9653e-02,\n",
      "         7.4298e-02,  3.2793e-02,  8.6242e-02,  1.3700e-03,  1.4234e-02,\n",
      "         7.6310e-02,  3.2565e-02, -5.5205e-02, -2.8722e-02, -3.9794e-02,\n",
      "         8.0323e-02, -1.0903e-01, -4.8134e-04,  4.3818e-02, -3.0959e-02,\n",
      "        -5.7084e-02,  4.3061e-02,  4.2138e-02,  7.2363e-02,  4.3792e-02,\n",
      "        -7.2850e-02,  5.2529e-03,  4.6195e-03, -6.2514e-02,  8.1972e-02,\n",
      "        -1.2628e-02,  1.1640e-01, -7.5081e-02,  2.6473e-02, -6.2586e-02,\n",
      "        -6.8327e-02,  5.4805e-03, -8.0045e-02, -1.0655e-02, -7.7074e-03,\n",
      "        -8.1215e-02, -1.6442e-02,  6.8840e-03, -6.9273e-03, -4.1731e-02,\n",
      "        -6.2782e-02,  6.2828e-02, -8.7719e-02,  1.7283e-02, -5.3315e-02,\n",
      "        -9.8364e-02, -9.7457e-02,  8.1505e-02,  2.6662e-02,  5.2712e-02,\n",
      "         5.1618e-02, -3.9540e-02, -1.0101e-01, -2.3273e-02,  1.6070e-02,\n",
      "        -3.2476e-02, -3.7883e-02, -1.9677e-02, -3.3466e-02,  1.7523e-02,\n",
      "        -9.1086e-02, -4.3556e-02,  7.8876e-02, -4.1143e-02, -3.5400e-02,\n",
      "        -1.7865e-02,  1.7630e-01,  1.3965e-01, -5.0848e-02, -3.6669e-02,\n",
      "         2.1116e-02, -1.0324e-01, -1.7145e-02,  6.3624e-02, -7.2753e-02,\n",
      "         8.1110e-04,  7.7122e-02,  6.0167e-02,  9.4302e-02,  3.3645e-02,\n",
      "         5.1997e-02,  9.3938e-03,  1.5380e-02,  3.0624e-02,  1.8364e-02,\n",
      "         9.4459e-02, -5.3204e-02,  5.3909e-02,  8.4368e-02, -2.6575e-02,\n",
      "         5.8741e-03,  1.7135e-01,  3.8734e-02,  1.1533e-01, -3.4991e-02,\n",
      "        -1.3902e-01, -5.0564e-02,  2.5342e-02,  1.9510e-03, -4.5458e-02,\n",
      "        -7.6664e-02,  1.0237e-01,  7.7267e-03,  5.8986e-02, -1.9288e-02,\n",
      "         5.3286e-02,  3.6359e-02,  8.0501e-02, -8.3045e-02,  3.3307e-02,\n",
      "         1.5659e-03,  9.6013e-03, -1.5590e-02, -5.1359e-02, -7.0246e-02,\n",
      "        -1.1975e-02,  2.6491e-02, -3.2005e-02,  6.8249e-02,  4.7669e-02,\n",
      "         4.7641e-02, -2.1512e-02, -6.3295e-02, -4.1788e-02, -1.5279e-02,\n",
      "        -9.7037e-02,  2.2685e-02,  2.0949e-02,  3.3309e-02,  9.4829e-03,\n",
      "         5.6710e-02, -7.6783e-03, -1.3969e-01, -4.1760e-02,  8.8335e-03,\n",
      "         4.3914e-02, -1.1144e-02,  2.1213e-02,  5.0143e-02, -1.7819e-02,\n",
      "        -3.6000e-02, -9.8346e-02,  1.8010e-02,  1.1031e-02, -4.7298e-02,\n",
      "        -2.5419e-02, -4.0803e-02,  3.5511e-02,  9.2070e-03,  6.9367e-03,\n",
      "        -4.2061e-02, -1.0377e-02,  8.0876e-02, -5.6107e-02,  5.7277e-02,\n",
      "         8.7439e-03,  1.8353e-02, -4.1559e-02,  3.4507e-02, -1.0548e-01,\n",
      "        -4.0571e-02, -2.1289e-02,  3.0586e-02,  5.1678e-03,  8.7577e-04,\n",
      "         1.3942e-01, -1.1645e-02,  7.2364e-02,  6.5043e-02,  2.4132e-02,\n",
      "         1.1002e-01,  6.1222e-03,  6.6061e-03, -5.2206e-02, -1.3325e-02,\n",
      "        -8.5573e-03, -2.0275e-03,  1.6365e-03,  2.6494e-02,  7.1705e-02,\n",
      "        -7.1865e-02,  8.4742e-02,  6.0429e-02, -5.9917e-04, -5.1137e-02,\n",
      "        -5.9481e-02, -7.6383e-02,  4.8239e-02, -3.4069e-02, -9.6994e-02,\n",
      "         1.8230e-02,  8.8950e-02,  8.6447e-02, -2.9383e-02, -9.0702e-02,\n",
      "        -3.7237e-02, -3.5979e-02, -4.2816e-02, -7.7253e-02,  7.3348e-03,\n",
      "         4.4436e-02, -1.5954e-01,  1.2394e-01,  1.1889e-02,  1.5041e-02,\n",
      "        -6.7389e-02, -4.5964e-02,  2.0859e-02, -3.0347e-02, -2.0750e-02,\n",
      "         3.9519e-02, -2.8886e-02, -8.1723e-02, -2.2986e-02, -2.3117e-03,\n",
      "         7.9396e-02, -4.6225e-02,  5.9592e-02, -6.6315e-02, -4.8456e-02,\n",
      "        -4.7836e-03, -6.7407e-02,  4.6288e-02,  1.5025e-01,  3.1964e-02,\n",
      "        -1.0685e-01, -3.1458e-02, -4.1457e-02,  7.1839e-02, -9.0231e-02,\n",
      "         3.3797e-02, -2.6273e-02, -6.0258e-02, -3.0063e-02, -9.9684e-02,\n",
      "         8.9154e-02,  4.6204e-02,  1.0030e-02, -2.1860e-02, -9.5296e-03,\n",
      "        -2.6632e-02, -2.0542e-02, -8.8112e-02, -3.1891e-02,  8.1285e-02,\n",
      "         3.4284e-02,  9.3343e-02, -7.2938e-02,  4.2222e-02,  8.5092e-02,\n",
      "        -6.9859e-02, -1.1665e-01, -1.7408e-02, -1.5403e-02,  5.4243e-02,\n",
      "         9.8341e-03, -2.8077e-02, -2.9991e-02,  3.4399e-02,  1.4826e-02,\n",
      "         1.0260e-02,  8.0673e-02,  5.1878e-03, -8.1736e-02,  8.6033e-02,\n",
      "         8.2636e-02,  5.0595e-02, -1.1922e-01,  9.3888e-03,  2.7255e-02,\n",
      "         2.7873e-02,  2.2796e-02,  1.8762e-02,  1.4380e-01, -1.4723e-01,\n",
      "        -1.4255e-02, -3.0604e-02, -3.7668e-03,  1.1167e-02, -8.0839e-02,\n",
      "         1.4414e-02, -2.5007e-02, -2.3666e-02, -2.7692e-02, -1.6474e-02,\n",
      "         5.1326e-02, -6.8901e-03,  2.6673e-02, -1.9049e-02, -4.9653e-02,\n",
      "         1.1313e-01,  8.5847e-02,  1.3205e-01, -4.7806e-02, -9.3220e-02,\n",
      "         4.1846e-02, -4.5715e-02,  2.4093e-02, -3.6066e-02,  5.0121e-02,\n",
      "         2.4745e-02, -9.0033e-02,  5.9747e-02, -5.9992e-02, -2.5795e-02,\n",
      "        -3.5649e-02,  2.3503e-02,  1.4340e-01, -5.7906e-02, -8.6132e-03,\n",
      "        -6.0701e-03,  3.0256e-03, -6.0207e-02,  1.3398e-02, -3.4405e-03,\n",
      "         3.6077e-02, -7.9061e-02, -4.5184e-02, -6.7206e-02,  8.3835e-02,\n",
      "        -1.4701e-02,  2.4760e-02,  1.7550e-02,  5.2360e-02, -1.1143e-01,\n",
      "        -6.0042e-02, -2.1617e-02, -2.3820e-02, -1.9716e-02, -1.1295e-01,\n",
      "        -1.7096e-02, -5.0607e-02,  9.7075e-02,  2.0780e-02, -4.8206e-02,\n",
      "         4.0675e-02, -5.4123e-02,  2.6274e-02, -1.1451e-01,  5.9652e-02,\n",
      "        -2.4965e-02, -2.3823e-02,  5.4150e-03, -2.5337e-03, -5.9982e-02,\n",
      "        -3.6474e-02, -1.8158e-02, -1.5301e-02,  1.1725e-02,  2.3499e-02,\n",
      "         7.4033e-02, -4.0130e-02, -5.1274e-02,  9.0815e-02,  5.4975e-02,\n",
      "        -3.4270e-02,  4.5382e-02, -7.2244e-02, -7.0036e-02, -9.7178e-03,\n",
      "        -3.3955e-02, -3.5253e-02,  8.1896e-02,  7.5562e-03, -7.9211e-02,\n",
      "        -1.0875e-01,  1.2409e-03,  7.7800e-02,  1.0634e-02, -8.2665e-02,\n",
      "         1.3230e-02, -3.4552e-02,  9.1453e-02, -6.4865e-02,  4.5128e-02,\n",
      "        -1.1324e-01, -5.8086e-02,  4.5286e-02, -3.5615e-02,  1.1491e-03,\n",
      "         4.5156e-02,  2.6197e-02, -9.7915e-02, -8.8574e-02,  6.3982e-02,\n",
      "        -7.3688e-02,  3.8706e-02,  8.2396e-02,  7.6938e-02, -2.0139e-02,\n",
      "        -6.2673e-02, -8.2048e-02,  5.6388e-02,  1.7644e-02,  4.3307e-02,\n",
      "         8.2072e-03, -4.8394e-02,  7.1145e-03, -1.4995e-01,  6.3767e-02,\n",
      "        -1.7300e-02, -4.0330e-04,  2.5645e-02,  6.1843e-02, -5.0088e-03,\n",
      "         3.9473e-03,  8.7710e-02,  3.0694e-02, -1.5863e-02,  1.2367e-01,\n",
      "         5.8815e-02,  6.1809e-02,  1.1823e-01,  3.4193e-02, -1.3734e-01,\n",
      "        -8.3475e-03, -1.3101e-02,  1.7372e-01,  3.1849e-02,  5.8699e-02,\n",
      "        -8.2168e-02,  2.9679e-02,  2.9754e-02, -1.9589e-02, -2.3867e-05,\n",
      "         2.9229e-03, -5.9795e-02,  1.0513e-01, -2.3250e-02,  1.5259e-02,\n",
      "        -9.9677e-04,  5.2436e-02,  4.5202e-02, -5.3536e-02, -3.1198e-02,\n",
      "         1.1600e-01,  8.2992e-02, -6.0462e-02, -6.9867e-02, -2.0561e-03,\n",
      "         6.2426e-02,  3.0686e-02,  7.3595e-03, -5.2512e-03, -8.7785e-02,\n",
      "         7.2232e-02, -5.5166e-02,  5.2830e-02, -3.4109e-02, -3.5072e-02,\n",
      "        -7.8913e-02,  3.6241e-02,  4.8680e-02, -2.4749e-02,  9.5748e-02,\n",
      "         1.1784e-01,  6.6303e-02, -3.3105e-02,  3.1397e-02,  4.8392e-02,\n",
      "        -9.6809e-02,  6.1331e-02,  3.0868e-02,  3.2937e-02,  1.4860e-02,\n",
      "        -8.8214e-02, -7.5167e-02, -2.6680e-02, -7.2619e-02, -3.8868e-02,\n",
      "         4.7005e-02, -1.5254e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-3.7034e-02, -4.5888e-02,  8.8781e-03,  2.7156e-02,  5.8858e-02,\n",
      "         1.2498e-03, -2.9473e-02, -2.4259e-02,  2.7695e-02,  4.8506e-02,\n",
      "        -1.3610e-02,  2.4264e-02, -1.0506e-02, -2.2343e-02, -1.2575e-02,\n",
      "        -2.7388e-02,  3.7047e-03, -9.8502e-02, -7.6187e-02, -1.3275e-02,\n",
      "         4.0868e-02,  3.1048e-02,  2.9744e-03, -3.4535e-02,  6.2692e-02,\n",
      "        -1.0555e-01, -1.8775e-03, -6.1323e-02,  1.1437e-02,  6.9841e-02,\n",
      "        -1.2952e-02,  7.9710e-02, -3.6756e-02,  1.2847e-02,  1.0407e-01,\n",
      "        -8.7324e-02, -1.0587e-01, -3.1902e-02, -8.2598e-03, -1.0516e-01,\n",
      "        -9.7262e-02,  1.1731e-02, -1.1542e-02, -1.0035e-01, -8.8628e-02,\n",
      "        -1.6604e-02, -6.6435e-04, -5.5660e-02, -5.8090e-03, -9.9288e-03,\n",
      "         2.7286e-02, -4.0562e-02, -1.3763e-02, -5.6210e-02, -8.2477e-03,\n",
      "         3.0968e-02, -2.2097e-02,  2.6884e-02, -4.4554e-03,  6.1624e-02,\n",
      "         5.7080e-02,  9.1388e-03, -2.2383e-02,  3.1594e-02,  9.0034e-02,\n",
      "         2.9283e-04, -2.4813e-03, -4.8279e-02,  2.9078e-02,  3.5868e-02,\n",
      "         4.1491e-02, -6.3660e-02, -8.4763e-02, -8.1597e-02, -5.1852e-02,\n",
      "         2.2601e-04,  1.0845e-01,  3.0973e-02, -1.5400e-01,  3.3164e-02,\n",
      "         7.9088e-02,  6.5250e-02,  5.1900e-02, -4.2283e-02, -1.1346e-01,\n",
      "        -9.0076e-03,  1.1980e-01, -1.1909e-02,  1.2310e-02,  1.8831e-02,\n",
      "        -4.9647e-02,  7.0969e-02, -2.3682e-02, -8.6618e-02,  5.2677e-02,\n",
      "         7.8079e-03, -1.0115e-01,  7.5915e-02, -4.8108e-02, -1.3128e-01,\n",
      "         6.4873e-02, -7.1029e-03, -1.4379e-01, -2.1432e-02, -5.3666e-02,\n",
      "         3.7874e-02, -8.1764e-02,  1.6618e-01,  7.1652e-02,  4.2189e-02,\n",
      "        -4.8112e-02,  5.0704e-02, -8.4332e-02,  2.3637e-02, -1.1713e-02,\n",
      "        -1.4738e-01, -5.6326e-02, -8.2328e-02, -6.9366e-03,  8.9393e-03,\n",
      "         9.0724e-02, -3.4346e-02, -1.7982e-02, -1.4817e-02, -9.2182e-02,\n",
      "         3.9414e-02, -1.3945e-02, -9.3391e-02,  1.0452e-01,  8.3443e-02,\n",
      "        -8.3101e-03,  5.8458e-02,  3.4724e-02, -9.1750e-02,  2.9846e-02,\n",
      "        -9.8895e-02, -2.4202e-02,  4.6580e-02,  4.4337e-02, -1.2447e-02,\n",
      "        -8.0480e-03, -5.6974e-03, -3.7265e-02,  7.7061e-02,  5.1464e-02,\n",
      "        -7.0224e-02, -4.4164e-02,  2.5564e-02,  1.2461e-02, -2.4537e-02,\n",
      "         2.2466e-02,  6.7765e-03, -2.1143e-02,  1.3173e-02, -4.8422e-02,\n",
      "        -2.4130e-02,  4.0795e-02, -6.9050e-02,  5.2960e-02,  2.9344e-02,\n",
      "         6.1323e-02,  2.6642e-02, -1.5501e-02,  1.1257e-02,  5.2199e-02,\n",
      "        -1.9131e-02, -7.1120e-02,  1.5206e-01, -5.5123e-02,  1.6600e-02,\n",
      "        -1.7471e-02,  5.4039e-02,  7.3465e-02, -1.4534e-02,  3.2988e-02,\n",
      "         1.0805e-01,  2.3235e-03,  2.6146e-02,  5.6207e-02,  2.4650e-02,\n",
      "         1.0190e-02, -4.5924e-03,  4.1432e-02, -4.8620e-02, -2.9034e-02,\n",
      "        -2.9012e-02,  1.4155e-02,  3.5942e-02, -9.4590e-03, -3.9627e-02,\n",
      "        -5.3268e-02,  1.3831e-01, -3.0257e-02, -5.7423e-03,  4.2466e-02,\n",
      "         1.2649e-01, -5.0767e-02, -1.1174e-02, -2.3112e-02,  3.8812e-02,\n",
      "        -6.3522e-02,  9.1453e-02,  2.6309e-02, -1.1686e-01, -3.9759e-02,\n",
      "         2.4578e-02, -4.7622e-03, -5.6869e-02,  9.6072e-02,  1.3556e-02,\n",
      "        -2.8459e-02, -4.5581e-02,  1.2914e-01, -1.1633e-02,  1.1193e-01,\n",
      "        -8.6753e-02, -8.5673e-03, -7.3127e-02, -3.6154e-02, -9.3040e-02,\n",
      "        -3.7462e-02,  1.2344e-01,  8.0146e-02, -1.7490e-02,  1.1924e-01,\n",
      "        -1.0738e-02,  6.7925e-02, -6.9445e-02, -2.5708e-02, -5.6665e-02,\n",
      "        -1.5419e-01,  1.2431e-01, -7.5615e-03, -1.0575e-01,  8.1955e-02,\n",
      "        -3.7937e-02,  8.6439e-02, -3.1533e-03,  1.4085e-01,  3.6980e-02,\n",
      "        -1.3440e-02, -5.1998e-02,  5.9634e-02, -4.4400e-02,  1.6468e-02,\n",
      "         3.7003e-02,  2.0843e-02,  4.8651e-02, -3.7829e-02,  1.0212e-01,\n",
      "        -1.8587e-02,  4.5990e-02, -4.5087e-03, -1.0517e-01, -7.8714e-02,\n",
      "        -2.2157e-02, -5.8386e-02,  7.0721e-02, -1.4240e-02, -1.0749e-01,\n",
      "        -6.8921e-02, -3.1443e-02, -3.2220e-02, -6.4972e-02,  1.1256e-02,\n",
      "         4.3494e-02,  1.8916e-02, -1.8547e-01, -2.1113e-02, -3.5792e-02,\n",
      "        -1.2145e-02,  4.6165e-02, -1.1010e-01,  3.3331e-04,  8.4547e-02,\n",
      "         5.4524e-02,  4.8118e-02, -9.5097e-02, -7.2445e-02, -6.6263e-05,\n",
      "         5.1787e-02,  4.9852e-02, -4.7932e-02, -1.2280e-02, -1.6250e-02,\n",
      "        -1.4342e-02, -1.1116e-01, -5.5778e-02, -7.7247e-03, -8.1662e-02,\n",
      "        -4.3206e-03,  6.6698e-02, -5.0373e-02, -1.2831e-01,  7.0735e-02,\n",
      "        -4.0484e-02, -2.6315e-02, -2.7391e-02, -8.0403e-02, -6.9732e-03,\n",
      "         5.4342e-02,  2.0656e-02,  1.5141e-01,  1.0275e-01,  1.5837e-03,\n",
      "        -1.4563e-01,  8.5911e-05,  4.7454e-03, -7.8300e-02,  4.8858e-02,\n",
      "        -2.1546e-02,  1.4427e-02,  4.6923e-02, -4.1582e-02,  3.4860e-02,\n",
      "         1.6094e-01, -2.8653e-02,  6.8671e-02,  3.9210e-02, -2.7989e-02,\n",
      "         1.2157e-01,  3.4874e-02,  1.0473e-01,  5.0698e-02, -6.6427e-02,\n",
      "        -8.5859e-02,  4.0868e-02, -8.1263e-02,  1.2227e-04, -4.1179e-02,\n",
      "         7.0834e-03,  8.5109e-02, -2.0567e-02,  6.0143e-03, -8.9583e-02,\n",
      "         6.3068e-02, -4.5089e-02,  2.6703e-02,  5.3511e-03,  9.8072e-03,\n",
      "         9.1949e-04,  4.8803e-02, -1.2944e-02, -1.6477e-02,  3.7466e-03,\n",
      "        -7.1968e-02, -6.9599e-02, -1.0072e-01, -7.0090e-02,  3.5817e-02,\n",
      "         6.2147e-02,  8.6350e-02,  8.2676e-02,  6.9734e-03, -1.6660e-01,\n",
      "         3.0636e-02, -7.5360e-02,  8.7070e-02,  4.6590e-02, -1.2240e-02,\n",
      "         4.7421e-02,  1.4499e-01, -3.2117e-02,  6.7256e-03, -9.1146e-03,\n",
      "         5.6627e-02,  3.4365e-02,  3.5674e-02,  1.1961e-03,  9.1195e-03,\n",
      "        -1.0258e-01, -2.6809e-02, -3.6439e-02, -5.3987e-02, -3.7285e-02,\n",
      "        -4.7299e-02,  2.0322e-02, -7.9408e-02, -7.7213e-02, -4.1219e-02,\n",
      "         1.1305e-01, -3.6860e-02,  3.4759e-02,  4.5197e-03, -1.8849e-02,\n",
      "        -1.1627e-02,  7.8283e-02, -5.6437e-02,  3.5024e-02,  6.2222e-02,\n",
      "        -8.2901e-02,  7.1049e-02,  9.9048e-03,  8.3881e-02,  3.7555e-03,\n",
      "         8.8532e-02,  9.2635e-02,  1.6246e-02, -3.0551e-02,  4.0173e-02,\n",
      "         3.9328e-02,  9.8969e-03,  7.2826e-04, -8.5527e-03,  1.9672e-02,\n",
      "         1.0268e-01, -4.0752e-03, -5.5843e-02,  1.5902e-02,  7.0855e-03,\n",
      "        -3.0325e-02,  2.9130e-02, -7.9757e-02,  2.0168e-02,  1.3599e-02,\n",
      "        -2.4822e-02, -8.0696e-03,  7.8805e-03,  3.1998e-04, -3.3752e-02,\n",
      "        -2.3653e-02,  7.4149e-02, -9.0394e-03, -6.5222e-03, -3.0573e-02,\n",
      "         1.1063e-01,  7.5828e-02,  4.1677e-02,  1.3911e-02, -7.0996e-03,\n",
      "         2.3597e-03,  2.6949e-03, -5.3042e-03,  7.1347e-02,  2.7978e-02,\n",
      "         9.5793e-04, -2.3873e-02, -7.2959e-02,  3.1148e-02, -6.5378e-02,\n",
      "         4.4773e-02, -4.6407e-02, -2.7808e-02,  6.0678e-02,  2.2824e-02,\n",
      "         1.2299e-02, -1.2252e-01, -9.4176e-02, -3.1335e-02,  6.1090e-02,\n",
      "        -8.9544e-02, -7.8463e-02, -1.0646e-01,  1.2856e-01,  5.3371e-02,\n",
      "        -3.5043e-02,  4.9204e-02, -2.7718e-02, -1.8169e-03, -3.2086e-02,\n",
      "         7.7823e-03,  6.8141e-03,  9.3693e-02,  1.6695e-02, -7.0995e-03,\n",
      "        -8.1406e-02, -1.0529e-02,  2.3930e-02, -2.4667e-02,  1.4599e-02,\n",
      "         2.2815e-02,  6.4431e-02, -8.6203e-02, -1.9157e-01,  3.7300e-02,\n",
      "        -2.8549e-02, -2.9900e-02,  2.0874e-02, -1.8929e-01,  6.7435e-02,\n",
      "        -4.1862e-02,  4.9628e-04,  7.5833e-03,  8.0471e-02, -1.7851e-02,\n",
      "        -4.5390e-02,  2.1833e-02, -1.6886e-02, -1.0043e-02, -7.4905e-02,\n",
      "        -9.9795e-04, -2.0626e-02,  8.3278e-02, -7.4464e-02,  3.2107e-02,\n",
      "         4.9412e-02, -5.9202e-02, -6.2015e-02,  1.0825e-02,  8.4142e-02,\n",
      "         6.0584e-02,  2.8453e-02, -6.4364e-02,  3.4312e-02, -3.1387e-02,\n",
      "        -1.0054e-02,  6.2364e-02,  9.9319e-02,  4.8268e-02,  3.6428e-02,\n",
      "         4.7602e-02, -2.9711e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 1.7651e-03,  3.1396e-03, -1.4551e-02, -3.5147e-03, -2.1400e-02,\n",
      "         1.1861e-01,  2.7863e-02, -5.9323e-02,  5.4408e-02, -3.9685e-02,\n",
      "        -2.9819e-02,  1.3290e-02, -3.3680e-02, -3.0514e-02, -9.5577e-02,\n",
      "        -2.7275e-02, -3.2411e-02,  1.0115e-01,  1.8278e-02,  5.4167e-02,\n",
      "        -6.5277e-02, -4.9623e-02,  3.8521e-02, -7.7113e-02,  2.3679e-02,\n",
      "        -2.1421e-02, -3.1328e-02, -3.8675e-02,  3.2301e-02,  8.0595e-02,\n",
      "        -1.7031e-01, -3.1086e-02,  7.4435e-02,  4.4086e-02, -5.2982e-02,\n",
      "        -3.7618e-02, -2.1757e-02,  2.6901e-02,  1.2416e-02, -6.1151e-02,\n",
      "        -5.6076e-03, -8.1322e-02,  1.2883e-01,  2.1244e-01,  6.3023e-03,\n",
      "        -6.4486e-02, -6.8903e-02, -5.2496e-02, -8.8419e-03, -4.8330e-03,\n",
      "        -9.8466e-02, -9.1724e-02, -4.6670e-03,  2.2265e-02,  7.4199e-03,\n",
      "        -6.7150e-03,  3.1992e-03, -1.9731e-02,  1.7806e-02, -7.7666e-03,\n",
      "         6.6665e-03, -4.9659e-03,  1.3266e-02, -3.1188e-02,  6.3222e-02,\n",
      "         2.4398e-02,  2.9437e-02, -7.7957e-04,  2.4054e-02,  1.6580e-01,\n",
      "        -7.9211e-02, -2.8934e-02,  3.4830e-02,  3.1386e-02,  1.2014e-03,\n",
      "         9.4098e-02, -5.5012e-03,  4.5756e-02,  3.2991e-02,  1.8693e-02,\n",
      "         4.4928e-02, -2.1605e-02,  4.0092e-02, -6.9511e-02, -8.6237e-02,\n",
      "        -1.2794e-01,  2.8559e-02, -3.4364e-02,  2.3834e-03,  9.2352e-03,\n",
      "        -2.0991e-03,  1.2794e-02,  1.0197e-02, -1.4751e-02, -5.3813e-03,\n",
      "         2.9286e-02,  8.8126e-02,  1.5448e-02, -1.4078e-02, -3.9143e-02,\n",
      "        -5.8560e-02,  5.4407e-02,  3.5490e-02, -7.9659e-02,  3.4453e-02,\n",
      "         2.5864e-02, -3.0899e-02, -1.5625e-02, -4.5447e-02,  4.7464e-02,\n",
      "        -3.1091e-02, -3.4445e-02, -5.4052e-02, -6.4918e-02,  4.4487e-02,\n",
      "         2.5045e-02, -1.1488e-02,  2.5262e-02, -1.2607e-02,  1.3235e-02,\n",
      "         2.8561e-02,  6.9778e-02,  3.5717e-02, -1.3796e-02, -1.6055e-01,\n",
      "        -6.3508e-02,  3.0388e-02,  3.6702e-02,  1.4510e-02,  8.2649e-02,\n",
      "        -9.6217e-03,  2.8959e-02,  3.8684e-02, -8.4300e-02, -1.5368e-01,\n",
      "         9.8709e-02, -7.2473e-02,  3.1997e-02,  1.1817e-01, -2.6140e-02,\n",
      "        -6.1742e-02, -1.6166e-02,  7.0216e-02, -1.2530e-01, -3.3601e-02,\n",
      "         1.8504e-02,  4.9253e-02,  1.5496e-01, -7.7431e-02, -1.4273e-02,\n",
      "        -1.3381e-02,  1.0467e-01, -7.3973e-02, -9.8395e-02, -2.9553e-02,\n",
      "         4.8231e-02,  6.4982e-02, -5.0469e-02,  3.5893e-02,  9.4489e-02,\n",
      "         6.2196e-02, -9.2381e-02, -8.7598e-02,  7.9401e-02, -6.6444e-02,\n",
      "        -1.0009e-02, -3.8275e-02, -2.5270e-02, -1.7952e-01, -9.5267e-03,\n",
      "        -1.3783e-01,  2.1312e-01, -1.1740e-02, -8.2986e-02,  3.5087e-02,\n",
      "        -1.9155e-02, -2.4328e-02, -4.0487e-02,  3.3686e-02, -1.7021e-02,\n",
      "        -5.0354e-02, -1.5596e-01, -1.7125e-03,  5.6674e-02,  6.6230e-03,\n",
      "         6.4058e-03, -3.7337e-03,  1.1259e-02, -2.4012e-02,  8.4532e-02,\n",
      "        -2.1994e-02,  3.6341e-03,  8.1102e-02, -5.8442e-02,  9.7022e-02,\n",
      "        -6.0901e-02,  5.0808e-02,  1.3352e-01,  1.6406e-02,  1.3148e-02,\n",
      "         2.8686e-02, -3.0704e-02, -4.3113e-02,  5.2098e-02, -5.5051e-02,\n",
      "        -1.1791e-01,  5.0002e-02,  2.3706e-03, -6.4074e-02,  5.0139e-02,\n",
      "        -3.7592e-02,  5.3099e-02,  3.9144e-02,  4.3691e-03,  1.4775e-02,\n",
      "        -7.3321e-02, -4.6698e-02,  1.2764e-01, -6.2895e-02, -2.6595e-02,\n",
      "         7.9530e-02,  3.6950e-02, -4.7796e-03,  3.2136e-02, -4.4875e-02,\n",
      "        -3.2131e-02,  8.3086e-02,  8.9513e-02, -6.2051e-03, -1.2118e-01,\n",
      "         2.6485e-02, -3.3139e-02,  4.4756e-02,  7.8008e-04,  7.1055e-02,\n",
      "         3.0050e-02,  8.2575e-03, -2.6538e-02, -3.9907e-02, -2.5800e-02,\n",
      "        -3.3800e-02,  1.8517e-02, -7.0688e-02, -1.3011e-01, -3.3101e-02,\n",
      "        -5.4424e-02,  3.0215e-02, -6.2839e-02,  2.4651e-02, -1.8812e-03,\n",
      "        -1.3442e-01,  1.2847e-02,  7.9453e-02,  8.0802e-02, -9.5993e-02,\n",
      "         3.4160e-02,  2.6102e-02, -8.6553e-03,  5.7268e-02,  8.5350e-02,\n",
      "         1.3918e-02,  1.1504e-02,  2.9779e-03,  1.0623e-02,  5.5536e-02,\n",
      "        -4.1146e-02, -9.3039e-02, -3.3455e-03,  1.5882e-02, -1.5050e-01,\n",
      "         7.5856e-03,  2.2823e-02, -3.8871e-02,  5.5844e-02,  5.4641e-03,\n",
      "        -2.4733e-02, -5.1179e-02, -1.8616e-02,  5.5658e-02, -6.9583e-02,\n",
      "        -6.0925e-02, -8.0161e-02, -1.0143e-01,  4.3837e-02,  1.3554e-01,\n",
      "         8.7156e-02,  2.5922e-02, -7.2726e-02, -1.8920e-02,  9.7482e-02,\n",
      "         2.0591e-02, -6.2224e-02,  5.4904e-02, -1.3960e-01, -7.6254e-02,\n",
      "         8.3799e-02, -3.9226e-02, -4.3723e-02, -3.3469e-02,  9.1810e-03,\n",
      "         4.9622e-02,  6.3080e-02, -2.8480e-02, -1.8700e-02,  6.6885e-02,\n",
      "        -6.8625e-03,  7.1043e-02,  7.1088e-02, -9.2783e-02,  9.1262e-02,\n",
      "         4.6247e-02, -2.9005e-02,  2.8690e-02,  1.9394e-02,  5.7164e-05,\n",
      "         2.2624e-02,  3.3163e-02,  1.7700e-02,  3.4232e-02, -2.9858e-02,\n",
      "        -7.4267e-02,  3.6014e-02, -4.4552e-02,  3.5258e-02, -1.0101e-01,\n",
      "        -6.7129e-03,  1.4119e-02,  2.7532e-02,  1.8333e-02,  1.0998e-01,\n",
      "        -4.3879e-04,  6.3078e-02,  1.9749e-02,  4.5188e-02,  1.7698e-02,\n",
      "        -1.6677e-02,  8.2497e-02, -7.5923e-02,  6.3407e-02,  6.3229e-02,\n",
      "         1.7209e-02,  8.9937e-02, -3.1758e-02,  2.4061e-02, -7.6937e-02,\n",
      "         2.9163e-03, -6.6448e-02, -1.3663e-02, -3.8498e-02, -6.1970e-02,\n",
      "        -5.3004e-02,  2.5560e-02,  1.7372e-01,  1.9347e-02,  7.7611e-02,\n",
      "         1.2019e-01, -1.5177e-01, -1.0369e-02, -3.0696e-02,  6.5096e-02,\n",
      "         1.3015e-02,  5.4550e-02, -5.5283e-02,  7.5891e-03, -2.0863e-02,\n",
      "        -2.2272e-02,  1.8210e-02, -6.6587e-03, -1.3865e-02,  5.7003e-02,\n",
      "        -1.9093e-02,  9.1872e-03,  9.9067e-02,  3.3590e-04,  4.0905e-02,\n",
      "         2.1044e-03, -6.7002e-03,  2.9374e-02, -1.1736e-02,  3.6019e-03,\n",
      "        -2.4367e-02, -3.7626e-02, -1.1231e-01,  1.7375e-02, -6.0035e-03,\n",
      "         5.7686e-02, -2.7193e-02,  1.9783e-02, -6.3263e-02,  2.2237e-02,\n",
      "         7.3779e-03, -2.8759e-03, -1.5603e-02,  5.7662e-02,  8.7457e-03,\n",
      "         1.0018e-02, -5.0072e-02, -3.7638e-02,  2.4585e-02, -9.2793e-02,\n",
      "        -1.1872e-01, -2.2116e-02, -1.0956e-01, -1.0836e-01,  8.6403e-02,\n",
      "         4.3467e-02,  2.0700e-02,  5.1945e-02,  3.0060e-02,  2.7744e-02,\n",
      "        -9.2273e-03,  7.5827e-02, -4.5446e-02,  8.2869e-02, -9.2931e-02,\n",
      "         1.2670e-02, -4.8179e-02, -1.5450e-01, -1.6038e-03, -2.9253e-02,\n",
      "        -2.7980e-02, -1.0475e-02,  2.7516e-02,  1.6998e-01,  2.4017e-02,\n",
      "         8.4535e-02, -2.9163e-04,  3.1187e-02,  5.4309e-02, -3.0479e-02,\n",
      "        -8.0611e-02, -6.6498e-02, -1.4551e-01,  2.1430e-03, -3.7552e-02,\n",
      "         7.6690e-02,  7.4113e-02, -1.0557e-01, -7.4909e-02,  6.7211e-02,\n",
      "        -7.8306e-02,  4.8829e-02, -4.6191e-02,  1.3408e-02,  2.7609e-02,\n",
      "        -7.1487e-03, -3.2693e-03,  6.9174e-02,  1.8630e-01,  8.2350e-02,\n",
      "        -7.1999e-02, -5.7636e-04, -1.0190e-01, -2.1849e-02, -2.4579e-02,\n",
      "         8.7751e-02, -3.5942e-02, -2.4704e-03, -1.1202e-01, -7.7516e-02,\n",
      "        -3.0877e-02,  5.2970e-02, -1.0476e-02,  9.5842e-03, -7.3720e-02,\n",
      "         4.0108e-02, -1.2442e-02,  3.8677e-02, -4.2649e-02,  3.2528e-02,\n",
      "         4.7383e-02, -1.2851e-03, -3.1630e-02,  9.8758e-02, -4.5205e-02,\n",
      "         9.8402e-02, -9.2297e-02, -1.9997e-02, -1.7744e-02, -2.2326e-02,\n",
      "         8.0307e-02, -1.7815e-02,  1.9394e-02, -5.2028e-02, -5.1993e-02,\n",
      "         5.9033e-03,  1.0825e-02, -1.7139e-02, -1.4043e-01,  3.3729e-02,\n",
      "         2.4079e-02,  3.1476e-02, -7.7750e-02,  4.4037e-03, -7.0054e-02,\n",
      "        -1.0412e-02,  8.7667e-03,  6.4475e-02, -6.7967e-02,  4.3379e-02,\n",
      "        -8.0798e-02,  1.3300e-01, -2.5715e-02,  4.1997e-02,  1.5607e-02,\n",
      "        -1.8457e-02, -1.4307e-02, -1.3592e-02, -8.4850e-04,  6.9601e-03,\n",
      "         1.7143e-02, -7.7591e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 7.0232e-02, -3.9177e-02, -5.6618e-02,  2.4083e-02, -1.1208e-01,\n",
      "        -8.1959e-02, -1.0443e-02,  3.7170e-02, -5.0199e-02,  3.4944e-02,\n",
      "        -1.1758e-01,  2.1176e-02, -1.3711e-02,  1.7198e-03,  5.5355e-03,\n",
      "        -2.0753e-02, -4.9122e-02, -1.6634e-02,  8.6761e-04,  5.3412e-02,\n",
      "        -5.2480e-02,  3.0198e-03, -2.5701e-02, -1.2355e-01,  5.1328e-02,\n",
      "        -6.1343e-02, -4.0117e-02,  6.6909e-02,  5.3694e-03,  2.3511e-02,\n",
      "        -8.8978e-03, -1.3987e-02,  5.8384e-02, -7.4027e-02,  7.0214e-03,\n",
      "        -2.3619e-02, -1.1647e-02,  5.0248e-03, -1.0472e-01,  5.0924e-02,\n",
      "         9.6729e-03,  4.4740e-02, -8.4304e-03,  3.1834e-02, -5.6028e-02,\n",
      "         1.2579e-02,  4.1046e-02, -7.6398e-02, -8.2215e-02, -7.4826e-02,\n",
      "        -5.8703e-03,  2.1899e-02, -3.4924e-02, -9.3841e-02,  9.2489e-02,\n",
      "        -3.9724e-02,  6.8778e-02,  6.2910e-02,  1.5035e-01, -8.5688e-02,\n",
      "         5.1889e-02, -9.3605e-02, -7.0402e-02,  4.7219e-02,  5.9798e-02,\n",
      "        -3.6312e-03, -1.3177e-02, -4.6579e-02,  2.6072e-02, -1.8031e-02,\n",
      "        -1.5455e-01,  1.6608e-01, -1.5167e-03, -2.2081e-02, -3.3239e-02,\n",
      "         7.1615e-03,  5.0772e-02,  6.4464e-03, -1.0717e-03,  1.1329e-01,\n",
      "        -4.0795e-03,  7.9883e-02, -4.3044e-02,  1.3580e-01, -1.0705e-02,\n",
      "         7.0666e-03, -7.1443e-03,  9.1426e-02, -1.3554e-03, -9.4658e-02,\n",
      "        -4.0040e-02,  5.9643e-02,  1.8720e-02, -6.1085e-03, -5.1143e-03,\n",
      "         5.2426e-03,  3.9795e-02,  5.7733e-02,  9.3336e-02,  4.6847e-03,\n",
      "         5.8702e-02,  2.5341e-02,  4.2893e-02,  7.5947e-02,  2.8520e-04,\n",
      "         7.1536e-03, -4.3884e-03, -4.4555e-02, -4.4503e-02,  5.6192e-02,\n",
      "        -5.1656e-02, -1.2393e-01,  4.3672e-02,  4.7996e-02, -1.0800e-02,\n",
      "         5.6755e-02, -8.3568e-02, -1.3538e-02, -5.6153e-02, -5.5316e-02,\n",
      "        -2.3975e-02, -1.1282e-01, -2.8566e-02,  7.2766e-02, -3.8624e-02,\n",
      "         7.6615e-02,  3.6164e-02,  1.0354e-01,  4.9160e-02,  1.9378e-02,\n",
      "        -2.2329e-02, -1.2350e-01,  1.2831e-01,  9.7161e-03,  8.3806e-02,\n",
      "        -5.0945e-02, -2.3909e-02, -2.4867e-02,  5.3618e-02,  4.3033e-02,\n",
      "        -8.6281e-03, -3.7764e-02, -1.2432e-01,  1.3901e-02, -8.2746e-02,\n",
      "         1.5292e-02, -1.0102e-01, -2.1163e-03,  2.4047e-02, -3.3842e-02,\n",
      "         1.7279e-01, -2.0493e-03, -1.4493e-02,  5.7667e-02, -2.8942e-02,\n",
      "        -3.2882e-03,  7.1961e-02,  1.5763e-02, -1.0857e-01,  3.1682e-02,\n",
      "        -1.5458e-02,  2.3903e-02, -7.8493e-02,  3.3385e-02, -1.1762e-02,\n",
      "         5.4726e-02, -1.0496e-01, -1.9116e-02,  4.4039e-02, -4.5159e-02,\n",
      "         1.1691e-01, -7.5459e-02, -3.4751e-02, -7.0932e-05, -5.4284e-03,\n",
      "        -3.1645e-02,  7.8052e-02, -1.3927e-02, -3.9138e-02, -6.9432e-02,\n",
      "        -5.6814e-02,  4.7092e-02, -9.7913e-02, -7.1706e-02, -7.4354e-02,\n",
      "         2.9061e-02,  1.2788e-01,  3.2878e-02,  6.8620e-02,  7.8050e-03,\n",
      "        -8.1034e-03,  1.2591e-01, -2.5306e-02,  2.3245e-02,  6.0525e-03,\n",
      "         5.1102e-02,  2.6583e-02, -2.1282e-03, -5.5411e-02,  4.6495e-02,\n",
      "        -2.4725e-02,  2.2852e-02, -1.2736e-02,  1.6637e-01, -5.4719e-02,\n",
      "         8.6107e-02, -5.4407e-02,  6.8237e-02, -6.1891e-02, -5.5849e-02,\n",
      "         7.3760e-03, -3.0345e-02, -3.1600e-02,  3.3583e-02,  2.8570e-02,\n",
      "         8.2200e-02,  2.6655e-02,  2.6249e-02, -1.2001e-02,  7.8356e-02,\n",
      "        -1.6183e-02, -1.4890e-02,  1.2511e-02,  3.7454e-02,  2.5717e-02,\n",
      "         2.4392e-03,  1.9375e-02,  6.4533e-02,  3.3817e-02, -6.6789e-02,\n",
      "        -8.1340e-02, -3.5166e-02, -2.8866e-02, -7.5490e-02,  3.9034e-02,\n",
      "        -4.9257e-02,  1.5981e-02,  1.2176e-02,  2.2973e-02,  1.0207e-02,\n",
      "        -1.0285e-03,  1.7862e-01,  6.4228e-02, -3.5339e-02,  8.1926e-02,\n",
      "         7.1711e-02, -1.0528e-02,  3.6034e-02, -2.3140e-02,  4.6343e-02,\n",
      "        -3.3368e-02, -4.6355e-02,  5.1168e-02,  1.8313e-02, -3.8195e-03,\n",
      "         1.0237e-01, -4.0303e-02,  3.3172e-02, -4.5773e-02, -1.4106e-02,\n",
      "        -3.1364e-02,  5.1665e-02, -3.1724e-02,  3.0433e-02, -3.9412e-02,\n",
      "        -3.3040e-02,  2.1146e-02, -1.1771e-01, -6.6739e-02, -3.3981e-02,\n",
      "        -4.4390e-03,  2.8506e-02,  1.9362e-02,  1.0839e-01, -1.5109e-02,\n",
      "         1.5135e-01, -2.9912e-02,  7.0132e-02, -6.2905e-02, -9.2045e-02,\n",
      "        -1.0811e-01, -6.4596e-02,  1.1569e-01,  4.2324e-02, -5.3588e-02,\n",
      "         2.2440e-02, -2.2649e-02, -7.6581e-02, -6.1811e-02,  6.7117e-02,\n",
      "        -8.8734e-02,  1.1926e-02, -1.0264e-02,  2.1893e-02,  6.1756e-02,\n",
      "         1.1959e-01, -9.6380e-02,  1.3470e-02, -7.0965e-02,  2.2478e-02,\n",
      "         5.0166e-02, -4.6788e-03,  9.3105e-02,  1.2183e-01, -1.0024e-01,\n",
      "         1.9777e-04,  8.1114e-02, -2.6921e-02,  1.0334e-01, -3.6504e-02,\n",
      "         1.0802e-02, -2.5081e-02, -6.5181e-02,  8.6339e-02,  3.7305e-02,\n",
      "        -1.2546e-01, -2.3171e-02, -5.1505e-02,  8.1840e-02,  4.9002e-02,\n",
      "         1.8363e-02,  2.9693e-02, -3.8902e-03, -4.1257e-02, -2.2935e-02,\n",
      "         8.3203e-02,  5.9329e-02,  7.7033e-03,  4.9673e-02, -3.4751e-02,\n",
      "        -3.4831e-03, -7.7208e-03,  1.0457e-01, -2.1170e-02,  6.3125e-02,\n",
      "        -1.2047e-02,  1.4499e-02, -5.0847e-02,  2.7684e-02,  8.1270e-02,\n",
      "        -2.4067e-02,  1.6061e-04,  4.5172e-02,  8.9830e-02,  5.0638e-03,\n",
      "        -2.7056e-02,  1.6215e-02, -1.2409e-01,  2.7129e-02, -3.0758e-02,\n",
      "        -4.1683e-02,  7.6068e-03,  1.4988e-02, -2.3955e-02, -8.0970e-02,\n",
      "        -1.0192e-01,  3.6965e-02, -2.6476e-02,  5.7144e-03,  7.6527e-02,\n",
      "         9.9065e-02,  4.3809e-02,  5.6087e-02, -6.8878e-02,  6.4834e-02,\n",
      "         2.2787e-02,  7.3976e-02,  9.9496e-03, -1.7695e-02,  8.8900e-02,\n",
      "        -9.6980e-02, -7.0818e-02,  4.9335e-02, -5.5873e-02, -6.7333e-03,\n",
      "        -1.0160e-02,  2.7102e-02, -2.2473e-02, -4.0724e-02, -4.3555e-02,\n",
      "        -4.3084e-02, -8.1544e-02, -2.8473e-02,  1.8932e-02,  2.8450e-02,\n",
      "        -8.1699e-02, -8.1030e-02,  4.9583e-02,  3.5871e-02, -1.5891e-02,\n",
      "        -8.8298e-03, -1.2130e-02, -8.1447e-02,  4.5123e-02,  6.6769e-02,\n",
      "         4.5007e-02, -1.0901e-02, -1.5257e-01,  1.2816e-02,  6.9188e-02,\n",
      "        -1.3537e-02,  1.0406e-01,  2.4015e-02,  4.3749e-02, -2.3074e-02,\n",
      "         6.2925e-02, -3.2508e-02, -5.4690e-02,  1.6847e-02, -5.7822e-02,\n",
      "         6.0195e-02, -1.6088e-02, -3.6611e-02,  4.3164e-03, -1.6129e-02,\n",
      "        -2.0027e-02,  2.7187e-02, -5.7546e-02,  1.6556e-02, -4.5320e-04,\n",
      "         2.4637e-02, -5.7647e-02, -4.5837e-02,  1.4810e-02,  1.4818e-02,\n",
      "        -9.2751e-03,  3.1316e-02,  4.6298e-02,  1.5679e-02,  2.5335e-02,\n",
      "         1.5162e-02, -6.5274e-02, -1.1448e-01,  3.5900e-02, -1.1034e-01,\n",
      "        -9.4011e-02,  3.3696e-02, -6.7059e-03,  1.4441e-02,  1.3973e-01,\n",
      "         7.2340e-02, -5.2067e-02, -1.5580e-02,  4.3312e-02, -6.7398e-02,\n",
      "         7.6808e-02, -4.1142e-02,  3.2319e-02,  1.2461e-01,  1.5610e-02,\n",
      "         7.3369e-02, -1.0851e-01, -4.5686e-02, -6.5544e-02,  7.0161e-02,\n",
      "        -4.9590e-03,  4.6399e-02,  4.5816e-02, -7.6833e-02,  5.7388e-02,\n",
      "         5.6216e-02,  1.7794e-02, -1.8920e-02, -4.4150e-02,  2.6347e-02,\n",
      "         8.7239e-02, -2.0536e-02, -1.2006e-02, -5.0354e-03,  3.5649e-02,\n",
      "        -8.1056e-02,  5.1311e-02,  1.9925e-02, -4.3425e-02,  2.6601e-02,\n",
      "        -7.5502e-02, -3.4638e-02, -7.5277e-02, -5.1211e-02, -4.9907e-02,\n",
      "         1.9271e-02,  2.3710e-02,  1.7192e-02, -7.7708e-02,  2.5729e-02,\n",
      "         5.5325e-02,  1.0182e-01, -9.2568e-02, -4.8824e-02,  2.3749e-02,\n",
      "         3.6623e-02, -1.6246e-02, -2.5600e-02, -8.7405e-02,  1.7550e-02,\n",
      "        -6.1699e-03, -4.0138e-02, -3.5954e-02, -6.4890e-02,  4.1684e-03,\n",
      "        -8.0014e-02, -7.6652e-02,  9.0478e-02,  2.2696e-03,  4.3178e-03,\n",
      "         1.3625e-01, -4.3848e-02,  3.4243e-02,  1.0695e-01,  2.2553e-02,\n",
      "        -1.3336e-02, -3.6943e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-6.3037e-02,  4.0266e-02, -4.1982e-02, -1.3677e-01, -2.8125e-02,\n",
      "        -3.5907e-02, -1.2441e-01,  3.9413e-02,  7.0257e-02,  2.5938e-02,\n",
      "         1.6168e-02, -4.9772e-03, -8.1783e-02,  5.8682e-02, -5.5678e-03,\n",
      "        -1.6609e-02, -7.0250e-02,  2.3007e-02, -1.1237e-01, -1.1362e-03,\n",
      "         2.6813e-02,  5.1206e-02, -1.0730e-01, -7.5571e-03,  6.9360e-02,\n",
      "        -3.0615e-02, -1.3997e-01, -1.6990e-02, -6.2863e-03,  3.9499e-02,\n",
      "         1.1053e-01,  7.3368e-02, -4.2755e-03, -6.1965e-02,  2.3791e-02,\n",
      "        -5.2535e-02, -2.8180e-02, -1.5272e-02, -2.4229e-02,  8.8722e-02,\n",
      "        -9.6024e-02,  4.9923e-02, -1.7209e-02,  1.6388e-02, -7.5840e-03,\n",
      "        -8.1901e-02,  6.1073e-02, -4.8348e-02, -1.0459e-02, -6.6470e-02,\n",
      "        -2.4781e-02,  5.3203e-02,  2.4020e-02,  4.9423e-02,  3.7947e-02,\n",
      "         2.0673e-01,  6.0895e-02, -4.0758e-02,  2.0071e-02,  1.0387e-01,\n",
      "         1.6163e-02, -1.5090e-02,  6.2854e-02,  2.6011e-02,  3.8561e-02,\n",
      "        -5.5162e-02, -7.5800e-02,  6.2836e-02,  1.3994e-02,  1.1939e-01,\n",
      "        -1.5908e-02,  4.2999e-02, -2.3383e-02,  1.5983e-02, -1.3671e-02,\n",
      "         9.9919e-02,  6.4056e-02, -7.5082e-02, -1.2190e-02, -1.4694e-02,\n",
      "         5.8069e-02,  5.5209e-02,  1.2079e-02,  5.1134e-02,  4.5578e-02,\n",
      "         5.2929e-02, -6.7412e-03,  9.7755e-02, -4.7786e-02, -1.6850e-02,\n",
      "        -5.9766e-02,  9.2122e-02, -2.9754e-02, -1.1698e-01,  2.3706e-02,\n",
      "        -2.3814e-02,  3.3031e-02, -1.1580e-01,  3.8596e-02, -3.3136e-03,\n",
      "        -1.2250e-02,  3.4611e-02, -8.4193e-02, -7.5750e-02, -3.5521e-02,\n",
      "        -5.1473e-03,  3.9007e-02,  6.4325e-03, -5.9280e-02, -1.3100e-02,\n",
      "        -4.1139e-02,  4.7848e-02,  8.4264e-03, -1.0753e-01, -4.3760e-02,\n",
      "         9.4994e-02, -1.7219e-02,  3.9596e-02, -4.1659e-02,  1.2531e-01,\n",
      "        -4.9070e-02,  1.2569e-02, -3.4510e-02,  5.6004e-02, -2.7773e-02,\n",
      "        -8.0413e-02,  7.7013e-02, -3.7365e-02, -7.8601e-02, -4.4590e-02,\n",
      "         1.6158e-02,  2.7064e-02,  1.0510e-01, -1.2408e-02,  1.6963e-02,\n",
      "        -9.1978e-03,  5.7486e-02, -4.6821e-02, -3.0573e-03, -1.0964e-02,\n",
      "        -8.9452e-02,  4.2682e-02, -1.1941e-02,  2.5132e-02, -3.7705e-02,\n",
      "         5.4186e-02, -7.1975e-02, -4.9173e-02, -6.7192e-02,  2.7494e-02,\n",
      "         2.4167e-03,  3.7371e-02,  4.2284e-02,  3.3118e-02,  5.1909e-02,\n",
      "        -6.6921e-02, -5.8869e-02, -6.1932e-02,  3.1455e-02, -2.2885e-02,\n",
      "        -9.3647e-02, -1.9637e-02,  5.1098e-02,  4.5610e-02, -4.1068e-02,\n",
      "         5.7816e-02, -8.5963e-04,  2.2186e-02, -1.8173e-02,  4.3025e-02,\n",
      "        -3.6500e-02,  4.6611e-02,  1.1417e-01, -6.0109e-02, -6.6532e-02,\n",
      "         9.2543e-02,  1.5739e-02, -7.0260e-03, -4.5298e-02, -4.6085e-02,\n",
      "        -1.7641e-02, -3.4245e-02, -2.9982e-02, -3.3564e-02, -2.3251e-02,\n",
      "        -9.0132e-02, -4.9113e-02, -1.5003e-02, -3.4544e-02, -1.2240e-02,\n",
      "        -6.6013e-02, -1.2225e-01,  2.1974e-02, -7.2869e-02,  7.3213e-02,\n",
      "         7.8171e-02, -1.1407e-02,  1.2900e-02,  1.3423e-02,  6.1885e-02,\n",
      "         8.2777e-02,  5.9639e-03, -2.9608e-02,  1.4335e-02, -3.0911e-02,\n",
      "        -2.6568e-02, -7.7970e-02,  5.7262e-02,  7.5148e-03, -8.3736e-02,\n",
      "         1.1164e-01, -3.6595e-02, -3.5647e-02, -2.2155e-02,  3.7071e-02,\n",
      "         5.2191e-03, -5.0187e-02, -1.0465e-02, -2.7389e-02,  2.4710e-02,\n",
      "         3.4442e-02, -3.3596e-02,  5.7857e-02,  4.2296e-02, -2.8121e-02,\n",
      "         3.7366e-02, -5.9914e-02,  1.6653e-02,  3.8050e-02,  5.3976e-02,\n",
      "         1.6561e-02,  5.0949e-02,  7.7352e-02,  8.3561e-02, -4.3670e-02,\n",
      "        -8.8957e-03,  2.0743e-03,  3.0768e-02, -3.4656e-02,  1.0132e-01,\n",
      "         2.0802e-02, -1.4734e-01, -1.1625e-02, -4.6762e-03,  1.0868e-01,\n",
      "         8.2071e-02, -1.4927e-02, -1.5449e-01, -7.1360e-02,  6.3504e-02,\n",
      "        -1.3678e-02, -3.2650e-02,  8.5200e-02, -4.5086e-02,  2.2611e-02,\n",
      "        -1.0392e-01, -6.0944e-02,  1.4738e-02,  3.9227e-02, -8.7592e-03,\n",
      "        -2.2234e-02, -5.5263e-03, -3.3027e-02,  3.9625e-03,  1.5417e-02,\n",
      "         1.2909e-02,  1.0592e-01, -5.5637e-02,  1.6255e-01, -8.2178e-02,\n",
      "         9.2043e-02,  1.9381e-03,  2.2714e-02,  3.5822e-02, -1.0901e-03,\n",
      "         1.2325e-02, -6.4859e-02, -2.5885e-02,  5.1314e-02, -4.6941e-04,\n",
      "        -2.8895e-03,  1.1293e-02, -1.7513e-02, -6.6949e-02,  6.9416e-02,\n",
      "         7.1142e-03, -1.4641e-03, -3.6779e-02,  1.1385e-01, -4.7641e-02,\n",
      "         1.4738e-02, -6.2718e-02,  8.7415e-02, -5.5629e-03,  2.7129e-02,\n",
      "         6.3722e-03,  3.4799e-02,  2.5760e-02, -7.6286e-02, -6.1321e-02,\n",
      "        -5.3081e-02, -1.3048e-03, -1.7442e-02, -1.6667e-01,  3.7299e-03,\n",
      "         1.3328e-02,  6.2362e-02,  1.6265e-02,  5.9280e-02, -9.6899e-02,\n",
      "        -9.9530e-03,  3.4732e-02,  5.4185e-03, -4.3835e-03,  3.4801e-02,\n",
      "         4.0341e-02, -1.1303e-02, -2.8805e-02,  2.6510e-02, -4.8988e-02,\n",
      "        -1.4906e-02, -8.7503e-02, -3.8591e-03,  3.9093e-02,  2.1345e-02,\n",
      "         4.3803e-02, -4.8825e-02, -3.8691e-02, -7.1864e-02, -5.9994e-02,\n",
      "         2.5898e-02, -4.4769e-02,  8.8324e-02, -7.2772e-02,  1.5155e-02,\n",
      "        -5.5817e-02,  5.3736e-02, -2.9101e-02,  1.5793e-03, -1.7930e-01,\n",
      "        -1.7445e-02, -6.8678e-02, -2.1378e-02, -4.4950e-02, -1.7106e-02,\n",
      "         1.5411e-01,  7.0336e-02,  3.1394e-02,  9.1400e-02, -6.3379e-02,\n",
      "         9.3097e-02,  6.2873e-02, -2.3895e-02,  4.8823e-02,  1.5050e-02,\n",
      "         1.5749e-01,  2.0483e-02,  2.5478e-02,  1.2565e-01,  6.4963e-02,\n",
      "        -3.3720e-02,  3.8453e-02, -6.7775e-02, -1.1753e-01,  6.8093e-02,\n",
      "         5.1249e-02, -1.5064e-01, -6.5369e-02,  4.8224e-02, -8.1458e-03,\n",
      "        -2.7762e-02, -2.5249e-02, -1.0149e-02, -1.9384e-02,  4.1005e-02,\n",
      "        -2.7609e-02, -9.2976e-02,  3.8276e-02,  7.2089e-02, -1.2936e-01,\n",
      "        -1.1778e-01, -6.5505e-02,  1.7166e-02,  1.5751e-02, -1.9162e-02,\n",
      "         5.5185e-03, -1.0558e-01, -2.3025e-02, -1.4394e-01,  1.1885e-01,\n",
      "         9.7875e-03, -9.7859e-02, -3.9622e-02, -4.5969e-02, -4.3369e-02,\n",
      "        -2.5617e-02, -5.2712e-02,  3.9468e-02,  1.0800e-01,  5.3185e-02,\n",
      "        -5.0451e-02,  5.3125e-02,  1.4214e-01,  1.0340e-01, -1.7702e-02,\n",
      "        -6.3901e-02,  3.0720e-02, -7.3908e-02,  9.5226e-02,  6.2002e-03,\n",
      "         5.0914e-02, -6.5561e-02,  5.4568e-02,  5.1027e-02, -4.2785e-02,\n",
      "        -7.9318e-02,  6.1157e-02,  6.2453e-02, -4.5603e-02, -2.7345e-02,\n",
      "        -5.6974e-02,  1.2981e-01,  1.0213e-01,  4.7302e-02, -2.4651e-02,\n",
      "        -3.3669e-02, -4.9926e-02,  7.3012e-02, -3.4709e-02,  1.2907e-01,\n",
      "         6.1702e-02,  3.1375e-02,  1.9113e-02, -9.1100e-02,  7.3931e-03,\n",
      "        -8.0293e-02, -3.6101e-02,  5.1210e-02, -2.9621e-02,  5.9973e-03,\n",
      "         9.6392e-02,  4.1492e-03,  2.3054e-02, -8.5028e-02,  1.3075e-03,\n",
      "        -8.0786e-02,  7.1889e-02, -3.7784e-02,  2.0823e-02, -5.1179e-02,\n",
      "         1.3547e-01,  4.0677e-02,  6.0206e-02, -4.4987e-03, -1.4705e-02,\n",
      "        -4.3924e-05,  2.2686e-02, -3.4385e-02, -3.4656e-02, -1.7687e-01,\n",
      "         4.2150e-02, -3.2622e-03, -4.4221e-02, -5.9327e-02, -1.2178e-01,\n",
      "        -9.8243e-02,  2.9285e-02,  1.0800e-01, -2.9136e-02, -1.2633e-02,\n",
      "        -8.9605e-02,  1.0191e-02,  2.9528e-02, -1.6184e-02, -2.1323e-02,\n",
      "         4.3191e-02, -5.9493e-02, -9.2964e-02, -2.2478e-02,  1.4769e-02,\n",
      "        -2.1768e-02,  4.5379e-02,  7.2459e-02,  6.8969e-02, -3.1864e-02,\n",
      "         9.2427e-03,  1.1675e-01, -1.2651e-02, -8.6167e-02, -7.0927e-02,\n",
      "        -3.2216e-02, -3.6091e-02,  1.1292e-02, -2.3667e-02,  1.0530e-01,\n",
      "        -2.7349e-02,  3.5006e-02, -8.4804e-02,  5.3443e-02, -3.9848e-02,\n",
      "        -1.8628e-02, -9.9607e-02, -1.0862e-01,  3.0266e-03,  6.8604e-02,\n",
      "        -2.6277e-02, -1.4869e-01, -4.6595e-02, -9.2243e-02,  5.9800e-02,\n",
      "         1.6846e-02, -5.4605e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss: 17.621099/310.738190, val:  37.92%, val_best:  37.92%, tr:  28.50%, tr_best:  28.50%, epoch time: 50.01 seconds, 0.83 minutes\n",
      "[module.layers.3] weight_fb parameter count: 5,120\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss: 15.434777/316.276489, val:  46.25%, val_best:  46.25%, tr:  40.14%, tr_best:  40.14%, epoch time: 50.24 seconds, 0.84 minutes\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss: 14.706804/346.766663, val:  49.17%, val_best:  49.17%, tr:  43.92%, tr_best:  43.92%, epoch time: 48.13 seconds, 0.80 minutes\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss: 13.462860/478.927277, val:  39.58%, val_best:  49.17%, tr:  47.70%, tr_best:  47.70%, epoch time: 49.09 seconds, 0.82 minutes\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss: 11.481095/462.697723, val:  46.25%, val_best:  49.17%, tr:  53.32%, tr_best:  53.32%, epoch time: 48.31 seconds, 0.81 minutes\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss: 12.307410/448.197815, val:  55.00%, val_best:  55.00%, tr:  52.60%, tr_best:  53.32%, epoch time: 47.02 seconds, 0.78 minutes\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss: 11.217600/378.438660, val:  50.83%, val_best:  55.00%, tr:  54.65%, tr_best:  54.65%, epoch time: 47.65 seconds, 0.79 minutes\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  9.912814/379.952942, val:  52.50%, val_best:  55.00%, tr:  59.86%, tr_best:  59.86%, epoch time: 49.01 seconds, 0.82 minutes\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss: 10.532617/308.447784, val:  55.00%, val_best:  55.00%, tr:  57.61%, tr_best:  59.86%, epoch time: 48.23 seconds, 0.80 minutes\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  9.496074/424.835266, val:  55.83%, val_best:  55.83%, tr:  60.67%, tr_best:  60.67%, epoch time: 47.95 seconds, 0.80 minutes\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  9.158463/426.863922, val:  50.83%, val_best:  55.83%, tr:  63.64%, tr_best:  63.64%, epoch time: 47.11 seconds, 0.79 minutes\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  8.962743/330.328583, val:  56.25%, val_best:  56.25%, tr:  63.74%, tr_best:  63.74%, epoch time: 48.01 seconds, 0.80 minutes\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  7.458382/312.417267, val:  57.92%, val_best:  57.92%, tr:  66.80%, tr_best:  66.80%, epoch time: 47.81 seconds, 0.80 minutes\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  8.300222/444.419037, val:  53.33%, val_best:  57.92%, tr:  63.74%, tr_best:  66.80%, epoch time: 48.47 seconds, 0.81 minutes\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  8.120771/508.560333, val:  41.67%, val_best:  57.92%, tr:  66.70%, tr_best:  66.80%, epoch time: 49.17 seconds, 0.82 minutes\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  8.165968/339.902466, val:  56.25%, val_best:  57.92%, tr:  65.58%, tr_best:  66.80%, epoch time: 48.30 seconds, 0.81 minutes\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  7.687799/401.156586, val:  55.42%, val_best:  57.92%, tr:  68.13%, tr_best:  68.13%, epoch time: 48.92 seconds, 0.82 minutes\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  7.975760/296.196259, val:  55.00%, val_best:  57.92%, tr:  67.62%, tr_best:  68.13%, epoch time: 47.55 seconds, 0.79 minutes\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  7.303701/411.896759, val:  52.08%, val_best:  57.92%, tr:  67.72%, tr_best:  68.13%, epoch time: 48.41 seconds, 0.81 minutes\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  6.952067/384.082886, val:  54.58%, val_best:  57.92%, tr:  69.46%, tr_best:  69.46%, epoch time: 47.96 seconds, 0.80 minutes\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  6.787798/394.221985, val:  55.83%, val_best:  57.92%, tr:  71.81%, tr_best:  71.81%, epoch time: 49.00 seconds, 0.82 minutes\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  6.803488/334.937378, val:  62.08%, val_best:  62.08%, tr:  70.28%, tr_best:  71.81%, epoch time: 48.12 seconds, 0.80 minutes\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  6.121444/479.752808, val:  56.67%, val_best:  62.08%, tr:  71.20%, tr_best:  71.81%, epoch time: 48.13 seconds, 0.80 minutes\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  6.688763/319.787720, val:  54.58%, val_best:  62.08%, tr:  71.30%, tr_best:  71.81%, epoch time: 48.19 seconds, 0.80 minutes\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  5.806986/410.663757, val:  53.75%, val_best:  62.08%, tr:  73.14%, tr_best:  73.14%, epoch time: 47.68 seconds, 0.79 minutes\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  6.082533/553.678833, val:  51.25%, val_best:  62.08%, tr:  71.71%, tr_best:  73.14%, epoch time: 48.79 seconds, 0.81 minutes\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  5.784139/449.924866, val:  55.42%, val_best:  62.08%, tr:  74.87%, tr_best:  74.87%, epoch time: 47.73 seconds, 0.80 minutes\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  5.397505/369.731812, val:  59.17%, val_best:  62.08%, tr:  75.69%, tr_best:  75.69%, epoch time: 47.76 seconds, 0.80 minutes\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  5.469144/386.624512, val:  55.83%, val_best:  62.08%, tr:  76.00%, tr_best:  76.00%, epoch time: 47.82 seconds, 0.80 minutes\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  5.086411/361.375336, val:  60.00%, val_best:  62.08%, tr:  75.79%, tr_best:  76.00%, epoch time: 47.72 seconds, 0.80 minutes\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  5.120136/319.875671, val:  57.08%, val_best:  62.08%, tr:  77.02%, tr_best:  77.02%, epoch time: 48.88 seconds, 0.81 minutes\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  4.630672/355.132782, val:  64.58%, val_best:  64.58%, tr:  79.57%, tr_best:  79.57%, epoch time: 49.63 seconds, 0.83 minutes\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  4.375930/426.804596, val:  54.58%, val_best:  64.58%, tr:  77.73%, tr_best:  79.57%, epoch time: 47.71 seconds, 0.80 minutes\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  4.954238/293.164215, val:  64.17%, val_best:  64.58%, tr:  77.43%, tr_best:  79.57%, epoch time: 47.59 seconds, 0.79 minutes\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  4.930120/306.985016, val:  60.83%, val_best:  64.58%, tr:  77.83%, tr_best:  79.57%, epoch time: 47.56 seconds, 0.79 minutes\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  4.993119/346.479095, val:  58.75%, val_best:  64.58%, tr:  77.73%, tr_best:  79.57%, epoch time: 47.97 seconds, 0.80 minutes\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  4.348004/457.833038, val:  51.25%, val_best:  64.58%, tr:  79.57%, tr_best:  79.57%, epoch time: 47.80 seconds, 0.80 minutes\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  4.405919/351.068024, val:  62.50%, val_best:  64.58%, tr:  78.86%, tr_best:  79.57%, epoch time: 48.14 seconds, 0.80 minutes\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  4.666927/415.526154, val:  52.92%, val_best:  64.58%, tr:  75.59%, tr_best:  79.57%, epoch time: 49.48 seconds, 0.82 minutes\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  4.433917/323.303802, val:  61.67%, val_best:  64.58%, tr:  78.35%, tr_best:  79.57%, epoch time: 48.25 seconds, 0.80 minutes\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  4.148351/489.407379, val:  54.58%, val_best:  64.58%, tr:  78.65%, tr_best:  79.57%, epoch time: 49.43 seconds, 0.82 minutes\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  4.446146/455.648468, val:  57.50%, val_best:  64.58%, tr:  80.39%, tr_best:  80.39%, epoch time: 46.64 seconds, 0.78 minutes\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  3.916797/306.058960, val:  62.92%, val_best:  64.58%, tr:  80.29%, tr_best:  80.39%, epoch time: 50.03 seconds, 0.83 minutes\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  3.874251/339.746674, val:  60.00%, val_best:  64.58%, tr:  80.49%, tr_best:  80.49%, epoch time: 47.37 seconds, 0.79 minutes\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  3.672170/296.345917, val:  62.08%, val_best:  64.58%, tr:  81.72%, tr_best:  81.72%, epoch time: 50.17 seconds, 0.84 minutes\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  4.186353/238.088791, val:  69.17%, val_best:  69.17%, tr:  79.57%, tr_best:  81.72%, epoch time: 49.47 seconds, 0.82 minutes\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  3.469946/429.882355, val:  54.17%, val_best:  69.17%, tr:  81.31%, tr_best:  81.72%, epoch time: 47.65 seconds, 0.79 minutes\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  3.970762/325.526764, val:  59.58%, val_best:  69.17%, tr:  83.25%, tr_best:  83.25%, epoch time: 49.16 seconds, 0.82 minutes\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  3.950113/442.620941, val:  56.67%, val_best:  69.17%, tr:  81.41%, tr_best:  83.25%, epoch time: 48.43 seconds, 0.81 minutes\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  3.327241/465.992767, val:  60.42%, val_best:  69.17%, tr:  82.84%, tr_best:  83.25%, epoch time: 48.40 seconds, 0.81 minutes\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  3.916406/364.416718, val:  60.83%, val_best:  69.17%, tr:  82.33%, tr_best:  83.25%, epoch time: 48.48 seconds, 0.81 minutes\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  3.557309/370.807190, val:  58.75%, val_best:  69.17%, tr:  82.84%, tr_best:  83.25%, epoch time: 48.86 seconds, 0.81 minutes\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  3.716599/368.923126, val:  59.58%, val_best:  69.17%, tr:  81.92%, tr_best:  83.25%, epoch time: 48.56 seconds, 0.81 minutes\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  3.477795/258.867340, val:  69.17%, val_best:  69.17%, tr:  83.04%, tr_best:  83.25%, epoch time: 49.42 seconds, 0.82 minutes\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  3.345170/446.346252, val:  60.42%, val_best:  69.17%, tr:  84.07%, tr_best:  84.07%, epoch time: 51.14 seconds, 0.85 minutes\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  3.395916/292.390350, val:  67.08%, val_best:  69.17%, tr:  82.53%, tr_best:  84.07%, epoch time: 49.58 seconds, 0.83 minutes\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  2.946704/423.167847, val:  60.42%, val_best:  69.17%, tr:  84.88%, tr_best:  84.88%, epoch time: 50.26 seconds, 0.84 minutes\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  3.204154/362.570312, val:  62.08%, val_best:  69.17%, tr:  84.98%, tr_best:  84.98%, epoch time: 51.55 seconds, 0.86 minutes\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  3.295257/488.500885, val:  53.75%, val_best:  69.17%, tr:  84.37%, tr_best:  84.98%, epoch time: 50.20 seconds, 0.84 minutes\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  2.768618/425.635986, val:  62.08%, val_best:  69.17%, tr:  86.31%, tr_best:  86.31%, epoch time: 50.91 seconds, 0.85 minutes\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  2.613534/451.139221, val:  55.83%, val_best:  69.17%, tr:  86.21%, tr_best:  86.31%, epoch time: 49.17 seconds, 0.82 minutes\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  2.350041/509.765198, val:  55.83%, val_best:  69.17%, tr:  87.23%, tr_best:  87.23%, epoch time: 50.84 seconds, 0.85 minutes\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  2.759838/466.550323, val:  53.75%, val_best:  69.17%, tr:  86.01%, tr_best:  87.23%, epoch time: 50.74 seconds, 0.85 minutes\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  2.566473/358.164581, val:  60.83%, val_best:  69.17%, tr:  87.13%, tr_best:  87.23%, epoch time: 50.06 seconds, 0.83 minutes\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  2.546676/494.479462, val:  55.83%, val_best:  69.17%, tr:  86.41%, tr_best:  87.23%, epoch time: 48.31 seconds, 0.81 minutes\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  2.567186/387.509460, val:  59.17%, val_best:  69.17%, tr:  86.82%, tr_best:  87.23%, epoch time: 49.15 seconds, 0.82 minutes\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  2.944531/435.900299, val:  55.83%, val_best:  69.17%, tr:  84.78%, tr_best:  87.23%, epoch time: 49.78 seconds, 0.83 minutes\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  2.271554/339.572662, val:  65.00%, val_best:  69.17%, tr:  86.52%, tr_best:  87.23%, epoch time: 49.41 seconds, 0.82 minutes\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  2.945419/299.312714, val:  63.33%, val_best:  69.17%, tr:  87.23%, tr_best:  87.23%, epoch time: 51.47 seconds, 0.86 minutes\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [False]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.125,0.25,0.5, 0.75]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.1, 0.5, 1.0, 5.0, 10.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        # \"synapse_trace_const2\": {\"values\": [0.5]}, #lif_layer_v_decay\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200], [512]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [0.01, 0.001, 0.0001]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['one']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [5, 10, 15, 20, 25]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [5_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [True, False]},\n",
    "\n",
    "        # \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        # \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        # \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "        \"output_threshold\": {\"values\": [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]},\n",
    "        \"random_select_ratio\": {\"values\": [1.0, 1.25, 1.5, 1.75, 2.0, 2.5 ,3.0, 4.0, 4.5, 5.0, 5.5, 6.0]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"0\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.lif_layer_v_decay,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "        random_select_ratio  =  wandb.config.random_select_ratio,\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = '1qaukqa6'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
