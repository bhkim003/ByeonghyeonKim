{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10604/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tkinter import FALSE\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "\n",
    "                    output_threshold=0.5,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    # if DFA_on == True:\n",
    "    #     assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "    ANPI_MODE = True\n",
    "    if ANPI_MODE == True:\n",
    "        assert BPTT_on == False\n",
    "        assert single_step == False\n",
    "        assert DFA_on == True\n",
    "        assert BATCH == 1\n",
    "        assert lif_layer_v_reset >= 10000    \n",
    "        hetero_timesteps = True\n",
    "        origin_timesteps = TIME\n",
    "\n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp,\n",
    "                    ANPI_MODE).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target, output_threshold):\n",
    "            ctx.save_for_backward(input, target, torch.tensor(output_threshold, device=input.device))\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            input, target, output_threshold = ctx.saved_tensors\n",
    "            output_threshold = output_threshold.item()\n",
    "            \n",
    "            mask = (input > output_threshold).float()\n",
    "\n",
    "            # input_argmax = input.argmax(dim=1)\n",
    "            # input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "\n",
    "            final_grad = mask - target_one_hot\n",
    "            # print(f'target{target}')\n",
    "            # print(f' mask {mask},\\n target_one_hot {target_one_hot},\\n final_grad {final_grad}')\n",
    "            return final_grad, None, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self, output_threshold):\n",
    "            super().__init__()\n",
    "            self.output_threshold = output_threshold\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target, self.output_threshold)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion(output_threshold=output_threshold).to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        # optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        # print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            # print(f'train input {inputs.shape}')\n",
    "\n",
    "            \n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            #######################################################################################################################\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                assert False\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                assert False\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                assert False\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "                            \n",
    "            ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "            if hetero_timesteps == True:\n",
    "                # this_data_timesteps = inputs.shape[1]\n",
    "                # TIME = this_data_timesteps//temporal_filter\n",
    "                TIME = origin_timesteps\n",
    "                net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "            ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "            \n",
    "\n",
    "            \n",
    "            # print(f\"inputs1 {inputs.shape}\")\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(inputs.shape[0]//temporal_filter):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2)\n",
    "                    slice_concat = slice_concat.reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    # print(slice_concat.shape)\n",
    "                    # torch.Size([1, 2, 14, 70])\n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        for ttt in range(temporal_filter):\n",
    "                            if ttt == 0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] = slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] + slice_concat[..., shape_temp[-1] * (ttt-1) : shape_temp[-1] * (ttt)]\n",
    "                        slice_bucket.append(slice_concat)\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                # if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                #     inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            \n",
    "            # print(f\"inputs2 {inputs.shape}\")\n",
    "            # inputs2 torch.Size([285, 1, 2, 14, 70])\n",
    "\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                this_sample_total_tw = inputs.shape[0]\n",
    "                # TIMEÍ∞ú ÎûúÎç§ Ïù∏Îç±Ïä§ ÏÑ†ÌÉù (Ï§ëÎ≥µ ÏóÜÏù¥)\n",
    "                idx = torch.randperm(this_sample_total_tw)[:TIME]\n",
    "\n",
    "                # Í∞Å timestepÎ≥Ñ Ìï© Í≥ÑÏÇ∞\n",
    "                time_sums = inputs.sum(dim=(1, 2, 3, 4))  # shape [285]\n",
    "                # print(f'time_sums {time_sums}, {time_sums.mean()}')\n",
    "                # threshold Ïù¥ÏÉÅÏù∏ timestep Ïù∏Îç±Ïä§Îßå ÏÑ†ÌÉù\n",
    "                valid_idx_temp = (time_sums >= time_sums.mean()).nonzero(as_tuple=True)[0]\n",
    "                assert len(valid_idx_temp) >= TIME, f'valid_idx_temp numel {valid_idx_temp.numel()} is less than TIME {TIME}'\n",
    "                valid_idx = valid_idx_temp[torch.randperm(len(valid_idx_temp))[:TIME]]\n",
    "                # ÏÑ†ÌÉùÌïú Ïù∏Îç±Ïä§ Ï†ïÎ†¨ (ÏàúÏÑúÎåÄÎ°ú Î≥¥Í∏∞ Ï¢ãÍ≤å)\n",
    "                chosen_idx, _ = torch.sort(valid_idx)\n",
    "                # ÏÑ†ÌÉùÌïú Ïù∏Îç±Ïä§Î°ú Ïä¨ÎùºÏù¥Ïã±\n",
    "                inputs = inputs[chosen_idx]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "                    \n",
    "            # print(f\"inputs3 {inputs.shape}\")\n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # print(f'TRAIN time {TIME}')\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                outputs = outputs.sum(dim=0)\n",
    "                # outputsshape : torch.Size([1, 10])\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                # print(f'outputs', outputs, outputs.shape)\n",
    "                # print(f'labels', labels, labels.shape)  \n",
    "                # print(f'iter_loss', iter_loss)  \n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "                        # print(f'tinputs_val {inputs_val.shape}')\n",
    "\n",
    "                        \n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                            \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            assert False\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "                        ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "                        if hetero_timesteps == True:\n",
    "                            # print(inputs_val.shape)\n",
    "                            this_data_timesteps = inputs_val.shape[0]\n",
    "                            TIME = this_data_timesteps//temporal_filter\n",
    "                            net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "                        ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "\n",
    "            \n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    for ttt in range(temporal_filter):\n",
    "                                        if ttt == 0:\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] = slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] + slice_concat[..., shape_temp[-1] * (ttt-1) : shape_temp[-1] * (ttt)]\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            # if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                            #     inputs_val = (inputs_val != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        \n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            # now_T = inputs_val.shape[0]\n",
    "                            # now_time_steps = temporal_filter*TIME\n",
    "                            # start_idx = 0\n",
    "                            # inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "                        ###################################################################################################\n",
    "\n",
    "\n",
    "                        \n",
    "                            \n",
    "                        # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "                        # print(f'TEST time {TIME}, inputs_val.shape {inputs_val.shape}')\n",
    "                        # ##############################################################################################\n",
    "                        # dvs_visualization(inputs_val, labels_val, TIME, BATCH, my_seed)\n",
    "                        # # #####################################################################################################\n",
    "\n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            # print(f'inputs_val, {inputs_val.shape}')\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            outputs = outputs.sum(dim=0)\n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "\n",
    "\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"1\",\n",
    "#                 single_step = False, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.1,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 1.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'one', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 25, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 5_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = True, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[],\n",
    "#                 scale_exp=[[-10,-10],[-10,-10],[-9,-9]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 output_threshold = 0.01,\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 80r73ise with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [512]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 5000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_threshold: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: one\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251110_154600-80r73ise</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/80r73ise' target=\"_blank\">gentle-sweep-265</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/80r73ise' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/80r73ise</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_threshold' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': '20251110_154609_165', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.75, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.75, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [512], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.005, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'one', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'output_threshold': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 7a22c8a0ef5b9b252dbf98632e270efd\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=512, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.0625, v_reset=10000, sg_width=1, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=False, ANPI_MODE=True)\n",
      "      (4): SYNAPSE_FC(in_features=512, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient(single_step=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 506,880\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.005\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "inFeed spike.shape torch.Size([10, 512]) self.weight_fb.shape torch.Size([10, 512])\n",
      "self.weight_fb[0] tensor([ 1.2009e-02,  1.3379e-01, -1.0650e-02,  5.2556e-02, -1.1912e-01,\n",
      "         4.0419e-02, -4.0199e-02, -5.0604e-02,  3.2680e-02, -7.8942e-02,\n",
      "        -1.0288e-01, -1.8775e-02, -5.7299e-03,  1.2332e-02, -6.9353e-02,\n",
      "         1.1499e-01, -4.4228e-02,  4.2593e-02,  4.9323e-02, -2.0675e-03,\n",
      "         9.2336e-02, -3.1971e-02, -1.5728e-02,  9.1276e-02, -2.0181e-02,\n",
      "        -7.1800e-02,  1.4578e-01, -4.2861e-02,  1.1373e-02, -7.3257e-02,\n",
      "        -1.1159e-01, -9.7846e-02,  5.1912e-02,  8.7845e-02,  4.0044e-02,\n",
      "         2.6324e-02, -9.8372e-02,  3.8522e-02,  1.0460e-01, -4.1150e-02,\n",
      "         5.8342e-02,  4.8482e-03,  5.2401e-03, -8.7172e-03,  2.0523e-02,\n",
      "        -3.6457e-02, -6.6373e-02,  5.9048e-03, -2.0717e-02, -3.2546e-02,\n",
      "        -5.4324e-02,  2.4378e-02,  1.0149e-02, -1.2236e-02,  6.2543e-02,\n",
      "        -8.3454e-02, -2.1650e-02, -3.9879e-02,  2.7655e-02, -3.3246e-02,\n",
      "         7.6898e-02, -5.0422e-02,  1.5484e-02, -2.6447e-02,  6.8359e-02,\n",
      "        -6.8262e-02,  3.4312e-02, -7.9518e-02, -2.3619e-02,  3.1812e-02,\n",
      "         6.2016e-03,  1.6009e-02,  2.2387e-02,  1.4105e-01,  1.4450e-03,\n",
      "         9.7970e-02, -7.1751e-02,  5.8704e-02, -2.8309e-02,  4.7077e-02,\n",
      "        -3.5820e-02, -4.3640e-02, -4.4777e-02, -3.1386e-02, -2.7226e-02,\n",
      "        -2.5884e-02,  1.0779e-02,  2.7401e-02,  3.1376e-02, -7.5319e-02,\n",
      "        -1.6829e-02,  1.7118e-02, -8.9122e-02, -4.0006e-02,  4.6343e-03,\n",
      "         1.2001e-02,  3.6892e-02,  1.4373e-02,  7.0655e-02, -4.2197e-02,\n",
      "        -1.0233e-01,  3.7360e-04,  8.5512e-02,  7.8637e-02,  1.4384e-03,\n",
      "        -8.0477e-02, -4.6482e-02,  2.3251e-02, -3.3886e-02, -2.4537e-03,\n",
      "        -4.8149e-02, -1.5486e-01,  4.3330e-02, -5.8045e-03, -1.3386e-02,\n",
      "         2.7755e-02, -1.9510e-02,  1.3393e-03,  3.8708e-02,  1.5263e-02,\n",
      "         4.6335e-02, -7.2374e-03, -6.3238e-03, -3.1016e-02, -3.1252e-02,\n",
      "        -7.4723e-02, -1.5088e-02, -4.1994e-02,  1.2212e-02,  6.0550e-02,\n",
      "        -1.7745e-03,  1.0415e-01,  6.7522e-02, -6.1409e-02, -4.1550e-02,\n",
      "         1.0644e-01,  1.5230e-01, -3.8367e-02,  7.8697e-02, -1.7323e-02,\n",
      "         2.6986e-02,  2.6370e-02,  6.5894e-02, -1.2553e-01, -3.9156e-02,\n",
      "         1.3065e-01, -5.8646e-03,  1.4600e-02, -4.5190e-02, -1.0434e-01,\n",
      "         5.6415e-02,  4.8810e-02, -3.8917e-02,  1.3367e-01,  7.2065e-02,\n",
      "        -2.6348e-02,  1.4814e-02, -7.9086e-02, -7.4679e-03, -3.7547e-02,\n",
      "        -4.9995e-02,  1.3292e-04, -1.2034e-02,  4.6384e-02,  5.0249e-02,\n",
      "         5.1038e-02, -3.7747e-02,  8.0393e-02, -6.6428e-02, -1.4425e-03,\n",
      "        -2.2637e-02, -3.0118e-02,  9.2677e-03, -9.3434e-02,  1.9207e-02,\n",
      "        -2.7770e-02, -6.7883e-02, -7.8605e-02, -9.7644e-02, -9.8327e-02,\n",
      "        -4.0612e-02,  4.7043e-02, -3.7591e-02,  1.8712e-02, -8.3181e-02,\n",
      "        -1.9715e-02,  3.6721e-02,  3.5419e-02, -4.6781e-02, -7.8367e-03,\n",
      "        -2.6748e-02, -8.6308e-02,  2.3989e-02, -1.2710e-02,  3.7118e-02,\n",
      "        -6.2088e-02, -2.2962e-04, -4.9640e-02,  2.4384e-02,  1.5691e-01,\n",
      "         1.5421e-02,  5.5528e-02,  4.8312e-02,  5.6640e-02, -2.2735e-02,\n",
      "         5.3113e-03, -5.2211e-02,  2.6325e-02,  6.9295e-02,  2.4738e-02,\n",
      "        -5.3518e-03,  5.2276e-02, -2.4634e-02, -5.3242e-03,  1.2084e-01,\n",
      "        -2.6133e-02,  3.3964e-02,  9.2582e-03, -1.2223e-01, -2.1360e-03,\n",
      "        -7.8244e-02, -1.5748e-02,  1.4439e-03,  1.2431e-01,  6.0634e-02,\n",
      "         8.5934e-02, -6.0989e-02, -2.9897e-02, -1.1970e-03, -1.0762e-01,\n",
      "         1.0423e-02,  1.6176e-02, -1.3812e-02, -5.2755e-02,  1.6920e-02,\n",
      "         6.1367e-02,  9.1813e-02,  2.1540e-02,  7.7856e-03, -4.0828e-02,\n",
      "        -9.7598e-02, -4.1089e-02,  9.0935e-02,  1.8519e-02, -3.4424e-02,\n",
      "         2.8530e-03, -6.6620e-02, -8.9594e-03, -6.7013e-03, -4.6130e-02,\n",
      "        -2.1535e-02,  5.8145e-03,  4.0000e-03, -5.7107e-02,  4.8855e-02,\n",
      "        -1.1148e-01, -1.1978e-01,  6.8131e-02,  1.5512e-03,  3.5912e-02,\n",
      "         3.3328e-02,  3.1726e-02, -8.8611e-02,  1.4725e-01, -9.5569e-02,\n",
      "        -1.0785e-02, -1.3891e-03,  1.3467e-02,  4.0348e-02,  9.6515e-02,\n",
      "         1.6649e-02,  3.0992e-02, -1.5092e-02, -5.3478e-02,  2.6478e-02,\n",
      "        -1.3042e-02, -9.5301e-02, -6.6575e-03, -1.5733e-03, -9.9895e-03,\n",
      "         3.4082e-02,  1.5740e-01, -9.9586e-03, -5.3744e-02,  8.7394e-02,\n",
      "         4.2685e-02,  5.2481e-02,  1.7623e-02,  1.0548e-03,  4.5100e-02,\n",
      "         7.4265e-02, -7.1658e-03, -8.7438e-02, -3.9754e-02,  5.4727e-02,\n",
      "         4.6412e-02,  4.2058e-02, -3.2855e-02, -1.1088e-01, -1.7722e-02,\n",
      "         4.9851e-03, -8.0476e-02,  8.2968e-02, -8.2024e-02,  1.6164e-02,\n",
      "         3.7377e-02, -9.2349e-02, -1.1127e-01,  6.9750e-02,  8.6820e-02,\n",
      "        -2.7057e-02, -2.3069e-02, -7.3103e-02, -1.6484e-01, -2.0014e-02,\n",
      "         6.3153e-03,  7.7782e-02, -8.4823e-02,  2.2121e-02,  1.0625e-01,\n",
      "        -1.4292e-01,  8.1527e-02, -7.1087e-02, -8.0429e-02, -4.0732e-03,\n",
      "         6.4006e-02, -1.4278e-01, -7.9276e-03,  5.2838e-02, -3.7510e-03,\n",
      "        -5.9070e-02, -1.1084e-01, -1.6297e-03,  5.6736e-03, -7.3166e-02,\n",
      "        -6.8036e-02,  1.5117e-01,  1.9150e-02, -9.3975e-02, -4.8127e-02,\n",
      "         4.4899e-02,  5.5049e-02,  6.3477e-02,  5.0466e-02,  1.4346e-01,\n",
      "        -1.4061e-02,  1.8790e-01,  3.4009e-02,  1.4160e-03, -2.5282e-02,\n",
      "        -1.6245e-02,  5.4068e-02, -7.5012e-02, -7.5148e-02, -1.8582e-02,\n",
      "        -2.3466e-02,  1.9578e-02, -6.2413e-02,  1.2314e-01,  1.3701e-02,\n",
      "        -5.7122e-03,  8.9041e-02,  3.7946e-02,  4.1243e-02,  4.7171e-02,\n",
      "         2.7039e-02, -5.9925e-03, -2.8245e-02, -7.2878e-02,  1.4521e-02,\n",
      "         9.9702e-02,  6.4296e-02,  7.4185e-02, -7.1993e-02,  1.4546e-02,\n",
      "         7.7495e-02, -9.2409e-03, -3.8808e-02,  7.1566e-02, -1.4977e-01,\n",
      "         4.2293e-02, -4.2540e-02, -5.6876e-03, -4.4148e-02, -8.0183e-02,\n",
      "         7.5278e-02, -2.9656e-03, -4.9337e-02,  2.6277e-02, -1.1994e-02,\n",
      "        -9.6900e-03, -8.8157e-03, -1.7625e-02, -8.9690e-02, -3.2884e-02,\n",
      "        -5.1021e-03, -1.0199e-01, -1.6831e-02,  1.1726e-01, -3.4447e-02,\n",
      "        -2.8511e-02, -1.9198e-02,  3.6576e-03,  3.2099e-02,  4.5579e-03,\n",
      "         8.7041e-02, -3.0138e-02,  1.8212e-02,  7.4119e-02, -1.3839e-02,\n",
      "         5.3415e-02,  2.2786e-02,  1.0557e-01, -5.6927e-02,  3.3285e-02,\n",
      "         7.3276e-02,  1.0244e-01, -1.4565e-02, -1.0259e-01,  1.2200e-01,\n",
      "         6.1812e-02,  4.8889e-02, -5.6486e-02,  5.1047e-02,  9.3909e-02,\n",
      "        -1.0201e-02,  6.4712e-02, -2.3649e-02,  3.8729e-02,  6.1245e-03,\n",
      "        -4.3430e-02,  6.4039e-03, -8.9212e-02,  1.5119e-01,  7.2071e-02,\n",
      "         1.5732e-02, -2.2774e-02,  5.2327e-02,  2.5401e-02,  2.9843e-02,\n",
      "        -1.1558e-01,  5.9937e-02, -5.8328e-02,  7.1370e-02,  4.9816e-02,\n",
      "         6.5657e-02,  3.2430e-02, -8.6861e-03,  8.5977e-02,  1.9082e-02,\n",
      "         2.7206e-02, -1.9106e-03, -6.5907e-02,  4.0442e-03,  1.7387e-02,\n",
      "         1.3066e-01, -8.5428e-02, -2.6442e-02,  5.6974e-02, -8.7909e-02,\n",
      "         3.4048e-02, -5.8666e-02,  1.8037e-02, -6.2223e-02, -1.8848e-02,\n",
      "         9.5296e-03, -5.1592e-03,  5.1242e-03,  9.5190e-02,  1.1389e-02,\n",
      "        -6.1644e-02,  2.7198e-02,  2.2262e-02, -4.7755e-02,  6.3539e-03,\n",
      "        -2.4203e-02,  1.3476e-02,  5.5816e-02,  3.3884e-02,  5.4144e-02,\n",
      "        -2.0123e-02, -2.5729e-02,  3.2092e-02, -3.4289e-02, -1.2439e-03,\n",
      "         1.8775e-01,  5.8437e-02,  1.8716e-02, -5.8857e-02, -6.8036e-02,\n",
      "        -5.9856e-04,  1.0747e-01, -7.1370e-02,  1.3296e-03, -3.0167e-02,\n",
      "        -5.6810e-02, -1.0447e-01, -8.7226e-03, -3.1270e-03,  1.2601e-02,\n",
      "         1.8155e-02, -9.4597e-02, -4.7340e-02,  2.7440e-02, -3.4883e-02,\n",
      "        -3.2968e-02, -6.2905e-02, -1.2657e-02,  3.2411e-02,  1.2026e-02,\n",
      "         2.2878e-02, -5.3231e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 6.6658e-02, -7.8302e-02, -3.9761e-02, -4.1793e-02,  4.5831e-02,\n",
      "         4.8306e-02, -6.7736e-03,  7.5574e-02, -7.4495e-02, -3.0042e-02,\n",
      "         5.2244e-03, -1.3071e-02, -5.5794e-03, -8.3971e-02, -6.9471e-03,\n",
      "        -2.4258e-02,  1.0854e-01, -6.1369e-02, -1.4674e-01,  1.1226e-01,\n",
      "        -6.0065e-02,  5.3451e-02,  1.1262e-01, -4.9005e-03,  1.5264e-01,\n",
      "         7.8240e-02,  3.1867e-02,  7.0535e-03, -8.8613e-02, -1.6180e-02,\n",
      "         7.1920e-03,  3.6067e-02, -1.8580e-02, -6.9305e-02,  5.7444e-02,\n",
      "        -9.3223e-02,  6.4325e-02, -1.2735e-01, -1.6280e-02, -5.1730e-02,\n",
      "        -1.6762e-02,  1.6986e-01,  2.8526e-02,  7.5887e-02,  4.1897e-03,\n",
      "         5.6685e-02,  4.6633e-02, -3.6862e-02, -3.9126e-02, -2.2331e-02,\n",
      "         9.3762e-02, -1.0613e-02,  1.1766e-01, -3.7826e-02,  6.4190e-02,\n",
      "         2.1247e-02, -9.1414e-03,  9.0567e-02, -1.1170e-01,  1.5015e-02,\n",
      "        -1.6912e-02,  1.8269e-02, -6.4949e-02, -5.4902e-02, -8.6944e-03,\n",
      "         1.3896e-01,  1.1010e-01,  1.0749e-02,  8.7195e-02, -6.8369e-03,\n",
      "        -3.5939e-02,  1.3870e-02,  5.9698e-02, -8.9737e-05,  8.3753e-02,\n",
      "        -4.8358e-03, -3.8847e-02, -1.0107e-01,  7.5683e-02, -1.1180e-01,\n",
      "         3.0140e-02, -4.3089e-02, -2.2418e-02, -3.6128e-02, -1.0527e-01,\n",
      "         2.2898e-02,  4.6009e-02, -7.4225e-03, -5.6874e-02,  8.5350e-02,\n",
      "         5.1923e-03,  2.5627e-02, -8.9285e-03, -5.8058e-02,  7.0525e-02,\n",
      "         3.8854e-02,  2.7697e-02,  1.4393e-01, -4.0282e-02,  2.0928e-02,\n",
      "        -2.4592e-02,  6.1504e-02,  8.4973e-02, -6.5030e-03, -1.1406e-02,\n",
      "        -1.5721e-01, -1.2213e-01, -3.2998e-02, -1.0606e-02,  1.5931e-01,\n",
      "         1.4261e-01,  2.5770e-02, -4.0473e-02, -6.6654e-02,  3.4934e-02,\n",
      "         9.9253e-02, -1.0173e-02, -1.4505e-02,  6.1864e-02,  4.7759e-02,\n",
      "        -1.6578e-02,  3.0713e-02,  1.4806e-02,  8.6155e-02, -1.2338e-02,\n",
      "         7.9021e-02, -7.8331e-02, -6.0098e-02,  7.8730e-02,  2.3303e-02,\n",
      "        -8.3858e-03,  4.4462e-02, -5.4935e-02,  4.2922e-02,  4.7366e-02,\n",
      "        -3.2290e-04,  1.8469e-02, -5.9237e-02,  6.0935e-02,  2.3421e-02,\n",
      "         7.0576e-02, -1.8194e-02,  5.7329e-03,  1.2694e-01, -1.6639e-02,\n",
      "         5.9829e-02, -7.5157e-02, -6.8489e-02, -1.1888e-01, -1.4575e-01,\n",
      "        -6.2740e-03,  8.6623e-02, -1.9370e-03, -1.2883e-01,  4.0742e-02,\n",
      "        -3.1368e-02, -6.8863e-03,  6.7565e-03, -5.5464e-02, -5.8365e-02,\n",
      "        -4.6925e-02, -1.8427e-03, -6.9821e-03, -5.4991e-02,  1.4936e-02,\n",
      "        -6.0094e-02,  2.1199e-02,  1.6101e-03, -6.6419e-02, -1.0129e-01,\n",
      "         3.2519e-04, -9.6969e-02,  2.2424e-02,  8.3956e-02, -1.0915e-01,\n",
      "        -5.2411e-02,  7.9012e-02,  7.7652e-02,  7.2692e-02,  5.3036e-02,\n",
      "         8.0605e-03,  1.2090e-01,  4.4321e-02, -1.3145e-02,  2.7608e-02,\n",
      "        -2.4626e-03, -8.6162e-02, -2.0906e-02, -8.0314e-02,  8.6478e-02,\n",
      "         3.2060e-02, -7.4949e-02, -4.5875e-02, -9.1144e-02,  8.5149e-02,\n",
      "         4.7841e-02, -5.8479e-02,  9.3823e-02, -8.9949e-02, -2.2137e-03,\n",
      "         5.3320e-02,  2.4241e-02,  7.6287e-02, -7.3501e-02,  5.9457e-02,\n",
      "         2.5991e-02, -4.9862e-02,  2.1058e-02,  3.7085e-02,  5.8227e-02,\n",
      "         1.6736e-02,  1.3518e-02, -3.6454e-02,  8.9511e-02, -6.0161e-02,\n",
      "         4.3647e-02,  2.5404e-02,  1.6810e-03, -3.8325e-02,  5.1655e-02,\n",
      "        -6.2435e-03, -7.4342e-02,  1.5280e-02, -3.8896e-02, -4.6945e-02,\n",
      "        -4.9156e-02,  5.0480e-02, -1.1144e-01,  4.6365e-02,  4.1312e-02,\n",
      "         4.3370e-02, -6.4439e-02,  1.4321e-01,  5.6491e-03,  4.6217e-02,\n",
      "        -7.8084e-02,  2.2043e-02,  2.4072e-02, -1.1090e-01, -5.7180e-02,\n",
      "         1.3553e-01,  2.0576e-03, -6.7463e-02, -3.7952e-02,  9.7044e-02,\n",
      "         3.9006e-02,  2.3112e-02,  3.6162e-02, -4.4879e-02, -5.0205e-02,\n",
      "        -6.6276e-02,  6.0393e-02, -1.6587e-02, -4.2223e-02,  4.9360e-02,\n",
      "        -5.2514e-02,  5.3070e-02,  3.0898e-02,  8.4096e-03,  4.2029e-02,\n",
      "         8.3128e-03,  7.7944e-02,  7.4944e-02,  3.7365e-02, -1.7412e-02,\n",
      "        -1.7034e-02, -5.1705e-02, -1.0178e-01,  8.1377e-03, -1.1124e-02,\n",
      "         6.0315e-02, -1.2464e-01, -8.2909e-02, -2.0721e-02,  1.5134e-01,\n",
      "        -7.6029e-03, -5.5703e-02,  1.3161e-01,  1.1009e-01,  8.7843e-02,\n",
      "        -1.1565e-02, -7.0188e-02, -1.7204e-01,  9.7961e-02,  1.4806e-01,\n",
      "        -4.5438e-02, -2.6664e-03, -4.6997e-02, -7.0638e-02, -7.9939e-02,\n",
      "        -7.0988e-02, -1.1400e-01, -7.8130e-03, -8.5862e-02, -3.9800e-02,\n",
      "         7.1482e-03, -1.3455e-01, -2.8474e-02, -8.3467e-02,  6.1789e-02,\n",
      "        -1.2440e-02, -1.4384e-01, -5.4934e-02,  1.7171e-02, -4.3710e-02,\n",
      "         5.2462e-03, -9.8457e-02,  6.4931e-02,  3.0336e-02, -8.2045e-03,\n",
      "        -2.1457e-02,  1.9863e-02, -3.9212e-02,  3.6250e-02, -2.9250e-02,\n",
      "         4.0146e-03,  9.8803e-02, -3.5044e-03, -1.3867e-01,  6.7823e-02,\n",
      "        -1.1386e-02,  4.5815e-02, -4.6995e-02, -6.0331e-02,  8.9048e-02,\n",
      "        -3.3910e-03,  5.5142e-02,  1.0962e-01,  7.8482e-02, -5.7451e-02,\n",
      "         6.7650e-02, -5.0193e-02, -1.0531e-01,  3.0873e-02,  4.0250e-02,\n",
      "         3.5226e-02,  3.5651e-02, -1.3163e-02, -1.5697e-02, -1.3301e-02,\n",
      "        -7.5622e-02,  4.6634e-02, -6.0863e-02,  1.1601e-02,  5.8555e-02,\n",
      "         1.9718e-02,  1.4490e-02,  4.6890e-02,  1.9770e-02,  1.8599e-02,\n",
      "         1.5324e-02,  9.0858e-02, -9.4841e-02,  4.4712e-02,  1.0196e-01,\n",
      "         7.1711e-02,  2.8857e-02, -7.6147e-02,  1.1056e-01,  3.8540e-02,\n",
      "        -7.5464e-02, -1.1109e-01,  1.1038e-02,  7.1191e-02,  3.8999e-02,\n",
      "         8.1577e-02,  1.4265e-01, -2.5305e-02,  7.0406e-02, -2.0950e-01,\n",
      "        -1.0905e-01, -7.9404e-02,  9.4908e-02, -6.2777e-02, -4.6448e-02,\n",
      "         6.7760e-02, -4.1111e-02, -3.0499e-02, -6.7737e-02, -1.6252e-02,\n",
      "         7.7219e-02, -9.5822e-02,  7.5935e-03, -2.3492e-02, -3.9966e-02,\n",
      "         2.2348e-02, -5.5910e-02, -2.2430e-02, -1.2789e-01,  1.1506e-02,\n",
      "        -3.6499e-02, -2.3789e-02,  8.8967e-02,  3.7748e-04,  1.4302e-01,\n",
      "        -3.3631e-02, -3.5510e-02, -1.5043e-01,  7.7718e-02,  1.4879e-01,\n",
      "         6.6394e-02, -1.8917e-02,  1.0423e-02, -4.4962e-03, -2.3098e-02,\n",
      "         8.4583e-02,  1.2187e-01,  2.5955e-02,  2.3483e-02, -1.2860e-01,\n",
      "         2.7167e-02,  3.6408e-02,  8.3306e-02,  1.1587e-01,  6.6651e-02,\n",
      "         5.9024e-02,  1.0206e-01, -6.6102e-02, -1.1416e-02,  6.7382e-02,\n",
      "        -1.8530e-01,  7.1940e-02, -3.7391e-02, -1.0281e-01,  5.0257e-02,\n",
      "         4.7398e-02,  2.7898e-02,  6.5546e-02, -3.5585e-02, -1.5329e-02,\n",
      "        -3.8707e-02, -5.4844e-02, -2.3227e-02,  3.0108e-02, -2.5781e-02,\n",
      "        -2.8408e-02,  3.9738e-03,  9.0303e-02,  8.2566e-03,  2.2979e-02,\n",
      "        -5.5796e-02, -3.8515e-02, -6.0057e-02,  7.1408e-02, -6.8506e-02,\n",
      "        -8.3587e-02, -1.1510e-01,  3.3540e-02, -1.6315e-02, -4.7617e-02,\n",
      "        -1.2741e-01, -2.6345e-02, -6.0932e-02, -2.5297e-02,  1.7280e-03,\n",
      "        -5.4365e-02, -5.7350e-02, -4.4366e-02, -1.8187e-02, -5.9762e-02,\n",
      "         1.8093e-02, -6.1407e-02,  1.3368e-01,  3.7309e-02, -2.3302e-02,\n",
      "        -3.6866e-02,  6.9024e-03,  7.7365e-03,  4.0508e-02, -2.5169e-02,\n",
      "        -8.2504e-02,  1.2014e-01, -6.4195e-02,  6.6726e-02,  1.5957e-02,\n",
      "         1.0247e-01,  9.6323e-02,  5.0310e-02, -7.1386e-02, -6.2054e-03,\n",
      "        -1.6760e-01,  3.7466e-03, -9.4249e-02,  7.7653e-02, -1.2555e-01,\n",
      "        -6.1608e-02, -2.9333e-02,  1.3478e-02, -1.4650e-02, -9.3798e-02,\n",
      "         6.4758e-02,  2.1284e-02,  1.5329e-01, -8.6474e-02, -5.4156e-03,\n",
      "        -2.4129e-02,  1.0983e-01, -2.6136e-02,  1.7877e-02,  7.2377e-02,\n",
      "         2.4865e-02,  5.1694e-02,  5.9210e-02,  1.3274e-01, -4.0805e-02,\n",
      "         2.4143e-02,  6.7355e-02,  6.0903e-02,  6.5552e-02,  1.7681e-01,\n",
      "         4.1771e-02,  1.2728e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 5.0966e-02, -1.4745e-01,  7.7494e-02,  1.4598e-02,  1.1066e-01,\n",
      "        -3.6061e-02, -3.4456e-02,  2.3449e-03,  3.6120e-02, -2.1529e-02,\n",
      "         1.0209e-01,  1.2287e-03, -5.0131e-02,  6.2569e-02, -2.0442e-02,\n",
      "         3.2035e-02,  6.1605e-02, -9.9639e-02,  1.5433e-02,  3.8132e-02,\n",
      "        -6.6866e-02, -6.3091e-02, -6.1747e-02,  6.8062e-02,  8.8035e-02,\n",
      "        -1.0674e-01,  5.1352e-02, -3.5963e-02, -4.7417e-03, -4.0600e-03,\n",
      "        -1.0709e-01, -8.8151e-02,  1.0923e-01, -5.1789e-02, -1.1943e-01,\n",
      "        -3.2427e-02,  8.7168e-02,  1.1600e-01, -3.1433e-02,  2.1007e-02,\n",
      "        -2.0211e-02,  5.1138e-02, -3.1195e-02, -1.7929e-02,  1.6682e-02,\n",
      "        -5.8549e-03, -3.0055e-02, -1.2022e-01,  4.2940e-02,  5.0219e-03,\n",
      "        -7.6352e-02,  1.2055e-02,  1.1379e-02,  7.7296e-02, -3.7195e-02,\n",
      "         6.2380e-02, -9.9886e-02,  1.3775e-02, -3.7782e-02, -8.0343e-03,\n",
      "         1.1148e-02, -1.7144e-02, -8.2952e-02,  6.2111e-02,  1.4023e-02,\n",
      "         9.3064e-02, -1.8222e-02,  8.8978e-02, -9.5613e-02,  5.1005e-02,\n",
      "         6.4407e-02, -1.5327e-02, -1.6592e-02, -4.5361e-02, -3.1602e-02,\n",
      "        -4.6708e-02, -4.0381e-02,  9.3572e-02,  1.4583e-02,  1.5900e-02,\n",
      "         5.2908e-02, -6.2023e-02,  9.5726e-02, -2.2317e-02, -1.0207e-02,\n",
      "        -8.4064e-02, -8.5376e-02,  1.4583e-02,  6.5636e-02,  8.2487e-02,\n",
      "         6.9251e-02, -3.3851e-03,  2.0579e-02, -6.4329e-03, -6.3405e-03,\n",
      "         2.8375e-02, -5.4557e-02,  4.9721e-02, -2.8327e-02,  7.1326e-02,\n",
      "        -2.7338e-02,  7.1745e-02,  2.0902e-02, -1.4693e-02, -6.4021e-03,\n",
      "        -3.6755e-02,  2.3320e-02, -1.8848e-02, -8.2152e-03, -7.3774e-02,\n",
      "        -6.4569e-02, -3.3738e-02,  2.3054e-02, -1.0855e-02,  3.3617e-02,\n",
      "         5.3611e-02, -6.7952e-02, -5.8561e-02, -4.5781e-02,  2.4040e-02,\n",
      "        -8.8937e-02,  3.5465e-02,  5.0535e-02,  2.5044e-02, -4.3513e-03,\n",
      "        -3.2971e-02, -1.3832e-01, -8.0301e-02,  1.5525e-01, -8.0106e-02,\n",
      "         2.0949e-02,  1.1226e-02,  5.7637e-02,  9.5634e-02, -4.6271e-02,\n",
      "         6.2753e-02, -4.8439e-02,  5.5866e-02, -5.6149e-02,  8.9882e-03,\n",
      "        -2.2475e-02,  2.6102e-03, -7.5365e-02, -3.5781e-02,  8.7820e-03,\n",
      "        -2.7019e-02,  5.6331e-02,  1.6614e-03, -3.3956e-02, -6.9785e-02,\n",
      "         1.1633e-01,  5.9738e-02, -8.4658e-02,  3.5563e-02,  1.0341e-01,\n",
      "         7.0607e-05, -4.0593e-02,  3.8467e-02,  1.0799e-01,  1.7658e-02,\n",
      "        -9.0117e-02, -9.2431e-02, -7.4624e-02,  3.1521e-02,  4.0765e-02,\n",
      "        -1.2515e-01,  3.0535e-02,  1.1851e-02, -4.0310e-02,  2.2916e-02,\n",
      "         1.2250e-01,  6.9152e-02, -6.2053e-03,  4.0321e-02,  1.6208e-02,\n",
      "        -6.8822e-02,  2.1849e-02, -3.6987e-02, -4.4603e-02, -1.5947e-01,\n",
      "        -1.6658e-02, -9.6214e-02, -3.7753e-02,  5.4041e-02, -1.7003e-02,\n",
      "         8.1025e-02,  2.4926e-02,  5.5767e-02, -7.9529e-02, -2.1234e-01,\n",
      "        -4.7282e-02, -5.5761e-02,  3.0091e-02,  1.4731e-01, -6.2581e-02,\n",
      "         2.2454e-02, -6.7485e-02,  1.5281e-01,  4.6557e-02,  8.2848e-02,\n",
      "        -9.2783e-03,  7.2040e-02, -9.9636e-02,  6.1564e-02, -5.9368e-02,\n",
      "        -1.9590e-02, -1.0435e-02, -4.1890e-02, -4.7181e-02, -1.2446e-02,\n",
      "        -4.0818e-02,  6.1132e-02, -8.5487e-03,  8.7448e-02,  2.1625e-02,\n",
      "        -1.7572e-02, -9.9109e-02,  3.0057e-02,  7.2901e-02, -1.2618e-02,\n",
      "         3.7349e-02, -2.1917e-02, -6.9758e-02, -1.2695e-03, -1.3122e-02,\n",
      "        -5.0221e-02,  2.3869e-02,  5.0954e-02,  7.0282e-04, -3.3970e-02,\n",
      "        -2.8963e-02, -8.4868e-02, -2.6569e-02, -6.5083e-02,  8.5820e-03,\n",
      "        -4.4336e-03,  5.8201e-03,  2.1587e-02,  7.3191e-03,  4.7043e-03,\n",
      "        -5.8309e-02,  2.1552e-02, -2.5648e-02, -2.2331e-02, -1.0112e-01,\n",
      "        -3.7041e-02, -4.1032e-02, -6.8042e-02,  1.7894e-02, -2.6997e-02,\n",
      "        -2.7584e-02,  1.7612e-02, -1.9444e-03,  5.9923e-02,  6.8182e-02,\n",
      "         2.6522e-02, -6.7600e-02,  3.6002e-02, -1.6933e-02,  9.7652e-03,\n",
      "        -1.0266e-01, -3.6495e-03,  1.1981e-01, -3.1746e-02, -2.1659e-02,\n",
      "        -4.1714e-02,  7.0952e-02, -8.4005e-02,  3.2536e-03, -2.2566e-02,\n",
      "        -3.9273e-02,  3.3117e-03, -8.4515e-02,  5.7761e-02,  9.1372e-02,\n",
      "         9.6171e-03, -1.2380e-01, -8.3872e-04, -1.1604e-02, -2.1467e-02,\n",
      "         3.9992e-02,  8.3243e-04, -5.9930e-03, -2.2868e-02,  2.3452e-02,\n",
      "         1.2934e-02,  1.4610e-01,  6.3666e-04, -4.7834e-02, -1.6290e-02,\n",
      "         6.7797e-02,  3.1905e-02, -6.1453e-02,  4.7708e-02,  4.9836e-02,\n",
      "        -3.2332e-02,  1.4693e-02, -8.0379e-02,  5.6533e-02,  6.9687e-02,\n",
      "         6.2967e-02, -3.5479e-02, -9.2222e-03, -6.3729e-03,  8.0024e-02,\n",
      "         1.0684e-02,  5.5488e-02, -5.7777e-03,  1.2793e-01,  2.4388e-02,\n",
      "         6.8428e-02, -2.1748e-03, -4.4633e-02,  1.3514e-02,  2.4887e-03,\n",
      "        -1.9060e-02, -1.2467e-01, -4.7357e-02, -4.9894e-02,  9.8269e-02,\n",
      "        -6.8453e-03,  3.6830e-02, -3.3399e-02, -4.3410e-02, -9.6036e-02,\n",
      "         8.1545e-02, -3.5613e-02,  6.0910e-02, -5.0575e-02,  6.5858e-03,\n",
      "         5.8657e-02,  2.9649e-02, -5.0301e-02, -1.8220e-02, -7.9198e-02,\n",
      "         4.7839e-02,  3.2613e-02, -9.3417e-02,  6.7337e-02, -8.7942e-03,\n",
      "        -1.6459e-02,  2.7349e-02, -4.9454e-02,  6.1516e-02,  6.7670e-02,\n",
      "         4.5408e-03,  3.2664e-02,  3.3849e-02, -8.3817e-03,  2.9799e-02,\n",
      "        -6.4481e-02,  6.9932e-02,  1.3802e-02, -7.4295e-02,  2.8266e-03,\n",
      "         1.3482e-01,  1.6569e-02, -4.2818e-02,  5.2147e-02,  4.8331e-02,\n",
      "        -2.2739e-02, -1.8746e-02,  2.8624e-02, -8.2209e-02, -4.9650e-02,\n",
      "        -2.9904e-02, -3.1530e-02, -4.7788e-02, -4.7805e-02,  4.2077e-02,\n",
      "        -5.1374e-03,  9.3389e-02,  7.7671e-02, -1.0206e-02, -5.3528e-02,\n",
      "        -6.0535e-03,  2.0553e-02,  2.7381e-02,  8.1292e-03, -6.6471e-02,\n",
      "        -1.9595e-02,  2.1768e-02,  4.5958e-02,  5.7396e-02,  1.7548e-02,\n",
      "        -6.3863e-03, -1.7971e-01,  2.8201e-02,  1.6888e-02, -6.0088e-02,\n",
      "        -4.4732e-02,  5.1204e-04,  5.4047e-02,  1.5042e-02,  8.6862e-02,\n",
      "        -5.6149e-02, -8.0252e-02, -1.7712e-02, -3.3251e-02,  6.7082e-02,\n",
      "         5.7277e-02,  7.4467e-02,  1.3210e-02,  8.0749e-02, -4.9230e-02,\n",
      "         4.0126e-02,  6.4328e-02,  3.2686e-02,  5.5669e-02, -4.5429e-02,\n",
      "        -6.0456e-02,  5.9471e-03, -7.2037e-03, -6.6578e-02,  6.4264e-02,\n",
      "        -3.4567e-02,  1.8057e-01,  9.6095e-02,  1.7282e-02, -5.5573e-03,\n",
      "        -1.5813e-02,  7.3891e-02, -9.6589e-03, -5.6928e-02,  3.5197e-02,\n",
      "        -3.6848e-02,  3.3619e-02, -7.9201e-02, -1.0853e-03, -6.1366e-02,\n",
      "        -4.6373e-02, -2.3210e-02,  2.4530e-02, -2.9117e-02, -2.6862e-02,\n",
      "         2.0443e-02, -1.0311e-02, -4.5818e-02,  3.2928e-02, -1.4177e-01,\n",
      "        -3.3394e-02, -8.0657e-02, -1.1610e-01,  2.7471e-03, -1.1582e-02,\n",
      "         1.8751e-03, -3.5150e-02,  9.0628e-02, -1.1234e-02, -6.3072e-03,\n",
      "        -2.9522e-03, -2.5991e-02,  7.4267e-02,  5.3881e-02, -4.0242e-03,\n",
      "         7.6560e-03,  8.1244e-02, -1.5535e-02, -7.0901e-02,  4.0996e-03,\n",
      "        -1.9212e-02,  1.5392e-02, -4.2169e-02,  1.7310e-02, -7.4863e-02,\n",
      "        -5.8399e-02, -4.7026e-02,  1.1410e-01, -1.0140e-01, -9.5707e-02,\n",
      "         2.0097e-02, -1.0625e-01,  6.2864e-02, -1.0046e-01,  4.0808e-02,\n",
      "        -5.9520e-02, -5.2804e-02,  1.8317e-02, -1.1327e-01, -1.7123e-02,\n",
      "        -2.9642e-03, -1.2108e-02,  4.3250e-02, -6.8001e-02,  2.8993e-02,\n",
      "         2.3379e-03,  6.4308e-03, -5.0257e-02, -2.6099e-02, -9.2139e-03,\n",
      "         1.4326e-01, -3.5042e-02, -5.5747e-03,  1.4443e-01,  6.4646e-02,\n",
      "        -3.6846e-02, -3.1642e-02,  1.8773e-04, -6.0860e-02,  7.3784e-02,\n",
      "         3.4365e-02, -5.6993e-02,  4.9817e-02, -4.8040e-02,  7.2079e-02,\n",
      "         6.0582e-02,  1.5344e-03, -6.8195e-02,  2.4479e-02, -6.7752e-02,\n",
      "        -7.2611e-02, -2.7682e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0768, -0.0110,  0.0261, -0.0717,  0.0138, -0.0634, -0.0912,  0.0113,\n",
      "        -0.0347, -0.0304, -0.0077, -0.0341, -0.0804, -0.0470, -0.0264,  0.0091,\n",
      "         0.0322,  0.0482, -0.0405, -0.0913,  0.0352, -0.0308,  0.0159,  0.0034,\n",
      "         0.0155, -0.0147,  0.0697,  0.0984,  0.0066,  0.0651, -0.1385, -0.0525,\n",
      "        -0.0866,  0.0596, -0.0648,  0.0693,  0.0717,  0.0327, -0.0749,  0.1113,\n",
      "         0.0407,  0.0465,  0.1108,  0.0816, -0.0240,  0.0117,  0.0365, -0.0328,\n",
      "         0.0209, -0.0589,  0.0395, -0.0040,  0.0484,  0.0579,  0.0430,  0.0961,\n",
      "         0.0019, -0.0478, -0.0156,  0.0328, -0.0624,  0.0715,  0.0612, -0.0883,\n",
      "         0.0393, -0.0688, -0.0231, -0.0230, -0.0219,  0.0156, -0.0243, -0.1010,\n",
      "        -0.0313,  0.0016, -0.0020, -0.0170, -0.0236, -0.0161, -0.0517, -0.0867,\n",
      "        -0.0712, -0.0125, -0.0954, -0.0109,  0.1592,  0.0375, -0.0574,  0.0412,\n",
      "        -0.0757,  0.1175,  0.0951, -0.0161, -0.0222, -0.1225,  0.0901,  0.0392,\n",
      "        -0.0461, -0.0242,  0.0155, -0.0975, -0.0425, -0.0112,  0.0040,  0.0077,\n",
      "         0.0669, -0.0678, -0.0185, -0.0830, -0.0124,  0.0362, -0.0285,  0.1085,\n",
      "        -0.0133,  0.0715, -0.0329, -0.0025,  0.0326, -0.0271,  0.0487, -0.0552,\n",
      "        -0.0141,  0.0521, -0.0023, -0.0375, -0.1438,  0.0137,  0.0634, -0.0483,\n",
      "        -0.0128,  0.0103,  0.0111,  0.0511,  0.1563,  0.0164,  0.0060, -0.1368,\n",
      "        -0.1142, -0.0285, -0.0205,  0.0208,  0.0782,  0.0446,  0.0960, -0.0340,\n",
      "        -0.0171,  0.0837,  0.1210,  0.0210, -0.0156, -0.0047,  0.0567,  0.1111,\n",
      "        -0.0234, -0.0498, -0.0705, -0.0082,  0.1107,  0.0074,  0.0705, -0.0538,\n",
      "         0.0613, -0.1379,  0.0155, -0.0276,  0.0236, -0.0070, -0.0942, -0.0741,\n",
      "         0.0344,  0.0320, -0.0537, -0.1111, -0.0324,  0.1613,  0.0198,  0.1086,\n",
      "        -0.0317,  0.0004, -0.0473,  0.0628,  0.0596, -0.0103, -0.0568,  0.0624,\n",
      "        -0.0776, -0.1148, -0.0166,  0.0027,  0.0078, -0.0937, -0.0514, -0.0138,\n",
      "        -0.1482, -0.0669, -0.0712,  0.0135,  0.1173, -0.0033, -0.0064, -0.0263,\n",
      "        -0.0567,  0.0106,  0.0777, -0.0619, -0.0526,  0.0932, -0.0841, -0.0340,\n",
      "        -0.1270,  0.0130,  0.0067, -0.0860,  0.1337, -0.0305, -0.0314, -0.0653,\n",
      "         0.1493, -0.0126, -0.0196, -0.0949, -0.0565,  0.0440, -0.0889,  0.0118,\n",
      "        -0.0558, -0.0214, -0.0157, -0.0387, -0.0158,  0.0084, -0.0396, -0.0521,\n",
      "        -0.0809,  0.0183,  0.0045,  0.0053, -0.0093, -0.0678, -0.1156,  0.0174,\n",
      "         0.1187,  0.0416,  0.0693, -0.0025,  0.0486,  0.0294, -0.0075, -0.0575,\n",
      "         0.1809,  0.0164,  0.0446, -0.0271, -0.0230,  0.0786, -0.0114, -0.0058,\n",
      "         0.0358, -0.0731, -0.0365, -0.0286,  0.1120, -0.0882,  0.0127,  0.0710,\n",
      "         0.0003,  0.0062, -0.0400,  0.0463,  0.0816,  0.0720,  0.0084,  0.0478,\n",
      "         0.0634,  0.0475,  0.0025, -0.0680, -0.0101,  0.0497,  0.0274,  0.0548,\n",
      "         0.0372, -0.0325,  0.1441,  0.0648,  0.0218,  0.0187,  0.0017,  0.0058,\n",
      "         0.0606,  0.0349, -0.0842, -0.0129,  0.1517, -0.0832, -0.0344,  0.0722,\n",
      "         0.0201, -0.0085,  0.0686, -0.0399, -0.1319,  0.0208, -0.0094, -0.0035,\n",
      "         0.0502,  0.0415,  0.0268,  0.0031, -0.0782, -0.0470,  0.0647, -0.0245,\n",
      "        -0.0220,  0.0053, -0.0115,  0.0109,  0.0431,  0.0079, -0.0562, -0.0070,\n",
      "         0.0463, -0.0588,  0.0339,  0.0052, -0.0210,  0.1090,  0.0647, -0.0540,\n",
      "         0.0085,  0.0879, -0.0313,  0.0073,  0.0437,  0.0494,  0.0060,  0.1026,\n",
      "         0.0076,  0.0393, -0.0335, -0.0069, -0.1043,  0.0803, -0.0891,  0.1589,\n",
      "        -0.0709, -0.0418, -0.0459, -0.0026,  0.1630, -0.0228,  0.0362,  0.0665,\n",
      "         0.0199,  0.0311, -0.0793,  0.0584, -0.0846, -0.0298,  0.0471,  0.1816,\n",
      "         0.1290, -0.0308, -0.0354,  0.0684,  0.0022,  0.1397,  0.1273, -0.0121,\n",
      "        -0.0255,  0.1549, -0.1043,  0.0030, -0.0070, -0.0533, -0.1327, -0.0505,\n",
      "        -0.0394, -0.0871, -0.1559, -0.1013, -0.0389,  0.0533, -0.0024,  0.0499,\n",
      "         0.0578, -0.0086, -0.0890, -0.0100,  0.0792, -0.0145, -0.0229, -0.0173,\n",
      "        -0.0718,  0.0246, -0.0108, -0.0746, -0.1079, -0.1119, -0.0225,  0.0620,\n",
      "        -0.0441,  0.0702,  0.1055, -0.0187,  0.0807,  0.0159,  0.0401,  0.0435,\n",
      "        -0.0720, -0.1575, -0.0476, -0.0490, -0.0268,  0.1036,  0.0390,  0.0015,\n",
      "        -0.1407, -0.0818, -0.0521, -0.0193,  0.0634,  0.0762, -0.0572,  0.0335,\n",
      "        -0.0147,  0.0902, -0.0812,  0.0083, -0.1243, -0.0758,  0.1391,  0.0418,\n",
      "         0.0337, -0.0012,  0.0702, -0.0611,  0.0674,  0.0109,  0.0365, -0.0833,\n",
      "        -0.0679, -0.0756,  0.0385, -0.0285,  0.0510, -0.0359,  0.0606,  0.0541,\n",
      "         0.0934, -0.0538, -0.0293,  0.0203, -0.0051,  0.1183, -0.0098,  0.0472,\n",
      "         0.0742, -0.0267, -0.0643, -0.0058,  0.0205,  0.0397, -0.0012,  0.0355,\n",
      "         0.0729,  0.0082,  0.0999,  0.0031,  0.0537,  0.0390,  0.0033,  0.0092,\n",
      "         0.0299, -0.0649,  0.0372,  0.0805,  0.0463, -0.0983, -0.0180, -0.0175,\n",
      "         0.0584, -0.0766,  0.0062, -0.0004,  0.0233, -0.0832,  0.0306,  0.0634,\n",
      "         0.0414, -0.0457,  0.0292, -0.0461,  0.0299,  0.0362,  0.0514,  0.0055,\n",
      "        -0.0551, -0.0026, -0.0381, -0.0229, -0.0396, -0.0021,  0.1161, -0.0633,\n",
      "         0.0352, -0.0886,  0.1244, -0.0195,  0.0971,  0.0900, -0.1717, -0.0553],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-2.8312e-02,  4.9911e-02,  9.7769e-03, -1.7147e-02,  4.0901e-02,\n",
      "        -1.2317e-01, -1.1881e-01,  8.5501e-02,  1.1018e-01,  6.2696e-02,\n",
      "         3.1070e-02, -1.0946e-01,  7.7663e-02,  6.7539e-02, -1.3375e-04,\n",
      "        -1.2912e-02,  5.7624e-02, -7.1261e-02,  9.6846e-04, -4.5915e-03,\n",
      "         6.0058e-02,  2.9872e-02,  4.2197e-02,  3.8850e-02,  5.4885e-02,\n",
      "         4.4528e-02, -8.8942e-02,  1.1722e-01, -4.4009e-02,  3.8589e-02,\n",
      "        -7.9293e-02, -1.1473e-02, -2.3653e-02, -4.3948e-02, -2.1827e-02,\n",
      "        -4.3308e-04,  8.2051e-02,  6.2999e-02,  3.0414e-02,  1.3454e-02,\n",
      "         5.9846e-03,  1.5785e-02, -6.2734e-02,  7.9752e-02, -1.4402e-01,\n",
      "        -5.4157e-02,  8.3404e-02, -5.4182e-02, -3.7938e-02,  1.9626e-03,\n",
      "         6.2376e-02, -9.8665e-02,  1.1238e-01,  8.4942e-02, -5.1376e-02,\n",
      "        -4.4197e-03,  1.0537e-02,  7.6728e-02,  7.0679e-02,  7.5002e-02,\n",
      "         2.3206e-02,  2.2686e-02,  3.7321e-02,  3.3898e-02, -2.2739e-02,\n",
      "        -1.1890e-01,  7.7856e-02,  1.0845e-01,  6.1648e-02, -2.4917e-02,\n",
      "        -5.6272e-02, -2.0143e-04, -6.7984e-02, -5.5723e-02,  1.5601e-03,\n",
      "         9.5723e-02, -1.2334e-01,  2.3138e-02,  1.5915e-03,  1.7391e-02,\n",
      "         1.0060e-03, -5.5752e-02, -7.3283e-03,  7.8786e-02, -8.5108e-02,\n",
      "         5.5049e-02,  1.5016e-01, -3.1859e-02,  4.4934e-03, -5.7109e-02,\n",
      "         8.0624e-03,  1.0309e-01, -3.0260e-03, -1.8075e-02,  1.0297e-01,\n",
      "         1.8190e-02,  8.1257e-02, -1.0586e-01,  4.6859e-02,  8.7545e-03,\n",
      "        -1.8347e-02,  7.8826e-04,  3.4076e-02,  3.4202e-02, -4.6036e-02,\n",
      "         7.8401e-02,  1.2534e-02, -2.9604e-02, -1.4013e-01, -1.2220e-01,\n",
      "        -3.9575e-02,  4.2375e-02,  6.8481e-02, -1.1031e-01,  1.7292e-03,\n",
      "         5.6505e-03, -1.3347e-01,  5.8967e-02,  1.0500e-01,  2.8959e-02,\n",
      "        -1.3579e-01, -3.6767e-02, -6.5603e-03,  5.9650e-02,  3.4714e-02,\n",
      "         3.4603e-02,  6.3472e-02,  8.8572e-02, -3.0379e-02,  1.2246e-02,\n",
      "         3.0892e-02, -1.9900e-02, -2.0532e-02, -9.3364e-02,  2.0879e-02,\n",
      "        -3.1082e-02,  7.4723e-02,  3.4827e-02,  9.9355e-03,  4.0432e-02,\n",
      "         9.0674e-02, -6.2378e-02, -1.7440e-02,  1.5880e-02, -1.3521e-02,\n",
      "         6.1648e-02, -2.5270e-02, -1.0506e-02,  1.8069e-02, -5.2453e-02,\n",
      "         1.3252e-02,  6.9504e-03, -5.8516e-02,  4.6623e-02,  1.4739e-02,\n",
      "         6.7765e-03,  3.7023e-03,  3.7319e-02,  1.9224e-02,  2.6738e-02,\n",
      "         8.2818e-02, -1.2007e-04,  7.7645e-02,  9.2141e-03,  4.3738e-03,\n",
      "        -1.0779e-01,  8.4956e-02,  3.7886e-02, -1.3384e-01, -1.1208e-01,\n",
      "        -5.7828e-02, -9.7238e-02,  1.0206e-02,  6.5645e-03, -2.8718e-02,\n",
      "         1.5325e-02,  6.6613e-02,  2.6445e-02, -2.4962e-02, -4.9788e-02,\n",
      "        -4.3545e-03, -4.5150e-02, -1.4951e-02,  6.1688e-02, -9.0608e-03,\n",
      "        -8.5805e-02, -1.0172e-01, -9.2241e-02, -1.5714e-03, -2.6098e-02,\n",
      "        -2.3720e-02, -4.2816e-03, -4.2465e-02,  4.0990e-03,  5.9952e-02,\n",
      "        -8.0171e-02,  3.4743e-02, -5.9418e-02, -5.0707e-04, -1.7003e-02,\n",
      "        -3.6289e-02,  9.0298e-02, -2.5486e-02,  2.2962e-02,  8.9927e-03,\n",
      "         3.8505e-02,  5.5345e-02, -2.0447e-02, -3.3111e-02,  3.7436e-02,\n",
      "         6.5773e-02, -4.5183e-02,  4.1996e-02, -8.7999e-02, -1.1769e-02,\n",
      "        -4.3234e-02, -6.6346e-02, -3.5659e-02, -5.7530e-03,  3.8261e-02,\n",
      "         6.5813e-02, -2.6030e-02, -7.3186e-03, -6.0748e-02, -5.1565e-02,\n",
      "        -2.2371e-02,  1.2256e-02,  7.5072e-02,  1.9970e-02,  2.4642e-02,\n",
      "        -7.0200e-02,  3.6686e-02,  2.4515e-02,  3.2946e-03,  6.7995e-03,\n",
      "         8.7247e-02, -6.1754e-02,  2.3224e-02,  4.8788e-02, -3.7919e-02,\n",
      "        -4.5916e-02, -6.3038e-03, -6.4867e-02,  9.7451e-03, -2.9809e-02,\n",
      "         1.9220e-02,  4.9873e-02, -8.4751e-02, -3.8756e-02,  2.4613e-03,\n",
      "         1.2979e-02, -1.9546e-02, -1.7456e-03,  6.0348e-02,  3.5478e-02,\n",
      "         8.5359e-02,  4.5793e-02, -2.9652e-02, -1.9533e-02,  2.8801e-02,\n",
      "         2.0128e-02, -1.6773e-02, -2.2567e-02,  8.6599e-02,  7.6258e-02,\n",
      "        -1.3919e-02, -5.2701e-03,  1.5254e-02, -5.6596e-03,  1.2512e-02,\n",
      "        -1.1107e-01, -3.9220e-02, -4.3274e-02, -1.4759e-02,  6.3456e-02,\n",
      "        -3.9313e-02,  6.6304e-02, -2.5031e-02, -8.0906e-02, -9.2574e-02,\n",
      "         7.7114e-03, -3.8525e-02,  2.6354e-02,  6.7656e-02, -3.6397e-02,\n",
      "        -6.6598e-02,  4.9100e-02, -4.5302e-02, -9.6687e-02,  3.2252e-03,\n",
      "        -1.6827e-02,  9.3235e-02, -2.9695e-02,  8.8593e-02,  1.0684e-01,\n",
      "         1.0159e-01,  7.8147e-02, -2.3984e-02,  7.4527e-02,  9.7435e-02,\n",
      "         9.9969e-02,  4.1802e-02,  5.5769e-02,  4.1883e-02,  3.7363e-02,\n",
      "        -1.2641e-02,  3.1162e-02, -5.7425e-04,  5.6984e-02,  2.1873e-03,\n",
      "         3.2089e-02, -7.0392e-02,  2.0635e-02,  9.4762e-03, -1.5822e-02,\n",
      "         5.4450e-02, -2.8916e-02,  1.6877e-02, -7.8206e-03, -1.1922e-01,\n",
      "         2.3058e-02,  6.5806e-02,  9.5983e-03,  4.4597e-02,  1.8453e-02,\n",
      "         4.3058e-02,  6.1493e-02, -6.8039e-02, -3.5424e-02, -3.8730e-02,\n",
      "        -4.6403e-02,  2.2619e-03,  1.3438e-02,  3.6322e-02, -9.0361e-02,\n",
      "         2.3885e-02, -6.8223e-02, -2.8933e-02,  1.0164e-01,  1.5505e-02,\n",
      "        -7.0034e-02,  7.1678e-02, -6.8170e-02,  4.8597e-02,  8.5489e-02,\n",
      "         3.4030e-02, -1.1827e-02,  4.7249e-02, -5.7491e-02,  6.4812e-02,\n",
      "        -3.8081e-02,  3.1269e-02,  4.8112e-02, -2.2889e-02, -1.2078e-01,\n",
      "         8.6875e-03,  2.7524e-03, -5.2020e-02, -1.3657e-02, -3.4252e-02,\n",
      "         1.2507e-01,  6.4650e-02, -4.3744e-02,  2.1554e-02,  7.2027e-02,\n",
      "         4.6084e-02,  1.0100e-01,  7.4042e-02, -5.4211e-02, -1.1455e-01,\n",
      "         5.7521e-02, -4.2710e-02, -7.8814e-02, -1.8124e-02,  4.4737e-02,\n",
      "        -5.1269e-02, -6.7855e-02, -8.3722e-02, -6.4286e-02,  3.4506e-02,\n",
      "         8.8117e-02,  4.1227e-02, -1.0366e-01, -5.4640e-02, -3.3339e-03,\n",
      "         1.3867e-01, -5.8631e-02,  1.0841e-02, -9.4331e-02,  1.0992e-01,\n",
      "        -1.8052e-02,  5.6607e-02, -3.0553e-03, -9.7665e-02,  3.6189e-03,\n",
      "         3.8424e-02, -2.0226e-02, -1.0399e-01,  7.1986e-02, -8.7396e-02,\n",
      "        -2.1321e-02, -3.3681e-02, -4.8806e-02, -9.9724e-03,  3.4821e-02,\n",
      "        -3.6701e-02, -1.0064e-01, -4.4952e-02, -2.9649e-02,  6.7568e-02,\n",
      "         1.0062e-01,  1.5413e-02, -5.2982e-03, -8.1491e-02,  6.9497e-02,\n",
      "         7.5970e-03,  2.6650e-02, -7.8061e-02,  8.9628e-02,  5.9069e-02,\n",
      "        -2.8076e-03,  2.2840e-02,  4.9031e-02, -3.0829e-02, -1.4460e-01,\n",
      "         2.0347e-02,  3.0446e-02,  4.5471e-02,  8.5173e-02, -1.1764e-02,\n",
      "        -1.9823e-02, -1.1526e-02, -1.4037e-02, -5.7210e-03,  3.2612e-02,\n",
      "         8.8098e-02,  2.5476e-02,  5.3235e-02,  9.3301e-02,  6.9620e-02,\n",
      "        -6.3628e-02,  6.8000e-02,  1.4908e-01, -5.6959e-02,  5.9116e-02,\n",
      "         2.2112e-02, -2.4973e-02, -2.7610e-02,  4.1903e-02, -2.0115e-02,\n",
      "         5.7806e-02,  1.3158e-03, -8.3065e-02,  4.6314e-02, -9.3857e-02,\n",
      "        -9.9200e-03,  4.4497e-02, -1.1722e-02, -6.1344e-02, -1.3309e-01,\n",
      "         4.0768e-02, -2.1628e-02, -5.0834e-02,  1.0866e-01,  1.6634e-02,\n",
      "         7.5386e-02,  1.1037e-01, -3.8678e-02,  5.1629e-02,  3.5886e-02,\n",
      "         3.2558e-02,  1.4227e-03,  5.5960e-02,  1.0197e-03, -5.6617e-02,\n",
      "         2.2816e-02, -1.3664e-01,  1.3298e-01, -3.5689e-02,  1.8169e-02,\n",
      "        -3.9363e-02, -4.9693e-02,  8.3050e-02, -1.3196e-02, -4.6567e-02,\n",
      "         3.9041e-02,  2.8396e-02, -2.6041e-02,  6.8008e-02, -1.0233e-01,\n",
      "        -1.5822e-02, -3.0579e-02, -4.8071e-02, -6.4514e-02,  1.8201e-02,\n",
      "        -4.3278e-02, -4.3680e-03, -8.4785e-02, -5.5908e-02, -6.7275e-02,\n",
      "         8.3114e-02,  1.3823e-02,  4.9019e-02,  4.0267e-02, -5.4514e-02,\n",
      "         4.9135e-02, -4.8312e-02, -2.4285e-02, -9.7027e-02,  2.4834e-02,\n",
      "         1.4886e-02,  6.9949e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 3.0289e-02,  3.1503e-02,  3.9986e-02,  1.3083e-01, -5.3132e-02,\n",
      "         2.9113e-02,  4.7187e-03,  5.0454e-02,  1.0700e-01, -2.2314e-02,\n",
      "         2.6524e-02, -1.1840e-02,  5.0855e-03,  7.3779e-04, -1.1865e-03,\n",
      "        -4.7954e-02,  1.0474e-02,  2.8582e-02, -7.9896e-02,  7.6038e-02,\n",
      "         4.5977e-02, -1.4148e-02,  3.9841e-02,  1.8766e-02,  8.0392e-02,\n",
      "         2.6746e-02,  2.9566e-02, -2.5976e-02,  1.6478e-02, -5.0035e-02,\n",
      "         2.4266e-02,  4.7684e-03, -4.6095e-02,  5.4383e-02, -5.5842e-02,\n",
      "        -6.3235e-02,  1.0002e-01, -7.9192e-03,  4.9059e-02, -2.9653e-02,\n",
      "         7.4298e-02,  3.2793e-02,  8.6242e-02,  1.3700e-03,  1.4234e-02,\n",
      "         7.6310e-02,  3.2565e-02, -5.5205e-02, -2.8722e-02, -3.9794e-02,\n",
      "         8.0323e-02, -1.0903e-01, -4.8134e-04,  4.3818e-02, -3.0959e-02,\n",
      "        -5.7084e-02,  4.3061e-02,  4.2138e-02,  7.2363e-02,  4.3792e-02,\n",
      "        -7.2850e-02,  5.2529e-03,  4.6195e-03, -6.2514e-02,  8.1972e-02,\n",
      "        -1.2628e-02,  1.1640e-01, -7.5081e-02,  2.6473e-02, -6.2586e-02,\n",
      "        -6.8327e-02,  5.4805e-03, -8.0045e-02, -1.0655e-02, -7.7074e-03,\n",
      "        -8.1215e-02, -1.6442e-02,  6.8840e-03, -6.9273e-03, -4.1731e-02,\n",
      "        -6.2782e-02,  6.2828e-02, -8.7719e-02,  1.7283e-02, -5.3315e-02,\n",
      "        -9.8364e-02, -9.7457e-02,  8.1505e-02,  2.6662e-02,  5.2712e-02,\n",
      "         5.1618e-02, -3.9540e-02, -1.0101e-01, -2.3273e-02,  1.6070e-02,\n",
      "        -3.2476e-02, -3.7883e-02, -1.9677e-02, -3.3466e-02,  1.7523e-02,\n",
      "        -9.1086e-02, -4.3556e-02,  7.8876e-02, -4.1143e-02, -3.5400e-02,\n",
      "        -1.7865e-02,  1.7630e-01,  1.3965e-01, -5.0848e-02, -3.6669e-02,\n",
      "         2.1116e-02, -1.0324e-01, -1.7145e-02,  6.3624e-02, -7.2753e-02,\n",
      "         8.1110e-04,  7.7122e-02,  6.0167e-02,  9.4302e-02,  3.3645e-02,\n",
      "         5.1997e-02,  9.3938e-03,  1.5380e-02,  3.0624e-02,  1.8364e-02,\n",
      "         9.4459e-02, -5.3204e-02,  5.3909e-02,  8.4368e-02, -2.6575e-02,\n",
      "         5.8741e-03,  1.7135e-01,  3.8734e-02,  1.1533e-01, -3.4991e-02,\n",
      "        -1.3902e-01, -5.0564e-02,  2.5342e-02,  1.9510e-03, -4.5458e-02,\n",
      "        -7.6664e-02,  1.0237e-01,  7.7267e-03,  5.8986e-02, -1.9288e-02,\n",
      "         5.3286e-02,  3.6359e-02,  8.0501e-02, -8.3045e-02,  3.3307e-02,\n",
      "         1.5659e-03,  9.6013e-03, -1.5590e-02, -5.1359e-02, -7.0246e-02,\n",
      "        -1.1975e-02,  2.6491e-02, -3.2005e-02,  6.8249e-02,  4.7669e-02,\n",
      "         4.7641e-02, -2.1512e-02, -6.3295e-02, -4.1788e-02, -1.5279e-02,\n",
      "        -9.7037e-02,  2.2685e-02,  2.0949e-02,  3.3309e-02,  9.4829e-03,\n",
      "         5.6710e-02, -7.6783e-03, -1.3969e-01, -4.1760e-02,  8.8335e-03,\n",
      "         4.3914e-02, -1.1144e-02,  2.1213e-02,  5.0143e-02, -1.7819e-02,\n",
      "        -3.6000e-02, -9.8346e-02,  1.8010e-02,  1.1031e-02, -4.7298e-02,\n",
      "        -2.5419e-02, -4.0803e-02,  3.5511e-02,  9.2070e-03,  6.9367e-03,\n",
      "        -4.2061e-02, -1.0377e-02,  8.0876e-02, -5.6107e-02,  5.7277e-02,\n",
      "         8.7439e-03,  1.8353e-02, -4.1559e-02,  3.4507e-02, -1.0548e-01,\n",
      "        -4.0571e-02, -2.1289e-02,  3.0586e-02,  5.1678e-03,  8.7577e-04,\n",
      "         1.3942e-01, -1.1645e-02,  7.2364e-02,  6.5043e-02,  2.4132e-02,\n",
      "         1.1002e-01,  6.1222e-03,  6.6061e-03, -5.2206e-02, -1.3325e-02,\n",
      "        -8.5573e-03, -2.0275e-03,  1.6365e-03,  2.6494e-02,  7.1705e-02,\n",
      "        -7.1865e-02,  8.4742e-02,  6.0429e-02, -5.9917e-04, -5.1137e-02,\n",
      "        -5.9481e-02, -7.6383e-02,  4.8239e-02, -3.4069e-02, -9.6994e-02,\n",
      "         1.8230e-02,  8.8950e-02,  8.6447e-02, -2.9383e-02, -9.0702e-02,\n",
      "        -3.7237e-02, -3.5979e-02, -4.2816e-02, -7.7253e-02,  7.3348e-03,\n",
      "         4.4436e-02, -1.5954e-01,  1.2394e-01,  1.1889e-02,  1.5041e-02,\n",
      "        -6.7389e-02, -4.5964e-02,  2.0859e-02, -3.0347e-02, -2.0750e-02,\n",
      "         3.9519e-02, -2.8886e-02, -8.1723e-02, -2.2986e-02, -2.3117e-03,\n",
      "         7.9396e-02, -4.6225e-02,  5.9592e-02, -6.6315e-02, -4.8456e-02,\n",
      "        -4.7836e-03, -6.7407e-02,  4.6288e-02,  1.5025e-01,  3.1964e-02,\n",
      "        -1.0685e-01, -3.1458e-02, -4.1457e-02,  7.1839e-02, -9.0231e-02,\n",
      "         3.3797e-02, -2.6273e-02, -6.0258e-02, -3.0063e-02, -9.9684e-02,\n",
      "         8.9154e-02,  4.6204e-02,  1.0030e-02, -2.1860e-02, -9.5296e-03,\n",
      "        -2.6632e-02, -2.0542e-02, -8.8112e-02, -3.1891e-02,  8.1285e-02,\n",
      "         3.4284e-02,  9.3343e-02, -7.2938e-02,  4.2222e-02,  8.5092e-02,\n",
      "        -6.9859e-02, -1.1665e-01, -1.7408e-02, -1.5403e-02,  5.4243e-02,\n",
      "         9.8341e-03, -2.8077e-02, -2.9991e-02,  3.4399e-02,  1.4826e-02,\n",
      "         1.0260e-02,  8.0673e-02,  5.1878e-03, -8.1736e-02,  8.6033e-02,\n",
      "         8.2636e-02,  5.0595e-02, -1.1922e-01,  9.3888e-03,  2.7255e-02,\n",
      "         2.7873e-02,  2.2796e-02,  1.8762e-02,  1.4380e-01, -1.4723e-01,\n",
      "        -1.4255e-02, -3.0604e-02, -3.7668e-03,  1.1167e-02, -8.0839e-02,\n",
      "         1.4414e-02, -2.5007e-02, -2.3666e-02, -2.7692e-02, -1.6474e-02,\n",
      "         5.1326e-02, -6.8901e-03,  2.6673e-02, -1.9049e-02, -4.9653e-02,\n",
      "         1.1313e-01,  8.5847e-02,  1.3205e-01, -4.7806e-02, -9.3220e-02,\n",
      "         4.1846e-02, -4.5715e-02,  2.4093e-02, -3.6066e-02,  5.0121e-02,\n",
      "         2.4745e-02, -9.0033e-02,  5.9747e-02, -5.9992e-02, -2.5795e-02,\n",
      "        -3.5649e-02,  2.3503e-02,  1.4340e-01, -5.7906e-02, -8.6132e-03,\n",
      "        -6.0701e-03,  3.0256e-03, -6.0207e-02,  1.3398e-02, -3.4405e-03,\n",
      "         3.6077e-02, -7.9061e-02, -4.5184e-02, -6.7206e-02,  8.3835e-02,\n",
      "        -1.4701e-02,  2.4760e-02,  1.7550e-02,  5.2360e-02, -1.1143e-01,\n",
      "        -6.0042e-02, -2.1617e-02, -2.3820e-02, -1.9716e-02, -1.1295e-01,\n",
      "        -1.7096e-02, -5.0607e-02,  9.7075e-02,  2.0780e-02, -4.8206e-02,\n",
      "         4.0675e-02, -5.4123e-02,  2.6274e-02, -1.1451e-01,  5.9652e-02,\n",
      "        -2.4965e-02, -2.3823e-02,  5.4150e-03, -2.5337e-03, -5.9982e-02,\n",
      "        -3.6474e-02, -1.8158e-02, -1.5301e-02,  1.1725e-02,  2.3499e-02,\n",
      "         7.4033e-02, -4.0130e-02, -5.1274e-02,  9.0815e-02,  5.4975e-02,\n",
      "        -3.4270e-02,  4.5382e-02, -7.2244e-02, -7.0036e-02, -9.7178e-03,\n",
      "        -3.3955e-02, -3.5253e-02,  8.1896e-02,  7.5562e-03, -7.9211e-02,\n",
      "        -1.0875e-01,  1.2409e-03,  7.7800e-02,  1.0634e-02, -8.2665e-02,\n",
      "         1.3230e-02, -3.4552e-02,  9.1453e-02, -6.4865e-02,  4.5128e-02,\n",
      "        -1.1324e-01, -5.8086e-02,  4.5286e-02, -3.5615e-02,  1.1491e-03,\n",
      "         4.5156e-02,  2.6197e-02, -9.7915e-02, -8.8574e-02,  6.3982e-02,\n",
      "        -7.3688e-02,  3.8706e-02,  8.2396e-02,  7.6938e-02, -2.0139e-02,\n",
      "        -6.2673e-02, -8.2048e-02,  5.6388e-02,  1.7644e-02,  4.3307e-02,\n",
      "         8.2072e-03, -4.8394e-02,  7.1145e-03, -1.4995e-01,  6.3767e-02,\n",
      "        -1.7300e-02, -4.0330e-04,  2.5645e-02,  6.1843e-02, -5.0088e-03,\n",
      "         3.9473e-03,  8.7710e-02,  3.0694e-02, -1.5863e-02,  1.2367e-01,\n",
      "         5.8815e-02,  6.1809e-02,  1.1823e-01,  3.4193e-02, -1.3734e-01,\n",
      "        -8.3475e-03, -1.3101e-02,  1.7372e-01,  3.1849e-02,  5.8699e-02,\n",
      "        -8.2168e-02,  2.9679e-02,  2.9754e-02, -1.9589e-02, -2.3867e-05,\n",
      "         2.9229e-03, -5.9795e-02,  1.0513e-01, -2.3250e-02,  1.5259e-02,\n",
      "        -9.9677e-04,  5.2436e-02,  4.5202e-02, -5.3536e-02, -3.1198e-02,\n",
      "         1.1600e-01,  8.2992e-02, -6.0462e-02, -6.9867e-02, -2.0561e-03,\n",
      "         6.2426e-02,  3.0686e-02,  7.3595e-03, -5.2512e-03, -8.7785e-02,\n",
      "         7.2232e-02, -5.5166e-02,  5.2830e-02, -3.4109e-02, -3.5072e-02,\n",
      "        -7.8913e-02,  3.6241e-02,  4.8680e-02, -2.4749e-02,  9.5748e-02,\n",
      "         1.1784e-01,  6.6303e-02, -3.3105e-02,  3.1397e-02,  4.8392e-02,\n",
      "        -9.6809e-02,  6.1331e-02,  3.0868e-02,  3.2937e-02,  1.4860e-02,\n",
      "        -8.8214e-02, -7.5167e-02, -2.6680e-02, -7.2619e-02, -3.8868e-02,\n",
      "         4.7005e-02, -1.5254e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-3.7034e-02, -4.5888e-02,  8.8781e-03,  2.7156e-02,  5.8858e-02,\n",
      "         1.2498e-03, -2.9473e-02, -2.4259e-02,  2.7695e-02,  4.8506e-02,\n",
      "        -1.3610e-02,  2.4264e-02, -1.0506e-02, -2.2343e-02, -1.2575e-02,\n",
      "        -2.7388e-02,  3.7047e-03, -9.8502e-02, -7.6187e-02, -1.3275e-02,\n",
      "         4.0868e-02,  3.1048e-02,  2.9744e-03, -3.4535e-02,  6.2692e-02,\n",
      "        -1.0555e-01, -1.8775e-03, -6.1323e-02,  1.1437e-02,  6.9841e-02,\n",
      "        -1.2952e-02,  7.9710e-02, -3.6756e-02,  1.2847e-02,  1.0407e-01,\n",
      "        -8.7324e-02, -1.0587e-01, -3.1902e-02, -8.2598e-03, -1.0516e-01,\n",
      "        -9.7262e-02,  1.1731e-02, -1.1542e-02, -1.0035e-01, -8.8628e-02,\n",
      "        -1.6604e-02, -6.6435e-04, -5.5660e-02, -5.8090e-03, -9.9288e-03,\n",
      "         2.7286e-02, -4.0562e-02, -1.3763e-02, -5.6210e-02, -8.2477e-03,\n",
      "         3.0968e-02, -2.2097e-02,  2.6884e-02, -4.4554e-03,  6.1624e-02,\n",
      "         5.7080e-02,  9.1388e-03, -2.2383e-02,  3.1594e-02,  9.0034e-02,\n",
      "         2.9283e-04, -2.4813e-03, -4.8279e-02,  2.9078e-02,  3.5868e-02,\n",
      "         4.1491e-02, -6.3660e-02, -8.4763e-02, -8.1597e-02, -5.1852e-02,\n",
      "         2.2601e-04,  1.0845e-01,  3.0973e-02, -1.5400e-01,  3.3164e-02,\n",
      "         7.9088e-02,  6.5250e-02,  5.1900e-02, -4.2283e-02, -1.1346e-01,\n",
      "        -9.0076e-03,  1.1980e-01, -1.1909e-02,  1.2310e-02,  1.8831e-02,\n",
      "        -4.9647e-02,  7.0969e-02, -2.3682e-02, -8.6618e-02,  5.2677e-02,\n",
      "         7.8079e-03, -1.0115e-01,  7.5915e-02, -4.8108e-02, -1.3128e-01,\n",
      "         6.4873e-02, -7.1029e-03, -1.4379e-01, -2.1432e-02, -5.3666e-02,\n",
      "         3.7874e-02, -8.1764e-02,  1.6618e-01,  7.1652e-02,  4.2189e-02,\n",
      "        -4.8112e-02,  5.0704e-02, -8.4332e-02,  2.3637e-02, -1.1713e-02,\n",
      "        -1.4738e-01, -5.6326e-02, -8.2328e-02, -6.9366e-03,  8.9393e-03,\n",
      "         9.0724e-02, -3.4346e-02, -1.7982e-02, -1.4817e-02, -9.2182e-02,\n",
      "         3.9414e-02, -1.3945e-02, -9.3391e-02,  1.0452e-01,  8.3443e-02,\n",
      "        -8.3101e-03,  5.8458e-02,  3.4724e-02, -9.1750e-02,  2.9846e-02,\n",
      "        -9.8895e-02, -2.4202e-02,  4.6580e-02,  4.4337e-02, -1.2447e-02,\n",
      "        -8.0480e-03, -5.6974e-03, -3.7265e-02,  7.7061e-02,  5.1464e-02,\n",
      "        -7.0224e-02, -4.4164e-02,  2.5564e-02,  1.2461e-02, -2.4537e-02,\n",
      "         2.2466e-02,  6.7765e-03, -2.1143e-02,  1.3173e-02, -4.8422e-02,\n",
      "        -2.4130e-02,  4.0795e-02, -6.9050e-02,  5.2960e-02,  2.9344e-02,\n",
      "         6.1323e-02,  2.6642e-02, -1.5501e-02,  1.1257e-02,  5.2199e-02,\n",
      "        -1.9131e-02, -7.1120e-02,  1.5206e-01, -5.5123e-02,  1.6600e-02,\n",
      "        -1.7471e-02,  5.4039e-02,  7.3465e-02, -1.4534e-02,  3.2988e-02,\n",
      "         1.0805e-01,  2.3235e-03,  2.6146e-02,  5.6207e-02,  2.4650e-02,\n",
      "         1.0190e-02, -4.5924e-03,  4.1432e-02, -4.8620e-02, -2.9034e-02,\n",
      "        -2.9012e-02,  1.4155e-02,  3.5942e-02, -9.4590e-03, -3.9627e-02,\n",
      "        -5.3268e-02,  1.3831e-01, -3.0257e-02, -5.7423e-03,  4.2466e-02,\n",
      "         1.2649e-01, -5.0767e-02, -1.1174e-02, -2.3112e-02,  3.8812e-02,\n",
      "        -6.3522e-02,  9.1453e-02,  2.6309e-02, -1.1686e-01, -3.9759e-02,\n",
      "         2.4578e-02, -4.7622e-03, -5.6869e-02,  9.6072e-02,  1.3556e-02,\n",
      "        -2.8459e-02, -4.5581e-02,  1.2914e-01, -1.1633e-02,  1.1193e-01,\n",
      "        -8.6753e-02, -8.5673e-03, -7.3127e-02, -3.6154e-02, -9.3040e-02,\n",
      "        -3.7462e-02,  1.2344e-01,  8.0146e-02, -1.7490e-02,  1.1924e-01,\n",
      "        -1.0738e-02,  6.7925e-02, -6.9445e-02, -2.5708e-02, -5.6665e-02,\n",
      "        -1.5419e-01,  1.2431e-01, -7.5615e-03, -1.0575e-01,  8.1955e-02,\n",
      "        -3.7937e-02,  8.6439e-02, -3.1533e-03,  1.4085e-01,  3.6980e-02,\n",
      "        -1.3440e-02, -5.1998e-02,  5.9634e-02, -4.4400e-02,  1.6468e-02,\n",
      "         3.7003e-02,  2.0843e-02,  4.8651e-02, -3.7829e-02,  1.0212e-01,\n",
      "        -1.8587e-02,  4.5990e-02, -4.5087e-03, -1.0517e-01, -7.8714e-02,\n",
      "        -2.2157e-02, -5.8386e-02,  7.0721e-02, -1.4240e-02, -1.0749e-01,\n",
      "        -6.8921e-02, -3.1443e-02, -3.2220e-02, -6.4972e-02,  1.1256e-02,\n",
      "         4.3494e-02,  1.8916e-02, -1.8547e-01, -2.1113e-02, -3.5792e-02,\n",
      "        -1.2145e-02,  4.6165e-02, -1.1010e-01,  3.3331e-04,  8.4547e-02,\n",
      "         5.4524e-02,  4.8118e-02, -9.5097e-02, -7.2445e-02, -6.6263e-05,\n",
      "         5.1787e-02,  4.9852e-02, -4.7932e-02, -1.2280e-02, -1.6250e-02,\n",
      "        -1.4342e-02, -1.1116e-01, -5.5778e-02, -7.7247e-03, -8.1662e-02,\n",
      "        -4.3206e-03,  6.6698e-02, -5.0373e-02, -1.2831e-01,  7.0735e-02,\n",
      "        -4.0484e-02, -2.6315e-02, -2.7391e-02, -8.0403e-02, -6.9732e-03,\n",
      "         5.4342e-02,  2.0656e-02,  1.5141e-01,  1.0275e-01,  1.5837e-03,\n",
      "        -1.4563e-01,  8.5911e-05,  4.7454e-03, -7.8300e-02,  4.8858e-02,\n",
      "        -2.1546e-02,  1.4427e-02,  4.6923e-02, -4.1582e-02,  3.4860e-02,\n",
      "         1.6094e-01, -2.8653e-02,  6.8671e-02,  3.9210e-02, -2.7989e-02,\n",
      "         1.2157e-01,  3.4874e-02,  1.0473e-01,  5.0698e-02, -6.6427e-02,\n",
      "        -8.5859e-02,  4.0868e-02, -8.1263e-02,  1.2227e-04, -4.1179e-02,\n",
      "         7.0834e-03,  8.5109e-02, -2.0567e-02,  6.0143e-03, -8.9583e-02,\n",
      "         6.3068e-02, -4.5089e-02,  2.6703e-02,  5.3511e-03,  9.8072e-03,\n",
      "         9.1949e-04,  4.8803e-02, -1.2944e-02, -1.6477e-02,  3.7466e-03,\n",
      "        -7.1968e-02, -6.9599e-02, -1.0072e-01, -7.0090e-02,  3.5817e-02,\n",
      "         6.2147e-02,  8.6350e-02,  8.2676e-02,  6.9734e-03, -1.6660e-01,\n",
      "         3.0636e-02, -7.5360e-02,  8.7070e-02,  4.6590e-02, -1.2240e-02,\n",
      "         4.7421e-02,  1.4499e-01, -3.2117e-02,  6.7256e-03, -9.1146e-03,\n",
      "         5.6627e-02,  3.4365e-02,  3.5674e-02,  1.1961e-03,  9.1195e-03,\n",
      "        -1.0258e-01, -2.6809e-02, -3.6439e-02, -5.3987e-02, -3.7285e-02,\n",
      "        -4.7299e-02,  2.0322e-02, -7.9408e-02, -7.7213e-02, -4.1219e-02,\n",
      "         1.1305e-01, -3.6860e-02,  3.4759e-02,  4.5197e-03, -1.8849e-02,\n",
      "        -1.1627e-02,  7.8283e-02, -5.6437e-02,  3.5024e-02,  6.2222e-02,\n",
      "        -8.2901e-02,  7.1049e-02,  9.9048e-03,  8.3881e-02,  3.7555e-03,\n",
      "         8.8532e-02,  9.2635e-02,  1.6246e-02, -3.0551e-02,  4.0173e-02,\n",
      "         3.9328e-02,  9.8969e-03,  7.2826e-04, -8.5527e-03,  1.9672e-02,\n",
      "         1.0268e-01, -4.0752e-03, -5.5843e-02,  1.5902e-02,  7.0855e-03,\n",
      "        -3.0325e-02,  2.9130e-02, -7.9757e-02,  2.0168e-02,  1.3599e-02,\n",
      "        -2.4822e-02, -8.0696e-03,  7.8805e-03,  3.1998e-04, -3.3752e-02,\n",
      "        -2.3653e-02,  7.4149e-02, -9.0394e-03, -6.5222e-03, -3.0573e-02,\n",
      "         1.1063e-01,  7.5828e-02,  4.1677e-02,  1.3911e-02, -7.0996e-03,\n",
      "         2.3597e-03,  2.6949e-03, -5.3042e-03,  7.1347e-02,  2.7978e-02,\n",
      "         9.5793e-04, -2.3873e-02, -7.2959e-02,  3.1148e-02, -6.5378e-02,\n",
      "         4.4773e-02, -4.6407e-02, -2.7808e-02,  6.0678e-02,  2.2824e-02,\n",
      "         1.2299e-02, -1.2252e-01, -9.4176e-02, -3.1335e-02,  6.1090e-02,\n",
      "        -8.9544e-02, -7.8463e-02, -1.0646e-01,  1.2856e-01,  5.3371e-02,\n",
      "        -3.5043e-02,  4.9204e-02, -2.7718e-02, -1.8169e-03, -3.2086e-02,\n",
      "         7.7823e-03,  6.8141e-03,  9.3693e-02,  1.6695e-02, -7.0995e-03,\n",
      "        -8.1406e-02, -1.0529e-02,  2.3930e-02, -2.4667e-02,  1.4599e-02,\n",
      "         2.2815e-02,  6.4431e-02, -8.6203e-02, -1.9157e-01,  3.7300e-02,\n",
      "        -2.8549e-02, -2.9900e-02,  2.0874e-02, -1.8929e-01,  6.7435e-02,\n",
      "        -4.1862e-02,  4.9628e-04,  7.5833e-03,  8.0471e-02, -1.7851e-02,\n",
      "        -4.5390e-02,  2.1833e-02, -1.6886e-02, -1.0043e-02, -7.4905e-02,\n",
      "        -9.9795e-04, -2.0626e-02,  8.3278e-02, -7.4464e-02,  3.2107e-02,\n",
      "         4.9412e-02, -5.9202e-02, -6.2015e-02,  1.0825e-02,  8.4142e-02,\n",
      "         6.0584e-02,  2.8453e-02, -6.4364e-02,  3.4312e-02, -3.1387e-02,\n",
      "        -1.0054e-02,  6.2364e-02,  9.9319e-02,  4.8268e-02,  3.6428e-02,\n",
      "         4.7602e-02, -2.9711e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 1.7651e-03,  3.1396e-03, -1.4551e-02, -3.5147e-03, -2.1400e-02,\n",
      "         1.1861e-01,  2.7863e-02, -5.9323e-02,  5.4408e-02, -3.9685e-02,\n",
      "        -2.9819e-02,  1.3290e-02, -3.3680e-02, -3.0514e-02, -9.5577e-02,\n",
      "        -2.7275e-02, -3.2411e-02,  1.0115e-01,  1.8278e-02,  5.4167e-02,\n",
      "        -6.5277e-02, -4.9623e-02,  3.8521e-02, -7.7113e-02,  2.3679e-02,\n",
      "        -2.1421e-02, -3.1328e-02, -3.8675e-02,  3.2301e-02,  8.0595e-02,\n",
      "        -1.7031e-01, -3.1086e-02,  7.4435e-02,  4.4086e-02, -5.2982e-02,\n",
      "        -3.7618e-02, -2.1757e-02,  2.6901e-02,  1.2416e-02, -6.1151e-02,\n",
      "        -5.6076e-03, -8.1322e-02,  1.2883e-01,  2.1244e-01,  6.3023e-03,\n",
      "        -6.4486e-02, -6.8903e-02, -5.2496e-02, -8.8419e-03, -4.8330e-03,\n",
      "        -9.8466e-02, -9.1724e-02, -4.6670e-03,  2.2265e-02,  7.4199e-03,\n",
      "        -6.7150e-03,  3.1992e-03, -1.9731e-02,  1.7806e-02, -7.7666e-03,\n",
      "         6.6665e-03, -4.9659e-03,  1.3266e-02, -3.1188e-02,  6.3222e-02,\n",
      "         2.4398e-02,  2.9437e-02, -7.7957e-04,  2.4054e-02,  1.6580e-01,\n",
      "        -7.9211e-02, -2.8934e-02,  3.4830e-02,  3.1386e-02,  1.2014e-03,\n",
      "         9.4098e-02, -5.5012e-03,  4.5756e-02,  3.2991e-02,  1.8693e-02,\n",
      "         4.4928e-02, -2.1605e-02,  4.0092e-02, -6.9511e-02, -8.6237e-02,\n",
      "        -1.2794e-01,  2.8559e-02, -3.4364e-02,  2.3834e-03,  9.2352e-03,\n",
      "        -2.0991e-03,  1.2794e-02,  1.0197e-02, -1.4751e-02, -5.3813e-03,\n",
      "         2.9286e-02,  8.8126e-02,  1.5448e-02, -1.4078e-02, -3.9143e-02,\n",
      "        -5.8560e-02,  5.4407e-02,  3.5490e-02, -7.9659e-02,  3.4453e-02,\n",
      "         2.5864e-02, -3.0899e-02, -1.5625e-02, -4.5447e-02,  4.7464e-02,\n",
      "        -3.1091e-02, -3.4445e-02, -5.4052e-02, -6.4918e-02,  4.4487e-02,\n",
      "         2.5045e-02, -1.1488e-02,  2.5262e-02, -1.2607e-02,  1.3235e-02,\n",
      "         2.8561e-02,  6.9778e-02,  3.5717e-02, -1.3796e-02, -1.6055e-01,\n",
      "        -6.3508e-02,  3.0388e-02,  3.6702e-02,  1.4510e-02,  8.2649e-02,\n",
      "        -9.6217e-03,  2.8959e-02,  3.8684e-02, -8.4300e-02, -1.5368e-01,\n",
      "         9.8709e-02, -7.2473e-02,  3.1997e-02,  1.1817e-01, -2.6140e-02,\n",
      "        -6.1742e-02, -1.6166e-02,  7.0216e-02, -1.2530e-01, -3.3601e-02,\n",
      "         1.8504e-02,  4.9253e-02,  1.5496e-01, -7.7431e-02, -1.4273e-02,\n",
      "        -1.3381e-02,  1.0467e-01, -7.3973e-02, -9.8395e-02, -2.9553e-02,\n",
      "         4.8231e-02,  6.4982e-02, -5.0469e-02,  3.5893e-02,  9.4489e-02,\n",
      "         6.2196e-02, -9.2381e-02, -8.7598e-02,  7.9401e-02, -6.6444e-02,\n",
      "        -1.0009e-02, -3.8275e-02, -2.5270e-02, -1.7952e-01, -9.5267e-03,\n",
      "        -1.3783e-01,  2.1312e-01, -1.1740e-02, -8.2986e-02,  3.5087e-02,\n",
      "        -1.9155e-02, -2.4328e-02, -4.0487e-02,  3.3686e-02, -1.7021e-02,\n",
      "        -5.0354e-02, -1.5596e-01, -1.7125e-03,  5.6674e-02,  6.6230e-03,\n",
      "         6.4058e-03, -3.7337e-03,  1.1259e-02, -2.4012e-02,  8.4532e-02,\n",
      "        -2.1994e-02,  3.6341e-03,  8.1102e-02, -5.8442e-02,  9.7022e-02,\n",
      "        -6.0901e-02,  5.0808e-02,  1.3352e-01,  1.6406e-02,  1.3148e-02,\n",
      "         2.8686e-02, -3.0704e-02, -4.3113e-02,  5.2098e-02, -5.5051e-02,\n",
      "        -1.1791e-01,  5.0002e-02,  2.3706e-03, -6.4074e-02,  5.0139e-02,\n",
      "        -3.7592e-02,  5.3099e-02,  3.9144e-02,  4.3691e-03,  1.4775e-02,\n",
      "        -7.3321e-02, -4.6698e-02,  1.2764e-01, -6.2895e-02, -2.6595e-02,\n",
      "         7.9530e-02,  3.6950e-02, -4.7796e-03,  3.2136e-02, -4.4875e-02,\n",
      "        -3.2131e-02,  8.3086e-02,  8.9513e-02, -6.2051e-03, -1.2118e-01,\n",
      "         2.6485e-02, -3.3139e-02,  4.4756e-02,  7.8008e-04,  7.1055e-02,\n",
      "         3.0050e-02,  8.2575e-03, -2.6538e-02, -3.9907e-02, -2.5800e-02,\n",
      "        -3.3800e-02,  1.8517e-02, -7.0688e-02, -1.3011e-01, -3.3101e-02,\n",
      "        -5.4424e-02,  3.0215e-02, -6.2839e-02,  2.4651e-02, -1.8812e-03,\n",
      "        -1.3442e-01,  1.2847e-02,  7.9453e-02,  8.0802e-02, -9.5993e-02,\n",
      "         3.4160e-02,  2.6102e-02, -8.6553e-03,  5.7268e-02,  8.5350e-02,\n",
      "         1.3918e-02,  1.1504e-02,  2.9779e-03,  1.0623e-02,  5.5536e-02,\n",
      "        -4.1146e-02, -9.3039e-02, -3.3455e-03,  1.5882e-02, -1.5050e-01,\n",
      "         7.5856e-03,  2.2823e-02, -3.8871e-02,  5.5844e-02,  5.4641e-03,\n",
      "        -2.4733e-02, -5.1179e-02, -1.8616e-02,  5.5658e-02, -6.9583e-02,\n",
      "        -6.0925e-02, -8.0161e-02, -1.0143e-01,  4.3837e-02,  1.3554e-01,\n",
      "         8.7156e-02,  2.5922e-02, -7.2726e-02, -1.8920e-02,  9.7482e-02,\n",
      "         2.0591e-02, -6.2224e-02,  5.4904e-02, -1.3960e-01, -7.6254e-02,\n",
      "         8.3799e-02, -3.9226e-02, -4.3723e-02, -3.3469e-02,  9.1810e-03,\n",
      "         4.9622e-02,  6.3080e-02, -2.8480e-02, -1.8700e-02,  6.6885e-02,\n",
      "        -6.8625e-03,  7.1043e-02,  7.1088e-02, -9.2783e-02,  9.1262e-02,\n",
      "         4.6247e-02, -2.9005e-02,  2.8690e-02,  1.9394e-02,  5.7164e-05,\n",
      "         2.2624e-02,  3.3163e-02,  1.7700e-02,  3.4232e-02, -2.9858e-02,\n",
      "        -7.4267e-02,  3.6014e-02, -4.4552e-02,  3.5258e-02, -1.0101e-01,\n",
      "        -6.7129e-03,  1.4119e-02,  2.7532e-02,  1.8333e-02,  1.0998e-01,\n",
      "        -4.3879e-04,  6.3078e-02,  1.9749e-02,  4.5188e-02,  1.7698e-02,\n",
      "        -1.6677e-02,  8.2497e-02, -7.5923e-02,  6.3407e-02,  6.3229e-02,\n",
      "         1.7209e-02,  8.9937e-02, -3.1758e-02,  2.4061e-02, -7.6937e-02,\n",
      "         2.9163e-03, -6.6448e-02, -1.3663e-02, -3.8498e-02, -6.1970e-02,\n",
      "        -5.3004e-02,  2.5560e-02,  1.7372e-01,  1.9347e-02,  7.7611e-02,\n",
      "         1.2019e-01, -1.5177e-01, -1.0369e-02, -3.0696e-02,  6.5096e-02,\n",
      "         1.3015e-02,  5.4550e-02, -5.5283e-02,  7.5891e-03, -2.0863e-02,\n",
      "        -2.2272e-02,  1.8210e-02, -6.6587e-03, -1.3865e-02,  5.7003e-02,\n",
      "        -1.9093e-02,  9.1872e-03,  9.9067e-02,  3.3590e-04,  4.0905e-02,\n",
      "         2.1044e-03, -6.7002e-03,  2.9374e-02, -1.1736e-02,  3.6019e-03,\n",
      "        -2.4367e-02, -3.7626e-02, -1.1231e-01,  1.7375e-02, -6.0035e-03,\n",
      "         5.7686e-02, -2.7193e-02,  1.9783e-02, -6.3263e-02,  2.2237e-02,\n",
      "         7.3779e-03, -2.8759e-03, -1.5603e-02,  5.7662e-02,  8.7457e-03,\n",
      "         1.0018e-02, -5.0072e-02, -3.7638e-02,  2.4585e-02, -9.2793e-02,\n",
      "        -1.1872e-01, -2.2116e-02, -1.0956e-01, -1.0836e-01,  8.6403e-02,\n",
      "         4.3467e-02,  2.0700e-02,  5.1945e-02,  3.0060e-02,  2.7744e-02,\n",
      "        -9.2273e-03,  7.5827e-02, -4.5446e-02,  8.2869e-02, -9.2931e-02,\n",
      "         1.2670e-02, -4.8179e-02, -1.5450e-01, -1.6038e-03, -2.9253e-02,\n",
      "        -2.7980e-02, -1.0475e-02,  2.7516e-02,  1.6998e-01,  2.4017e-02,\n",
      "         8.4535e-02, -2.9163e-04,  3.1187e-02,  5.4309e-02, -3.0479e-02,\n",
      "        -8.0611e-02, -6.6498e-02, -1.4551e-01,  2.1430e-03, -3.7552e-02,\n",
      "         7.6690e-02,  7.4113e-02, -1.0557e-01, -7.4909e-02,  6.7211e-02,\n",
      "        -7.8306e-02,  4.8829e-02, -4.6191e-02,  1.3408e-02,  2.7609e-02,\n",
      "        -7.1487e-03, -3.2693e-03,  6.9174e-02,  1.8630e-01,  8.2350e-02,\n",
      "        -7.1999e-02, -5.7636e-04, -1.0190e-01, -2.1849e-02, -2.4579e-02,\n",
      "         8.7751e-02, -3.5942e-02, -2.4704e-03, -1.1202e-01, -7.7516e-02,\n",
      "        -3.0877e-02,  5.2970e-02, -1.0476e-02,  9.5842e-03, -7.3720e-02,\n",
      "         4.0108e-02, -1.2442e-02,  3.8677e-02, -4.2649e-02,  3.2528e-02,\n",
      "         4.7383e-02, -1.2851e-03, -3.1630e-02,  9.8758e-02, -4.5205e-02,\n",
      "         9.8402e-02, -9.2297e-02, -1.9997e-02, -1.7744e-02, -2.2326e-02,\n",
      "         8.0307e-02, -1.7815e-02,  1.9394e-02, -5.2028e-02, -5.1993e-02,\n",
      "         5.9033e-03,  1.0825e-02, -1.7139e-02, -1.4043e-01,  3.3729e-02,\n",
      "         2.4079e-02,  3.1476e-02, -7.7750e-02,  4.4037e-03, -7.0054e-02,\n",
      "        -1.0412e-02,  8.7667e-03,  6.4475e-02, -6.7967e-02,  4.3379e-02,\n",
      "        -8.0798e-02,  1.3300e-01, -2.5715e-02,  4.1997e-02,  1.5607e-02,\n",
      "        -1.8457e-02, -1.4307e-02, -1.3592e-02, -8.4850e-04,  6.9601e-03,\n",
      "         1.7143e-02, -7.7591e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 7.0232e-02, -3.9177e-02, -5.6618e-02,  2.4083e-02, -1.1208e-01,\n",
      "        -8.1959e-02, -1.0443e-02,  3.7170e-02, -5.0199e-02,  3.4944e-02,\n",
      "        -1.1758e-01,  2.1176e-02, -1.3711e-02,  1.7198e-03,  5.5355e-03,\n",
      "        -2.0753e-02, -4.9122e-02, -1.6634e-02,  8.6761e-04,  5.3412e-02,\n",
      "        -5.2480e-02,  3.0198e-03, -2.5701e-02, -1.2355e-01,  5.1328e-02,\n",
      "        -6.1343e-02, -4.0117e-02,  6.6909e-02,  5.3694e-03,  2.3511e-02,\n",
      "        -8.8978e-03, -1.3987e-02,  5.8384e-02, -7.4027e-02,  7.0214e-03,\n",
      "        -2.3619e-02, -1.1647e-02,  5.0248e-03, -1.0472e-01,  5.0924e-02,\n",
      "         9.6729e-03,  4.4740e-02, -8.4304e-03,  3.1834e-02, -5.6028e-02,\n",
      "         1.2579e-02,  4.1046e-02, -7.6398e-02, -8.2215e-02, -7.4826e-02,\n",
      "        -5.8703e-03,  2.1899e-02, -3.4924e-02, -9.3841e-02,  9.2489e-02,\n",
      "        -3.9724e-02,  6.8778e-02,  6.2910e-02,  1.5035e-01, -8.5688e-02,\n",
      "         5.1889e-02, -9.3605e-02, -7.0402e-02,  4.7219e-02,  5.9798e-02,\n",
      "        -3.6312e-03, -1.3177e-02, -4.6579e-02,  2.6072e-02, -1.8031e-02,\n",
      "        -1.5455e-01,  1.6608e-01, -1.5167e-03, -2.2081e-02, -3.3239e-02,\n",
      "         7.1615e-03,  5.0772e-02,  6.4464e-03, -1.0717e-03,  1.1329e-01,\n",
      "        -4.0795e-03,  7.9883e-02, -4.3044e-02,  1.3580e-01, -1.0705e-02,\n",
      "         7.0666e-03, -7.1443e-03,  9.1426e-02, -1.3554e-03, -9.4658e-02,\n",
      "        -4.0040e-02,  5.9643e-02,  1.8720e-02, -6.1085e-03, -5.1143e-03,\n",
      "         5.2426e-03,  3.9795e-02,  5.7733e-02,  9.3336e-02,  4.6847e-03,\n",
      "         5.8702e-02,  2.5341e-02,  4.2893e-02,  7.5947e-02,  2.8520e-04,\n",
      "         7.1536e-03, -4.3884e-03, -4.4555e-02, -4.4503e-02,  5.6192e-02,\n",
      "        -5.1656e-02, -1.2393e-01,  4.3672e-02,  4.7996e-02, -1.0800e-02,\n",
      "         5.6755e-02, -8.3568e-02, -1.3538e-02, -5.6153e-02, -5.5316e-02,\n",
      "        -2.3975e-02, -1.1282e-01, -2.8566e-02,  7.2766e-02, -3.8624e-02,\n",
      "         7.6615e-02,  3.6164e-02,  1.0354e-01,  4.9160e-02,  1.9378e-02,\n",
      "        -2.2329e-02, -1.2350e-01,  1.2831e-01,  9.7161e-03,  8.3806e-02,\n",
      "        -5.0945e-02, -2.3909e-02, -2.4867e-02,  5.3618e-02,  4.3033e-02,\n",
      "        -8.6281e-03, -3.7764e-02, -1.2432e-01,  1.3901e-02, -8.2746e-02,\n",
      "         1.5292e-02, -1.0102e-01, -2.1163e-03,  2.4047e-02, -3.3842e-02,\n",
      "         1.7279e-01, -2.0493e-03, -1.4493e-02,  5.7667e-02, -2.8942e-02,\n",
      "        -3.2882e-03,  7.1961e-02,  1.5763e-02, -1.0857e-01,  3.1682e-02,\n",
      "        -1.5458e-02,  2.3903e-02, -7.8493e-02,  3.3385e-02, -1.1762e-02,\n",
      "         5.4726e-02, -1.0496e-01, -1.9116e-02,  4.4039e-02, -4.5159e-02,\n",
      "         1.1691e-01, -7.5459e-02, -3.4751e-02, -7.0932e-05, -5.4284e-03,\n",
      "        -3.1645e-02,  7.8052e-02, -1.3927e-02, -3.9138e-02, -6.9432e-02,\n",
      "        -5.6814e-02,  4.7092e-02, -9.7913e-02, -7.1706e-02, -7.4354e-02,\n",
      "         2.9061e-02,  1.2788e-01,  3.2878e-02,  6.8620e-02,  7.8050e-03,\n",
      "        -8.1034e-03,  1.2591e-01, -2.5306e-02,  2.3245e-02,  6.0525e-03,\n",
      "         5.1102e-02,  2.6583e-02, -2.1282e-03, -5.5411e-02,  4.6495e-02,\n",
      "        -2.4725e-02,  2.2852e-02, -1.2736e-02,  1.6637e-01, -5.4719e-02,\n",
      "         8.6107e-02, -5.4407e-02,  6.8237e-02, -6.1891e-02, -5.5849e-02,\n",
      "         7.3760e-03, -3.0345e-02, -3.1600e-02,  3.3583e-02,  2.8570e-02,\n",
      "         8.2200e-02,  2.6655e-02,  2.6249e-02, -1.2001e-02,  7.8356e-02,\n",
      "        -1.6183e-02, -1.4890e-02,  1.2511e-02,  3.7454e-02,  2.5717e-02,\n",
      "         2.4392e-03,  1.9375e-02,  6.4533e-02,  3.3817e-02, -6.6789e-02,\n",
      "        -8.1340e-02, -3.5166e-02, -2.8866e-02, -7.5490e-02,  3.9034e-02,\n",
      "        -4.9257e-02,  1.5981e-02,  1.2176e-02,  2.2973e-02,  1.0207e-02,\n",
      "        -1.0285e-03,  1.7862e-01,  6.4228e-02, -3.5339e-02,  8.1926e-02,\n",
      "         7.1711e-02, -1.0528e-02,  3.6034e-02, -2.3140e-02,  4.6343e-02,\n",
      "        -3.3368e-02, -4.6355e-02,  5.1168e-02,  1.8313e-02, -3.8195e-03,\n",
      "         1.0237e-01, -4.0303e-02,  3.3172e-02, -4.5773e-02, -1.4106e-02,\n",
      "        -3.1364e-02,  5.1665e-02, -3.1724e-02,  3.0433e-02, -3.9412e-02,\n",
      "        -3.3040e-02,  2.1146e-02, -1.1771e-01, -6.6739e-02, -3.3981e-02,\n",
      "        -4.4390e-03,  2.8506e-02,  1.9362e-02,  1.0839e-01, -1.5109e-02,\n",
      "         1.5135e-01, -2.9912e-02,  7.0132e-02, -6.2905e-02, -9.2045e-02,\n",
      "        -1.0811e-01, -6.4596e-02,  1.1569e-01,  4.2324e-02, -5.3588e-02,\n",
      "         2.2440e-02, -2.2649e-02, -7.6581e-02, -6.1811e-02,  6.7117e-02,\n",
      "        -8.8734e-02,  1.1926e-02, -1.0264e-02,  2.1893e-02,  6.1756e-02,\n",
      "         1.1959e-01, -9.6380e-02,  1.3470e-02, -7.0965e-02,  2.2478e-02,\n",
      "         5.0166e-02, -4.6788e-03,  9.3105e-02,  1.2183e-01, -1.0024e-01,\n",
      "         1.9777e-04,  8.1114e-02, -2.6921e-02,  1.0334e-01, -3.6504e-02,\n",
      "         1.0802e-02, -2.5081e-02, -6.5181e-02,  8.6339e-02,  3.7305e-02,\n",
      "        -1.2546e-01, -2.3171e-02, -5.1505e-02,  8.1840e-02,  4.9002e-02,\n",
      "         1.8363e-02,  2.9693e-02, -3.8902e-03, -4.1257e-02, -2.2935e-02,\n",
      "         8.3203e-02,  5.9329e-02,  7.7033e-03,  4.9673e-02, -3.4751e-02,\n",
      "        -3.4831e-03, -7.7208e-03,  1.0457e-01, -2.1170e-02,  6.3125e-02,\n",
      "        -1.2047e-02,  1.4499e-02, -5.0847e-02,  2.7684e-02,  8.1270e-02,\n",
      "        -2.4067e-02,  1.6061e-04,  4.5172e-02,  8.9830e-02,  5.0638e-03,\n",
      "        -2.7056e-02,  1.6215e-02, -1.2409e-01,  2.7129e-02, -3.0758e-02,\n",
      "        -4.1683e-02,  7.6068e-03,  1.4988e-02, -2.3955e-02, -8.0970e-02,\n",
      "        -1.0192e-01,  3.6965e-02, -2.6476e-02,  5.7144e-03,  7.6527e-02,\n",
      "         9.9065e-02,  4.3809e-02,  5.6087e-02, -6.8878e-02,  6.4834e-02,\n",
      "         2.2787e-02,  7.3976e-02,  9.9496e-03, -1.7695e-02,  8.8900e-02,\n",
      "        -9.6980e-02, -7.0818e-02,  4.9335e-02, -5.5873e-02, -6.7333e-03,\n",
      "        -1.0160e-02,  2.7102e-02, -2.2473e-02, -4.0724e-02, -4.3555e-02,\n",
      "        -4.3084e-02, -8.1544e-02, -2.8473e-02,  1.8932e-02,  2.8450e-02,\n",
      "        -8.1699e-02, -8.1030e-02,  4.9583e-02,  3.5871e-02, -1.5891e-02,\n",
      "        -8.8298e-03, -1.2130e-02, -8.1447e-02,  4.5123e-02,  6.6769e-02,\n",
      "         4.5007e-02, -1.0901e-02, -1.5257e-01,  1.2816e-02,  6.9188e-02,\n",
      "        -1.3537e-02,  1.0406e-01,  2.4015e-02,  4.3749e-02, -2.3074e-02,\n",
      "         6.2925e-02, -3.2508e-02, -5.4690e-02,  1.6847e-02, -5.7822e-02,\n",
      "         6.0195e-02, -1.6088e-02, -3.6611e-02,  4.3164e-03, -1.6129e-02,\n",
      "        -2.0027e-02,  2.7187e-02, -5.7546e-02,  1.6556e-02, -4.5320e-04,\n",
      "         2.4637e-02, -5.7647e-02, -4.5837e-02,  1.4810e-02,  1.4818e-02,\n",
      "        -9.2751e-03,  3.1316e-02,  4.6298e-02,  1.5679e-02,  2.5335e-02,\n",
      "         1.5162e-02, -6.5274e-02, -1.1448e-01,  3.5900e-02, -1.1034e-01,\n",
      "        -9.4011e-02,  3.3696e-02, -6.7059e-03,  1.4441e-02,  1.3973e-01,\n",
      "         7.2340e-02, -5.2067e-02, -1.5580e-02,  4.3312e-02, -6.7398e-02,\n",
      "         7.6808e-02, -4.1142e-02,  3.2319e-02,  1.2461e-01,  1.5610e-02,\n",
      "         7.3369e-02, -1.0851e-01, -4.5686e-02, -6.5544e-02,  7.0161e-02,\n",
      "        -4.9590e-03,  4.6399e-02,  4.5816e-02, -7.6833e-02,  5.7388e-02,\n",
      "         5.6216e-02,  1.7794e-02, -1.8920e-02, -4.4150e-02,  2.6347e-02,\n",
      "         8.7239e-02, -2.0536e-02, -1.2006e-02, -5.0354e-03,  3.5649e-02,\n",
      "        -8.1056e-02,  5.1311e-02,  1.9925e-02, -4.3425e-02,  2.6601e-02,\n",
      "        -7.5502e-02, -3.4638e-02, -7.5277e-02, -5.1211e-02, -4.9907e-02,\n",
      "         1.9271e-02,  2.3710e-02,  1.7192e-02, -7.7708e-02,  2.5729e-02,\n",
      "         5.5325e-02,  1.0182e-01, -9.2568e-02, -4.8824e-02,  2.3749e-02,\n",
      "         3.6623e-02, -1.6246e-02, -2.5600e-02, -8.7405e-02,  1.7550e-02,\n",
      "        -6.1699e-03, -4.0138e-02, -3.5954e-02, -6.4890e-02,  4.1684e-03,\n",
      "        -8.0014e-02, -7.6652e-02,  9.0478e-02,  2.2696e-03,  4.3178e-03,\n",
      "         1.3625e-01, -4.3848e-02,  3.4243e-02,  1.0695e-01,  2.2553e-02,\n",
      "        -1.3336e-02, -3.6943e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-6.3037e-02,  4.0266e-02, -4.1982e-02, -1.3677e-01, -2.8125e-02,\n",
      "        -3.5907e-02, -1.2441e-01,  3.9413e-02,  7.0257e-02,  2.5938e-02,\n",
      "         1.6168e-02, -4.9772e-03, -8.1783e-02,  5.8682e-02, -5.5678e-03,\n",
      "        -1.6609e-02, -7.0250e-02,  2.3007e-02, -1.1237e-01, -1.1362e-03,\n",
      "         2.6813e-02,  5.1206e-02, -1.0730e-01, -7.5571e-03,  6.9360e-02,\n",
      "        -3.0615e-02, -1.3997e-01, -1.6990e-02, -6.2863e-03,  3.9499e-02,\n",
      "         1.1053e-01,  7.3368e-02, -4.2755e-03, -6.1965e-02,  2.3791e-02,\n",
      "        -5.2535e-02, -2.8180e-02, -1.5272e-02, -2.4229e-02,  8.8722e-02,\n",
      "        -9.6024e-02,  4.9923e-02, -1.7209e-02,  1.6388e-02, -7.5840e-03,\n",
      "        -8.1901e-02,  6.1073e-02, -4.8348e-02, -1.0459e-02, -6.6470e-02,\n",
      "        -2.4781e-02,  5.3203e-02,  2.4020e-02,  4.9423e-02,  3.7947e-02,\n",
      "         2.0673e-01,  6.0895e-02, -4.0758e-02,  2.0071e-02,  1.0387e-01,\n",
      "         1.6163e-02, -1.5090e-02,  6.2854e-02,  2.6011e-02,  3.8561e-02,\n",
      "        -5.5162e-02, -7.5800e-02,  6.2836e-02,  1.3994e-02,  1.1939e-01,\n",
      "        -1.5908e-02,  4.2999e-02, -2.3383e-02,  1.5983e-02, -1.3671e-02,\n",
      "         9.9919e-02,  6.4056e-02, -7.5082e-02, -1.2190e-02, -1.4694e-02,\n",
      "         5.8069e-02,  5.5209e-02,  1.2079e-02,  5.1134e-02,  4.5578e-02,\n",
      "         5.2929e-02, -6.7412e-03,  9.7755e-02, -4.7786e-02, -1.6850e-02,\n",
      "        -5.9766e-02,  9.2122e-02, -2.9754e-02, -1.1698e-01,  2.3706e-02,\n",
      "        -2.3814e-02,  3.3031e-02, -1.1580e-01,  3.8596e-02, -3.3136e-03,\n",
      "        -1.2250e-02,  3.4611e-02, -8.4193e-02, -7.5750e-02, -3.5521e-02,\n",
      "        -5.1473e-03,  3.9007e-02,  6.4325e-03, -5.9280e-02, -1.3100e-02,\n",
      "        -4.1139e-02,  4.7848e-02,  8.4264e-03, -1.0753e-01, -4.3760e-02,\n",
      "         9.4994e-02, -1.7219e-02,  3.9596e-02, -4.1659e-02,  1.2531e-01,\n",
      "        -4.9070e-02,  1.2569e-02, -3.4510e-02,  5.6004e-02, -2.7773e-02,\n",
      "        -8.0413e-02,  7.7013e-02, -3.7365e-02, -7.8601e-02, -4.4590e-02,\n",
      "         1.6158e-02,  2.7064e-02,  1.0510e-01, -1.2408e-02,  1.6963e-02,\n",
      "        -9.1978e-03,  5.7486e-02, -4.6821e-02, -3.0573e-03, -1.0964e-02,\n",
      "        -8.9452e-02,  4.2682e-02, -1.1941e-02,  2.5132e-02, -3.7705e-02,\n",
      "         5.4186e-02, -7.1975e-02, -4.9173e-02, -6.7192e-02,  2.7494e-02,\n",
      "         2.4167e-03,  3.7371e-02,  4.2284e-02,  3.3118e-02,  5.1909e-02,\n",
      "        -6.6921e-02, -5.8869e-02, -6.1932e-02,  3.1455e-02, -2.2885e-02,\n",
      "        -9.3647e-02, -1.9637e-02,  5.1098e-02,  4.5610e-02, -4.1068e-02,\n",
      "         5.7816e-02, -8.5963e-04,  2.2186e-02, -1.8173e-02,  4.3025e-02,\n",
      "        -3.6500e-02,  4.6611e-02,  1.1417e-01, -6.0109e-02, -6.6532e-02,\n",
      "         9.2543e-02,  1.5739e-02, -7.0260e-03, -4.5298e-02, -4.6085e-02,\n",
      "        -1.7641e-02, -3.4245e-02, -2.9982e-02, -3.3564e-02, -2.3251e-02,\n",
      "        -9.0132e-02, -4.9113e-02, -1.5003e-02, -3.4544e-02, -1.2240e-02,\n",
      "        -6.6013e-02, -1.2225e-01,  2.1974e-02, -7.2869e-02,  7.3213e-02,\n",
      "         7.8171e-02, -1.1407e-02,  1.2900e-02,  1.3423e-02,  6.1885e-02,\n",
      "         8.2777e-02,  5.9639e-03, -2.9608e-02,  1.4335e-02, -3.0911e-02,\n",
      "        -2.6568e-02, -7.7970e-02,  5.7262e-02,  7.5148e-03, -8.3736e-02,\n",
      "         1.1164e-01, -3.6595e-02, -3.5647e-02, -2.2155e-02,  3.7071e-02,\n",
      "         5.2191e-03, -5.0187e-02, -1.0465e-02, -2.7389e-02,  2.4710e-02,\n",
      "         3.4442e-02, -3.3596e-02,  5.7857e-02,  4.2296e-02, -2.8121e-02,\n",
      "         3.7366e-02, -5.9914e-02,  1.6653e-02,  3.8050e-02,  5.3976e-02,\n",
      "         1.6561e-02,  5.0949e-02,  7.7352e-02,  8.3561e-02, -4.3670e-02,\n",
      "        -8.8957e-03,  2.0743e-03,  3.0768e-02, -3.4656e-02,  1.0132e-01,\n",
      "         2.0802e-02, -1.4734e-01, -1.1625e-02, -4.6762e-03,  1.0868e-01,\n",
      "         8.2071e-02, -1.4927e-02, -1.5449e-01, -7.1360e-02,  6.3504e-02,\n",
      "        -1.3678e-02, -3.2650e-02,  8.5200e-02, -4.5086e-02,  2.2611e-02,\n",
      "        -1.0392e-01, -6.0944e-02,  1.4738e-02,  3.9227e-02, -8.7592e-03,\n",
      "        -2.2234e-02, -5.5263e-03, -3.3027e-02,  3.9625e-03,  1.5417e-02,\n",
      "         1.2909e-02,  1.0592e-01, -5.5637e-02,  1.6255e-01, -8.2178e-02,\n",
      "         9.2043e-02,  1.9381e-03,  2.2714e-02,  3.5822e-02, -1.0901e-03,\n",
      "         1.2325e-02, -6.4859e-02, -2.5885e-02,  5.1314e-02, -4.6941e-04,\n",
      "        -2.8895e-03,  1.1293e-02, -1.7513e-02, -6.6949e-02,  6.9416e-02,\n",
      "         7.1142e-03, -1.4641e-03, -3.6779e-02,  1.1385e-01, -4.7641e-02,\n",
      "         1.4738e-02, -6.2718e-02,  8.7415e-02, -5.5629e-03,  2.7129e-02,\n",
      "         6.3722e-03,  3.4799e-02,  2.5760e-02, -7.6286e-02, -6.1321e-02,\n",
      "        -5.3081e-02, -1.3048e-03, -1.7442e-02, -1.6667e-01,  3.7299e-03,\n",
      "         1.3328e-02,  6.2362e-02,  1.6265e-02,  5.9280e-02, -9.6899e-02,\n",
      "        -9.9530e-03,  3.4732e-02,  5.4185e-03, -4.3835e-03,  3.4801e-02,\n",
      "         4.0341e-02, -1.1303e-02, -2.8805e-02,  2.6510e-02, -4.8988e-02,\n",
      "        -1.4906e-02, -8.7503e-02, -3.8591e-03,  3.9093e-02,  2.1345e-02,\n",
      "         4.3803e-02, -4.8825e-02, -3.8691e-02, -7.1864e-02, -5.9994e-02,\n",
      "         2.5898e-02, -4.4769e-02,  8.8324e-02, -7.2772e-02,  1.5155e-02,\n",
      "        -5.5817e-02,  5.3736e-02, -2.9101e-02,  1.5793e-03, -1.7930e-01,\n",
      "        -1.7445e-02, -6.8678e-02, -2.1378e-02, -4.4950e-02, -1.7106e-02,\n",
      "         1.5411e-01,  7.0336e-02,  3.1394e-02,  9.1400e-02, -6.3379e-02,\n",
      "         9.3097e-02,  6.2873e-02, -2.3895e-02,  4.8823e-02,  1.5050e-02,\n",
      "         1.5749e-01,  2.0483e-02,  2.5478e-02,  1.2565e-01,  6.4963e-02,\n",
      "        -3.3720e-02,  3.8453e-02, -6.7775e-02, -1.1753e-01,  6.8093e-02,\n",
      "         5.1249e-02, -1.5064e-01, -6.5369e-02,  4.8224e-02, -8.1458e-03,\n",
      "        -2.7762e-02, -2.5249e-02, -1.0149e-02, -1.9384e-02,  4.1005e-02,\n",
      "        -2.7609e-02, -9.2976e-02,  3.8276e-02,  7.2089e-02, -1.2936e-01,\n",
      "        -1.1778e-01, -6.5505e-02,  1.7166e-02,  1.5751e-02, -1.9162e-02,\n",
      "         5.5185e-03, -1.0558e-01, -2.3025e-02, -1.4394e-01,  1.1885e-01,\n",
      "         9.7875e-03, -9.7859e-02, -3.9622e-02, -4.5969e-02, -4.3369e-02,\n",
      "        -2.5617e-02, -5.2712e-02,  3.9468e-02,  1.0800e-01,  5.3185e-02,\n",
      "        -5.0451e-02,  5.3125e-02,  1.4214e-01,  1.0340e-01, -1.7702e-02,\n",
      "        -6.3901e-02,  3.0720e-02, -7.3908e-02,  9.5226e-02,  6.2002e-03,\n",
      "         5.0914e-02, -6.5561e-02,  5.4568e-02,  5.1027e-02, -4.2785e-02,\n",
      "        -7.9318e-02,  6.1157e-02,  6.2453e-02, -4.5603e-02, -2.7345e-02,\n",
      "        -5.6974e-02,  1.2981e-01,  1.0213e-01,  4.7302e-02, -2.4651e-02,\n",
      "        -3.3669e-02, -4.9926e-02,  7.3012e-02, -3.4709e-02,  1.2907e-01,\n",
      "         6.1702e-02,  3.1375e-02,  1.9113e-02, -9.1100e-02,  7.3931e-03,\n",
      "        -8.0293e-02, -3.6101e-02,  5.1210e-02, -2.9621e-02,  5.9973e-03,\n",
      "         9.6392e-02,  4.1492e-03,  2.3054e-02, -8.5028e-02,  1.3075e-03,\n",
      "        -8.0786e-02,  7.1889e-02, -3.7784e-02,  2.0823e-02, -5.1179e-02,\n",
      "         1.3547e-01,  4.0677e-02,  6.0206e-02, -4.4987e-03, -1.4705e-02,\n",
      "        -4.3924e-05,  2.2686e-02, -3.4385e-02, -3.4656e-02, -1.7687e-01,\n",
      "         4.2150e-02, -3.2622e-03, -4.4221e-02, -5.9327e-02, -1.2178e-01,\n",
      "        -9.8243e-02,  2.9285e-02,  1.0800e-01, -2.9136e-02, -1.2633e-02,\n",
      "        -8.9605e-02,  1.0191e-02,  2.9528e-02, -1.6184e-02, -2.1323e-02,\n",
      "         4.3191e-02, -5.9493e-02, -9.2964e-02, -2.2478e-02,  1.4769e-02,\n",
      "        -2.1768e-02,  4.5379e-02,  7.2459e-02,  6.8969e-02, -3.1864e-02,\n",
      "         9.2427e-03,  1.1675e-01, -1.2651e-02, -8.6167e-02, -7.0927e-02,\n",
      "        -3.2216e-02, -3.6091e-02,  1.1292e-02, -2.3667e-02,  1.0530e-01,\n",
      "        -2.7349e-02,  3.5006e-02, -8.4804e-02,  5.3443e-02, -3.9848e-02,\n",
      "        -1.8628e-02, -9.9607e-02, -1.0862e-01,  3.0266e-03,  6.8604e-02,\n",
      "        -2.6277e-02, -1.4869e-01, -4.6595e-02, -9.2243e-02,  5.9800e-02,\n",
      "         1.6846e-02, -5.4605e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0050000'], tr/val_loss: 32.200645/939.081238, val:  37.50%, val_best:  37.50%, tr:  34.93%, tr_best:  34.93%, epoch time: 100.15 seconds, 1.67 minutes\n",
      "[module.layers.3] weight_fb parameter count: 5,120\n",
      "epoch-1   lr=['0.0050000'], tr/val_loss: 29.736021/1034.413452, val:  43.75%, val_best:  43.75%, tr:  48.62%, tr_best:  48.62%, epoch time: 99.23 seconds, 1.65 minutes\n",
      "epoch-2   lr=['0.0050000'], tr/val_loss: 29.086630/1140.322021, val:  44.58%, val_best:  44.58%, tr:  49.85%, tr_best:  49.85%, epoch time: 96.18 seconds, 1.60 minutes\n",
      "epoch-3   lr=['0.0050000'], tr/val_loss: 28.456009/1224.989380, val:  44.17%, val_best:  44.58%, tr:  53.63%, tr_best:  53.63%, epoch time: 95.65 seconds, 1.59 minutes\n",
      "epoch-4   lr=['0.0050000'], tr/val_loss: 27.080132/569.748840, val:  55.42%, val_best:  55.42%, tr:  55.98%, tr_best:  55.98%, epoch time: 97.12 seconds, 1.62 minutes\n",
      "epoch-5   lr=['0.0050000'], tr/val_loss: 24.483168/702.311951, val:  60.83%, val_best:  60.83%, tr:  59.75%, tr_best:  59.75%, epoch time: 95.98 seconds, 1.60 minutes\n",
      "epoch-6   lr=['0.0050000'], tr/val_loss: 24.412962/1208.904541, val:  52.50%, val_best:  60.83%, tr:  60.37%, tr_best:  60.37%, epoch time: 95.42 seconds, 1.59 minutes\n",
      "epoch-7   lr=['0.0050000'], tr/val_loss: 22.313862/856.938232, val:  56.67%, val_best:  60.83%, tr:  64.35%, tr_best:  64.35%, epoch time: 94.59 seconds, 1.58 minutes\n",
      "epoch-8   lr=['0.0050000'], tr/val_loss: 23.269821/538.144165, val:  62.92%, val_best:  62.92%, tr:  61.90%, tr_best:  64.35%, epoch time: 96.05 seconds, 1.60 minutes\n",
      "epoch-9   lr=['0.0050000'], tr/val_loss: 21.407503/940.650391, val:  53.33%, val_best:  62.92%, tr:  65.27%, tr_best:  65.27%, epoch time: 97.12 seconds, 1.62 minutes\n",
      "epoch-10  lr=['0.0050000'], tr/val_loss: 19.622105/673.510559, val:  58.33%, val_best:  62.92%, tr:  67.52%, tr_best:  67.52%, epoch time: 96.48 seconds, 1.61 minutes\n",
      "epoch-11  lr=['0.0050000'], tr/val_loss: 21.616524/741.079224, val:  60.42%, val_best:  62.92%, tr:  65.68%, tr_best:  67.52%, epoch time: 92.44 seconds, 1.54 minutes\n",
      "epoch-12  lr=['0.0050000'], tr/val_loss: 19.449638/755.165955, val:  58.33%, val_best:  62.92%, tr:  68.85%, tr_best:  68.85%, epoch time: 94.60 seconds, 1.58 minutes\n",
      "epoch-13  lr=['0.0050000'], tr/val_loss: 19.985003/1111.609253, val:  51.67%, val_best:  62.92%, tr:  66.50%, tr_best:  68.85%, epoch time: 94.29 seconds, 1.57 minutes\n",
      "epoch-14  lr=['0.0050000'], tr/val_loss: 17.968346/1222.235596, val:  56.25%, val_best:  62.92%, tr:  69.36%, tr_best:  69.36%, epoch time: 96.43 seconds, 1.61 minutes\n",
      "epoch-15  lr=['0.0050000'], tr/val_loss: 18.641645/603.408203, val:  60.83%, val_best:  62.92%, tr:  69.97%, tr_best:  69.97%, epoch time: 93.21 seconds, 1.55 minutes\n",
      "epoch-16  lr=['0.0050000'], tr/val_loss: 18.005095/1058.220825, val:  55.00%, val_best:  62.92%, tr:  69.36%, tr_best:  69.97%, epoch time: 95.92 seconds, 1.60 minutes\n",
      "epoch-17  lr=['0.0050000'], tr/val_loss: 17.173660/703.935120, val:  61.25%, val_best:  62.92%, tr:  72.42%, tr_best:  72.42%, epoch time: 94.70 seconds, 1.58 minutes\n",
      "epoch-18  lr=['0.0050000'], tr/val_loss: 18.922791/926.472961, val:  56.67%, val_best:  62.92%, tr:  68.54%, tr_best:  72.42%, epoch time: 94.21 seconds, 1.57 minutes\n",
      "epoch-19  lr=['0.0050000'], tr/val_loss: 16.643999/980.868347, val:  59.58%, val_best:  62.92%, tr:  72.52%, tr_best:  72.52%, epoch time: 92.63 seconds, 1.54 minutes\n",
      "epoch-20  lr=['0.0050000'], tr/val_loss: 16.256109/1328.743652, val:  44.17%, val_best:  62.92%, tr:  71.81%, tr_best:  72.52%, epoch time: 94.88 seconds, 1.58 minutes\n",
      "epoch-21  lr=['0.0050000'], tr/val_loss: 15.852273/570.220703, val:  68.33%, val_best:  68.33%, tr:  71.20%, tr_best:  72.52%, epoch time: 95.47 seconds, 1.59 minutes\n",
      "epoch-22  lr=['0.0050000'], tr/val_loss: 16.338331/996.748169, val:  60.83%, val_best:  68.33%, tr:  72.93%, tr_best:  72.93%, epoch time: 95.77 seconds, 1.60 minutes\n",
      "epoch-23  lr=['0.0050000'], tr/val_loss: 17.564928/493.744843, val:  67.50%, val_best:  68.33%, tr:  72.42%, tr_best:  72.93%, epoch time: 95.19 seconds, 1.59 minutes\n",
      "epoch-24  lr=['0.0050000'], tr/val_loss: 14.362679/930.680115, val:  61.67%, val_best:  68.33%, tr:  72.01%, tr_best:  72.93%, epoch time: 94.87 seconds, 1.58 minutes\n",
      "epoch-25  lr=['0.0050000'], tr/val_loss: 16.364542/646.733215, val:  60.83%, val_best:  68.33%, tr:  72.32%, tr_best:  72.93%, epoch time: 96.50 seconds, 1.61 minutes\n",
      "epoch-26  lr=['0.0050000'], tr/val_loss: 14.705619/978.693542, val:  57.50%, val_best:  68.33%, tr:  74.87%, tr_best:  74.87%, epoch time: 95.33 seconds, 1.59 minutes\n",
      "epoch-27  lr=['0.0050000'], tr/val_loss: 14.071274/891.876038, val:  59.17%, val_best:  68.33%, tr:  74.87%, tr_best:  74.87%, epoch time: 95.34 seconds, 1.59 minutes\n",
      "epoch-28  lr=['0.0050000'], tr/val_loss: 13.516249/723.146057, val:  64.58%, val_best:  68.33%, tr:  75.08%, tr_best:  75.08%, epoch time: 98.02 seconds, 1.63 minutes\n",
      "epoch-29  lr=['0.0050000'], tr/val_loss: 14.098601/718.287964, val:  67.92%, val_best:  68.33%, tr:  76.00%, tr_best:  76.00%, epoch time: 95.47 seconds, 1.59 minutes\n",
      "epoch-30  lr=['0.0050000'], tr/val_loss: 12.710099/707.184387, val:  64.17%, val_best:  68.33%, tr:  74.57%, tr_best:  76.00%, epoch time: 97.31 seconds, 1.62 minutes\n",
      "epoch-31  lr=['0.0050000'], tr/val_loss: 12.476982/861.654114, val:  67.50%, val_best:  68.33%, tr:  76.61%, tr_best:  76.61%, epoch time: 95.33 seconds, 1.59 minutes\n",
      "epoch-32  lr=['0.0050000'], tr/val_loss: 13.277128/607.948669, val:  67.92%, val_best:  68.33%, tr:  76.20%, tr_best:  76.61%, epoch time: 93.91 seconds, 1.57 minutes\n",
      "epoch-33  lr=['0.0050000'], tr/val_loss: 13.834126/675.389893, val:  66.67%, val_best:  68.33%, tr:  75.18%, tr_best:  76.61%, epoch time: 94.50 seconds, 1.58 minutes\n",
      "epoch-34  lr=['0.0050000'], tr/val_loss: 12.134634/1227.750366, val:  56.67%, val_best:  68.33%, tr:  76.20%, tr_best:  76.61%, epoch time: 97.92 seconds, 1.63 minutes\n",
      "epoch-35  lr=['0.0050000'], tr/val_loss: 13.661773/689.828979, val:  63.33%, val_best:  68.33%, tr:  77.22%, tr_best:  77.22%, epoch time: 96.57 seconds, 1.61 minutes\n",
      "epoch-36  lr=['0.0050000'], tr/val_loss: 12.168870/704.054871, val:  58.75%, val_best:  68.33%, tr:  76.61%, tr_best:  77.22%, epoch time: 96.18 seconds, 1.60 minutes\n",
      "epoch-37  lr=['0.0050000'], tr/val_loss:  9.938197/680.181946, val:  68.33%, val_best:  68.33%, tr:  80.08%, tr_best:  80.08%, epoch time: 94.10 seconds, 1.57 minutes\n",
      "epoch-38  lr=['0.0050000'], tr/val_loss: 12.717237/605.530762, val:  62.08%, val_best:  68.33%, tr:  77.43%, tr_best:  80.08%, epoch time: 93.41 seconds, 1.56 minutes\n",
      "epoch-39  lr=['0.0050000'], tr/val_loss: 11.057559/638.274292, val:  66.67%, val_best:  68.33%, tr:  80.18%, tr_best:  80.18%, epoch time: 93.64 seconds, 1.56 minutes\n",
      "epoch-40  lr=['0.0050000'], tr/val_loss: 13.509216/1069.699341, val:  67.08%, val_best:  68.33%, tr:  77.83%, tr_best:  80.18%, epoch time: 96.01 seconds, 1.60 minutes\n",
      "epoch-41  lr=['0.0050000'], tr/val_loss: 13.090283/813.234436, val:  66.67%, val_best:  68.33%, tr:  76.30%, tr_best:  80.18%, epoch time: 95.11 seconds, 1.59 minutes\n",
      "epoch-42  lr=['0.0050000'], tr/val_loss: 12.992921/590.404663, val:  66.67%, val_best:  68.33%, tr:  77.32%, tr_best:  80.18%, epoch time: 96.27 seconds, 1.60 minutes\n",
      "epoch-43  lr=['0.0050000'], tr/val_loss: 10.413960/1026.011475, val:  62.92%, val_best:  68.33%, tr:  79.78%, tr_best:  80.18%, epoch time: 95.01 seconds, 1.58 minutes\n",
      "epoch-44  lr=['0.0050000'], tr/val_loss: 11.483119/484.277466, val:  72.50%, val_best:  72.50%, tr:  78.96%, tr_best:  80.18%, epoch time: 94.22 seconds, 1.57 minutes\n",
      "epoch-45  lr=['0.0050000'], tr/val_loss: 11.664019/498.668365, val:  68.33%, val_best:  72.50%, tr:  79.37%, tr_best:  80.18%, epoch time: 94.62 seconds, 1.58 minutes\n",
      "epoch-46  lr=['0.0050000'], tr/val_loss: 10.841739/730.195801, val:  62.92%, val_best:  72.50%, tr:  79.88%, tr_best:  80.18%, epoch time: 94.20 seconds, 1.57 minutes\n",
      "epoch-47  lr=['0.0050000'], tr/val_loss: 11.541752/741.308838, val:  70.42%, val_best:  72.50%, tr:  80.49%, tr_best:  80.49%, epoch time: 95.22 seconds, 1.59 minutes\n",
      "epoch-48  lr=['0.0050000'], tr/val_loss:  9.885758/793.422424, val:  64.58%, val_best:  72.50%, tr:  80.90%, tr_best:  80.90%, epoch time: 93.86 seconds, 1.56 minutes\n",
      "epoch-49  lr=['0.0050000'], tr/val_loss: 10.319137/726.020447, val:  64.17%, val_best:  72.50%, tr:  79.47%, tr_best:  80.90%, epoch time: 95.58 seconds, 1.59 minutes\n",
      "epoch-50  lr=['0.0050000'], tr/val_loss: 11.424056/1076.377563, val:  63.75%, val_best:  72.50%, tr:  78.65%, tr_best:  80.90%, epoch time: 93.20 seconds, 1.55 minutes\n",
      "epoch-51  lr=['0.0050000'], tr/val_loss: 10.948692/806.533752, val:  60.83%, val_best:  72.50%, tr:  80.80%, tr_best:  80.90%, epoch time: 93.81 seconds, 1.56 minutes\n",
      "epoch-52  lr=['0.0050000'], tr/val_loss: 10.466483/1046.117065, val:  59.58%, val_best:  72.50%, tr:  78.65%, tr_best:  80.90%, epoch time: 92.36 seconds, 1.54 minutes\n",
      "epoch-53  lr=['0.0050000'], tr/val_loss: 10.743808/867.777893, val:  60.83%, val_best:  72.50%, tr:  80.90%, tr_best:  80.90%, epoch time: 97.13 seconds, 1.62 minutes\n",
      "epoch-54  lr=['0.0050000'], tr/val_loss: 10.647368/580.465698, val:  70.42%, val_best:  72.50%, tr:  81.21%, tr_best:  81.21%, epoch time: 96.02 seconds, 1.60 minutes\n",
      "epoch-55  lr=['0.0050000'], tr/val_loss: 10.754914/602.817993, val:  67.92%, val_best:  72.50%, tr:  79.37%, tr_best:  81.21%, epoch time: 96.09 seconds, 1.60 minutes\n",
      "epoch-56  lr=['0.0050000'], tr/val_loss: 11.447998/906.493408, val:  64.17%, val_best:  72.50%, tr:  79.16%, tr_best:  81.21%, epoch time: 94.76 seconds, 1.58 minutes\n",
      "epoch-57  lr=['0.0050000'], tr/val_loss:  9.347643/679.438782, val:  67.92%, val_best:  72.50%, tr:  82.53%, tr_best:  82.53%, epoch time: 92.47 seconds, 1.54 minutes\n",
      "epoch-58  lr=['0.0050000'], tr/val_loss:  9.105612/546.595764, val:  68.33%, val_best:  72.50%, tr:  82.02%, tr_best:  82.53%, epoch time: 93.71 seconds, 1.56 minutes\n",
      "epoch-59  lr=['0.0050000'], tr/val_loss:  9.378350/855.195923, val:  64.17%, val_best:  72.50%, tr:  81.72%, tr_best:  82.53%, epoch time: 94.73 seconds, 1.58 minutes\n",
      "epoch-60  lr=['0.0050000'], tr/val_loss:  8.877699/783.289185, val:  64.58%, val_best:  72.50%, tr:  83.76%, tr_best:  83.76%, epoch time: 94.89 seconds, 1.58 minutes\n",
      "epoch-61  lr=['0.0050000'], tr/val_loss:  9.625852/850.868469, val:  61.25%, val_best:  72.50%, tr:  81.61%, tr_best:  83.76%, epoch time: 96.50 seconds, 1.61 minutes\n",
      "epoch-62  lr=['0.0050000'], tr/val_loss:  9.332896/755.498474, val:  65.42%, val_best:  72.50%, tr:  82.02%, tr_best:  83.76%, epoch time: 96.61 seconds, 1.61 minutes\n",
      "epoch-63  lr=['0.0050000'], tr/val_loss:  8.576845/664.708862, val:  68.33%, val_best:  72.50%, tr:  83.66%, tr_best:  83.76%, epoch time: 94.97 seconds, 1.58 minutes\n",
      "epoch-64  lr=['0.0050000'], tr/val_loss:  9.909633/641.165649, val:  70.83%, val_best:  72.50%, tr:  81.21%, tr_best:  83.76%, epoch time: 94.69 seconds, 1.58 minutes\n",
      "epoch-65  lr=['0.0050000'], tr/val_loss:  8.519831/544.842468, val:  71.25%, val_best:  72.50%, tr:  82.53%, tr_best:  83.76%, epoch time: 91.80 seconds, 1.53 minutes\n",
      "epoch-66  lr=['0.0050000'], tr/val_loss:  9.347444/551.108032, val:  67.92%, val_best:  72.50%, tr:  79.98%, tr_best:  83.76%, epoch time: 95.25 seconds, 1.59 minutes\n",
      "epoch-67  lr=['0.0050000'], tr/val_loss:  9.557556/705.311096, val:  64.58%, val_best:  72.50%, tr:  82.84%, tr_best:  83.76%, epoch time: 96.37 seconds, 1.61 minutes\n",
      "epoch-68  lr=['0.0050000'], tr/val_loss:  8.428926/807.744629, val:  67.92%, val_best:  72.50%, tr:  82.74%, tr_best:  83.76%, epoch time: 94.71 seconds, 1.58 minutes\n",
      "epoch-69  lr=['0.0050000'], tr/val_loss:  8.550207/954.043579, val:  59.17%, val_best:  72.50%, tr:  82.74%, tr_best:  83.76%, epoch time: 92.33 seconds, 1.54 minutes\n",
      "epoch-70  lr=['0.0050000'], tr/val_loss:  8.730215/630.180115, val:  67.50%, val_best:  72.50%, tr:  82.12%, tr_best:  83.76%, epoch time: 94.38 seconds, 1.57 minutes\n",
      "epoch-71  lr=['0.0050000'], tr/val_loss:  8.199492/840.997559, val:  64.58%, val_best:  72.50%, tr:  83.15%, tr_best:  83.76%, epoch time: 92.52 seconds, 1.54 minutes\n",
      "epoch-72  lr=['0.0050000'], tr/val_loss:  7.794683/724.615112, val:  68.75%, val_best:  72.50%, tr:  84.47%, tr_best:  84.47%, epoch time: 94.48 seconds, 1.57 minutes\n",
      "epoch-73  lr=['0.0050000'], tr/val_loss:  7.718278/1039.786743, val:  60.00%, val_best:  72.50%, tr:  85.09%, tr_best:  85.09%, epoch time: 96.31 seconds, 1.61 minutes\n",
      "epoch-74  lr=['0.0050000'], tr/val_loss:  8.884095/622.426025, val:  70.42%, val_best:  72.50%, tr:  82.84%, tr_best:  85.09%, epoch time: 95.23 seconds, 1.59 minutes\n",
      "epoch-75  lr=['0.0050000'], tr/val_loss:  8.642353/707.644897, val:  65.83%, val_best:  72.50%, tr:  84.88%, tr_best:  85.09%, epoch time: 94.64 seconds, 1.58 minutes\n",
      "epoch-76  lr=['0.0050000'], tr/val_loss:  9.713762/930.958435, val:  63.75%, val_best:  72.50%, tr:  81.31%, tr_best:  85.09%, epoch time: 94.12 seconds, 1.57 minutes\n",
      "epoch-77  lr=['0.0050000'], tr/val_loss:  8.965604/1052.506836, val:  60.83%, val_best:  72.50%, tr:  82.94%, tr_best:  85.09%, epoch time: 92.10 seconds, 1.54 minutes\n",
      "epoch-78  lr=['0.0050000'], tr/val_loss:  8.352291/1057.616211, val:  58.75%, val_best:  72.50%, tr:  83.66%, tr_best:  85.09%, epoch time: 93.72 seconds, 1.56 minutes\n",
      "epoch-79  lr=['0.0050000'], tr/val_loss:  9.291589/862.386658, val:  62.50%, val_best:  72.50%, tr:  82.84%, tr_best:  85.09%, epoch time: 93.75 seconds, 1.56 minutes\n",
      "epoch-80  lr=['0.0050000'], tr/val_loss:  8.383682/1076.433594, val:  62.50%, val_best:  72.50%, tr:  83.55%, tr_best:  85.09%, epoch time: 94.81 seconds, 1.58 minutes\n",
      "epoch-81  lr=['0.0050000'], tr/val_loss:  7.137745/933.428955, val:  61.67%, val_best:  72.50%, tr:  85.09%, tr_best:  85.09%, epoch time: 94.41 seconds, 1.57 minutes\n",
      "epoch-82  lr=['0.0050000'], tr/val_loss:  7.679748/773.431396, val:  65.00%, val_best:  72.50%, tr:  83.55%, tr_best:  85.09%, epoch time: 94.13 seconds, 1.57 minutes\n",
      "epoch-83  lr=['0.0050000'], tr/val_loss:  7.666350/805.533203, val:  60.42%, val_best:  72.50%, tr:  83.96%, tr_best:  85.09%, epoch time: 93.63 seconds, 1.56 minutes\n",
      "epoch-84  lr=['0.0050000'], tr/val_loss:  7.744278/751.530334, val:  62.92%, val_best:  72.50%, tr:  83.86%, tr_best:  85.09%, epoch time: 93.36 seconds, 1.56 minutes\n",
      "epoch-85  lr=['0.0050000'], tr/val_loss:  7.645153/527.729553, val:  71.67%, val_best:  72.50%, tr:  84.68%, tr_best:  85.09%, epoch time: 95.09 seconds, 1.58 minutes\n",
      "epoch-86  lr=['0.0050000'], tr/val_loss:  8.584991/586.587769, val:  70.83%, val_best:  72.50%, tr:  84.27%, tr_best:  85.09%, epoch time: 97.13 seconds, 1.62 minutes\n",
      "epoch-87  lr=['0.0050000'], tr/val_loss:  7.736405/560.516052, val:  70.00%, val_best:  72.50%, tr:  83.86%, tr_best:  85.09%, epoch time: 96.11 seconds, 1.60 minutes\n",
      "epoch-88  lr=['0.0050000'], tr/val_loss:  8.745651/779.924866, val:  65.00%, val_best:  72.50%, tr:  82.64%, tr_best:  85.09%, epoch time: 95.17 seconds, 1.59 minutes\n",
      "epoch-89  lr=['0.0050000'], tr/val_loss:  8.080810/824.678894, val:  65.42%, val_best:  72.50%, tr:  83.25%, tr_best:  85.09%, epoch time: 95.61 seconds, 1.59 minutes\n",
      "epoch-90  lr=['0.0050000'], tr/val_loss:  7.428845/777.207214, val:  64.58%, val_best:  72.50%, tr:  84.37%, tr_best:  85.09%, epoch time: 95.16 seconds, 1.59 minutes\n",
      "epoch-91  lr=['0.0050000'], tr/val_loss:  7.919383/594.619141, val:  69.17%, val_best:  72.50%, tr:  84.68%, tr_best:  85.09%, epoch time: 95.13 seconds, 1.59 minutes\n",
      "epoch-92  lr=['0.0050000'], tr/val_loss:  7.846384/689.182922, val:  69.58%, val_best:  72.50%, tr:  84.88%, tr_best:  85.09%, epoch time: 95.96 seconds, 1.60 minutes\n",
      "epoch-93  lr=['0.0050000'], tr/val_loss:  7.710916/503.959656, val:  76.67%, val_best:  76.67%, tr:  84.07%, tr_best:  85.09%, epoch time: 98.02 seconds, 1.63 minutes\n",
      "epoch-94  lr=['0.0050000'], tr/val_loss:  6.721581/818.249268, val:  65.00%, val_best:  76.67%, tr:  86.41%, tr_best:  86.41%, epoch time: 94.50 seconds, 1.58 minutes\n",
      "epoch-95  lr=['0.0050000'], tr/val_loss:  6.995628/797.338074, val:  71.67%, val_best:  76.67%, tr:  83.45%, tr_best:  86.41%, epoch time: 93.42 seconds, 1.56 minutes\n",
      "epoch-96  lr=['0.0050000'], tr/val_loss:  9.224736/558.230347, val:  71.25%, val_best:  76.67%, tr:  82.84%, tr_best:  86.41%, epoch time: 95.02 seconds, 1.58 minutes\n",
      "epoch-97  lr=['0.0050000'], tr/val_loss:  6.950474/758.861206, val:  71.67%, val_best:  76.67%, tr:  86.31%, tr_best:  86.41%, epoch time: 95.92 seconds, 1.60 minutes\n",
      "epoch-98  lr=['0.0050000'], tr/val_loss:  6.759806/1039.788818, val:  66.25%, val_best:  76.67%, tr:  85.50%, tr_best:  86.41%, epoch time: 93.25 seconds, 1.55 minutes\n",
      "epoch-99  lr=['0.0050000'], tr/val_loss:  6.840725/835.844299, val:  65.42%, val_best:  76.67%, tr:  85.80%, tr_best:  86.41%, epoch time: 96.59 seconds, 1.61 minutes\n",
      "epoch-100 lr=['0.0050000'], tr/val_loss:  7.217239/718.569824, val:  67.50%, val_best:  76.67%, tr:  85.60%, tr_best:  86.41%, epoch time: 100.42 seconds, 1.67 minutes\n",
      "epoch-101 lr=['0.0050000'], tr/val_loss:  7.222617/759.072327, val:  69.17%, val_best:  76.67%, tr:  86.72%, tr_best:  86.72%, epoch time: 95.49 seconds, 1.59 minutes\n",
      "epoch-102 lr=['0.0050000'], tr/val_loss:  7.416660/770.278015, val:  66.25%, val_best:  76.67%, tr:  85.39%, tr_best:  86.72%, epoch time: 94.34 seconds, 1.57 minutes\n",
      "epoch-103 lr=['0.0050000'], tr/val_loss:  7.433340/634.353516, val:  72.08%, val_best:  76.67%, tr:  85.19%, tr_best:  86.72%, epoch time: 94.01 seconds, 1.57 minutes\n",
      "epoch-104 lr=['0.0050000'], tr/val_loss:  5.871538/990.561523, val:  65.00%, val_best:  76.67%, tr:  87.54%, tr_best:  87.54%, epoch time: 93.37 seconds, 1.56 minutes\n",
      "epoch-105 lr=['0.0050000'], tr/val_loss:  7.679503/571.277222, val:  75.00%, val_best:  76.67%, tr:  85.80%, tr_best:  87.54%, epoch time: 95.75 seconds, 1.60 minutes\n",
      "epoch-106 lr=['0.0050000'], tr/val_loss:  7.162719/780.475220, val:  62.50%, val_best:  76.67%, tr:  86.62%, tr_best:  87.54%, epoch time: 95.41 seconds, 1.59 minutes\n",
      "epoch-107 lr=['0.0050000'], tr/val_loss:  6.721024/645.961426, val:  69.58%, val_best:  76.67%, tr:  85.60%, tr_best:  87.54%, epoch time: 95.06 seconds, 1.58 minutes\n",
      "epoch-108 lr=['0.0050000'], tr/val_loss:  6.308017/661.128418, val:  72.08%, val_best:  76.67%, tr:  87.03%, tr_best:  87.54%, epoch time: 95.03 seconds, 1.58 minutes\n",
      "epoch-109 lr=['0.0050000'], tr/val_loss:  6.646308/703.556213, val:  69.58%, val_best:  76.67%, tr:  86.93%, tr_best:  87.54%, epoch time: 94.69 seconds, 1.58 minutes\n",
      "epoch-110 lr=['0.0050000'], tr/val_loss:  6.838495/621.141846, val:  72.50%, val_best:  76.67%, tr:  84.98%, tr_best:  87.54%, epoch time: 94.58 seconds, 1.58 minutes\n",
      "epoch-111 lr=['0.0050000'], tr/val_loss:  7.440520/1139.852295, val:  67.50%, val_best:  76.67%, tr:  84.47%, tr_best:  87.54%, epoch time: 94.88 seconds, 1.58 minutes\n",
      "epoch-112 lr=['0.0050000'], tr/val_loss:  7.313872/810.695801, val:  66.67%, val_best:  76.67%, tr:  85.70%, tr_best:  87.54%, epoch time: 96.20 seconds, 1.60 minutes\n",
      "epoch-113 lr=['0.0050000'], tr/val_loss:  7.929482/1088.448486, val:  61.67%, val_best:  76.67%, tr:  85.19%, tr_best:  87.54%, epoch time: 95.46 seconds, 1.59 minutes\n",
      "epoch-114 lr=['0.0050000'], tr/val_loss:  6.885328/593.233521, val:  70.00%, val_best:  76.67%, tr:  86.72%, tr_best:  87.54%, epoch time: 96.33 seconds, 1.61 minutes\n",
      "epoch-115 lr=['0.0050000'], tr/val_loss:  6.566727/771.911316, val:  68.33%, val_best:  76.67%, tr:  87.74%, tr_best:  87.74%, epoch time: 92.71 seconds, 1.55 minutes\n",
      "epoch-116 lr=['0.0050000'], tr/val_loss:  6.068125/681.922546, val:  72.50%, val_best:  76.67%, tr:  86.62%, tr_best:  87.74%, epoch time: 93.60 seconds, 1.56 minutes\n",
      "epoch-117 lr=['0.0050000'], tr/val_loss:  6.975599/675.784546, val:  70.00%, val_best:  76.67%, tr:  85.39%, tr_best:  87.74%, epoch time: 94.52 seconds, 1.58 minutes\n",
      "epoch-118 lr=['0.0050000'], tr/val_loss:  5.597545/813.142212, val:  67.50%, val_best:  76.67%, tr:  88.15%, tr_best:  88.15%, epoch time: 95.05 seconds, 1.58 minutes\n",
      "epoch-119 lr=['0.0050000'], tr/val_loss:  6.741981/587.015747, val:  74.17%, val_best:  76.67%, tr:  85.39%, tr_best:  88.15%, epoch time: 96.75 seconds, 1.61 minutes\n",
      "epoch-120 lr=['0.0050000'], tr/val_loss:  5.369807/685.832031, val:  70.83%, val_best:  76.67%, tr:  87.23%, tr_best:  88.15%, epoch time: 94.24 seconds, 1.57 minutes\n",
      "epoch-121 lr=['0.0050000'], tr/val_loss:  6.367483/802.092651, val:  70.83%, val_best:  76.67%, tr:  86.52%, tr_best:  88.15%, epoch time: 94.20 seconds, 1.57 minutes\n",
      "epoch-122 lr=['0.0050000'], tr/val_loss:  7.068367/554.263672, val:  74.17%, val_best:  76.67%, tr:  85.39%, tr_best:  88.15%, epoch time: 95.07 seconds, 1.58 minutes\n",
      "epoch-123 lr=['0.0050000'], tr/val_loss:  6.764059/676.234863, val:  72.92%, val_best:  76.67%, tr:  86.41%, tr_best:  88.15%, epoch time: 93.58 seconds, 1.56 minutes\n",
      "epoch-124 lr=['0.0050000'], tr/val_loss:  5.348883/686.658264, val:  67.50%, val_best:  76.67%, tr:  87.23%, tr_best:  88.15%, epoch time: 93.95 seconds, 1.57 minutes\n",
      "epoch-125 lr=['0.0050000'], tr/val_loss:  5.210720/618.232910, val:  72.08%, val_best:  76.67%, tr:  88.15%, tr_best:  88.15%, epoch time: 95.21 seconds, 1.59 minutes\n",
      "epoch-126 lr=['0.0050000'], tr/val_loss:  6.244618/682.455322, val:  65.83%, val_best:  76.67%, tr:  86.52%, tr_best:  88.15%, epoch time: 95.97 seconds, 1.60 minutes\n",
      "epoch-127 lr=['0.0050000'], tr/val_loss:  6.446518/770.960999, val:  69.17%, val_best:  76.67%, tr:  88.05%, tr_best:  88.15%, epoch time: 95.99 seconds, 1.60 minutes\n",
      "epoch-128 lr=['0.0050000'], tr/val_loss:  6.234514/689.823303, val:  71.67%, val_best:  76.67%, tr:  86.82%, tr_best:  88.15%, epoch time: 95.29 seconds, 1.59 minutes\n",
      "epoch-129 lr=['0.0050000'], tr/val_loss:  5.874538/624.802368, val:  71.67%, val_best:  76.67%, tr:  87.54%, tr_best:  88.15%, epoch time: 93.47 seconds, 1.56 minutes\n",
      "epoch-130 lr=['0.0050000'], tr/val_loss:  6.276399/1137.327515, val:  65.42%, val_best:  76.67%, tr:  86.72%, tr_best:  88.15%, epoch time: 93.79 seconds, 1.56 minutes\n",
      "epoch-131 lr=['0.0050000'], tr/val_loss:  6.465603/837.008484, val:  67.50%, val_best:  76.67%, tr:  85.80%, tr_best:  88.15%, epoch time: 94.69 seconds, 1.58 minutes\n",
      "epoch-132 lr=['0.0050000'], tr/val_loss:  7.368882/689.372253, val:  71.67%, val_best:  76.67%, tr:  86.62%, tr_best:  88.15%, epoch time: 96.41 seconds, 1.61 minutes\n",
      "epoch-133 lr=['0.0050000'], tr/val_loss:  6.198889/577.745422, val:  75.83%, val_best:  76.67%, tr:  85.90%, tr_best:  88.15%, epoch time: 95.92 seconds, 1.60 minutes\n",
      "epoch-134 lr=['0.0050000'], tr/val_loss:  4.915522/746.706726, val:  68.33%, val_best:  76.67%, tr:  88.76%, tr_best:  88.76%, epoch time: 95.49 seconds, 1.59 minutes\n",
      "epoch-135 lr=['0.0050000'], tr/val_loss:  5.100879/746.627075, val:  67.50%, val_best:  76.67%, tr:  88.05%, tr_best:  88.76%, epoch time: 94.17 seconds, 1.57 minutes\n",
      "epoch-136 lr=['0.0050000'], tr/val_loss:  6.850376/752.674988, val:  66.67%, val_best:  76.67%, tr:  86.31%, tr_best:  88.76%, epoch time: 95.51 seconds, 1.59 minutes\n",
      "epoch-137 lr=['0.0050000'], tr/val_loss:  5.998547/644.090454, val:  72.50%, val_best:  76.67%, tr:  87.64%, tr_best:  88.76%, epoch time: 95.38 seconds, 1.59 minutes\n",
      "epoch-138 lr=['0.0050000'], tr/val_loss:  4.912835/680.798767, val:  70.42%, val_best:  76.67%, tr:  88.66%, tr_best:  88.76%, epoch time: 93.29 seconds, 1.55 minutes\n",
      "epoch-139 lr=['0.0050000'], tr/val_loss:  6.959559/653.042175, val:  69.17%, val_best:  76.67%, tr:  87.33%, tr_best:  88.76%, epoch time: 95.04 seconds, 1.58 minutes\n",
      "epoch-140 lr=['0.0050000'], tr/val_loss:  5.515973/575.582092, val:  74.58%, val_best:  76.67%, tr:  89.38%, tr_best:  89.38%, epoch time: 94.91 seconds, 1.58 minutes\n",
      "epoch-141 lr=['0.0050000'], tr/val_loss:  5.719130/950.387268, val:  65.42%, val_best:  76.67%, tr:  88.66%, tr_best:  89.38%, epoch time: 93.90 seconds, 1.57 minutes\n",
      "epoch-142 lr=['0.0050000'], tr/val_loss:  6.153230/675.514832, val:  74.17%, val_best:  76.67%, tr:  87.33%, tr_best:  89.38%, epoch time: 94.33 seconds, 1.57 minutes\n",
      "epoch-143 lr=['0.0050000'], tr/val_loss:  6.331652/664.595764, val:  70.42%, val_best:  76.67%, tr:  88.56%, tr_best:  89.38%, epoch time: 94.06 seconds, 1.57 minutes\n",
      "epoch-144 lr=['0.0050000'], tr/val_loss:  5.001657/795.098633, val:  66.67%, val_best:  76.67%, tr:  88.05%, tr_best:  89.38%, epoch time: 93.67 seconds, 1.56 minutes\n",
      "epoch-145 lr=['0.0050000'], tr/val_loss:  4.832594/853.605896, val:  69.58%, val_best:  76.67%, tr:  89.68%, tr_best:  89.68%, epoch time: 94.90 seconds, 1.58 minutes\n",
      "epoch-146 lr=['0.0050000'], tr/val_loss:  4.982769/582.750061, val:  73.75%, val_best:  76.67%, tr:  89.58%, tr_best:  89.68%, epoch time: 96.16 seconds, 1.60 minutes\n",
      "epoch-147 lr=['0.0050000'], tr/val_loss:  5.011391/707.429077, val:  70.83%, val_best:  76.67%, tr:  89.17%, tr_best:  89.68%, epoch time: 95.99 seconds, 1.60 minutes\n",
      "epoch-148 lr=['0.0050000'], tr/val_loss:  6.203764/724.230530, val:  71.67%, val_best:  76.67%, tr:  88.15%, tr_best:  89.68%, epoch time: 93.49 seconds, 1.56 minutes\n",
      "epoch-149 lr=['0.0050000'], tr/val_loss:  5.354697/623.004944, val:  73.75%, val_best:  76.67%, tr:  88.76%, tr_best:  89.68%, epoch time: 92.55 seconds, 1.54 minutes\n",
      "epoch-150 lr=['0.0050000'], tr/val_loss:  5.676147/699.458679, val:  67.92%, val_best:  76.67%, tr:  88.87%, tr_best:  89.68%, epoch time: 92.51 seconds, 1.54 minutes\n",
      "epoch-151 lr=['0.0050000'], tr/val_loss:  6.773579/1016.575317, val:  67.50%, val_best:  76.67%, tr:  85.39%, tr_best:  89.68%, epoch time: 95.20 seconds, 1.59 minutes\n",
      "epoch-152 lr=['0.0050000'], tr/val_loss:  5.376781/650.280396, val:  72.08%, val_best:  76.67%, tr:  89.07%, tr_best:  89.68%, epoch time: 94.74 seconds, 1.58 minutes\n",
      "epoch-153 lr=['0.0050000'], tr/val_loss:  5.976955/673.152649, val:  70.42%, val_best:  76.67%, tr:  87.74%, tr_best:  89.68%, epoch time: 93.65 seconds, 1.56 minutes\n",
      "epoch-154 lr=['0.0050000'], tr/val_loss:  5.291351/710.129028, val:  70.83%, val_best:  76.67%, tr:  89.17%, tr_best:  89.68%, epoch time: 94.84 seconds, 1.58 minutes\n",
      "epoch-155 lr=['0.0050000'], tr/val_loss:  5.896884/713.960876, val:  70.00%, val_best:  76.67%, tr:  87.74%, tr_best:  89.68%, epoch time: 95.43 seconds, 1.59 minutes\n",
      "epoch-156 lr=['0.0050000'], tr/val_loss:  5.891284/660.947510, val:  71.67%, val_best:  76.67%, tr:  88.15%, tr_best:  89.68%, epoch time: 95.45 seconds, 1.59 minutes\n",
      "epoch-157 lr=['0.0050000'], tr/val_loss:  6.159033/675.813660, val:  69.17%, val_best:  76.67%, tr:  87.13%, tr_best:  89.68%, epoch time: 94.08 seconds, 1.57 minutes\n",
      "epoch-158 lr=['0.0050000'], tr/val_loss:  5.073549/631.295105, val:  73.75%, val_best:  76.67%, tr:  88.56%, tr_best:  89.68%, epoch time: 96.33 seconds, 1.61 minutes\n",
      "epoch-159 lr=['0.0050000'], tr/val_loss:  5.158353/568.942078, val:  75.83%, val_best:  76.67%, tr:  87.84%, tr_best:  89.68%, epoch time: 97.00 seconds, 1.62 minutes\n",
      "epoch-160 lr=['0.0050000'], tr/val_loss:  5.815560/687.714294, val:  71.25%, val_best:  76.67%, tr:  89.79%, tr_best:  89.79%, epoch time: 96.78 seconds, 1.61 minutes\n",
      "epoch-161 lr=['0.0050000'], tr/val_loss:  5.295877/615.293518, val:  70.83%, val_best:  76.67%, tr:  87.54%, tr_best:  89.79%, epoch time: 96.27 seconds, 1.60 minutes\n",
      "epoch-162 lr=['0.0050000'], tr/val_loss:  5.564847/707.335327, val:  72.08%, val_best:  76.67%, tr:  88.66%, tr_best:  89.79%, epoch time: 93.81 seconds, 1.56 minutes\n",
      "epoch-163 lr=['0.0050000'], tr/val_loss:  5.446994/601.763672, val:  72.92%, val_best:  76.67%, tr:  88.05%, tr_best:  89.79%, epoch time: 93.79 seconds, 1.56 minutes\n",
      "epoch-164 lr=['0.0050000'], tr/val_loss:  5.644383/725.532593, val:  68.33%, val_best:  76.67%, tr:  88.15%, tr_best:  89.79%, epoch time: 94.86 seconds, 1.58 minutes\n",
      "epoch-165 lr=['0.0050000'], tr/val_loss:  5.881214/791.210632, val:  70.00%, val_best:  76.67%, tr:  87.95%, tr_best:  89.79%, epoch time: 93.90 seconds, 1.57 minutes\n",
      "epoch-166 lr=['0.0050000'], tr/val_loss:  5.414649/625.759521, val:  71.67%, val_best:  76.67%, tr:  88.15%, tr_best:  89.79%, epoch time: 95.27 seconds, 1.59 minutes\n",
      "epoch-167 lr=['0.0050000'], tr/val_loss:  5.558202/843.746460, val:  69.58%, val_best:  76.67%, tr:  88.76%, tr_best:  89.79%, epoch time: 95.74 seconds, 1.60 minutes\n",
      "epoch-168 lr=['0.0050000'], tr/val_loss:  5.512180/784.285889, val:  67.08%, val_best:  76.67%, tr:  87.84%, tr_best:  89.79%, epoch time: 93.29 seconds, 1.55 minutes\n",
      "epoch-169 lr=['0.0050000'], tr/val_loss:  6.026239/754.888611, val:  69.17%, val_best:  76.67%, tr:  87.84%, tr_best:  89.79%, epoch time: 94.02 seconds, 1.57 minutes\n",
      "epoch-170 lr=['0.0050000'], tr/val_loss:  6.209208/849.026611, val:  67.50%, val_best:  76.67%, tr:  88.56%, tr_best:  89.79%, epoch time: 94.49 seconds, 1.57 minutes\n",
      "epoch-171 lr=['0.0050000'], tr/val_loss:  5.760499/643.526306, val:  73.33%, val_best:  76.67%, tr:  87.64%, tr_best:  89.79%, epoch time: 94.87 seconds, 1.58 minutes\n",
      "epoch-172 lr=['0.0050000'], tr/val_loss:  5.388708/749.875122, val:  67.08%, val_best:  76.67%, tr:  87.54%, tr_best:  89.79%, epoch time: 96.13 seconds, 1.60 minutes\n",
      "epoch-173 lr=['0.0050000'], tr/val_loss:  4.915111/659.400940, val:  74.17%, val_best:  76.67%, tr:  89.58%, tr_best:  89.79%, epoch time: 94.94 seconds, 1.58 minutes\n",
      "epoch-174 lr=['0.0050000'], tr/val_loss:  4.481442/706.276184, val:  69.58%, val_best:  76.67%, tr:  89.38%, tr_best:  89.79%, epoch time: 93.70 seconds, 1.56 minutes\n",
      "epoch-175 lr=['0.0050000'], tr/val_loss:  4.525081/565.419678, val:  74.58%, val_best:  76.67%, tr:  90.09%, tr_best:  90.09%, epoch time: 92.84 seconds, 1.55 minutes\n",
      "epoch-176 lr=['0.0050000'], tr/val_loss:  4.898507/582.487122, val:  76.67%, val_best:  76.67%, tr:  90.30%, tr_best:  90.30%, epoch time: 94.82 seconds, 1.58 minutes\n",
      "epoch-177 lr=['0.0050000'], tr/val_loss:  4.853304/722.380005, val:  71.25%, val_best:  76.67%, tr:  87.84%, tr_best:  90.30%, epoch time: 92.45 seconds, 1.54 minutes\n",
      "epoch-178 lr=['0.0050000'], tr/val_loss:  4.581241/612.541138, val:  71.67%, val_best:  76.67%, tr:  89.68%, tr_best:  90.30%, epoch time: 94.96 seconds, 1.58 minutes\n",
      "epoch-179 lr=['0.0050000'], tr/val_loss:  4.441131/552.029724, val:  76.67%, val_best:  76.67%, tr:  88.66%, tr_best:  90.30%, epoch time: 97.20 seconds, 1.62 minutes\n",
      "epoch-180 lr=['0.0050000'], tr/val_loss:  5.047407/854.618103, val:  67.50%, val_best:  76.67%, tr:  88.87%, tr_best:  90.30%, epoch time: 93.82 seconds, 1.56 minutes\n",
      "epoch-181 lr=['0.0050000'], tr/val_loss:  5.162522/829.623352, val:  71.25%, val_best:  76.67%, tr:  88.25%, tr_best:  90.30%, epoch time: 93.84 seconds, 1.56 minutes\n",
      "epoch-182 lr=['0.0050000'], tr/val_loss:  5.151754/640.408997, val:  72.50%, val_best:  76.67%, tr:  89.48%, tr_best:  90.30%, epoch time: 91.97 seconds, 1.53 minutes\n",
      "epoch-183 lr=['0.0050000'], tr/val_loss:  4.390235/831.977356, val:  68.33%, val_best:  76.67%, tr:  91.01%, tr_best:  91.01%, epoch time: 94.70 seconds, 1.58 minutes\n",
      "epoch-184 lr=['0.0050000'], tr/val_loss:  4.854737/819.413452, val:  70.00%, val_best:  76.67%, tr:  90.60%, tr_best:  91.01%, epoch time: 96.08 seconds, 1.60 minutes\n",
      "epoch-185 lr=['0.0050000'], tr/val_loss:  3.569257/777.383911, val:  67.08%, val_best:  76.67%, tr:  89.89%, tr_best:  91.01%, epoch time: 97.31 seconds, 1.62 minutes\n",
      "epoch-186 lr=['0.0050000'], tr/val_loss:  4.736609/724.170166, val:  69.17%, val_best:  76.67%, tr:  90.30%, tr_best:  91.01%, epoch time: 95.16 seconds, 1.59 minutes\n",
      "epoch-187 lr=['0.0050000'], tr/val_loss:  5.578574/757.562927, val:  70.00%, val_best:  76.67%, tr:  88.46%, tr_best:  91.01%, epoch time: 94.30 seconds, 1.57 minutes\n",
      "epoch-188 lr=['0.0050000'], tr/val_loss:  3.947816/727.802185, val:  69.58%, val_best:  76.67%, tr:  90.40%, tr_best:  91.01%, epoch time: 94.17 seconds, 1.57 minutes\n",
      "epoch-189 lr=['0.0050000'], tr/val_loss:  4.984643/676.833862, val:  70.42%, val_best:  76.67%, tr:  89.27%, tr_best:  91.01%, epoch time: 96.71 seconds, 1.61 minutes\n",
      "epoch-190 lr=['0.0050000'], tr/val_loss:  4.527967/605.535339, val:  75.00%, val_best:  76.67%, tr:  90.09%, tr_best:  91.01%, epoch time: 94.71 seconds, 1.58 minutes\n",
      "epoch-191 lr=['0.0050000'], tr/val_loss:  4.487660/829.170776, val:  68.75%, val_best:  76.67%, tr:  89.99%, tr_best:  91.01%, epoch time: 95.34 seconds, 1.59 minutes\n",
      "epoch-192 lr=['0.0050000'], tr/val_loss:  4.265622/937.075684, val:  64.17%, val_best:  76.67%, tr:  88.97%, tr_best:  91.01%, epoch time: 95.51 seconds, 1.59 minutes\n",
      "epoch-193 lr=['0.0050000'], tr/val_loss:  4.531450/686.022400, val:  70.42%, val_best:  76.67%, tr:  90.30%, tr_best:  91.01%, epoch time: 89.65 seconds, 1.49 minutes\n",
      "epoch-194 lr=['0.0050000'], tr/val_loss:  3.786029/952.994446, val:  67.08%, val_best:  76.67%, tr:  90.81%, tr_best:  91.01%, epoch time: 63.87 seconds, 1.06 minutes\n",
      "epoch-195 lr=['0.0050000'], tr/val_loss:  5.330844/873.181396, val:  67.92%, val_best:  76.67%, tr:  88.46%, tr_best:  91.01%, epoch time: 62.10 seconds, 1.03 minutes\n",
      "epoch-196 lr=['0.0050000'], tr/val_loss:  4.091687/681.089600, val:  70.83%, val_best:  76.67%, tr:  90.40%, tr_best:  91.01%, epoch time: 64.50 seconds, 1.07 minutes\n",
      "epoch-197 lr=['0.0050000'], tr/val_loss:  4.220359/875.101135, val:  67.92%, val_best:  76.67%, tr:  90.19%, tr_best:  91.01%, epoch time: 63.90 seconds, 1.07 minutes\n",
      "epoch-198 lr=['0.0050000'], tr/val_loss:  4.066583/680.110474, val:  70.00%, val_best:  76.67%, tr:  90.60%, tr_best:  91.01%, epoch time: 60.93 seconds, 1.02 minutes\n",
      "epoch-199 lr=['0.0050000'], tr/val_loss:  4.960958/714.750854, val:  67.92%, val_best:  76.67%, tr:  90.30%, tr_best:  91.01%, epoch time: 62.55 seconds, 1.04 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89dd0aeba6948b6aa67901d7989d180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñá‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.90296</td></tr><tr><td>tr_epoch_loss</td><td>4.96096</td></tr><tr><td>val_acc_best</td><td>0.76667</td></tr><tr><td>val_acc_now</td><td>0.67917</td></tr><tr><td>val_loss</td><td>714.75085</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-sweep-265</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/80r73ise' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/80r73ise</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251110_154600-80r73ise/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: as556e6d with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [512]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 5000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_threshold: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: one\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251110_210014-as556e6d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/as556e6d' target=\"_blank\">dulcet-sweep-269</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/as556e6d' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/as556e6d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_threshold' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': '20251110_210023_443', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.75, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.75, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [512], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 5e-05, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'one', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'output_threshold': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 7a22c8a0ef5b9b252dbf98632e270efd\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=512, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=False, ANPI_MODE=True)\n",
      "      (4): SYNAPSE_FC(in_features=512, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient(single_step=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 506,880\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 5e-05\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "inFeed spike.shape torch.Size([10, 512]) self.weight_fb.shape torch.Size([10, 512])\n",
      "self.weight_fb[0] tensor([ 1.2009e-02,  1.3379e-01, -1.0650e-02,  5.2556e-02, -1.1912e-01,\n",
      "         4.0419e-02, -4.0199e-02, -5.0604e-02,  3.2680e-02, -7.8942e-02,\n",
      "        -1.0288e-01, -1.8775e-02, -5.7299e-03,  1.2332e-02, -6.9353e-02,\n",
      "         1.1499e-01, -4.4228e-02,  4.2593e-02,  4.9323e-02, -2.0675e-03,\n",
      "         9.2336e-02, -3.1971e-02, -1.5728e-02,  9.1276e-02, -2.0181e-02,\n",
      "        -7.1800e-02,  1.4578e-01, -4.2861e-02,  1.1373e-02, -7.3257e-02,\n",
      "        -1.1159e-01, -9.7846e-02,  5.1912e-02,  8.7845e-02,  4.0044e-02,\n",
      "         2.6324e-02, -9.8372e-02,  3.8522e-02,  1.0460e-01, -4.1150e-02,\n",
      "         5.8342e-02,  4.8482e-03,  5.2401e-03, -8.7172e-03,  2.0523e-02,\n",
      "        -3.6457e-02, -6.6373e-02,  5.9048e-03, -2.0717e-02, -3.2546e-02,\n",
      "        -5.4324e-02,  2.4378e-02,  1.0149e-02, -1.2236e-02,  6.2543e-02,\n",
      "        -8.3454e-02, -2.1650e-02, -3.9879e-02,  2.7655e-02, -3.3246e-02,\n",
      "         7.6898e-02, -5.0422e-02,  1.5484e-02, -2.6447e-02,  6.8359e-02,\n",
      "        -6.8262e-02,  3.4312e-02, -7.9518e-02, -2.3619e-02,  3.1812e-02,\n",
      "         6.2016e-03,  1.6009e-02,  2.2387e-02,  1.4105e-01,  1.4450e-03,\n",
      "         9.7970e-02, -7.1751e-02,  5.8704e-02, -2.8309e-02,  4.7077e-02,\n",
      "        -3.5820e-02, -4.3640e-02, -4.4777e-02, -3.1386e-02, -2.7226e-02,\n",
      "        -2.5884e-02,  1.0779e-02,  2.7401e-02,  3.1376e-02, -7.5319e-02,\n",
      "        -1.6829e-02,  1.7118e-02, -8.9122e-02, -4.0006e-02,  4.6343e-03,\n",
      "         1.2001e-02,  3.6892e-02,  1.4373e-02,  7.0655e-02, -4.2197e-02,\n",
      "        -1.0233e-01,  3.7360e-04,  8.5512e-02,  7.8637e-02,  1.4384e-03,\n",
      "        -8.0477e-02, -4.6482e-02,  2.3251e-02, -3.3886e-02, -2.4537e-03,\n",
      "        -4.8149e-02, -1.5486e-01,  4.3330e-02, -5.8045e-03, -1.3386e-02,\n",
      "         2.7755e-02, -1.9510e-02,  1.3393e-03,  3.8708e-02,  1.5263e-02,\n",
      "         4.6335e-02, -7.2374e-03, -6.3238e-03, -3.1016e-02, -3.1252e-02,\n",
      "        -7.4723e-02, -1.5088e-02, -4.1994e-02,  1.2212e-02,  6.0550e-02,\n",
      "        -1.7745e-03,  1.0415e-01,  6.7522e-02, -6.1409e-02, -4.1550e-02,\n",
      "         1.0644e-01,  1.5230e-01, -3.8367e-02,  7.8697e-02, -1.7323e-02,\n",
      "         2.6986e-02,  2.6370e-02,  6.5894e-02, -1.2553e-01, -3.9156e-02,\n",
      "         1.3065e-01, -5.8646e-03,  1.4600e-02, -4.5190e-02, -1.0434e-01,\n",
      "         5.6415e-02,  4.8810e-02, -3.8917e-02,  1.3367e-01,  7.2065e-02,\n",
      "        -2.6348e-02,  1.4814e-02, -7.9086e-02, -7.4679e-03, -3.7547e-02,\n",
      "        -4.9995e-02,  1.3292e-04, -1.2034e-02,  4.6384e-02,  5.0249e-02,\n",
      "         5.1038e-02, -3.7747e-02,  8.0393e-02, -6.6428e-02, -1.4425e-03,\n",
      "        -2.2637e-02, -3.0118e-02,  9.2677e-03, -9.3434e-02,  1.9207e-02,\n",
      "        -2.7770e-02, -6.7883e-02, -7.8605e-02, -9.7644e-02, -9.8327e-02,\n",
      "        -4.0612e-02,  4.7043e-02, -3.7591e-02,  1.8712e-02, -8.3181e-02,\n",
      "        -1.9715e-02,  3.6721e-02,  3.5419e-02, -4.6781e-02, -7.8367e-03,\n",
      "        -2.6748e-02, -8.6308e-02,  2.3989e-02, -1.2710e-02,  3.7118e-02,\n",
      "        -6.2088e-02, -2.2962e-04, -4.9640e-02,  2.4384e-02,  1.5691e-01,\n",
      "         1.5421e-02,  5.5528e-02,  4.8312e-02,  5.6640e-02, -2.2735e-02,\n",
      "         5.3113e-03, -5.2211e-02,  2.6325e-02,  6.9295e-02,  2.4738e-02,\n",
      "        -5.3518e-03,  5.2276e-02, -2.4634e-02, -5.3242e-03,  1.2084e-01,\n",
      "        -2.6133e-02,  3.3964e-02,  9.2582e-03, -1.2223e-01, -2.1360e-03,\n",
      "        -7.8244e-02, -1.5748e-02,  1.4439e-03,  1.2431e-01,  6.0634e-02,\n",
      "         8.5934e-02, -6.0989e-02, -2.9897e-02, -1.1970e-03, -1.0762e-01,\n",
      "         1.0423e-02,  1.6176e-02, -1.3812e-02, -5.2755e-02,  1.6920e-02,\n",
      "         6.1367e-02,  9.1813e-02,  2.1540e-02,  7.7856e-03, -4.0828e-02,\n",
      "        -9.7598e-02, -4.1089e-02,  9.0935e-02,  1.8519e-02, -3.4424e-02,\n",
      "         2.8530e-03, -6.6620e-02, -8.9594e-03, -6.7013e-03, -4.6130e-02,\n",
      "        -2.1535e-02,  5.8145e-03,  4.0000e-03, -5.7107e-02,  4.8855e-02,\n",
      "        -1.1148e-01, -1.1978e-01,  6.8131e-02,  1.5512e-03,  3.5912e-02,\n",
      "         3.3328e-02,  3.1726e-02, -8.8611e-02,  1.4725e-01, -9.5569e-02,\n",
      "        -1.0785e-02, -1.3891e-03,  1.3467e-02,  4.0348e-02,  9.6515e-02,\n",
      "         1.6649e-02,  3.0992e-02, -1.5092e-02, -5.3478e-02,  2.6478e-02,\n",
      "        -1.3042e-02, -9.5301e-02, -6.6575e-03, -1.5733e-03, -9.9895e-03,\n",
      "         3.4082e-02,  1.5740e-01, -9.9586e-03, -5.3744e-02,  8.7394e-02,\n",
      "         4.2685e-02,  5.2481e-02,  1.7623e-02,  1.0548e-03,  4.5100e-02,\n",
      "         7.4265e-02, -7.1658e-03, -8.7438e-02, -3.9754e-02,  5.4727e-02,\n",
      "         4.6412e-02,  4.2058e-02, -3.2855e-02, -1.1088e-01, -1.7722e-02,\n",
      "         4.9851e-03, -8.0476e-02,  8.2968e-02, -8.2024e-02,  1.6164e-02,\n",
      "         3.7377e-02, -9.2349e-02, -1.1127e-01,  6.9750e-02,  8.6820e-02,\n",
      "        -2.7057e-02, -2.3069e-02, -7.3103e-02, -1.6484e-01, -2.0014e-02,\n",
      "         6.3153e-03,  7.7782e-02, -8.4823e-02,  2.2121e-02,  1.0625e-01,\n",
      "        -1.4292e-01,  8.1527e-02, -7.1087e-02, -8.0429e-02, -4.0732e-03,\n",
      "         6.4006e-02, -1.4278e-01, -7.9276e-03,  5.2838e-02, -3.7510e-03,\n",
      "        -5.9070e-02, -1.1084e-01, -1.6297e-03,  5.6736e-03, -7.3166e-02,\n",
      "        -6.8036e-02,  1.5117e-01,  1.9150e-02, -9.3975e-02, -4.8127e-02,\n",
      "         4.4899e-02,  5.5049e-02,  6.3477e-02,  5.0466e-02,  1.4346e-01,\n",
      "        -1.4061e-02,  1.8790e-01,  3.4009e-02,  1.4160e-03, -2.5282e-02,\n",
      "        -1.6245e-02,  5.4068e-02, -7.5012e-02, -7.5148e-02, -1.8582e-02,\n",
      "        -2.3466e-02,  1.9578e-02, -6.2413e-02,  1.2314e-01,  1.3701e-02,\n",
      "        -5.7122e-03,  8.9041e-02,  3.7946e-02,  4.1243e-02,  4.7171e-02,\n",
      "         2.7039e-02, -5.9925e-03, -2.8245e-02, -7.2878e-02,  1.4521e-02,\n",
      "         9.9702e-02,  6.4296e-02,  7.4185e-02, -7.1993e-02,  1.4546e-02,\n",
      "         7.7495e-02, -9.2409e-03, -3.8808e-02,  7.1566e-02, -1.4977e-01,\n",
      "         4.2293e-02, -4.2540e-02, -5.6876e-03, -4.4148e-02, -8.0183e-02,\n",
      "         7.5278e-02, -2.9656e-03, -4.9337e-02,  2.6277e-02, -1.1994e-02,\n",
      "        -9.6900e-03, -8.8157e-03, -1.7625e-02, -8.9690e-02, -3.2884e-02,\n",
      "        -5.1021e-03, -1.0199e-01, -1.6831e-02,  1.1726e-01, -3.4447e-02,\n",
      "        -2.8511e-02, -1.9198e-02,  3.6576e-03,  3.2099e-02,  4.5579e-03,\n",
      "         8.7041e-02, -3.0138e-02,  1.8212e-02,  7.4119e-02, -1.3839e-02,\n",
      "         5.3415e-02,  2.2786e-02,  1.0557e-01, -5.6927e-02,  3.3285e-02,\n",
      "         7.3276e-02,  1.0244e-01, -1.4565e-02, -1.0259e-01,  1.2200e-01,\n",
      "         6.1812e-02,  4.8889e-02, -5.6486e-02,  5.1047e-02,  9.3909e-02,\n",
      "        -1.0201e-02,  6.4712e-02, -2.3649e-02,  3.8729e-02,  6.1245e-03,\n",
      "        -4.3430e-02,  6.4039e-03, -8.9212e-02,  1.5119e-01,  7.2071e-02,\n",
      "         1.5732e-02, -2.2774e-02,  5.2327e-02,  2.5401e-02,  2.9843e-02,\n",
      "        -1.1558e-01,  5.9937e-02, -5.8328e-02,  7.1370e-02,  4.9816e-02,\n",
      "         6.5657e-02,  3.2430e-02, -8.6861e-03,  8.5977e-02,  1.9082e-02,\n",
      "         2.7206e-02, -1.9106e-03, -6.5907e-02,  4.0442e-03,  1.7387e-02,\n",
      "         1.3066e-01, -8.5428e-02, -2.6442e-02,  5.6974e-02, -8.7909e-02,\n",
      "         3.4048e-02, -5.8666e-02,  1.8037e-02, -6.2223e-02, -1.8848e-02,\n",
      "         9.5296e-03, -5.1592e-03,  5.1242e-03,  9.5190e-02,  1.1389e-02,\n",
      "        -6.1644e-02,  2.7198e-02,  2.2262e-02, -4.7755e-02,  6.3539e-03,\n",
      "        -2.4203e-02,  1.3476e-02,  5.5816e-02,  3.3884e-02,  5.4144e-02,\n",
      "        -2.0123e-02, -2.5729e-02,  3.2092e-02, -3.4289e-02, -1.2439e-03,\n",
      "         1.8775e-01,  5.8437e-02,  1.8716e-02, -5.8857e-02, -6.8036e-02,\n",
      "        -5.9856e-04,  1.0747e-01, -7.1370e-02,  1.3296e-03, -3.0167e-02,\n",
      "        -5.6810e-02, -1.0447e-01, -8.7226e-03, -3.1270e-03,  1.2601e-02,\n",
      "         1.8155e-02, -9.4597e-02, -4.7340e-02,  2.7440e-02, -3.4883e-02,\n",
      "        -3.2968e-02, -6.2905e-02, -1.2657e-02,  3.2411e-02,  1.2026e-02,\n",
      "         2.2878e-02, -5.3231e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 6.6658e-02, -7.8302e-02, -3.9761e-02, -4.1793e-02,  4.5831e-02,\n",
      "         4.8306e-02, -6.7736e-03,  7.5574e-02, -7.4495e-02, -3.0042e-02,\n",
      "         5.2244e-03, -1.3071e-02, -5.5794e-03, -8.3971e-02, -6.9471e-03,\n",
      "        -2.4258e-02,  1.0854e-01, -6.1369e-02, -1.4674e-01,  1.1226e-01,\n",
      "        -6.0065e-02,  5.3451e-02,  1.1262e-01, -4.9005e-03,  1.5264e-01,\n",
      "         7.8240e-02,  3.1867e-02,  7.0535e-03, -8.8613e-02, -1.6180e-02,\n",
      "         7.1920e-03,  3.6067e-02, -1.8580e-02, -6.9305e-02,  5.7444e-02,\n",
      "        -9.3223e-02,  6.4325e-02, -1.2735e-01, -1.6280e-02, -5.1730e-02,\n",
      "        -1.6762e-02,  1.6986e-01,  2.8526e-02,  7.5887e-02,  4.1897e-03,\n",
      "         5.6685e-02,  4.6633e-02, -3.6862e-02, -3.9126e-02, -2.2331e-02,\n",
      "         9.3762e-02, -1.0613e-02,  1.1766e-01, -3.7826e-02,  6.4190e-02,\n",
      "         2.1247e-02, -9.1414e-03,  9.0567e-02, -1.1170e-01,  1.5015e-02,\n",
      "        -1.6912e-02,  1.8269e-02, -6.4949e-02, -5.4902e-02, -8.6944e-03,\n",
      "         1.3896e-01,  1.1010e-01,  1.0749e-02,  8.7195e-02, -6.8369e-03,\n",
      "        -3.5939e-02,  1.3870e-02,  5.9698e-02, -8.9737e-05,  8.3753e-02,\n",
      "        -4.8358e-03, -3.8847e-02, -1.0107e-01,  7.5683e-02, -1.1180e-01,\n",
      "         3.0140e-02, -4.3089e-02, -2.2418e-02, -3.6128e-02, -1.0527e-01,\n",
      "         2.2898e-02,  4.6009e-02, -7.4225e-03, -5.6874e-02,  8.5350e-02,\n",
      "         5.1923e-03,  2.5627e-02, -8.9285e-03, -5.8058e-02,  7.0525e-02,\n",
      "         3.8854e-02,  2.7697e-02,  1.4393e-01, -4.0282e-02,  2.0928e-02,\n",
      "        -2.4592e-02,  6.1504e-02,  8.4973e-02, -6.5030e-03, -1.1406e-02,\n",
      "        -1.5721e-01, -1.2213e-01, -3.2998e-02, -1.0606e-02,  1.5931e-01,\n",
      "         1.4261e-01,  2.5770e-02, -4.0473e-02, -6.6654e-02,  3.4934e-02,\n",
      "         9.9253e-02, -1.0173e-02, -1.4505e-02,  6.1864e-02,  4.7759e-02,\n",
      "        -1.6578e-02,  3.0713e-02,  1.4806e-02,  8.6155e-02, -1.2338e-02,\n",
      "         7.9021e-02, -7.8331e-02, -6.0098e-02,  7.8730e-02,  2.3303e-02,\n",
      "        -8.3858e-03,  4.4462e-02, -5.4935e-02,  4.2922e-02,  4.7366e-02,\n",
      "        -3.2290e-04,  1.8469e-02, -5.9237e-02,  6.0935e-02,  2.3421e-02,\n",
      "         7.0576e-02, -1.8194e-02,  5.7329e-03,  1.2694e-01, -1.6639e-02,\n",
      "         5.9829e-02, -7.5157e-02, -6.8489e-02, -1.1888e-01, -1.4575e-01,\n",
      "        -6.2740e-03,  8.6623e-02, -1.9370e-03, -1.2883e-01,  4.0742e-02,\n",
      "        -3.1368e-02, -6.8863e-03,  6.7565e-03, -5.5464e-02, -5.8365e-02,\n",
      "        -4.6925e-02, -1.8427e-03, -6.9821e-03, -5.4991e-02,  1.4936e-02,\n",
      "        -6.0094e-02,  2.1199e-02,  1.6101e-03, -6.6419e-02, -1.0129e-01,\n",
      "         3.2519e-04, -9.6969e-02,  2.2424e-02,  8.3956e-02, -1.0915e-01,\n",
      "        -5.2411e-02,  7.9012e-02,  7.7652e-02,  7.2692e-02,  5.3036e-02,\n",
      "         8.0605e-03,  1.2090e-01,  4.4321e-02, -1.3145e-02,  2.7608e-02,\n",
      "        -2.4626e-03, -8.6162e-02, -2.0906e-02, -8.0314e-02,  8.6478e-02,\n",
      "         3.2060e-02, -7.4949e-02, -4.5875e-02, -9.1144e-02,  8.5149e-02,\n",
      "         4.7841e-02, -5.8479e-02,  9.3823e-02, -8.9949e-02, -2.2137e-03,\n",
      "         5.3320e-02,  2.4241e-02,  7.6287e-02, -7.3501e-02,  5.9457e-02,\n",
      "         2.5991e-02, -4.9862e-02,  2.1058e-02,  3.7085e-02,  5.8227e-02,\n",
      "         1.6736e-02,  1.3518e-02, -3.6454e-02,  8.9511e-02, -6.0161e-02,\n",
      "         4.3647e-02,  2.5404e-02,  1.6810e-03, -3.8325e-02,  5.1655e-02,\n",
      "        -6.2435e-03, -7.4342e-02,  1.5280e-02, -3.8896e-02, -4.6945e-02,\n",
      "        -4.9156e-02,  5.0480e-02, -1.1144e-01,  4.6365e-02,  4.1312e-02,\n",
      "         4.3370e-02, -6.4439e-02,  1.4321e-01,  5.6491e-03,  4.6217e-02,\n",
      "        -7.8084e-02,  2.2043e-02,  2.4072e-02, -1.1090e-01, -5.7180e-02,\n",
      "         1.3553e-01,  2.0576e-03, -6.7463e-02, -3.7952e-02,  9.7044e-02,\n",
      "         3.9006e-02,  2.3112e-02,  3.6162e-02, -4.4879e-02, -5.0205e-02,\n",
      "        -6.6276e-02,  6.0393e-02, -1.6587e-02, -4.2223e-02,  4.9360e-02,\n",
      "        -5.2514e-02,  5.3070e-02,  3.0898e-02,  8.4096e-03,  4.2029e-02,\n",
      "         8.3128e-03,  7.7944e-02,  7.4944e-02,  3.7365e-02, -1.7412e-02,\n",
      "        -1.7034e-02, -5.1705e-02, -1.0178e-01,  8.1377e-03, -1.1124e-02,\n",
      "         6.0315e-02, -1.2464e-01, -8.2909e-02, -2.0721e-02,  1.5134e-01,\n",
      "        -7.6029e-03, -5.5703e-02,  1.3161e-01,  1.1009e-01,  8.7843e-02,\n",
      "        -1.1565e-02, -7.0188e-02, -1.7204e-01,  9.7961e-02,  1.4806e-01,\n",
      "        -4.5438e-02, -2.6664e-03, -4.6997e-02, -7.0638e-02, -7.9939e-02,\n",
      "        -7.0988e-02, -1.1400e-01, -7.8130e-03, -8.5862e-02, -3.9800e-02,\n",
      "         7.1482e-03, -1.3455e-01, -2.8474e-02, -8.3467e-02,  6.1789e-02,\n",
      "        -1.2440e-02, -1.4384e-01, -5.4934e-02,  1.7171e-02, -4.3710e-02,\n",
      "         5.2462e-03, -9.8457e-02,  6.4931e-02,  3.0336e-02, -8.2045e-03,\n",
      "        -2.1457e-02,  1.9863e-02, -3.9212e-02,  3.6250e-02, -2.9250e-02,\n",
      "         4.0146e-03,  9.8803e-02, -3.5044e-03, -1.3867e-01,  6.7823e-02,\n",
      "        -1.1386e-02,  4.5815e-02, -4.6995e-02, -6.0331e-02,  8.9048e-02,\n",
      "        -3.3910e-03,  5.5142e-02,  1.0962e-01,  7.8482e-02, -5.7451e-02,\n",
      "         6.7650e-02, -5.0193e-02, -1.0531e-01,  3.0873e-02,  4.0250e-02,\n",
      "         3.5226e-02,  3.5651e-02, -1.3163e-02, -1.5697e-02, -1.3301e-02,\n",
      "        -7.5622e-02,  4.6634e-02, -6.0863e-02,  1.1601e-02,  5.8555e-02,\n",
      "         1.9718e-02,  1.4490e-02,  4.6890e-02,  1.9770e-02,  1.8599e-02,\n",
      "         1.5324e-02,  9.0858e-02, -9.4841e-02,  4.4712e-02,  1.0196e-01,\n",
      "         7.1711e-02,  2.8857e-02, -7.6147e-02,  1.1056e-01,  3.8540e-02,\n",
      "        -7.5464e-02, -1.1109e-01,  1.1038e-02,  7.1191e-02,  3.8999e-02,\n",
      "         8.1577e-02,  1.4265e-01, -2.5305e-02,  7.0406e-02, -2.0950e-01,\n",
      "        -1.0905e-01, -7.9404e-02,  9.4908e-02, -6.2777e-02, -4.6448e-02,\n",
      "         6.7760e-02, -4.1111e-02, -3.0499e-02, -6.7737e-02, -1.6252e-02,\n",
      "         7.7219e-02, -9.5822e-02,  7.5935e-03, -2.3492e-02, -3.9966e-02,\n",
      "         2.2348e-02, -5.5910e-02, -2.2430e-02, -1.2789e-01,  1.1506e-02,\n",
      "        -3.6499e-02, -2.3789e-02,  8.8967e-02,  3.7748e-04,  1.4302e-01,\n",
      "        -3.3631e-02, -3.5510e-02, -1.5043e-01,  7.7718e-02,  1.4879e-01,\n",
      "         6.6394e-02, -1.8917e-02,  1.0423e-02, -4.4962e-03, -2.3098e-02,\n",
      "         8.4583e-02,  1.2187e-01,  2.5955e-02,  2.3483e-02, -1.2860e-01,\n",
      "         2.7167e-02,  3.6408e-02,  8.3306e-02,  1.1587e-01,  6.6651e-02,\n",
      "         5.9024e-02,  1.0206e-01, -6.6102e-02, -1.1416e-02,  6.7382e-02,\n",
      "        -1.8530e-01,  7.1940e-02, -3.7391e-02, -1.0281e-01,  5.0257e-02,\n",
      "         4.7398e-02,  2.7898e-02,  6.5546e-02, -3.5585e-02, -1.5329e-02,\n",
      "        -3.8707e-02, -5.4844e-02, -2.3227e-02,  3.0108e-02, -2.5781e-02,\n",
      "        -2.8408e-02,  3.9738e-03,  9.0303e-02,  8.2566e-03,  2.2979e-02,\n",
      "        -5.5796e-02, -3.8515e-02, -6.0057e-02,  7.1408e-02, -6.8506e-02,\n",
      "        -8.3587e-02, -1.1510e-01,  3.3540e-02, -1.6315e-02, -4.7617e-02,\n",
      "        -1.2741e-01, -2.6345e-02, -6.0932e-02, -2.5297e-02,  1.7280e-03,\n",
      "        -5.4365e-02, -5.7350e-02, -4.4366e-02, -1.8187e-02, -5.9762e-02,\n",
      "         1.8093e-02, -6.1407e-02,  1.3368e-01,  3.7309e-02, -2.3302e-02,\n",
      "        -3.6866e-02,  6.9024e-03,  7.7365e-03,  4.0508e-02, -2.5169e-02,\n",
      "        -8.2504e-02,  1.2014e-01, -6.4195e-02,  6.6726e-02,  1.5957e-02,\n",
      "         1.0247e-01,  9.6323e-02,  5.0310e-02, -7.1386e-02, -6.2054e-03,\n",
      "        -1.6760e-01,  3.7466e-03, -9.4249e-02,  7.7653e-02, -1.2555e-01,\n",
      "        -6.1608e-02, -2.9333e-02,  1.3478e-02, -1.4650e-02, -9.3798e-02,\n",
      "         6.4758e-02,  2.1284e-02,  1.5329e-01, -8.6474e-02, -5.4156e-03,\n",
      "        -2.4129e-02,  1.0983e-01, -2.6136e-02,  1.7877e-02,  7.2377e-02,\n",
      "         2.4865e-02,  5.1694e-02,  5.9210e-02,  1.3274e-01, -4.0805e-02,\n",
      "         2.4143e-02,  6.7355e-02,  6.0903e-02,  6.5552e-02,  1.7681e-01,\n",
      "         4.1771e-02,  1.2728e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 5.0966e-02, -1.4745e-01,  7.7494e-02,  1.4598e-02,  1.1066e-01,\n",
      "        -3.6061e-02, -3.4456e-02,  2.3449e-03,  3.6120e-02, -2.1529e-02,\n",
      "         1.0209e-01,  1.2287e-03, -5.0131e-02,  6.2569e-02, -2.0442e-02,\n",
      "         3.2035e-02,  6.1605e-02, -9.9639e-02,  1.5433e-02,  3.8132e-02,\n",
      "        -6.6866e-02, -6.3091e-02, -6.1747e-02,  6.8062e-02,  8.8035e-02,\n",
      "        -1.0674e-01,  5.1352e-02, -3.5963e-02, -4.7417e-03, -4.0600e-03,\n",
      "        -1.0709e-01, -8.8151e-02,  1.0923e-01, -5.1789e-02, -1.1943e-01,\n",
      "        -3.2427e-02,  8.7168e-02,  1.1600e-01, -3.1433e-02,  2.1007e-02,\n",
      "        -2.0211e-02,  5.1138e-02, -3.1195e-02, -1.7929e-02,  1.6682e-02,\n",
      "        -5.8549e-03, -3.0055e-02, -1.2022e-01,  4.2940e-02,  5.0219e-03,\n",
      "        -7.6352e-02,  1.2055e-02,  1.1379e-02,  7.7296e-02, -3.7195e-02,\n",
      "         6.2380e-02, -9.9886e-02,  1.3775e-02, -3.7782e-02, -8.0343e-03,\n",
      "         1.1148e-02, -1.7144e-02, -8.2952e-02,  6.2111e-02,  1.4023e-02,\n",
      "         9.3064e-02, -1.8222e-02,  8.8978e-02, -9.5613e-02,  5.1005e-02,\n",
      "         6.4407e-02, -1.5327e-02, -1.6592e-02, -4.5361e-02, -3.1602e-02,\n",
      "        -4.6708e-02, -4.0381e-02,  9.3572e-02,  1.4583e-02,  1.5900e-02,\n",
      "         5.2908e-02, -6.2023e-02,  9.5726e-02, -2.2317e-02, -1.0207e-02,\n",
      "        -8.4064e-02, -8.5376e-02,  1.4583e-02,  6.5636e-02,  8.2487e-02,\n",
      "         6.9251e-02, -3.3851e-03,  2.0579e-02, -6.4329e-03, -6.3405e-03,\n",
      "         2.8375e-02, -5.4557e-02,  4.9721e-02, -2.8327e-02,  7.1326e-02,\n",
      "        -2.7338e-02,  7.1745e-02,  2.0902e-02, -1.4693e-02, -6.4021e-03,\n",
      "        -3.6755e-02,  2.3320e-02, -1.8848e-02, -8.2152e-03, -7.3774e-02,\n",
      "        -6.4569e-02, -3.3738e-02,  2.3054e-02, -1.0855e-02,  3.3617e-02,\n",
      "         5.3611e-02, -6.7952e-02, -5.8561e-02, -4.5781e-02,  2.4040e-02,\n",
      "        -8.8937e-02,  3.5465e-02,  5.0535e-02,  2.5044e-02, -4.3513e-03,\n",
      "        -3.2971e-02, -1.3832e-01, -8.0301e-02,  1.5525e-01, -8.0106e-02,\n",
      "         2.0949e-02,  1.1226e-02,  5.7637e-02,  9.5634e-02, -4.6271e-02,\n",
      "         6.2753e-02, -4.8439e-02,  5.5866e-02, -5.6149e-02,  8.9882e-03,\n",
      "        -2.2475e-02,  2.6102e-03, -7.5365e-02, -3.5781e-02,  8.7820e-03,\n",
      "        -2.7019e-02,  5.6331e-02,  1.6614e-03, -3.3956e-02, -6.9785e-02,\n",
      "         1.1633e-01,  5.9738e-02, -8.4658e-02,  3.5563e-02,  1.0341e-01,\n",
      "         7.0607e-05, -4.0593e-02,  3.8467e-02,  1.0799e-01,  1.7658e-02,\n",
      "        -9.0117e-02, -9.2431e-02, -7.4624e-02,  3.1521e-02,  4.0765e-02,\n",
      "        -1.2515e-01,  3.0535e-02,  1.1851e-02, -4.0310e-02,  2.2916e-02,\n",
      "         1.2250e-01,  6.9152e-02, -6.2053e-03,  4.0321e-02,  1.6208e-02,\n",
      "        -6.8822e-02,  2.1849e-02, -3.6987e-02, -4.4603e-02, -1.5947e-01,\n",
      "        -1.6658e-02, -9.6214e-02, -3.7753e-02,  5.4041e-02, -1.7003e-02,\n",
      "         8.1025e-02,  2.4926e-02,  5.5767e-02, -7.9529e-02, -2.1234e-01,\n",
      "        -4.7282e-02, -5.5761e-02,  3.0091e-02,  1.4731e-01, -6.2581e-02,\n",
      "         2.2454e-02, -6.7485e-02,  1.5281e-01,  4.6557e-02,  8.2848e-02,\n",
      "        -9.2783e-03,  7.2040e-02, -9.9636e-02,  6.1564e-02, -5.9368e-02,\n",
      "        -1.9590e-02, -1.0435e-02, -4.1890e-02, -4.7181e-02, -1.2446e-02,\n",
      "        -4.0818e-02,  6.1132e-02, -8.5487e-03,  8.7448e-02,  2.1625e-02,\n",
      "        -1.7572e-02, -9.9109e-02,  3.0057e-02,  7.2901e-02, -1.2618e-02,\n",
      "         3.7349e-02, -2.1917e-02, -6.9758e-02, -1.2695e-03, -1.3122e-02,\n",
      "        -5.0221e-02,  2.3869e-02,  5.0954e-02,  7.0282e-04, -3.3970e-02,\n",
      "        -2.8963e-02, -8.4868e-02, -2.6569e-02, -6.5083e-02,  8.5820e-03,\n",
      "        -4.4336e-03,  5.8201e-03,  2.1587e-02,  7.3191e-03,  4.7043e-03,\n",
      "        -5.8309e-02,  2.1552e-02, -2.5648e-02, -2.2331e-02, -1.0112e-01,\n",
      "        -3.7041e-02, -4.1032e-02, -6.8042e-02,  1.7894e-02, -2.6997e-02,\n",
      "        -2.7584e-02,  1.7612e-02, -1.9444e-03,  5.9923e-02,  6.8182e-02,\n",
      "         2.6522e-02, -6.7600e-02,  3.6002e-02, -1.6933e-02,  9.7652e-03,\n",
      "        -1.0266e-01, -3.6495e-03,  1.1981e-01, -3.1746e-02, -2.1659e-02,\n",
      "        -4.1714e-02,  7.0952e-02, -8.4005e-02,  3.2536e-03, -2.2566e-02,\n",
      "        -3.9273e-02,  3.3117e-03, -8.4515e-02,  5.7761e-02,  9.1372e-02,\n",
      "         9.6171e-03, -1.2380e-01, -8.3872e-04, -1.1604e-02, -2.1467e-02,\n",
      "         3.9992e-02,  8.3243e-04, -5.9930e-03, -2.2868e-02,  2.3452e-02,\n",
      "         1.2934e-02,  1.4610e-01,  6.3666e-04, -4.7834e-02, -1.6290e-02,\n",
      "         6.7797e-02,  3.1905e-02, -6.1453e-02,  4.7708e-02,  4.9836e-02,\n",
      "        -3.2332e-02,  1.4693e-02, -8.0379e-02,  5.6533e-02,  6.9687e-02,\n",
      "         6.2967e-02, -3.5479e-02, -9.2222e-03, -6.3729e-03,  8.0024e-02,\n",
      "         1.0684e-02,  5.5488e-02, -5.7777e-03,  1.2793e-01,  2.4388e-02,\n",
      "         6.8428e-02, -2.1748e-03, -4.4633e-02,  1.3514e-02,  2.4887e-03,\n",
      "        -1.9060e-02, -1.2467e-01, -4.7357e-02, -4.9894e-02,  9.8269e-02,\n",
      "        -6.8453e-03,  3.6830e-02, -3.3399e-02, -4.3410e-02, -9.6036e-02,\n",
      "         8.1545e-02, -3.5613e-02,  6.0910e-02, -5.0575e-02,  6.5858e-03,\n",
      "         5.8657e-02,  2.9649e-02, -5.0301e-02, -1.8220e-02, -7.9198e-02,\n",
      "         4.7839e-02,  3.2613e-02, -9.3417e-02,  6.7337e-02, -8.7942e-03,\n",
      "        -1.6459e-02,  2.7349e-02, -4.9454e-02,  6.1516e-02,  6.7670e-02,\n",
      "         4.5408e-03,  3.2664e-02,  3.3849e-02, -8.3817e-03,  2.9799e-02,\n",
      "        -6.4481e-02,  6.9932e-02,  1.3802e-02, -7.4295e-02,  2.8266e-03,\n",
      "         1.3482e-01,  1.6569e-02, -4.2818e-02,  5.2147e-02,  4.8331e-02,\n",
      "        -2.2739e-02, -1.8746e-02,  2.8624e-02, -8.2209e-02, -4.9650e-02,\n",
      "        -2.9904e-02, -3.1530e-02, -4.7788e-02, -4.7805e-02,  4.2077e-02,\n",
      "        -5.1374e-03,  9.3389e-02,  7.7671e-02, -1.0206e-02, -5.3528e-02,\n",
      "        -6.0535e-03,  2.0553e-02,  2.7381e-02,  8.1292e-03, -6.6471e-02,\n",
      "        -1.9595e-02,  2.1768e-02,  4.5958e-02,  5.7396e-02,  1.7548e-02,\n",
      "        -6.3863e-03, -1.7971e-01,  2.8201e-02,  1.6888e-02, -6.0088e-02,\n",
      "        -4.4732e-02,  5.1204e-04,  5.4047e-02,  1.5042e-02,  8.6862e-02,\n",
      "        -5.6149e-02, -8.0252e-02, -1.7712e-02, -3.3251e-02,  6.7082e-02,\n",
      "         5.7277e-02,  7.4467e-02,  1.3210e-02,  8.0749e-02, -4.9230e-02,\n",
      "         4.0126e-02,  6.4328e-02,  3.2686e-02,  5.5669e-02, -4.5429e-02,\n",
      "        -6.0456e-02,  5.9471e-03, -7.2037e-03, -6.6578e-02,  6.4264e-02,\n",
      "        -3.4567e-02,  1.8057e-01,  9.6095e-02,  1.7282e-02, -5.5573e-03,\n",
      "        -1.5813e-02,  7.3891e-02, -9.6589e-03, -5.6928e-02,  3.5197e-02,\n",
      "        -3.6848e-02,  3.3619e-02, -7.9201e-02, -1.0853e-03, -6.1366e-02,\n",
      "        -4.6373e-02, -2.3210e-02,  2.4530e-02, -2.9117e-02, -2.6862e-02,\n",
      "         2.0443e-02, -1.0311e-02, -4.5818e-02,  3.2928e-02, -1.4177e-01,\n",
      "        -3.3394e-02, -8.0657e-02, -1.1610e-01,  2.7471e-03, -1.1582e-02,\n",
      "         1.8751e-03, -3.5150e-02,  9.0628e-02, -1.1234e-02, -6.3072e-03,\n",
      "        -2.9522e-03, -2.5991e-02,  7.4267e-02,  5.3881e-02, -4.0242e-03,\n",
      "         7.6560e-03,  8.1244e-02, -1.5535e-02, -7.0901e-02,  4.0996e-03,\n",
      "        -1.9212e-02,  1.5392e-02, -4.2169e-02,  1.7310e-02, -7.4863e-02,\n",
      "        -5.8399e-02, -4.7026e-02,  1.1410e-01, -1.0140e-01, -9.5707e-02,\n",
      "         2.0097e-02, -1.0625e-01,  6.2864e-02, -1.0046e-01,  4.0808e-02,\n",
      "        -5.9520e-02, -5.2804e-02,  1.8317e-02, -1.1327e-01, -1.7123e-02,\n",
      "        -2.9642e-03, -1.2108e-02,  4.3250e-02, -6.8001e-02,  2.8993e-02,\n",
      "         2.3379e-03,  6.4308e-03, -5.0257e-02, -2.6099e-02, -9.2139e-03,\n",
      "         1.4326e-01, -3.5042e-02, -5.5747e-03,  1.4443e-01,  6.4646e-02,\n",
      "        -3.6846e-02, -3.1642e-02,  1.8773e-04, -6.0860e-02,  7.3784e-02,\n",
      "         3.4365e-02, -5.6993e-02,  4.9817e-02, -4.8040e-02,  7.2079e-02,\n",
      "         6.0582e-02,  1.5344e-03, -6.8195e-02,  2.4479e-02, -6.7752e-02,\n",
      "        -7.2611e-02, -2.7682e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0768, -0.0110,  0.0261, -0.0717,  0.0138, -0.0634, -0.0912,  0.0113,\n",
      "        -0.0347, -0.0304, -0.0077, -0.0341, -0.0804, -0.0470, -0.0264,  0.0091,\n",
      "         0.0322,  0.0482, -0.0405, -0.0913,  0.0352, -0.0308,  0.0159,  0.0034,\n",
      "         0.0155, -0.0147,  0.0697,  0.0984,  0.0066,  0.0651, -0.1385, -0.0525,\n",
      "        -0.0866,  0.0596, -0.0648,  0.0693,  0.0717,  0.0327, -0.0749,  0.1113,\n",
      "         0.0407,  0.0465,  0.1108,  0.0816, -0.0240,  0.0117,  0.0365, -0.0328,\n",
      "         0.0209, -0.0589,  0.0395, -0.0040,  0.0484,  0.0579,  0.0430,  0.0961,\n",
      "         0.0019, -0.0478, -0.0156,  0.0328, -0.0624,  0.0715,  0.0612, -0.0883,\n",
      "         0.0393, -0.0688, -0.0231, -0.0230, -0.0219,  0.0156, -0.0243, -0.1010,\n",
      "        -0.0313,  0.0016, -0.0020, -0.0170, -0.0236, -0.0161, -0.0517, -0.0867,\n",
      "        -0.0712, -0.0125, -0.0954, -0.0109,  0.1592,  0.0375, -0.0574,  0.0412,\n",
      "        -0.0757,  0.1175,  0.0951, -0.0161, -0.0222, -0.1225,  0.0901,  0.0392,\n",
      "        -0.0461, -0.0242,  0.0155, -0.0975, -0.0425, -0.0112,  0.0040,  0.0077,\n",
      "         0.0669, -0.0678, -0.0185, -0.0830, -0.0124,  0.0362, -0.0285,  0.1085,\n",
      "        -0.0133,  0.0715, -0.0329, -0.0025,  0.0326, -0.0271,  0.0487, -0.0552,\n",
      "        -0.0141,  0.0521, -0.0023, -0.0375, -0.1438,  0.0137,  0.0634, -0.0483,\n",
      "        -0.0128,  0.0103,  0.0111,  0.0511,  0.1563,  0.0164,  0.0060, -0.1368,\n",
      "        -0.1142, -0.0285, -0.0205,  0.0208,  0.0782,  0.0446,  0.0960, -0.0340,\n",
      "        -0.0171,  0.0837,  0.1210,  0.0210, -0.0156, -0.0047,  0.0567,  0.1111,\n",
      "        -0.0234, -0.0498, -0.0705, -0.0082,  0.1107,  0.0074,  0.0705, -0.0538,\n",
      "         0.0613, -0.1379,  0.0155, -0.0276,  0.0236, -0.0070, -0.0942, -0.0741,\n",
      "         0.0344,  0.0320, -0.0537, -0.1111, -0.0324,  0.1613,  0.0198,  0.1086,\n",
      "        -0.0317,  0.0004, -0.0473,  0.0628,  0.0596, -0.0103, -0.0568,  0.0624,\n",
      "        -0.0776, -0.1148, -0.0166,  0.0027,  0.0078, -0.0937, -0.0514, -0.0138,\n",
      "        -0.1482, -0.0669, -0.0712,  0.0135,  0.1173, -0.0033, -0.0064, -0.0263,\n",
      "        -0.0567,  0.0106,  0.0777, -0.0619, -0.0526,  0.0932, -0.0841, -0.0340,\n",
      "        -0.1270,  0.0130,  0.0067, -0.0860,  0.1337, -0.0305, -0.0314, -0.0653,\n",
      "         0.1493, -0.0126, -0.0196, -0.0949, -0.0565,  0.0440, -0.0889,  0.0118,\n",
      "        -0.0558, -0.0214, -0.0157, -0.0387, -0.0158,  0.0084, -0.0396, -0.0521,\n",
      "        -0.0809,  0.0183,  0.0045,  0.0053, -0.0093, -0.0678, -0.1156,  0.0174,\n",
      "         0.1187,  0.0416,  0.0693, -0.0025,  0.0486,  0.0294, -0.0075, -0.0575,\n",
      "         0.1809,  0.0164,  0.0446, -0.0271, -0.0230,  0.0786, -0.0114, -0.0058,\n",
      "         0.0358, -0.0731, -0.0365, -0.0286,  0.1120, -0.0882,  0.0127,  0.0710,\n",
      "         0.0003,  0.0062, -0.0400,  0.0463,  0.0816,  0.0720,  0.0084,  0.0478,\n",
      "         0.0634,  0.0475,  0.0025, -0.0680, -0.0101,  0.0497,  0.0274,  0.0548,\n",
      "         0.0372, -0.0325,  0.1441,  0.0648,  0.0218,  0.0187,  0.0017,  0.0058,\n",
      "         0.0606,  0.0349, -0.0842, -0.0129,  0.1517, -0.0832, -0.0344,  0.0722,\n",
      "         0.0201, -0.0085,  0.0686, -0.0399, -0.1319,  0.0208, -0.0094, -0.0035,\n",
      "         0.0502,  0.0415,  0.0268,  0.0031, -0.0782, -0.0470,  0.0647, -0.0245,\n",
      "        -0.0220,  0.0053, -0.0115,  0.0109,  0.0431,  0.0079, -0.0562, -0.0070,\n",
      "         0.0463, -0.0588,  0.0339,  0.0052, -0.0210,  0.1090,  0.0647, -0.0540,\n",
      "         0.0085,  0.0879, -0.0313,  0.0073,  0.0437,  0.0494,  0.0060,  0.1026,\n",
      "         0.0076,  0.0393, -0.0335, -0.0069, -0.1043,  0.0803, -0.0891,  0.1589,\n",
      "        -0.0709, -0.0418, -0.0459, -0.0026,  0.1630, -0.0228,  0.0362,  0.0665,\n",
      "         0.0199,  0.0311, -0.0793,  0.0584, -0.0846, -0.0298,  0.0471,  0.1816,\n",
      "         0.1290, -0.0308, -0.0354,  0.0684,  0.0022,  0.1397,  0.1273, -0.0121,\n",
      "        -0.0255,  0.1549, -0.1043,  0.0030, -0.0070, -0.0533, -0.1327, -0.0505,\n",
      "        -0.0394, -0.0871, -0.1559, -0.1013, -0.0389,  0.0533, -0.0024,  0.0499,\n",
      "         0.0578, -0.0086, -0.0890, -0.0100,  0.0792, -0.0145, -0.0229, -0.0173,\n",
      "        -0.0718,  0.0246, -0.0108, -0.0746, -0.1079, -0.1119, -0.0225,  0.0620,\n",
      "        -0.0441,  0.0702,  0.1055, -0.0187,  0.0807,  0.0159,  0.0401,  0.0435,\n",
      "        -0.0720, -0.1575, -0.0476, -0.0490, -0.0268,  0.1036,  0.0390,  0.0015,\n",
      "        -0.1407, -0.0818, -0.0521, -0.0193,  0.0634,  0.0762, -0.0572,  0.0335,\n",
      "        -0.0147,  0.0902, -0.0812,  0.0083, -0.1243, -0.0758,  0.1391,  0.0418,\n",
      "         0.0337, -0.0012,  0.0702, -0.0611,  0.0674,  0.0109,  0.0365, -0.0833,\n",
      "        -0.0679, -0.0756,  0.0385, -0.0285,  0.0510, -0.0359,  0.0606,  0.0541,\n",
      "         0.0934, -0.0538, -0.0293,  0.0203, -0.0051,  0.1183, -0.0098,  0.0472,\n",
      "         0.0742, -0.0267, -0.0643, -0.0058,  0.0205,  0.0397, -0.0012,  0.0355,\n",
      "         0.0729,  0.0082,  0.0999,  0.0031,  0.0537,  0.0390,  0.0033,  0.0092,\n",
      "         0.0299, -0.0649,  0.0372,  0.0805,  0.0463, -0.0983, -0.0180, -0.0175,\n",
      "         0.0584, -0.0766,  0.0062, -0.0004,  0.0233, -0.0832,  0.0306,  0.0634,\n",
      "         0.0414, -0.0457,  0.0292, -0.0461,  0.0299,  0.0362,  0.0514,  0.0055,\n",
      "        -0.0551, -0.0026, -0.0381, -0.0229, -0.0396, -0.0021,  0.1161, -0.0633,\n",
      "         0.0352, -0.0886,  0.1244, -0.0195,  0.0971,  0.0900, -0.1717, -0.0553],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-2.8312e-02,  4.9911e-02,  9.7769e-03, -1.7147e-02,  4.0901e-02,\n",
      "        -1.2317e-01, -1.1881e-01,  8.5501e-02,  1.1018e-01,  6.2696e-02,\n",
      "         3.1070e-02, -1.0946e-01,  7.7663e-02,  6.7539e-02, -1.3375e-04,\n",
      "        -1.2912e-02,  5.7624e-02, -7.1261e-02,  9.6846e-04, -4.5915e-03,\n",
      "         6.0058e-02,  2.9872e-02,  4.2197e-02,  3.8850e-02,  5.4885e-02,\n",
      "         4.4528e-02, -8.8942e-02,  1.1722e-01, -4.4009e-02,  3.8589e-02,\n",
      "        -7.9293e-02, -1.1473e-02, -2.3653e-02, -4.3948e-02, -2.1827e-02,\n",
      "        -4.3308e-04,  8.2051e-02,  6.2999e-02,  3.0414e-02,  1.3454e-02,\n",
      "         5.9846e-03,  1.5785e-02, -6.2734e-02,  7.9752e-02, -1.4402e-01,\n",
      "        -5.4157e-02,  8.3404e-02, -5.4182e-02, -3.7938e-02,  1.9626e-03,\n",
      "         6.2376e-02, -9.8665e-02,  1.1238e-01,  8.4942e-02, -5.1376e-02,\n",
      "        -4.4197e-03,  1.0537e-02,  7.6728e-02,  7.0679e-02,  7.5002e-02,\n",
      "         2.3206e-02,  2.2686e-02,  3.7321e-02,  3.3898e-02, -2.2739e-02,\n",
      "        -1.1890e-01,  7.7856e-02,  1.0845e-01,  6.1648e-02, -2.4917e-02,\n",
      "        -5.6272e-02, -2.0143e-04, -6.7984e-02, -5.5723e-02,  1.5601e-03,\n",
      "         9.5723e-02, -1.2334e-01,  2.3138e-02,  1.5915e-03,  1.7391e-02,\n",
      "         1.0060e-03, -5.5752e-02, -7.3283e-03,  7.8786e-02, -8.5108e-02,\n",
      "         5.5049e-02,  1.5016e-01, -3.1859e-02,  4.4934e-03, -5.7109e-02,\n",
      "         8.0624e-03,  1.0309e-01, -3.0260e-03, -1.8075e-02,  1.0297e-01,\n",
      "         1.8190e-02,  8.1257e-02, -1.0586e-01,  4.6859e-02,  8.7545e-03,\n",
      "        -1.8347e-02,  7.8826e-04,  3.4076e-02,  3.4202e-02, -4.6036e-02,\n",
      "         7.8401e-02,  1.2534e-02, -2.9604e-02, -1.4013e-01, -1.2220e-01,\n",
      "        -3.9575e-02,  4.2375e-02,  6.8481e-02, -1.1031e-01,  1.7292e-03,\n",
      "         5.6505e-03, -1.3347e-01,  5.8967e-02,  1.0500e-01,  2.8959e-02,\n",
      "        -1.3579e-01, -3.6767e-02, -6.5603e-03,  5.9650e-02,  3.4714e-02,\n",
      "         3.4603e-02,  6.3472e-02,  8.8572e-02, -3.0379e-02,  1.2246e-02,\n",
      "         3.0892e-02, -1.9900e-02, -2.0532e-02, -9.3364e-02,  2.0879e-02,\n",
      "        -3.1082e-02,  7.4723e-02,  3.4827e-02,  9.9355e-03,  4.0432e-02,\n",
      "         9.0674e-02, -6.2378e-02, -1.7440e-02,  1.5880e-02, -1.3521e-02,\n",
      "         6.1648e-02, -2.5270e-02, -1.0506e-02,  1.8069e-02, -5.2453e-02,\n",
      "         1.3252e-02,  6.9504e-03, -5.8516e-02,  4.6623e-02,  1.4739e-02,\n",
      "         6.7765e-03,  3.7023e-03,  3.7319e-02,  1.9224e-02,  2.6738e-02,\n",
      "         8.2818e-02, -1.2007e-04,  7.7645e-02,  9.2141e-03,  4.3738e-03,\n",
      "        -1.0779e-01,  8.4956e-02,  3.7886e-02, -1.3384e-01, -1.1208e-01,\n",
      "        -5.7828e-02, -9.7238e-02,  1.0206e-02,  6.5645e-03, -2.8718e-02,\n",
      "         1.5325e-02,  6.6613e-02,  2.6445e-02, -2.4962e-02, -4.9788e-02,\n",
      "        -4.3545e-03, -4.5150e-02, -1.4951e-02,  6.1688e-02, -9.0608e-03,\n",
      "        -8.5805e-02, -1.0172e-01, -9.2241e-02, -1.5714e-03, -2.6098e-02,\n",
      "        -2.3720e-02, -4.2816e-03, -4.2465e-02,  4.0990e-03,  5.9952e-02,\n",
      "        -8.0171e-02,  3.4743e-02, -5.9418e-02, -5.0707e-04, -1.7003e-02,\n",
      "        -3.6289e-02,  9.0298e-02, -2.5486e-02,  2.2962e-02,  8.9927e-03,\n",
      "         3.8505e-02,  5.5345e-02, -2.0447e-02, -3.3111e-02,  3.7436e-02,\n",
      "         6.5773e-02, -4.5183e-02,  4.1996e-02, -8.7999e-02, -1.1769e-02,\n",
      "        -4.3234e-02, -6.6346e-02, -3.5659e-02, -5.7530e-03,  3.8261e-02,\n",
      "         6.5813e-02, -2.6030e-02, -7.3186e-03, -6.0748e-02, -5.1565e-02,\n",
      "        -2.2371e-02,  1.2256e-02,  7.5072e-02,  1.9970e-02,  2.4642e-02,\n",
      "        -7.0200e-02,  3.6686e-02,  2.4515e-02,  3.2946e-03,  6.7995e-03,\n",
      "         8.7247e-02, -6.1754e-02,  2.3224e-02,  4.8788e-02, -3.7919e-02,\n",
      "        -4.5916e-02, -6.3038e-03, -6.4867e-02,  9.7451e-03, -2.9809e-02,\n",
      "         1.9220e-02,  4.9873e-02, -8.4751e-02, -3.8756e-02,  2.4613e-03,\n",
      "         1.2979e-02, -1.9546e-02, -1.7456e-03,  6.0348e-02,  3.5478e-02,\n",
      "         8.5359e-02,  4.5793e-02, -2.9652e-02, -1.9533e-02,  2.8801e-02,\n",
      "         2.0128e-02, -1.6773e-02, -2.2567e-02,  8.6599e-02,  7.6258e-02,\n",
      "        -1.3919e-02, -5.2701e-03,  1.5254e-02, -5.6596e-03,  1.2512e-02,\n",
      "        -1.1107e-01, -3.9220e-02, -4.3274e-02, -1.4759e-02,  6.3456e-02,\n",
      "        -3.9313e-02,  6.6304e-02, -2.5031e-02, -8.0906e-02, -9.2574e-02,\n",
      "         7.7114e-03, -3.8525e-02,  2.6354e-02,  6.7656e-02, -3.6397e-02,\n",
      "        -6.6598e-02,  4.9100e-02, -4.5302e-02, -9.6687e-02,  3.2252e-03,\n",
      "        -1.6827e-02,  9.3235e-02, -2.9695e-02,  8.8593e-02,  1.0684e-01,\n",
      "         1.0159e-01,  7.8147e-02, -2.3984e-02,  7.4527e-02,  9.7435e-02,\n",
      "         9.9969e-02,  4.1802e-02,  5.5769e-02,  4.1883e-02,  3.7363e-02,\n",
      "        -1.2641e-02,  3.1162e-02, -5.7425e-04,  5.6984e-02,  2.1873e-03,\n",
      "         3.2089e-02, -7.0392e-02,  2.0635e-02,  9.4762e-03, -1.5822e-02,\n",
      "         5.4450e-02, -2.8916e-02,  1.6877e-02, -7.8206e-03, -1.1922e-01,\n",
      "         2.3058e-02,  6.5806e-02,  9.5983e-03,  4.4597e-02,  1.8453e-02,\n",
      "         4.3058e-02,  6.1493e-02, -6.8039e-02, -3.5424e-02, -3.8730e-02,\n",
      "        -4.6403e-02,  2.2619e-03,  1.3438e-02,  3.6322e-02, -9.0361e-02,\n",
      "         2.3885e-02, -6.8223e-02, -2.8933e-02,  1.0164e-01,  1.5505e-02,\n",
      "        -7.0034e-02,  7.1678e-02, -6.8170e-02,  4.8597e-02,  8.5489e-02,\n",
      "         3.4030e-02, -1.1827e-02,  4.7249e-02, -5.7491e-02,  6.4812e-02,\n",
      "        -3.8081e-02,  3.1269e-02,  4.8112e-02, -2.2889e-02, -1.2078e-01,\n",
      "         8.6875e-03,  2.7524e-03, -5.2020e-02, -1.3657e-02, -3.4252e-02,\n",
      "         1.2507e-01,  6.4650e-02, -4.3744e-02,  2.1554e-02,  7.2027e-02,\n",
      "         4.6084e-02,  1.0100e-01,  7.4042e-02, -5.4211e-02, -1.1455e-01,\n",
      "         5.7521e-02, -4.2710e-02, -7.8814e-02, -1.8124e-02,  4.4737e-02,\n",
      "        -5.1269e-02, -6.7855e-02, -8.3722e-02, -6.4286e-02,  3.4506e-02,\n",
      "         8.8117e-02,  4.1227e-02, -1.0366e-01, -5.4640e-02, -3.3339e-03,\n",
      "         1.3867e-01, -5.8631e-02,  1.0841e-02, -9.4331e-02,  1.0992e-01,\n",
      "        -1.8052e-02,  5.6607e-02, -3.0553e-03, -9.7665e-02,  3.6189e-03,\n",
      "         3.8424e-02, -2.0226e-02, -1.0399e-01,  7.1986e-02, -8.7396e-02,\n",
      "        -2.1321e-02, -3.3681e-02, -4.8806e-02, -9.9724e-03,  3.4821e-02,\n",
      "        -3.6701e-02, -1.0064e-01, -4.4952e-02, -2.9649e-02,  6.7568e-02,\n",
      "         1.0062e-01,  1.5413e-02, -5.2982e-03, -8.1491e-02,  6.9497e-02,\n",
      "         7.5970e-03,  2.6650e-02, -7.8061e-02,  8.9628e-02,  5.9069e-02,\n",
      "        -2.8076e-03,  2.2840e-02,  4.9031e-02, -3.0829e-02, -1.4460e-01,\n",
      "         2.0347e-02,  3.0446e-02,  4.5471e-02,  8.5173e-02, -1.1764e-02,\n",
      "        -1.9823e-02, -1.1526e-02, -1.4037e-02, -5.7210e-03,  3.2612e-02,\n",
      "         8.8098e-02,  2.5476e-02,  5.3235e-02,  9.3301e-02,  6.9620e-02,\n",
      "        -6.3628e-02,  6.8000e-02,  1.4908e-01, -5.6959e-02,  5.9116e-02,\n",
      "         2.2112e-02, -2.4973e-02, -2.7610e-02,  4.1903e-02, -2.0115e-02,\n",
      "         5.7806e-02,  1.3158e-03, -8.3065e-02,  4.6314e-02, -9.3857e-02,\n",
      "        -9.9200e-03,  4.4497e-02, -1.1722e-02, -6.1344e-02, -1.3309e-01,\n",
      "         4.0768e-02, -2.1628e-02, -5.0834e-02,  1.0866e-01,  1.6634e-02,\n",
      "         7.5386e-02,  1.1037e-01, -3.8678e-02,  5.1629e-02,  3.5886e-02,\n",
      "         3.2558e-02,  1.4227e-03,  5.5960e-02,  1.0197e-03, -5.6617e-02,\n",
      "         2.2816e-02, -1.3664e-01,  1.3298e-01, -3.5689e-02,  1.8169e-02,\n",
      "        -3.9363e-02, -4.9693e-02,  8.3050e-02, -1.3196e-02, -4.6567e-02,\n",
      "         3.9041e-02,  2.8396e-02, -2.6041e-02,  6.8008e-02, -1.0233e-01,\n",
      "        -1.5822e-02, -3.0579e-02, -4.8071e-02, -6.4514e-02,  1.8201e-02,\n",
      "        -4.3278e-02, -4.3680e-03, -8.4785e-02, -5.5908e-02, -6.7275e-02,\n",
      "         8.3114e-02,  1.3823e-02,  4.9019e-02,  4.0267e-02, -5.4514e-02,\n",
      "         4.9135e-02, -4.8312e-02, -2.4285e-02, -9.7027e-02,  2.4834e-02,\n",
      "         1.4886e-02,  6.9949e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 3.0289e-02,  3.1503e-02,  3.9986e-02,  1.3083e-01, -5.3132e-02,\n",
      "         2.9113e-02,  4.7187e-03,  5.0454e-02,  1.0700e-01, -2.2314e-02,\n",
      "         2.6524e-02, -1.1840e-02,  5.0855e-03,  7.3779e-04, -1.1865e-03,\n",
      "        -4.7954e-02,  1.0474e-02,  2.8582e-02, -7.9896e-02,  7.6038e-02,\n",
      "         4.5977e-02, -1.4148e-02,  3.9841e-02,  1.8766e-02,  8.0392e-02,\n",
      "         2.6746e-02,  2.9566e-02, -2.5976e-02,  1.6478e-02, -5.0035e-02,\n",
      "         2.4266e-02,  4.7684e-03, -4.6095e-02,  5.4383e-02, -5.5842e-02,\n",
      "        -6.3235e-02,  1.0002e-01, -7.9192e-03,  4.9059e-02, -2.9653e-02,\n",
      "         7.4298e-02,  3.2793e-02,  8.6242e-02,  1.3700e-03,  1.4234e-02,\n",
      "         7.6310e-02,  3.2565e-02, -5.5205e-02, -2.8722e-02, -3.9794e-02,\n",
      "         8.0323e-02, -1.0903e-01, -4.8134e-04,  4.3818e-02, -3.0959e-02,\n",
      "        -5.7084e-02,  4.3061e-02,  4.2138e-02,  7.2363e-02,  4.3792e-02,\n",
      "        -7.2850e-02,  5.2529e-03,  4.6195e-03, -6.2514e-02,  8.1972e-02,\n",
      "        -1.2628e-02,  1.1640e-01, -7.5081e-02,  2.6473e-02, -6.2586e-02,\n",
      "        -6.8327e-02,  5.4805e-03, -8.0045e-02, -1.0655e-02, -7.7074e-03,\n",
      "        -8.1215e-02, -1.6442e-02,  6.8840e-03, -6.9273e-03, -4.1731e-02,\n",
      "        -6.2782e-02,  6.2828e-02, -8.7719e-02,  1.7283e-02, -5.3315e-02,\n",
      "        -9.8364e-02, -9.7457e-02,  8.1505e-02,  2.6662e-02,  5.2712e-02,\n",
      "         5.1618e-02, -3.9540e-02, -1.0101e-01, -2.3273e-02,  1.6070e-02,\n",
      "        -3.2476e-02, -3.7883e-02, -1.9677e-02, -3.3466e-02,  1.7523e-02,\n",
      "        -9.1086e-02, -4.3556e-02,  7.8876e-02, -4.1143e-02, -3.5400e-02,\n",
      "        -1.7865e-02,  1.7630e-01,  1.3965e-01, -5.0848e-02, -3.6669e-02,\n",
      "         2.1116e-02, -1.0324e-01, -1.7145e-02,  6.3624e-02, -7.2753e-02,\n",
      "         8.1110e-04,  7.7122e-02,  6.0167e-02,  9.4302e-02,  3.3645e-02,\n",
      "         5.1997e-02,  9.3938e-03,  1.5380e-02,  3.0624e-02,  1.8364e-02,\n",
      "         9.4459e-02, -5.3204e-02,  5.3909e-02,  8.4368e-02, -2.6575e-02,\n",
      "         5.8741e-03,  1.7135e-01,  3.8734e-02,  1.1533e-01, -3.4991e-02,\n",
      "        -1.3902e-01, -5.0564e-02,  2.5342e-02,  1.9510e-03, -4.5458e-02,\n",
      "        -7.6664e-02,  1.0237e-01,  7.7267e-03,  5.8986e-02, -1.9288e-02,\n",
      "         5.3286e-02,  3.6359e-02,  8.0501e-02, -8.3045e-02,  3.3307e-02,\n",
      "         1.5659e-03,  9.6013e-03, -1.5590e-02, -5.1359e-02, -7.0246e-02,\n",
      "        -1.1975e-02,  2.6491e-02, -3.2005e-02,  6.8249e-02,  4.7669e-02,\n",
      "         4.7641e-02, -2.1512e-02, -6.3295e-02, -4.1788e-02, -1.5279e-02,\n",
      "        -9.7037e-02,  2.2685e-02,  2.0949e-02,  3.3309e-02,  9.4829e-03,\n",
      "         5.6710e-02, -7.6783e-03, -1.3969e-01, -4.1760e-02,  8.8335e-03,\n",
      "         4.3914e-02, -1.1144e-02,  2.1213e-02,  5.0143e-02, -1.7819e-02,\n",
      "        -3.6000e-02, -9.8346e-02,  1.8010e-02,  1.1031e-02, -4.7298e-02,\n",
      "        -2.5419e-02, -4.0803e-02,  3.5511e-02,  9.2070e-03,  6.9367e-03,\n",
      "        -4.2061e-02, -1.0377e-02,  8.0876e-02, -5.6107e-02,  5.7277e-02,\n",
      "         8.7439e-03,  1.8353e-02, -4.1559e-02,  3.4507e-02, -1.0548e-01,\n",
      "        -4.0571e-02, -2.1289e-02,  3.0586e-02,  5.1678e-03,  8.7577e-04,\n",
      "         1.3942e-01, -1.1645e-02,  7.2364e-02,  6.5043e-02,  2.4132e-02,\n",
      "         1.1002e-01,  6.1222e-03,  6.6061e-03, -5.2206e-02, -1.3325e-02,\n",
      "        -8.5573e-03, -2.0275e-03,  1.6365e-03,  2.6494e-02,  7.1705e-02,\n",
      "        -7.1865e-02,  8.4742e-02,  6.0429e-02, -5.9917e-04, -5.1137e-02,\n",
      "        -5.9481e-02, -7.6383e-02,  4.8239e-02, -3.4069e-02, -9.6994e-02,\n",
      "         1.8230e-02,  8.8950e-02,  8.6447e-02, -2.9383e-02, -9.0702e-02,\n",
      "        -3.7237e-02, -3.5979e-02, -4.2816e-02, -7.7253e-02,  7.3348e-03,\n",
      "         4.4436e-02, -1.5954e-01,  1.2394e-01,  1.1889e-02,  1.5041e-02,\n",
      "        -6.7389e-02, -4.5964e-02,  2.0859e-02, -3.0347e-02, -2.0750e-02,\n",
      "         3.9519e-02, -2.8886e-02, -8.1723e-02, -2.2986e-02, -2.3117e-03,\n",
      "         7.9396e-02, -4.6225e-02,  5.9592e-02, -6.6315e-02, -4.8456e-02,\n",
      "        -4.7836e-03, -6.7407e-02,  4.6288e-02,  1.5025e-01,  3.1964e-02,\n",
      "        -1.0685e-01, -3.1458e-02, -4.1457e-02,  7.1839e-02, -9.0231e-02,\n",
      "         3.3797e-02, -2.6273e-02, -6.0258e-02, -3.0063e-02, -9.9684e-02,\n",
      "         8.9154e-02,  4.6204e-02,  1.0030e-02, -2.1860e-02, -9.5296e-03,\n",
      "        -2.6632e-02, -2.0542e-02, -8.8112e-02, -3.1891e-02,  8.1285e-02,\n",
      "         3.4284e-02,  9.3343e-02, -7.2938e-02,  4.2222e-02,  8.5092e-02,\n",
      "        -6.9859e-02, -1.1665e-01, -1.7408e-02, -1.5403e-02,  5.4243e-02,\n",
      "         9.8341e-03, -2.8077e-02, -2.9991e-02,  3.4399e-02,  1.4826e-02,\n",
      "         1.0260e-02,  8.0673e-02,  5.1878e-03, -8.1736e-02,  8.6033e-02,\n",
      "         8.2636e-02,  5.0595e-02, -1.1922e-01,  9.3888e-03,  2.7255e-02,\n",
      "         2.7873e-02,  2.2796e-02,  1.8762e-02,  1.4380e-01, -1.4723e-01,\n",
      "        -1.4255e-02, -3.0604e-02, -3.7668e-03,  1.1167e-02, -8.0839e-02,\n",
      "         1.4414e-02, -2.5007e-02, -2.3666e-02, -2.7692e-02, -1.6474e-02,\n",
      "         5.1326e-02, -6.8901e-03,  2.6673e-02, -1.9049e-02, -4.9653e-02,\n",
      "         1.1313e-01,  8.5847e-02,  1.3205e-01, -4.7806e-02, -9.3220e-02,\n",
      "         4.1846e-02, -4.5715e-02,  2.4093e-02, -3.6066e-02,  5.0121e-02,\n",
      "         2.4745e-02, -9.0033e-02,  5.9747e-02, -5.9992e-02, -2.5795e-02,\n",
      "        -3.5649e-02,  2.3503e-02,  1.4340e-01, -5.7906e-02, -8.6132e-03,\n",
      "        -6.0701e-03,  3.0256e-03, -6.0207e-02,  1.3398e-02, -3.4405e-03,\n",
      "         3.6077e-02, -7.9061e-02, -4.5184e-02, -6.7206e-02,  8.3835e-02,\n",
      "        -1.4701e-02,  2.4760e-02,  1.7550e-02,  5.2360e-02, -1.1143e-01,\n",
      "        -6.0042e-02, -2.1617e-02, -2.3820e-02, -1.9716e-02, -1.1295e-01,\n",
      "        -1.7096e-02, -5.0607e-02,  9.7075e-02,  2.0780e-02, -4.8206e-02,\n",
      "         4.0675e-02, -5.4123e-02,  2.6274e-02, -1.1451e-01,  5.9652e-02,\n",
      "        -2.4965e-02, -2.3823e-02,  5.4150e-03, -2.5337e-03, -5.9982e-02,\n",
      "        -3.6474e-02, -1.8158e-02, -1.5301e-02,  1.1725e-02,  2.3499e-02,\n",
      "         7.4033e-02, -4.0130e-02, -5.1274e-02,  9.0815e-02,  5.4975e-02,\n",
      "        -3.4270e-02,  4.5382e-02, -7.2244e-02, -7.0036e-02, -9.7178e-03,\n",
      "        -3.3955e-02, -3.5253e-02,  8.1896e-02,  7.5562e-03, -7.9211e-02,\n",
      "        -1.0875e-01,  1.2409e-03,  7.7800e-02,  1.0634e-02, -8.2665e-02,\n",
      "         1.3230e-02, -3.4552e-02,  9.1453e-02, -6.4865e-02,  4.5128e-02,\n",
      "        -1.1324e-01, -5.8086e-02,  4.5286e-02, -3.5615e-02,  1.1491e-03,\n",
      "         4.5156e-02,  2.6197e-02, -9.7915e-02, -8.8574e-02,  6.3982e-02,\n",
      "        -7.3688e-02,  3.8706e-02,  8.2396e-02,  7.6938e-02, -2.0139e-02,\n",
      "        -6.2673e-02, -8.2048e-02,  5.6388e-02,  1.7644e-02,  4.3307e-02,\n",
      "         8.2072e-03, -4.8394e-02,  7.1145e-03, -1.4995e-01,  6.3767e-02,\n",
      "        -1.7300e-02, -4.0330e-04,  2.5645e-02,  6.1843e-02, -5.0088e-03,\n",
      "         3.9473e-03,  8.7710e-02,  3.0694e-02, -1.5863e-02,  1.2367e-01,\n",
      "         5.8815e-02,  6.1809e-02,  1.1823e-01,  3.4193e-02, -1.3734e-01,\n",
      "        -8.3475e-03, -1.3101e-02,  1.7372e-01,  3.1849e-02,  5.8699e-02,\n",
      "        -8.2168e-02,  2.9679e-02,  2.9754e-02, -1.9589e-02, -2.3867e-05,\n",
      "         2.9229e-03, -5.9795e-02,  1.0513e-01, -2.3250e-02,  1.5259e-02,\n",
      "        -9.9677e-04,  5.2436e-02,  4.5202e-02, -5.3536e-02, -3.1198e-02,\n",
      "         1.1600e-01,  8.2992e-02, -6.0462e-02, -6.9867e-02, -2.0561e-03,\n",
      "         6.2426e-02,  3.0686e-02,  7.3595e-03, -5.2512e-03, -8.7785e-02,\n",
      "         7.2232e-02, -5.5166e-02,  5.2830e-02, -3.4109e-02, -3.5072e-02,\n",
      "        -7.8913e-02,  3.6241e-02,  4.8680e-02, -2.4749e-02,  9.5748e-02,\n",
      "         1.1784e-01,  6.6303e-02, -3.3105e-02,  3.1397e-02,  4.8392e-02,\n",
      "        -9.6809e-02,  6.1331e-02,  3.0868e-02,  3.2937e-02,  1.4860e-02,\n",
      "        -8.8214e-02, -7.5167e-02, -2.6680e-02, -7.2619e-02, -3.8868e-02,\n",
      "         4.7005e-02, -1.5254e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-3.7034e-02, -4.5888e-02,  8.8781e-03,  2.7156e-02,  5.8858e-02,\n",
      "         1.2498e-03, -2.9473e-02, -2.4259e-02,  2.7695e-02,  4.8506e-02,\n",
      "        -1.3610e-02,  2.4264e-02, -1.0506e-02, -2.2343e-02, -1.2575e-02,\n",
      "        -2.7388e-02,  3.7047e-03, -9.8502e-02, -7.6187e-02, -1.3275e-02,\n",
      "         4.0868e-02,  3.1048e-02,  2.9744e-03, -3.4535e-02,  6.2692e-02,\n",
      "        -1.0555e-01, -1.8775e-03, -6.1323e-02,  1.1437e-02,  6.9841e-02,\n",
      "        -1.2952e-02,  7.9710e-02, -3.6756e-02,  1.2847e-02,  1.0407e-01,\n",
      "        -8.7324e-02, -1.0587e-01, -3.1902e-02, -8.2598e-03, -1.0516e-01,\n",
      "        -9.7262e-02,  1.1731e-02, -1.1542e-02, -1.0035e-01, -8.8628e-02,\n",
      "        -1.6604e-02, -6.6435e-04, -5.5660e-02, -5.8090e-03, -9.9288e-03,\n",
      "         2.7286e-02, -4.0562e-02, -1.3763e-02, -5.6210e-02, -8.2477e-03,\n",
      "         3.0968e-02, -2.2097e-02,  2.6884e-02, -4.4554e-03,  6.1624e-02,\n",
      "         5.7080e-02,  9.1388e-03, -2.2383e-02,  3.1594e-02,  9.0034e-02,\n",
      "         2.9283e-04, -2.4813e-03, -4.8279e-02,  2.9078e-02,  3.5868e-02,\n",
      "         4.1491e-02, -6.3660e-02, -8.4763e-02, -8.1597e-02, -5.1852e-02,\n",
      "         2.2601e-04,  1.0845e-01,  3.0973e-02, -1.5400e-01,  3.3164e-02,\n",
      "         7.9088e-02,  6.5250e-02,  5.1900e-02, -4.2283e-02, -1.1346e-01,\n",
      "        -9.0076e-03,  1.1980e-01, -1.1909e-02,  1.2310e-02,  1.8831e-02,\n",
      "        -4.9647e-02,  7.0969e-02, -2.3682e-02, -8.6618e-02,  5.2677e-02,\n",
      "         7.8079e-03, -1.0115e-01,  7.5915e-02, -4.8108e-02, -1.3128e-01,\n",
      "         6.4873e-02, -7.1029e-03, -1.4379e-01, -2.1432e-02, -5.3666e-02,\n",
      "         3.7874e-02, -8.1764e-02,  1.6618e-01,  7.1652e-02,  4.2189e-02,\n",
      "        -4.8112e-02,  5.0704e-02, -8.4332e-02,  2.3637e-02, -1.1713e-02,\n",
      "        -1.4738e-01, -5.6326e-02, -8.2328e-02, -6.9366e-03,  8.9393e-03,\n",
      "         9.0724e-02, -3.4346e-02, -1.7982e-02, -1.4817e-02, -9.2182e-02,\n",
      "         3.9414e-02, -1.3945e-02, -9.3391e-02,  1.0452e-01,  8.3443e-02,\n",
      "        -8.3101e-03,  5.8458e-02,  3.4724e-02, -9.1750e-02,  2.9846e-02,\n",
      "        -9.8895e-02, -2.4202e-02,  4.6580e-02,  4.4337e-02, -1.2447e-02,\n",
      "        -8.0480e-03, -5.6974e-03, -3.7265e-02,  7.7061e-02,  5.1464e-02,\n",
      "        -7.0224e-02, -4.4164e-02,  2.5564e-02,  1.2461e-02, -2.4537e-02,\n",
      "         2.2466e-02,  6.7765e-03, -2.1143e-02,  1.3173e-02, -4.8422e-02,\n",
      "        -2.4130e-02,  4.0795e-02, -6.9050e-02,  5.2960e-02,  2.9344e-02,\n",
      "         6.1323e-02,  2.6642e-02, -1.5501e-02,  1.1257e-02,  5.2199e-02,\n",
      "        -1.9131e-02, -7.1120e-02,  1.5206e-01, -5.5123e-02,  1.6600e-02,\n",
      "        -1.7471e-02,  5.4039e-02,  7.3465e-02, -1.4534e-02,  3.2988e-02,\n",
      "         1.0805e-01,  2.3235e-03,  2.6146e-02,  5.6207e-02,  2.4650e-02,\n",
      "         1.0190e-02, -4.5924e-03,  4.1432e-02, -4.8620e-02, -2.9034e-02,\n",
      "        -2.9012e-02,  1.4155e-02,  3.5942e-02, -9.4590e-03, -3.9627e-02,\n",
      "        -5.3268e-02,  1.3831e-01, -3.0257e-02, -5.7423e-03,  4.2466e-02,\n",
      "         1.2649e-01, -5.0767e-02, -1.1174e-02, -2.3112e-02,  3.8812e-02,\n",
      "        -6.3522e-02,  9.1453e-02,  2.6309e-02, -1.1686e-01, -3.9759e-02,\n",
      "         2.4578e-02, -4.7622e-03, -5.6869e-02,  9.6072e-02,  1.3556e-02,\n",
      "        -2.8459e-02, -4.5581e-02,  1.2914e-01, -1.1633e-02,  1.1193e-01,\n",
      "        -8.6753e-02, -8.5673e-03, -7.3127e-02, -3.6154e-02, -9.3040e-02,\n",
      "        -3.7462e-02,  1.2344e-01,  8.0146e-02, -1.7490e-02,  1.1924e-01,\n",
      "        -1.0738e-02,  6.7925e-02, -6.9445e-02, -2.5708e-02, -5.6665e-02,\n",
      "        -1.5419e-01,  1.2431e-01, -7.5615e-03, -1.0575e-01,  8.1955e-02,\n",
      "        -3.7937e-02,  8.6439e-02, -3.1533e-03,  1.4085e-01,  3.6980e-02,\n",
      "        -1.3440e-02, -5.1998e-02,  5.9634e-02, -4.4400e-02,  1.6468e-02,\n",
      "         3.7003e-02,  2.0843e-02,  4.8651e-02, -3.7829e-02,  1.0212e-01,\n",
      "        -1.8587e-02,  4.5990e-02, -4.5087e-03, -1.0517e-01, -7.8714e-02,\n",
      "        -2.2157e-02, -5.8386e-02,  7.0721e-02, -1.4240e-02, -1.0749e-01,\n",
      "        -6.8921e-02, -3.1443e-02, -3.2220e-02, -6.4972e-02,  1.1256e-02,\n",
      "         4.3494e-02,  1.8916e-02, -1.8547e-01, -2.1113e-02, -3.5792e-02,\n",
      "        -1.2145e-02,  4.6165e-02, -1.1010e-01,  3.3331e-04,  8.4547e-02,\n",
      "         5.4524e-02,  4.8118e-02, -9.5097e-02, -7.2445e-02, -6.6263e-05,\n",
      "         5.1787e-02,  4.9852e-02, -4.7932e-02, -1.2280e-02, -1.6250e-02,\n",
      "        -1.4342e-02, -1.1116e-01, -5.5778e-02, -7.7247e-03, -8.1662e-02,\n",
      "        -4.3206e-03,  6.6698e-02, -5.0373e-02, -1.2831e-01,  7.0735e-02,\n",
      "        -4.0484e-02, -2.6315e-02, -2.7391e-02, -8.0403e-02, -6.9732e-03,\n",
      "         5.4342e-02,  2.0656e-02,  1.5141e-01,  1.0275e-01,  1.5837e-03,\n",
      "        -1.4563e-01,  8.5911e-05,  4.7454e-03, -7.8300e-02,  4.8858e-02,\n",
      "        -2.1546e-02,  1.4427e-02,  4.6923e-02, -4.1582e-02,  3.4860e-02,\n",
      "         1.6094e-01, -2.8653e-02,  6.8671e-02,  3.9210e-02, -2.7989e-02,\n",
      "         1.2157e-01,  3.4874e-02,  1.0473e-01,  5.0698e-02, -6.6427e-02,\n",
      "        -8.5859e-02,  4.0868e-02, -8.1263e-02,  1.2227e-04, -4.1179e-02,\n",
      "         7.0834e-03,  8.5109e-02, -2.0567e-02,  6.0143e-03, -8.9583e-02,\n",
      "         6.3068e-02, -4.5089e-02,  2.6703e-02,  5.3511e-03,  9.8072e-03,\n",
      "         9.1949e-04,  4.8803e-02, -1.2944e-02, -1.6477e-02,  3.7466e-03,\n",
      "        -7.1968e-02, -6.9599e-02, -1.0072e-01, -7.0090e-02,  3.5817e-02,\n",
      "         6.2147e-02,  8.6350e-02,  8.2676e-02,  6.9734e-03, -1.6660e-01,\n",
      "         3.0636e-02, -7.5360e-02,  8.7070e-02,  4.6590e-02, -1.2240e-02,\n",
      "         4.7421e-02,  1.4499e-01, -3.2117e-02,  6.7256e-03, -9.1146e-03,\n",
      "         5.6627e-02,  3.4365e-02,  3.5674e-02,  1.1961e-03,  9.1195e-03,\n",
      "        -1.0258e-01, -2.6809e-02, -3.6439e-02, -5.3987e-02, -3.7285e-02,\n",
      "        -4.7299e-02,  2.0322e-02, -7.9408e-02, -7.7213e-02, -4.1219e-02,\n",
      "         1.1305e-01, -3.6860e-02,  3.4759e-02,  4.5197e-03, -1.8849e-02,\n",
      "        -1.1627e-02,  7.8283e-02, -5.6437e-02,  3.5024e-02,  6.2222e-02,\n",
      "        -8.2901e-02,  7.1049e-02,  9.9048e-03,  8.3881e-02,  3.7555e-03,\n",
      "         8.8532e-02,  9.2635e-02,  1.6246e-02, -3.0551e-02,  4.0173e-02,\n",
      "         3.9328e-02,  9.8969e-03,  7.2826e-04, -8.5527e-03,  1.9672e-02,\n",
      "         1.0268e-01, -4.0752e-03, -5.5843e-02,  1.5902e-02,  7.0855e-03,\n",
      "        -3.0325e-02,  2.9130e-02, -7.9757e-02,  2.0168e-02,  1.3599e-02,\n",
      "        -2.4822e-02, -8.0696e-03,  7.8805e-03,  3.1998e-04, -3.3752e-02,\n",
      "        -2.3653e-02,  7.4149e-02, -9.0394e-03, -6.5222e-03, -3.0573e-02,\n",
      "         1.1063e-01,  7.5828e-02,  4.1677e-02,  1.3911e-02, -7.0996e-03,\n",
      "         2.3597e-03,  2.6949e-03, -5.3042e-03,  7.1347e-02,  2.7978e-02,\n",
      "         9.5793e-04, -2.3873e-02, -7.2959e-02,  3.1148e-02, -6.5378e-02,\n",
      "         4.4773e-02, -4.6407e-02, -2.7808e-02,  6.0678e-02,  2.2824e-02,\n",
      "         1.2299e-02, -1.2252e-01, -9.4176e-02, -3.1335e-02,  6.1090e-02,\n",
      "        -8.9544e-02, -7.8463e-02, -1.0646e-01,  1.2856e-01,  5.3371e-02,\n",
      "        -3.5043e-02,  4.9204e-02, -2.7718e-02, -1.8169e-03, -3.2086e-02,\n",
      "         7.7823e-03,  6.8141e-03,  9.3693e-02,  1.6695e-02, -7.0995e-03,\n",
      "        -8.1406e-02, -1.0529e-02,  2.3930e-02, -2.4667e-02,  1.4599e-02,\n",
      "         2.2815e-02,  6.4431e-02, -8.6203e-02, -1.9157e-01,  3.7300e-02,\n",
      "        -2.8549e-02, -2.9900e-02,  2.0874e-02, -1.8929e-01,  6.7435e-02,\n",
      "        -4.1862e-02,  4.9628e-04,  7.5833e-03,  8.0471e-02, -1.7851e-02,\n",
      "        -4.5390e-02,  2.1833e-02, -1.6886e-02, -1.0043e-02, -7.4905e-02,\n",
      "        -9.9795e-04, -2.0626e-02,  8.3278e-02, -7.4464e-02,  3.2107e-02,\n",
      "         4.9412e-02, -5.9202e-02, -6.2015e-02,  1.0825e-02,  8.4142e-02,\n",
      "         6.0584e-02,  2.8453e-02, -6.4364e-02,  3.4312e-02, -3.1387e-02,\n",
      "        -1.0054e-02,  6.2364e-02,  9.9319e-02,  4.8268e-02,  3.6428e-02,\n",
      "         4.7602e-02, -2.9711e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 1.7651e-03,  3.1396e-03, -1.4551e-02, -3.5147e-03, -2.1400e-02,\n",
      "         1.1861e-01,  2.7863e-02, -5.9323e-02,  5.4408e-02, -3.9685e-02,\n",
      "        -2.9819e-02,  1.3290e-02, -3.3680e-02, -3.0514e-02, -9.5577e-02,\n",
      "        -2.7275e-02, -3.2411e-02,  1.0115e-01,  1.8278e-02,  5.4167e-02,\n",
      "        -6.5277e-02, -4.9623e-02,  3.8521e-02, -7.7113e-02,  2.3679e-02,\n",
      "        -2.1421e-02, -3.1328e-02, -3.8675e-02,  3.2301e-02,  8.0595e-02,\n",
      "        -1.7031e-01, -3.1086e-02,  7.4435e-02,  4.4086e-02, -5.2982e-02,\n",
      "        -3.7618e-02, -2.1757e-02,  2.6901e-02,  1.2416e-02, -6.1151e-02,\n",
      "        -5.6076e-03, -8.1322e-02,  1.2883e-01,  2.1244e-01,  6.3023e-03,\n",
      "        -6.4486e-02, -6.8903e-02, -5.2496e-02, -8.8419e-03, -4.8330e-03,\n",
      "        -9.8466e-02, -9.1724e-02, -4.6670e-03,  2.2265e-02,  7.4199e-03,\n",
      "        -6.7150e-03,  3.1992e-03, -1.9731e-02,  1.7806e-02, -7.7666e-03,\n",
      "         6.6665e-03, -4.9659e-03,  1.3266e-02, -3.1188e-02,  6.3222e-02,\n",
      "         2.4398e-02,  2.9437e-02, -7.7957e-04,  2.4054e-02,  1.6580e-01,\n",
      "        -7.9211e-02, -2.8934e-02,  3.4830e-02,  3.1386e-02,  1.2014e-03,\n",
      "         9.4098e-02, -5.5012e-03,  4.5756e-02,  3.2991e-02,  1.8693e-02,\n",
      "         4.4928e-02, -2.1605e-02,  4.0092e-02, -6.9511e-02, -8.6237e-02,\n",
      "        -1.2794e-01,  2.8559e-02, -3.4364e-02,  2.3834e-03,  9.2352e-03,\n",
      "        -2.0991e-03,  1.2794e-02,  1.0197e-02, -1.4751e-02, -5.3813e-03,\n",
      "         2.9286e-02,  8.8126e-02,  1.5448e-02, -1.4078e-02, -3.9143e-02,\n",
      "        -5.8560e-02,  5.4407e-02,  3.5490e-02, -7.9659e-02,  3.4453e-02,\n",
      "         2.5864e-02, -3.0899e-02, -1.5625e-02, -4.5447e-02,  4.7464e-02,\n",
      "        -3.1091e-02, -3.4445e-02, -5.4052e-02, -6.4918e-02,  4.4487e-02,\n",
      "         2.5045e-02, -1.1488e-02,  2.5262e-02, -1.2607e-02,  1.3235e-02,\n",
      "         2.8561e-02,  6.9778e-02,  3.5717e-02, -1.3796e-02, -1.6055e-01,\n",
      "        -6.3508e-02,  3.0388e-02,  3.6702e-02,  1.4510e-02,  8.2649e-02,\n",
      "        -9.6217e-03,  2.8959e-02,  3.8684e-02, -8.4300e-02, -1.5368e-01,\n",
      "         9.8709e-02, -7.2473e-02,  3.1997e-02,  1.1817e-01, -2.6140e-02,\n",
      "        -6.1742e-02, -1.6166e-02,  7.0216e-02, -1.2530e-01, -3.3601e-02,\n",
      "         1.8504e-02,  4.9253e-02,  1.5496e-01, -7.7431e-02, -1.4273e-02,\n",
      "        -1.3381e-02,  1.0467e-01, -7.3973e-02, -9.8395e-02, -2.9553e-02,\n",
      "         4.8231e-02,  6.4982e-02, -5.0469e-02,  3.5893e-02,  9.4489e-02,\n",
      "         6.2196e-02, -9.2381e-02, -8.7598e-02,  7.9401e-02, -6.6444e-02,\n",
      "        -1.0009e-02, -3.8275e-02, -2.5270e-02, -1.7952e-01, -9.5267e-03,\n",
      "        -1.3783e-01,  2.1312e-01, -1.1740e-02, -8.2986e-02,  3.5087e-02,\n",
      "        -1.9155e-02, -2.4328e-02, -4.0487e-02,  3.3686e-02, -1.7021e-02,\n",
      "        -5.0354e-02, -1.5596e-01, -1.7125e-03,  5.6674e-02,  6.6230e-03,\n",
      "         6.4058e-03, -3.7337e-03,  1.1259e-02, -2.4012e-02,  8.4532e-02,\n",
      "        -2.1994e-02,  3.6341e-03,  8.1102e-02, -5.8442e-02,  9.7022e-02,\n",
      "        -6.0901e-02,  5.0808e-02,  1.3352e-01,  1.6406e-02,  1.3148e-02,\n",
      "         2.8686e-02, -3.0704e-02, -4.3113e-02,  5.2098e-02, -5.5051e-02,\n",
      "        -1.1791e-01,  5.0002e-02,  2.3706e-03, -6.4074e-02,  5.0139e-02,\n",
      "        -3.7592e-02,  5.3099e-02,  3.9144e-02,  4.3691e-03,  1.4775e-02,\n",
      "        -7.3321e-02, -4.6698e-02,  1.2764e-01, -6.2895e-02, -2.6595e-02,\n",
      "         7.9530e-02,  3.6950e-02, -4.7796e-03,  3.2136e-02, -4.4875e-02,\n",
      "        -3.2131e-02,  8.3086e-02,  8.9513e-02, -6.2051e-03, -1.2118e-01,\n",
      "         2.6485e-02, -3.3139e-02,  4.4756e-02,  7.8008e-04,  7.1055e-02,\n",
      "         3.0050e-02,  8.2575e-03, -2.6538e-02, -3.9907e-02, -2.5800e-02,\n",
      "        -3.3800e-02,  1.8517e-02, -7.0688e-02, -1.3011e-01, -3.3101e-02,\n",
      "        -5.4424e-02,  3.0215e-02, -6.2839e-02,  2.4651e-02, -1.8812e-03,\n",
      "        -1.3442e-01,  1.2847e-02,  7.9453e-02,  8.0802e-02, -9.5993e-02,\n",
      "         3.4160e-02,  2.6102e-02, -8.6553e-03,  5.7268e-02,  8.5350e-02,\n",
      "         1.3918e-02,  1.1504e-02,  2.9779e-03,  1.0623e-02,  5.5536e-02,\n",
      "        -4.1146e-02, -9.3039e-02, -3.3455e-03,  1.5882e-02, -1.5050e-01,\n",
      "         7.5856e-03,  2.2823e-02, -3.8871e-02,  5.5844e-02,  5.4641e-03,\n",
      "        -2.4733e-02, -5.1179e-02, -1.8616e-02,  5.5658e-02, -6.9583e-02,\n",
      "        -6.0925e-02, -8.0161e-02, -1.0143e-01,  4.3837e-02,  1.3554e-01,\n",
      "         8.7156e-02,  2.5922e-02, -7.2726e-02, -1.8920e-02,  9.7482e-02,\n",
      "         2.0591e-02, -6.2224e-02,  5.4904e-02, -1.3960e-01, -7.6254e-02,\n",
      "         8.3799e-02, -3.9226e-02, -4.3723e-02, -3.3469e-02,  9.1810e-03,\n",
      "         4.9622e-02,  6.3080e-02, -2.8480e-02, -1.8700e-02,  6.6885e-02,\n",
      "        -6.8625e-03,  7.1043e-02,  7.1088e-02, -9.2783e-02,  9.1262e-02,\n",
      "         4.6247e-02, -2.9005e-02,  2.8690e-02,  1.9394e-02,  5.7164e-05,\n",
      "         2.2624e-02,  3.3163e-02,  1.7700e-02,  3.4232e-02, -2.9858e-02,\n",
      "        -7.4267e-02,  3.6014e-02, -4.4552e-02,  3.5258e-02, -1.0101e-01,\n",
      "        -6.7129e-03,  1.4119e-02,  2.7532e-02,  1.8333e-02,  1.0998e-01,\n",
      "        -4.3879e-04,  6.3078e-02,  1.9749e-02,  4.5188e-02,  1.7698e-02,\n",
      "        -1.6677e-02,  8.2497e-02, -7.5923e-02,  6.3407e-02,  6.3229e-02,\n",
      "         1.7209e-02,  8.9937e-02, -3.1758e-02,  2.4061e-02, -7.6937e-02,\n",
      "         2.9163e-03, -6.6448e-02, -1.3663e-02, -3.8498e-02, -6.1970e-02,\n",
      "        -5.3004e-02,  2.5560e-02,  1.7372e-01,  1.9347e-02,  7.7611e-02,\n",
      "         1.2019e-01, -1.5177e-01, -1.0369e-02, -3.0696e-02,  6.5096e-02,\n",
      "         1.3015e-02,  5.4550e-02, -5.5283e-02,  7.5891e-03, -2.0863e-02,\n",
      "        -2.2272e-02,  1.8210e-02, -6.6587e-03, -1.3865e-02,  5.7003e-02,\n",
      "        -1.9093e-02,  9.1872e-03,  9.9067e-02,  3.3590e-04,  4.0905e-02,\n",
      "         2.1044e-03, -6.7002e-03,  2.9374e-02, -1.1736e-02,  3.6019e-03,\n",
      "        -2.4367e-02, -3.7626e-02, -1.1231e-01,  1.7375e-02, -6.0035e-03,\n",
      "         5.7686e-02, -2.7193e-02,  1.9783e-02, -6.3263e-02,  2.2237e-02,\n",
      "         7.3779e-03, -2.8759e-03, -1.5603e-02,  5.7662e-02,  8.7457e-03,\n",
      "         1.0018e-02, -5.0072e-02, -3.7638e-02,  2.4585e-02, -9.2793e-02,\n",
      "        -1.1872e-01, -2.2116e-02, -1.0956e-01, -1.0836e-01,  8.6403e-02,\n",
      "         4.3467e-02,  2.0700e-02,  5.1945e-02,  3.0060e-02,  2.7744e-02,\n",
      "        -9.2273e-03,  7.5827e-02, -4.5446e-02,  8.2869e-02, -9.2931e-02,\n",
      "         1.2670e-02, -4.8179e-02, -1.5450e-01, -1.6038e-03, -2.9253e-02,\n",
      "        -2.7980e-02, -1.0475e-02,  2.7516e-02,  1.6998e-01,  2.4017e-02,\n",
      "         8.4535e-02, -2.9163e-04,  3.1187e-02,  5.4309e-02, -3.0479e-02,\n",
      "        -8.0611e-02, -6.6498e-02, -1.4551e-01,  2.1430e-03, -3.7552e-02,\n",
      "         7.6690e-02,  7.4113e-02, -1.0557e-01, -7.4909e-02,  6.7211e-02,\n",
      "        -7.8306e-02,  4.8829e-02, -4.6191e-02,  1.3408e-02,  2.7609e-02,\n",
      "        -7.1487e-03, -3.2693e-03,  6.9174e-02,  1.8630e-01,  8.2350e-02,\n",
      "        -7.1999e-02, -5.7636e-04, -1.0190e-01, -2.1849e-02, -2.4579e-02,\n",
      "         8.7751e-02, -3.5942e-02, -2.4704e-03, -1.1202e-01, -7.7516e-02,\n",
      "        -3.0877e-02,  5.2970e-02, -1.0476e-02,  9.5842e-03, -7.3720e-02,\n",
      "         4.0108e-02, -1.2442e-02,  3.8677e-02, -4.2649e-02,  3.2528e-02,\n",
      "         4.7383e-02, -1.2851e-03, -3.1630e-02,  9.8758e-02, -4.5205e-02,\n",
      "         9.8402e-02, -9.2297e-02, -1.9997e-02, -1.7744e-02, -2.2326e-02,\n",
      "         8.0307e-02, -1.7815e-02,  1.9394e-02, -5.2028e-02, -5.1993e-02,\n",
      "         5.9033e-03,  1.0825e-02, -1.7139e-02, -1.4043e-01,  3.3729e-02,\n",
      "         2.4079e-02,  3.1476e-02, -7.7750e-02,  4.4037e-03, -7.0054e-02,\n",
      "        -1.0412e-02,  8.7667e-03,  6.4475e-02, -6.7967e-02,  4.3379e-02,\n",
      "        -8.0798e-02,  1.3300e-01, -2.5715e-02,  4.1997e-02,  1.5607e-02,\n",
      "        -1.8457e-02, -1.4307e-02, -1.3592e-02, -8.4850e-04,  6.9601e-03,\n",
      "         1.7143e-02, -7.7591e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 7.0232e-02, -3.9177e-02, -5.6618e-02,  2.4083e-02, -1.1208e-01,\n",
      "        -8.1959e-02, -1.0443e-02,  3.7170e-02, -5.0199e-02,  3.4944e-02,\n",
      "        -1.1758e-01,  2.1176e-02, -1.3711e-02,  1.7198e-03,  5.5355e-03,\n",
      "        -2.0753e-02, -4.9122e-02, -1.6634e-02,  8.6761e-04,  5.3412e-02,\n",
      "        -5.2480e-02,  3.0198e-03, -2.5701e-02, -1.2355e-01,  5.1328e-02,\n",
      "        -6.1343e-02, -4.0117e-02,  6.6909e-02,  5.3694e-03,  2.3511e-02,\n",
      "        -8.8978e-03, -1.3987e-02,  5.8384e-02, -7.4027e-02,  7.0214e-03,\n",
      "        -2.3619e-02, -1.1647e-02,  5.0248e-03, -1.0472e-01,  5.0924e-02,\n",
      "         9.6729e-03,  4.4740e-02, -8.4304e-03,  3.1834e-02, -5.6028e-02,\n",
      "         1.2579e-02,  4.1046e-02, -7.6398e-02, -8.2215e-02, -7.4826e-02,\n",
      "        -5.8703e-03,  2.1899e-02, -3.4924e-02, -9.3841e-02,  9.2489e-02,\n",
      "        -3.9724e-02,  6.8778e-02,  6.2910e-02,  1.5035e-01, -8.5688e-02,\n",
      "         5.1889e-02, -9.3605e-02, -7.0402e-02,  4.7219e-02,  5.9798e-02,\n",
      "        -3.6312e-03, -1.3177e-02, -4.6579e-02,  2.6072e-02, -1.8031e-02,\n",
      "        -1.5455e-01,  1.6608e-01, -1.5167e-03, -2.2081e-02, -3.3239e-02,\n",
      "         7.1615e-03,  5.0772e-02,  6.4464e-03, -1.0717e-03,  1.1329e-01,\n",
      "        -4.0795e-03,  7.9883e-02, -4.3044e-02,  1.3580e-01, -1.0705e-02,\n",
      "         7.0666e-03, -7.1443e-03,  9.1426e-02, -1.3554e-03, -9.4658e-02,\n",
      "        -4.0040e-02,  5.9643e-02,  1.8720e-02, -6.1085e-03, -5.1143e-03,\n",
      "         5.2426e-03,  3.9795e-02,  5.7733e-02,  9.3336e-02,  4.6847e-03,\n",
      "         5.8702e-02,  2.5341e-02,  4.2893e-02,  7.5947e-02,  2.8520e-04,\n",
      "         7.1536e-03, -4.3884e-03, -4.4555e-02, -4.4503e-02,  5.6192e-02,\n",
      "        -5.1656e-02, -1.2393e-01,  4.3672e-02,  4.7996e-02, -1.0800e-02,\n",
      "         5.6755e-02, -8.3568e-02, -1.3538e-02, -5.6153e-02, -5.5316e-02,\n",
      "        -2.3975e-02, -1.1282e-01, -2.8566e-02,  7.2766e-02, -3.8624e-02,\n",
      "         7.6615e-02,  3.6164e-02,  1.0354e-01,  4.9160e-02,  1.9378e-02,\n",
      "        -2.2329e-02, -1.2350e-01,  1.2831e-01,  9.7161e-03,  8.3806e-02,\n",
      "        -5.0945e-02, -2.3909e-02, -2.4867e-02,  5.3618e-02,  4.3033e-02,\n",
      "        -8.6281e-03, -3.7764e-02, -1.2432e-01,  1.3901e-02, -8.2746e-02,\n",
      "         1.5292e-02, -1.0102e-01, -2.1163e-03,  2.4047e-02, -3.3842e-02,\n",
      "         1.7279e-01, -2.0493e-03, -1.4493e-02,  5.7667e-02, -2.8942e-02,\n",
      "        -3.2882e-03,  7.1961e-02,  1.5763e-02, -1.0857e-01,  3.1682e-02,\n",
      "        -1.5458e-02,  2.3903e-02, -7.8493e-02,  3.3385e-02, -1.1762e-02,\n",
      "         5.4726e-02, -1.0496e-01, -1.9116e-02,  4.4039e-02, -4.5159e-02,\n",
      "         1.1691e-01, -7.5459e-02, -3.4751e-02, -7.0932e-05, -5.4284e-03,\n",
      "        -3.1645e-02,  7.8052e-02, -1.3927e-02, -3.9138e-02, -6.9432e-02,\n",
      "        -5.6814e-02,  4.7092e-02, -9.7913e-02, -7.1706e-02, -7.4354e-02,\n",
      "         2.9061e-02,  1.2788e-01,  3.2878e-02,  6.8620e-02,  7.8050e-03,\n",
      "        -8.1034e-03,  1.2591e-01, -2.5306e-02,  2.3245e-02,  6.0525e-03,\n",
      "         5.1102e-02,  2.6583e-02, -2.1282e-03, -5.5411e-02,  4.6495e-02,\n",
      "        -2.4725e-02,  2.2852e-02, -1.2736e-02,  1.6637e-01, -5.4719e-02,\n",
      "         8.6107e-02, -5.4407e-02,  6.8237e-02, -6.1891e-02, -5.5849e-02,\n",
      "         7.3760e-03, -3.0345e-02, -3.1600e-02,  3.3583e-02,  2.8570e-02,\n",
      "         8.2200e-02,  2.6655e-02,  2.6249e-02, -1.2001e-02,  7.8356e-02,\n",
      "        -1.6183e-02, -1.4890e-02,  1.2511e-02,  3.7454e-02,  2.5717e-02,\n",
      "         2.4392e-03,  1.9375e-02,  6.4533e-02,  3.3817e-02, -6.6789e-02,\n",
      "        -8.1340e-02, -3.5166e-02, -2.8866e-02, -7.5490e-02,  3.9034e-02,\n",
      "        -4.9257e-02,  1.5981e-02,  1.2176e-02,  2.2973e-02,  1.0207e-02,\n",
      "        -1.0285e-03,  1.7862e-01,  6.4228e-02, -3.5339e-02,  8.1926e-02,\n",
      "         7.1711e-02, -1.0528e-02,  3.6034e-02, -2.3140e-02,  4.6343e-02,\n",
      "        -3.3368e-02, -4.6355e-02,  5.1168e-02,  1.8313e-02, -3.8195e-03,\n",
      "         1.0237e-01, -4.0303e-02,  3.3172e-02, -4.5773e-02, -1.4106e-02,\n",
      "        -3.1364e-02,  5.1665e-02, -3.1724e-02,  3.0433e-02, -3.9412e-02,\n",
      "        -3.3040e-02,  2.1146e-02, -1.1771e-01, -6.6739e-02, -3.3981e-02,\n",
      "        -4.4390e-03,  2.8506e-02,  1.9362e-02,  1.0839e-01, -1.5109e-02,\n",
      "         1.5135e-01, -2.9912e-02,  7.0132e-02, -6.2905e-02, -9.2045e-02,\n",
      "        -1.0811e-01, -6.4596e-02,  1.1569e-01,  4.2324e-02, -5.3588e-02,\n",
      "         2.2440e-02, -2.2649e-02, -7.6581e-02, -6.1811e-02,  6.7117e-02,\n",
      "        -8.8734e-02,  1.1926e-02, -1.0264e-02,  2.1893e-02,  6.1756e-02,\n",
      "         1.1959e-01, -9.6380e-02,  1.3470e-02, -7.0965e-02,  2.2478e-02,\n",
      "         5.0166e-02, -4.6788e-03,  9.3105e-02,  1.2183e-01, -1.0024e-01,\n",
      "         1.9777e-04,  8.1114e-02, -2.6921e-02,  1.0334e-01, -3.6504e-02,\n",
      "         1.0802e-02, -2.5081e-02, -6.5181e-02,  8.6339e-02,  3.7305e-02,\n",
      "        -1.2546e-01, -2.3171e-02, -5.1505e-02,  8.1840e-02,  4.9002e-02,\n",
      "         1.8363e-02,  2.9693e-02, -3.8902e-03, -4.1257e-02, -2.2935e-02,\n",
      "         8.3203e-02,  5.9329e-02,  7.7033e-03,  4.9673e-02, -3.4751e-02,\n",
      "        -3.4831e-03, -7.7208e-03,  1.0457e-01, -2.1170e-02,  6.3125e-02,\n",
      "        -1.2047e-02,  1.4499e-02, -5.0847e-02,  2.7684e-02,  8.1270e-02,\n",
      "        -2.4067e-02,  1.6061e-04,  4.5172e-02,  8.9830e-02,  5.0638e-03,\n",
      "        -2.7056e-02,  1.6215e-02, -1.2409e-01,  2.7129e-02, -3.0758e-02,\n",
      "        -4.1683e-02,  7.6068e-03,  1.4988e-02, -2.3955e-02, -8.0970e-02,\n",
      "        -1.0192e-01,  3.6965e-02, -2.6476e-02,  5.7144e-03,  7.6527e-02,\n",
      "         9.9065e-02,  4.3809e-02,  5.6087e-02, -6.8878e-02,  6.4834e-02,\n",
      "         2.2787e-02,  7.3976e-02,  9.9496e-03, -1.7695e-02,  8.8900e-02,\n",
      "        -9.6980e-02, -7.0818e-02,  4.9335e-02, -5.5873e-02, -6.7333e-03,\n",
      "        -1.0160e-02,  2.7102e-02, -2.2473e-02, -4.0724e-02, -4.3555e-02,\n",
      "        -4.3084e-02, -8.1544e-02, -2.8473e-02,  1.8932e-02,  2.8450e-02,\n",
      "        -8.1699e-02, -8.1030e-02,  4.9583e-02,  3.5871e-02, -1.5891e-02,\n",
      "        -8.8298e-03, -1.2130e-02, -8.1447e-02,  4.5123e-02,  6.6769e-02,\n",
      "         4.5007e-02, -1.0901e-02, -1.5257e-01,  1.2816e-02,  6.9188e-02,\n",
      "        -1.3537e-02,  1.0406e-01,  2.4015e-02,  4.3749e-02, -2.3074e-02,\n",
      "         6.2925e-02, -3.2508e-02, -5.4690e-02,  1.6847e-02, -5.7822e-02,\n",
      "         6.0195e-02, -1.6088e-02, -3.6611e-02,  4.3164e-03, -1.6129e-02,\n",
      "        -2.0027e-02,  2.7187e-02, -5.7546e-02,  1.6556e-02, -4.5320e-04,\n",
      "         2.4637e-02, -5.7647e-02, -4.5837e-02,  1.4810e-02,  1.4818e-02,\n",
      "        -9.2751e-03,  3.1316e-02,  4.6298e-02,  1.5679e-02,  2.5335e-02,\n",
      "         1.5162e-02, -6.5274e-02, -1.1448e-01,  3.5900e-02, -1.1034e-01,\n",
      "        -9.4011e-02,  3.3696e-02, -6.7059e-03,  1.4441e-02,  1.3973e-01,\n",
      "         7.2340e-02, -5.2067e-02, -1.5580e-02,  4.3312e-02, -6.7398e-02,\n",
      "         7.6808e-02, -4.1142e-02,  3.2319e-02,  1.2461e-01,  1.5610e-02,\n",
      "         7.3369e-02, -1.0851e-01, -4.5686e-02, -6.5544e-02,  7.0161e-02,\n",
      "        -4.9590e-03,  4.6399e-02,  4.5816e-02, -7.6833e-02,  5.7388e-02,\n",
      "         5.6216e-02,  1.7794e-02, -1.8920e-02, -4.4150e-02,  2.6347e-02,\n",
      "         8.7239e-02, -2.0536e-02, -1.2006e-02, -5.0354e-03,  3.5649e-02,\n",
      "        -8.1056e-02,  5.1311e-02,  1.9925e-02, -4.3425e-02,  2.6601e-02,\n",
      "        -7.5502e-02, -3.4638e-02, -7.5277e-02, -5.1211e-02, -4.9907e-02,\n",
      "         1.9271e-02,  2.3710e-02,  1.7192e-02, -7.7708e-02,  2.5729e-02,\n",
      "         5.5325e-02,  1.0182e-01, -9.2568e-02, -4.8824e-02,  2.3749e-02,\n",
      "         3.6623e-02, -1.6246e-02, -2.5600e-02, -8.7405e-02,  1.7550e-02,\n",
      "        -6.1699e-03, -4.0138e-02, -3.5954e-02, -6.4890e-02,  4.1684e-03,\n",
      "        -8.0014e-02, -7.6652e-02,  9.0478e-02,  2.2696e-03,  4.3178e-03,\n",
      "         1.3625e-01, -4.3848e-02,  3.4243e-02,  1.0695e-01,  2.2553e-02,\n",
      "        -1.3336e-02, -3.6943e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-6.3037e-02,  4.0266e-02, -4.1982e-02, -1.3677e-01, -2.8125e-02,\n",
      "        -3.5907e-02, -1.2441e-01,  3.9413e-02,  7.0257e-02,  2.5938e-02,\n",
      "         1.6168e-02, -4.9772e-03, -8.1783e-02,  5.8682e-02, -5.5678e-03,\n",
      "        -1.6609e-02, -7.0250e-02,  2.3007e-02, -1.1237e-01, -1.1362e-03,\n",
      "         2.6813e-02,  5.1206e-02, -1.0730e-01, -7.5571e-03,  6.9360e-02,\n",
      "        -3.0615e-02, -1.3997e-01, -1.6990e-02, -6.2863e-03,  3.9499e-02,\n",
      "         1.1053e-01,  7.3368e-02, -4.2755e-03, -6.1965e-02,  2.3791e-02,\n",
      "        -5.2535e-02, -2.8180e-02, -1.5272e-02, -2.4229e-02,  8.8722e-02,\n",
      "        -9.6024e-02,  4.9923e-02, -1.7209e-02,  1.6388e-02, -7.5840e-03,\n",
      "        -8.1901e-02,  6.1073e-02, -4.8348e-02, -1.0459e-02, -6.6470e-02,\n",
      "        -2.4781e-02,  5.3203e-02,  2.4020e-02,  4.9423e-02,  3.7947e-02,\n",
      "         2.0673e-01,  6.0895e-02, -4.0758e-02,  2.0071e-02,  1.0387e-01,\n",
      "         1.6163e-02, -1.5090e-02,  6.2854e-02,  2.6011e-02,  3.8561e-02,\n",
      "        -5.5162e-02, -7.5800e-02,  6.2836e-02,  1.3994e-02,  1.1939e-01,\n",
      "        -1.5908e-02,  4.2999e-02, -2.3383e-02,  1.5983e-02, -1.3671e-02,\n",
      "         9.9919e-02,  6.4056e-02, -7.5082e-02, -1.2190e-02, -1.4694e-02,\n",
      "         5.8069e-02,  5.5209e-02,  1.2079e-02,  5.1134e-02,  4.5578e-02,\n",
      "         5.2929e-02, -6.7412e-03,  9.7755e-02, -4.7786e-02, -1.6850e-02,\n",
      "        -5.9766e-02,  9.2122e-02, -2.9754e-02, -1.1698e-01,  2.3706e-02,\n",
      "        -2.3814e-02,  3.3031e-02, -1.1580e-01,  3.8596e-02, -3.3136e-03,\n",
      "        -1.2250e-02,  3.4611e-02, -8.4193e-02, -7.5750e-02, -3.5521e-02,\n",
      "        -5.1473e-03,  3.9007e-02,  6.4325e-03, -5.9280e-02, -1.3100e-02,\n",
      "        -4.1139e-02,  4.7848e-02,  8.4264e-03, -1.0753e-01, -4.3760e-02,\n",
      "         9.4994e-02, -1.7219e-02,  3.9596e-02, -4.1659e-02,  1.2531e-01,\n",
      "        -4.9070e-02,  1.2569e-02, -3.4510e-02,  5.6004e-02, -2.7773e-02,\n",
      "        -8.0413e-02,  7.7013e-02, -3.7365e-02, -7.8601e-02, -4.4590e-02,\n",
      "         1.6158e-02,  2.7064e-02,  1.0510e-01, -1.2408e-02,  1.6963e-02,\n",
      "        -9.1978e-03,  5.7486e-02, -4.6821e-02, -3.0573e-03, -1.0964e-02,\n",
      "        -8.9452e-02,  4.2682e-02, -1.1941e-02,  2.5132e-02, -3.7705e-02,\n",
      "         5.4186e-02, -7.1975e-02, -4.9173e-02, -6.7192e-02,  2.7494e-02,\n",
      "         2.4167e-03,  3.7371e-02,  4.2284e-02,  3.3118e-02,  5.1909e-02,\n",
      "        -6.6921e-02, -5.8869e-02, -6.1932e-02,  3.1455e-02, -2.2885e-02,\n",
      "        -9.3647e-02, -1.9637e-02,  5.1098e-02,  4.5610e-02, -4.1068e-02,\n",
      "         5.7816e-02, -8.5963e-04,  2.2186e-02, -1.8173e-02,  4.3025e-02,\n",
      "        -3.6500e-02,  4.6611e-02,  1.1417e-01, -6.0109e-02, -6.6532e-02,\n",
      "         9.2543e-02,  1.5739e-02, -7.0260e-03, -4.5298e-02, -4.6085e-02,\n",
      "        -1.7641e-02, -3.4245e-02, -2.9982e-02, -3.3564e-02, -2.3251e-02,\n",
      "        -9.0132e-02, -4.9113e-02, -1.5003e-02, -3.4544e-02, -1.2240e-02,\n",
      "        -6.6013e-02, -1.2225e-01,  2.1974e-02, -7.2869e-02,  7.3213e-02,\n",
      "         7.8171e-02, -1.1407e-02,  1.2900e-02,  1.3423e-02,  6.1885e-02,\n",
      "         8.2777e-02,  5.9639e-03, -2.9608e-02,  1.4335e-02, -3.0911e-02,\n",
      "        -2.6568e-02, -7.7970e-02,  5.7262e-02,  7.5148e-03, -8.3736e-02,\n",
      "         1.1164e-01, -3.6595e-02, -3.5647e-02, -2.2155e-02,  3.7071e-02,\n",
      "         5.2191e-03, -5.0187e-02, -1.0465e-02, -2.7389e-02,  2.4710e-02,\n",
      "         3.4442e-02, -3.3596e-02,  5.7857e-02,  4.2296e-02, -2.8121e-02,\n",
      "         3.7366e-02, -5.9914e-02,  1.6653e-02,  3.8050e-02,  5.3976e-02,\n",
      "         1.6561e-02,  5.0949e-02,  7.7352e-02,  8.3561e-02, -4.3670e-02,\n",
      "        -8.8957e-03,  2.0743e-03,  3.0768e-02, -3.4656e-02,  1.0132e-01,\n",
      "         2.0802e-02, -1.4734e-01, -1.1625e-02, -4.6762e-03,  1.0868e-01,\n",
      "         8.2071e-02, -1.4927e-02, -1.5449e-01, -7.1360e-02,  6.3504e-02,\n",
      "        -1.3678e-02, -3.2650e-02,  8.5200e-02, -4.5086e-02,  2.2611e-02,\n",
      "        -1.0392e-01, -6.0944e-02,  1.4738e-02,  3.9227e-02, -8.7592e-03,\n",
      "        -2.2234e-02, -5.5263e-03, -3.3027e-02,  3.9625e-03,  1.5417e-02,\n",
      "         1.2909e-02,  1.0592e-01, -5.5637e-02,  1.6255e-01, -8.2178e-02,\n",
      "         9.2043e-02,  1.9381e-03,  2.2714e-02,  3.5822e-02, -1.0901e-03,\n",
      "         1.2325e-02, -6.4859e-02, -2.5885e-02,  5.1314e-02, -4.6941e-04,\n",
      "        -2.8895e-03,  1.1293e-02, -1.7513e-02, -6.6949e-02,  6.9416e-02,\n",
      "         7.1142e-03, -1.4641e-03, -3.6779e-02,  1.1385e-01, -4.7641e-02,\n",
      "         1.4738e-02, -6.2718e-02,  8.7415e-02, -5.5629e-03,  2.7129e-02,\n",
      "         6.3722e-03,  3.4799e-02,  2.5760e-02, -7.6286e-02, -6.1321e-02,\n",
      "        -5.3081e-02, -1.3048e-03, -1.7442e-02, -1.6667e-01,  3.7299e-03,\n",
      "         1.3328e-02,  6.2362e-02,  1.6265e-02,  5.9280e-02, -9.6899e-02,\n",
      "        -9.9530e-03,  3.4732e-02,  5.4185e-03, -4.3835e-03,  3.4801e-02,\n",
      "         4.0341e-02, -1.1303e-02, -2.8805e-02,  2.6510e-02, -4.8988e-02,\n",
      "        -1.4906e-02, -8.7503e-02, -3.8591e-03,  3.9093e-02,  2.1345e-02,\n",
      "         4.3803e-02, -4.8825e-02, -3.8691e-02, -7.1864e-02, -5.9994e-02,\n",
      "         2.5898e-02, -4.4769e-02,  8.8324e-02, -7.2772e-02,  1.5155e-02,\n",
      "        -5.5817e-02,  5.3736e-02, -2.9101e-02,  1.5793e-03, -1.7930e-01,\n",
      "        -1.7445e-02, -6.8678e-02, -2.1378e-02, -4.4950e-02, -1.7106e-02,\n",
      "         1.5411e-01,  7.0336e-02,  3.1394e-02,  9.1400e-02, -6.3379e-02,\n",
      "         9.3097e-02,  6.2873e-02, -2.3895e-02,  4.8823e-02,  1.5050e-02,\n",
      "         1.5749e-01,  2.0483e-02,  2.5478e-02,  1.2565e-01,  6.4963e-02,\n",
      "        -3.3720e-02,  3.8453e-02, -6.7775e-02, -1.1753e-01,  6.8093e-02,\n",
      "         5.1249e-02, -1.5064e-01, -6.5369e-02,  4.8224e-02, -8.1458e-03,\n",
      "        -2.7762e-02, -2.5249e-02, -1.0149e-02, -1.9384e-02,  4.1005e-02,\n",
      "        -2.7609e-02, -9.2976e-02,  3.8276e-02,  7.2089e-02, -1.2936e-01,\n",
      "        -1.1778e-01, -6.5505e-02,  1.7166e-02,  1.5751e-02, -1.9162e-02,\n",
      "         5.5185e-03, -1.0558e-01, -2.3025e-02, -1.4394e-01,  1.1885e-01,\n",
      "         9.7875e-03, -9.7859e-02, -3.9622e-02, -4.5969e-02, -4.3369e-02,\n",
      "        -2.5617e-02, -5.2712e-02,  3.9468e-02,  1.0800e-01,  5.3185e-02,\n",
      "        -5.0451e-02,  5.3125e-02,  1.4214e-01,  1.0340e-01, -1.7702e-02,\n",
      "        -6.3901e-02,  3.0720e-02, -7.3908e-02,  9.5226e-02,  6.2002e-03,\n",
      "         5.0914e-02, -6.5561e-02,  5.4568e-02,  5.1027e-02, -4.2785e-02,\n",
      "        -7.9318e-02,  6.1157e-02,  6.2453e-02, -4.5603e-02, -2.7345e-02,\n",
      "        -5.6974e-02,  1.2981e-01,  1.0213e-01,  4.7302e-02, -2.4651e-02,\n",
      "        -3.3669e-02, -4.9926e-02,  7.3012e-02, -3.4709e-02,  1.2907e-01,\n",
      "         6.1702e-02,  3.1375e-02,  1.9113e-02, -9.1100e-02,  7.3931e-03,\n",
      "        -8.0293e-02, -3.6101e-02,  5.1210e-02, -2.9621e-02,  5.9973e-03,\n",
      "         9.6392e-02,  4.1492e-03,  2.3054e-02, -8.5028e-02,  1.3075e-03,\n",
      "        -8.0786e-02,  7.1889e-02, -3.7784e-02,  2.0823e-02, -5.1179e-02,\n",
      "         1.3547e-01,  4.0677e-02,  6.0206e-02, -4.4987e-03, -1.4705e-02,\n",
      "        -4.3924e-05,  2.2686e-02, -3.4385e-02, -3.4656e-02, -1.7687e-01,\n",
      "         4.2150e-02, -3.2622e-03, -4.4221e-02, -5.9327e-02, -1.2178e-01,\n",
      "        -9.8243e-02,  2.9285e-02,  1.0800e-01, -2.9136e-02, -1.2633e-02,\n",
      "        -8.9605e-02,  1.0191e-02,  2.9528e-02, -1.6184e-02, -2.1323e-02,\n",
      "         4.3191e-02, -5.9493e-02, -9.2964e-02, -2.2478e-02,  1.4769e-02,\n",
      "        -2.1768e-02,  4.5379e-02,  7.2459e-02,  6.8969e-02, -3.1864e-02,\n",
      "         9.2427e-03,  1.1675e-01, -1.2651e-02, -8.6167e-02, -7.0927e-02,\n",
      "        -3.2216e-02, -3.6091e-02,  1.1292e-02, -2.3667e-02,  1.0530e-01,\n",
      "        -2.7349e-02,  3.5006e-02, -8.4804e-02,  5.3443e-02, -3.9848e-02,\n",
      "        -1.8628e-02, -9.9607e-02, -1.0862e-01,  3.0266e-03,  6.8604e-02,\n",
      "        -2.6277e-02, -1.4869e-01, -4.6595e-02, -9.2243e-02,  5.9800e-02,\n",
      "         1.6846e-02, -5.4605e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0000500'], tr/val_loss:  1.973059/ 14.972519, val:  42.08%, val_best:  42.08%, tr:  33.09%, tr_best:  33.09%, epoch time: 53.57 seconds, 0.89 minutes\n",
      "[module.layers.3] weight_fb parameter count: 5,120\n",
      "epoch-1   lr=['0.0000500'], tr/val_loss:  1.517165/ 12.473540, val:  47.08%, val_best:  47.08%, tr:  46.99%, tr_best:  46.99%, epoch time: 53.20 seconds, 0.89 minutes\n",
      "epoch-2   lr=['0.0000500'], tr/val_loss:  1.400437/  8.668548, val:  52.50%, val_best:  52.50%, tr:  51.58%, tr_best:  51.58%, epoch time: 54.08 seconds, 0.90 minutes\n",
      "epoch-3   lr=['0.0000500'], tr/val_loss:  1.322631/  8.413262, val:  58.33%, val_best:  58.33%, tr:  57.71%, tr_best:  57.71%, epoch time: 53.04 seconds, 0.88 minutes\n",
      "epoch-4   lr=['0.0000500'], tr/val_loss:  1.294339/  6.704001, val:  57.08%, val_best:  58.33%, tr:  57.61%, tr_best:  57.71%, epoch time: 53.23 seconds, 0.89 minutes\n",
      "epoch-5   lr=['0.0000500'], tr/val_loss:  1.266445/  9.186819, val:  53.33%, val_best:  58.33%, tr:  57.10%, tr_best:  57.71%, epoch time: 54.20 seconds, 0.90 minutes\n",
      "epoch-6   lr=['0.0000500'], tr/val_loss:  1.247221/  7.386467, val:  60.83%, val_best:  60.83%, tr:  59.04%, tr_best:  59.04%, epoch time: 54.17 seconds, 0.90 minutes\n",
      "epoch-7   lr=['0.0000500'], tr/val_loss:  1.215757/  7.099099, val:  60.83%, val_best:  60.83%, tr:  60.78%, tr_best:  60.78%, epoch time: 54.19 seconds, 0.90 minutes\n",
      "epoch-8   lr=['0.0000500'], tr/val_loss:  1.195770/  7.231518, val:  56.67%, val_best:  60.83%, tr:  60.67%, tr_best:  60.78%, epoch time: 53.36 seconds, 0.89 minutes\n",
      "epoch-9   lr=['0.0000500'], tr/val_loss:  1.164039/  7.142202, val:  55.83%, val_best:  60.83%, tr:  64.35%, tr_best:  64.35%, epoch time: 53.46 seconds, 0.89 minutes\n",
      "epoch-10  lr=['0.0000500'], tr/val_loss:  1.159632/  6.931098, val:  60.83%, val_best:  60.83%, tr:  66.91%, tr_best:  66.91%, epoch time: 53.81 seconds, 0.90 minutes\n",
      "epoch-11  lr=['0.0000500'], tr/val_loss:  1.168191/  7.723883, val:  59.58%, val_best:  60.83%, tr:  64.56%, tr_best:  66.91%, epoch time: 53.42 seconds, 0.89 minutes\n",
      "epoch-12  lr=['0.0000500'], tr/val_loss:  1.144554/  6.054123, val:  61.25%, val_best:  61.25%, tr:  66.19%, tr_best:  66.91%, epoch time: 53.99 seconds, 0.90 minutes\n",
      "epoch-13  lr=['0.0000500'], tr/val_loss:  1.128837/  6.968773, val:  60.00%, val_best:  61.25%, tr:  67.93%, tr_best:  67.93%, epoch time: 53.10 seconds, 0.89 minutes\n",
      "epoch-14  lr=['0.0000500'], tr/val_loss:  1.089789/ 10.083254, val:  52.92%, val_best:  61.25%, tr:  69.36%, tr_best:  69.36%, epoch time: 53.77 seconds, 0.90 minutes\n",
      "epoch-15  lr=['0.0000500'], tr/val_loss:  1.081241/  7.739712, val:  58.33%, val_best:  61.25%, tr:  71.71%, tr_best:  71.71%, epoch time: 54.39 seconds, 0.91 minutes\n",
      "epoch-16  lr=['0.0000500'], tr/val_loss:  1.078950/  6.372216, val:  60.00%, val_best:  61.25%, tr:  68.74%, tr_best:  71.71%, epoch time: 53.87 seconds, 0.90 minutes\n",
      "epoch-17  lr=['0.0000500'], tr/val_loss:  1.096651/  5.030980, val:  68.33%, val_best:  68.33%, tr:  70.38%, tr_best:  71.71%, epoch time: 54.13 seconds, 0.90 minutes\n",
      "epoch-18  lr=['0.0000500'], tr/val_loss:  1.065311/  6.182175, val:  65.42%, val_best:  68.33%, tr:  68.95%, tr_best:  71.71%, epoch time: 54.49 seconds, 0.91 minutes\n",
      "epoch-19  lr=['0.0000500'], tr/val_loss:  1.050732/  6.654850, val:  59.58%, val_best:  68.33%, tr:  72.32%, tr_best:  72.32%, epoch time: 53.95 seconds, 0.90 minutes\n",
      "epoch-20  lr=['0.0000500'], tr/val_loss:  1.070529/  6.394422, val:  61.25%, val_best:  68.33%, tr:  71.71%, tr_best:  72.32%, epoch time: 54.85 seconds, 0.91 minutes\n",
      "epoch-21  lr=['0.0000500'], tr/val_loss:  1.043109/  5.589303, val:  66.67%, val_best:  68.33%, tr:  72.42%, tr_best:  72.42%, epoch time: 53.91 seconds, 0.90 minutes\n",
      "epoch-22  lr=['0.0000500'], tr/val_loss:  1.044027/  6.127003, val:  65.42%, val_best:  68.33%, tr:  73.54%, tr_best:  73.54%, epoch time: 53.64 seconds, 0.89 minutes\n",
      "epoch-23  lr=['0.0000500'], tr/val_loss:  1.072382/  5.746812, val:  68.33%, val_best:  68.33%, tr:  72.52%, tr_best:  73.54%, epoch time: 54.89 seconds, 0.91 minutes\n",
      "epoch-24  lr=['0.0000500'], tr/val_loss:  1.048165/  5.930317, val:  64.17%, val_best:  68.33%, tr:  73.03%, tr_best:  73.54%, epoch time: 54.93 seconds, 0.92 minutes\n",
      "epoch-25  lr=['0.0000500'], tr/val_loss:  1.032153/  7.114172, val:  65.00%, val_best:  68.33%, tr:  73.65%, tr_best:  73.65%, epoch time: 55.44 seconds, 0.92 minutes\n",
      "epoch-26  lr=['0.0000500'], tr/val_loss:  1.025746/  6.066053, val:  66.25%, val_best:  68.33%, tr:  74.46%, tr_best:  74.46%, epoch time: 55.36 seconds, 0.92 minutes\n",
      "epoch-27  lr=['0.0000500'], tr/val_loss:  1.014665/  8.563416, val:  63.33%, val_best:  68.33%, tr:  74.87%, tr_best:  74.87%, epoch time: 55.07 seconds, 0.92 minutes\n",
      "epoch-28  lr=['0.0000500'], tr/val_loss:  1.035520/  5.219870, val:  68.75%, val_best:  68.75%, tr:  73.44%, tr_best:  74.87%, epoch time: 55.18 seconds, 0.92 minutes\n",
      "epoch-29  lr=['0.0000500'], tr/val_loss:  1.023628/  5.940592, val:  68.33%, val_best:  68.75%, tr:  73.95%, tr_best:  74.87%, epoch time: 54.83 seconds, 0.91 minutes\n",
      "epoch-30  lr=['0.0000500'], tr/val_loss:  1.006020/  6.480176, val:  65.42%, val_best:  68.75%, tr:  75.49%, tr_best:  75.49%, epoch time: 54.61 seconds, 0.91 minutes\n",
      "epoch-31  lr=['0.0000500'], tr/val_loss:  1.006251/  7.230022, val:  63.75%, val_best:  68.75%, tr:  74.97%, tr_best:  75.49%, epoch time: 54.53 seconds, 0.91 minutes\n",
      "epoch-32  lr=['0.0000500'], tr/val_loss:  0.984720/  6.494116, val:  65.00%, val_best:  68.75%, tr:  75.69%, tr_best:  75.69%, epoch time: 54.42 seconds, 0.91 minutes\n",
      "epoch-33  lr=['0.0000500'], tr/val_loss:  1.018487/  6.523087, val:  65.00%, val_best:  68.75%, tr:  75.18%, tr_best:  75.69%, epoch time: 55.44 seconds, 0.92 minutes\n",
      "epoch-34  lr=['0.0000500'], tr/val_loss:  0.980624/  5.566863, val:  69.58%, val_best:  69.58%, tr:  77.32%, tr_best:  77.32%, epoch time: 55.25 seconds, 0.92 minutes\n",
      "epoch-35  lr=['0.0000500'], tr/val_loss:  1.006489/  6.108522, val:  68.75%, val_best:  69.58%, tr:  75.89%, tr_best:  77.32%, epoch time: 52.31 seconds, 0.87 minutes\n",
      "epoch-36  lr=['0.0000500'], tr/val_loss:  1.009339/  6.714945, val:  63.33%, val_best:  69.58%, tr:  76.20%, tr_best:  77.32%, epoch time: 48.31 seconds, 0.81 minutes\n",
      "epoch-37  lr=['0.0000500'], tr/val_loss:  0.991793/  6.411686, val:  69.17%, val_best:  69.58%, tr:  77.32%, tr_best:  77.32%, epoch time: 48.53 seconds, 0.81 minutes\n",
      "epoch-38  lr=['0.0000500'], tr/val_loss:  0.986826/  5.538456, val:  71.25%, val_best:  71.25%, tr:  76.92%, tr_best:  77.32%, epoch time: 47.22 seconds, 0.79 minutes\n",
      "epoch-39  lr=['0.0000500'], tr/val_loss:  0.991769/  4.819398, val:  70.42%, val_best:  71.25%, tr:  76.71%, tr_best:  77.32%, epoch time: 47.90 seconds, 0.80 minutes\n",
      "epoch-40  lr=['0.0000500'], tr/val_loss:  0.997778/  6.622748, val:  64.58%, val_best:  71.25%, tr:  77.73%, tr_best:  77.73%, epoch time: 47.28 seconds, 0.79 minutes\n",
      "epoch-41  lr=['0.0000500'], tr/val_loss:  0.985162/  6.983171, val:  67.08%, val_best:  71.25%, tr:  75.79%, tr_best:  77.73%, epoch time: 48.29 seconds, 0.80 minutes\n",
      "epoch-42  lr=['0.0000500'], tr/val_loss:  0.969240/  5.086970, val:  71.67%, val_best:  71.67%, tr:  77.53%, tr_best:  77.73%, epoch time: 47.55 seconds, 0.79 minutes\n",
      "epoch-43  lr=['0.0000500'], tr/val_loss:  0.964491/  5.489644, val:  67.50%, val_best:  71.67%, tr:  77.73%, tr_best:  77.73%, epoch time: 47.37 seconds, 0.79 minutes\n",
      "epoch-44  lr=['0.0000500'], tr/val_loss:  0.958482/  5.243567, val:  70.42%, val_best:  71.67%, tr:  79.06%, tr_best:  79.06%, epoch time: 48.29 seconds, 0.80 minutes\n",
      "epoch-45  lr=['0.0000500'], tr/val_loss:  0.989680/  5.355937, val:  74.58%, val_best:  74.58%, tr:  76.51%, tr_best:  79.06%, epoch time: 48.00 seconds, 0.80 minutes\n",
      "epoch-46  lr=['0.0000500'], tr/val_loss:  0.981042/  6.897336, val:  67.08%, val_best:  74.58%, tr:  78.96%, tr_best:  79.06%, epoch time: 47.61 seconds, 0.79 minutes\n",
      "epoch-47  lr=['0.0000500'], tr/val_loss:  0.982997/  6.255910, val:  67.50%, val_best:  74.58%, tr:  77.22%, tr_best:  79.06%, epoch time: 47.06 seconds, 0.78 minutes\n",
      "epoch-48  lr=['0.0000500'], tr/val_loss:  0.963622/  5.616910, val:  70.00%, val_best:  74.58%, tr:  79.47%, tr_best:  79.47%, epoch time: 47.85 seconds, 0.80 minutes\n",
      "epoch-49  lr=['0.0000500'], tr/val_loss:  0.978694/  5.704346, val:  67.92%, val_best:  74.58%, tr:  78.04%, tr_best:  79.47%, epoch time: 48.41 seconds, 0.81 minutes\n",
      "epoch-50  lr=['0.0000500'], tr/val_loss:  0.967687/  6.158077, val:  69.17%, val_best:  74.58%, tr:  78.86%, tr_best:  79.47%, epoch time: 47.48 seconds, 0.79 minutes\n",
      "epoch-51  lr=['0.0000500'], tr/val_loss:  0.971265/  7.275070, val:  59.58%, val_best:  74.58%, tr:  77.83%, tr_best:  79.47%, epoch time: 47.95 seconds, 0.80 minutes\n",
      "epoch-52  lr=['0.0000500'], tr/val_loss:  0.980544/  6.210299, val:  66.25%, val_best:  74.58%, tr:  79.16%, tr_best:  79.47%, epoch time: 48.06 seconds, 0.80 minutes\n",
      "epoch-53  lr=['0.0000500'], tr/val_loss:  0.966629/  6.510934, val:  67.08%, val_best:  74.58%, tr:  80.39%, tr_best:  80.39%, epoch time: 48.08 seconds, 0.80 minutes\n",
      "epoch-54  lr=['0.0000500'], tr/val_loss:  0.967354/  5.832312, val:  68.75%, val_best:  74.58%, tr:  79.06%, tr_best:  80.39%, epoch time: 47.59 seconds, 0.79 minutes\n",
      "epoch-55  lr=['0.0000500'], tr/val_loss:  0.971087/  5.730943, val:  68.75%, val_best:  74.58%, tr:  77.63%, tr_best:  80.39%, epoch time: 46.77 seconds, 0.78 minutes\n",
      "epoch-56  lr=['0.0000500'], tr/val_loss:  0.955727/  6.649152, val:  67.92%, val_best:  74.58%, tr:  81.00%, tr_best:  81.00%, epoch time: 48.33 seconds, 0.81 minutes\n",
      "epoch-57  lr=['0.0000500'], tr/val_loss:  0.951193/  5.850866, val:  69.17%, val_best:  74.58%, tr:  79.37%, tr_best:  81.00%, epoch time: 47.87 seconds, 0.80 minutes\n",
      "epoch-58  lr=['0.0000500'], tr/val_loss:  0.923696/  6.369786, val:  65.83%, val_best:  74.58%, tr:  80.90%, tr_best:  81.00%, epoch time: 47.01 seconds, 0.78 minutes\n",
      "epoch-59  lr=['0.0000500'], tr/val_loss:  0.939084/  5.784513, val:  67.50%, val_best:  74.58%, tr:  79.78%, tr_best:  81.00%, epoch time: 47.62 seconds, 0.79 minutes\n",
      "epoch-60  lr=['0.0000500'], tr/val_loss:  0.960478/  5.727883, val:  69.58%, val_best:  74.58%, tr:  79.26%, tr_best:  81.00%, epoch time: 47.49 seconds, 0.79 minutes\n",
      "epoch-61  lr=['0.0000500'], tr/val_loss:  0.947630/  7.769932, val:  65.00%, val_best:  74.58%, tr:  82.84%, tr_best:  82.84%, epoch time: 48.49 seconds, 0.81 minutes\n",
      "epoch-62  lr=['0.0000500'], tr/val_loss:  0.950488/  5.623424, val:  70.42%, val_best:  74.58%, tr:  81.92%, tr_best:  82.84%, epoch time: 47.74 seconds, 0.80 minutes\n",
      "epoch-63  lr=['0.0000500'], tr/val_loss:  0.952818/  5.369309, val:  70.83%, val_best:  74.58%, tr:  81.61%, tr_best:  82.84%, epoch time: 47.90 seconds, 0.80 minutes\n",
      "epoch-64  lr=['0.0000500'], tr/val_loss:  0.944414/  5.885884, val:  69.17%, val_best:  74.58%, tr:  80.90%, tr_best:  82.84%, epoch time: 47.74 seconds, 0.80 minutes\n",
      "epoch-65  lr=['0.0000500'], tr/val_loss:  0.941633/  4.923985, val:  75.00%, val_best:  75.00%, tr:  81.82%, tr_best:  82.84%, epoch time: 49.07 seconds, 0.82 minutes\n",
      "epoch-66  lr=['0.0000500'], tr/val_loss:  0.948662/  5.613826, val:  70.00%, val_best:  75.00%, tr:  79.37%, tr_best:  82.84%, epoch time: 48.01 seconds, 0.80 minutes\n",
      "epoch-67  lr=['0.0000500'], tr/val_loss:  0.944147/  5.813816, val:  68.75%, val_best:  75.00%, tr:  81.61%, tr_best:  82.84%, epoch time: 47.69 seconds, 0.79 minutes\n",
      "epoch-68  lr=['0.0000500'], tr/val_loss:  0.932985/  5.713308, val:  68.75%, val_best:  75.00%, tr:  81.51%, tr_best:  82.84%, epoch time: 47.86 seconds, 0.80 minutes\n",
      "epoch-69  lr=['0.0000500'], tr/val_loss:  0.922230/  5.157470, val:  70.83%, val_best:  75.00%, tr:  83.25%, tr_best:  83.25%, epoch time: 47.90 seconds, 0.80 minutes\n",
      "epoch-70  lr=['0.0000500'], tr/val_loss:  0.936742/  6.461662, val:  66.67%, val_best:  75.00%, tr:  81.51%, tr_best:  83.25%, epoch time: 47.85 seconds, 0.80 minutes\n",
      "epoch-71  lr=['0.0000500'], tr/val_loss:  0.945142/  5.304729, val:  71.25%, val_best:  75.00%, tr:  81.51%, tr_best:  83.25%, epoch time: 47.65 seconds, 0.79 minutes\n",
      "epoch-72  lr=['0.0000500'], tr/val_loss:  0.929409/  5.381523, val:  72.92%, val_best:  75.00%, tr:  82.12%, tr_best:  83.25%, epoch time: 47.30 seconds, 0.79 minutes\n",
      "epoch-73  lr=['0.0000500'], tr/val_loss:  0.931622/  5.717292, val:  69.58%, val_best:  75.00%, tr:  83.45%, tr_best:  83.45%, epoch time: 47.67 seconds, 0.79 minutes\n",
      "epoch-74  lr=['0.0000500'], tr/val_loss:  0.944015/  5.017454, val:  73.75%, val_best:  75.00%, tr:  81.51%, tr_best:  83.45%, epoch time: 47.98 seconds, 0.80 minutes\n",
      "epoch-75  lr=['0.0000500'], tr/val_loss:  0.932070/  6.758173, val:  67.92%, val_best:  75.00%, tr:  82.43%, tr_best:  83.45%, epoch time: 47.89 seconds, 0.80 minutes\n",
      "epoch-76  lr=['0.0000500'], tr/val_loss:  0.955215/  5.626262, val:  67.92%, val_best:  75.00%, tr:  81.51%, tr_best:  83.45%, epoch time: 47.22 seconds, 0.79 minutes\n",
      "epoch-77  lr=['0.0000500'], tr/val_loss:  0.928383/  7.610475, val:  64.17%, val_best:  75.00%, tr:  81.21%, tr_best:  83.45%, epoch time: 47.46 seconds, 0.79 minutes\n",
      "epoch-78  lr=['0.0000500'], tr/val_loss:  0.930194/  6.412702, val:  70.42%, val_best:  75.00%, tr:  81.41%, tr_best:  83.45%, epoch time: 48.36 seconds, 0.81 minutes\n",
      "epoch-79  lr=['0.0000500'], tr/val_loss:  0.930273/  6.364036, val:  67.08%, val_best:  75.00%, tr:  81.51%, tr_best:  83.45%, epoch time: 47.35 seconds, 0.79 minutes\n",
      "epoch-80  lr=['0.0000500'], tr/val_loss:  0.936065/  5.394638, val:  74.58%, val_best:  75.00%, tr:  82.12%, tr_best:  83.45%, epoch time: 47.61 seconds, 0.79 minutes\n",
      "epoch-81  lr=['0.0000500'], tr/val_loss:  0.937436/  5.104557, val:  72.50%, val_best:  75.00%, tr:  82.33%, tr_best:  83.45%, epoch time: 48.72 seconds, 0.81 minutes\n",
      "epoch-82  lr=['0.0000500'], tr/val_loss:  0.923795/  6.373896, val:  67.08%, val_best:  75.00%, tr:  84.17%, tr_best:  84.17%, epoch time: 47.25 seconds, 0.79 minutes\n",
      "epoch-83  lr=['0.0000500'], tr/val_loss:  0.947104/  6.354397, val:  66.25%, val_best:  75.00%, tr:  80.80%, tr_best:  84.17%, epoch time: 47.78 seconds, 0.80 minutes\n",
      "epoch-84  lr=['0.0000500'], tr/val_loss:  0.933882/  5.486581, val:  70.83%, val_best:  75.00%, tr:  83.86%, tr_best:  84.17%, epoch time: 48.49 seconds, 0.81 minutes\n",
      "epoch-85  lr=['0.0000500'], tr/val_loss:  0.923543/  6.047271, val:  68.33%, val_best:  75.00%, tr:  84.27%, tr_best:  84.27%, epoch time: 48.06 seconds, 0.80 minutes\n",
      "epoch-86  lr=['0.0000500'], tr/val_loss:  0.944027/  6.326870, val:  65.42%, val_best:  75.00%, tr:  82.74%, tr_best:  84.27%, epoch time: 48.17 seconds, 0.80 minutes\n",
      "epoch-87  lr=['0.0000500'], tr/val_loss:  0.921926/  5.828440, val:  72.08%, val_best:  75.00%, tr:  83.66%, tr_best:  84.27%, epoch time: 47.48 seconds, 0.79 minutes\n",
      "epoch-88  lr=['0.0000500'], tr/val_loss:  0.927169/  5.396798, val:  72.50%, val_best:  75.00%, tr:  84.17%, tr_best:  84.27%, epoch time: 47.25 seconds, 0.79 minutes\n",
      "epoch-89  lr=['0.0000500'], tr/val_loss:  0.920131/  5.258764, val:  73.33%, val_best:  75.00%, tr:  84.88%, tr_best:  84.88%, epoch time: 48.18 seconds, 0.80 minutes\n",
      "epoch-90  lr=['0.0000500'], tr/val_loss:  0.931174/  5.672998, val:  67.92%, val_best:  75.00%, tr:  82.53%, tr_best:  84.88%, epoch time: 47.03 seconds, 0.78 minutes\n",
      "epoch-91  lr=['0.0000500'], tr/val_loss:  0.932471/  5.697805, val:  70.42%, val_best:  75.00%, tr:  84.37%, tr_best:  84.88%, epoch time: 47.58 seconds, 0.79 minutes\n",
      "epoch-92  lr=['0.0000500'], tr/val_loss:  0.925991/  5.650002, val:  70.00%, val_best:  75.00%, tr:  83.15%, tr_best:  84.88%, epoch time: 48.29 seconds, 0.80 minutes\n",
      "epoch-93  lr=['0.0000500'], tr/val_loss:  0.926012/  5.982122, val:  73.75%, val_best:  75.00%, tr:  83.55%, tr_best:  84.88%, epoch time: 47.26 seconds, 0.79 minutes\n",
      "epoch-94  lr=['0.0000500'], tr/val_loss:  0.927810/  6.653294, val:  65.42%, val_best:  75.00%, tr:  84.47%, tr_best:  84.88%, epoch time: 47.92 seconds, 0.80 minutes\n",
      "epoch-95  lr=['0.0000500'], tr/val_loss:  0.933998/  6.199827, val:  67.08%, val_best:  75.00%, tr:  85.09%, tr_best:  85.09%, epoch time: 47.57 seconds, 0.79 minutes\n",
      "epoch-96  lr=['0.0000500'], tr/val_loss:  0.917239/  5.763963, val:  71.25%, val_best:  75.00%, tr:  83.96%, tr_best:  85.09%, epoch time: 47.62 seconds, 0.79 minutes\n",
      "epoch-97  lr=['0.0000500'], tr/val_loss:  0.916338/  7.460304, val:  64.17%, val_best:  75.00%, tr:  83.66%, tr_best:  85.09%, epoch time: 47.37 seconds, 0.79 minutes\n",
      "epoch-98  lr=['0.0000500'], tr/val_loss:  0.908188/  6.924414, val:  67.92%, val_best:  75.00%, tr:  85.09%, tr_best:  85.09%, epoch time: 48.20 seconds, 0.80 minutes\n",
      "epoch-99  lr=['0.0000500'], tr/val_loss:  0.891696/  7.104965, val:  67.50%, val_best:  75.00%, tr:  86.01%, tr_best:  86.01%, epoch time: 46.89 seconds, 0.78 minutes\n",
      "epoch-100 lr=['0.0000500'], tr/val_loss:  0.917678/  5.311433, val:  72.92%, val_best:  75.00%, tr:  84.07%, tr_best:  86.01%, epoch time: 47.21 seconds, 0.79 minutes\n",
      "epoch-101 lr=['0.0000500'], tr/val_loss:  0.907986/  5.341021, val:  71.67%, val_best:  75.00%, tr:  84.88%, tr_best:  86.01%, epoch time: 48.06 seconds, 0.80 minutes\n",
      "epoch-102 lr=['0.0000500'], tr/val_loss:  0.932596/  6.492283, val:  67.08%, val_best:  75.00%, tr:  83.66%, tr_best:  86.01%, epoch time: 47.57 seconds, 0.79 minutes\n",
      "epoch-103 lr=['0.0000500'], tr/val_loss:  0.919945/  5.817637, val:  71.25%, val_best:  75.00%, tr:  84.07%, tr_best:  86.01%, epoch time: 47.51 seconds, 0.79 minutes\n",
      "epoch-104 lr=['0.0000500'], tr/val_loss:  0.920929/  6.027260, val:  69.17%, val_best:  75.00%, tr:  83.76%, tr_best:  86.01%, epoch time: 47.26 seconds, 0.79 minutes\n",
      "epoch-105 lr=['0.0000500'], tr/val_loss:  0.932970/  5.412874, val:  70.42%, val_best:  75.00%, tr:  82.74%, tr_best:  86.01%, epoch time: 48.03 seconds, 0.80 minutes\n",
      "epoch-106 lr=['0.0000500'], tr/val_loss:  0.920260/  5.679322, val:  71.67%, val_best:  75.00%, tr:  84.78%, tr_best:  86.01%, epoch time: 47.34 seconds, 0.79 minutes\n",
      "epoch-107 lr=['0.0000500'], tr/val_loss:  0.918397/  5.490904, val:  73.33%, val_best:  75.00%, tr:  84.78%, tr_best:  86.01%, epoch time: 47.12 seconds, 0.79 minutes\n",
      "epoch-108 lr=['0.0000500'], tr/val_loss:  0.904431/  6.588930, val:  69.17%, val_best:  75.00%, tr:  83.25%, tr_best:  86.01%, epoch time: 47.17 seconds, 0.79 minutes\n",
      "epoch-109 lr=['0.0000500'], tr/val_loss:  0.909773/  5.145809, val:  72.50%, val_best:  75.00%, tr:  85.70%, tr_best:  86.01%, epoch time: 46.98 seconds, 0.78 minutes\n",
      "epoch-110 lr=['0.0000500'], tr/val_loss:  0.926002/  6.325972, val:  66.67%, val_best:  75.00%, tr:  84.47%, tr_best:  86.01%, epoch time: 47.21 seconds, 0.79 minutes\n",
      "epoch-111 lr=['0.0000500'], tr/val_loss:  0.904109/  5.814358, val:  68.75%, val_best:  75.00%, tr:  83.66%, tr_best:  86.01%, epoch time: 46.90 seconds, 0.78 minutes\n",
      "epoch-112 lr=['0.0000500'], tr/val_loss:  0.922140/  5.997357, val:  71.25%, val_best:  75.00%, tr:  84.37%, tr_best:  86.01%, epoch time: 46.48 seconds, 0.77 minutes\n",
      "epoch-113 lr=['0.0000500'], tr/val_loss:  0.899429/  7.283893, val:  67.08%, val_best:  75.00%, tr:  84.27%, tr_best:  86.01%, epoch time: 47.50 seconds, 0.79 minutes\n",
      "epoch-114 lr=['0.0000500'], tr/val_loss:  0.911267/  6.453305, val:  66.67%, val_best:  75.00%, tr:  85.19%, tr_best:  86.01%, epoch time: 47.50 seconds, 0.79 minutes\n",
      "epoch-115 lr=['0.0000500'], tr/val_loss:  0.923379/  6.789057, val:  66.67%, val_best:  75.00%, tr:  83.55%, tr_best:  86.01%, epoch time: 46.42 seconds, 0.77 minutes\n",
      "epoch-116 lr=['0.0000500'], tr/val_loss:  0.911826/  6.645750, val:  66.67%, val_best:  75.00%, tr:  85.29%, tr_best:  86.01%, epoch time: 46.49 seconds, 0.77 minutes\n",
      "epoch-117 lr=['0.0000500'], tr/val_loss:  0.903214/  5.934871, val:  67.50%, val_best:  75.00%, tr:  85.60%, tr_best:  86.01%, epoch time: 46.88 seconds, 0.78 minutes\n",
      "epoch-118 lr=['0.0000500'], tr/val_loss:  0.912596/  5.920732, val:  69.58%, val_best:  75.00%, tr:  84.78%, tr_best:  86.01%, epoch time: 47.54 seconds, 0.79 minutes\n",
      "epoch-119 lr=['0.0000500'], tr/val_loss:  0.916931/  5.550146, val:  72.08%, val_best:  75.00%, tr:  84.17%, tr_best:  86.01%, epoch time: 47.87 seconds, 0.80 minutes\n",
      "epoch-120 lr=['0.0000500'], tr/val_loss:  0.911214/  5.243999, val:  70.42%, val_best:  75.00%, tr:  85.19%, tr_best:  86.01%, epoch time: 46.51 seconds, 0.78 minutes\n",
      "epoch-121 lr=['0.0000500'], tr/val_loss:  0.913138/  5.683104, val:  69.58%, val_best:  75.00%, tr:  85.19%, tr_best:  86.01%, epoch time: 47.53 seconds, 0.79 minutes\n",
      "epoch-122 lr=['0.0000500'], tr/val_loss:  0.917002/  6.344856, val:  68.33%, val_best:  75.00%, tr:  85.19%, tr_best:  86.01%, epoch time: 47.53 seconds, 0.79 minutes\n",
      "epoch-123 lr=['0.0000500'], tr/val_loss:  0.897486/  5.654894, val:  71.67%, val_best:  75.00%, tr:  86.62%, tr_best:  86.62%, epoch time: 47.19 seconds, 0.79 minutes\n",
      "epoch-124 lr=['0.0000500'], tr/val_loss:  0.912274/  5.722748, val:  70.00%, val_best:  75.00%, tr:  83.76%, tr_best:  86.62%, epoch time: 47.46 seconds, 0.79 minutes\n",
      "epoch-125 lr=['0.0000500'], tr/val_loss:  0.905304/  5.635252, val:  70.00%, val_best:  75.00%, tr:  85.09%, tr_best:  86.62%, epoch time: 47.41 seconds, 0.79 minutes\n",
      "epoch-126 lr=['0.0000500'], tr/val_loss:  0.908260/  5.128490, val:  71.25%, val_best:  75.00%, tr:  86.01%, tr_best:  86.62%, epoch time: 47.44 seconds, 0.79 minutes\n",
      "epoch-127 lr=['0.0000500'], tr/val_loss:  0.894007/  7.038152, val:  67.50%, val_best:  75.00%, tr:  86.72%, tr_best:  86.72%, epoch time: 47.68 seconds, 0.79 minutes\n",
      "epoch-128 lr=['0.0000500'], tr/val_loss:  0.903845/  6.447301, val:  70.00%, val_best:  75.00%, tr:  85.39%, tr_best:  86.72%, epoch time: 46.82 seconds, 0.78 minutes\n",
      "epoch-129 lr=['0.0000500'], tr/val_loss:  0.876986/  5.640530, val:  71.67%, val_best:  75.00%, tr:  87.95%, tr_best:  87.95%, epoch time: 47.41 seconds, 0.79 minutes\n",
      "epoch-130 lr=['0.0000500'], tr/val_loss:  0.910760/  6.531787, val:  67.50%, val_best:  75.00%, tr:  84.98%, tr_best:  87.95%, epoch time: 47.08 seconds, 0.78 minutes\n",
      "epoch-131 lr=['0.0000500'], tr/val_loss:  0.907335/  5.965259, val:  70.00%, val_best:  75.00%, tr:  85.80%, tr_best:  87.95%, epoch time: 47.65 seconds, 0.79 minutes\n",
      "epoch-132 lr=['0.0000500'], tr/val_loss:  0.905297/  6.000324, val:  72.08%, val_best:  75.00%, tr:  87.95%, tr_best:  87.95%, epoch time: 46.42 seconds, 0.77 minutes\n",
      "epoch-133 lr=['0.0000500'], tr/val_loss:  0.900941/  5.191353, val:  73.75%, val_best:  75.00%, tr:  87.23%, tr_best:  87.95%, epoch time: 47.71 seconds, 0.80 minutes\n",
      "epoch-134 lr=['0.0000500'], tr/val_loss:  0.903261/  6.631096, val:  66.67%, val_best:  75.00%, tr:  85.60%, tr_best:  87.95%, epoch time: 48.42 seconds, 0.81 minutes\n",
      "epoch-135 lr=['0.0000500'], tr/val_loss:  0.930522/  5.441681, val:  70.42%, val_best:  75.00%, tr:  83.86%, tr_best:  87.95%, epoch time: 47.06 seconds, 0.78 minutes\n",
      "epoch-136 lr=['0.0000500'], tr/val_loss:  0.906841/  5.848567, val:  66.25%, val_best:  75.00%, tr:  86.11%, tr_best:  87.95%, epoch time: 46.87 seconds, 0.78 minutes\n",
      "epoch-137 lr=['0.0000500'], tr/val_loss:  0.901375/  5.572637, val:  71.25%, val_best:  75.00%, tr:  87.13%, tr_best:  87.95%, epoch time: 47.37 seconds, 0.79 minutes\n",
      "epoch-138 lr=['0.0000500'], tr/val_loss:  0.905805/  5.868958, val:  67.08%, val_best:  75.00%, tr:  84.98%, tr_best:  87.95%, epoch time: 47.45 seconds, 0.79 minutes\n",
      "epoch-139 lr=['0.0000500'], tr/val_loss:  0.899407/  5.714075, val:  70.42%, val_best:  75.00%, tr:  87.13%, tr_best:  87.95%, epoch time: 46.93 seconds, 0.78 minutes\n",
      "epoch-140 lr=['0.0000500'], tr/val_loss:  0.885062/  5.289536, val:  72.92%, val_best:  75.00%, tr:  86.72%, tr_best:  87.95%, epoch time: 46.75 seconds, 0.78 minutes\n",
      "epoch-141 lr=['0.0000500'], tr/val_loss:  0.893541/  6.889556, val:  65.42%, val_best:  75.00%, tr:  87.74%, tr_best:  87.95%, epoch time: 47.90 seconds, 0.80 minutes\n",
      "epoch-142 lr=['0.0000500'], tr/val_loss:  0.891477/  5.870909, val:  69.17%, val_best:  75.00%, tr:  85.50%, tr_best:  87.95%, epoch time: 46.85 seconds, 0.78 minutes\n",
      "epoch-143 lr=['0.0000500'], tr/val_loss:  0.886230/  5.944651, val:  70.00%, val_best:  75.00%, tr:  86.62%, tr_best:  87.95%, epoch time: 46.81 seconds, 0.78 minutes\n",
      "epoch-144 lr=['0.0000500'], tr/val_loss:  0.890771/  6.449818, val:  67.50%, val_best:  75.00%, tr:  86.11%, tr_best:  87.95%, epoch time: 47.59 seconds, 0.79 minutes\n",
      "epoch-145 lr=['0.0000500'], tr/val_loss:  0.893212/  5.881834, val:  67.92%, val_best:  75.00%, tr:  86.21%, tr_best:  87.95%, epoch time: 47.27 seconds, 0.79 minutes\n",
      "epoch-146 lr=['0.0000500'], tr/val_loss:  0.899328/  6.522715, val:  67.50%, val_best:  75.00%, tr:  87.23%, tr_best:  87.95%, epoch time: 47.24 seconds, 0.79 minutes\n",
      "epoch-147 lr=['0.0000500'], tr/val_loss:  0.897424/  5.660346, val:  69.17%, val_best:  75.00%, tr:  86.82%, tr_best:  87.95%, epoch time: 46.85 seconds, 0.78 minutes\n",
      "epoch-148 lr=['0.0000500'], tr/val_loss:  0.914764/  6.146351, val:  67.50%, val_best:  75.00%, tr:  85.09%, tr_best:  87.95%, epoch time: 46.97 seconds, 0.78 minutes\n",
      "epoch-149 lr=['0.0000500'], tr/val_loss:  0.904865/  6.212506, val:  67.92%, val_best:  75.00%, tr:  86.01%, tr_best:  87.95%, epoch time: 47.11 seconds, 0.79 minutes\n",
      "epoch-150 lr=['0.0000500'], tr/val_loss:  0.910040/  6.277928, val:  68.33%, val_best:  75.00%, tr:  85.80%, tr_best:  87.95%, epoch time: 47.25 seconds, 0.79 minutes\n",
      "epoch-151 lr=['0.0000500'], tr/val_loss:  0.920248/  5.917144, val:  70.00%, val_best:  75.00%, tr:  84.58%, tr_best:  87.95%, epoch time: 47.94 seconds, 0.80 minutes\n",
      "epoch-152 lr=['0.0000500'], tr/val_loss:  0.907437/  6.250206, val:  68.75%, val_best:  75.00%, tr:  85.50%, tr_best:  87.95%, epoch time: 47.57 seconds, 0.79 minutes\n",
      "epoch-153 lr=['0.0000500'], tr/val_loss:  0.901771/  6.334706, val:  67.08%, val_best:  75.00%, tr:  86.11%, tr_best:  87.95%, epoch time: 46.96 seconds, 0.78 minutes\n",
      "epoch-154 lr=['0.0000500'], tr/val_loss:  0.904639/  5.598006, val:  68.33%, val_best:  75.00%, tr:  86.01%, tr_best:  87.95%, epoch time: 47.39 seconds, 0.79 minutes\n",
      "epoch-155 lr=['0.0000500'], tr/val_loss:  0.893020/  6.235932, val:  69.58%, val_best:  75.00%, tr:  86.41%, tr_best:  87.95%, epoch time: 47.67 seconds, 0.79 minutes\n",
      "epoch-156 lr=['0.0000500'], tr/val_loss:  0.913136/  5.282073, val:  71.25%, val_best:  75.00%, tr:  85.39%, tr_best:  87.95%, epoch time: 47.29 seconds, 0.79 minutes\n",
      "epoch-157 lr=['0.0000500'], tr/val_loss:  0.888036/  5.740566, val:  71.67%, val_best:  75.00%, tr:  87.95%, tr_best:  87.95%, epoch time: 47.82 seconds, 0.80 minutes\n",
      "epoch-158 lr=['0.0000500'], tr/val_loss:  0.897423/  6.404361, val:  69.17%, val_best:  75.00%, tr:  85.70%, tr_best:  87.95%, epoch time: 47.69 seconds, 0.79 minutes\n",
      "epoch-159 lr=['0.0000500'], tr/val_loss:  0.892595/  5.564750, val:  73.75%, val_best:  75.00%, tr:  84.88%, tr_best:  87.95%, epoch time: 47.06 seconds, 0.78 minutes\n",
      "epoch-160 lr=['0.0000500'], tr/val_loss:  0.889792/  5.628567, val:  72.08%, val_best:  75.00%, tr:  86.62%, tr_best:  87.95%, epoch time: 47.39 seconds, 0.79 minutes\n",
      "epoch-161 lr=['0.0000500'], tr/val_loss:  0.912066/  5.529965, val:  69.58%, val_best:  75.00%, tr:  85.70%, tr_best:  87.95%, epoch time: 47.02 seconds, 0.78 minutes\n",
      "epoch-162 lr=['0.0000500'], tr/val_loss:  0.899655/  6.176525, val:  66.25%, val_best:  75.00%, tr:  87.03%, tr_best:  87.95%, epoch time: 47.90 seconds, 0.80 minutes\n",
      "epoch-163 lr=['0.0000500'], tr/val_loss:  0.886393/  5.646828, val:  70.42%, val_best:  75.00%, tr:  89.07%, tr_best:  89.07%, epoch time: 47.76 seconds, 0.80 minutes\n",
      "epoch-164 lr=['0.0000500'], tr/val_loss:  0.886951/  5.880934, val:  70.83%, val_best:  75.00%, tr:  86.72%, tr_best:  89.07%, epoch time: 47.02 seconds, 0.78 minutes\n",
      "epoch-165 lr=['0.0000500'], tr/val_loss:  0.893340/  6.937730, val:  66.67%, val_best:  75.00%, tr:  86.62%, tr_best:  89.07%, epoch time: 46.84 seconds, 0.78 minutes\n",
      "epoch-166 lr=['0.0000500'], tr/val_loss:  0.898263/  6.195568, val:  70.00%, val_best:  75.00%, tr:  87.13%, tr_best:  89.07%, epoch time: 46.67 seconds, 0.78 minutes\n",
      "epoch-167 lr=['0.0000500'], tr/val_loss:  0.885753/  7.094246, val:  66.67%, val_best:  75.00%, tr:  87.03%, tr_best:  89.07%, epoch time: 46.93 seconds, 0.78 minutes\n",
      "epoch-168 lr=['0.0000500'], tr/val_loss:  0.896053/  6.688518, val:  66.25%, val_best:  75.00%, tr:  87.64%, tr_best:  89.07%, epoch time: 46.68 seconds, 0.78 minutes\n",
      "epoch-169 lr=['0.0000500'], tr/val_loss:  0.909756/  6.126598, val:  71.67%, val_best:  75.00%, tr:  85.70%, tr_best:  89.07%, epoch time: 47.23 seconds, 0.79 minutes\n",
      "epoch-170 lr=['0.0000500'], tr/val_loss:  0.888757/  5.447521, val:  71.25%, val_best:  75.00%, tr:  87.03%, tr_best:  89.07%, epoch time: 47.48 seconds, 0.79 minutes\n",
      "epoch-171 lr=['0.0000500'], tr/val_loss:  0.903116/  6.201984, val:  68.75%, val_best:  75.00%, tr:  84.78%, tr_best:  89.07%, epoch time: 48.01 seconds, 0.80 minutes\n",
      "epoch-172 lr=['0.0000500'], tr/val_loss:  0.904968/  6.536308, val:  65.00%, val_best:  75.00%, tr:  86.31%, tr_best:  89.07%, epoch time: 47.36 seconds, 0.79 minutes\n",
      "epoch-173 lr=['0.0000500'], tr/val_loss:  0.897752/  6.714792, val:  67.50%, val_best:  75.00%, tr:  86.41%, tr_best:  89.07%, epoch time: 47.19 seconds, 0.79 minutes\n",
      "epoch-174 lr=['0.0000500'], tr/val_loss:  0.894265/  5.257569, val:  71.25%, val_best:  75.00%, tr:  87.95%, tr_best:  89.07%, epoch time: 47.27 seconds, 0.79 minutes\n",
      "epoch-175 lr=['0.0000500'], tr/val_loss:  0.898879/  4.953053, val:  74.58%, val_best:  75.00%, tr:  86.11%, tr_best:  89.07%, epoch time: 46.95 seconds, 0.78 minutes\n",
      "epoch-176 lr=['0.0000500'], tr/val_loss:  0.900583/  5.037206, val:  72.92%, val_best:  75.00%, tr:  86.93%, tr_best:  89.07%, epoch time: 47.80 seconds, 0.80 minutes\n",
      "epoch-177 lr=['0.0000500'], tr/val_loss:  0.904366/  6.170810, val:  67.50%, val_best:  75.00%, tr:  87.13%, tr_best:  89.07%, epoch time: 47.70 seconds, 0.79 minutes\n",
      "epoch-178 lr=['0.0000500'], tr/val_loss:  0.910060/  5.251079, val:  72.08%, val_best:  75.00%, tr:  86.82%, tr_best:  89.07%, epoch time: 47.40 seconds, 0.79 minutes\n",
      "epoch-179 lr=['0.0000500'], tr/val_loss:  0.922644/  5.025647, val:  71.67%, val_best:  75.00%, tr:  85.50%, tr_best:  89.07%, epoch time: 47.48 seconds, 0.79 minutes\n",
      "epoch-180 lr=['0.0000500'], tr/val_loss:  0.924537/  5.529953, val:  71.67%, val_best:  75.00%, tr:  84.78%, tr_best:  89.07%, epoch time: 46.62 seconds, 0.78 minutes\n",
      "epoch-181 lr=['0.0000500'], tr/val_loss:  0.888795/  6.046984, val:  67.50%, val_best:  75.00%, tr:  86.62%, tr_best:  89.07%, epoch time: 46.57 seconds, 0.78 minutes\n",
      "epoch-182 lr=['0.0000500'], tr/val_loss:  0.887863/  6.096209, val:  69.17%, val_best:  75.00%, tr:  88.76%, tr_best:  89.07%, epoch time: 46.12 seconds, 0.77 minutes\n",
      "epoch-183 lr=['0.0000500'], tr/val_loss:  0.884873/  5.860127, val:  68.75%, val_best:  75.00%, tr:  88.97%, tr_best:  89.07%, epoch time: 46.46 seconds, 0.77 minutes\n",
      "epoch-184 lr=['0.0000500'], tr/val_loss:  0.886528/  5.096365, val:  71.25%, val_best:  75.00%, tr:  87.23%, tr_best:  89.07%, epoch time: 47.29 seconds, 0.79 minutes\n",
      "epoch-185 lr=['0.0000500'], tr/val_loss:  0.899329/  5.282217, val:  72.92%, val_best:  75.00%, tr:  87.13%, tr_best:  89.07%, epoch time: 48.01 seconds, 0.80 minutes\n",
      "epoch-186 lr=['0.0000500'], tr/val_loss:  0.900435/  5.935082, val:  69.17%, val_best:  75.00%, tr:  86.21%, tr_best:  89.07%, epoch time: 46.62 seconds, 0.78 minutes\n",
      "epoch-187 lr=['0.0000500'], tr/val_loss:  0.910117/  6.951404, val:  69.17%, val_best:  75.00%, tr:  86.01%, tr_best:  89.07%, epoch time: 46.67 seconds, 0.78 minutes\n",
      "epoch-188 lr=['0.0000500'], tr/val_loss:  0.881976/  6.463988, val:  67.08%, val_best:  75.00%, tr:  89.79%, tr_best:  89.79%, epoch time: 47.96 seconds, 0.80 minutes\n",
      "epoch-189 lr=['0.0000500'], tr/val_loss:  0.902703/  6.426046, val:  69.58%, val_best:  75.00%, tr:  87.13%, tr_best:  89.79%, epoch time: 46.85 seconds, 0.78 minutes\n",
      "epoch-190 lr=['0.0000500'], tr/val_loss:  0.888094/  5.701928, val:  71.67%, val_best:  75.00%, tr:  88.76%, tr_best:  89.79%, epoch time: 47.33 seconds, 0.79 minutes\n",
      "epoch-191 lr=['0.0000500'], tr/val_loss:  0.891504/  5.544982, val:  70.00%, val_best:  75.00%, tr:  87.84%, tr_best:  89.79%, epoch time: 47.07 seconds, 0.78 minutes\n",
      "epoch-192 lr=['0.0000500'], tr/val_loss:  0.908384/  4.971150, val:  73.33%, val_best:  75.00%, tr:  87.33%, tr_best:  89.79%, epoch time: 47.42 seconds, 0.79 minutes\n",
      "epoch-193 lr=['0.0000500'], tr/val_loss:  0.906165/  4.917381, val:  71.25%, val_best:  75.00%, tr:  87.44%, tr_best:  89.79%, epoch time: 47.42 seconds, 0.79 minutes\n",
      "epoch-194 lr=['0.0000500'], tr/val_loss:  0.907846/  5.725409, val:  71.25%, val_best:  75.00%, tr:  87.64%, tr_best:  89.79%, epoch time: 47.99 seconds, 0.80 minutes\n",
      "epoch-195 lr=['0.0000500'], tr/val_loss:  0.898993/  5.142812, val:  72.50%, val_best:  75.00%, tr:  86.62%, tr_best:  89.79%, epoch time: 47.85 seconds, 0.80 minutes\n",
      "epoch-196 lr=['0.0000500'], tr/val_loss:  0.894678/  4.861615, val:  75.00%, val_best:  75.00%, tr:  88.25%, tr_best:  89.79%, epoch time: 46.96 seconds, 0.78 minutes\n",
      "epoch-197 lr=['0.0000500'], tr/val_loss:  0.893099/  5.146162, val:  74.58%, val_best:  75.00%, tr:  87.74%, tr_best:  89.79%, epoch time: 47.21 seconds, 0.79 minutes\n",
      "epoch-198 lr=['0.0000500'], tr/val_loss:  0.895136/  5.193384, val:  70.83%, val_best:  75.00%, tr:  88.25%, tr_best:  89.79%, epoch time: 47.85 seconds, 0.80 minutes\n",
      "epoch-199 lr=['0.0000500'], tr/val_loss:  0.891564/  5.923281, val:  71.67%, val_best:  75.00%, tr:  87.84%, tr_best:  89.79%, epoch time: 46.89 seconds, 0.78 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de39af314024dadba8b9f5b288c2f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÑ‚ñá‚ñÜ‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÑ‚ñá‚ñÜ‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.87845</td></tr><tr><td>tr_epoch_loss</td><td>0.89156</td></tr><tr><td>val_acc_best</td><td>0.75</td></tr><tr><td>val_acc_now</td><td>0.71667</td></tr><tr><td>val_loss</td><td>5.92328</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dulcet-sweep-269</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/as556e6d' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/as556e6d</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251110_210014-as556e6d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j86h1dlg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [512]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 5000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_threshold: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: one\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251110_234316-j86h1dlg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j86h1dlg' target=\"_blank\">fancy-sweep-275</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j86h1dlg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j86h1dlg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_threshold' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': '20251110_234323_715', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 0.25, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [512], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0005, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'one', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'output_threshold': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 7a22c8a0ef5b9b252dbf98632e270efd\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=512, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=10000, sg_width=0.25, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=False, ANPI_MODE=True)\n",
      "      (4): SYNAPSE_FC(in_features=512, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient(single_step=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 506,880\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.0005\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "inFeed spike.shape torch.Size([10, 512]) self.weight_fb.shape torch.Size([10, 512])\n",
      "self.weight_fb[0] tensor([ 1.2009e-02,  1.3379e-01, -1.0650e-02,  5.2556e-02, -1.1912e-01,\n",
      "         4.0419e-02, -4.0199e-02, -5.0604e-02,  3.2680e-02, -7.8942e-02,\n",
      "        -1.0288e-01, -1.8775e-02, -5.7299e-03,  1.2332e-02, -6.9353e-02,\n",
      "         1.1499e-01, -4.4228e-02,  4.2593e-02,  4.9323e-02, -2.0675e-03,\n",
      "         9.2336e-02, -3.1971e-02, -1.5728e-02,  9.1276e-02, -2.0181e-02,\n",
      "        -7.1800e-02,  1.4578e-01, -4.2861e-02,  1.1373e-02, -7.3257e-02,\n",
      "        -1.1159e-01, -9.7846e-02,  5.1912e-02,  8.7845e-02,  4.0044e-02,\n",
      "         2.6324e-02, -9.8372e-02,  3.8522e-02,  1.0460e-01, -4.1150e-02,\n",
      "         5.8342e-02,  4.8482e-03,  5.2401e-03, -8.7172e-03,  2.0523e-02,\n",
      "        -3.6457e-02, -6.6373e-02,  5.9048e-03, -2.0717e-02, -3.2546e-02,\n",
      "        -5.4324e-02,  2.4378e-02,  1.0149e-02, -1.2236e-02,  6.2543e-02,\n",
      "        -8.3454e-02, -2.1650e-02, -3.9879e-02,  2.7655e-02, -3.3246e-02,\n",
      "         7.6898e-02, -5.0422e-02,  1.5484e-02, -2.6447e-02,  6.8359e-02,\n",
      "        -6.8262e-02,  3.4312e-02, -7.9518e-02, -2.3619e-02,  3.1812e-02,\n",
      "         6.2016e-03,  1.6009e-02,  2.2387e-02,  1.4105e-01,  1.4450e-03,\n",
      "         9.7970e-02, -7.1751e-02,  5.8704e-02, -2.8309e-02,  4.7077e-02,\n",
      "        -3.5820e-02, -4.3640e-02, -4.4777e-02, -3.1386e-02, -2.7226e-02,\n",
      "        -2.5884e-02,  1.0779e-02,  2.7401e-02,  3.1376e-02, -7.5319e-02,\n",
      "        -1.6829e-02,  1.7118e-02, -8.9122e-02, -4.0006e-02,  4.6343e-03,\n",
      "         1.2001e-02,  3.6892e-02,  1.4373e-02,  7.0655e-02, -4.2197e-02,\n",
      "        -1.0233e-01,  3.7360e-04,  8.5512e-02,  7.8637e-02,  1.4384e-03,\n",
      "        -8.0477e-02, -4.6482e-02,  2.3251e-02, -3.3886e-02, -2.4537e-03,\n",
      "        -4.8149e-02, -1.5486e-01,  4.3330e-02, -5.8045e-03, -1.3386e-02,\n",
      "         2.7755e-02, -1.9510e-02,  1.3393e-03,  3.8708e-02,  1.5263e-02,\n",
      "         4.6335e-02, -7.2374e-03, -6.3238e-03, -3.1016e-02, -3.1252e-02,\n",
      "        -7.4723e-02, -1.5088e-02, -4.1994e-02,  1.2212e-02,  6.0550e-02,\n",
      "        -1.7745e-03,  1.0415e-01,  6.7522e-02, -6.1409e-02, -4.1550e-02,\n",
      "         1.0644e-01,  1.5230e-01, -3.8367e-02,  7.8697e-02, -1.7323e-02,\n",
      "         2.6986e-02,  2.6370e-02,  6.5894e-02, -1.2553e-01, -3.9156e-02,\n",
      "         1.3065e-01, -5.8646e-03,  1.4600e-02, -4.5190e-02, -1.0434e-01,\n",
      "         5.6415e-02,  4.8810e-02, -3.8917e-02,  1.3367e-01,  7.2065e-02,\n",
      "        -2.6348e-02,  1.4814e-02, -7.9086e-02, -7.4679e-03, -3.7547e-02,\n",
      "        -4.9995e-02,  1.3292e-04, -1.2034e-02,  4.6384e-02,  5.0249e-02,\n",
      "         5.1038e-02, -3.7747e-02,  8.0393e-02, -6.6428e-02, -1.4425e-03,\n",
      "        -2.2637e-02, -3.0118e-02,  9.2677e-03, -9.3434e-02,  1.9207e-02,\n",
      "        -2.7770e-02, -6.7883e-02, -7.8605e-02, -9.7644e-02, -9.8327e-02,\n",
      "        -4.0612e-02,  4.7043e-02, -3.7591e-02,  1.8712e-02, -8.3181e-02,\n",
      "        -1.9715e-02,  3.6721e-02,  3.5419e-02, -4.6781e-02, -7.8367e-03,\n",
      "        -2.6748e-02, -8.6308e-02,  2.3989e-02, -1.2710e-02,  3.7118e-02,\n",
      "        -6.2088e-02, -2.2962e-04, -4.9640e-02,  2.4384e-02,  1.5691e-01,\n",
      "         1.5421e-02,  5.5528e-02,  4.8312e-02,  5.6640e-02, -2.2735e-02,\n",
      "         5.3113e-03, -5.2211e-02,  2.6325e-02,  6.9295e-02,  2.4738e-02,\n",
      "        -5.3518e-03,  5.2276e-02, -2.4634e-02, -5.3242e-03,  1.2084e-01,\n",
      "        -2.6133e-02,  3.3964e-02,  9.2582e-03, -1.2223e-01, -2.1360e-03,\n",
      "        -7.8244e-02, -1.5748e-02,  1.4439e-03,  1.2431e-01,  6.0634e-02,\n",
      "         8.5934e-02, -6.0989e-02, -2.9897e-02, -1.1970e-03, -1.0762e-01,\n",
      "         1.0423e-02,  1.6176e-02, -1.3812e-02, -5.2755e-02,  1.6920e-02,\n",
      "         6.1367e-02,  9.1813e-02,  2.1540e-02,  7.7856e-03, -4.0828e-02,\n",
      "        -9.7598e-02, -4.1089e-02,  9.0935e-02,  1.8519e-02, -3.4424e-02,\n",
      "         2.8530e-03, -6.6620e-02, -8.9594e-03, -6.7013e-03, -4.6130e-02,\n",
      "        -2.1535e-02,  5.8145e-03,  4.0000e-03, -5.7107e-02,  4.8855e-02,\n",
      "        -1.1148e-01, -1.1978e-01,  6.8131e-02,  1.5512e-03,  3.5912e-02,\n",
      "         3.3328e-02,  3.1726e-02, -8.8611e-02,  1.4725e-01, -9.5569e-02,\n",
      "        -1.0785e-02, -1.3891e-03,  1.3467e-02,  4.0348e-02,  9.6515e-02,\n",
      "         1.6649e-02,  3.0992e-02, -1.5092e-02, -5.3478e-02,  2.6478e-02,\n",
      "        -1.3042e-02, -9.5301e-02, -6.6575e-03, -1.5733e-03, -9.9895e-03,\n",
      "         3.4082e-02,  1.5740e-01, -9.9586e-03, -5.3744e-02,  8.7394e-02,\n",
      "         4.2685e-02,  5.2481e-02,  1.7623e-02,  1.0548e-03,  4.5100e-02,\n",
      "         7.4265e-02, -7.1658e-03, -8.7438e-02, -3.9754e-02,  5.4727e-02,\n",
      "         4.6412e-02,  4.2058e-02, -3.2855e-02, -1.1088e-01, -1.7722e-02,\n",
      "         4.9851e-03, -8.0476e-02,  8.2968e-02, -8.2024e-02,  1.6164e-02,\n",
      "         3.7377e-02, -9.2349e-02, -1.1127e-01,  6.9750e-02,  8.6820e-02,\n",
      "        -2.7057e-02, -2.3069e-02, -7.3103e-02, -1.6484e-01, -2.0014e-02,\n",
      "         6.3153e-03,  7.7782e-02, -8.4823e-02,  2.2121e-02,  1.0625e-01,\n",
      "        -1.4292e-01,  8.1527e-02, -7.1087e-02, -8.0429e-02, -4.0732e-03,\n",
      "         6.4006e-02, -1.4278e-01, -7.9276e-03,  5.2838e-02, -3.7510e-03,\n",
      "        -5.9070e-02, -1.1084e-01, -1.6297e-03,  5.6736e-03, -7.3166e-02,\n",
      "        -6.8036e-02,  1.5117e-01,  1.9150e-02, -9.3975e-02, -4.8127e-02,\n",
      "         4.4899e-02,  5.5049e-02,  6.3477e-02,  5.0466e-02,  1.4346e-01,\n",
      "        -1.4061e-02,  1.8790e-01,  3.4009e-02,  1.4160e-03, -2.5282e-02,\n",
      "        -1.6245e-02,  5.4068e-02, -7.5012e-02, -7.5148e-02, -1.8582e-02,\n",
      "        -2.3466e-02,  1.9578e-02, -6.2413e-02,  1.2314e-01,  1.3701e-02,\n",
      "        -5.7122e-03,  8.9041e-02,  3.7946e-02,  4.1243e-02,  4.7171e-02,\n",
      "         2.7039e-02, -5.9925e-03, -2.8245e-02, -7.2878e-02,  1.4521e-02,\n",
      "         9.9702e-02,  6.4296e-02,  7.4185e-02, -7.1993e-02,  1.4546e-02,\n",
      "         7.7495e-02, -9.2409e-03, -3.8808e-02,  7.1566e-02, -1.4977e-01,\n",
      "         4.2293e-02, -4.2540e-02, -5.6876e-03, -4.4148e-02, -8.0183e-02,\n",
      "         7.5278e-02, -2.9656e-03, -4.9337e-02,  2.6277e-02, -1.1994e-02,\n",
      "        -9.6900e-03, -8.8157e-03, -1.7625e-02, -8.9690e-02, -3.2884e-02,\n",
      "        -5.1021e-03, -1.0199e-01, -1.6831e-02,  1.1726e-01, -3.4447e-02,\n",
      "        -2.8511e-02, -1.9198e-02,  3.6576e-03,  3.2099e-02,  4.5579e-03,\n",
      "         8.7041e-02, -3.0138e-02,  1.8212e-02,  7.4119e-02, -1.3839e-02,\n",
      "         5.3415e-02,  2.2786e-02,  1.0557e-01, -5.6927e-02,  3.3285e-02,\n",
      "         7.3276e-02,  1.0244e-01, -1.4565e-02, -1.0259e-01,  1.2200e-01,\n",
      "         6.1812e-02,  4.8889e-02, -5.6486e-02,  5.1047e-02,  9.3909e-02,\n",
      "        -1.0201e-02,  6.4712e-02, -2.3649e-02,  3.8729e-02,  6.1245e-03,\n",
      "        -4.3430e-02,  6.4039e-03, -8.9212e-02,  1.5119e-01,  7.2071e-02,\n",
      "         1.5732e-02, -2.2774e-02,  5.2327e-02,  2.5401e-02,  2.9843e-02,\n",
      "        -1.1558e-01,  5.9937e-02, -5.8328e-02,  7.1370e-02,  4.9816e-02,\n",
      "         6.5657e-02,  3.2430e-02, -8.6861e-03,  8.5977e-02,  1.9082e-02,\n",
      "         2.7206e-02, -1.9106e-03, -6.5907e-02,  4.0442e-03,  1.7387e-02,\n",
      "         1.3066e-01, -8.5428e-02, -2.6442e-02,  5.6974e-02, -8.7909e-02,\n",
      "         3.4048e-02, -5.8666e-02,  1.8037e-02, -6.2223e-02, -1.8848e-02,\n",
      "         9.5296e-03, -5.1592e-03,  5.1242e-03,  9.5190e-02,  1.1389e-02,\n",
      "        -6.1644e-02,  2.7198e-02,  2.2262e-02, -4.7755e-02,  6.3539e-03,\n",
      "        -2.4203e-02,  1.3476e-02,  5.5816e-02,  3.3884e-02,  5.4144e-02,\n",
      "        -2.0123e-02, -2.5729e-02,  3.2092e-02, -3.4289e-02, -1.2439e-03,\n",
      "         1.8775e-01,  5.8437e-02,  1.8716e-02, -5.8857e-02, -6.8036e-02,\n",
      "        -5.9856e-04,  1.0747e-01, -7.1370e-02,  1.3296e-03, -3.0167e-02,\n",
      "        -5.6810e-02, -1.0447e-01, -8.7226e-03, -3.1270e-03,  1.2601e-02,\n",
      "         1.8155e-02, -9.4597e-02, -4.7340e-02,  2.7440e-02, -3.4883e-02,\n",
      "        -3.2968e-02, -6.2905e-02, -1.2657e-02,  3.2411e-02,  1.2026e-02,\n",
      "         2.2878e-02, -5.3231e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 6.6658e-02, -7.8302e-02, -3.9761e-02, -4.1793e-02,  4.5831e-02,\n",
      "         4.8306e-02, -6.7736e-03,  7.5574e-02, -7.4495e-02, -3.0042e-02,\n",
      "         5.2244e-03, -1.3071e-02, -5.5794e-03, -8.3971e-02, -6.9471e-03,\n",
      "        -2.4258e-02,  1.0854e-01, -6.1369e-02, -1.4674e-01,  1.1226e-01,\n",
      "        -6.0065e-02,  5.3451e-02,  1.1262e-01, -4.9005e-03,  1.5264e-01,\n",
      "         7.8240e-02,  3.1867e-02,  7.0535e-03, -8.8613e-02, -1.6180e-02,\n",
      "         7.1920e-03,  3.6067e-02, -1.8580e-02, -6.9305e-02,  5.7444e-02,\n",
      "        -9.3223e-02,  6.4325e-02, -1.2735e-01, -1.6280e-02, -5.1730e-02,\n",
      "        -1.6762e-02,  1.6986e-01,  2.8526e-02,  7.5887e-02,  4.1897e-03,\n",
      "         5.6685e-02,  4.6633e-02, -3.6862e-02, -3.9126e-02, -2.2331e-02,\n",
      "         9.3762e-02, -1.0613e-02,  1.1766e-01, -3.7826e-02,  6.4190e-02,\n",
      "         2.1247e-02, -9.1414e-03,  9.0567e-02, -1.1170e-01,  1.5015e-02,\n",
      "        -1.6912e-02,  1.8269e-02, -6.4949e-02, -5.4902e-02, -8.6944e-03,\n",
      "         1.3896e-01,  1.1010e-01,  1.0749e-02,  8.7195e-02, -6.8369e-03,\n",
      "        -3.5939e-02,  1.3870e-02,  5.9698e-02, -8.9737e-05,  8.3753e-02,\n",
      "        -4.8358e-03, -3.8847e-02, -1.0107e-01,  7.5683e-02, -1.1180e-01,\n",
      "         3.0140e-02, -4.3089e-02, -2.2418e-02, -3.6128e-02, -1.0527e-01,\n",
      "         2.2898e-02,  4.6009e-02, -7.4225e-03, -5.6874e-02,  8.5350e-02,\n",
      "         5.1923e-03,  2.5627e-02, -8.9285e-03, -5.8058e-02,  7.0525e-02,\n",
      "         3.8854e-02,  2.7697e-02,  1.4393e-01, -4.0282e-02,  2.0928e-02,\n",
      "        -2.4592e-02,  6.1504e-02,  8.4973e-02, -6.5030e-03, -1.1406e-02,\n",
      "        -1.5721e-01, -1.2213e-01, -3.2998e-02, -1.0606e-02,  1.5931e-01,\n",
      "         1.4261e-01,  2.5770e-02, -4.0473e-02, -6.6654e-02,  3.4934e-02,\n",
      "         9.9253e-02, -1.0173e-02, -1.4505e-02,  6.1864e-02,  4.7759e-02,\n",
      "        -1.6578e-02,  3.0713e-02,  1.4806e-02,  8.6155e-02, -1.2338e-02,\n",
      "         7.9021e-02, -7.8331e-02, -6.0098e-02,  7.8730e-02,  2.3303e-02,\n",
      "        -8.3858e-03,  4.4462e-02, -5.4935e-02,  4.2922e-02,  4.7366e-02,\n",
      "        -3.2290e-04,  1.8469e-02, -5.9237e-02,  6.0935e-02,  2.3421e-02,\n",
      "         7.0576e-02, -1.8194e-02,  5.7329e-03,  1.2694e-01, -1.6639e-02,\n",
      "         5.9829e-02, -7.5157e-02, -6.8489e-02, -1.1888e-01, -1.4575e-01,\n",
      "        -6.2740e-03,  8.6623e-02, -1.9370e-03, -1.2883e-01,  4.0742e-02,\n",
      "        -3.1368e-02, -6.8863e-03,  6.7565e-03, -5.5464e-02, -5.8365e-02,\n",
      "        -4.6925e-02, -1.8427e-03, -6.9821e-03, -5.4991e-02,  1.4936e-02,\n",
      "        -6.0094e-02,  2.1199e-02,  1.6101e-03, -6.6419e-02, -1.0129e-01,\n",
      "         3.2519e-04, -9.6969e-02,  2.2424e-02,  8.3956e-02, -1.0915e-01,\n",
      "        -5.2411e-02,  7.9012e-02,  7.7652e-02,  7.2692e-02,  5.3036e-02,\n",
      "         8.0605e-03,  1.2090e-01,  4.4321e-02, -1.3145e-02,  2.7608e-02,\n",
      "        -2.4626e-03, -8.6162e-02, -2.0906e-02, -8.0314e-02,  8.6478e-02,\n",
      "         3.2060e-02, -7.4949e-02, -4.5875e-02, -9.1144e-02,  8.5149e-02,\n",
      "         4.7841e-02, -5.8479e-02,  9.3823e-02, -8.9949e-02, -2.2137e-03,\n",
      "         5.3320e-02,  2.4241e-02,  7.6287e-02, -7.3501e-02,  5.9457e-02,\n",
      "         2.5991e-02, -4.9862e-02,  2.1058e-02,  3.7085e-02,  5.8227e-02,\n",
      "         1.6736e-02,  1.3518e-02, -3.6454e-02,  8.9511e-02, -6.0161e-02,\n",
      "         4.3647e-02,  2.5404e-02,  1.6810e-03, -3.8325e-02,  5.1655e-02,\n",
      "        -6.2435e-03, -7.4342e-02,  1.5280e-02, -3.8896e-02, -4.6945e-02,\n",
      "        -4.9156e-02,  5.0480e-02, -1.1144e-01,  4.6365e-02,  4.1312e-02,\n",
      "         4.3370e-02, -6.4439e-02,  1.4321e-01,  5.6491e-03,  4.6217e-02,\n",
      "        -7.8084e-02,  2.2043e-02,  2.4072e-02, -1.1090e-01, -5.7180e-02,\n",
      "         1.3553e-01,  2.0576e-03, -6.7463e-02, -3.7952e-02,  9.7044e-02,\n",
      "         3.9006e-02,  2.3112e-02,  3.6162e-02, -4.4879e-02, -5.0205e-02,\n",
      "        -6.6276e-02,  6.0393e-02, -1.6587e-02, -4.2223e-02,  4.9360e-02,\n",
      "        -5.2514e-02,  5.3070e-02,  3.0898e-02,  8.4096e-03,  4.2029e-02,\n",
      "         8.3128e-03,  7.7944e-02,  7.4944e-02,  3.7365e-02, -1.7412e-02,\n",
      "        -1.7034e-02, -5.1705e-02, -1.0178e-01,  8.1377e-03, -1.1124e-02,\n",
      "         6.0315e-02, -1.2464e-01, -8.2909e-02, -2.0721e-02,  1.5134e-01,\n",
      "        -7.6029e-03, -5.5703e-02,  1.3161e-01,  1.1009e-01,  8.7843e-02,\n",
      "        -1.1565e-02, -7.0188e-02, -1.7204e-01,  9.7961e-02,  1.4806e-01,\n",
      "        -4.5438e-02, -2.6664e-03, -4.6997e-02, -7.0638e-02, -7.9939e-02,\n",
      "        -7.0988e-02, -1.1400e-01, -7.8130e-03, -8.5862e-02, -3.9800e-02,\n",
      "         7.1482e-03, -1.3455e-01, -2.8474e-02, -8.3467e-02,  6.1789e-02,\n",
      "        -1.2440e-02, -1.4384e-01, -5.4934e-02,  1.7171e-02, -4.3710e-02,\n",
      "         5.2462e-03, -9.8457e-02,  6.4931e-02,  3.0336e-02, -8.2045e-03,\n",
      "        -2.1457e-02,  1.9863e-02, -3.9212e-02,  3.6250e-02, -2.9250e-02,\n",
      "         4.0146e-03,  9.8803e-02, -3.5044e-03, -1.3867e-01,  6.7823e-02,\n",
      "        -1.1386e-02,  4.5815e-02, -4.6995e-02, -6.0331e-02,  8.9048e-02,\n",
      "        -3.3910e-03,  5.5142e-02,  1.0962e-01,  7.8482e-02, -5.7451e-02,\n",
      "         6.7650e-02, -5.0193e-02, -1.0531e-01,  3.0873e-02,  4.0250e-02,\n",
      "         3.5226e-02,  3.5651e-02, -1.3163e-02, -1.5697e-02, -1.3301e-02,\n",
      "        -7.5622e-02,  4.6634e-02, -6.0863e-02,  1.1601e-02,  5.8555e-02,\n",
      "         1.9718e-02,  1.4490e-02,  4.6890e-02,  1.9770e-02,  1.8599e-02,\n",
      "         1.5324e-02,  9.0858e-02, -9.4841e-02,  4.4712e-02,  1.0196e-01,\n",
      "         7.1711e-02,  2.8857e-02, -7.6147e-02,  1.1056e-01,  3.8540e-02,\n",
      "        -7.5464e-02, -1.1109e-01,  1.1038e-02,  7.1191e-02,  3.8999e-02,\n",
      "         8.1577e-02,  1.4265e-01, -2.5305e-02,  7.0406e-02, -2.0950e-01,\n",
      "        -1.0905e-01, -7.9404e-02,  9.4908e-02, -6.2777e-02, -4.6448e-02,\n",
      "         6.7760e-02, -4.1111e-02, -3.0499e-02, -6.7737e-02, -1.6252e-02,\n",
      "         7.7219e-02, -9.5822e-02,  7.5935e-03, -2.3492e-02, -3.9966e-02,\n",
      "         2.2348e-02, -5.5910e-02, -2.2430e-02, -1.2789e-01,  1.1506e-02,\n",
      "        -3.6499e-02, -2.3789e-02,  8.8967e-02,  3.7748e-04,  1.4302e-01,\n",
      "        -3.3631e-02, -3.5510e-02, -1.5043e-01,  7.7718e-02,  1.4879e-01,\n",
      "         6.6394e-02, -1.8917e-02,  1.0423e-02, -4.4962e-03, -2.3098e-02,\n",
      "         8.4583e-02,  1.2187e-01,  2.5955e-02,  2.3483e-02, -1.2860e-01,\n",
      "         2.7167e-02,  3.6408e-02,  8.3306e-02,  1.1587e-01,  6.6651e-02,\n",
      "         5.9024e-02,  1.0206e-01, -6.6102e-02, -1.1416e-02,  6.7382e-02,\n",
      "        -1.8530e-01,  7.1940e-02, -3.7391e-02, -1.0281e-01,  5.0257e-02,\n",
      "         4.7398e-02,  2.7898e-02,  6.5546e-02, -3.5585e-02, -1.5329e-02,\n",
      "        -3.8707e-02, -5.4844e-02, -2.3227e-02,  3.0108e-02, -2.5781e-02,\n",
      "        -2.8408e-02,  3.9738e-03,  9.0303e-02,  8.2566e-03,  2.2979e-02,\n",
      "        -5.5796e-02, -3.8515e-02, -6.0057e-02,  7.1408e-02, -6.8506e-02,\n",
      "        -8.3587e-02, -1.1510e-01,  3.3540e-02, -1.6315e-02, -4.7617e-02,\n",
      "        -1.2741e-01, -2.6345e-02, -6.0932e-02, -2.5297e-02,  1.7280e-03,\n",
      "        -5.4365e-02, -5.7350e-02, -4.4366e-02, -1.8187e-02, -5.9762e-02,\n",
      "         1.8093e-02, -6.1407e-02,  1.3368e-01,  3.7309e-02, -2.3302e-02,\n",
      "        -3.6866e-02,  6.9024e-03,  7.7365e-03,  4.0508e-02, -2.5169e-02,\n",
      "        -8.2504e-02,  1.2014e-01, -6.4195e-02,  6.6726e-02,  1.5957e-02,\n",
      "         1.0247e-01,  9.6323e-02,  5.0310e-02, -7.1386e-02, -6.2054e-03,\n",
      "        -1.6760e-01,  3.7466e-03, -9.4249e-02,  7.7653e-02, -1.2555e-01,\n",
      "        -6.1608e-02, -2.9333e-02,  1.3478e-02, -1.4650e-02, -9.3798e-02,\n",
      "         6.4758e-02,  2.1284e-02,  1.5329e-01, -8.6474e-02, -5.4156e-03,\n",
      "        -2.4129e-02,  1.0983e-01, -2.6136e-02,  1.7877e-02,  7.2377e-02,\n",
      "         2.4865e-02,  5.1694e-02,  5.9210e-02,  1.3274e-01, -4.0805e-02,\n",
      "         2.4143e-02,  6.7355e-02,  6.0903e-02,  6.5552e-02,  1.7681e-01,\n",
      "         4.1771e-02,  1.2728e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 5.0966e-02, -1.4745e-01,  7.7494e-02,  1.4598e-02,  1.1066e-01,\n",
      "        -3.6061e-02, -3.4456e-02,  2.3449e-03,  3.6120e-02, -2.1529e-02,\n",
      "         1.0209e-01,  1.2287e-03, -5.0131e-02,  6.2569e-02, -2.0442e-02,\n",
      "         3.2035e-02,  6.1605e-02, -9.9639e-02,  1.5433e-02,  3.8132e-02,\n",
      "        -6.6866e-02, -6.3091e-02, -6.1747e-02,  6.8062e-02,  8.8035e-02,\n",
      "        -1.0674e-01,  5.1352e-02, -3.5963e-02, -4.7417e-03, -4.0600e-03,\n",
      "        -1.0709e-01, -8.8151e-02,  1.0923e-01, -5.1789e-02, -1.1943e-01,\n",
      "        -3.2427e-02,  8.7168e-02,  1.1600e-01, -3.1433e-02,  2.1007e-02,\n",
      "        -2.0211e-02,  5.1138e-02, -3.1195e-02, -1.7929e-02,  1.6682e-02,\n",
      "        -5.8549e-03, -3.0055e-02, -1.2022e-01,  4.2940e-02,  5.0219e-03,\n",
      "        -7.6352e-02,  1.2055e-02,  1.1379e-02,  7.7296e-02, -3.7195e-02,\n",
      "         6.2380e-02, -9.9886e-02,  1.3775e-02, -3.7782e-02, -8.0343e-03,\n",
      "         1.1148e-02, -1.7144e-02, -8.2952e-02,  6.2111e-02,  1.4023e-02,\n",
      "         9.3064e-02, -1.8222e-02,  8.8978e-02, -9.5613e-02,  5.1005e-02,\n",
      "         6.4407e-02, -1.5327e-02, -1.6592e-02, -4.5361e-02, -3.1602e-02,\n",
      "        -4.6708e-02, -4.0381e-02,  9.3572e-02,  1.4583e-02,  1.5900e-02,\n",
      "         5.2908e-02, -6.2023e-02,  9.5726e-02, -2.2317e-02, -1.0207e-02,\n",
      "        -8.4064e-02, -8.5376e-02,  1.4583e-02,  6.5636e-02,  8.2487e-02,\n",
      "         6.9251e-02, -3.3851e-03,  2.0579e-02, -6.4329e-03, -6.3405e-03,\n",
      "         2.8375e-02, -5.4557e-02,  4.9721e-02, -2.8327e-02,  7.1326e-02,\n",
      "        -2.7338e-02,  7.1745e-02,  2.0902e-02, -1.4693e-02, -6.4021e-03,\n",
      "        -3.6755e-02,  2.3320e-02, -1.8848e-02, -8.2152e-03, -7.3774e-02,\n",
      "        -6.4569e-02, -3.3738e-02,  2.3054e-02, -1.0855e-02,  3.3617e-02,\n",
      "         5.3611e-02, -6.7952e-02, -5.8561e-02, -4.5781e-02,  2.4040e-02,\n",
      "        -8.8937e-02,  3.5465e-02,  5.0535e-02,  2.5044e-02, -4.3513e-03,\n",
      "        -3.2971e-02, -1.3832e-01, -8.0301e-02,  1.5525e-01, -8.0106e-02,\n",
      "         2.0949e-02,  1.1226e-02,  5.7637e-02,  9.5634e-02, -4.6271e-02,\n",
      "         6.2753e-02, -4.8439e-02,  5.5866e-02, -5.6149e-02,  8.9882e-03,\n",
      "        -2.2475e-02,  2.6102e-03, -7.5365e-02, -3.5781e-02,  8.7820e-03,\n",
      "        -2.7019e-02,  5.6331e-02,  1.6614e-03, -3.3956e-02, -6.9785e-02,\n",
      "         1.1633e-01,  5.9738e-02, -8.4658e-02,  3.5563e-02,  1.0341e-01,\n",
      "         7.0607e-05, -4.0593e-02,  3.8467e-02,  1.0799e-01,  1.7658e-02,\n",
      "        -9.0117e-02, -9.2431e-02, -7.4624e-02,  3.1521e-02,  4.0765e-02,\n",
      "        -1.2515e-01,  3.0535e-02,  1.1851e-02, -4.0310e-02,  2.2916e-02,\n",
      "         1.2250e-01,  6.9152e-02, -6.2053e-03,  4.0321e-02,  1.6208e-02,\n",
      "        -6.8822e-02,  2.1849e-02, -3.6987e-02, -4.4603e-02, -1.5947e-01,\n",
      "        -1.6658e-02, -9.6214e-02, -3.7753e-02,  5.4041e-02, -1.7003e-02,\n",
      "         8.1025e-02,  2.4926e-02,  5.5767e-02, -7.9529e-02, -2.1234e-01,\n",
      "        -4.7282e-02, -5.5761e-02,  3.0091e-02,  1.4731e-01, -6.2581e-02,\n",
      "         2.2454e-02, -6.7485e-02,  1.5281e-01,  4.6557e-02,  8.2848e-02,\n",
      "        -9.2783e-03,  7.2040e-02, -9.9636e-02,  6.1564e-02, -5.9368e-02,\n",
      "        -1.9590e-02, -1.0435e-02, -4.1890e-02, -4.7181e-02, -1.2446e-02,\n",
      "        -4.0818e-02,  6.1132e-02, -8.5487e-03,  8.7448e-02,  2.1625e-02,\n",
      "        -1.7572e-02, -9.9109e-02,  3.0057e-02,  7.2901e-02, -1.2618e-02,\n",
      "         3.7349e-02, -2.1917e-02, -6.9758e-02, -1.2695e-03, -1.3122e-02,\n",
      "        -5.0221e-02,  2.3869e-02,  5.0954e-02,  7.0282e-04, -3.3970e-02,\n",
      "        -2.8963e-02, -8.4868e-02, -2.6569e-02, -6.5083e-02,  8.5820e-03,\n",
      "        -4.4336e-03,  5.8201e-03,  2.1587e-02,  7.3191e-03,  4.7043e-03,\n",
      "        -5.8309e-02,  2.1552e-02, -2.5648e-02, -2.2331e-02, -1.0112e-01,\n",
      "        -3.7041e-02, -4.1032e-02, -6.8042e-02,  1.7894e-02, -2.6997e-02,\n",
      "        -2.7584e-02,  1.7612e-02, -1.9444e-03,  5.9923e-02,  6.8182e-02,\n",
      "         2.6522e-02, -6.7600e-02,  3.6002e-02, -1.6933e-02,  9.7652e-03,\n",
      "        -1.0266e-01, -3.6495e-03,  1.1981e-01, -3.1746e-02, -2.1659e-02,\n",
      "        -4.1714e-02,  7.0952e-02, -8.4005e-02,  3.2536e-03, -2.2566e-02,\n",
      "        -3.9273e-02,  3.3117e-03, -8.4515e-02,  5.7761e-02,  9.1372e-02,\n",
      "         9.6171e-03, -1.2380e-01, -8.3872e-04, -1.1604e-02, -2.1467e-02,\n",
      "         3.9992e-02,  8.3243e-04, -5.9930e-03, -2.2868e-02,  2.3452e-02,\n",
      "         1.2934e-02,  1.4610e-01,  6.3666e-04, -4.7834e-02, -1.6290e-02,\n",
      "         6.7797e-02,  3.1905e-02, -6.1453e-02,  4.7708e-02,  4.9836e-02,\n",
      "        -3.2332e-02,  1.4693e-02, -8.0379e-02,  5.6533e-02,  6.9687e-02,\n",
      "         6.2967e-02, -3.5479e-02, -9.2222e-03, -6.3729e-03,  8.0024e-02,\n",
      "         1.0684e-02,  5.5488e-02, -5.7777e-03,  1.2793e-01,  2.4388e-02,\n",
      "         6.8428e-02, -2.1748e-03, -4.4633e-02,  1.3514e-02,  2.4887e-03,\n",
      "        -1.9060e-02, -1.2467e-01, -4.7357e-02, -4.9894e-02,  9.8269e-02,\n",
      "        -6.8453e-03,  3.6830e-02, -3.3399e-02, -4.3410e-02, -9.6036e-02,\n",
      "         8.1545e-02, -3.5613e-02,  6.0910e-02, -5.0575e-02,  6.5858e-03,\n",
      "         5.8657e-02,  2.9649e-02, -5.0301e-02, -1.8220e-02, -7.9198e-02,\n",
      "         4.7839e-02,  3.2613e-02, -9.3417e-02,  6.7337e-02, -8.7942e-03,\n",
      "        -1.6459e-02,  2.7349e-02, -4.9454e-02,  6.1516e-02,  6.7670e-02,\n",
      "         4.5408e-03,  3.2664e-02,  3.3849e-02, -8.3817e-03,  2.9799e-02,\n",
      "        -6.4481e-02,  6.9932e-02,  1.3802e-02, -7.4295e-02,  2.8266e-03,\n",
      "         1.3482e-01,  1.6569e-02, -4.2818e-02,  5.2147e-02,  4.8331e-02,\n",
      "        -2.2739e-02, -1.8746e-02,  2.8624e-02, -8.2209e-02, -4.9650e-02,\n",
      "        -2.9904e-02, -3.1530e-02, -4.7788e-02, -4.7805e-02,  4.2077e-02,\n",
      "        -5.1374e-03,  9.3389e-02,  7.7671e-02, -1.0206e-02, -5.3528e-02,\n",
      "        -6.0535e-03,  2.0553e-02,  2.7381e-02,  8.1292e-03, -6.6471e-02,\n",
      "        -1.9595e-02,  2.1768e-02,  4.5958e-02,  5.7396e-02,  1.7548e-02,\n",
      "        -6.3863e-03, -1.7971e-01,  2.8201e-02,  1.6888e-02, -6.0088e-02,\n",
      "        -4.4732e-02,  5.1204e-04,  5.4047e-02,  1.5042e-02,  8.6862e-02,\n",
      "        -5.6149e-02, -8.0252e-02, -1.7712e-02, -3.3251e-02,  6.7082e-02,\n",
      "         5.7277e-02,  7.4467e-02,  1.3210e-02,  8.0749e-02, -4.9230e-02,\n",
      "         4.0126e-02,  6.4328e-02,  3.2686e-02,  5.5669e-02, -4.5429e-02,\n",
      "        -6.0456e-02,  5.9471e-03, -7.2037e-03, -6.6578e-02,  6.4264e-02,\n",
      "        -3.4567e-02,  1.8057e-01,  9.6095e-02,  1.7282e-02, -5.5573e-03,\n",
      "        -1.5813e-02,  7.3891e-02, -9.6589e-03, -5.6928e-02,  3.5197e-02,\n",
      "        -3.6848e-02,  3.3619e-02, -7.9201e-02, -1.0853e-03, -6.1366e-02,\n",
      "        -4.6373e-02, -2.3210e-02,  2.4530e-02, -2.9117e-02, -2.6862e-02,\n",
      "         2.0443e-02, -1.0311e-02, -4.5818e-02,  3.2928e-02, -1.4177e-01,\n",
      "        -3.3394e-02, -8.0657e-02, -1.1610e-01,  2.7471e-03, -1.1582e-02,\n",
      "         1.8751e-03, -3.5150e-02,  9.0628e-02, -1.1234e-02, -6.3072e-03,\n",
      "        -2.9522e-03, -2.5991e-02,  7.4267e-02,  5.3881e-02, -4.0242e-03,\n",
      "         7.6560e-03,  8.1244e-02, -1.5535e-02, -7.0901e-02,  4.0996e-03,\n",
      "        -1.9212e-02,  1.5392e-02, -4.2169e-02,  1.7310e-02, -7.4863e-02,\n",
      "        -5.8399e-02, -4.7026e-02,  1.1410e-01, -1.0140e-01, -9.5707e-02,\n",
      "         2.0097e-02, -1.0625e-01,  6.2864e-02, -1.0046e-01,  4.0808e-02,\n",
      "        -5.9520e-02, -5.2804e-02,  1.8317e-02, -1.1327e-01, -1.7123e-02,\n",
      "        -2.9642e-03, -1.2108e-02,  4.3250e-02, -6.8001e-02,  2.8993e-02,\n",
      "         2.3379e-03,  6.4308e-03, -5.0257e-02, -2.6099e-02, -9.2139e-03,\n",
      "         1.4326e-01, -3.5042e-02, -5.5747e-03,  1.4443e-01,  6.4646e-02,\n",
      "        -3.6846e-02, -3.1642e-02,  1.8773e-04, -6.0860e-02,  7.3784e-02,\n",
      "         3.4365e-02, -5.6993e-02,  4.9817e-02, -4.8040e-02,  7.2079e-02,\n",
      "         6.0582e-02,  1.5344e-03, -6.8195e-02,  2.4479e-02, -6.7752e-02,\n",
      "        -7.2611e-02, -2.7682e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0768, -0.0110,  0.0261, -0.0717,  0.0138, -0.0634, -0.0912,  0.0113,\n",
      "        -0.0347, -0.0304, -0.0077, -0.0341, -0.0804, -0.0470, -0.0264,  0.0091,\n",
      "         0.0322,  0.0482, -0.0405, -0.0913,  0.0352, -0.0308,  0.0159,  0.0034,\n",
      "         0.0155, -0.0147,  0.0697,  0.0984,  0.0066,  0.0651, -0.1385, -0.0525,\n",
      "        -0.0866,  0.0596, -0.0648,  0.0693,  0.0717,  0.0327, -0.0749,  0.1113,\n",
      "         0.0407,  0.0465,  0.1108,  0.0816, -0.0240,  0.0117,  0.0365, -0.0328,\n",
      "         0.0209, -0.0589,  0.0395, -0.0040,  0.0484,  0.0579,  0.0430,  0.0961,\n",
      "         0.0019, -0.0478, -0.0156,  0.0328, -0.0624,  0.0715,  0.0612, -0.0883,\n",
      "         0.0393, -0.0688, -0.0231, -0.0230, -0.0219,  0.0156, -0.0243, -0.1010,\n",
      "        -0.0313,  0.0016, -0.0020, -0.0170, -0.0236, -0.0161, -0.0517, -0.0867,\n",
      "        -0.0712, -0.0125, -0.0954, -0.0109,  0.1592,  0.0375, -0.0574,  0.0412,\n",
      "        -0.0757,  0.1175,  0.0951, -0.0161, -0.0222, -0.1225,  0.0901,  0.0392,\n",
      "        -0.0461, -0.0242,  0.0155, -0.0975, -0.0425, -0.0112,  0.0040,  0.0077,\n",
      "         0.0669, -0.0678, -0.0185, -0.0830, -0.0124,  0.0362, -0.0285,  0.1085,\n",
      "        -0.0133,  0.0715, -0.0329, -0.0025,  0.0326, -0.0271,  0.0487, -0.0552,\n",
      "        -0.0141,  0.0521, -0.0023, -0.0375, -0.1438,  0.0137,  0.0634, -0.0483,\n",
      "        -0.0128,  0.0103,  0.0111,  0.0511,  0.1563,  0.0164,  0.0060, -0.1368,\n",
      "        -0.1142, -0.0285, -0.0205,  0.0208,  0.0782,  0.0446,  0.0960, -0.0340,\n",
      "        -0.0171,  0.0837,  0.1210,  0.0210, -0.0156, -0.0047,  0.0567,  0.1111,\n",
      "        -0.0234, -0.0498, -0.0705, -0.0082,  0.1107,  0.0074,  0.0705, -0.0538,\n",
      "         0.0613, -0.1379,  0.0155, -0.0276,  0.0236, -0.0070, -0.0942, -0.0741,\n",
      "         0.0344,  0.0320, -0.0537, -0.1111, -0.0324,  0.1613,  0.0198,  0.1086,\n",
      "        -0.0317,  0.0004, -0.0473,  0.0628,  0.0596, -0.0103, -0.0568,  0.0624,\n",
      "        -0.0776, -0.1148, -0.0166,  0.0027,  0.0078, -0.0937, -0.0514, -0.0138,\n",
      "        -0.1482, -0.0669, -0.0712,  0.0135,  0.1173, -0.0033, -0.0064, -0.0263,\n",
      "        -0.0567,  0.0106,  0.0777, -0.0619, -0.0526,  0.0932, -0.0841, -0.0340,\n",
      "        -0.1270,  0.0130,  0.0067, -0.0860,  0.1337, -0.0305, -0.0314, -0.0653,\n",
      "         0.1493, -0.0126, -0.0196, -0.0949, -0.0565,  0.0440, -0.0889,  0.0118,\n",
      "        -0.0558, -0.0214, -0.0157, -0.0387, -0.0158,  0.0084, -0.0396, -0.0521,\n",
      "        -0.0809,  0.0183,  0.0045,  0.0053, -0.0093, -0.0678, -0.1156,  0.0174,\n",
      "         0.1187,  0.0416,  0.0693, -0.0025,  0.0486,  0.0294, -0.0075, -0.0575,\n",
      "         0.1809,  0.0164,  0.0446, -0.0271, -0.0230,  0.0786, -0.0114, -0.0058,\n",
      "         0.0358, -0.0731, -0.0365, -0.0286,  0.1120, -0.0882,  0.0127,  0.0710,\n",
      "         0.0003,  0.0062, -0.0400,  0.0463,  0.0816,  0.0720,  0.0084,  0.0478,\n",
      "         0.0634,  0.0475,  0.0025, -0.0680, -0.0101,  0.0497,  0.0274,  0.0548,\n",
      "         0.0372, -0.0325,  0.1441,  0.0648,  0.0218,  0.0187,  0.0017,  0.0058,\n",
      "         0.0606,  0.0349, -0.0842, -0.0129,  0.1517, -0.0832, -0.0344,  0.0722,\n",
      "         0.0201, -0.0085,  0.0686, -0.0399, -0.1319,  0.0208, -0.0094, -0.0035,\n",
      "         0.0502,  0.0415,  0.0268,  0.0031, -0.0782, -0.0470,  0.0647, -0.0245,\n",
      "        -0.0220,  0.0053, -0.0115,  0.0109,  0.0431,  0.0079, -0.0562, -0.0070,\n",
      "         0.0463, -0.0588,  0.0339,  0.0052, -0.0210,  0.1090,  0.0647, -0.0540,\n",
      "         0.0085,  0.0879, -0.0313,  0.0073,  0.0437,  0.0494,  0.0060,  0.1026,\n",
      "         0.0076,  0.0393, -0.0335, -0.0069, -0.1043,  0.0803, -0.0891,  0.1589,\n",
      "        -0.0709, -0.0418, -0.0459, -0.0026,  0.1630, -0.0228,  0.0362,  0.0665,\n",
      "         0.0199,  0.0311, -0.0793,  0.0584, -0.0846, -0.0298,  0.0471,  0.1816,\n",
      "         0.1290, -0.0308, -0.0354,  0.0684,  0.0022,  0.1397,  0.1273, -0.0121,\n",
      "        -0.0255,  0.1549, -0.1043,  0.0030, -0.0070, -0.0533, -0.1327, -0.0505,\n",
      "        -0.0394, -0.0871, -0.1559, -0.1013, -0.0389,  0.0533, -0.0024,  0.0499,\n",
      "         0.0578, -0.0086, -0.0890, -0.0100,  0.0792, -0.0145, -0.0229, -0.0173,\n",
      "        -0.0718,  0.0246, -0.0108, -0.0746, -0.1079, -0.1119, -0.0225,  0.0620,\n",
      "        -0.0441,  0.0702,  0.1055, -0.0187,  0.0807,  0.0159,  0.0401,  0.0435,\n",
      "        -0.0720, -0.1575, -0.0476, -0.0490, -0.0268,  0.1036,  0.0390,  0.0015,\n",
      "        -0.1407, -0.0818, -0.0521, -0.0193,  0.0634,  0.0762, -0.0572,  0.0335,\n",
      "        -0.0147,  0.0902, -0.0812,  0.0083, -0.1243, -0.0758,  0.1391,  0.0418,\n",
      "         0.0337, -0.0012,  0.0702, -0.0611,  0.0674,  0.0109,  0.0365, -0.0833,\n",
      "        -0.0679, -0.0756,  0.0385, -0.0285,  0.0510, -0.0359,  0.0606,  0.0541,\n",
      "         0.0934, -0.0538, -0.0293,  0.0203, -0.0051,  0.1183, -0.0098,  0.0472,\n",
      "         0.0742, -0.0267, -0.0643, -0.0058,  0.0205,  0.0397, -0.0012,  0.0355,\n",
      "         0.0729,  0.0082,  0.0999,  0.0031,  0.0537,  0.0390,  0.0033,  0.0092,\n",
      "         0.0299, -0.0649,  0.0372,  0.0805,  0.0463, -0.0983, -0.0180, -0.0175,\n",
      "         0.0584, -0.0766,  0.0062, -0.0004,  0.0233, -0.0832,  0.0306,  0.0634,\n",
      "         0.0414, -0.0457,  0.0292, -0.0461,  0.0299,  0.0362,  0.0514,  0.0055,\n",
      "        -0.0551, -0.0026, -0.0381, -0.0229, -0.0396, -0.0021,  0.1161, -0.0633,\n",
      "         0.0352, -0.0886,  0.1244, -0.0195,  0.0971,  0.0900, -0.1717, -0.0553],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-2.8312e-02,  4.9911e-02,  9.7769e-03, -1.7147e-02,  4.0901e-02,\n",
      "        -1.2317e-01, -1.1881e-01,  8.5501e-02,  1.1018e-01,  6.2696e-02,\n",
      "         3.1070e-02, -1.0946e-01,  7.7663e-02,  6.7539e-02, -1.3375e-04,\n",
      "        -1.2912e-02,  5.7624e-02, -7.1261e-02,  9.6846e-04, -4.5915e-03,\n",
      "         6.0058e-02,  2.9872e-02,  4.2197e-02,  3.8850e-02,  5.4885e-02,\n",
      "         4.4528e-02, -8.8942e-02,  1.1722e-01, -4.4009e-02,  3.8589e-02,\n",
      "        -7.9293e-02, -1.1473e-02, -2.3653e-02, -4.3948e-02, -2.1827e-02,\n",
      "        -4.3308e-04,  8.2051e-02,  6.2999e-02,  3.0414e-02,  1.3454e-02,\n",
      "         5.9846e-03,  1.5785e-02, -6.2734e-02,  7.9752e-02, -1.4402e-01,\n",
      "        -5.4157e-02,  8.3404e-02, -5.4182e-02, -3.7938e-02,  1.9626e-03,\n",
      "         6.2376e-02, -9.8665e-02,  1.1238e-01,  8.4942e-02, -5.1376e-02,\n",
      "        -4.4197e-03,  1.0537e-02,  7.6728e-02,  7.0679e-02,  7.5002e-02,\n",
      "         2.3206e-02,  2.2686e-02,  3.7321e-02,  3.3898e-02, -2.2739e-02,\n",
      "        -1.1890e-01,  7.7856e-02,  1.0845e-01,  6.1648e-02, -2.4917e-02,\n",
      "        -5.6272e-02, -2.0143e-04, -6.7984e-02, -5.5723e-02,  1.5601e-03,\n",
      "         9.5723e-02, -1.2334e-01,  2.3138e-02,  1.5915e-03,  1.7391e-02,\n",
      "         1.0060e-03, -5.5752e-02, -7.3283e-03,  7.8786e-02, -8.5108e-02,\n",
      "         5.5049e-02,  1.5016e-01, -3.1859e-02,  4.4934e-03, -5.7109e-02,\n",
      "         8.0624e-03,  1.0309e-01, -3.0260e-03, -1.8075e-02,  1.0297e-01,\n",
      "         1.8190e-02,  8.1257e-02, -1.0586e-01,  4.6859e-02,  8.7545e-03,\n",
      "        -1.8347e-02,  7.8826e-04,  3.4076e-02,  3.4202e-02, -4.6036e-02,\n",
      "         7.8401e-02,  1.2534e-02, -2.9604e-02, -1.4013e-01, -1.2220e-01,\n",
      "        -3.9575e-02,  4.2375e-02,  6.8481e-02, -1.1031e-01,  1.7292e-03,\n",
      "         5.6505e-03, -1.3347e-01,  5.8967e-02,  1.0500e-01,  2.8959e-02,\n",
      "        -1.3579e-01, -3.6767e-02, -6.5603e-03,  5.9650e-02,  3.4714e-02,\n",
      "         3.4603e-02,  6.3472e-02,  8.8572e-02, -3.0379e-02,  1.2246e-02,\n",
      "         3.0892e-02, -1.9900e-02, -2.0532e-02, -9.3364e-02,  2.0879e-02,\n",
      "        -3.1082e-02,  7.4723e-02,  3.4827e-02,  9.9355e-03,  4.0432e-02,\n",
      "         9.0674e-02, -6.2378e-02, -1.7440e-02,  1.5880e-02, -1.3521e-02,\n",
      "         6.1648e-02, -2.5270e-02, -1.0506e-02,  1.8069e-02, -5.2453e-02,\n",
      "         1.3252e-02,  6.9504e-03, -5.8516e-02,  4.6623e-02,  1.4739e-02,\n",
      "         6.7765e-03,  3.7023e-03,  3.7319e-02,  1.9224e-02,  2.6738e-02,\n",
      "         8.2818e-02, -1.2007e-04,  7.7645e-02,  9.2141e-03,  4.3738e-03,\n",
      "        -1.0779e-01,  8.4956e-02,  3.7886e-02, -1.3384e-01, -1.1208e-01,\n",
      "        -5.7828e-02, -9.7238e-02,  1.0206e-02,  6.5645e-03, -2.8718e-02,\n",
      "         1.5325e-02,  6.6613e-02,  2.6445e-02, -2.4962e-02, -4.9788e-02,\n",
      "        -4.3545e-03, -4.5150e-02, -1.4951e-02,  6.1688e-02, -9.0608e-03,\n",
      "        -8.5805e-02, -1.0172e-01, -9.2241e-02, -1.5714e-03, -2.6098e-02,\n",
      "        -2.3720e-02, -4.2816e-03, -4.2465e-02,  4.0990e-03,  5.9952e-02,\n",
      "        -8.0171e-02,  3.4743e-02, -5.9418e-02, -5.0707e-04, -1.7003e-02,\n",
      "        -3.6289e-02,  9.0298e-02, -2.5486e-02,  2.2962e-02,  8.9927e-03,\n",
      "         3.8505e-02,  5.5345e-02, -2.0447e-02, -3.3111e-02,  3.7436e-02,\n",
      "         6.5773e-02, -4.5183e-02,  4.1996e-02, -8.7999e-02, -1.1769e-02,\n",
      "        -4.3234e-02, -6.6346e-02, -3.5659e-02, -5.7530e-03,  3.8261e-02,\n",
      "         6.5813e-02, -2.6030e-02, -7.3186e-03, -6.0748e-02, -5.1565e-02,\n",
      "        -2.2371e-02,  1.2256e-02,  7.5072e-02,  1.9970e-02,  2.4642e-02,\n",
      "        -7.0200e-02,  3.6686e-02,  2.4515e-02,  3.2946e-03,  6.7995e-03,\n",
      "         8.7247e-02, -6.1754e-02,  2.3224e-02,  4.8788e-02, -3.7919e-02,\n",
      "        -4.5916e-02, -6.3038e-03, -6.4867e-02,  9.7451e-03, -2.9809e-02,\n",
      "         1.9220e-02,  4.9873e-02, -8.4751e-02, -3.8756e-02,  2.4613e-03,\n",
      "         1.2979e-02, -1.9546e-02, -1.7456e-03,  6.0348e-02,  3.5478e-02,\n",
      "         8.5359e-02,  4.5793e-02, -2.9652e-02, -1.9533e-02,  2.8801e-02,\n",
      "         2.0128e-02, -1.6773e-02, -2.2567e-02,  8.6599e-02,  7.6258e-02,\n",
      "        -1.3919e-02, -5.2701e-03,  1.5254e-02, -5.6596e-03,  1.2512e-02,\n",
      "        -1.1107e-01, -3.9220e-02, -4.3274e-02, -1.4759e-02,  6.3456e-02,\n",
      "        -3.9313e-02,  6.6304e-02, -2.5031e-02, -8.0906e-02, -9.2574e-02,\n",
      "         7.7114e-03, -3.8525e-02,  2.6354e-02,  6.7656e-02, -3.6397e-02,\n",
      "        -6.6598e-02,  4.9100e-02, -4.5302e-02, -9.6687e-02,  3.2252e-03,\n",
      "        -1.6827e-02,  9.3235e-02, -2.9695e-02,  8.8593e-02,  1.0684e-01,\n",
      "         1.0159e-01,  7.8147e-02, -2.3984e-02,  7.4527e-02,  9.7435e-02,\n",
      "         9.9969e-02,  4.1802e-02,  5.5769e-02,  4.1883e-02,  3.7363e-02,\n",
      "        -1.2641e-02,  3.1162e-02, -5.7425e-04,  5.6984e-02,  2.1873e-03,\n",
      "         3.2089e-02, -7.0392e-02,  2.0635e-02,  9.4762e-03, -1.5822e-02,\n",
      "         5.4450e-02, -2.8916e-02,  1.6877e-02, -7.8206e-03, -1.1922e-01,\n",
      "         2.3058e-02,  6.5806e-02,  9.5983e-03,  4.4597e-02,  1.8453e-02,\n",
      "         4.3058e-02,  6.1493e-02, -6.8039e-02, -3.5424e-02, -3.8730e-02,\n",
      "        -4.6403e-02,  2.2619e-03,  1.3438e-02,  3.6322e-02, -9.0361e-02,\n",
      "         2.3885e-02, -6.8223e-02, -2.8933e-02,  1.0164e-01,  1.5505e-02,\n",
      "        -7.0034e-02,  7.1678e-02, -6.8170e-02,  4.8597e-02,  8.5489e-02,\n",
      "         3.4030e-02, -1.1827e-02,  4.7249e-02, -5.7491e-02,  6.4812e-02,\n",
      "        -3.8081e-02,  3.1269e-02,  4.8112e-02, -2.2889e-02, -1.2078e-01,\n",
      "         8.6875e-03,  2.7524e-03, -5.2020e-02, -1.3657e-02, -3.4252e-02,\n",
      "         1.2507e-01,  6.4650e-02, -4.3744e-02,  2.1554e-02,  7.2027e-02,\n",
      "         4.6084e-02,  1.0100e-01,  7.4042e-02, -5.4211e-02, -1.1455e-01,\n",
      "         5.7521e-02, -4.2710e-02, -7.8814e-02, -1.8124e-02,  4.4737e-02,\n",
      "        -5.1269e-02, -6.7855e-02, -8.3722e-02, -6.4286e-02,  3.4506e-02,\n",
      "         8.8117e-02,  4.1227e-02, -1.0366e-01, -5.4640e-02, -3.3339e-03,\n",
      "         1.3867e-01, -5.8631e-02,  1.0841e-02, -9.4331e-02,  1.0992e-01,\n",
      "        -1.8052e-02,  5.6607e-02, -3.0553e-03, -9.7665e-02,  3.6189e-03,\n",
      "         3.8424e-02, -2.0226e-02, -1.0399e-01,  7.1986e-02, -8.7396e-02,\n",
      "        -2.1321e-02, -3.3681e-02, -4.8806e-02, -9.9724e-03,  3.4821e-02,\n",
      "        -3.6701e-02, -1.0064e-01, -4.4952e-02, -2.9649e-02,  6.7568e-02,\n",
      "         1.0062e-01,  1.5413e-02, -5.2982e-03, -8.1491e-02,  6.9497e-02,\n",
      "         7.5970e-03,  2.6650e-02, -7.8061e-02,  8.9628e-02,  5.9069e-02,\n",
      "        -2.8076e-03,  2.2840e-02,  4.9031e-02, -3.0829e-02, -1.4460e-01,\n",
      "         2.0347e-02,  3.0446e-02,  4.5471e-02,  8.5173e-02, -1.1764e-02,\n",
      "        -1.9823e-02, -1.1526e-02, -1.4037e-02, -5.7210e-03,  3.2612e-02,\n",
      "         8.8098e-02,  2.5476e-02,  5.3235e-02,  9.3301e-02,  6.9620e-02,\n",
      "        -6.3628e-02,  6.8000e-02,  1.4908e-01, -5.6959e-02,  5.9116e-02,\n",
      "         2.2112e-02, -2.4973e-02, -2.7610e-02,  4.1903e-02, -2.0115e-02,\n",
      "         5.7806e-02,  1.3158e-03, -8.3065e-02,  4.6314e-02, -9.3857e-02,\n",
      "        -9.9200e-03,  4.4497e-02, -1.1722e-02, -6.1344e-02, -1.3309e-01,\n",
      "         4.0768e-02, -2.1628e-02, -5.0834e-02,  1.0866e-01,  1.6634e-02,\n",
      "         7.5386e-02,  1.1037e-01, -3.8678e-02,  5.1629e-02,  3.5886e-02,\n",
      "         3.2558e-02,  1.4227e-03,  5.5960e-02,  1.0197e-03, -5.6617e-02,\n",
      "         2.2816e-02, -1.3664e-01,  1.3298e-01, -3.5689e-02,  1.8169e-02,\n",
      "        -3.9363e-02, -4.9693e-02,  8.3050e-02, -1.3196e-02, -4.6567e-02,\n",
      "         3.9041e-02,  2.8396e-02, -2.6041e-02,  6.8008e-02, -1.0233e-01,\n",
      "        -1.5822e-02, -3.0579e-02, -4.8071e-02, -6.4514e-02,  1.8201e-02,\n",
      "        -4.3278e-02, -4.3680e-03, -8.4785e-02, -5.5908e-02, -6.7275e-02,\n",
      "         8.3114e-02,  1.3823e-02,  4.9019e-02,  4.0267e-02, -5.4514e-02,\n",
      "         4.9135e-02, -4.8312e-02, -2.4285e-02, -9.7027e-02,  2.4834e-02,\n",
      "         1.4886e-02,  6.9949e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 3.0289e-02,  3.1503e-02,  3.9986e-02,  1.3083e-01, -5.3132e-02,\n",
      "         2.9113e-02,  4.7187e-03,  5.0454e-02,  1.0700e-01, -2.2314e-02,\n",
      "         2.6524e-02, -1.1840e-02,  5.0855e-03,  7.3779e-04, -1.1865e-03,\n",
      "        -4.7954e-02,  1.0474e-02,  2.8582e-02, -7.9896e-02,  7.6038e-02,\n",
      "         4.5977e-02, -1.4148e-02,  3.9841e-02,  1.8766e-02,  8.0392e-02,\n",
      "         2.6746e-02,  2.9566e-02, -2.5976e-02,  1.6478e-02, -5.0035e-02,\n",
      "         2.4266e-02,  4.7684e-03, -4.6095e-02,  5.4383e-02, -5.5842e-02,\n",
      "        -6.3235e-02,  1.0002e-01, -7.9192e-03,  4.9059e-02, -2.9653e-02,\n",
      "         7.4298e-02,  3.2793e-02,  8.6242e-02,  1.3700e-03,  1.4234e-02,\n",
      "         7.6310e-02,  3.2565e-02, -5.5205e-02, -2.8722e-02, -3.9794e-02,\n",
      "         8.0323e-02, -1.0903e-01, -4.8134e-04,  4.3818e-02, -3.0959e-02,\n",
      "        -5.7084e-02,  4.3061e-02,  4.2138e-02,  7.2363e-02,  4.3792e-02,\n",
      "        -7.2850e-02,  5.2529e-03,  4.6195e-03, -6.2514e-02,  8.1972e-02,\n",
      "        -1.2628e-02,  1.1640e-01, -7.5081e-02,  2.6473e-02, -6.2586e-02,\n",
      "        -6.8327e-02,  5.4805e-03, -8.0045e-02, -1.0655e-02, -7.7074e-03,\n",
      "        -8.1215e-02, -1.6442e-02,  6.8840e-03, -6.9273e-03, -4.1731e-02,\n",
      "        -6.2782e-02,  6.2828e-02, -8.7719e-02,  1.7283e-02, -5.3315e-02,\n",
      "        -9.8364e-02, -9.7457e-02,  8.1505e-02,  2.6662e-02,  5.2712e-02,\n",
      "         5.1618e-02, -3.9540e-02, -1.0101e-01, -2.3273e-02,  1.6070e-02,\n",
      "        -3.2476e-02, -3.7883e-02, -1.9677e-02, -3.3466e-02,  1.7523e-02,\n",
      "        -9.1086e-02, -4.3556e-02,  7.8876e-02, -4.1143e-02, -3.5400e-02,\n",
      "        -1.7865e-02,  1.7630e-01,  1.3965e-01, -5.0848e-02, -3.6669e-02,\n",
      "         2.1116e-02, -1.0324e-01, -1.7145e-02,  6.3624e-02, -7.2753e-02,\n",
      "         8.1110e-04,  7.7122e-02,  6.0167e-02,  9.4302e-02,  3.3645e-02,\n",
      "         5.1997e-02,  9.3938e-03,  1.5380e-02,  3.0624e-02,  1.8364e-02,\n",
      "         9.4459e-02, -5.3204e-02,  5.3909e-02,  8.4368e-02, -2.6575e-02,\n",
      "         5.8741e-03,  1.7135e-01,  3.8734e-02,  1.1533e-01, -3.4991e-02,\n",
      "        -1.3902e-01, -5.0564e-02,  2.5342e-02,  1.9510e-03, -4.5458e-02,\n",
      "        -7.6664e-02,  1.0237e-01,  7.7267e-03,  5.8986e-02, -1.9288e-02,\n",
      "         5.3286e-02,  3.6359e-02,  8.0501e-02, -8.3045e-02,  3.3307e-02,\n",
      "         1.5659e-03,  9.6013e-03, -1.5590e-02, -5.1359e-02, -7.0246e-02,\n",
      "        -1.1975e-02,  2.6491e-02, -3.2005e-02,  6.8249e-02,  4.7669e-02,\n",
      "         4.7641e-02, -2.1512e-02, -6.3295e-02, -4.1788e-02, -1.5279e-02,\n",
      "        -9.7037e-02,  2.2685e-02,  2.0949e-02,  3.3309e-02,  9.4829e-03,\n",
      "         5.6710e-02, -7.6783e-03, -1.3969e-01, -4.1760e-02,  8.8335e-03,\n",
      "         4.3914e-02, -1.1144e-02,  2.1213e-02,  5.0143e-02, -1.7819e-02,\n",
      "        -3.6000e-02, -9.8346e-02,  1.8010e-02,  1.1031e-02, -4.7298e-02,\n",
      "        -2.5419e-02, -4.0803e-02,  3.5511e-02,  9.2070e-03,  6.9367e-03,\n",
      "        -4.2061e-02, -1.0377e-02,  8.0876e-02, -5.6107e-02,  5.7277e-02,\n",
      "         8.7439e-03,  1.8353e-02, -4.1559e-02,  3.4507e-02, -1.0548e-01,\n",
      "        -4.0571e-02, -2.1289e-02,  3.0586e-02,  5.1678e-03,  8.7577e-04,\n",
      "         1.3942e-01, -1.1645e-02,  7.2364e-02,  6.5043e-02,  2.4132e-02,\n",
      "         1.1002e-01,  6.1222e-03,  6.6061e-03, -5.2206e-02, -1.3325e-02,\n",
      "        -8.5573e-03, -2.0275e-03,  1.6365e-03,  2.6494e-02,  7.1705e-02,\n",
      "        -7.1865e-02,  8.4742e-02,  6.0429e-02, -5.9917e-04, -5.1137e-02,\n",
      "        -5.9481e-02, -7.6383e-02,  4.8239e-02, -3.4069e-02, -9.6994e-02,\n",
      "         1.8230e-02,  8.8950e-02,  8.6447e-02, -2.9383e-02, -9.0702e-02,\n",
      "        -3.7237e-02, -3.5979e-02, -4.2816e-02, -7.7253e-02,  7.3348e-03,\n",
      "         4.4436e-02, -1.5954e-01,  1.2394e-01,  1.1889e-02,  1.5041e-02,\n",
      "        -6.7389e-02, -4.5964e-02,  2.0859e-02, -3.0347e-02, -2.0750e-02,\n",
      "         3.9519e-02, -2.8886e-02, -8.1723e-02, -2.2986e-02, -2.3117e-03,\n",
      "         7.9396e-02, -4.6225e-02,  5.9592e-02, -6.6315e-02, -4.8456e-02,\n",
      "        -4.7836e-03, -6.7407e-02,  4.6288e-02,  1.5025e-01,  3.1964e-02,\n",
      "        -1.0685e-01, -3.1458e-02, -4.1457e-02,  7.1839e-02, -9.0231e-02,\n",
      "         3.3797e-02, -2.6273e-02, -6.0258e-02, -3.0063e-02, -9.9684e-02,\n",
      "         8.9154e-02,  4.6204e-02,  1.0030e-02, -2.1860e-02, -9.5296e-03,\n",
      "        -2.6632e-02, -2.0542e-02, -8.8112e-02, -3.1891e-02,  8.1285e-02,\n",
      "         3.4284e-02,  9.3343e-02, -7.2938e-02,  4.2222e-02,  8.5092e-02,\n",
      "        -6.9859e-02, -1.1665e-01, -1.7408e-02, -1.5403e-02,  5.4243e-02,\n",
      "         9.8341e-03, -2.8077e-02, -2.9991e-02,  3.4399e-02,  1.4826e-02,\n",
      "         1.0260e-02,  8.0673e-02,  5.1878e-03, -8.1736e-02,  8.6033e-02,\n",
      "         8.2636e-02,  5.0595e-02, -1.1922e-01,  9.3888e-03,  2.7255e-02,\n",
      "         2.7873e-02,  2.2796e-02,  1.8762e-02,  1.4380e-01, -1.4723e-01,\n",
      "        -1.4255e-02, -3.0604e-02, -3.7668e-03,  1.1167e-02, -8.0839e-02,\n",
      "         1.4414e-02, -2.5007e-02, -2.3666e-02, -2.7692e-02, -1.6474e-02,\n",
      "         5.1326e-02, -6.8901e-03,  2.6673e-02, -1.9049e-02, -4.9653e-02,\n",
      "         1.1313e-01,  8.5847e-02,  1.3205e-01, -4.7806e-02, -9.3220e-02,\n",
      "         4.1846e-02, -4.5715e-02,  2.4093e-02, -3.6066e-02,  5.0121e-02,\n",
      "         2.4745e-02, -9.0033e-02,  5.9747e-02, -5.9992e-02, -2.5795e-02,\n",
      "        -3.5649e-02,  2.3503e-02,  1.4340e-01, -5.7906e-02, -8.6132e-03,\n",
      "        -6.0701e-03,  3.0256e-03, -6.0207e-02,  1.3398e-02, -3.4405e-03,\n",
      "         3.6077e-02, -7.9061e-02, -4.5184e-02, -6.7206e-02,  8.3835e-02,\n",
      "        -1.4701e-02,  2.4760e-02,  1.7550e-02,  5.2360e-02, -1.1143e-01,\n",
      "        -6.0042e-02, -2.1617e-02, -2.3820e-02, -1.9716e-02, -1.1295e-01,\n",
      "        -1.7096e-02, -5.0607e-02,  9.7075e-02,  2.0780e-02, -4.8206e-02,\n",
      "         4.0675e-02, -5.4123e-02,  2.6274e-02, -1.1451e-01,  5.9652e-02,\n",
      "        -2.4965e-02, -2.3823e-02,  5.4150e-03, -2.5337e-03, -5.9982e-02,\n",
      "        -3.6474e-02, -1.8158e-02, -1.5301e-02,  1.1725e-02,  2.3499e-02,\n",
      "         7.4033e-02, -4.0130e-02, -5.1274e-02,  9.0815e-02,  5.4975e-02,\n",
      "        -3.4270e-02,  4.5382e-02, -7.2244e-02, -7.0036e-02, -9.7178e-03,\n",
      "        -3.3955e-02, -3.5253e-02,  8.1896e-02,  7.5562e-03, -7.9211e-02,\n",
      "        -1.0875e-01,  1.2409e-03,  7.7800e-02,  1.0634e-02, -8.2665e-02,\n",
      "         1.3230e-02, -3.4552e-02,  9.1453e-02, -6.4865e-02,  4.5128e-02,\n",
      "        -1.1324e-01, -5.8086e-02,  4.5286e-02, -3.5615e-02,  1.1491e-03,\n",
      "         4.5156e-02,  2.6197e-02, -9.7915e-02, -8.8574e-02,  6.3982e-02,\n",
      "        -7.3688e-02,  3.8706e-02,  8.2396e-02,  7.6938e-02, -2.0139e-02,\n",
      "        -6.2673e-02, -8.2048e-02,  5.6388e-02,  1.7644e-02,  4.3307e-02,\n",
      "         8.2072e-03, -4.8394e-02,  7.1145e-03, -1.4995e-01,  6.3767e-02,\n",
      "        -1.7300e-02, -4.0330e-04,  2.5645e-02,  6.1843e-02, -5.0088e-03,\n",
      "         3.9473e-03,  8.7710e-02,  3.0694e-02, -1.5863e-02,  1.2367e-01,\n",
      "         5.8815e-02,  6.1809e-02,  1.1823e-01,  3.4193e-02, -1.3734e-01,\n",
      "        -8.3475e-03, -1.3101e-02,  1.7372e-01,  3.1849e-02,  5.8699e-02,\n",
      "        -8.2168e-02,  2.9679e-02,  2.9754e-02, -1.9589e-02, -2.3867e-05,\n",
      "         2.9229e-03, -5.9795e-02,  1.0513e-01, -2.3250e-02,  1.5259e-02,\n",
      "        -9.9677e-04,  5.2436e-02,  4.5202e-02, -5.3536e-02, -3.1198e-02,\n",
      "         1.1600e-01,  8.2992e-02, -6.0462e-02, -6.9867e-02, -2.0561e-03,\n",
      "         6.2426e-02,  3.0686e-02,  7.3595e-03, -5.2512e-03, -8.7785e-02,\n",
      "         7.2232e-02, -5.5166e-02,  5.2830e-02, -3.4109e-02, -3.5072e-02,\n",
      "        -7.8913e-02,  3.6241e-02,  4.8680e-02, -2.4749e-02,  9.5748e-02,\n",
      "         1.1784e-01,  6.6303e-02, -3.3105e-02,  3.1397e-02,  4.8392e-02,\n",
      "        -9.6809e-02,  6.1331e-02,  3.0868e-02,  3.2937e-02,  1.4860e-02,\n",
      "        -8.8214e-02, -7.5167e-02, -2.6680e-02, -7.2619e-02, -3.8868e-02,\n",
      "         4.7005e-02, -1.5254e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-3.7034e-02, -4.5888e-02,  8.8781e-03,  2.7156e-02,  5.8858e-02,\n",
      "         1.2498e-03, -2.9473e-02, -2.4259e-02,  2.7695e-02,  4.8506e-02,\n",
      "        -1.3610e-02,  2.4264e-02, -1.0506e-02, -2.2343e-02, -1.2575e-02,\n",
      "        -2.7388e-02,  3.7047e-03, -9.8502e-02, -7.6187e-02, -1.3275e-02,\n",
      "         4.0868e-02,  3.1048e-02,  2.9744e-03, -3.4535e-02,  6.2692e-02,\n",
      "        -1.0555e-01, -1.8775e-03, -6.1323e-02,  1.1437e-02,  6.9841e-02,\n",
      "        -1.2952e-02,  7.9710e-02, -3.6756e-02,  1.2847e-02,  1.0407e-01,\n",
      "        -8.7324e-02, -1.0587e-01, -3.1902e-02, -8.2598e-03, -1.0516e-01,\n",
      "        -9.7262e-02,  1.1731e-02, -1.1542e-02, -1.0035e-01, -8.8628e-02,\n",
      "        -1.6604e-02, -6.6435e-04, -5.5660e-02, -5.8090e-03, -9.9288e-03,\n",
      "         2.7286e-02, -4.0562e-02, -1.3763e-02, -5.6210e-02, -8.2477e-03,\n",
      "         3.0968e-02, -2.2097e-02,  2.6884e-02, -4.4554e-03,  6.1624e-02,\n",
      "         5.7080e-02,  9.1388e-03, -2.2383e-02,  3.1594e-02,  9.0034e-02,\n",
      "         2.9283e-04, -2.4813e-03, -4.8279e-02,  2.9078e-02,  3.5868e-02,\n",
      "         4.1491e-02, -6.3660e-02, -8.4763e-02, -8.1597e-02, -5.1852e-02,\n",
      "         2.2601e-04,  1.0845e-01,  3.0973e-02, -1.5400e-01,  3.3164e-02,\n",
      "         7.9088e-02,  6.5250e-02,  5.1900e-02, -4.2283e-02, -1.1346e-01,\n",
      "        -9.0076e-03,  1.1980e-01, -1.1909e-02,  1.2310e-02,  1.8831e-02,\n",
      "        -4.9647e-02,  7.0969e-02, -2.3682e-02, -8.6618e-02,  5.2677e-02,\n",
      "         7.8079e-03, -1.0115e-01,  7.5915e-02, -4.8108e-02, -1.3128e-01,\n",
      "         6.4873e-02, -7.1029e-03, -1.4379e-01, -2.1432e-02, -5.3666e-02,\n",
      "         3.7874e-02, -8.1764e-02,  1.6618e-01,  7.1652e-02,  4.2189e-02,\n",
      "        -4.8112e-02,  5.0704e-02, -8.4332e-02,  2.3637e-02, -1.1713e-02,\n",
      "        -1.4738e-01, -5.6326e-02, -8.2328e-02, -6.9366e-03,  8.9393e-03,\n",
      "         9.0724e-02, -3.4346e-02, -1.7982e-02, -1.4817e-02, -9.2182e-02,\n",
      "         3.9414e-02, -1.3945e-02, -9.3391e-02,  1.0452e-01,  8.3443e-02,\n",
      "        -8.3101e-03,  5.8458e-02,  3.4724e-02, -9.1750e-02,  2.9846e-02,\n",
      "        -9.8895e-02, -2.4202e-02,  4.6580e-02,  4.4337e-02, -1.2447e-02,\n",
      "        -8.0480e-03, -5.6974e-03, -3.7265e-02,  7.7061e-02,  5.1464e-02,\n",
      "        -7.0224e-02, -4.4164e-02,  2.5564e-02,  1.2461e-02, -2.4537e-02,\n",
      "         2.2466e-02,  6.7765e-03, -2.1143e-02,  1.3173e-02, -4.8422e-02,\n",
      "        -2.4130e-02,  4.0795e-02, -6.9050e-02,  5.2960e-02,  2.9344e-02,\n",
      "         6.1323e-02,  2.6642e-02, -1.5501e-02,  1.1257e-02,  5.2199e-02,\n",
      "        -1.9131e-02, -7.1120e-02,  1.5206e-01, -5.5123e-02,  1.6600e-02,\n",
      "        -1.7471e-02,  5.4039e-02,  7.3465e-02, -1.4534e-02,  3.2988e-02,\n",
      "         1.0805e-01,  2.3235e-03,  2.6146e-02,  5.6207e-02,  2.4650e-02,\n",
      "         1.0190e-02, -4.5924e-03,  4.1432e-02, -4.8620e-02, -2.9034e-02,\n",
      "        -2.9012e-02,  1.4155e-02,  3.5942e-02, -9.4590e-03, -3.9627e-02,\n",
      "        -5.3268e-02,  1.3831e-01, -3.0257e-02, -5.7423e-03,  4.2466e-02,\n",
      "         1.2649e-01, -5.0767e-02, -1.1174e-02, -2.3112e-02,  3.8812e-02,\n",
      "        -6.3522e-02,  9.1453e-02,  2.6309e-02, -1.1686e-01, -3.9759e-02,\n",
      "         2.4578e-02, -4.7622e-03, -5.6869e-02,  9.6072e-02,  1.3556e-02,\n",
      "        -2.8459e-02, -4.5581e-02,  1.2914e-01, -1.1633e-02,  1.1193e-01,\n",
      "        -8.6753e-02, -8.5673e-03, -7.3127e-02, -3.6154e-02, -9.3040e-02,\n",
      "        -3.7462e-02,  1.2344e-01,  8.0146e-02, -1.7490e-02,  1.1924e-01,\n",
      "        -1.0738e-02,  6.7925e-02, -6.9445e-02, -2.5708e-02, -5.6665e-02,\n",
      "        -1.5419e-01,  1.2431e-01, -7.5615e-03, -1.0575e-01,  8.1955e-02,\n",
      "        -3.7937e-02,  8.6439e-02, -3.1533e-03,  1.4085e-01,  3.6980e-02,\n",
      "        -1.3440e-02, -5.1998e-02,  5.9634e-02, -4.4400e-02,  1.6468e-02,\n",
      "         3.7003e-02,  2.0843e-02,  4.8651e-02, -3.7829e-02,  1.0212e-01,\n",
      "        -1.8587e-02,  4.5990e-02, -4.5087e-03, -1.0517e-01, -7.8714e-02,\n",
      "        -2.2157e-02, -5.8386e-02,  7.0721e-02, -1.4240e-02, -1.0749e-01,\n",
      "        -6.8921e-02, -3.1443e-02, -3.2220e-02, -6.4972e-02,  1.1256e-02,\n",
      "         4.3494e-02,  1.8916e-02, -1.8547e-01, -2.1113e-02, -3.5792e-02,\n",
      "        -1.2145e-02,  4.6165e-02, -1.1010e-01,  3.3331e-04,  8.4547e-02,\n",
      "         5.4524e-02,  4.8118e-02, -9.5097e-02, -7.2445e-02, -6.6263e-05,\n",
      "         5.1787e-02,  4.9852e-02, -4.7932e-02, -1.2280e-02, -1.6250e-02,\n",
      "        -1.4342e-02, -1.1116e-01, -5.5778e-02, -7.7247e-03, -8.1662e-02,\n",
      "        -4.3206e-03,  6.6698e-02, -5.0373e-02, -1.2831e-01,  7.0735e-02,\n",
      "        -4.0484e-02, -2.6315e-02, -2.7391e-02, -8.0403e-02, -6.9732e-03,\n",
      "         5.4342e-02,  2.0656e-02,  1.5141e-01,  1.0275e-01,  1.5837e-03,\n",
      "        -1.4563e-01,  8.5911e-05,  4.7454e-03, -7.8300e-02,  4.8858e-02,\n",
      "        -2.1546e-02,  1.4427e-02,  4.6923e-02, -4.1582e-02,  3.4860e-02,\n",
      "         1.6094e-01, -2.8653e-02,  6.8671e-02,  3.9210e-02, -2.7989e-02,\n",
      "         1.2157e-01,  3.4874e-02,  1.0473e-01,  5.0698e-02, -6.6427e-02,\n",
      "        -8.5859e-02,  4.0868e-02, -8.1263e-02,  1.2227e-04, -4.1179e-02,\n",
      "         7.0834e-03,  8.5109e-02, -2.0567e-02,  6.0143e-03, -8.9583e-02,\n",
      "         6.3068e-02, -4.5089e-02,  2.6703e-02,  5.3511e-03,  9.8072e-03,\n",
      "         9.1949e-04,  4.8803e-02, -1.2944e-02, -1.6477e-02,  3.7466e-03,\n",
      "        -7.1968e-02, -6.9599e-02, -1.0072e-01, -7.0090e-02,  3.5817e-02,\n",
      "         6.2147e-02,  8.6350e-02,  8.2676e-02,  6.9734e-03, -1.6660e-01,\n",
      "         3.0636e-02, -7.5360e-02,  8.7070e-02,  4.6590e-02, -1.2240e-02,\n",
      "         4.7421e-02,  1.4499e-01, -3.2117e-02,  6.7256e-03, -9.1146e-03,\n",
      "         5.6627e-02,  3.4365e-02,  3.5674e-02,  1.1961e-03,  9.1195e-03,\n",
      "        -1.0258e-01, -2.6809e-02, -3.6439e-02, -5.3987e-02, -3.7285e-02,\n",
      "        -4.7299e-02,  2.0322e-02, -7.9408e-02, -7.7213e-02, -4.1219e-02,\n",
      "         1.1305e-01, -3.6860e-02,  3.4759e-02,  4.5197e-03, -1.8849e-02,\n",
      "        -1.1627e-02,  7.8283e-02, -5.6437e-02,  3.5024e-02,  6.2222e-02,\n",
      "        -8.2901e-02,  7.1049e-02,  9.9048e-03,  8.3881e-02,  3.7555e-03,\n",
      "         8.8532e-02,  9.2635e-02,  1.6246e-02, -3.0551e-02,  4.0173e-02,\n",
      "         3.9328e-02,  9.8969e-03,  7.2826e-04, -8.5527e-03,  1.9672e-02,\n",
      "         1.0268e-01, -4.0752e-03, -5.5843e-02,  1.5902e-02,  7.0855e-03,\n",
      "        -3.0325e-02,  2.9130e-02, -7.9757e-02,  2.0168e-02,  1.3599e-02,\n",
      "        -2.4822e-02, -8.0696e-03,  7.8805e-03,  3.1998e-04, -3.3752e-02,\n",
      "        -2.3653e-02,  7.4149e-02, -9.0394e-03, -6.5222e-03, -3.0573e-02,\n",
      "         1.1063e-01,  7.5828e-02,  4.1677e-02,  1.3911e-02, -7.0996e-03,\n",
      "         2.3597e-03,  2.6949e-03, -5.3042e-03,  7.1347e-02,  2.7978e-02,\n",
      "         9.5793e-04, -2.3873e-02, -7.2959e-02,  3.1148e-02, -6.5378e-02,\n",
      "         4.4773e-02, -4.6407e-02, -2.7808e-02,  6.0678e-02,  2.2824e-02,\n",
      "         1.2299e-02, -1.2252e-01, -9.4176e-02, -3.1335e-02,  6.1090e-02,\n",
      "        -8.9544e-02, -7.8463e-02, -1.0646e-01,  1.2856e-01,  5.3371e-02,\n",
      "        -3.5043e-02,  4.9204e-02, -2.7718e-02, -1.8169e-03, -3.2086e-02,\n",
      "         7.7823e-03,  6.8141e-03,  9.3693e-02,  1.6695e-02, -7.0995e-03,\n",
      "        -8.1406e-02, -1.0529e-02,  2.3930e-02, -2.4667e-02,  1.4599e-02,\n",
      "         2.2815e-02,  6.4431e-02, -8.6203e-02, -1.9157e-01,  3.7300e-02,\n",
      "        -2.8549e-02, -2.9900e-02,  2.0874e-02, -1.8929e-01,  6.7435e-02,\n",
      "        -4.1862e-02,  4.9628e-04,  7.5833e-03,  8.0471e-02, -1.7851e-02,\n",
      "        -4.5390e-02,  2.1833e-02, -1.6886e-02, -1.0043e-02, -7.4905e-02,\n",
      "        -9.9795e-04, -2.0626e-02,  8.3278e-02, -7.4464e-02,  3.2107e-02,\n",
      "         4.9412e-02, -5.9202e-02, -6.2015e-02,  1.0825e-02,  8.4142e-02,\n",
      "         6.0584e-02,  2.8453e-02, -6.4364e-02,  3.4312e-02, -3.1387e-02,\n",
      "        -1.0054e-02,  6.2364e-02,  9.9319e-02,  4.8268e-02,  3.6428e-02,\n",
      "         4.7602e-02, -2.9711e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 1.7651e-03,  3.1396e-03, -1.4551e-02, -3.5147e-03, -2.1400e-02,\n",
      "         1.1861e-01,  2.7863e-02, -5.9323e-02,  5.4408e-02, -3.9685e-02,\n",
      "        -2.9819e-02,  1.3290e-02, -3.3680e-02, -3.0514e-02, -9.5577e-02,\n",
      "        -2.7275e-02, -3.2411e-02,  1.0115e-01,  1.8278e-02,  5.4167e-02,\n",
      "        -6.5277e-02, -4.9623e-02,  3.8521e-02, -7.7113e-02,  2.3679e-02,\n",
      "        -2.1421e-02, -3.1328e-02, -3.8675e-02,  3.2301e-02,  8.0595e-02,\n",
      "        -1.7031e-01, -3.1086e-02,  7.4435e-02,  4.4086e-02, -5.2982e-02,\n",
      "        -3.7618e-02, -2.1757e-02,  2.6901e-02,  1.2416e-02, -6.1151e-02,\n",
      "        -5.6076e-03, -8.1322e-02,  1.2883e-01,  2.1244e-01,  6.3023e-03,\n",
      "        -6.4486e-02, -6.8903e-02, -5.2496e-02, -8.8419e-03, -4.8330e-03,\n",
      "        -9.8466e-02, -9.1724e-02, -4.6670e-03,  2.2265e-02,  7.4199e-03,\n",
      "        -6.7150e-03,  3.1992e-03, -1.9731e-02,  1.7806e-02, -7.7666e-03,\n",
      "         6.6665e-03, -4.9659e-03,  1.3266e-02, -3.1188e-02,  6.3222e-02,\n",
      "         2.4398e-02,  2.9437e-02, -7.7957e-04,  2.4054e-02,  1.6580e-01,\n",
      "        -7.9211e-02, -2.8934e-02,  3.4830e-02,  3.1386e-02,  1.2014e-03,\n",
      "         9.4098e-02, -5.5012e-03,  4.5756e-02,  3.2991e-02,  1.8693e-02,\n",
      "         4.4928e-02, -2.1605e-02,  4.0092e-02, -6.9511e-02, -8.6237e-02,\n",
      "        -1.2794e-01,  2.8559e-02, -3.4364e-02,  2.3834e-03,  9.2352e-03,\n",
      "        -2.0991e-03,  1.2794e-02,  1.0197e-02, -1.4751e-02, -5.3813e-03,\n",
      "         2.9286e-02,  8.8126e-02,  1.5448e-02, -1.4078e-02, -3.9143e-02,\n",
      "        -5.8560e-02,  5.4407e-02,  3.5490e-02, -7.9659e-02,  3.4453e-02,\n",
      "         2.5864e-02, -3.0899e-02, -1.5625e-02, -4.5447e-02,  4.7464e-02,\n",
      "        -3.1091e-02, -3.4445e-02, -5.4052e-02, -6.4918e-02,  4.4487e-02,\n",
      "         2.5045e-02, -1.1488e-02,  2.5262e-02, -1.2607e-02,  1.3235e-02,\n",
      "         2.8561e-02,  6.9778e-02,  3.5717e-02, -1.3796e-02, -1.6055e-01,\n",
      "        -6.3508e-02,  3.0388e-02,  3.6702e-02,  1.4510e-02,  8.2649e-02,\n",
      "        -9.6217e-03,  2.8959e-02,  3.8684e-02, -8.4300e-02, -1.5368e-01,\n",
      "         9.8709e-02, -7.2473e-02,  3.1997e-02,  1.1817e-01, -2.6140e-02,\n",
      "        -6.1742e-02, -1.6166e-02,  7.0216e-02, -1.2530e-01, -3.3601e-02,\n",
      "         1.8504e-02,  4.9253e-02,  1.5496e-01, -7.7431e-02, -1.4273e-02,\n",
      "        -1.3381e-02,  1.0467e-01, -7.3973e-02, -9.8395e-02, -2.9553e-02,\n",
      "         4.8231e-02,  6.4982e-02, -5.0469e-02,  3.5893e-02,  9.4489e-02,\n",
      "         6.2196e-02, -9.2381e-02, -8.7598e-02,  7.9401e-02, -6.6444e-02,\n",
      "        -1.0009e-02, -3.8275e-02, -2.5270e-02, -1.7952e-01, -9.5267e-03,\n",
      "        -1.3783e-01,  2.1312e-01, -1.1740e-02, -8.2986e-02,  3.5087e-02,\n",
      "        -1.9155e-02, -2.4328e-02, -4.0487e-02,  3.3686e-02, -1.7021e-02,\n",
      "        -5.0354e-02, -1.5596e-01, -1.7125e-03,  5.6674e-02,  6.6230e-03,\n",
      "         6.4058e-03, -3.7337e-03,  1.1259e-02, -2.4012e-02,  8.4532e-02,\n",
      "        -2.1994e-02,  3.6341e-03,  8.1102e-02, -5.8442e-02,  9.7022e-02,\n",
      "        -6.0901e-02,  5.0808e-02,  1.3352e-01,  1.6406e-02,  1.3148e-02,\n",
      "         2.8686e-02, -3.0704e-02, -4.3113e-02,  5.2098e-02, -5.5051e-02,\n",
      "        -1.1791e-01,  5.0002e-02,  2.3706e-03, -6.4074e-02,  5.0139e-02,\n",
      "        -3.7592e-02,  5.3099e-02,  3.9144e-02,  4.3691e-03,  1.4775e-02,\n",
      "        -7.3321e-02, -4.6698e-02,  1.2764e-01, -6.2895e-02, -2.6595e-02,\n",
      "         7.9530e-02,  3.6950e-02, -4.7796e-03,  3.2136e-02, -4.4875e-02,\n",
      "        -3.2131e-02,  8.3086e-02,  8.9513e-02, -6.2051e-03, -1.2118e-01,\n",
      "         2.6485e-02, -3.3139e-02,  4.4756e-02,  7.8008e-04,  7.1055e-02,\n",
      "         3.0050e-02,  8.2575e-03, -2.6538e-02, -3.9907e-02, -2.5800e-02,\n",
      "        -3.3800e-02,  1.8517e-02, -7.0688e-02, -1.3011e-01, -3.3101e-02,\n",
      "        -5.4424e-02,  3.0215e-02, -6.2839e-02,  2.4651e-02, -1.8812e-03,\n",
      "        -1.3442e-01,  1.2847e-02,  7.9453e-02,  8.0802e-02, -9.5993e-02,\n",
      "         3.4160e-02,  2.6102e-02, -8.6553e-03,  5.7268e-02,  8.5350e-02,\n",
      "         1.3918e-02,  1.1504e-02,  2.9779e-03,  1.0623e-02,  5.5536e-02,\n",
      "        -4.1146e-02, -9.3039e-02, -3.3455e-03,  1.5882e-02, -1.5050e-01,\n",
      "         7.5856e-03,  2.2823e-02, -3.8871e-02,  5.5844e-02,  5.4641e-03,\n",
      "        -2.4733e-02, -5.1179e-02, -1.8616e-02,  5.5658e-02, -6.9583e-02,\n",
      "        -6.0925e-02, -8.0161e-02, -1.0143e-01,  4.3837e-02,  1.3554e-01,\n",
      "         8.7156e-02,  2.5922e-02, -7.2726e-02, -1.8920e-02,  9.7482e-02,\n",
      "         2.0591e-02, -6.2224e-02,  5.4904e-02, -1.3960e-01, -7.6254e-02,\n",
      "         8.3799e-02, -3.9226e-02, -4.3723e-02, -3.3469e-02,  9.1810e-03,\n",
      "         4.9622e-02,  6.3080e-02, -2.8480e-02, -1.8700e-02,  6.6885e-02,\n",
      "        -6.8625e-03,  7.1043e-02,  7.1088e-02, -9.2783e-02,  9.1262e-02,\n",
      "         4.6247e-02, -2.9005e-02,  2.8690e-02,  1.9394e-02,  5.7164e-05,\n",
      "         2.2624e-02,  3.3163e-02,  1.7700e-02,  3.4232e-02, -2.9858e-02,\n",
      "        -7.4267e-02,  3.6014e-02, -4.4552e-02,  3.5258e-02, -1.0101e-01,\n",
      "        -6.7129e-03,  1.4119e-02,  2.7532e-02,  1.8333e-02,  1.0998e-01,\n",
      "        -4.3879e-04,  6.3078e-02,  1.9749e-02,  4.5188e-02,  1.7698e-02,\n",
      "        -1.6677e-02,  8.2497e-02, -7.5923e-02,  6.3407e-02,  6.3229e-02,\n",
      "         1.7209e-02,  8.9937e-02, -3.1758e-02,  2.4061e-02, -7.6937e-02,\n",
      "         2.9163e-03, -6.6448e-02, -1.3663e-02, -3.8498e-02, -6.1970e-02,\n",
      "        -5.3004e-02,  2.5560e-02,  1.7372e-01,  1.9347e-02,  7.7611e-02,\n",
      "         1.2019e-01, -1.5177e-01, -1.0369e-02, -3.0696e-02,  6.5096e-02,\n",
      "         1.3015e-02,  5.4550e-02, -5.5283e-02,  7.5891e-03, -2.0863e-02,\n",
      "        -2.2272e-02,  1.8210e-02, -6.6587e-03, -1.3865e-02,  5.7003e-02,\n",
      "        -1.9093e-02,  9.1872e-03,  9.9067e-02,  3.3590e-04,  4.0905e-02,\n",
      "         2.1044e-03, -6.7002e-03,  2.9374e-02, -1.1736e-02,  3.6019e-03,\n",
      "        -2.4367e-02, -3.7626e-02, -1.1231e-01,  1.7375e-02, -6.0035e-03,\n",
      "         5.7686e-02, -2.7193e-02,  1.9783e-02, -6.3263e-02,  2.2237e-02,\n",
      "         7.3779e-03, -2.8759e-03, -1.5603e-02,  5.7662e-02,  8.7457e-03,\n",
      "         1.0018e-02, -5.0072e-02, -3.7638e-02,  2.4585e-02, -9.2793e-02,\n",
      "        -1.1872e-01, -2.2116e-02, -1.0956e-01, -1.0836e-01,  8.6403e-02,\n",
      "         4.3467e-02,  2.0700e-02,  5.1945e-02,  3.0060e-02,  2.7744e-02,\n",
      "        -9.2273e-03,  7.5827e-02, -4.5446e-02,  8.2869e-02, -9.2931e-02,\n",
      "         1.2670e-02, -4.8179e-02, -1.5450e-01, -1.6038e-03, -2.9253e-02,\n",
      "        -2.7980e-02, -1.0475e-02,  2.7516e-02,  1.6998e-01,  2.4017e-02,\n",
      "         8.4535e-02, -2.9163e-04,  3.1187e-02,  5.4309e-02, -3.0479e-02,\n",
      "        -8.0611e-02, -6.6498e-02, -1.4551e-01,  2.1430e-03, -3.7552e-02,\n",
      "         7.6690e-02,  7.4113e-02, -1.0557e-01, -7.4909e-02,  6.7211e-02,\n",
      "        -7.8306e-02,  4.8829e-02, -4.6191e-02,  1.3408e-02,  2.7609e-02,\n",
      "        -7.1487e-03, -3.2693e-03,  6.9174e-02,  1.8630e-01,  8.2350e-02,\n",
      "        -7.1999e-02, -5.7636e-04, -1.0190e-01, -2.1849e-02, -2.4579e-02,\n",
      "         8.7751e-02, -3.5942e-02, -2.4704e-03, -1.1202e-01, -7.7516e-02,\n",
      "        -3.0877e-02,  5.2970e-02, -1.0476e-02,  9.5842e-03, -7.3720e-02,\n",
      "         4.0108e-02, -1.2442e-02,  3.8677e-02, -4.2649e-02,  3.2528e-02,\n",
      "         4.7383e-02, -1.2851e-03, -3.1630e-02,  9.8758e-02, -4.5205e-02,\n",
      "         9.8402e-02, -9.2297e-02, -1.9997e-02, -1.7744e-02, -2.2326e-02,\n",
      "         8.0307e-02, -1.7815e-02,  1.9394e-02, -5.2028e-02, -5.1993e-02,\n",
      "         5.9033e-03,  1.0825e-02, -1.7139e-02, -1.4043e-01,  3.3729e-02,\n",
      "         2.4079e-02,  3.1476e-02, -7.7750e-02,  4.4037e-03, -7.0054e-02,\n",
      "        -1.0412e-02,  8.7667e-03,  6.4475e-02, -6.7967e-02,  4.3379e-02,\n",
      "        -8.0798e-02,  1.3300e-01, -2.5715e-02,  4.1997e-02,  1.5607e-02,\n",
      "        -1.8457e-02, -1.4307e-02, -1.3592e-02, -8.4850e-04,  6.9601e-03,\n",
      "         1.7143e-02, -7.7591e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 7.0232e-02, -3.9177e-02, -5.6618e-02,  2.4083e-02, -1.1208e-01,\n",
      "        -8.1959e-02, -1.0443e-02,  3.7170e-02, -5.0199e-02,  3.4944e-02,\n",
      "        -1.1758e-01,  2.1176e-02, -1.3711e-02,  1.7198e-03,  5.5355e-03,\n",
      "        -2.0753e-02, -4.9122e-02, -1.6634e-02,  8.6761e-04,  5.3412e-02,\n",
      "        -5.2480e-02,  3.0198e-03, -2.5701e-02, -1.2355e-01,  5.1328e-02,\n",
      "        -6.1343e-02, -4.0117e-02,  6.6909e-02,  5.3694e-03,  2.3511e-02,\n",
      "        -8.8978e-03, -1.3987e-02,  5.8384e-02, -7.4027e-02,  7.0214e-03,\n",
      "        -2.3619e-02, -1.1647e-02,  5.0248e-03, -1.0472e-01,  5.0924e-02,\n",
      "         9.6729e-03,  4.4740e-02, -8.4304e-03,  3.1834e-02, -5.6028e-02,\n",
      "         1.2579e-02,  4.1046e-02, -7.6398e-02, -8.2215e-02, -7.4826e-02,\n",
      "        -5.8703e-03,  2.1899e-02, -3.4924e-02, -9.3841e-02,  9.2489e-02,\n",
      "        -3.9724e-02,  6.8778e-02,  6.2910e-02,  1.5035e-01, -8.5688e-02,\n",
      "         5.1889e-02, -9.3605e-02, -7.0402e-02,  4.7219e-02,  5.9798e-02,\n",
      "        -3.6312e-03, -1.3177e-02, -4.6579e-02,  2.6072e-02, -1.8031e-02,\n",
      "        -1.5455e-01,  1.6608e-01, -1.5167e-03, -2.2081e-02, -3.3239e-02,\n",
      "         7.1615e-03,  5.0772e-02,  6.4464e-03, -1.0717e-03,  1.1329e-01,\n",
      "        -4.0795e-03,  7.9883e-02, -4.3044e-02,  1.3580e-01, -1.0705e-02,\n",
      "         7.0666e-03, -7.1443e-03,  9.1426e-02, -1.3554e-03, -9.4658e-02,\n",
      "        -4.0040e-02,  5.9643e-02,  1.8720e-02, -6.1085e-03, -5.1143e-03,\n",
      "         5.2426e-03,  3.9795e-02,  5.7733e-02,  9.3336e-02,  4.6847e-03,\n",
      "         5.8702e-02,  2.5341e-02,  4.2893e-02,  7.5947e-02,  2.8520e-04,\n",
      "         7.1536e-03, -4.3884e-03, -4.4555e-02, -4.4503e-02,  5.6192e-02,\n",
      "        -5.1656e-02, -1.2393e-01,  4.3672e-02,  4.7996e-02, -1.0800e-02,\n",
      "         5.6755e-02, -8.3568e-02, -1.3538e-02, -5.6153e-02, -5.5316e-02,\n",
      "        -2.3975e-02, -1.1282e-01, -2.8566e-02,  7.2766e-02, -3.8624e-02,\n",
      "         7.6615e-02,  3.6164e-02,  1.0354e-01,  4.9160e-02,  1.9378e-02,\n",
      "        -2.2329e-02, -1.2350e-01,  1.2831e-01,  9.7161e-03,  8.3806e-02,\n",
      "        -5.0945e-02, -2.3909e-02, -2.4867e-02,  5.3618e-02,  4.3033e-02,\n",
      "        -8.6281e-03, -3.7764e-02, -1.2432e-01,  1.3901e-02, -8.2746e-02,\n",
      "         1.5292e-02, -1.0102e-01, -2.1163e-03,  2.4047e-02, -3.3842e-02,\n",
      "         1.7279e-01, -2.0493e-03, -1.4493e-02,  5.7667e-02, -2.8942e-02,\n",
      "        -3.2882e-03,  7.1961e-02,  1.5763e-02, -1.0857e-01,  3.1682e-02,\n",
      "        -1.5458e-02,  2.3903e-02, -7.8493e-02,  3.3385e-02, -1.1762e-02,\n",
      "         5.4726e-02, -1.0496e-01, -1.9116e-02,  4.4039e-02, -4.5159e-02,\n",
      "         1.1691e-01, -7.5459e-02, -3.4751e-02, -7.0932e-05, -5.4284e-03,\n",
      "        -3.1645e-02,  7.8052e-02, -1.3927e-02, -3.9138e-02, -6.9432e-02,\n",
      "        -5.6814e-02,  4.7092e-02, -9.7913e-02, -7.1706e-02, -7.4354e-02,\n",
      "         2.9061e-02,  1.2788e-01,  3.2878e-02,  6.8620e-02,  7.8050e-03,\n",
      "        -8.1034e-03,  1.2591e-01, -2.5306e-02,  2.3245e-02,  6.0525e-03,\n",
      "         5.1102e-02,  2.6583e-02, -2.1282e-03, -5.5411e-02,  4.6495e-02,\n",
      "        -2.4725e-02,  2.2852e-02, -1.2736e-02,  1.6637e-01, -5.4719e-02,\n",
      "         8.6107e-02, -5.4407e-02,  6.8237e-02, -6.1891e-02, -5.5849e-02,\n",
      "         7.3760e-03, -3.0345e-02, -3.1600e-02,  3.3583e-02,  2.8570e-02,\n",
      "         8.2200e-02,  2.6655e-02,  2.6249e-02, -1.2001e-02,  7.8356e-02,\n",
      "        -1.6183e-02, -1.4890e-02,  1.2511e-02,  3.7454e-02,  2.5717e-02,\n",
      "         2.4392e-03,  1.9375e-02,  6.4533e-02,  3.3817e-02, -6.6789e-02,\n",
      "        -8.1340e-02, -3.5166e-02, -2.8866e-02, -7.5490e-02,  3.9034e-02,\n",
      "        -4.9257e-02,  1.5981e-02,  1.2176e-02,  2.2973e-02,  1.0207e-02,\n",
      "        -1.0285e-03,  1.7862e-01,  6.4228e-02, -3.5339e-02,  8.1926e-02,\n",
      "         7.1711e-02, -1.0528e-02,  3.6034e-02, -2.3140e-02,  4.6343e-02,\n",
      "        -3.3368e-02, -4.6355e-02,  5.1168e-02,  1.8313e-02, -3.8195e-03,\n",
      "         1.0237e-01, -4.0303e-02,  3.3172e-02, -4.5773e-02, -1.4106e-02,\n",
      "        -3.1364e-02,  5.1665e-02, -3.1724e-02,  3.0433e-02, -3.9412e-02,\n",
      "        -3.3040e-02,  2.1146e-02, -1.1771e-01, -6.6739e-02, -3.3981e-02,\n",
      "        -4.4390e-03,  2.8506e-02,  1.9362e-02,  1.0839e-01, -1.5109e-02,\n",
      "         1.5135e-01, -2.9912e-02,  7.0132e-02, -6.2905e-02, -9.2045e-02,\n",
      "        -1.0811e-01, -6.4596e-02,  1.1569e-01,  4.2324e-02, -5.3588e-02,\n",
      "         2.2440e-02, -2.2649e-02, -7.6581e-02, -6.1811e-02,  6.7117e-02,\n",
      "        -8.8734e-02,  1.1926e-02, -1.0264e-02,  2.1893e-02,  6.1756e-02,\n",
      "         1.1959e-01, -9.6380e-02,  1.3470e-02, -7.0965e-02,  2.2478e-02,\n",
      "         5.0166e-02, -4.6788e-03,  9.3105e-02,  1.2183e-01, -1.0024e-01,\n",
      "         1.9777e-04,  8.1114e-02, -2.6921e-02,  1.0334e-01, -3.6504e-02,\n",
      "         1.0802e-02, -2.5081e-02, -6.5181e-02,  8.6339e-02,  3.7305e-02,\n",
      "        -1.2546e-01, -2.3171e-02, -5.1505e-02,  8.1840e-02,  4.9002e-02,\n",
      "         1.8363e-02,  2.9693e-02, -3.8902e-03, -4.1257e-02, -2.2935e-02,\n",
      "         8.3203e-02,  5.9329e-02,  7.7033e-03,  4.9673e-02, -3.4751e-02,\n",
      "        -3.4831e-03, -7.7208e-03,  1.0457e-01, -2.1170e-02,  6.3125e-02,\n",
      "        -1.2047e-02,  1.4499e-02, -5.0847e-02,  2.7684e-02,  8.1270e-02,\n",
      "        -2.4067e-02,  1.6061e-04,  4.5172e-02,  8.9830e-02,  5.0638e-03,\n",
      "        -2.7056e-02,  1.6215e-02, -1.2409e-01,  2.7129e-02, -3.0758e-02,\n",
      "        -4.1683e-02,  7.6068e-03,  1.4988e-02, -2.3955e-02, -8.0970e-02,\n",
      "        -1.0192e-01,  3.6965e-02, -2.6476e-02,  5.7144e-03,  7.6527e-02,\n",
      "         9.9065e-02,  4.3809e-02,  5.6087e-02, -6.8878e-02,  6.4834e-02,\n",
      "         2.2787e-02,  7.3976e-02,  9.9496e-03, -1.7695e-02,  8.8900e-02,\n",
      "        -9.6980e-02, -7.0818e-02,  4.9335e-02, -5.5873e-02, -6.7333e-03,\n",
      "        -1.0160e-02,  2.7102e-02, -2.2473e-02, -4.0724e-02, -4.3555e-02,\n",
      "        -4.3084e-02, -8.1544e-02, -2.8473e-02,  1.8932e-02,  2.8450e-02,\n",
      "        -8.1699e-02, -8.1030e-02,  4.9583e-02,  3.5871e-02, -1.5891e-02,\n",
      "        -8.8298e-03, -1.2130e-02, -8.1447e-02,  4.5123e-02,  6.6769e-02,\n",
      "         4.5007e-02, -1.0901e-02, -1.5257e-01,  1.2816e-02,  6.9188e-02,\n",
      "        -1.3537e-02,  1.0406e-01,  2.4015e-02,  4.3749e-02, -2.3074e-02,\n",
      "         6.2925e-02, -3.2508e-02, -5.4690e-02,  1.6847e-02, -5.7822e-02,\n",
      "         6.0195e-02, -1.6088e-02, -3.6611e-02,  4.3164e-03, -1.6129e-02,\n",
      "        -2.0027e-02,  2.7187e-02, -5.7546e-02,  1.6556e-02, -4.5320e-04,\n",
      "         2.4637e-02, -5.7647e-02, -4.5837e-02,  1.4810e-02,  1.4818e-02,\n",
      "        -9.2751e-03,  3.1316e-02,  4.6298e-02,  1.5679e-02,  2.5335e-02,\n",
      "         1.5162e-02, -6.5274e-02, -1.1448e-01,  3.5900e-02, -1.1034e-01,\n",
      "        -9.4011e-02,  3.3696e-02, -6.7059e-03,  1.4441e-02,  1.3973e-01,\n",
      "         7.2340e-02, -5.2067e-02, -1.5580e-02,  4.3312e-02, -6.7398e-02,\n",
      "         7.6808e-02, -4.1142e-02,  3.2319e-02,  1.2461e-01,  1.5610e-02,\n",
      "         7.3369e-02, -1.0851e-01, -4.5686e-02, -6.5544e-02,  7.0161e-02,\n",
      "        -4.9590e-03,  4.6399e-02,  4.5816e-02, -7.6833e-02,  5.7388e-02,\n",
      "         5.6216e-02,  1.7794e-02, -1.8920e-02, -4.4150e-02,  2.6347e-02,\n",
      "         8.7239e-02, -2.0536e-02, -1.2006e-02, -5.0354e-03,  3.5649e-02,\n",
      "        -8.1056e-02,  5.1311e-02,  1.9925e-02, -4.3425e-02,  2.6601e-02,\n",
      "        -7.5502e-02, -3.4638e-02, -7.5277e-02, -5.1211e-02, -4.9907e-02,\n",
      "         1.9271e-02,  2.3710e-02,  1.7192e-02, -7.7708e-02,  2.5729e-02,\n",
      "         5.5325e-02,  1.0182e-01, -9.2568e-02, -4.8824e-02,  2.3749e-02,\n",
      "         3.6623e-02, -1.6246e-02, -2.5600e-02, -8.7405e-02,  1.7550e-02,\n",
      "        -6.1699e-03, -4.0138e-02, -3.5954e-02, -6.4890e-02,  4.1684e-03,\n",
      "        -8.0014e-02, -7.6652e-02,  9.0478e-02,  2.2696e-03,  4.3178e-03,\n",
      "         1.3625e-01, -4.3848e-02,  3.4243e-02,  1.0695e-01,  2.2553e-02,\n",
      "        -1.3336e-02, -3.6943e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-6.3037e-02,  4.0266e-02, -4.1982e-02, -1.3677e-01, -2.8125e-02,\n",
      "        -3.5907e-02, -1.2441e-01,  3.9413e-02,  7.0257e-02,  2.5938e-02,\n",
      "         1.6168e-02, -4.9772e-03, -8.1783e-02,  5.8682e-02, -5.5678e-03,\n",
      "        -1.6609e-02, -7.0250e-02,  2.3007e-02, -1.1237e-01, -1.1362e-03,\n",
      "         2.6813e-02,  5.1206e-02, -1.0730e-01, -7.5571e-03,  6.9360e-02,\n",
      "        -3.0615e-02, -1.3997e-01, -1.6990e-02, -6.2863e-03,  3.9499e-02,\n",
      "         1.1053e-01,  7.3368e-02, -4.2755e-03, -6.1965e-02,  2.3791e-02,\n",
      "        -5.2535e-02, -2.8180e-02, -1.5272e-02, -2.4229e-02,  8.8722e-02,\n",
      "        -9.6024e-02,  4.9923e-02, -1.7209e-02,  1.6388e-02, -7.5840e-03,\n",
      "        -8.1901e-02,  6.1073e-02, -4.8348e-02, -1.0459e-02, -6.6470e-02,\n",
      "        -2.4781e-02,  5.3203e-02,  2.4020e-02,  4.9423e-02,  3.7947e-02,\n",
      "         2.0673e-01,  6.0895e-02, -4.0758e-02,  2.0071e-02,  1.0387e-01,\n",
      "         1.6163e-02, -1.5090e-02,  6.2854e-02,  2.6011e-02,  3.8561e-02,\n",
      "        -5.5162e-02, -7.5800e-02,  6.2836e-02,  1.3994e-02,  1.1939e-01,\n",
      "        -1.5908e-02,  4.2999e-02, -2.3383e-02,  1.5983e-02, -1.3671e-02,\n",
      "         9.9919e-02,  6.4056e-02, -7.5082e-02, -1.2190e-02, -1.4694e-02,\n",
      "         5.8069e-02,  5.5209e-02,  1.2079e-02,  5.1134e-02,  4.5578e-02,\n",
      "         5.2929e-02, -6.7412e-03,  9.7755e-02, -4.7786e-02, -1.6850e-02,\n",
      "        -5.9766e-02,  9.2122e-02, -2.9754e-02, -1.1698e-01,  2.3706e-02,\n",
      "        -2.3814e-02,  3.3031e-02, -1.1580e-01,  3.8596e-02, -3.3136e-03,\n",
      "        -1.2250e-02,  3.4611e-02, -8.4193e-02, -7.5750e-02, -3.5521e-02,\n",
      "        -5.1473e-03,  3.9007e-02,  6.4325e-03, -5.9280e-02, -1.3100e-02,\n",
      "        -4.1139e-02,  4.7848e-02,  8.4264e-03, -1.0753e-01, -4.3760e-02,\n",
      "         9.4994e-02, -1.7219e-02,  3.9596e-02, -4.1659e-02,  1.2531e-01,\n",
      "        -4.9070e-02,  1.2569e-02, -3.4510e-02,  5.6004e-02, -2.7773e-02,\n",
      "        -8.0413e-02,  7.7013e-02, -3.7365e-02, -7.8601e-02, -4.4590e-02,\n",
      "         1.6158e-02,  2.7064e-02,  1.0510e-01, -1.2408e-02,  1.6963e-02,\n",
      "        -9.1978e-03,  5.7486e-02, -4.6821e-02, -3.0573e-03, -1.0964e-02,\n",
      "        -8.9452e-02,  4.2682e-02, -1.1941e-02,  2.5132e-02, -3.7705e-02,\n",
      "         5.4186e-02, -7.1975e-02, -4.9173e-02, -6.7192e-02,  2.7494e-02,\n",
      "         2.4167e-03,  3.7371e-02,  4.2284e-02,  3.3118e-02,  5.1909e-02,\n",
      "        -6.6921e-02, -5.8869e-02, -6.1932e-02,  3.1455e-02, -2.2885e-02,\n",
      "        -9.3647e-02, -1.9637e-02,  5.1098e-02,  4.5610e-02, -4.1068e-02,\n",
      "         5.7816e-02, -8.5963e-04,  2.2186e-02, -1.8173e-02,  4.3025e-02,\n",
      "        -3.6500e-02,  4.6611e-02,  1.1417e-01, -6.0109e-02, -6.6532e-02,\n",
      "         9.2543e-02,  1.5739e-02, -7.0260e-03, -4.5298e-02, -4.6085e-02,\n",
      "        -1.7641e-02, -3.4245e-02, -2.9982e-02, -3.3564e-02, -2.3251e-02,\n",
      "        -9.0132e-02, -4.9113e-02, -1.5003e-02, -3.4544e-02, -1.2240e-02,\n",
      "        -6.6013e-02, -1.2225e-01,  2.1974e-02, -7.2869e-02,  7.3213e-02,\n",
      "         7.8171e-02, -1.1407e-02,  1.2900e-02,  1.3423e-02,  6.1885e-02,\n",
      "         8.2777e-02,  5.9639e-03, -2.9608e-02,  1.4335e-02, -3.0911e-02,\n",
      "        -2.6568e-02, -7.7970e-02,  5.7262e-02,  7.5148e-03, -8.3736e-02,\n",
      "         1.1164e-01, -3.6595e-02, -3.5647e-02, -2.2155e-02,  3.7071e-02,\n",
      "         5.2191e-03, -5.0187e-02, -1.0465e-02, -2.7389e-02,  2.4710e-02,\n",
      "         3.4442e-02, -3.3596e-02,  5.7857e-02,  4.2296e-02, -2.8121e-02,\n",
      "         3.7366e-02, -5.9914e-02,  1.6653e-02,  3.8050e-02,  5.3976e-02,\n",
      "         1.6561e-02,  5.0949e-02,  7.7352e-02,  8.3561e-02, -4.3670e-02,\n",
      "        -8.8957e-03,  2.0743e-03,  3.0768e-02, -3.4656e-02,  1.0132e-01,\n",
      "         2.0802e-02, -1.4734e-01, -1.1625e-02, -4.6762e-03,  1.0868e-01,\n",
      "         8.2071e-02, -1.4927e-02, -1.5449e-01, -7.1360e-02,  6.3504e-02,\n",
      "        -1.3678e-02, -3.2650e-02,  8.5200e-02, -4.5086e-02,  2.2611e-02,\n",
      "        -1.0392e-01, -6.0944e-02,  1.4738e-02,  3.9227e-02, -8.7592e-03,\n",
      "        -2.2234e-02, -5.5263e-03, -3.3027e-02,  3.9625e-03,  1.5417e-02,\n",
      "         1.2909e-02,  1.0592e-01, -5.5637e-02,  1.6255e-01, -8.2178e-02,\n",
      "         9.2043e-02,  1.9381e-03,  2.2714e-02,  3.5822e-02, -1.0901e-03,\n",
      "         1.2325e-02, -6.4859e-02, -2.5885e-02,  5.1314e-02, -4.6941e-04,\n",
      "        -2.8895e-03,  1.1293e-02, -1.7513e-02, -6.6949e-02,  6.9416e-02,\n",
      "         7.1142e-03, -1.4641e-03, -3.6779e-02,  1.1385e-01, -4.7641e-02,\n",
      "         1.4738e-02, -6.2718e-02,  8.7415e-02, -5.5629e-03,  2.7129e-02,\n",
      "         6.3722e-03,  3.4799e-02,  2.5760e-02, -7.6286e-02, -6.1321e-02,\n",
      "        -5.3081e-02, -1.3048e-03, -1.7442e-02, -1.6667e-01,  3.7299e-03,\n",
      "         1.3328e-02,  6.2362e-02,  1.6265e-02,  5.9280e-02, -9.6899e-02,\n",
      "        -9.9530e-03,  3.4732e-02,  5.4185e-03, -4.3835e-03,  3.4801e-02,\n",
      "         4.0341e-02, -1.1303e-02, -2.8805e-02,  2.6510e-02, -4.8988e-02,\n",
      "        -1.4906e-02, -8.7503e-02, -3.8591e-03,  3.9093e-02,  2.1345e-02,\n",
      "         4.3803e-02, -4.8825e-02, -3.8691e-02, -7.1864e-02, -5.9994e-02,\n",
      "         2.5898e-02, -4.4769e-02,  8.8324e-02, -7.2772e-02,  1.5155e-02,\n",
      "        -5.5817e-02,  5.3736e-02, -2.9101e-02,  1.5793e-03, -1.7930e-01,\n",
      "        -1.7445e-02, -6.8678e-02, -2.1378e-02, -4.4950e-02, -1.7106e-02,\n",
      "         1.5411e-01,  7.0336e-02,  3.1394e-02,  9.1400e-02, -6.3379e-02,\n",
      "         9.3097e-02,  6.2873e-02, -2.3895e-02,  4.8823e-02,  1.5050e-02,\n",
      "         1.5749e-01,  2.0483e-02,  2.5478e-02,  1.2565e-01,  6.4963e-02,\n",
      "        -3.3720e-02,  3.8453e-02, -6.7775e-02, -1.1753e-01,  6.8093e-02,\n",
      "         5.1249e-02, -1.5064e-01, -6.5369e-02,  4.8224e-02, -8.1458e-03,\n",
      "        -2.7762e-02, -2.5249e-02, -1.0149e-02, -1.9384e-02,  4.1005e-02,\n",
      "        -2.7609e-02, -9.2976e-02,  3.8276e-02,  7.2089e-02, -1.2936e-01,\n",
      "        -1.1778e-01, -6.5505e-02,  1.7166e-02,  1.5751e-02, -1.9162e-02,\n",
      "         5.5185e-03, -1.0558e-01, -2.3025e-02, -1.4394e-01,  1.1885e-01,\n",
      "         9.7875e-03, -9.7859e-02, -3.9622e-02, -4.5969e-02, -4.3369e-02,\n",
      "        -2.5617e-02, -5.2712e-02,  3.9468e-02,  1.0800e-01,  5.3185e-02,\n",
      "        -5.0451e-02,  5.3125e-02,  1.4214e-01,  1.0340e-01, -1.7702e-02,\n",
      "        -6.3901e-02,  3.0720e-02, -7.3908e-02,  9.5226e-02,  6.2002e-03,\n",
      "         5.0914e-02, -6.5561e-02,  5.4568e-02,  5.1027e-02, -4.2785e-02,\n",
      "        -7.9318e-02,  6.1157e-02,  6.2453e-02, -4.5603e-02, -2.7345e-02,\n",
      "        -5.6974e-02,  1.2981e-01,  1.0213e-01,  4.7302e-02, -2.4651e-02,\n",
      "        -3.3669e-02, -4.9926e-02,  7.3012e-02, -3.4709e-02,  1.2907e-01,\n",
      "         6.1702e-02,  3.1375e-02,  1.9113e-02, -9.1100e-02,  7.3931e-03,\n",
      "        -8.0293e-02, -3.6101e-02,  5.1210e-02, -2.9621e-02,  5.9973e-03,\n",
      "         9.6392e-02,  4.1492e-03,  2.3054e-02, -8.5028e-02,  1.3075e-03,\n",
      "        -8.0786e-02,  7.1889e-02, -3.7784e-02,  2.0823e-02, -5.1179e-02,\n",
      "         1.3547e-01,  4.0677e-02,  6.0206e-02, -4.4987e-03, -1.4705e-02,\n",
      "        -4.3924e-05,  2.2686e-02, -3.4385e-02, -3.4656e-02, -1.7687e-01,\n",
      "         4.2150e-02, -3.2622e-03, -4.4221e-02, -5.9327e-02, -1.2178e-01,\n",
      "        -9.8243e-02,  2.9285e-02,  1.0800e-01, -2.9136e-02, -1.2633e-02,\n",
      "        -8.9605e-02,  1.0191e-02,  2.9528e-02, -1.6184e-02, -2.1323e-02,\n",
      "         4.3191e-02, -5.9493e-02, -9.2964e-02, -2.2478e-02,  1.4769e-02,\n",
      "        -2.1768e-02,  4.5379e-02,  7.2459e-02,  6.8969e-02, -3.1864e-02,\n",
      "         9.2427e-03,  1.1675e-01, -1.2651e-02, -8.6167e-02, -7.0927e-02,\n",
      "        -3.2216e-02, -3.6091e-02,  1.1292e-02, -2.3667e-02,  1.0530e-01,\n",
      "        -2.7349e-02,  3.5006e-02, -8.4804e-02,  5.3443e-02, -3.9848e-02,\n",
      "        -1.8628e-02, -9.9607e-02, -1.0862e-01,  3.0266e-03,  6.8604e-02,\n",
      "        -2.6277e-02, -1.4869e-01, -4.6595e-02, -9.2243e-02,  5.9800e-02,\n",
      "         1.6846e-02, -5.4605e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.90 seconds, 0.48 minutes\n",
      "[module.layers.3] weight_fb parameter count: 5,120\n",
      "epoch-1   lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.25 seconds, 0.47 minutes\n",
      "epoch-2   lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.95 seconds, 0.48 minutes\n",
      "epoch-3   lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.30 seconds, 0.47 minutes\n",
      "epoch-4   lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.05 seconds, 0.47 minutes\n",
      "epoch-5   lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.58 seconds, 0.48 minutes\n",
      "epoch-6   lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.89 seconds, 0.48 minutes\n",
      "epoch-7   lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.55 seconds, 0.48 minutes\n",
      "epoch-8   lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.65 seconds, 0.48 minutes\n",
      "epoch-9   lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.87 seconds, 0.48 minutes\n",
      "epoch-10  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 29.13 seconds, 0.49 minutes\n",
      "epoch-11  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.81 seconds, 0.48 minutes\n",
      "epoch-12  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.62 seconds, 0.48 minutes\n",
      "epoch-13  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.81 seconds, 0.48 minutes\n",
      "epoch-14  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.61 seconds, 0.48 minutes\n",
      "epoch-15  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.53 seconds, 0.48 minutes\n",
      "epoch-16  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.76 seconds, 0.48 minutes\n",
      "epoch-17  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.27 seconds, 0.47 minutes\n",
      "epoch-18  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.98 seconds, 0.48 minutes\n",
      "epoch-19  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.25 seconds, 0.47 minutes\n",
      "epoch-20  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.00 seconds, 0.47 minutes\n",
      "epoch-21  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.51 seconds, 0.48 minutes\n",
      "epoch-22  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.18 seconds, 0.47 minutes\n",
      "epoch-23  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.80 seconds, 0.48 minutes\n",
      "epoch-24  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.62 seconds, 0.48 minutes\n",
      "epoch-25  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.96 seconds, 0.48 minutes\n",
      "epoch-26  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.72 seconds, 0.48 minutes\n",
      "epoch-27  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 29.04 seconds, 0.48 minutes\n",
      "epoch-28  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.56 seconds, 0.48 minutes\n",
      "epoch-29  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.85 seconds, 0.48 minutes\n",
      "epoch-30  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.42 seconds, 0.47 minutes\n",
      "epoch-31  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.25 seconds, 0.47 minutes\n",
      "epoch-32  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.73 seconds, 0.48 minutes\n",
      "epoch-33  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.13 seconds, 0.47 minutes\n",
      "epoch-34  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.54 seconds, 0.48 minutes\n",
      "epoch-35  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.81 seconds, 0.48 minutes\n",
      "epoch-36  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.10 seconds, 0.47 minutes\n",
      "epoch-37  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.56 seconds, 0.48 minutes\n",
      "epoch-38  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.87 seconds, 0.48 minutes\n",
      "epoch-39  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.54 seconds, 0.48 minutes\n",
      "epoch-40  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.62 seconds, 0.48 minutes\n",
      "epoch-41  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.73 seconds, 0.48 minutes\n",
      "epoch-42  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.06 seconds, 0.47 minutes\n",
      "epoch-43  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.07 seconds, 0.47 minutes\n",
      "epoch-44  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.42 seconds, 0.47 minutes\n",
      "epoch-45  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.50 seconds, 0.48 minutes\n",
      "epoch-46  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 27.94 seconds, 0.47 minutes\n",
      "epoch-47  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.03 seconds, 0.47 minutes\n",
      "epoch-48  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.42 seconds, 0.47 minutes\n",
      "epoch-49  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.15 seconds, 0.47 minutes\n",
      "epoch-50  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 29.14 seconds, 0.49 minutes\n",
      "epoch-51  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 27.84 seconds, 0.46 minutes\n",
      "epoch-52  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.47 seconds, 0.47 minutes\n",
      "epoch-53  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.70 seconds, 0.48 minutes\n",
      "epoch-54  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.31 seconds, 0.47 minutes\n",
      "epoch-55  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.03 seconds, 0.47 minutes\n",
      "epoch-56  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.34 seconds, 0.47 minutes\n",
      "epoch-57  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.73 seconds, 0.48 minutes\n",
      "epoch-58  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.45 seconds, 0.47 minutes\n",
      "epoch-59  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.57 seconds, 0.48 minutes\n",
      "epoch-60  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.36 seconds, 0.47 minutes\n",
      "epoch-61  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.09 seconds, 0.47 minutes\n",
      "epoch-62  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.54 seconds, 0.48 minutes\n",
      "epoch-63  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.60 seconds, 0.48 minutes\n",
      "epoch-64  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.30 seconds, 0.47 minutes\n",
      "epoch-65  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.51 seconds, 0.48 minutes\n",
      "epoch-66  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.65 seconds, 0.48 minutes\n",
      "epoch-67  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.05 seconds, 0.47 minutes\n",
      "epoch-68  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.43 seconds, 0.47 minutes\n",
      "epoch-69  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.60 seconds, 0.48 minutes\n",
      "epoch-70  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.07 seconds, 0.47 minutes\n",
      "epoch-71  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.68 seconds, 0.48 minutes\n",
      "epoch-72  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.56 seconds, 0.48 minutes\n",
      "epoch-73  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.02 seconds, 0.47 minutes\n",
      "epoch-74  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.31 seconds, 0.47 minutes\n",
      "epoch-75  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.35 seconds, 0.47 minutes\n",
      "epoch-76  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.15 seconds, 0.47 minutes\n",
      "epoch-77  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.59 seconds, 0.48 minutes\n",
      "epoch-78  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.51 seconds, 0.48 minutes\n",
      "epoch-79  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.75 seconds, 0.48 minutes\n",
      "epoch-80  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.28 seconds, 0.47 minutes\n",
      "epoch-81  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.35 seconds, 0.47 minutes\n",
      "epoch-82  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.07 seconds, 0.47 minutes\n",
      "epoch-83  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.23 seconds, 0.47 minutes\n",
      "epoch-84  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.39 seconds, 0.47 minutes\n",
      "epoch-85  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.24 seconds, 0.47 minutes\n",
      "epoch-86  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.46 seconds, 0.47 minutes\n",
      "epoch-87  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.97 seconds, 0.48 minutes\n",
      "epoch-88  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.08 seconds, 0.47 minutes\n",
      "epoch-89  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.26 seconds, 0.47 minutes\n",
      "epoch-90  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.75 seconds, 0.48 minutes\n",
      "epoch-91  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.54 seconds, 0.48 minutes\n",
      "epoch-92  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.48 seconds, 0.47 minutes\n",
      "epoch-93  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.07 seconds, 0.47 minutes\n",
      "epoch-94  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.97 seconds, 0.48 minutes\n",
      "epoch-95  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.46 seconds, 0.47 minutes\n",
      "epoch-96  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.36 seconds, 0.47 minutes\n",
      "epoch-97  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.88 seconds, 0.48 minutes\n",
      "epoch-98  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.02 seconds, 0.47 minutes\n",
      "epoch-99  lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.47 seconds, 0.47 minutes\n",
      "epoch-100 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.88 seconds, 0.48 minutes\n",
      "epoch-101 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.31 seconds, 0.47 minutes\n",
      "epoch-102 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.47 seconds, 0.47 minutes\n",
      "epoch-103 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 29.08 seconds, 0.48 minutes\n",
      "epoch-104 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.52 seconds, 0.48 minutes\n",
      "epoch-105 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.66 seconds, 0.48 minutes\n",
      "epoch-106 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.71 seconds, 0.48 minutes\n",
      "epoch-107 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.57 seconds, 0.48 minutes\n",
      "epoch-108 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.09 seconds, 0.47 minutes\n",
      "epoch-109 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.39 seconds, 0.47 minutes\n",
      "epoch-110 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 27.82 seconds, 0.46 minutes\n",
      "epoch-111 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.12 seconds, 0.47 minutes\n",
      "epoch-112 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.72 seconds, 0.48 minutes\n",
      "epoch-113 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.89 seconds, 0.48 minutes\n",
      "epoch-114 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.31 seconds, 0.47 minutes\n",
      "epoch-115 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.92 seconds, 0.48 minutes\n",
      "epoch-116 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.62 seconds, 0.48 minutes\n",
      "epoch-117 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.81 seconds, 0.48 minutes\n",
      "epoch-118 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.63 seconds, 0.48 minutes\n",
      "epoch-119 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.38 seconds, 0.47 minutes\n",
      "epoch-120 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.58 seconds, 0.48 minutes\n",
      "epoch-121 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.37 seconds, 0.47 minutes\n",
      "epoch-122 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.66 seconds, 0.48 minutes\n",
      "epoch-123 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.89 seconds, 0.48 minutes\n",
      "epoch-124 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.28 seconds, 0.47 minutes\n",
      "epoch-125 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 29.04 seconds, 0.48 minutes\n",
      "epoch-126 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.13 seconds, 0.47 minutes\n",
      "epoch-127 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.70 seconds, 0.48 minutes\n",
      "epoch-128 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.88 seconds, 0.48 minutes\n",
      "epoch-129 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.18 seconds, 0.47 minutes\n",
      "epoch-130 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.30 seconds, 0.47 minutes\n",
      "epoch-131 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.77 seconds, 0.48 minutes\n",
      "epoch-132 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.23 seconds, 0.47 minutes\n",
      "epoch-133 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.26 seconds, 0.47 minutes\n",
      "epoch-134 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.70 seconds, 0.48 minutes\n",
      "epoch-135 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.33 seconds, 0.47 minutes\n",
      "epoch-136 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.53 seconds, 0.48 minutes\n",
      "epoch-137 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.69 seconds, 0.48 minutes\n",
      "epoch-138 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.45 seconds, 0.47 minutes\n",
      "epoch-139 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 27.98 seconds, 0.47 minutes\n",
      "epoch-140 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.55 seconds, 0.48 minutes\n",
      "epoch-141 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.67 seconds, 0.48 minutes\n",
      "epoch-142 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.34 seconds, 0.47 minutes\n",
      "epoch-143 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.60 seconds, 0.48 minutes\n",
      "epoch-144 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.28 seconds, 0.47 minutes\n",
      "epoch-145 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 27.86 seconds, 0.46 minutes\n",
      "epoch-146 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.73 seconds, 0.48 minutes\n",
      "epoch-147 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 27.90 seconds, 0.47 minutes\n",
      "epoch-148 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.17 seconds, 0.47 minutes\n",
      "epoch-149 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.61 seconds, 0.48 minutes\n",
      "epoch-150 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.18 seconds, 0.47 minutes\n",
      "epoch-151 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.49 seconds, 0.47 minutes\n",
      "epoch-152 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.62 seconds, 0.48 minutes\n",
      "epoch-153 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.90 seconds, 0.48 minutes\n",
      "epoch-154 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.19 seconds, 0.47 minutes\n",
      "epoch-155 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.58 seconds, 0.48 minutes\n",
      "epoch-156 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.80 seconds, 0.48 minutes\n",
      "epoch-157 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.20 seconds, 0.47 minutes\n",
      "epoch-158 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.35 seconds, 0.47 minutes\n",
      "epoch-159 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.83 seconds, 0.48 minutes\n",
      "epoch-160 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.34 seconds, 0.47 minutes\n",
      "epoch-161 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.46 seconds, 0.47 minutes\n",
      "epoch-162 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.14 seconds, 0.47 minutes\n",
      "epoch-163 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.74 seconds, 0.48 minutes\n",
      "epoch-164 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.33 seconds, 0.47 minutes\n",
      "epoch-165 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.82 seconds, 0.48 minutes\n",
      "epoch-166 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.91 seconds, 0.48 minutes\n",
      "epoch-167 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.50 seconds, 0.47 minutes\n",
      "epoch-168 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.70 seconds, 0.48 minutes\n",
      "epoch-169 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.62 seconds, 0.48 minutes\n",
      "epoch-170 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.23 seconds, 0.47 minutes\n",
      "epoch-171 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.60 seconds, 0.48 minutes\n",
      "epoch-172 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.33 seconds, 0.47 minutes\n",
      "epoch-173 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.00 seconds, 0.47 minutes\n",
      "epoch-174 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.25 seconds, 0.47 minutes\n",
      "epoch-175 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.91 seconds, 0.48 minutes\n",
      "epoch-176 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.24 seconds, 0.47 minutes\n",
      "epoch-177 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.79 seconds, 0.48 minutes\n",
      "epoch-178 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.68 seconds, 0.48 minutes\n",
      "epoch-179 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.05 seconds, 0.47 minutes\n",
      "epoch-180 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.62 seconds, 0.48 minutes\n",
      "epoch-181 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.05 seconds, 0.47 minutes\n",
      "epoch-182 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 29.04 seconds, 0.48 minutes\n",
      "epoch-183 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.47 seconds, 0.47 minutes\n",
      "epoch-184 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.69 seconds, 0.48 minutes\n",
      "epoch-185 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 29.27 seconds, 0.49 minutes\n",
      "epoch-186 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.03 seconds, 0.47 minutes\n",
      "epoch-187 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.83 seconds, 0.48 minutes\n",
      "epoch-188 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.45 seconds, 0.47 minutes\n",
      "epoch-189 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 27.75 seconds, 0.46 minutes\n",
      "epoch-190 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.89 seconds, 0.48 minutes\n",
      "epoch-191 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.50 seconds, 0.47 minutes\n",
      "epoch-192 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 27.90 seconds, 0.46 minutes\n",
      "epoch-193 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.19 seconds, 0.47 minutes\n",
      "epoch-194 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.58 seconds, 0.48 minutes\n",
      "epoch-195 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.33 seconds, 0.47 minutes\n",
      "epoch-196 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.29 seconds, 0.47 minutes\n",
      "epoch-197 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.56 seconds, 0.48 minutes\n",
      "epoch-198 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.26 seconds, 0.47 minutes\n",
      "epoch-199 lr=['0.0005000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 28.18 seconds, 0.47 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8e8cd32e3a4af0b1bf0529f0dacd5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.09908</td></tr><tr><td>tr_epoch_loss</td><td>2.3026</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>2.30259</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fancy-sweep-275</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j86h1dlg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j86h1dlg</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251110_234316-j86h1dlg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7754jft0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 5000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_threshold: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: one\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251111_011902-7754jft0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7754jft0' target=\"_blank\">lilac-sweep-279</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7754jft0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7754jft0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_threshold' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': '20251111_011910_533', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.75, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.75, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0005, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'one', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'output_threshold': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 7a22c8a0ef5b9b252dbf98632e270efd\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=False, ANPI_MODE=True)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=False, ANPI_MODE=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient(single_step=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.0005\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "inFeed spike.shape torch.Size([10, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([ 1.8934e-02,  2.1093e-01, -1.6790e-02,  8.2860e-02, -1.8780e-01,\n",
      "         6.3725e-02, -6.3379e-02, -7.9782e-02,  5.1524e-02, -1.2446e-01,\n",
      "        -1.6221e-01, -2.9600e-02, -9.0339e-03,  1.9444e-02, -1.0934e-01,\n",
      "         1.8129e-01, -6.9730e-02,  6.7152e-02,  7.7763e-02, -3.2597e-03,\n",
      "         1.4558e-01, -5.0406e-02, -2.4797e-02,  1.4391e-01, -3.1818e-02,\n",
      "        -1.1320e-01,  2.2984e-01, -6.7576e-02,  1.7931e-02, -1.1550e-01,\n",
      "        -1.7594e-01, -1.5427e-01,  8.1846e-02,  1.3850e-01,  6.3135e-02,\n",
      "         4.1502e-02, -1.5509e-01,  6.0735e-02,  1.6491e-01, -6.4878e-02,\n",
      "         9.1983e-02,  7.6438e-03,  8.2616e-03, -1.3744e-02,  3.2357e-02,\n",
      "        -5.7478e-02, -1.0464e-01,  9.3097e-03, -3.2663e-02, -5.1313e-02,\n",
      "        -8.5647e-02,  3.8434e-02,  1.6001e-02, -1.9292e-02,  9.8606e-02,\n",
      "        -1.3158e-01, -3.4134e-02, -6.2874e-02,  4.3602e-02, -5.2417e-02,\n",
      "         1.2124e-01, -7.9496e-02,  2.4412e-02, -4.1696e-02,  1.0778e-01,\n",
      "        -1.0762e-01,  5.4097e-02, -1.2537e-01, -3.7238e-02,  5.0156e-02,\n",
      "         9.7776e-03,  2.5239e-02,  3.5296e-02,  2.2238e-01,  2.2782e-03,\n",
      "         1.5446e-01, -1.1312e-01,  9.2554e-02, -4.4633e-02,  7.4222e-02,\n",
      "        -5.6474e-02, -6.8803e-02, -7.0595e-02, -4.9484e-02, -4.2925e-02,\n",
      "        -4.0809e-02,  1.6994e-02,  4.3201e-02,  4.9468e-02, -1.1875e-01,\n",
      "        -2.6532e-02,  2.6988e-02, -1.4051e-01, -6.3074e-02,  7.3065e-03,\n",
      "         1.8921e-02,  5.8165e-02,  2.2661e-02,  1.1140e-01, -6.6528e-02,\n",
      "        -1.6133e-01,  5.8902e-04,  1.3482e-01,  1.2398e-01,  2.2678e-03,\n",
      "        -1.2688e-01, -7.3284e-02,  3.6657e-02, -5.3425e-02, -3.8686e-03,\n",
      "        -7.5912e-02, -2.4416e-01,  6.8315e-02, -9.1515e-03, -2.1105e-02,\n",
      "         4.3759e-02, -3.0760e-02,  2.1116e-03,  6.1028e-02,  2.4064e-02,\n",
      "         7.3052e-02, -1.1411e-02, -9.9703e-03, -4.8900e-02, -4.9272e-02,\n",
      "        -1.1781e-01, -2.3789e-02, -6.6208e-02,  1.9253e-02,  9.5465e-02,\n",
      "        -2.7977e-03,  1.6420e-01,  1.0646e-01, -9.6819e-02, -6.5508e-02,\n",
      "         1.6782e-01,  2.4013e-01, -6.0490e-02,  1.2407e-01, -2.7312e-02,\n",
      "         4.2546e-02,  4.1576e-02,  1.0389e-01, -1.9791e-01, -6.1734e-02,\n",
      "         2.0598e-01, -9.2463e-03,  2.3019e-02, -7.1248e-02, -1.6451e-01,\n",
      "         8.8945e-02,  7.6954e-02, -6.1358e-02,  2.1075e-01,  1.1362e-01,\n",
      "        -4.1540e-02,  2.3355e-02, -1.2469e-01, -1.1774e-02, -5.9197e-02,\n",
      "        -7.8824e-02,  2.0957e-04, -1.8973e-02,  7.3129e-02,  7.9223e-02,\n",
      "         8.0468e-02, -5.9513e-02,  1.2675e-01, -1.0473e-01, -2.2743e-03,\n",
      "        -3.5689e-02, -4.7485e-02,  1.4612e-02, -1.4731e-01,  3.0283e-02,\n",
      "        -4.3783e-02, -1.0702e-01, -1.2393e-01, -1.5395e-01, -1.5502e-01,\n",
      "        -6.4030e-02,  7.4169e-02, -5.9266e-02,  2.9501e-02, -1.3114e-01,\n",
      "        -3.1084e-02,  5.7895e-02,  5.5842e-02, -7.3755e-02, -1.2356e-02,\n",
      "        -4.2171e-02, -1.3607e-01,  3.7821e-02, -2.0038e-02,  5.8521e-02,\n",
      "        -9.7889e-02, -3.6203e-04, -7.8263e-02,  3.8445e-02,  2.4739e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 0.0243,  0.0875,  0.0762,  0.0893, -0.0358,  0.0084, -0.0823,  0.0415,\n",
      "         0.1093,  0.0390, -0.0084,  0.0824, -0.0388, -0.0084,  0.1905, -0.0412,\n",
      "         0.0535,  0.0146, -0.1927, -0.0034, -0.1234, -0.0248,  0.0023,  0.1960,\n",
      "         0.0956,  0.1355, -0.0962, -0.0471, -0.0019, -0.1697,  0.0164,  0.0255,\n",
      "        -0.0218, -0.0832,  0.0267,  0.0968,  0.1448,  0.0340,  0.0123, -0.0644,\n",
      "        -0.1539, -0.0648,  0.1434,  0.0292, -0.0543,  0.0045, -0.1050, -0.0141,\n",
      "        -0.0106, -0.0727, -0.0340,  0.0092,  0.0063, -0.0900,  0.0770, -0.1758,\n",
      "        -0.1888,  0.1074,  0.0024,  0.0566,  0.0525,  0.0500, -0.1397,  0.2322,\n",
      "        -0.1507, -0.0170, -0.0022,  0.0212,  0.0636,  0.1522,  0.0262,  0.0489,\n",
      "        -0.0238, -0.0843,  0.0417, -0.0206, -0.1503, -0.0105, -0.0025, -0.0157,\n",
      "         0.0537,  0.2482, -0.0157, -0.0847,  0.1378,  0.0673,  0.0827,  0.0278,\n",
      "         0.0017,  0.0711,  0.1171, -0.0113, -0.1379, -0.0627,  0.0863,  0.0732,\n",
      "         0.0663, -0.0518, -0.1748, -0.0279,  0.0079, -0.1269,  0.1308, -0.1293,\n",
      "         0.0255,  0.0589, -0.1456, -0.1754,  0.1100,  0.1369, -0.0427, -0.0364,\n",
      "        -0.1153, -0.2599, -0.0316,  0.0100,  0.1226, -0.1337,  0.0349,  0.1675,\n",
      "        -0.2253,  0.1285, -0.1121, -0.1268, -0.0064,  0.1009, -0.2251, -0.0125,\n",
      "         0.0833, -0.0059, -0.0931, -0.1747, -0.0026,  0.0089, -0.1154, -0.1073,\n",
      "         0.2383,  0.0302, -0.1482, -0.0759,  0.0708,  0.0868,  0.1001,  0.0796,\n",
      "         0.2262, -0.0222,  0.2962,  0.0536,  0.0022, -0.0399, -0.0256,  0.0852,\n",
      "        -0.1183, -0.1185, -0.0293, -0.0370,  0.0309, -0.0984,  0.1941,  0.0216,\n",
      "        -0.0090,  0.1404,  0.0598,  0.0650,  0.0744,  0.0426, -0.0094, -0.0445,\n",
      "        -0.1149,  0.0229,  0.1572,  0.1014,  0.1170, -0.1135,  0.0229,  0.1222,\n",
      "        -0.0146, -0.0612,  0.1128, -0.2361,  0.0667, -0.0671, -0.0090, -0.0696,\n",
      "        -0.1264,  0.1187, -0.0047, -0.0778,  0.0414, -0.0189, -0.0153, -0.0139,\n",
      "        -0.0278, -0.1414, -0.0518, -0.0080, -0.1608, -0.0265,  0.1849, -0.0543],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([-4.4951e-02, -3.0269e-02,  5.7666e-03,  5.0608e-02,  7.1861e-03,\n",
      "         1.3723e-01, -4.7515e-02,  2.8713e-02,  1.1686e-01, -2.1819e-02,\n",
      "         8.4215e-02,  3.5925e-02,  1.6644e-01, -8.9751e-02,  5.2478e-02,\n",
      "         1.1553e-01,  1.6151e-01, -2.2964e-02, -1.6174e-01,  1.9235e-01,\n",
      "         9.7453e-02,  7.7079e-02, -8.9056e-02,  8.0481e-02,  1.4806e-01,\n",
      "        -1.6083e-02,  1.0203e-01, -3.7285e-02,  6.1060e-02,  9.6561e-03,\n",
      "        -6.8472e-02,  1.0096e-02, -1.4065e-01,  2.3837e-01,  1.1363e-01,\n",
      "         2.4803e-02, -3.5905e-02,  8.2499e-02,  4.0047e-02,  4.7051e-02,\n",
      "        -1.8222e-01,  9.4498e-02, -9.1961e-02,  1.1252e-01,  7.8541e-02,\n",
      "         1.0352e-01,  5.1129e-02, -1.3695e-02,  1.3555e-01,  3.0085e-02,\n",
      "         4.2894e-02, -3.0123e-03, -1.0391e-01,  6.3762e-03,  2.7413e-02,\n",
      "         2.0599e-01, -1.3469e-01, -4.1689e-02,  8.9827e-02, -1.3860e-01,\n",
      "         5.3680e-02, -9.2493e-02,  2.8438e-02, -9.8101e-02, -2.9716e-02,\n",
      "         1.5025e-02, -8.1340e-03,  8.0789e-03,  1.5008e-01,  1.7955e-02,\n",
      "        -9.7189e-02,  4.2880e-02,  3.5098e-02, -7.5292e-02,  1.0018e-02,\n",
      "        -3.8158e-02,  2.1247e-02,  8.8000e-02,  5.3422e-02,  8.5364e-02,\n",
      "        -3.1726e-02, -4.0565e-02,  5.0597e-02, -5.4060e-02, -1.9612e-03,\n",
      "         2.9600e-01,  9.2132e-02,  2.9507e-02, -9.2795e-02, -1.0727e-01,\n",
      "        -9.4369e-04,  1.6943e-01, -1.1252e-01,  2.0963e-03, -4.7562e-02,\n",
      "        -8.9568e-02, -1.6470e-01, -1.3752e-02, -4.9301e-03,  1.9867e-02,\n",
      "         2.8623e-02, -1.4914e-01, -7.4637e-02,  4.3263e-02, -5.4997e-02,\n",
      "        -5.1978e-02, -9.9176e-02, -1.9955e-02,  5.1099e-02,  1.8961e-02,\n",
      "         3.6069e-02, -8.3924e-02,  1.0509e-01, -1.2345e-01, -6.2687e-02,\n",
      "        -6.5892e-02,  7.2257e-02,  7.6160e-02, -1.0679e-02,  1.1915e-01,\n",
      "        -1.1745e-01, -4.7364e-02,  8.2369e-03, -2.0607e-02, -8.7966e-03,\n",
      "        -1.3239e-01, -1.0953e-02, -3.8245e-02,  1.7113e-01, -9.6756e-02,\n",
      "        -2.3135e-01,  1.7700e-01, -9.4699e-02,  8.4271e-02,  1.7756e-01,\n",
      "        -7.7262e-03,  2.4065e-01,  1.2335e-01,  5.0242e-02,  1.1121e-02,\n",
      "        -1.3971e-01, -2.5510e-02,  1.1339e-02,  5.6864e-02, -2.9294e-02,\n",
      "        -1.0927e-01,  9.0566e-02, -1.4698e-01,  1.0142e-01, -2.0078e-01,\n",
      "        -2.5668e-02, -8.1559e-02, -2.6427e-02,  2.6780e-01,  4.4975e-02,\n",
      "         1.1964e-01,  6.6056e-03,  8.9370e-02,  7.3522e-02, -5.8117e-02,\n",
      "        -6.1687e-02, -3.5208e-02,  1.4783e-01, -1.6733e-02,  1.8550e-01,\n",
      "        -5.9637e-02,  1.0120e-01,  3.3498e-02, -1.4412e-02,  1.4279e-01,\n",
      "        -1.7611e-01,  2.3674e-02, -2.6663e-02,  2.8803e-02, -1.0240e-01,\n",
      "        -8.6560e-02, -1.3708e-02,  2.1908e-01,  1.7359e-01,  1.6947e-02,\n",
      "         1.3747e-01, -1.0779e-02, -5.6662e-02,  2.1867e-02,  9.4120e-02,\n",
      "        -1.4148e-04,  1.3205e-01, -7.6242e-03, -6.1247e-02, -1.5935e-01,\n",
      "         1.1932e-01, -1.7626e-01,  4.7519e-02, -6.7935e-02, -3.5344e-02,\n",
      "        -5.6960e-02, -1.6597e-01,  3.6102e-02,  7.2538e-02, -1.1702e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0897,  0.1346,  0.0082,  0.0404, -0.0141, -0.0915,  0.1112,  0.0613,\n",
      "         0.0437,  0.2269, -0.0635,  0.0330, -0.0388,  0.0970,  0.1340, -0.0103,\n",
      "        -0.0180, -0.2479, -0.1925, -0.0520, -0.0167,  0.2512,  0.2248,  0.0406,\n",
      "        -0.0638, -0.1051,  0.0551,  0.1565, -0.0160, -0.0229,  0.0975,  0.0753,\n",
      "        -0.0261,  0.0484,  0.0233,  0.1358, -0.0195,  0.1246, -0.1235, -0.0948,\n",
      "         0.1241,  0.0367, -0.0132,  0.0701, -0.0866,  0.0677,  0.0747, -0.0005,\n",
      "         0.0291, -0.0934,  0.0961,  0.0369,  0.1113, -0.0287,  0.0090,  0.2001,\n",
      "        -0.0262,  0.0943, -0.1185, -0.1080, -0.1874, -0.2298, -0.0099,  0.1366,\n",
      "        -0.0031, -0.2031,  0.0642, -0.0495, -0.0109,  0.0107, -0.0874, -0.0920,\n",
      "        -0.0740, -0.0029, -0.0110, -0.0867,  0.0235, -0.0947,  0.0334,  0.0025,\n",
      "        -0.1047, -0.1597,  0.0005, -0.1529,  0.0354,  0.1324, -0.1721, -0.0826,\n",
      "         0.1246,  0.1224,  0.1146,  0.0836,  0.0127,  0.1906,  0.0699, -0.0207,\n",
      "         0.0435, -0.0039, -0.1358, -0.0330, -0.1266,  0.1363,  0.0505, -0.1182,\n",
      "        -0.0723, -0.1437,  0.1342,  0.0754, -0.0922,  0.1479, -0.1418, -0.0035,\n",
      "         0.0841,  0.0382,  0.1203, -0.1159,  0.0937,  0.0410, -0.0786,  0.0332,\n",
      "         0.0585,  0.0918,  0.0264,  0.0213, -0.0575,  0.1411, -0.0949,  0.0688,\n",
      "         0.0401,  0.0027, -0.0604,  0.0814, -0.0098, -0.1172,  0.0241, -0.0613,\n",
      "        -0.0740, -0.0775,  0.0796, -0.1757,  0.0731,  0.0651,  0.0684, -0.1016,\n",
      "         0.2258,  0.0089,  0.0729, -0.1231,  0.0348,  0.0380, -0.1748, -0.0902,\n",
      "         0.2137,  0.0032, -0.1064, -0.0598,  0.1530,  0.0615,  0.0364,  0.0570,\n",
      "        -0.0708, -0.0792, -0.1045,  0.0952, -0.0262, -0.0666,  0.0778, -0.0828,\n",
      "         0.0837,  0.0487,  0.0133,  0.0663,  0.0131,  0.1229,  0.1182,  0.0589,\n",
      "        -0.0275, -0.0269, -0.0815, -0.1605,  0.0128, -0.0175,  0.0951, -0.1965,\n",
      "        -0.1307, -0.0327,  0.2386, -0.0120, -0.0878,  0.2075,  0.1736,  0.1385,\n",
      "        -0.0182, -0.1107, -0.2712,  0.1544,  0.2334, -0.0716, -0.0042, -0.0741],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-0.1114, -0.1260, -0.1119, -0.1797, -0.0123, -0.1354, -0.0627,  0.0113,\n",
      "        -0.2121, -0.0449, -0.1316,  0.0974, -0.0196, -0.2268, -0.0866,  0.0271,\n",
      "        -0.0689,  0.0083, -0.1552,  0.1024,  0.0478, -0.0129, -0.0338,  0.0313,\n",
      "        -0.0618,  0.0572, -0.0461,  0.0063,  0.1558, -0.0055, -0.2186,  0.1069,\n",
      "        -0.0180,  0.0722, -0.0741, -0.0951,  0.1404, -0.0053,  0.0869,  0.1728,\n",
      "         0.1237, -0.0906,  0.1067, -0.0791, -0.1660,  0.0487,  0.0635,  0.0555,\n",
      "         0.0562, -0.0208, -0.0247, -0.0210, -0.1192,  0.0735, -0.0960,  0.0183,\n",
      "         0.0923,  0.0311,  0.0228,  0.0739,  0.0312,  0.0293,  0.0242,  0.1432,\n",
      "        -0.1495,  0.0705,  0.1607,  0.1131,  0.0455, -0.1201,  0.1743,  0.0608,\n",
      "        -0.1190, -0.1751,  0.0174,  0.1122,  0.0615,  0.1286,  0.2249, -0.0399,\n",
      "         0.1110, -0.3303, -0.1719, -0.1252,  0.1496, -0.0990, -0.0732,  0.1068,\n",
      "        -0.0648, -0.0481, -0.1068, -0.0256,  0.1217, -0.1511,  0.0120, -0.0370,\n",
      "        -0.0630,  0.0352, -0.0881, -0.0354, -0.2016,  0.0181, -0.0575, -0.0375,\n",
      "         0.1403,  0.0006,  0.2255, -0.0530, -0.0560, -0.2372,  0.1225,  0.2346,\n",
      "         0.1047, -0.0298,  0.0164, -0.0071, -0.0364,  0.1334,  0.1921,  0.0409,\n",
      "         0.0370, -0.2028,  0.0428,  0.0574,  0.1313,  0.1827,  0.1051,  0.0931,\n",
      "         0.1609, -0.1042, -0.0180,  0.1062, -0.2921,  0.1134, -0.0590, -0.1621,\n",
      "         0.0792,  0.0747,  0.0440,  0.1033, -0.0561, -0.0242, -0.0610, -0.0865,\n",
      "        -0.0366,  0.0475, -0.0406, -0.0448,  0.0063,  0.1424,  0.0130,  0.0362,\n",
      "        -0.0880, -0.0607, -0.0947,  0.1126, -0.1080, -0.1318, -0.1815,  0.0529,\n",
      "        -0.0257, -0.0751, -0.2009, -0.0415, -0.0961, -0.0399,  0.0027, -0.0857,\n",
      "        -0.0904, -0.0699, -0.0287, -0.0942,  0.0285, -0.0968,  0.2108,  0.0588,\n",
      "        -0.0367, -0.0581,  0.0109,  0.0122,  0.0639, -0.0397, -0.1301,  0.1894,\n",
      "        -0.1012,  0.1052,  0.0252,  0.1616,  0.1519,  0.0793, -0.1125, -0.0098,\n",
      "        -0.2642,  0.0059, -0.1486,  0.1224, -0.1979, -0.0971, -0.0462,  0.0212],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([-2.3097e-02, -1.4788e-01,  1.0210e-01,  3.3557e-02,  2.4169e-01,\n",
      "        -1.3634e-01, -8.5383e-03, -3.8043e-02,  1.7316e-01, -4.1206e-02,\n",
      "         2.8185e-02,  1.1411e-01,  3.9203e-02,  8.1501e-02,  9.3352e-02,\n",
      "         2.0928e-01, -6.4334e-02,  3.8064e-02,  1.0619e-01,  9.6020e-02,\n",
      "         1.0335e-01,  2.7876e-01,  6.5856e-02,  2.0067e-01,  8.0354e-02,\n",
      "        -2.3247e-01,  1.2218e-01,  2.3015e-02,  1.7446e-01, -5.6854e-02,\n",
      "        -5.4324e-02,  3.6970e-03,  5.6947e-02, -3.3943e-02,  1.6095e-01,\n",
      "         1.9372e-03, -7.9038e-02,  9.8647e-02, -3.2229e-02,  5.0507e-02,\n",
      "         9.7127e-02, -1.5709e-01,  2.4332e-02,  6.0119e-02, -1.0542e-01,\n",
      "        -9.9471e-02, -9.7351e-02,  1.0731e-01,  1.3880e-01, -1.6828e-01,\n",
      "         8.0962e-02, -5.6699e-02, -7.4759e-03, -6.4011e-03, -1.6884e-01,\n",
      "        -1.3898e-01,  1.7222e-01, -8.1651e-02, -1.8829e-01, -5.1125e-02,\n",
      "         1.3743e-01,  1.8289e-01, -4.9558e-02,  3.3120e-02, -3.1865e-02,\n",
      "         8.0625e-02, -4.9182e-02, -2.8268e-02,  2.6301e-02, -9.2309e-03,\n",
      "        -4.7385e-02, -1.8954e-01,  6.7699e-02,  7.9177e-03, -1.2038e-01,\n",
      "         1.9006e-02,  1.7940e-02,  1.2187e-01, -5.8642e-02,  9.8349e-02,\n",
      "        -1.5748e-01,  2.1717e-02, -5.9567e-02, -1.2667e-02,  1.7577e-02,\n",
      "        -2.7029e-02, -1.3078e-01,  9.7926e-02,  2.2109e-02,  1.4673e-01,\n",
      "        -2.8729e-02,  1.4028e-01, -1.5075e-01,  8.0415e-02,  1.0154e-01,\n",
      "        -2.4165e-02, -2.6159e-02, -7.1517e-02, -4.9824e-02, -7.3641e-02,\n",
      "        -6.3665e-02,  1.4753e-01,  2.2992e-02,  2.5068e-02,  8.3415e-02,\n",
      "        -9.7786e-02,  1.5092e-01, -3.5185e-02, -1.6092e-02, -1.3254e-01,\n",
      "        -1.3460e-01,  2.2992e-02,  1.0348e-01,  1.3005e-01,  1.0918e-01,\n",
      "        -5.3370e-03,  3.2445e-02, -1.0142e-02, -9.9965e-03,  4.4736e-02,\n",
      "        -8.6016e-02,  7.8391e-02, -4.4661e-02,  1.1245e-01, -4.3102e-02,\n",
      "         1.1311e-01,  3.2954e-02, -2.3165e-02, -1.0094e-02, -5.7948e-02,\n",
      "         3.6766e-02, -2.9717e-02, -1.2952e-02, -1.1631e-01, -1.0180e-01,\n",
      "        -5.3192e-02,  3.6347e-02, -1.7114e-02,  5.3002e-02,  8.4525e-02,\n",
      "        -1.0713e-01, -9.2328e-02, -7.2179e-02,  3.7902e-02, -1.4022e-01,\n",
      "         5.5914e-02,  7.9675e-02,  3.9484e-02, -6.8603e-03, -5.1982e-02,\n",
      "        -2.1808e-01, -1.2660e-01,  2.4476e-01, -1.2630e-01,  3.3029e-02,\n",
      "         1.7699e-02,  9.0871e-02,  1.5078e-01, -7.2952e-02,  9.8937e-02,\n",
      "        -7.6370e-02,  8.8079e-02, -8.8526e-02,  1.4171e-02, -3.5435e-02,\n",
      "         4.1153e-03, -1.1882e-01, -5.6413e-02,  1.3846e-02, -4.2599e-02,\n",
      "         8.8813e-02,  2.6194e-03, -5.3535e-02, -1.1002e-01,  1.8341e-01,\n",
      "         9.4184e-02, -1.3347e-01,  5.6069e-02,  1.6303e-01,  1.1132e-04,\n",
      "        -6.4000e-02,  6.0648e-02,  1.7026e-01,  2.7840e-02, -1.4208e-01,\n",
      "        -1.4573e-01, -1.1765e-01,  4.9697e-02,  6.4271e-02, -1.9731e-01,\n",
      "         4.8141e-02,  1.8684e-02, -6.3554e-02,  3.6129e-02,  1.9314e-01,\n",
      "         1.0903e-01, -9.7833e-03,  6.3571e-02,  2.5554e-02, -1.0850e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([ 0.0344, -0.0583, -0.0703, -0.2514, -0.0263, -0.1517, -0.0595,  0.0852,\n",
      "        -0.0268,  0.1277,  0.0393,  0.0879, -0.1254, -0.3348, -0.0745, -0.0879,\n",
      "         0.0474,  0.2322, -0.0987,  0.0354, -0.1064,  0.2409,  0.0734,  0.1306,\n",
      "        -0.0146,  0.1136, -0.1571,  0.0971, -0.0936, -0.0309, -0.0165, -0.0660,\n",
      "        -0.0744, -0.0196, -0.0644,  0.0964, -0.0135,  0.1379,  0.0341, -0.0277,\n",
      "        -0.1563,  0.0474,  0.1149, -0.0199,  0.0589, -0.0346, -0.1100, -0.0020,\n",
      "        -0.0207, -0.0792,  0.0376,  0.0803,  0.0011, -0.0536, -0.0457, -0.1338,\n",
      "        -0.0419, -0.1026,  0.0135, -0.0070,  0.0092,  0.0340,  0.0115,  0.0074,\n",
      "        -0.0919,  0.0340, -0.0404, -0.0352, -0.1594, -0.0584, -0.0647, -0.1073,\n",
      "         0.0282, -0.0426, -0.0435,  0.0278, -0.0031,  0.0945,  0.1075,  0.0418,\n",
      "        -0.1066,  0.0568, -0.0267,  0.0154, -0.1619, -0.0058,  0.1889, -0.0501,\n",
      "        -0.0341, -0.0658,  0.1119, -0.1324,  0.0051, -0.0356, -0.0619,  0.0052,\n",
      "        -0.1332,  0.0911,  0.1441,  0.0152, -0.1952, -0.0013, -0.0183, -0.0338,\n",
      "         0.0631,  0.0013, -0.0094, -0.0361,  0.0370,  0.0204,  0.2303,  0.0010,\n",
      "        -0.0754, -0.0257,  0.1069,  0.0503, -0.0969,  0.0752,  0.0786, -0.0510,\n",
      "         0.0232, -0.1267,  0.0891,  0.1099,  0.0993, -0.0559, -0.0145, -0.0100,\n",
      "         0.1262,  0.0168,  0.0875, -0.0091,  0.2017,  0.0385,  0.1079, -0.0034,\n",
      "        -0.0704,  0.0213,  0.0039, -0.0300, -0.1966, -0.0747, -0.0787,  0.1549,\n",
      "        -0.0108,  0.0581, -0.0527, -0.0684, -0.1514,  0.1286, -0.0561,  0.0960,\n",
      "        -0.0797,  0.0104,  0.0925,  0.0467, -0.0793, -0.0287, -0.1249,  0.0754,\n",
      "         0.0514, -0.1473,  0.1062, -0.0139, -0.0259,  0.0431, -0.0780,  0.0970,\n",
      "         0.1067,  0.0072,  0.0515,  0.0534, -0.0132,  0.0470, -0.1017,  0.1103,\n",
      "         0.0218, -0.1171,  0.0045,  0.2126,  0.0261, -0.0675,  0.0822,  0.0762,\n",
      "        -0.0359, -0.0296,  0.0451, -0.1296, -0.0783, -0.0471, -0.0497, -0.0753,\n",
      "        -0.0754,  0.0663, -0.0081,  0.1472,  0.1225, -0.0161, -0.0844, -0.0095],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 0.0324,  0.0432,  0.0128, -0.1048, -0.0309,  0.0343,  0.0725,  0.0905,\n",
      "         0.0277, -0.0101, -0.2833,  0.0445,  0.0266, -0.0947, -0.0705,  0.0008,\n",
      "         0.0852,  0.0237,  0.1369, -0.0885, -0.1265, -0.0279, -0.0524,  0.1058,\n",
      "         0.0903,  0.1174,  0.0208,  0.1273, -0.0776,  0.0633,  0.1014,  0.0515,\n",
      "         0.0878, -0.0716, -0.0953,  0.0094, -0.0114, -0.1050,  0.1013, -0.0545,\n",
      "         0.2847,  0.1515,  0.0272, -0.0088, -0.0249,  0.1165, -0.0152, -0.0898,\n",
      "         0.0555, -0.0581,  0.0530, -0.1249, -0.0017, -0.0968, -0.0731, -0.0366,\n",
      "         0.0387, -0.0459, -0.0424,  0.0322, -0.0163, -0.0722,  0.0519, -0.2235,\n",
      "        -0.0526, -0.1272, -0.1831,  0.0043, -0.0183,  0.0030, -0.0554,  0.1429,\n",
      "        -0.0177, -0.0099, -0.0047, -0.0410,  0.1171,  0.0849, -0.0063,  0.0121,\n",
      "         0.1281, -0.0245, -0.1118,  0.0065, -0.0303,  0.0243, -0.0665,  0.0273,\n",
      "        -0.1180, -0.0921, -0.0741,  0.1799, -0.1599, -0.1509,  0.0317, -0.1675,\n",
      "         0.0991, -0.1584,  0.0643, -0.0938, -0.0833,  0.0289, -0.1786, -0.0270,\n",
      "        -0.0047, -0.0191,  0.0682, -0.1072,  0.0457,  0.0037,  0.0101, -0.0792,\n",
      "        -0.0411, -0.0145,  0.2259, -0.0552, -0.0088,  0.2277,  0.1019, -0.0581,\n",
      "        -0.0499,  0.0003, -0.0960,  0.1163,  0.0542, -0.0899,  0.0785, -0.0757,\n",
      "         0.1136,  0.0955,  0.0024, -0.1075,  0.0386, -0.1068, -0.1145, -0.0436,\n",
      "        -0.1211, -0.0174,  0.0412, -0.1130,  0.0218, -0.0999, -0.1438,  0.0179,\n",
      "        -0.0548, -0.0479, -0.0121, -0.0538, -0.1268, -0.0741, -0.0415,  0.0144,\n",
      "         0.0508,  0.0760, -0.0638, -0.1439,  0.0555, -0.0486,  0.0251,  0.0053,\n",
      "         0.0245, -0.0231,  0.1099,  0.1551,  0.0105,  0.1027, -0.2184, -0.0827,\n",
      "        -0.1366,  0.0940, -0.1021,  0.1093,  0.1131,  0.0516, -0.1181,  0.1755,\n",
      "         0.0641,  0.0734,  0.1746,  0.1287, -0.0379,  0.0184,  0.0576, -0.0517,\n",
      "         0.0330, -0.0928,  0.0623, -0.0064,  0.0763,  0.0913,  0.0678,  0.1516,\n",
      "         0.0029, -0.0753, -0.0246,  0.0518, -0.0983,  0.1127,  0.0964, -0.1392],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 0.0620, -0.1085, -0.0364, -0.0362, -0.0345,  0.0245, -0.0383, -0.1592,\n",
      "        -0.0493,  0.0025, -0.0031, -0.0267, -0.0372, -0.0254, -0.0815, -0.1367,\n",
      "        -0.1122, -0.0198, -0.1505, -0.0171,  0.2510,  0.0592, -0.0906,  0.0650,\n",
      "        -0.1193,  0.1853,  0.1500, -0.0254, -0.0351, -0.1932,  0.1420,  0.0618,\n",
      "        -0.0726, -0.0382,  0.0244, -0.1537, -0.0670, -0.0176,  0.0063,  0.0121,\n",
      "         0.1055, -0.1070, -0.0291, -0.1309, -0.0195,  0.0570, -0.0449,  0.1710,\n",
      "        -0.0209,  0.1127, -0.0518, -0.0039,  0.0513, -0.0427,  0.0767, -0.0871,\n",
      "        -0.0222,  0.0821, -0.0037, -0.0591, -0.2268,  0.0216,  0.0999, -0.0761,\n",
      "        -0.0202,  0.0163,  0.0175,  0.0805,  0.2464,  0.0258,  0.0094, -0.2156,\n",
      "        -0.1800, -0.0450, -0.0323,  0.0328,  0.1234,  0.0703,  0.1513, -0.0536,\n",
      "        -0.0270,  0.1320,  0.1908,  0.0332, -0.0246, -0.0074,  0.0894,  0.1751,\n",
      "        -0.0369, -0.0786, -0.1112, -0.0129,  0.1746,  0.0117,  0.1111, -0.0848,\n",
      "         0.0966, -0.2175,  0.0244, -0.0435,  0.0372, -0.0111, -0.1484, -0.1168,\n",
      "         0.0543,  0.0504, -0.0847, -0.1751, -0.0511,  0.2542,  0.0312,  0.1713,\n",
      "        -0.0500,  0.0007, -0.0746,  0.0990,  0.0940, -0.0162, -0.0896,  0.0984,\n",
      "        -0.1223, -0.1810, -0.0262,  0.0043,  0.0123, -0.1478, -0.0811, -0.0217,\n",
      "        -0.2336, -0.1054, -0.1123,  0.0213,  0.1850, -0.0052, -0.0100, -0.0414,\n",
      "        -0.0894,  0.0167,  0.1224, -0.0976, -0.0830,  0.1470, -0.1326, -0.0536,\n",
      "        -0.2003,  0.0206,  0.0106, -0.1356,  0.2107, -0.0481, -0.0495, -0.1029,\n",
      "         0.2354, -0.0199, -0.0308, -0.1495, -0.0890,  0.0694, -0.1402,  0.0187,\n",
      "        -0.0880, -0.0337, -0.0247, -0.0611, -0.0248,  0.0132, -0.0624, -0.0821,\n",
      "        -0.1276,  0.0288,  0.0071,  0.0083, -0.0146, -0.1069, -0.1822,  0.0275,\n",
      "         0.1872,  0.0655,  0.1092, -0.0039,  0.0767,  0.0464, -0.0118, -0.0907,\n",
      "         0.2852,  0.0258,  0.0703, -0.0427, -0.0363,  0.1239, -0.0180, -0.0091,\n",
      "         0.0565, -0.1153, -0.0575, -0.0451,  0.1766, -0.1391,  0.0200,  0.1119],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([ 0.0004,  0.0098, -0.0630,  0.0729,  0.1286,  0.1135,  0.0132,  0.0753,\n",
      "         0.1000,  0.0748,  0.0039, -0.1073, -0.0160,  0.0784,  0.0432,  0.0864,\n",
      "         0.0586, -0.0512,  0.2272,  0.1022,  0.0344,  0.0295,  0.0027,  0.0091,\n",
      "         0.0955,  0.0551, -0.1328, -0.0203,  0.2392, -0.1312, -0.0542,  0.1138,\n",
      "         0.0317, -0.0134,  0.1082, -0.0630, -0.2079,  0.0328, -0.0148, -0.0055,\n",
      "         0.0792,  0.0654,  0.0422,  0.0049, -0.1233, -0.0741,  0.1021, -0.0386,\n",
      "        -0.0348,  0.0083, -0.0181,  0.0171,  0.0679,  0.0125, -0.0887, -0.0110,\n",
      "         0.0730, -0.0927,  0.0534,  0.0082, -0.0331,  0.1718,  0.1019, -0.0851,\n",
      "         0.0134,  0.1386, -0.0494,  0.0115,  0.0689,  0.0778,  0.0095,  0.1618,\n",
      "         0.0119,  0.0620, -0.0528, -0.0109, -0.1644,  0.1267, -0.1405,  0.2506,\n",
      "        -0.1118, -0.0659, -0.0724, -0.0041,  0.2570, -0.0359,  0.0570,  0.1049,\n",
      "         0.0314,  0.0490, -0.1250,  0.0921, -0.1334, -0.0470,  0.0743,  0.2864,\n",
      "         0.2034, -0.0486, -0.0558,  0.1079,  0.0035,  0.2203,  0.2007, -0.0190,\n",
      "        -0.0402,  0.2442, -0.1645,  0.0048, -0.0110, -0.0840, -0.2092, -0.0796,\n",
      "        -0.0620, -0.1374, -0.2458, -0.1597, -0.0614,  0.0840, -0.0038,  0.0786,\n",
      "         0.0911, -0.0136, -0.1404, -0.0157,  0.1248, -0.0229, -0.0362, -0.0273,\n",
      "        -0.1132,  0.0388, -0.0171, -0.1177, -0.1702, -0.1764, -0.0355,  0.0977,\n",
      "        -0.0696,  0.1107,  0.1663, -0.0294,  0.1273,  0.0251,  0.0632,  0.0685,\n",
      "        -0.1134, -0.2482, -0.0751, -0.0773, -0.0422,  0.1633,  0.0615,  0.0023,\n",
      "        -0.2218, -0.1290, -0.0822, -0.0304,  0.0999,  0.1201, -0.0902,  0.0529,\n",
      "        -0.0231,  0.1423, -0.1279,  0.0132, -0.1959, -0.1194,  0.2194,  0.0659,\n",
      "         0.0531, -0.0018,  0.1107, -0.0963,  0.1062,  0.0171,  0.0575, -0.1313,\n",
      "        -0.1070, -0.1193,  0.0607, -0.0449,  0.0803, -0.0565,  0.0955,  0.0853,\n",
      "         0.1472, -0.0848, -0.0461,  0.0320, -0.0080,  0.1865, -0.0155,  0.0745,\n",
      "         0.1170, -0.0421, -0.1014, -0.0092,  0.0323,  0.0626, -0.0019,  0.0560],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([10, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([ 0.0136, -0.0106, -0.0700,  0.0738,  0.0363, -0.0981,  0.0008,  0.0320,\n",
      "         0.0276, -0.0871, -0.0159, -0.0787, -0.0114, -0.1574, -0.0150, -0.0063,\n",
      "        -0.0520,  0.0052, -0.0031, -0.0725, -0.1130, -0.0024, -0.0740, -0.0406,\n",
      "         0.0623, -0.0248, -0.1201, -0.1544,  0.0628,  0.0948, -0.1293, -0.0977,\n",
      "        -0.0445,  0.0393, -0.0752, -0.1147, -0.0152, -0.0637,  0.0246,  0.0448,\n",
      "         0.1848,  0.0159, -0.0257, -0.1081,  0.0749, -0.1096,  0.0297,  0.0380,\n",
      "        -0.0246, -0.0982,  0.0286, -0.1074,  0.1856, -0.1482,  0.1644, -0.1338,\n",
      "         0.0282,  0.0384,  0.0637, -0.0539,  0.0754,  0.0908,  0.0045,  0.0340,\n",
      "         0.1005,  0.0013,  0.0536,  0.0342, -0.0304, -0.0040, -0.0625,  0.1837,\n",
      "        -0.0095, -0.0813, -0.0995, -0.0855, -0.0032, -0.0013, -0.0328,  0.0576,\n",
      "         0.0864,  0.0268,  0.0238,  0.0534,  0.0807, -0.0262, -0.0349,  0.0197,\n",
      "        -0.1325,  0.0013,  0.0123, -0.1658, -0.0778, -0.0948, -0.1309,  0.1143,\n",
      "         0.0970,  0.1279,  0.1991, -0.1371, -0.0006,  0.0825, -0.1059,  0.0706,\n",
      "        -0.0534,  0.0225,  0.1133,  0.1787, -0.1257, -0.1148,  0.1220,  0.0250,\n",
      "        -0.1914, -0.2004,  0.0467,  0.0144,  0.0167, -0.0004, -0.1363, -0.0944,\n",
      "        -0.0846, -0.1734,  0.0105,  0.1364, -0.1122, -0.0127,  0.1167, -0.0659,\n",
      "        -0.0496,  0.0946,  0.0137,  0.1505, -0.0582,  0.0473, -0.2295, -0.1048,\n",
      "        -0.0118, -0.1000, -0.0767,  0.0394, -0.0572,  0.0360, -0.1225,  0.1299,\n",
      "        -0.0898,  0.0036, -0.1052,  0.1738, -0.1547, -0.0046, -0.0737,  0.0932,\n",
      "        -0.1146, -0.0283,  0.0006,  0.0109,  0.1251,  0.1539, -0.1232,  0.0430,\n",
      "        -0.1511,  0.0493, -0.0529,  0.0596,  0.0372,  0.1905, -0.1539,  0.0558,\n",
      "         0.1288,  0.1966,  0.0664, -0.0392,  0.0058, -0.1144,  0.0046, -0.0110,\n",
      "         0.0415,  0.1495,  0.1298, -0.0630, -0.1861, -0.0034, -0.0232, -0.1339,\n",
      "         0.0198, -0.0104,  0.0531,  0.0360,  0.0969, -0.0470, -0.1119,  0.1577,\n",
      "         0.0795, -0.0187, -0.0610, -0.0518,  0.0902, -0.1077,  0.0492, -0.0430],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 3.9019e-02,  2.5154e-02,  9.0798e-02, -9.7679e-02,  2.8764e-02,\n",
      "         7.0434e-02,  8.4063e-02,  2.5928e-02,  9.7380e-02,  1.6275e-02,\n",
      "         8.7966e-02,  1.8267e-02, -2.7582e-02,  3.0036e-02, -1.6191e-01,\n",
      "        -8.6450e-02, -1.8375e-03,  8.3683e-02,  1.2906e-02, -1.4225e-02,\n",
      "         1.1338e-01,  3.5779e-02,  1.1134e-01,  3.2772e-02,  3.0779e-02,\n",
      "         4.5491e-02,  6.9465e-02, -7.9200e-02,  1.2984e-01,  1.1974e-01,\n",
      "         5.7609e-02, -1.1568e-01, -1.6414e-01,  3.0509e-02,  6.8447e-02,\n",
      "        -6.2259e-02,  1.0754e-01, -2.4923e-02,  4.4897e-02,  5.2062e-02,\n",
      "         1.1688e-01,  1.9003e-01,  2.0063e-02,  6.7085e-02,  1.0077e-01,\n",
      "         5.4456e-02, -6.7470e-02,  1.3327e-01, -1.5840e-03, -6.3881e-02,\n",
      "         3.4684e-02,  3.7240e-02,  6.3400e-02, -2.8443e-02, -9.6012e-02,\n",
      "        -6.2349e-02, -1.1489e-01,  7.2854e-02, -1.0937e-01,  9.2610e-02,\n",
      "        -2.1004e-04, -3.7679e-02, -9.4393e-03,  2.1505e-01,  1.5926e-01,\n",
      "        -7.5369e-02,  1.9417e-01,  2.1636e-02, -8.1999e-02, -1.8520e-01,\n",
      "         1.7142e-02, -2.9865e-02, -4.0532e-02, -9.7150e-02,  1.5350e-01,\n",
      "        -1.8486e-01, -5.9385e-03,  1.1371e-02, -3.8125e-02, -1.3841e-02,\n",
      "        -2.0428e-02, -1.0368e-01, -4.4102e-02, -2.1970e-02,  1.0765e-02,\n",
      "        -5.5344e-02,  7.0728e-02,  4.6190e-02,  6.2990e-02, -5.4066e-02,\n",
      "        -1.6213e-01, -3.9405e-02,  1.3400e-01,  8.2171e-02, -2.3741e-01,\n",
      "         1.0548e-01, -9.5347e-02, -3.4751e-02, -3.8221e-02,  9.9864e-02,\n",
      "        -1.1664e-01,  1.0195e-01,  3.3650e-02,  8.7214e-03, -5.3215e-02,\n",
      "         7.9702e-02, -9.8969e-02,  1.6521e-01,  4.4730e-02, -2.8468e-02,\n",
      "        -7.8716e-02, -6.4015e-03,  1.3884e-01, -2.8019e-02,  2.1341e-01,\n",
      "        -9.0670e-02,  9.4125e-02, -6.3092e-02,  5.8177e-02,  6.7590e-02,\n",
      "         4.8755e-02, -9.2646e-02, -1.4212e-01, -6.4560e-02, -2.2409e-01,\n",
      "         1.1573e-03,  1.1887e-01, -7.8905e-02, -7.3512e-02, -3.9799e-02,\n",
      "         1.3260e-01,  4.2742e-02,  7.5729e-02,  3.1080e-02, -6.4224e-02,\n",
      "        -1.1484e-01,  1.0520e-01,  7.3674e-02, -1.0186e-01, -3.0466e-02,\n",
      "        -4.0072e-02,  3.1730e-02,  9.9246e-02, -7.0664e-02,  2.0741e-02,\n",
      "        -1.1007e-03, -2.3973e-01,  9.8560e-02,  4.4328e-02,  1.3719e-01,\n",
      "         1.7300e-01,  5.2069e-02, -1.3163e-02, -4.8457e-03,  1.1188e-01,\n",
      "        -4.3641e-02,  9.1629e-02, -7.9687e-02, -3.0985e-02, -5.0387e-02,\n",
      "        -1.0267e-02, -8.4093e-02, -1.3090e-01, -9.6986e-02, -1.1397e-01,\n",
      "         1.7749e-01, -7.0455e-03,  8.1856e-02, -3.4062e-02,  5.2526e-02,\n",
      "        -2.2659e-01,  1.9467e-01,  7.6326e-02,  2.5363e-03,  6.2256e-02,\n",
      "         9.6879e-02,  1.5179e-01, -1.3537e-01, -1.3574e-01, -2.1640e-03,\n",
      "        -1.6477e-01,  1.0380e-01,  6.0904e-02,  1.8073e-02, -5.3058e-02,\n",
      "         4.4933e-02,  2.5375e-01,  2.1265e-03, -2.3270e-02, -1.6723e-02,\n",
      "        -3.1699e-02, -1.4938e-01, -2.2361e-02,  2.1272e-03, -7.7590e-02,\n",
      "        -1.0800e-01, -2.0303e-02,  1.5057e-01, -2.4740e-02,  8.9488e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([-0.1176, -0.0570, -0.0881,  0.0382, -0.0470, -0.0022, -0.0419,  0.0347,\n",
      "        -0.0455,  0.1512, -0.2212,  0.0205,  0.0492,  0.0188,  0.1280,  0.0916,\n",
      "        -0.2246, -0.0407, -0.0579, -0.0697, -0.0332, -0.0426, -0.0589, -0.0386,\n",
      "         0.1296, -0.1049,  0.0089, -0.1006,  0.0838,  0.0634, -0.0685,  0.0280,\n",
      "         0.0230, -0.0933, -0.0234,  0.0639,  0.0010,  0.0445,  0.0817, -0.1759,\n",
      "         0.0889, -0.1021,  0.0453,  0.0165, -0.1924,  0.0662,  0.0422, -0.1617,\n",
      "         0.0394, -0.0525, -0.1020,  0.0969,  0.1674, -0.0956,  0.0062, -0.2188,\n",
      "        -0.1477, -0.0795, -0.0521,  0.0115, -0.0537, -0.0015,  0.0250,  0.0240,\n",
      "        -0.2109, -0.0307, -0.1652, -0.1158,  0.0194, -0.1232, -0.0790,  0.0154,\n",
      "        -0.1488, -0.0429, -0.0399,  0.0404, -0.0934,  0.0394, -0.0222,  0.0784,\n",
      "        -0.0192,  0.0321,  0.1407,  0.0594,  0.0645,  0.2660,  0.1538, -0.0809,\n",
      "         0.0616,  0.0261,  0.0989, -0.0503, -0.0319,  0.0858, -0.1420, -0.0518,\n",
      "         0.1445, -0.0124, -0.0205, -0.1577, -0.0543, -0.0750,  0.0879,  0.0205,\n",
      "        -0.0879,  0.1437, -0.1579, -0.1448, -0.1457, -0.0660, -0.0659, -0.0375,\n",
      "         0.0527, -0.0404,  0.1653,  0.0289,  0.1094, -0.1015,  0.1626, -0.0075,\n",
      "         0.1662,  0.2571,  0.0893,  0.0890, -0.0148, -0.0483, -0.0005,  0.0456,\n",
      "         0.0326, -0.0538, -0.2244,  0.0251, -0.0627,  0.0730, -0.0966, -0.1493,\n",
      "        -0.0732,  0.0858,  0.1233,  0.2035,  0.0183,  0.1147,  0.1068,  0.0161,\n",
      "         0.0902, -0.0651, -0.0715,  0.0183,  0.0563,  0.1722,  0.0345, -0.0602,\n",
      "        -0.1160,  0.0269, -0.1717,  0.0408,  0.0133,  0.1677,  0.0753,  0.2172,\n",
      "        -0.0146,  0.0282, -0.0693,  0.1199, -0.0784, -0.0165,  0.1006,  0.0451,\n",
      "        -0.1145,  0.1461,  0.0647, -0.0945,  0.0623, -0.1480, -0.0218, -0.1240,\n",
      "        -0.0219,  0.0630, -0.0281, -0.0556,  0.0266,  0.0834, -0.0797, -0.1292,\n",
      "         0.0918,  0.0871,  0.0766, -0.0546, -0.1768, -0.0866, -0.0672,  0.0826,\n",
      "        -0.1746,  0.0408, -0.1357, -0.1097, -0.0017,  0.0469, -0.0158, -0.1136],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0365,  0.1492,  0.1437,  0.1214, -0.1905,  0.0414,  0.0920, -0.1194,\n",
      "        -0.0793,  0.0523, -0.0206, -0.0877, -0.1264, -0.0247,  0.0060,  0.1693,\n",
      "        -0.1031,  0.0021,  0.1151,  0.0680, -0.0114, -0.1083,  0.0192, -0.1049,\n",
      "         0.0003,  0.1907, -0.1256,  0.0967,  0.0163,  0.0660,  0.1730, -0.0695,\n",
      "         0.1811,  0.0153, -0.0434, -0.0872,  0.0316, -0.0302, -0.0075, -0.0259,\n",
      "        -0.0154,  0.0773, -0.1435,  0.1663, -0.0149, -0.0396,  0.1477,  0.2279,\n",
      "         0.0989,  0.0160,  0.0486,  0.0256,  0.0010,  0.0822,  0.0260,  0.0013,\n",
      "        -0.0255, -0.0615,  0.0043, -0.1938, -0.0396, -0.0095,  0.0934,  0.0380,\n",
      "        -0.0638,  0.1835, -0.0801, -0.1317,  0.2654,  0.0595, -0.0071, -0.1468,\n",
      "        -0.0288, -0.1604,  0.0815, -0.0099, -0.0089, -0.0636,  0.0295, -0.0150,\n",
      "        -0.1872, -0.0362,  0.1606, -0.1631, -0.0683,  0.1717, -0.1120, -0.0207,\n",
      "        -0.0738, -0.0534,  0.1167,  0.0465, -0.1569,  0.0623,  0.1992, -0.0154,\n",
      "        -0.0664,  0.0287,  0.0269, -0.0208,  0.0981,  0.1508,  0.0021,  0.0982,\n",
      "         0.0723,  0.0228,  0.0112,  0.0613,  0.1816, -0.1070,  0.0553,  0.0236,\n",
      "        -0.0179, -0.0131,  0.0833, -0.0739,  0.0031,  0.0167,  0.0272, -0.1319,\n",
      "         0.0396,  0.0491,  0.0588,  0.0015, -0.0071,  0.1565, -0.0120, -0.1833,\n",
      "         0.1558,  0.1691,  0.0405, -0.1244, -0.0768, -0.0768, -0.1019,  0.1306,\n",
      "         0.0825, -0.0200, -0.0242,  0.0078,  0.0199, -0.1735, -0.1019, -0.0651,\n",
      "        -0.1056, -0.0799,  0.1560,  0.1587, -0.0255,  0.1213, -0.0596,  0.1384,\n",
      "        -0.0424,  0.0252, -0.1582,  0.1200, -0.0028, -0.1406, -0.2185,  0.1694,\n",
      "         0.0334,  0.1966, -0.0360,  0.1704,  0.2084, -0.1135, -0.0869,  0.1150,\n",
      "         0.0498,  0.0928, -0.0948, -0.0336,  0.1267,  0.0648, -0.0360,  0.0417,\n",
      "        -0.0814, -0.0225,  0.2085, -0.0853,  0.0700,  0.1192, -0.0077,  0.0075,\n",
      "        -0.0558, -0.1166,  0.0397,  0.0466,  0.0524, -0.0005,  0.0760,  0.0648,\n",
      "        -0.0572, -0.0123, -0.1625, -0.2079,  0.0009,  0.1398, -0.1563, -0.0761],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-0.0874, -0.2878,  0.0162,  0.1137, -0.0216, -0.0095, -0.0791, -0.0112,\n",
      "         0.1378,  0.0276, -0.0191,  0.0996, -0.0046,  0.0440, -0.1734, -0.1609,\n",
      "        -0.0709, -0.2547,  0.0804,  0.1218,  0.2098, -0.1937, -0.0028, -0.1298,\n",
      "        -0.0120,  0.0623, -0.1709, -0.0088,  0.0245, -0.0329, -0.1025, -0.0525,\n",
      "        -0.0827,  0.2040, -0.0738, -0.0449, -0.0073,  0.0925,  0.0297, -0.0470,\n",
      "        -0.0164,  0.1825, -0.2102, -0.0393, -0.0126,  0.0600, -0.0207, -0.1989,\n",
      "         0.0681,  0.0819, -0.0137,  0.0233,  0.0062, -0.0912, -0.0540, -0.0826,\n",
      "         0.0754,  0.1373,  0.1662, -0.0536, -0.0954, -0.1096, -0.1003, -0.0158,\n",
      "        -0.2121,  0.0899, -0.2017,  0.0826,  0.1063,  0.1378, -0.0192,  0.1304,\n",
      "         0.2962, -0.0886,  0.0716,  0.2377,  0.0810,  0.0063,  0.0698,  0.1793,\n",
      "        -0.0389,  0.0985, -0.1307, -0.2917, -0.1084, -0.1743,  0.0437,  0.0713,\n",
      "        -0.0190, -0.1089, -0.1086,  0.0735,  0.0281,  0.0158,  0.0761, -0.2097,\n",
      "         0.1789, -0.1032,  0.0685, -0.1225,  0.0463,  0.2206,  0.0182,  0.0383,\n",
      "        -0.0163,  0.0126, -0.0634,  0.0739, -0.1272, -0.0418,  0.0581, -0.0152,\n",
      "        -0.1131,  0.0571,  0.0255, -0.0762, -0.1088, -0.0394,  0.0134, -0.1451,\n",
      "        -0.0832, -0.0144,  0.1111, -0.0474,  0.1000,  0.0990,  0.0673, -0.0446,\n",
      "        -0.0586,  0.0127,  0.0218, -0.0582,  0.2106,  0.0093, -0.1277, -0.0548,\n",
      "        -0.0257,  0.2035, -0.1348, -0.0572, -0.0537, -0.0828, -0.1483,  0.0311,\n",
      "        -0.0788,  0.0215, -0.1438, -0.0198, -0.1424,  0.1543, -0.1733, -0.0286,\n",
      "        -0.1172, -0.0252,  0.0334, -0.0683,  0.1256, -0.0856, -0.0397, -0.0185,\n",
      "        -0.1631, -0.1860,  0.0950, -0.1286, -0.0509,  0.0371,  0.0340, -0.0386,\n",
      "        -0.0047,  0.0548, -0.0648, -0.0035,  0.2056, -0.1304,  0.0738,  0.0144,\n",
      "         0.0309, -0.1177, -0.1432,  0.0321,  0.0579, -0.1703, -0.0908,  0.0473,\n",
      "         0.1625, -0.0337,  0.0552,  0.0707,  0.0216, -0.0073,  0.0840, -0.0357,\n",
      "        -0.0778,  0.0333,  0.0204,  0.1302, -0.0612, -0.0760,  0.0045, -0.0715],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 0.1300, -0.0290,  0.0912, -0.1956,  0.0597,  0.0285, -0.1028,  0.0317,\n",
      "        -0.0103,  0.1087,  0.0391,  0.0249,  0.1348,  0.0188,  0.2061, -0.0331,\n",
      "        -0.0511,  0.1448, -0.0687, -0.0512,  0.1953, -0.1046, -0.1168,  0.0371,\n",
      "        -0.0454,  0.1605,  0.2064,  0.0105,  0.0217, -0.1555,  0.0123, -0.0809,\n",
      "         0.0847,  0.0267, -0.2364, -0.1307,  0.0148, -0.1221,  0.1381, -0.0256,\n",
      "         0.0939,  0.0111, -0.0287, -0.2360,  0.0629,  0.1009,  0.0447,  0.1768,\n",
      "        -0.0366, -0.0532,  0.2044,  0.1877, -0.0016,  0.1828, -0.1052, -0.1184,\n",
      "        -0.2589,  0.1056,  0.2014, -0.0456,  0.0197, -0.0621,  0.0749, -0.0556,\n",
      "        -0.1770,  0.1183,  0.2220, -0.0243, -0.0030,  0.0711,  0.0164, -0.0922,\n",
      "         0.0700, -0.0595,  0.1727,  0.0145,  0.0382, -0.3578, -0.0543,  0.0084,\n",
      "         0.0855, -0.0105, -0.0112,  0.0679, -0.1331,  0.0217,  0.0218,  0.0706,\n",
      "        -0.1121, -0.0037,  0.0218, -0.0264, -0.0442,  0.0885, -0.0255, -0.0512,\n",
      "        -0.0117, -0.0858, -0.1294, -0.0291, -0.0313,  0.1300, -0.1170,  0.0544,\n",
      "         0.0718,  0.1288, -0.1224,  0.1531, -0.0877,  0.0856, -0.0822, -0.0295,\n",
      "         0.0070, -0.0089, -0.1059, -0.1029,  0.1119, -0.1977,  0.1345, -0.1227,\n",
      "        -0.1313, -0.0701,  0.0925,  0.0867, -0.0602, -0.0454, -0.1162,  0.1331,\n",
      "        -0.0070,  0.0286,  0.1760, -0.1712,  0.0136,  0.0362,  0.0169, -0.0475,\n",
      "        -0.0543, -0.1353, -0.0515,  0.0787, -0.0935, -0.2302,  0.0657, -0.0193,\n",
      "        -0.0741, -0.1347, -0.1320, -0.0633,  0.0482,  0.0237,  0.0235,  0.1115,\n",
      "         0.0948,  0.2133,  0.0239,  0.0536,  0.0297,  0.1637, -0.1271,  0.0527,\n",
      "         0.0373, -0.0568,  0.0910, -0.0004, -0.1461,  0.1952, -0.0235,  0.0972,\n",
      "         0.0026,  0.0695,  0.1002,  0.0492, -0.0383, -0.0427, -0.0296,  0.0046,\n",
      "         0.0502,  0.0025,  0.0582,  0.1880, -0.0915, -0.0357,  0.0783,  0.0295,\n",
      "        -0.0559, -0.1317,  0.0729, -0.0005,  0.0163, -0.0260, -0.0636, -0.0965,\n",
      "        -0.0161,  0.0863,  0.0049, -0.0130, -0.0310,  0.0269,  0.0490, -0.0194],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-0.0755,  0.1157,  0.1487,  0.0312,  0.0088, -0.0934,  0.1386,  0.0494,\n",
      "         0.1581,  0.0538, -0.1382,  0.1405, -0.0518, -0.1131, -0.0435,  0.0108,\n",
      "        -0.0136, -0.0261,  0.0036, -0.0735,  0.0057,  0.1861, -0.0205, -0.0147,\n",
      "         0.1115, -0.1055, -0.0333, -0.0581,  0.0298, -0.1677,  0.0853, -0.1542,\n",
      "         0.0841, -0.0789,  0.1823,  0.0546, -0.1157, -0.0021, -0.0624, -0.0842,\n",
      "        -0.0643,  0.0172, -0.0460, -0.0905, -0.1069,  0.0294,  0.1544, -0.0694,\n",
      "         0.0053,  0.0024,  0.1008,  0.1026,  0.2395,  0.0446, -0.0760,  0.0411,\n",
      "        -0.0543,  0.0697, -0.0095,  0.2348, -0.0666,  0.0692, -0.0046,  0.0664,\n",
      "        -0.0122, -0.0853,  0.0614, -0.0631,  0.1126, -0.1014, -0.0805,  0.0063,\n",
      "         0.0694, -0.0091, -0.1463,  0.0367, -0.0787,  0.1328, -0.1187,  0.0344,\n",
      "         0.1351, -0.1024,  0.0370, -0.0922, -0.0747,  0.0126,  0.0906, -0.0895,\n",
      "        -0.1623,  0.0673,  0.0384, -0.0280,  0.0264,  0.0056,  0.0317, -0.0555,\n",
      "         0.0636, -0.2060,  0.0456,  0.0843, -0.0197,  0.0903,  0.2240,  0.1724,\n",
      "         0.0046,  0.0300,  0.0270,  0.0626,  0.1121,  0.0250, -0.0804,  0.0623,\n",
      "         0.0696,  0.0334,  0.0043, -0.2056,  0.1308,  0.0856, -0.0198, -0.0268,\n",
      "         0.1300, -0.0193,  0.0783,  0.0175, -0.0085,  0.0610, -0.0509,  0.0733,\n",
      "         0.0186, -0.0053,  0.0114, -0.0056, -0.0509,  0.0588,  0.1049,  0.0542,\n",
      "         0.1166, -0.1209,  0.0434,  0.1118,  0.0814, -0.0112,  0.0703, -0.0407,\n",
      "        -0.0151, -0.0558,  0.0140,  0.1696, -0.0412,  0.1057,  0.0986, -0.0241,\n",
      "        -0.0095, -0.0080, -0.0638, -0.0291,  0.0576, -0.0776, -0.1520, -0.1818,\n",
      "         0.0851, -0.0793,  0.1604, -0.0665,  0.0501,  0.1502, -0.1287,  0.0479,\n",
      "        -0.1453,  0.0263, -0.0035,  0.0960,  0.0638, -0.1566,  0.0418,  0.0929,\n",
      "         0.1142, -0.0140,  0.0344,  0.1191, -0.1846, -0.0907,  0.1322, -0.0362,\n",
      "         0.0544, -0.1205, -0.0043,  0.0934, -0.0007,  0.0405,  0.1871, -0.0829,\n",
      "        -0.0702,  0.1021,  0.0760,  0.0168, -0.0093, -0.1217,  0.0335,  0.1808],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 0.0619,  0.1403, -0.1077, -0.2767,  0.2080,  0.0575, -0.0134, -0.1055,\n",
      "         0.1499,  0.1858,  0.0217,  0.1187, -0.0649,  0.2870, -0.1568,  0.1042,\n",
      "        -0.1069, -0.1348,  0.0584, -0.1022, -0.0325, -0.0836,  0.0143,  0.0472,\n",
      "         0.0017, -0.0113, -0.0767, -0.1494,  0.0455, -0.1245, -0.0429,  0.0766,\n",
      "         0.0107, -0.0946, -0.1371,  0.1191,  0.1512,  0.0053,  0.0394, -0.0465,\n",
      "         0.0175, -0.1627,  0.0771,  0.0206,  0.1903, -0.1087,  0.1277,  0.0255,\n",
      "         0.1399,  0.1205,  0.1156, -0.0843,  0.0157,  0.0475, -0.1077, -0.0837,\n",
      "        -0.0339, -0.1300, -0.0038, -0.0063, -0.1576,  0.0628,  0.1015, -0.0629,\n",
      "        -0.0113,  0.1316,  0.1097,  0.0832,  0.0638, -0.0506, -0.0556, -0.0724,\n",
      "        -0.0188, -0.0401, -0.1590,  0.0450,  0.0624, -0.0515, -0.0752, -0.0796,\n",
      "        -0.0007, -0.3024, -0.2144, -0.0835, -0.0758, -0.1519, -0.1939,  0.1572,\n",
      "        -0.1857,  0.1345, -0.1711, -0.0674,  0.0412, -0.1714, -0.0072, -0.2019,\n",
      "         0.0787,  0.0984,  0.0226, -0.0245, -0.0423,  0.1742, -0.2161, -0.0183,\n",
      "        -0.0029, -0.0044,  0.1192,  0.0439,  0.1932,  0.0082,  0.1160, -0.1654,\n",
      "        -0.0213, -0.1270, -0.0089, -0.0236, -0.0589,  0.1086,  0.1448, -0.0744,\n",
      "        -0.0636, -0.0634,  0.0920, -0.0227, -0.0138, -0.2378,  0.0915,  0.0410,\n",
      "        -0.0771, -0.2554, -0.0056, -0.0511,  0.0792, -0.1065,  0.0931,  0.0945,\n",
      "         0.1621, -0.0576,  0.0169,  0.0327,  0.0696,  0.0691, -0.0391,  0.1367,\n",
      "        -0.0409, -0.0418,  0.0063, -0.0193, -0.0210, -0.0267,  0.1050, -0.0354,\n",
      "         0.1055,  0.0792,  0.0318, -0.0377,  0.0276, -0.1005, -0.1378, -0.0189,\n",
      "         0.0850,  0.1300, -0.0152, -0.1383,  0.0798, -0.1142, -0.0370, -0.0458,\n",
      "        -0.1406,  0.0460, -0.0168,  0.1019,  0.0849,  0.0184,  0.0901, -0.1363,\n",
      "         0.2124,  0.0447, -0.1437,  0.0413, -0.3102,  0.0478,  0.0038, -0.0860,\n",
      "        -0.1465, -0.0472, -0.1059,  0.2646,  0.0621, -0.1125,  0.0021,  0.0192,\n",
      "        -0.0153, -0.1243, -0.0433, -0.1272, -0.0750,  0.0914, -0.1973,  0.0438],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([-4.2623e-02, -3.6569e-02,  2.1554e-01,  7.3948e-02,  4.5928e-02,\n",
      "         8.9498e-02,  5.3931e-02,  7.0713e-02, -1.2092e-01,  1.1116e-01,\n",
      "        -3.0323e-02, -1.2760e-01, -8.4349e-02,  8.3266e-02, -1.4023e-01,\n",
      "        -1.5514e-02, -1.0077e-01,  1.6821e-02,  1.4161e-01,  4.8444e-02,\n",
      "         2.6380e-01,  1.6819e-02,  2.7638e-02,  1.6771e-01, -4.9164e-02,\n",
      "        -5.8186e-02,  3.7733e-02, -7.5490e-02, -1.2715e-01,  6.3545e-03,\n",
      "         4.5001e-02, -1.1637e-01,  4.3808e-02, -6.4871e-02, -1.9854e-01,\n",
      "        -8.2011e-02,  4.6362e-02,  4.2026e-02,  3.5503e-02, -9.2668e-03,\n",
      "        -6.1567e-02, -7.3332e-02,  1.2077e-01,  9.6574e-03, -1.1398e-01,\n",
      "        -8.3217e-02,  9.7229e-02, -1.8468e-01,  1.0019e-01,  1.1664e-01,\n",
      "        -4.1808e-03, -1.5724e-01, -1.2025e-01, -4.9540e-02,  8.5170e-02,\n",
      "         7.6881e-02,  1.2248e-01, -4.9930e-02, -1.4443e-01,  1.7782e-01,\n",
      "         4.8178e-02,  5.4946e-02, -4.6922e-02,  2.5569e-02,  6.4497e-02,\n",
      "         6.3852e-03, -2.8890e-02, -2.0665e-02,  6.1656e-02, -8.2958e-03,\n",
      "        -1.1087e-01, -4.8137e-02,  1.0483e-01,  6.2579e-02, -1.6083e-04,\n",
      "        -1.1344e-01, -8.9015e-02, -1.7166e-02, -4.1521e-02, -1.5454e-01,\n",
      "        -1.4731e-01,  8.7373e-02, -2.9756e-02,  2.0193e-02, -1.1296e-01,\n",
      "        -3.1650e-02,  4.1894e-03, -1.8676e-01,  6.8287e-02, -2.2091e-02,\n",
      "        -1.2922e-01,  2.7839e-02, -1.4709e-01, -4.7553e-02, -3.7346e-03,\n",
      "         2.2710e-02,  2.5883e-02, -2.9356e-02,  1.6152e-01,  1.6246e-01,\n",
      "        -1.3543e-01,  8.9756e-02, -4.2627e-02, -8.3722e-02, -1.5255e-01,\n",
      "         2.2945e-01, -1.2727e-01, -2.3556e-01, -2.0420e-02,  1.3193e-01,\n",
      "         4.7064e-02,  1.3503e-01, -1.1775e-01,  4.4378e-02, -3.4163e-02,\n",
      "        -1.9214e-01,  1.5731e-01, -3.1321e-02,  6.0982e-02, -5.8376e-03,\n",
      "         1.5299e-01, -5.1013e-02, -1.3059e-01, -4.6080e-02, -4.5595e-02,\n",
      "         1.3429e-02, -6.2867e-02, -9.5215e-02, -1.7407e-02,  7.2585e-02,\n",
      "         3.3903e-02, -8.6348e-02, -1.3825e-01,  4.7451e-02, -5.0096e-02,\n",
      "        -3.1774e-02,  6.5457e-02,  1.4885e-01, -2.2025e-01, -4.0868e-02,\n",
      "         9.5003e-02, -2.9685e-02, -9.5620e-03, -1.2427e-01,  1.7506e-02,\n",
      "        -6.4477e-02,  7.1862e-02,  7.0507e-03, -5.8418e-02,  1.5294e-03,\n",
      "         3.2851e-02,  1.9095e-02, -8.0235e-02,  6.8884e-02,  6.0092e-02,\n",
      "         1.5271e-01,  1.9154e-01, -1.1346e-01,  6.4830e-02,  3.5148e-02,\n",
      "         1.3541e-01, -3.6145e-02,  7.7164e-02,  6.6006e-02, -9.8362e-02,\n",
      "        -5.8074e-02, -6.8333e-02, -6.2350e-02, -1.1440e-01, -7.7808e-02,\n",
      "        -5.0657e-02, -1.3394e-01,  7.3470e-03,  2.3200e-02,  5.6048e-02,\n",
      "        -1.1629e-01,  1.7295e-02, -3.8359e-02, -9.3535e-02,  5.3482e-02,\n",
      "         9.3346e-02, -5.7120e-03,  4.7397e-02, -1.3661e-01,  4.1543e-02,\n",
      "        -3.0960e-02, -2.6104e-02,  3.4355e-02, -8.6857e-02,  9.2401e-03,\n",
      "        -1.4995e-01,  1.1801e-01, -5.6550e-03, -9.9898e-02,  1.3610e-02,\n",
      "         1.8182e-02, -1.3491e-01, -8.2483e-02, -6.1044e-02, -5.1449e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-0.0404,  0.0334, -0.0873, -0.0599, -0.0711,  0.0188,  0.0374,  0.0498,\n",
      "         0.0180,  0.0348, -0.1043,  0.1819, -0.0947,  0.0222,  0.0732,  0.0492,\n",
      "         0.1486,  0.1599, -0.0004,  0.1500,  0.0163, -0.0051,  0.1059,  0.0772,\n",
      "         0.0216,  0.1549, -0.0047,  0.0367,  0.0610, -0.1326,  0.2364, -0.0735,\n",
      "        -0.1667,  0.0856,  0.0432, -0.1610, -0.1542,  0.0944, -0.0258,  0.0386,\n",
      "         0.0133,  0.0888,  0.0012, -0.1798,  0.0380, -0.0701,  0.0899, -0.0979,\n",
      "         0.1967, -0.0406, -0.0714, -0.1593,  0.0574,  0.0158, -0.1353, -0.0116,\n",
      "        -0.0659, -0.0594, -0.0836,  0.1687,  0.1432, -0.1235, -0.1411, -0.1314,\n",
      "        -0.0911,  0.2131, -0.1926, -0.0544, -0.1664,  0.0304,  0.1048, -0.0392,\n",
      "        -0.0422, -0.1233,  0.0076, -0.0738, -0.1557,  0.1152,  0.0373, -0.0577,\n",
      "         0.0090, -0.0523,  0.0164,  0.0149, -0.0101, -0.0796,  0.0258, -0.1029,\n",
      "         0.0345, -0.0131, -0.0666, -0.0779,  0.0692, -0.0341, -0.0940, -0.1161,\n",
      "        -0.0177,  0.1019, -0.1398,  0.0373, -0.1376, -0.0576,  0.0860, -0.0356,\n",
      "         0.1078,  0.0419,  0.2311, -0.0366,  0.1426, -0.1082, -0.0577, -0.0187,\n",
      "         0.0658, -0.1028, -0.0874, -0.0337,  0.1840,  0.0999,  0.2344,  0.0839,\n",
      "         0.1116,  0.0364,  0.0575,  0.0951,  0.0945,  0.0339, -0.0087,  0.0691,\n",
      "         0.0113,  0.0132, -0.0651,  0.0311, -0.0628,  0.0944, -0.0417,  0.1208,\n",
      "         0.0239,  0.1379,  0.1112,  0.1446, -0.1183, -0.0495, -0.0503, -0.1729,\n",
      "         0.0460, -0.1241, -0.0561,  0.0750, -0.1225, -0.0539,  0.2356,  0.0499,\n",
      "        -0.0831,  0.0315,  0.1233,  0.1682, -0.1815,  0.1077,  0.1187,  0.1555,\n",
      "         0.1233, -0.0816, -0.0952, -0.1361,  0.0105,  0.0832,  0.0652,  0.0702,\n",
      "         0.0459,  0.0453, -0.0204, -0.0968, -0.0660,  0.0714, -0.0433,  0.0744,\n",
      "        -0.0709,  0.0625,  0.0672, -0.1082, -0.0944,  0.1826,  0.0588,  0.0226,\n",
      "         0.1245, -0.0560,  0.1531,  0.0472,  0.0311, -0.0327, -0.1191,  0.0361,\n",
      "        -0.1240,  0.0982,  0.0693,  0.0030, -0.1002,  0.2047,  0.0471, -0.0009],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0005000'], tr/val_loss:  1.822531/ 24.532928, val:  37.92%, val_best:  37.92%, tr:  38.92%, tr_best:  38.92%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0005000'], tr/val_loss:  1.543547/ 12.400267, val:  47.08%, val_best:  47.08%, tr:  47.29%, tr_best:  47.29%, epoch time: 43.74 seconds, 0.73 minutes\n",
      "epoch-2   lr=['0.0005000'], tr/val_loss:  1.376058/ 13.056368, val:  49.17%, val_best:  49.17%, tr:  49.64%, tr_best:  49.64%, epoch time: 43.77 seconds, 0.73 minutes\n",
      "epoch-3   lr=['0.0005000'], tr/val_loss:  1.302927/  9.570583, val:  50.00%, val_best:  50.00%, tr:  52.40%, tr_best:  52.40%, epoch time: 44.15 seconds, 0.74 minutes\n",
      "epoch-4   lr=['0.0005000'], tr/val_loss:  1.252800/  6.996183, val:  61.25%, val_best:  61.25%, tr:  57.71%, tr_best:  57.71%, epoch time: 43.60 seconds, 0.73 minutes\n",
      "epoch-5   lr=['0.0005000'], tr/val_loss:  1.210568/ 11.301035, val:  49.17%, val_best:  61.25%, tr:  57.71%, tr_best:  57.71%, epoch time: 42.95 seconds, 0.72 minutes\n",
      "epoch-6   lr=['0.0005000'], tr/val_loss:  1.189291/ 16.107555, val:  51.25%, val_best:  61.25%, tr:  58.73%, tr_best:  58.73%, epoch time: 43.29 seconds, 0.72 minutes\n",
      "epoch-7   lr=['0.0005000'], tr/val_loss:  1.137870/ 12.434402, val:  60.00%, val_best:  61.25%, tr:  59.45%, tr_best:  59.45%, epoch time: 43.86 seconds, 0.73 minutes\n",
      "epoch-8   lr=['0.0005000'], tr/val_loss:  1.173998/  6.967353, val:  63.33%, val_best:  63.33%, tr:  59.14%, tr_best:  59.45%, epoch time: 43.60 seconds, 0.73 minutes\n",
      "epoch-9   lr=['0.0005000'], tr/val_loss:  1.105370/ 18.414204, val:  49.58%, val_best:  63.33%, tr:  59.24%, tr_best:  59.45%, epoch time: 43.60 seconds, 0.73 minutes\n",
      "epoch-10  lr=['0.0005000'], tr/val_loss:  1.050690/ 19.068913, val:  55.42%, val_best:  63.33%, tr:  64.45%, tr_best:  64.45%, epoch time: 43.43 seconds, 0.72 minutes\n",
      "epoch-11  lr=['0.0005000'], tr/val_loss:  1.085561/ 11.651156, val:  56.25%, val_best:  63.33%, tr:  60.78%, tr_best:  64.45%, epoch time: 42.90 seconds, 0.72 minutes\n",
      "epoch-12  lr=['0.0005000'], tr/val_loss:  1.034317/ 21.428219, val:  50.83%, val_best:  63.33%, tr:  62.41%, tr_best:  64.45%, epoch time: 43.26 seconds, 0.72 minutes\n",
      "epoch-13  lr=['0.0005000'], tr/val_loss:  1.071355/ 19.751383, val:  49.58%, val_best:  63.33%, tr:  60.06%, tr_best:  64.45%, epoch time: 43.20 seconds, 0.72 minutes\n",
      "epoch-14  lr=['0.0005000'], tr/val_loss:  0.975966/ 22.512310, val:  52.92%, val_best:  63.33%, tr:  65.07%, tr_best:  65.07%, epoch time: 43.65 seconds, 0.73 minutes\n",
      "epoch-15  lr=['0.0005000'], tr/val_loss:  0.984075/ 17.049929, val:  57.50%, val_best:  63.33%, tr:  64.45%, tr_best:  65.07%, epoch time: 44.00 seconds, 0.73 minutes\n",
      "epoch-16  lr=['0.0005000'], tr/val_loss:  0.979974/ 21.859529, val:  58.75%, val_best:  63.33%, tr:  65.17%, tr_best:  65.17%, epoch time: 43.44 seconds, 0.72 minutes\n",
      "epoch-17  lr=['0.0005000'], tr/val_loss:  0.991598/ 10.054672, val:  59.58%, val_best:  63.33%, tr:  64.56%, tr_best:  65.17%, epoch time: 43.79 seconds, 0.73 minutes\n",
      "epoch-18  lr=['0.0005000'], tr/val_loss:  0.990635/ 16.876419, val:  60.00%, val_best:  63.33%, tr:  64.04%, tr_best:  65.17%, epoch time: 43.19 seconds, 0.72 minutes\n",
      "epoch-19  lr=['0.0005000'], tr/val_loss:  0.922267/ 25.162151, val:  52.50%, val_best:  63.33%, tr:  66.60%, tr_best:  66.60%, epoch time: 42.43 seconds, 0.71 minutes\n",
      "epoch-20  lr=['0.0005000'], tr/val_loss:  0.961829/ 22.668015, val:  57.50%, val_best:  63.33%, tr:  65.37%, tr_best:  66.60%, epoch time: 43.90 seconds, 0.73 minutes\n",
      "epoch-21  lr=['0.0005000'], tr/val_loss:  0.944947/ 14.159207, val:  61.25%, val_best:  63.33%, tr:  65.88%, tr_best:  66.60%, epoch time: 44.04 seconds, 0.73 minutes\n",
      "epoch-22  lr=['0.0005000'], tr/val_loss:  0.979264/ 14.856361, val:  57.92%, val_best:  63.33%, tr:  64.45%, tr_best:  66.60%, epoch time: 43.50 seconds, 0.73 minutes\n",
      "epoch-23  lr=['0.0005000'], tr/val_loss:  0.921392/ 14.055450, val:  57.50%, val_best:  63.33%, tr:  65.78%, tr_best:  66.60%, epoch time: 43.60 seconds, 0.73 minutes\n",
      "epoch-24  lr=['0.0005000'], tr/val_loss:  0.934146/ 21.087107, val:  53.75%, val_best:  63.33%, tr:  65.99%, tr_best:  66.60%, epoch time: 43.47 seconds, 0.72 minutes\n",
      "epoch-25  lr=['0.0005000'], tr/val_loss:  0.950954/ 12.409653, val:  59.58%, val_best:  63.33%, tr:  65.58%, tr_best:  66.60%, epoch time: 43.32 seconds, 0.72 minutes\n",
      "epoch-26  lr=['0.0005000'], tr/val_loss:  0.893019/ 19.070408, val:  52.92%, val_best:  63.33%, tr:  69.46%, tr_best:  69.46%, epoch time: 43.53 seconds, 0.73 minutes\n",
      "epoch-27  lr=['0.0005000'], tr/val_loss:  0.904018/ 22.056566, val:  56.25%, val_best:  63.33%, tr:  67.11%, tr_best:  69.46%, epoch time: 44.03 seconds, 0.73 minutes\n",
      "epoch-28  lr=['0.0005000'], tr/val_loss:  0.833866/ 16.026237, val:  59.17%, val_best:  63.33%, tr:  69.87%, tr_best:  69.87%, epoch time: 43.81 seconds, 0.73 minutes\n",
      "epoch-29  lr=['0.0005000'], tr/val_loss:  0.878228/ 19.784136, val:  61.67%, val_best:  63.33%, tr:  66.80%, tr_best:  69.87%, epoch time: 44.35 seconds, 0.74 minutes\n",
      "epoch-30  lr=['0.0005000'], tr/val_loss:  0.893658/ 13.869474, val:  60.00%, val_best:  63.33%, tr:  66.91%, tr_best:  69.87%, epoch time: 43.71 seconds, 0.73 minutes\n",
      "epoch-31  lr=['0.0005000'], tr/val_loss:  0.886922/ 19.072994, val:  62.50%, val_best:  63.33%, tr:  67.31%, tr_best:  69.87%, epoch time: 43.89 seconds, 0.73 minutes\n",
      "epoch-32  lr=['0.0005000'], tr/val_loss:  0.875715/ 23.502182, val:  58.75%, val_best:  63.33%, tr:  67.93%, tr_best:  69.87%, epoch time: 43.18 seconds, 0.72 minutes\n",
      "epoch-33  lr=['0.0005000'], tr/val_loss:  0.886517/ 22.877003, val:  57.08%, val_best:  63.33%, tr:  67.21%, tr_best:  69.87%, epoch time: 43.63 seconds, 0.73 minutes\n",
      "epoch-34  lr=['0.0005000'], tr/val_loss:  0.873074/ 30.836187, val:  49.58%, val_best:  63.33%, tr:  69.46%, tr_best:  69.87%, epoch time: 43.42 seconds, 0.72 minutes\n",
      "epoch-35  lr=['0.0005000'], tr/val_loss:  0.825115/ 13.168785, val:  60.42%, val_best:  63.33%, tr:  68.44%, tr_best:  69.87%, epoch time: 43.42 seconds, 0.72 minutes\n",
      "epoch-36  lr=['0.0005000'], tr/val_loss:  0.847406/ 13.862246, val:  63.33%, val_best:  63.33%, tr:  69.77%, tr_best:  69.87%, epoch time: 43.53 seconds, 0.73 minutes\n",
      "epoch-37  lr=['0.0005000'], tr/val_loss:  0.776596/ 15.803477, val:  63.33%, val_best:  63.33%, tr:  72.01%, tr_best:  72.01%, epoch time: 43.41 seconds, 0.72 minutes\n",
      "epoch-38  lr=['0.0005000'], tr/val_loss:  0.786034/ 23.348022, val:  58.33%, val_best:  63.33%, tr:  69.25%, tr_best:  72.01%, epoch time: 44.13 seconds, 0.74 minutes\n",
      "epoch-39  lr=['0.0005000'], tr/val_loss:  0.858249/ 24.616358, val:  59.17%, val_best:  63.33%, tr:  68.74%, tr_best:  72.01%, epoch time: 43.49 seconds, 0.72 minutes\n",
      "epoch-40  lr=['0.0005000'], tr/val_loss:  0.804666/ 19.806602, val:  63.75%, val_best:  63.75%, tr:  69.97%, tr_best:  72.01%, epoch time: 43.69 seconds, 0.73 minutes\n",
      "epoch-41  lr=['0.0005000'], tr/val_loss:  0.817220/ 17.685410, val:  61.25%, val_best:  63.75%, tr:  71.30%, tr_best:  72.01%, epoch time: 43.40 seconds, 0.72 minutes\n",
      "epoch-42  lr=['0.0005000'], tr/val_loss:  0.812270/ 26.700645, val:  58.75%, val_best:  63.75%, tr:  69.15%, tr_best:  72.01%, epoch time: 43.33 seconds, 0.72 minutes\n",
      "epoch-43  lr=['0.0005000'], tr/val_loss:  0.796197/ 22.707180, val:  60.00%, val_best:  63.75%, tr:  70.17%, tr_best:  72.01%, epoch time: 42.92 seconds, 0.72 minutes\n",
      "epoch-44  lr=['0.0005000'], tr/val_loss:  0.773717/ 19.253090, val:  62.50%, val_best:  63.75%, tr:  71.60%, tr_best:  72.01%, epoch time: 43.45 seconds, 0.72 minutes\n",
      "epoch-45  lr=['0.0005000'], tr/val_loss:  0.787273/ 17.157986, val:  65.00%, val_best:  65.00%, tr:  70.89%, tr_best:  72.01%, epoch time: 43.50 seconds, 0.73 minutes\n",
      "epoch-46  lr=['0.0005000'], tr/val_loss:  0.797220/ 18.977068, val:  59.58%, val_best:  65.00%, tr:  70.48%, tr_best:  72.01%, epoch time: 44.02 seconds, 0.73 minutes\n",
      "epoch-47  lr=['0.0005000'], tr/val_loss:  0.741981/ 22.007710, val:  64.58%, val_best:  65.00%, tr:  71.71%, tr_best:  72.01%, epoch time: 42.81 seconds, 0.71 minutes\n",
      "epoch-48  lr=['0.0005000'], tr/val_loss:  0.781298/ 15.721438, val:  61.67%, val_best:  65.00%, tr:  71.50%, tr_best:  72.01%, epoch time: 42.64 seconds, 0.71 minutes\n",
      "epoch-49  lr=['0.0005000'], tr/val_loss:  0.764265/ 12.613366, val:  72.08%, val_best:  72.08%, tr:  71.50%, tr_best:  72.01%, epoch time: 43.82 seconds, 0.73 minutes\n",
      "epoch-50  lr=['0.0005000'], tr/val_loss:  0.773536/ 28.780405, val:  59.17%, val_best:  72.08%, tr:  72.93%, tr_best:  72.93%, epoch time: 43.14 seconds, 0.72 minutes\n",
      "epoch-51  lr=['0.0005000'], tr/val_loss:  0.764893/ 21.484385, val:  57.08%, val_best:  72.08%, tr:  71.81%, tr_best:  72.93%, epoch time: 43.20 seconds, 0.72 minutes\n",
      "epoch-52  lr=['0.0005000'], tr/val_loss:  0.734748/ 26.950840, val:  60.00%, val_best:  72.08%, tr:  72.11%, tr_best:  72.93%, epoch time: 43.63 seconds, 0.73 minutes\n",
      "epoch-53  lr=['0.0005000'], tr/val_loss:  0.795357/ 21.427765, val:  63.75%, val_best:  72.08%, tr:  71.81%, tr_best:  72.93%, epoch time: 43.31 seconds, 0.72 minutes\n",
      "epoch-54  lr=['0.0005000'], tr/val_loss:  0.753108/ 18.711618, val:  65.00%, val_best:  72.08%, tr:  74.16%, tr_best:  74.16%, epoch time: 43.35 seconds, 0.72 minutes\n",
      "epoch-55  lr=['0.0005000'], tr/val_loss:  0.726122/ 17.537619, val:  64.58%, val_best:  72.08%, tr:  72.73%, tr_best:  74.16%, epoch time: 43.55 seconds, 0.73 minutes\n",
      "epoch-56  lr=['0.0005000'], tr/val_loss:  0.752780/ 23.718288, val:  60.00%, val_best:  72.08%, tr:  72.01%, tr_best:  74.16%, epoch time: 43.35 seconds, 0.72 minutes\n",
      "epoch-57  lr=['0.0005000'], tr/val_loss:  0.664467/ 20.711655, val:  60.83%, val_best:  72.08%, tr:  75.08%, tr_best:  75.08%, epoch time: 43.28 seconds, 0.72 minutes\n",
      "epoch-58  lr=['0.0005000'], tr/val_loss:  0.723728/ 17.987682, val:  64.58%, val_best:  72.08%, tr:  74.06%, tr_best:  75.08%, epoch time: 44.26 seconds, 0.74 minutes\n",
      "epoch-59  lr=['0.0005000'], tr/val_loss:  0.714394/ 41.306698, val:  55.42%, val_best:  72.08%, tr:  74.46%, tr_best:  75.08%, epoch time: 43.58 seconds, 0.73 minutes\n",
      "epoch-60  lr=['0.0005000'], tr/val_loss:  0.740512/ 28.788916, val:  57.08%, val_best:  72.08%, tr:  73.03%, tr_best:  75.08%, epoch time: 43.28 seconds, 0.72 minutes\n",
      "epoch-61  lr=['0.0005000'], tr/val_loss:  0.740992/ 32.316006, val:  59.58%, val_best:  72.08%, tr:  71.60%, tr_best:  75.08%, epoch time: 43.63 seconds, 0.73 minutes\n",
      "epoch-62  lr=['0.0005000'], tr/val_loss:  0.755698/ 16.047607, val:  64.17%, val_best:  72.08%, tr:  72.63%, tr_best:  75.08%, epoch time: 43.74 seconds, 0.73 minutes\n",
      "epoch-63  lr=['0.0005000'], tr/val_loss:  0.687687/ 17.553238, val:  63.75%, val_best:  72.08%, tr:  75.49%, tr_best:  75.49%, epoch time: 43.70 seconds, 0.73 minutes\n",
      "epoch-64  lr=['0.0005000'], tr/val_loss:  0.711376/ 19.246830, val:  62.08%, val_best:  72.08%, tr:  73.34%, tr_best:  75.49%, epoch time: 43.61 seconds, 0.73 minutes\n",
      "epoch-65  lr=['0.0005000'], tr/val_loss:  0.701248/ 13.356367, val:  68.33%, val_best:  72.08%, tr:  73.85%, tr_best:  75.49%, epoch time: 43.49 seconds, 0.72 minutes\n",
      "epoch-66  lr=['0.0005000'], tr/val_loss:  0.727162/ 21.204643, val:  64.17%, val_best:  72.08%, tr:  72.52%, tr_best:  75.49%, epoch time: 44.24 seconds, 0.74 minutes\n",
      "epoch-67  lr=['0.0005000'], tr/val_loss:  0.739079/ 24.458937, val:  60.00%, val_best:  72.08%, tr:  72.32%, tr_best:  75.49%, epoch time: 43.52 seconds, 0.73 minutes\n",
      "epoch-68  lr=['0.0005000'], tr/val_loss:  0.727849/ 24.359682, val:  60.83%, val_best:  72.08%, tr:  74.36%, tr_best:  75.49%, epoch time: 43.50 seconds, 0.73 minutes\n",
      "epoch-69  lr=['0.0005000'], tr/val_loss:  0.693572/ 17.182774, val:  65.83%, val_best:  72.08%, tr:  74.77%, tr_best:  75.49%, epoch time: 43.56 seconds, 0.73 minutes\n",
      "epoch-70  lr=['0.0005000'], tr/val_loss:  0.698932/ 17.277136, val:  70.42%, val_best:  72.08%, tr:  71.71%, tr_best:  75.49%, epoch time: 43.70 seconds, 0.73 minutes\n",
      "epoch-71  lr=['0.0005000'], tr/val_loss:  0.735399/ 19.801517, val:  61.25%, val_best:  72.08%, tr:  72.73%, tr_best:  75.49%, epoch time: 43.23 seconds, 0.72 minutes\n",
      "epoch-72  lr=['0.0005000'], tr/val_loss:  0.697403/ 23.840729, val:  59.58%, val_best:  72.08%, tr:  73.24%, tr_best:  75.49%, epoch time: 43.35 seconds, 0.72 minutes\n",
      "epoch-73  lr=['0.0005000'], tr/val_loss:  0.692975/ 23.510363, val:  61.25%, val_best:  72.08%, tr:  73.95%, tr_best:  75.49%, epoch time: 43.20 seconds, 0.72 minutes\n",
      "epoch-74  lr=['0.0005000'], tr/val_loss:  0.755210/ 25.482620, val:  62.92%, val_best:  72.08%, tr:  72.63%, tr_best:  75.49%, epoch time: 43.82 seconds, 0.73 minutes\n",
      "epoch-75  lr=['0.0005000'], tr/val_loss:  0.655733/ 22.451424, val:  57.08%, val_best:  72.08%, tr:  75.08%, tr_best:  75.49%, epoch time: 43.21 seconds, 0.72 minutes\n",
      "epoch-76  lr=['0.0005000'], tr/val_loss:  0.683469/ 31.085155, val:  60.83%, val_best:  72.08%, tr:  73.85%, tr_best:  75.49%, epoch time: 43.01 seconds, 0.72 minutes\n",
      "epoch-77  lr=['0.0005000'], tr/val_loss:  0.687979/ 42.215534, val:  59.17%, val_best:  72.08%, tr:  75.08%, tr_best:  75.49%, epoch time: 43.70 seconds, 0.73 minutes\n",
      "epoch-78  lr=['0.0005000'], tr/val_loss:  0.673057/ 31.318327, val:  57.50%, val_best:  72.08%, tr:  76.20%, tr_best:  76.20%, epoch time: 43.16 seconds, 0.72 minutes\n",
      "epoch-79  lr=['0.0005000'], tr/val_loss:  0.624200/ 18.330675, val:  63.33%, val_best:  72.08%, tr:  77.32%, tr_best:  77.32%, epoch time: 43.24 seconds, 0.72 minutes\n",
      "epoch-80  lr=['0.0005000'], tr/val_loss:  0.667630/ 22.957462, val:  65.42%, val_best:  72.08%, tr:  75.38%, tr_best:  77.32%, epoch time: 43.61 seconds, 0.73 minutes\n",
      "epoch-81  lr=['0.0005000'], tr/val_loss:  0.644219/ 32.997112, val:  57.08%, val_best:  72.08%, tr:  75.18%, tr_best:  77.32%, epoch time: 43.40 seconds, 0.72 minutes\n",
      "epoch-82  lr=['0.0005000'], tr/val_loss:  0.674474/ 27.818312, val:  60.83%, val_best:  72.08%, tr:  74.97%, tr_best:  77.32%, epoch time: 43.61 seconds, 0.73 minutes\n",
      "epoch-83  lr=['0.0005000'], tr/val_loss:  0.661026/ 23.915524, val:  63.75%, val_best:  72.08%, tr:  75.18%, tr_best:  77.32%, epoch time: 43.46 seconds, 0.72 minutes\n",
      "epoch-84  lr=['0.0005000'], tr/val_loss:  0.720009/ 23.575916, val:  60.42%, val_best:  72.08%, tr:  74.77%, tr_best:  77.32%, epoch time: 43.44 seconds, 0.72 minutes\n",
      "epoch-85  lr=['0.0005000'], tr/val_loss:  0.648953/ 29.953728, val:  62.08%, val_best:  72.08%, tr:  75.89%, tr_best:  77.32%, epoch time: 43.75 seconds, 0.73 minutes\n",
      "epoch-86  lr=['0.0005000'], tr/val_loss:  0.639540/ 25.803528, val:  57.08%, val_best:  72.08%, tr:  76.30%, tr_best:  77.32%, epoch time: 43.38 seconds, 0.72 minutes\n",
      "epoch-87  lr=['0.0005000'], tr/val_loss:  0.630757/ 21.284540, val:  69.17%, val_best:  72.08%, tr:  77.63%, tr_best:  77.63%, epoch time: 43.57 seconds, 0.73 minutes\n",
      "epoch-88  lr=['0.0005000'], tr/val_loss:  0.674370/ 29.250996, val:  59.58%, val_best:  72.08%, tr:  75.38%, tr_best:  77.63%, epoch time: 43.18 seconds, 0.72 minutes\n",
      "epoch-89  lr=['0.0005000'], tr/val_loss:  0.715244/ 25.301359, val:  60.00%, val_best:  72.08%, tr:  73.03%, tr_best:  77.63%, epoch time: 43.17 seconds, 0.72 minutes\n",
      "epoch-90  lr=['0.0005000'], tr/val_loss:  0.685085/ 17.505756, val:  68.75%, val_best:  72.08%, tr:  75.59%, tr_best:  77.63%, epoch time: 42.80 seconds, 0.71 minutes\n",
      "epoch-91  lr=['0.0005000'], tr/val_loss:  0.613203/ 21.334915, val:  67.50%, val_best:  72.08%, tr:  77.94%, tr_best:  77.94%, epoch time: 43.46 seconds, 0.72 minutes\n",
      "epoch-92  lr=['0.0005000'], tr/val_loss:  0.602819/ 25.380749, val:  62.92%, val_best:  72.08%, tr:  76.92%, tr_best:  77.94%, epoch time: 42.86 seconds, 0.71 minutes\n",
      "epoch-93  lr=['0.0005000'], tr/val_loss:  0.554424/ 19.198780, val:  62.50%, val_best:  72.08%, tr:  79.88%, tr_best:  79.88%, epoch time: 43.51 seconds, 0.73 minutes\n",
      "epoch-94  lr=['0.0005000'], tr/val_loss:  0.633523/ 30.465052, val:  57.50%, val_best:  72.08%, tr:  77.02%, tr_best:  79.88%, epoch time: 43.70 seconds, 0.73 minutes\n",
      "epoch-95  lr=['0.0005000'], tr/val_loss:  0.598234/ 14.737295, val:  70.83%, val_best:  72.08%, tr:  77.43%, tr_best:  79.88%, epoch time: 44.11 seconds, 0.74 minutes\n",
      "epoch-96  lr=['0.0005000'], tr/val_loss:  0.611099/ 22.846174, val:  62.08%, val_best:  72.08%, tr:  77.43%, tr_best:  79.88%, epoch time: 44.94 seconds, 0.75 minutes\n",
      "epoch-97  lr=['0.0005000'], tr/val_loss:  0.596753/ 15.752152, val:  65.42%, val_best:  72.08%, tr:  79.78%, tr_best:  79.88%, epoch time: 46.35 seconds, 0.77 minutes\n",
      "epoch-98  lr=['0.0005000'], tr/val_loss:  0.589491/ 28.749474, val:  58.75%, val_best:  72.08%, tr:  79.16%, tr_best:  79.88%, epoch time: 46.21 seconds, 0.77 minutes\n",
      "epoch-99  lr=['0.0005000'], tr/val_loss:  0.621413/ 22.782131, val:  62.08%, val_best:  72.08%, tr:  76.71%, tr_best:  79.88%, epoch time: 44.70 seconds, 0.74 minutes\n",
      "epoch-100 lr=['0.0005000'], tr/val_loss:  0.651710/ 21.997852, val:  65.42%, val_best:  72.08%, tr:  75.69%, tr_best:  79.88%, epoch time: 45.09 seconds, 0.75 minutes\n",
      "epoch-101 lr=['0.0005000'], tr/val_loss:  0.620908/ 24.835350, val:  60.42%, val_best:  72.08%, tr:  76.20%, tr_best:  79.88%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "epoch-102 lr=['0.0005000'], tr/val_loss:  0.609570/ 29.656767, val:  58.75%, val_best:  72.08%, tr:  77.53%, tr_best:  79.88%, epoch time: 46.29 seconds, 0.77 minutes\n",
      "epoch-103 lr=['0.0005000'], tr/val_loss:  0.609225/ 22.506598, val:  61.67%, val_best:  72.08%, tr:  77.43%, tr_best:  79.88%, epoch time: 45.76 seconds, 0.76 minutes\n",
      "epoch-104 lr=['0.0005000'], tr/val_loss:  0.646680/ 25.418964, val:  66.25%, val_best:  72.08%, tr:  75.08%, tr_best:  79.88%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "epoch-105 lr=['0.0005000'], tr/val_loss:  0.582671/ 17.718945, val:  67.50%, val_best:  72.08%, tr:  78.86%, tr_best:  79.88%, epoch time: 46.53 seconds, 0.78 minutes\n",
      "epoch-106 lr=['0.0005000'], tr/val_loss:  0.610652/ 24.406466, val:  66.25%, val_best:  72.08%, tr:  76.92%, tr_best:  79.88%, epoch time: 45.58 seconds, 0.76 minutes\n",
      "epoch-107 lr=['0.0005000'], tr/val_loss:  0.596118/ 23.207783, val:  61.67%, val_best:  72.08%, tr:  78.14%, tr_best:  79.88%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "epoch-108 lr=['0.0005000'], tr/val_loss:  0.562529/ 21.207975, val:  63.33%, val_best:  72.08%, tr:  79.67%, tr_best:  79.88%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "epoch-109 lr=['0.0005000'], tr/val_loss:  0.557499/ 23.281546, val:  64.17%, val_best:  72.08%, tr:  78.65%, tr_best:  79.88%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "epoch-110 lr=['0.0005000'], tr/val_loss:  0.613427/ 33.226738, val:  65.42%, val_best:  72.08%, tr:  78.24%, tr_best:  79.88%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "epoch-111 lr=['0.0005000'], tr/val_loss:  0.566411/ 22.451601, val:  67.08%, val_best:  72.08%, tr:  78.45%, tr_best:  79.88%, epoch time: 45.51 seconds, 0.76 minutes\n",
      "epoch-112 lr=['0.0005000'], tr/val_loss:  0.591323/ 25.309118, val:  62.92%, val_best:  72.08%, tr:  77.83%, tr_best:  79.88%, epoch time: 45.39 seconds, 0.76 minutes\n",
      "epoch-113 lr=['0.0005000'], tr/val_loss:  0.588195/ 30.717901, val:  61.25%, val_best:  72.08%, tr:  78.65%, tr_best:  79.88%, epoch time: 45.42 seconds, 0.76 minutes\n",
      "epoch-114 lr=['0.0005000'], tr/val_loss:  0.596741/ 17.606846, val:  66.67%, val_best:  72.08%, tr:  77.94%, tr_best:  79.88%, epoch time: 44.47 seconds, 0.74 minutes\n",
      "epoch-115 lr=['0.0005000'], tr/val_loss:  0.580052/ 17.568068, val:  69.58%, val_best:  72.08%, tr:  79.06%, tr_best:  79.88%, epoch time: 45.54 seconds, 0.76 minutes\n",
      "epoch-116 lr=['0.0005000'], tr/val_loss:  0.596633/ 16.834309, val:  71.25%, val_best:  72.08%, tr:  77.73%, tr_best:  79.88%, epoch time: 46.28 seconds, 0.77 minutes\n",
      "epoch-117 lr=['0.0005000'], tr/val_loss:  0.578036/ 25.526733, val:  57.92%, val_best:  72.08%, tr:  78.86%, tr_best:  79.88%, epoch time: 44.05 seconds, 0.73 minutes\n",
      "epoch-118 lr=['0.0005000'], tr/val_loss:  0.513794/ 28.014696, val:  57.08%, val_best:  72.08%, tr:  81.61%, tr_best:  81.61%, epoch time: 45.18 seconds, 0.75 minutes\n",
      "epoch-119 lr=['0.0005000'], tr/val_loss:  0.555129/ 20.039566, val:  64.58%, val_best:  72.08%, tr:  78.65%, tr_best:  81.61%, epoch time: 45.72 seconds, 0.76 minutes\n",
      "epoch-120 lr=['0.0005000'], tr/val_loss:  0.575193/ 35.210171, val:  56.67%, val_best:  72.08%, tr:  79.47%, tr_best:  81.61%, epoch time: 45.32 seconds, 0.76 minutes\n",
      "epoch-121 lr=['0.0005000'], tr/val_loss:  0.586293/ 38.034466, val:  62.08%, val_best:  72.08%, tr:  77.73%, tr_best:  81.61%, epoch time: 45.48 seconds, 0.76 minutes\n",
      "epoch-122 lr=['0.0005000'], tr/val_loss:  0.586408/ 19.846735, val:  67.50%, val_best:  72.08%, tr:  77.53%, tr_best:  81.61%, epoch time: 46.27 seconds, 0.77 minutes\n",
      "epoch-123 lr=['0.0005000'], tr/val_loss:  0.565598/ 15.428339, val:  72.08%, val_best:  72.08%, tr:  77.12%, tr_best:  81.61%, epoch time: 45.48 seconds, 0.76 minutes\n",
      "epoch-124 lr=['0.0005000'], tr/val_loss:  0.578107/ 19.844227, val:  68.75%, val_best:  72.08%, tr:  77.12%, tr_best:  81.61%, epoch time: 45.34 seconds, 0.76 minutes\n",
      "epoch-125 lr=['0.0005000'], tr/val_loss:  0.592979/ 19.737940, val:  65.42%, val_best:  72.08%, tr:  77.63%, tr_best:  81.61%, epoch time: 44.05 seconds, 0.73 minutes\n",
      "epoch-126 lr=['0.0005000'], tr/val_loss:  0.538056/ 27.286676, val:  62.92%, val_best:  72.08%, tr:  80.59%, tr_best:  81.61%, epoch time: 45.00 seconds, 0.75 minutes\n",
      "epoch-127 lr=['0.0005000'], tr/val_loss:  0.548454/ 18.967163, val:  71.67%, val_best:  72.08%, tr:  78.75%, tr_best:  81.61%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "epoch-128 lr=['0.0005000'], tr/val_loss:  0.526462/ 22.305212, val:  65.42%, val_best:  72.08%, tr:  80.80%, tr_best:  81.61%, epoch time: 45.17 seconds, 0.75 minutes\n",
      "epoch-129 lr=['0.0005000'], tr/val_loss:  0.560003/ 28.172400, val:  61.67%, val_best:  72.08%, tr:  79.67%, tr_best:  81.61%, epoch time: 45.69 seconds, 0.76 minutes\n",
      "epoch-130 lr=['0.0005000'], tr/val_loss:  0.542095/ 35.348763, val:  63.33%, val_best:  72.08%, tr:  79.88%, tr_best:  81.61%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "epoch-131 lr=['0.0005000'], tr/val_loss:  0.568475/ 20.397564, val:  66.25%, val_best:  72.08%, tr:  78.86%, tr_best:  81.61%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "epoch-132 lr=['0.0005000'], tr/val_loss:  0.525219/ 19.098663, val:  67.92%, val_best:  72.08%, tr:  79.78%, tr_best:  81.61%, epoch time: 43.63 seconds, 0.73 minutes\n",
      "epoch-133 lr=['0.0005000'], tr/val_loss:  0.552070/ 20.169548, val:  66.25%, val_best:  72.08%, tr:  79.47%, tr_best:  81.61%, epoch time: 45.10 seconds, 0.75 minutes\n",
      "epoch-134 lr=['0.0005000'], tr/val_loss:  0.485248/ 19.176247, val:  70.00%, val_best:  72.08%, tr:  82.94%, tr_best:  82.94%, epoch time: 47.96 seconds, 0.80 minutes\n",
      "epoch-135 lr=['0.0005000'], tr/val_loss:  0.583807/ 26.418425, val:  64.17%, val_best:  72.08%, tr:  78.45%, tr_best:  82.94%, epoch time: 45.89 seconds, 0.76 minutes\n",
      "epoch-136 lr=['0.0005000'], tr/val_loss:  0.553759/ 20.974997, val:  67.08%, val_best:  72.08%, tr:  78.96%, tr_best:  82.94%, epoch time: 46.06 seconds, 0.77 minutes\n",
      "epoch-137 lr=['0.0005000'], tr/val_loss:  0.535542/ 28.991854, val:  63.33%, val_best:  72.08%, tr:  79.16%, tr_best:  82.94%, epoch time: 44.05 seconds, 0.73 minutes\n",
      "epoch-138 lr=['0.0005000'], tr/val_loss:  0.517187/ 19.498720, val:  64.58%, val_best:  72.08%, tr:  81.00%, tr_best:  82.94%, epoch time: 45.08 seconds, 0.75 minutes\n",
      "epoch-139 lr=['0.0005000'], tr/val_loss:  0.548695/ 18.049568, val:  69.17%, val_best:  72.08%, tr:  78.75%, tr_best:  82.94%, epoch time: 43.44 seconds, 0.72 minutes\n",
      "epoch-140 lr=['0.0005000'], tr/val_loss:  0.510948/ 18.220737, val:  68.33%, val_best:  72.08%, tr:  81.10%, tr_best:  82.94%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "epoch-141 lr=['0.0005000'], tr/val_loss:  0.554204/ 33.313595, val:  63.75%, val_best:  72.08%, tr:  80.18%, tr_best:  82.94%, epoch time: 45.12 seconds, 0.75 minutes\n",
      "epoch-142 lr=['0.0005000'], tr/val_loss:  0.559560/ 21.119436, val:  64.17%, val_best:  72.08%, tr:  77.94%, tr_best:  82.94%, epoch time: 43.69 seconds, 0.73 minutes\n",
      "epoch-143 lr=['0.0005000'], tr/val_loss:  0.507222/ 28.385040, val:  62.50%, val_best:  72.08%, tr:  80.49%, tr_best:  82.94%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "epoch-144 lr=['0.0005000'], tr/val_loss:  0.551621/ 22.926346, val:  65.42%, val_best:  72.08%, tr:  79.67%, tr_best:  82.94%, epoch time: 43.98 seconds, 0.73 minutes\n",
      "epoch-145 lr=['0.0005000'], tr/val_loss:  0.566397/ 28.643456, val:  64.58%, val_best:  72.08%, tr:  78.86%, tr_best:  82.94%, epoch time: 46.11 seconds, 0.77 minutes\n",
      "epoch-146 lr=['0.0005000'], tr/val_loss:  0.581667/ 26.153427, val:  60.83%, val_best:  72.08%, tr:  78.14%, tr_best:  82.94%, epoch time: 44.63 seconds, 0.74 minutes\n",
      "epoch-147 lr=['0.0005000'], tr/val_loss:  0.505000/ 26.925732, val:  65.83%, val_best:  72.08%, tr:  79.47%, tr_best:  82.94%, epoch time: 44.39 seconds, 0.74 minutes\n",
      "epoch-148 lr=['0.0005000'], tr/val_loss:  0.512958/ 16.354181, val:  72.08%, val_best:  72.08%, tr:  79.88%, tr_best:  82.94%, epoch time: 43.95 seconds, 0.73 minutes\n",
      "epoch-149 lr=['0.0005000'], tr/val_loss:  0.526896/ 16.538361, val:  70.83%, val_best:  72.08%, tr:  79.98%, tr_best:  82.94%, epoch time: 44.21 seconds, 0.74 minutes\n",
      "epoch-150 lr=['0.0005000'], tr/val_loss:  0.581261/ 15.975915, val:  70.83%, val_best:  72.08%, tr:  78.96%, tr_best:  82.94%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "epoch-151 lr=['0.0005000'], tr/val_loss:  0.541419/ 19.831482, val:  70.00%, val_best:  72.08%, tr:  78.55%, tr_best:  82.94%, epoch time: 44.87 seconds, 0.75 minutes\n",
      "epoch-152 lr=['0.0005000'], tr/val_loss:  0.491044/ 23.649553, val:  65.83%, val_best:  72.08%, tr:  81.51%, tr_best:  82.94%, epoch time: 45.36 seconds, 0.76 minutes\n",
      "epoch-153 lr=['0.0005000'], tr/val_loss:  0.529295/ 27.732697, val:  63.75%, val_best:  72.08%, tr:  79.26%, tr_best:  82.94%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "epoch-154 lr=['0.0005000'], tr/val_loss:  0.532187/ 22.587734, val:  68.33%, val_best:  72.08%, tr:  79.26%, tr_best:  82.94%, epoch time: 45.39 seconds, 0.76 minutes\n",
      "epoch-155 lr=['0.0005000'], tr/val_loss:  0.532856/ 28.751654, val:  61.67%, val_best:  72.08%, tr:  80.80%, tr_best:  82.94%, epoch time: 44.97 seconds, 0.75 minutes\n",
      "epoch-156 lr=['0.0005000'], tr/val_loss:  0.540489/ 16.712181, val:  72.08%, val_best:  72.08%, tr:  78.35%, tr_best:  82.94%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "epoch-157 lr=['0.0005000'], tr/val_loss:  0.441348/ 30.615034, val:  62.92%, val_best:  72.08%, tr:  82.64%, tr_best:  82.94%, epoch time: 46.10 seconds, 0.77 minutes\n",
      "epoch-158 lr=['0.0005000'], tr/val_loss:  0.500791/ 21.965551, val:  69.17%, val_best:  72.08%, tr:  81.31%, tr_best:  82.94%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "epoch-159 lr=['0.0005000'], tr/val_loss:  0.492489/ 15.274106, val:  73.75%, val_best:  73.75%, tr:  82.64%, tr_best:  82.94%, epoch time: 43.98 seconds, 0.73 minutes\n",
      "epoch-160 lr=['0.0005000'], tr/val_loss:  0.468626/ 27.608952, val:  63.75%, val_best:  73.75%, tr:  82.02%, tr_best:  82.94%, epoch time: 44.54 seconds, 0.74 minutes\n",
      "epoch-161 lr=['0.0005000'], tr/val_loss:  0.496102/ 20.921099, val:  66.25%, val_best:  73.75%, tr:  81.21%, tr_best:  82.94%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "epoch-162 lr=['0.0005000'], tr/val_loss:  0.532429/ 21.800310, val:  67.08%, val_best:  73.75%, tr:  80.90%, tr_best:  82.94%, epoch time: 45.18 seconds, 0.75 minutes\n",
      "epoch-163 lr=['0.0005000'], tr/val_loss:  0.473165/ 29.587109, val:  61.25%, val_best:  73.75%, tr:  82.84%, tr_best:  82.94%, epoch time: 44.21 seconds, 0.74 minutes\n",
      "epoch-164 lr=['0.0005000'], tr/val_loss:  0.491596/ 32.417484, val:  64.58%, val_best:  73.75%, tr:  81.51%, tr_best:  82.94%, epoch time: 45.38 seconds, 0.76 minutes\n",
      "epoch-165 lr=['0.0005000'], tr/val_loss:  0.516475/ 26.935753, val:  63.33%, val_best:  73.75%, tr:  81.00%, tr_best:  82.94%, epoch time: 45.11 seconds, 0.75 minutes\n",
      "epoch-166 lr=['0.0005000'], tr/val_loss:  0.438875/ 23.068460, val:  64.58%, val_best:  73.75%, tr:  82.94%, tr_best:  82.94%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "epoch-167 lr=['0.0005000'], tr/val_loss:  0.504154/ 27.286594, val:  62.08%, val_best:  73.75%, tr:  81.82%, tr_best:  82.94%, epoch time: 44.84 seconds, 0.75 minutes\n",
      "epoch-168 lr=['0.0005000'], tr/val_loss:  0.443493/ 22.972862, val:  68.75%, val_best:  73.75%, tr:  83.76%, tr_best:  83.76%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "epoch-169 lr=['0.0005000'], tr/val_loss:  0.531040/ 22.677937, val:  63.75%, val_best:  73.75%, tr:  81.00%, tr_best:  83.76%, epoch time: 45.88 seconds, 0.76 minutes\n",
      "epoch-170 lr=['0.0005000'], tr/val_loss:  0.460145/ 35.025280, val:  60.00%, val_best:  73.75%, tr:  80.59%, tr_best:  83.76%, epoch time: 43.85 seconds, 0.73 minutes\n",
      "epoch-171 lr=['0.0005000'], tr/val_loss:  0.464336/ 31.577883, val:  61.67%, val_best:  73.75%, tr:  81.00%, tr_best:  83.76%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "epoch-172 lr=['0.0005000'], tr/val_loss:  0.495886/ 24.197384, val:  66.67%, val_best:  73.75%, tr:  81.10%, tr_best:  83.76%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "epoch-173 lr=['0.0005000'], tr/val_loss:  0.523242/ 27.562105, val:  65.42%, val_best:  73.75%, tr:  80.80%, tr_best:  83.76%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "epoch-174 lr=['0.0005000'], tr/val_loss:  0.583927/ 22.423122, val:  65.00%, val_best:  73.75%, tr:  78.65%, tr_best:  83.76%, epoch time: 46.39 seconds, 0.77 minutes\n",
      "epoch-175 lr=['0.0005000'], tr/val_loss:  0.520511/ 20.811346, val:  70.83%, val_best:  73.75%, tr:  80.39%, tr_best:  83.76%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "epoch-176 lr=['0.0005000'], tr/val_loss:  0.455825/ 23.752649, val:  67.50%, val_best:  73.75%, tr:  83.25%, tr_best:  83.76%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "epoch-177 lr=['0.0005000'], tr/val_loss:  0.452313/ 19.608175, val:  69.58%, val_best:  73.75%, tr:  82.12%, tr_best:  83.76%, epoch time: 44.59 seconds, 0.74 minutes\n",
      "epoch-178 lr=['0.0005000'], tr/val_loss:  0.454960/ 26.380234, val:  63.75%, val_best:  73.75%, tr:  82.64%, tr_best:  83.76%, epoch time: 44.25 seconds, 0.74 minutes\n",
      "epoch-179 lr=['0.0005000'], tr/val_loss:  0.434984/ 18.982853, val:  70.42%, val_best:  73.75%, tr:  82.94%, tr_best:  83.76%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "epoch-180 lr=['0.0005000'], tr/val_loss:  0.473635/ 28.176973, val:  65.00%, val_best:  73.75%, tr:  81.41%, tr_best:  83.76%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "epoch-181 lr=['0.0005000'], tr/val_loss:  0.461092/ 26.082321, val:  64.17%, val_best:  73.75%, tr:  82.94%, tr_best:  83.76%, epoch time: 45.05 seconds, 0.75 minutes\n",
      "epoch-182 lr=['0.0005000'], tr/val_loss:  0.492501/ 23.027822, val:  65.83%, val_best:  73.75%, tr:  81.61%, tr_best:  83.76%, epoch time: 45.25 seconds, 0.75 minutes\n",
      "epoch-183 lr=['0.0005000'], tr/val_loss:  0.434815/ 28.418505, val:  64.58%, val_best:  73.75%, tr:  82.84%, tr_best:  83.76%, epoch time: 44.08 seconds, 0.73 minutes\n",
      "epoch-184 lr=['0.0005000'], tr/val_loss:  0.437948/ 22.742310, val:  66.25%, val_best:  73.75%, tr:  84.58%, tr_best:  84.58%, epoch time: 45.27 seconds, 0.75 minutes\n",
      "epoch-185 lr=['0.0005000'], tr/val_loss:  0.480631/ 25.850536, val:  66.25%, val_best:  73.75%, tr:  81.61%, tr_best:  84.58%, epoch time: 45.07 seconds, 0.75 minutes\n",
      "epoch-186 lr=['0.0005000'], tr/val_loss:  0.475818/ 24.764301, val:  63.33%, val_best:  73.75%, tr:  81.41%, tr_best:  84.58%, epoch time: 45.60 seconds, 0.76 minutes\n",
      "epoch-187 lr=['0.0005000'], tr/val_loss:  0.481608/ 22.863558, val:  66.67%, val_best:  73.75%, tr:  81.51%, tr_best:  84.58%, epoch time: 46.33 seconds, 0.77 minutes\n",
      "epoch-188 lr=['0.0005000'], tr/val_loss:  0.454230/ 22.836157, val:  66.25%, val_best:  73.75%, tr:  83.55%, tr_best:  84.58%, epoch time: 43.87 seconds, 0.73 minutes\n",
      "epoch-189 lr=['0.0005000'], tr/val_loss:  0.433647/ 23.461559, val:  67.50%, val_best:  73.75%, tr:  83.55%, tr_best:  84.58%, epoch time: 43.75 seconds, 0.73 minutes\n",
      "epoch-190 lr=['0.0005000'], tr/val_loss:  0.444631/ 25.644180, val:  67.92%, val_best:  73.75%, tr:  83.86%, tr_best:  84.58%, epoch time: 45.53 seconds, 0.76 minutes\n",
      "epoch-191 lr=['0.0005000'], tr/val_loss:  0.508804/ 41.508804, val:  54.58%, val_best:  73.75%, tr:  81.92%, tr_best:  84.58%, epoch time: 43.99 seconds, 0.73 minutes\n",
      "epoch-192 lr=['0.0005000'], tr/val_loss:  0.491066/ 27.233574, val:  61.25%, val_best:  73.75%, tr:  83.15%, tr_best:  84.58%, epoch time: 45.99 seconds, 0.77 minutes\n",
      "epoch-193 lr=['0.0005000'], tr/val_loss:  0.441286/ 29.635321, val:  62.50%, val_best:  73.75%, tr:  83.76%, tr_best:  84.58%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "epoch-194 lr=['0.0005000'], tr/val_loss:  0.479455/ 24.153250, val:  67.50%, val_best:  73.75%, tr:  81.92%, tr_best:  84.58%, epoch time: 45.03 seconds, 0.75 minutes\n",
      "epoch-195 lr=['0.0005000'], tr/val_loss:  0.472427/ 31.040464, val:  64.17%, val_best:  73.75%, tr:  81.72%, tr_best:  84.58%, epoch time: 43.40 seconds, 0.72 minutes\n",
      "epoch-196 lr=['0.0005000'], tr/val_loss:  0.508892/ 29.567015, val:  63.33%, val_best:  73.75%, tr:  81.00%, tr_best:  84.58%, epoch time: 46.57 seconds, 0.78 minutes\n",
      "epoch-197 lr=['0.0005000'], tr/val_loss:  0.426334/ 31.227459, val:  60.83%, val_best:  73.75%, tr:  82.94%, tr_best:  84.58%, epoch time: 44.59 seconds, 0.74 minutes\n",
      "epoch-198 lr=['0.0005000'], tr/val_loss:  0.435078/ 24.893408, val:  64.17%, val_best:  73.75%, tr:  83.96%, tr_best:  84.58%, epoch time: 43.93 seconds, 0.73 minutes\n",
      "epoch-199 lr=['0.0005000'], tr/val_loss:  0.476781/ 34.924397, val:  60.42%, val_best:  73.75%, tr:  82.02%, tr_best:  84.58%, epoch time: 44.77 seconds, 0.75 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e142f8df6944618ca62d5618d3a49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÜ‚ñÑ‚ñÉ‚ñà‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÜ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.82022</td></tr><tr><td>tr_epoch_loss</td><td>0.47678</td></tr><tr><td>val_acc_best</td><td>0.7375</td></tr><tr><td>val_acc_now</td><td>0.60417</td></tr><tr><td>val_loss</td><td>34.9244</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lilac-sweep-279</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7754jft0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7754jft0</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251111_011902-7754jft0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8wh4p60e with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [512]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 5000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_threshold: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: one\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251111_034726-8wh4p60e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8wh4p60e' target=\"_blank\">dark-sweep-286</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8wh4p60e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8wh4p60e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_threshold' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': '20251111_034734_581', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.75, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 0.25, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.75, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [512], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'one', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'output_threshold': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 7a22c8a0ef5b9b252dbf98632e270efd\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=512, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.0625, v_reset=10000, sg_width=0.25, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=False, ANPI_MODE=True)\n",
      "      (4): SYNAPSE_FC(in_features=512, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient(single_step=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 506,880\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "inFeed spike.shape torch.Size([10, 512]) self.weight_fb.shape torch.Size([10, 512])\n",
      "self.weight_fb[0] tensor([ 1.2009e-02,  1.3379e-01, -1.0650e-02,  5.2556e-02, -1.1912e-01,\n",
      "         4.0419e-02, -4.0199e-02, -5.0604e-02,  3.2680e-02, -7.8942e-02,\n",
      "        -1.0288e-01, -1.8775e-02, -5.7299e-03,  1.2332e-02, -6.9353e-02,\n",
      "         1.1499e-01, -4.4228e-02,  4.2593e-02,  4.9323e-02, -2.0675e-03,\n",
      "         9.2336e-02, -3.1971e-02, -1.5728e-02,  9.1276e-02, -2.0181e-02,\n",
      "        -7.1800e-02,  1.4578e-01, -4.2861e-02,  1.1373e-02, -7.3257e-02,\n",
      "        -1.1159e-01, -9.7846e-02,  5.1912e-02,  8.7845e-02,  4.0044e-02,\n",
      "         2.6324e-02, -9.8372e-02,  3.8522e-02,  1.0460e-01, -4.1150e-02,\n",
      "         5.8342e-02,  4.8482e-03,  5.2401e-03, -8.7172e-03,  2.0523e-02,\n",
      "        -3.6457e-02, -6.6373e-02,  5.9048e-03, -2.0717e-02, -3.2546e-02,\n",
      "        -5.4324e-02,  2.4378e-02,  1.0149e-02, -1.2236e-02,  6.2543e-02,\n",
      "        -8.3454e-02, -2.1650e-02, -3.9879e-02,  2.7655e-02, -3.3246e-02,\n",
      "         7.6898e-02, -5.0422e-02,  1.5484e-02, -2.6447e-02,  6.8359e-02,\n",
      "        -6.8262e-02,  3.4312e-02, -7.9518e-02, -2.3619e-02,  3.1812e-02,\n",
      "         6.2016e-03,  1.6009e-02,  2.2387e-02,  1.4105e-01,  1.4450e-03,\n",
      "         9.7970e-02, -7.1751e-02,  5.8704e-02, -2.8309e-02,  4.7077e-02,\n",
      "        -3.5820e-02, -4.3640e-02, -4.4777e-02, -3.1386e-02, -2.7226e-02,\n",
      "        -2.5884e-02,  1.0779e-02,  2.7401e-02,  3.1376e-02, -7.5319e-02,\n",
      "        -1.6829e-02,  1.7118e-02, -8.9122e-02, -4.0006e-02,  4.6343e-03,\n",
      "         1.2001e-02,  3.6892e-02,  1.4373e-02,  7.0655e-02, -4.2197e-02,\n",
      "        -1.0233e-01,  3.7360e-04,  8.5512e-02,  7.8637e-02,  1.4384e-03,\n",
      "        -8.0477e-02, -4.6482e-02,  2.3251e-02, -3.3886e-02, -2.4537e-03,\n",
      "        -4.8149e-02, -1.5486e-01,  4.3330e-02, -5.8045e-03, -1.3386e-02,\n",
      "         2.7755e-02, -1.9510e-02,  1.3393e-03,  3.8708e-02,  1.5263e-02,\n",
      "         4.6335e-02, -7.2374e-03, -6.3238e-03, -3.1016e-02, -3.1252e-02,\n",
      "        -7.4723e-02, -1.5088e-02, -4.1994e-02,  1.2212e-02,  6.0550e-02,\n",
      "        -1.7745e-03,  1.0415e-01,  6.7522e-02, -6.1409e-02, -4.1550e-02,\n",
      "         1.0644e-01,  1.5230e-01, -3.8367e-02,  7.8697e-02, -1.7323e-02,\n",
      "         2.6986e-02,  2.6370e-02,  6.5894e-02, -1.2553e-01, -3.9156e-02,\n",
      "         1.3065e-01, -5.8646e-03,  1.4600e-02, -4.5190e-02, -1.0434e-01,\n",
      "         5.6415e-02,  4.8810e-02, -3.8917e-02,  1.3367e-01,  7.2065e-02,\n",
      "        -2.6348e-02,  1.4814e-02, -7.9086e-02, -7.4679e-03, -3.7547e-02,\n",
      "        -4.9995e-02,  1.3292e-04, -1.2034e-02,  4.6384e-02,  5.0249e-02,\n",
      "         5.1038e-02, -3.7747e-02,  8.0393e-02, -6.6428e-02, -1.4425e-03,\n",
      "        -2.2637e-02, -3.0118e-02,  9.2677e-03, -9.3434e-02,  1.9207e-02,\n",
      "        -2.7770e-02, -6.7883e-02, -7.8605e-02, -9.7644e-02, -9.8327e-02,\n",
      "        -4.0612e-02,  4.7043e-02, -3.7591e-02,  1.8712e-02, -8.3181e-02,\n",
      "        -1.9715e-02,  3.6721e-02,  3.5419e-02, -4.6781e-02, -7.8367e-03,\n",
      "        -2.6748e-02, -8.6308e-02,  2.3989e-02, -1.2710e-02,  3.7118e-02,\n",
      "        -6.2088e-02, -2.2962e-04, -4.9640e-02,  2.4384e-02,  1.5691e-01,\n",
      "         1.5421e-02,  5.5528e-02,  4.8312e-02,  5.6640e-02, -2.2735e-02,\n",
      "         5.3113e-03, -5.2211e-02,  2.6325e-02,  6.9295e-02,  2.4738e-02,\n",
      "        -5.3518e-03,  5.2276e-02, -2.4634e-02, -5.3242e-03,  1.2084e-01,\n",
      "        -2.6133e-02,  3.3964e-02,  9.2582e-03, -1.2223e-01, -2.1360e-03,\n",
      "        -7.8244e-02, -1.5748e-02,  1.4439e-03,  1.2431e-01,  6.0634e-02,\n",
      "         8.5934e-02, -6.0989e-02, -2.9897e-02, -1.1970e-03, -1.0762e-01,\n",
      "         1.0423e-02,  1.6176e-02, -1.3812e-02, -5.2755e-02,  1.6920e-02,\n",
      "         6.1367e-02,  9.1813e-02,  2.1540e-02,  7.7856e-03, -4.0828e-02,\n",
      "        -9.7598e-02, -4.1089e-02,  9.0935e-02,  1.8519e-02, -3.4424e-02,\n",
      "         2.8530e-03, -6.6620e-02, -8.9594e-03, -6.7013e-03, -4.6130e-02,\n",
      "        -2.1535e-02,  5.8145e-03,  4.0000e-03, -5.7107e-02,  4.8855e-02,\n",
      "        -1.1148e-01, -1.1978e-01,  6.8131e-02,  1.5512e-03,  3.5912e-02,\n",
      "         3.3328e-02,  3.1726e-02, -8.8611e-02,  1.4725e-01, -9.5569e-02,\n",
      "        -1.0785e-02, -1.3891e-03,  1.3467e-02,  4.0348e-02,  9.6515e-02,\n",
      "         1.6649e-02,  3.0992e-02, -1.5092e-02, -5.3478e-02,  2.6478e-02,\n",
      "        -1.3042e-02, -9.5301e-02, -6.6575e-03, -1.5733e-03, -9.9895e-03,\n",
      "         3.4082e-02,  1.5740e-01, -9.9586e-03, -5.3744e-02,  8.7394e-02,\n",
      "         4.2685e-02,  5.2481e-02,  1.7623e-02,  1.0548e-03,  4.5100e-02,\n",
      "         7.4265e-02, -7.1658e-03, -8.7438e-02, -3.9754e-02,  5.4727e-02,\n",
      "         4.6412e-02,  4.2058e-02, -3.2855e-02, -1.1088e-01, -1.7722e-02,\n",
      "         4.9851e-03, -8.0476e-02,  8.2968e-02, -8.2024e-02,  1.6164e-02,\n",
      "         3.7377e-02, -9.2349e-02, -1.1127e-01,  6.9750e-02,  8.6820e-02,\n",
      "        -2.7057e-02, -2.3069e-02, -7.3103e-02, -1.6484e-01, -2.0014e-02,\n",
      "         6.3153e-03,  7.7782e-02, -8.4823e-02,  2.2121e-02,  1.0625e-01,\n",
      "        -1.4292e-01,  8.1527e-02, -7.1087e-02, -8.0429e-02, -4.0732e-03,\n",
      "         6.4006e-02, -1.4278e-01, -7.9276e-03,  5.2838e-02, -3.7510e-03,\n",
      "        -5.9070e-02, -1.1084e-01, -1.6297e-03,  5.6736e-03, -7.3166e-02,\n",
      "        -6.8036e-02,  1.5117e-01,  1.9150e-02, -9.3975e-02, -4.8127e-02,\n",
      "         4.4899e-02,  5.5049e-02,  6.3477e-02,  5.0466e-02,  1.4346e-01,\n",
      "        -1.4061e-02,  1.8790e-01,  3.4009e-02,  1.4160e-03, -2.5282e-02,\n",
      "        -1.6245e-02,  5.4068e-02, -7.5012e-02, -7.5148e-02, -1.8582e-02,\n",
      "        -2.3466e-02,  1.9578e-02, -6.2413e-02,  1.2314e-01,  1.3701e-02,\n",
      "        -5.7122e-03,  8.9041e-02,  3.7946e-02,  4.1243e-02,  4.7171e-02,\n",
      "         2.7039e-02, -5.9925e-03, -2.8245e-02, -7.2878e-02,  1.4521e-02,\n",
      "         9.9702e-02,  6.4296e-02,  7.4185e-02, -7.1993e-02,  1.4546e-02,\n",
      "         7.7495e-02, -9.2409e-03, -3.8808e-02,  7.1566e-02, -1.4977e-01,\n",
      "         4.2293e-02, -4.2540e-02, -5.6876e-03, -4.4148e-02, -8.0183e-02,\n",
      "         7.5278e-02, -2.9656e-03, -4.9337e-02,  2.6277e-02, -1.1994e-02,\n",
      "        -9.6900e-03, -8.8157e-03, -1.7625e-02, -8.9690e-02, -3.2884e-02,\n",
      "        -5.1021e-03, -1.0199e-01, -1.6831e-02,  1.1726e-01, -3.4447e-02,\n",
      "        -2.8511e-02, -1.9198e-02,  3.6576e-03,  3.2099e-02,  4.5579e-03,\n",
      "         8.7041e-02, -3.0138e-02,  1.8212e-02,  7.4119e-02, -1.3839e-02,\n",
      "         5.3415e-02,  2.2786e-02,  1.0557e-01, -5.6927e-02,  3.3285e-02,\n",
      "         7.3276e-02,  1.0244e-01, -1.4565e-02, -1.0259e-01,  1.2200e-01,\n",
      "         6.1812e-02,  4.8889e-02, -5.6486e-02,  5.1047e-02,  9.3909e-02,\n",
      "        -1.0201e-02,  6.4712e-02, -2.3649e-02,  3.8729e-02,  6.1245e-03,\n",
      "        -4.3430e-02,  6.4039e-03, -8.9212e-02,  1.5119e-01,  7.2071e-02,\n",
      "         1.5732e-02, -2.2774e-02,  5.2327e-02,  2.5401e-02,  2.9843e-02,\n",
      "        -1.1558e-01,  5.9937e-02, -5.8328e-02,  7.1370e-02,  4.9816e-02,\n",
      "         6.5657e-02,  3.2430e-02, -8.6861e-03,  8.5977e-02,  1.9082e-02,\n",
      "         2.7206e-02, -1.9106e-03, -6.5907e-02,  4.0442e-03,  1.7387e-02,\n",
      "         1.3066e-01, -8.5428e-02, -2.6442e-02,  5.6974e-02, -8.7909e-02,\n",
      "         3.4048e-02, -5.8666e-02,  1.8037e-02, -6.2223e-02, -1.8848e-02,\n",
      "         9.5296e-03, -5.1592e-03,  5.1242e-03,  9.5190e-02,  1.1389e-02,\n",
      "        -6.1644e-02,  2.7198e-02,  2.2262e-02, -4.7755e-02,  6.3539e-03,\n",
      "        -2.4203e-02,  1.3476e-02,  5.5816e-02,  3.3884e-02,  5.4144e-02,\n",
      "        -2.0123e-02, -2.5729e-02,  3.2092e-02, -3.4289e-02, -1.2439e-03,\n",
      "         1.8775e-01,  5.8437e-02,  1.8716e-02, -5.8857e-02, -6.8036e-02,\n",
      "        -5.9856e-04,  1.0747e-01, -7.1370e-02,  1.3296e-03, -3.0167e-02,\n",
      "        -5.6810e-02, -1.0447e-01, -8.7226e-03, -3.1270e-03,  1.2601e-02,\n",
      "         1.8155e-02, -9.4597e-02, -4.7340e-02,  2.7440e-02, -3.4883e-02,\n",
      "        -3.2968e-02, -6.2905e-02, -1.2657e-02,  3.2411e-02,  1.2026e-02,\n",
      "         2.2878e-02, -5.3231e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 6.6658e-02, -7.8302e-02, -3.9761e-02, -4.1793e-02,  4.5831e-02,\n",
      "         4.8306e-02, -6.7736e-03,  7.5574e-02, -7.4495e-02, -3.0042e-02,\n",
      "         5.2244e-03, -1.3071e-02, -5.5794e-03, -8.3971e-02, -6.9471e-03,\n",
      "        -2.4258e-02,  1.0854e-01, -6.1369e-02, -1.4674e-01,  1.1226e-01,\n",
      "        -6.0065e-02,  5.3451e-02,  1.1262e-01, -4.9005e-03,  1.5264e-01,\n",
      "         7.8240e-02,  3.1867e-02,  7.0535e-03, -8.8613e-02, -1.6180e-02,\n",
      "         7.1920e-03,  3.6067e-02, -1.8580e-02, -6.9305e-02,  5.7444e-02,\n",
      "        -9.3223e-02,  6.4325e-02, -1.2735e-01, -1.6280e-02, -5.1730e-02,\n",
      "        -1.6762e-02,  1.6986e-01,  2.8526e-02,  7.5887e-02,  4.1897e-03,\n",
      "         5.6685e-02,  4.6633e-02, -3.6862e-02, -3.9126e-02, -2.2331e-02,\n",
      "         9.3762e-02, -1.0613e-02,  1.1766e-01, -3.7826e-02,  6.4190e-02,\n",
      "         2.1247e-02, -9.1414e-03,  9.0567e-02, -1.1170e-01,  1.5015e-02,\n",
      "        -1.6912e-02,  1.8269e-02, -6.4949e-02, -5.4902e-02, -8.6944e-03,\n",
      "         1.3896e-01,  1.1010e-01,  1.0749e-02,  8.7195e-02, -6.8369e-03,\n",
      "        -3.5939e-02,  1.3870e-02,  5.9698e-02, -8.9737e-05,  8.3753e-02,\n",
      "        -4.8358e-03, -3.8847e-02, -1.0107e-01,  7.5683e-02, -1.1180e-01,\n",
      "         3.0140e-02, -4.3089e-02, -2.2418e-02, -3.6128e-02, -1.0527e-01,\n",
      "         2.2898e-02,  4.6009e-02, -7.4225e-03, -5.6874e-02,  8.5350e-02,\n",
      "         5.1923e-03,  2.5627e-02, -8.9285e-03, -5.8058e-02,  7.0525e-02,\n",
      "         3.8854e-02,  2.7697e-02,  1.4393e-01, -4.0282e-02,  2.0928e-02,\n",
      "        -2.4592e-02,  6.1504e-02,  8.4973e-02, -6.5030e-03, -1.1406e-02,\n",
      "        -1.5721e-01, -1.2213e-01, -3.2998e-02, -1.0606e-02,  1.5931e-01,\n",
      "         1.4261e-01,  2.5770e-02, -4.0473e-02, -6.6654e-02,  3.4934e-02,\n",
      "         9.9253e-02, -1.0173e-02, -1.4505e-02,  6.1864e-02,  4.7759e-02,\n",
      "        -1.6578e-02,  3.0713e-02,  1.4806e-02,  8.6155e-02, -1.2338e-02,\n",
      "         7.9021e-02, -7.8331e-02, -6.0098e-02,  7.8730e-02,  2.3303e-02,\n",
      "        -8.3858e-03,  4.4462e-02, -5.4935e-02,  4.2922e-02,  4.7366e-02,\n",
      "        -3.2290e-04,  1.8469e-02, -5.9237e-02,  6.0935e-02,  2.3421e-02,\n",
      "         7.0576e-02, -1.8194e-02,  5.7329e-03,  1.2694e-01, -1.6639e-02,\n",
      "         5.9829e-02, -7.5157e-02, -6.8489e-02, -1.1888e-01, -1.4575e-01,\n",
      "        -6.2740e-03,  8.6623e-02, -1.9370e-03, -1.2883e-01,  4.0742e-02,\n",
      "        -3.1368e-02, -6.8863e-03,  6.7565e-03, -5.5464e-02, -5.8365e-02,\n",
      "        -4.6925e-02, -1.8427e-03, -6.9821e-03, -5.4991e-02,  1.4936e-02,\n",
      "        -6.0094e-02,  2.1199e-02,  1.6101e-03, -6.6419e-02, -1.0129e-01,\n",
      "         3.2519e-04, -9.6969e-02,  2.2424e-02,  8.3956e-02, -1.0915e-01,\n",
      "        -5.2411e-02,  7.9012e-02,  7.7652e-02,  7.2692e-02,  5.3036e-02,\n",
      "         8.0605e-03,  1.2090e-01,  4.4321e-02, -1.3145e-02,  2.7608e-02,\n",
      "        -2.4626e-03, -8.6162e-02, -2.0906e-02, -8.0314e-02,  8.6478e-02,\n",
      "         3.2060e-02, -7.4949e-02, -4.5875e-02, -9.1144e-02,  8.5149e-02,\n",
      "         4.7841e-02, -5.8479e-02,  9.3823e-02, -8.9949e-02, -2.2137e-03,\n",
      "         5.3320e-02,  2.4241e-02,  7.6287e-02, -7.3501e-02,  5.9457e-02,\n",
      "         2.5991e-02, -4.9862e-02,  2.1058e-02,  3.7085e-02,  5.8227e-02,\n",
      "         1.6736e-02,  1.3518e-02, -3.6454e-02,  8.9511e-02, -6.0161e-02,\n",
      "         4.3647e-02,  2.5404e-02,  1.6810e-03, -3.8325e-02,  5.1655e-02,\n",
      "        -6.2435e-03, -7.4342e-02,  1.5280e-02, -3.8896e-02, -4.6945e-02,\n",
      "        -4.9156e-02,  5.0480e-02, -1.1144e-01,  4.6365e-02,  4.1312e-02,\n",
      "         4.3370e-02, -6.4439e-02,  1.4321e-01,  5.6491e-03,  4.6217e-02,\n",
      "        -7.8084e-02,  2.2043e-02,  2.4072e-02, -1.1090e-01, -5.7180e-02,\n",
      "         1.3553e-01,  2.0576e-03, -6.7463e-02, -3.7952e-02,  9.7044e-02,\n",
      "         3.9006e-02,  2.3112e-02,  3.6162e-02, -4.4879e-02, -5.0205e-02,\n",
      "        -6.6276e-02,  6.0393e-02, -1.6587e-02, -4.2223e-02,  4.9360e-02,\n",
      "        -5.2514e-02,  5.3070e-02,  3.0898e-02,  8.4096e-03,  4.2029e-02,\n",
      "         8.3128e-03,  7.7944e-02,  7.4944e-02,  3.7365e-02, -1.7412e-02,\n",
      "        -1.7034e-02, -5.1705e-02, -1.0178e-01,  8.1377e-03, -1.1124e-02,\n",
      "         6.0315e-02, -1.2464e-01, -8.2909e-02, -2.0721e-02,  1.5134e-01,\n",
      "        -7.6029e-03, -5.5703e-02,  1.3161e-01,  1.1009e-01,  8.7843e-02,\n",
      "        -1.1565e-02, -7.0188e-02, -1.7204e-01,  9.7961e-02,  1.4806e-01,\n",
      "        -4.5438e-02, -2.6664e-03, -4.6997e-02, -7.0638e-02, -7.9939e-02,\n",
      "        -7.0988e-02, -1.1400e-01, -7.8130e-03, -8.5862e-02, -3.9800e-02,\n",
      "         7.1482e-03, -1.3455e-01, -2.8474e-02, -8.3467e-02,  6.1789e-02,\n",
      "        -1.2440e-02, -1.4384e-01, -5.4934e-02,  1.7171e-02, -4.3710e-02,\n",
      "         5.2462e-03, -9.8457e-02,  6.4931e-02,  3.0336e-02, -8.2045e-03,\n",
      "        -2.1457e-02,  1.9863e-02, -3.9212e-02,  3.6250e-02, -2.9250e-02,\n",
      "         4.0146e-03,  9.8803e-02, -3.5044e-03, -1.3867e-01,  6.7823e-02,\n",
      "        -1.1386e-02,  4.5815e-02, -4.6995e-02, -6.0331e-02,  8.9048e-02,\n",
      "        -3.3910e-03,  5.5142e-02,  1.0962e-01,  7.8482e-02, -5.7451e-02,\n",
      "         6.7650e-02, -5.0193e-02, -1.0531e-01,  3.0873e-02,  4.0250e-02,\n",
      "         3.5226e-02,  3.5651e-02, -1.3163e-02, -1.5697e-02, -1.3301e-02,\n",
      "        -7.5622e-02,  4.6634e-02, -6.0863e-02,  1.1601e-02,  5.8555e-02,\n",
      "         1.9718e-02,  1.4490e-02,  4.6890e-02,  1.9770e-02,  1.8599e-02,\n",
      "         1.5324e-02,  9.0858e-02, -9.4841e-02,  4.4712e-02,  1.0196e-01,\n",
      "         7.1711e-02,  2.8857e-02, -7.6147e-02,  1.1056e-01,  3.8540e-02,\n",
      "        -7.5464e-02, -1.1109e-01,  1.1038e-02,  7.1191e-02,  3.8999e-02,\n",
      "         8.1577e-02,  1.4265e-01, -2.5305e-02,  7.0406e-02, -2.0950e-01,\n",
      "        -1.0905e-01, -7.9404e-02,  9.4908e-02, -6.2777e-02, -4.6448e-02,\n",
      "         6.7760e-02, -4.1111e-02, -3.0499e-02, -6.7737e-02, -1.6252e-02,\n",
      "         7.7219e-02, -9.5822e-02,  7.5935e-03, -2.3492e-02, -3.9966e-02,\n",
      "         2.2348e-02, -5.5910e-02, -2.2430e-02, -1.2789e-01,  1.1506e-02,\n",
      "        -3.6499e-02, -2.3789e-02,  8.8967e-02,  3.7748e-04,  1.4302e-01,\n",
      "        -3.3631e-02, -3.5510e-02, -1.5043e-01,  7.7718e-02,  1.4879e-01,\n",
      "         6.6394e-02, -1.8917e-02,  1.0423e-02, -4.4962e-03, -2.3098e-02,\n",
      "         8.4583e-02,  1.2187e-01,  2.5955e-02,  2.3483e-02, -1.2860e-01,\n",
      "         2.7167e-02,  3.6408e-02,  8.3306e-02,  1.1587e-01,  6.6651e-02,\n",
      "         5.9024e-02,  1.0206e-01, -6.6102e-02, -1.1416e-02,  6.7382e-02,\n",
      "        -1.8530e-01,  7.1940e-02, -3.7391e-02, -1.0281e-01,  5.0257e-02,\n",
      "         4.7398e-02,  2.7898e-02,  6.5546e-02, -3.5585e-02, -1.5329e-02,\n",
      "        -3.8707e-02, -5.4844e-02, -2.3227e-02,  3.0108e-02, -2.5781e-02,\n",
      "        -2.8408e-02,  3.9738e-03,  9.0303e-02,  8.2566e-03,  2.2979e-02,\n",
      "        -5.5796e-02, -3.8515e-02, -6.0057e-02,  7.1408e-02, -6.8506e-02,\n",
      "        -8.3587e-02, -1.1510e-01,  3.3540e-02, -1.6315e-02, -4.7617e-02,\n",
      "        -1.2741e-01, -2.6345e-02, -6.0932e-02, -2.5297e-02,  1.7280e-03,\n",
      "        -5.4365e-02, -5.7350e-02, -4.4366e-02, -1.8187e-02, -5.9762e-02,\n",
      "         1.8093e-02, -6.1407e-02,  1.3368e-01,  3.7309e-02, -2.3302e-02,\n",
      "        -3.6866e-02,  6.9024e-03,  7.7365e-03,  4.0508e-02, -2.5169e-02,\n",
      "        -8.2504e-02,  1.2014e-01, -6.4195e-02,  6.6726e-02,  1.5957e-02,\n",
      "         1.0247e-01,  9.6323e-02,  5.0310e-02, -7.1386e-02, -6.2054e-03,\n",
      "        -1.6760e-01,  3.7466e-03, -9.4249e-02,  7.7653e-02, -1.2555e-01,\n",
      "        -6.1608e-02, -2.9333e-02,  1.3478e-02, -1.4650e-02, -9.3798e-02,\n",
      "         6.4758e-02,  2.1284e-02,  1.5329e-01, -8.6474e-02, -5.4156e-03,\n",
      "        -2.4129e-02,  1.0983e-01, -2.6136e-02,  1.7877e-02,  7.2377e-02,\n",
      "         2.4865e-02,  5.1694e-02,  5.9210e-02,  1.3274e-01, -4.0805e-02,\n",
      "         2.4143e-02,  6.7355e-02,  6.0903e-02,  6.5552e-02,  1.7681e-01,\n",
      "         4.1771e-02,  1.2728e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 5.0966e-02, -1.4745e-01,  7.7494e-02,  1.4598e-02,  1.1066e-01,\n",
      "        -3.6061e-02, -3.4456e-02,  2.3449e-03,  3.6120e-02, -2.1529e-02,\n",
      "         1.0209e-01,  1.2287e-03, -5.0131e-02,  6.2569e-02, -2.0442e-02,\n",
      "         3.2035e-02,  6.1605e-02, -9.9639e-02,  1.5433e-02,  3.8132e-02,\n",
      "        -6.6866e-02, -6.3091e-02, -6.1747e-02,  6.8062e-02,  8.8035e-02,\n",
      "        -1.0674e-01,  5.1352e-02, -3.5963e-02, -4.7417e-03, -4.0600e-03,\n",
      "        -1.0709e-01, -8.8151e-02,  1.0923e-01, -5.1789e-02, -1.1943e-01,\n",
      "        -3.2427e-02,  8.7168e-02,  1.1600e-01, -3.1433e-02,  2.1007e-02,\n",
      "        -2.0211e-02,  5.1138e-02, -3.1195e-02, -1.7929e-02,  1.6682e-02,\n",
      "        -5.8549e-03, -3.0055e-02, -1.2022e-01,  4.2940e-02,  5.0219e-03,\n",
      "        -7.6352e-02,  1.2055e-02,  1.1379e-02,  7.7296e-02, -3.7195e-02,\n",
      "         6.2380e-02, -9.9886e-02,  1.3775e-02, -3.7782e-02, -8.0343e-03,\n",
      "         1.1148e-02, -1.7144e-02, -8.2952e-02,  6.2111e-02,  1.4023e-02,\n",
      "         9.3064e-02, -1.8222e-02,  8.8978e-02, -9.5613e-02,  5.1005e-02,\n",
      "         6.4407e-02, -1.5327e-02, -1.6592e-02, -4.5361e-02, -3.1602e-02,\n",
      "        -4.6708e-02, -4.0381e-02,  9.3572e-02,  1.4583e-02,  1.5900e-02,\n",
      "         5.2908e-02, -6.2023e-02,  9.5726e-02, -2.2317e-02, -1.0207e-02,\n",
      "        -8.4064e-02, -8.5376e-02,  1.4583e-02,  6.5636e-02,  8.2487e-02,\n",
      "         6.9251e-02, -3.3851e-03,  2.0579e-02, -6.4329e-03, -6.3405e-03,\n",
      "         2.8375e-02, -5.4557e-02,  4.9721e-02, -2.8327e-02,  7.1326e-02,\n",
      "        -2.7338e-02,  7.1745e-02,  2.0902e-02, -1.4693e-02, -6.4021e-03,\n",
      "        -3.6755e-02,  2.3320e-02, -1.8848e-02, -8.2152e-03, -7.3774e-02,\n",
      "        -6.4569e-02, -3.3738e-02,  2.3054e-02, -1.0855e-02,  3.3617e-02,\n",
      "         5.3611e-02, -6.7952e-02, -5.8561e-02, -4.5781e-02,  2.4040e-02,\n",
      "        -8.8937e-02,  3.5465e-02,  5.0535e-02,  2.5044e-02, -4.3513e-03,\n",
      "        -3.2971e-02, -1.3832e-01, -8.0301e-02,  1.5525e-01, -8.0106e-02,\n",
      "         2.0949e-02,  1.1226e-02,  5.7637e-02,  9.5634e-02, -4.6271e-02,\n",
      "         6.2753e-02, -4.8439e-02,  5.5866e-02, -5.6149e-02,  8.9882e-03,\n",
      "        -2.2475e-02,  2.6102e-03, -7.5365e-02, -3.5781e-02,  8.7820e-03,\n",
      "        -2.7019e-02,  5.6331e-02,  1.6614e-03, -3.3956e-02, -6.9785e-02,\n",
      "         1.1633e-01,  5.9738e-02, -8.4658e-02,  3.5563e-02,  1.0341e-01,\n",
      "         7.0607e-05, -4.0593e-02,  3.8467e-02,  1.0799e-01,  1.7658e-02,\n",
      "        -9.0117e-02, -9.2431e-02, -7.4624e-02,  3.1521e-02,  4.0765e-02,\n",
      "        -1.2515e-01,  3.0535e-02,  1.1851e-02, -4.0310e-02,  2.2916e-02,\n",
      "         1.2250e-01,  6.9152e-02, -6.2053e-03,  4.0321e-02,  1.6208e-02,\n",
      "        -6.8822e-02,  2.1849e-02, -3.6987e-02, -4.4603e-02, -1.5947e-01,\n",
      "        -1.6658e-02, -9.6214e-02, -3.7753e-02,  5.4041e-02, -1.7003e-02,\n",
      "         8.1025e-02,  2.4926e-02,  5.5767e-02, -7.9529e-02, -2.1234e-01,\n",
      "        -4.7282e-02, -5.5761e-02,  3.0091e-02,  1.4731e-01, -6.2581e-02,\n",
      "         2.2454e-02, -6.7485e-02,  1.5281e-01,  4.6557e-02,  8.2848e-02,\n",
      "        -9.2783e-03,  7.2040e-02, -9.9636e-02,  6.1564e-02, -5.9368e-02,\n",
      "        -1.9590e-02, -1.0435e-02, -4.1890e-02, -4.7181e-02, -1.2446e-02,\n",
      "        -4.0818e-02,  6.1132e-02, -8.5487e-03,  8.7448e-02,  2.1625e-02,\n",
      "        -1.7572e-02, -9.9109e-02,  3.0057e-02,  7.2901e-02, -1.2618e-02,\n",
      "         3.7349e-02, -2.1917e-02, -6.9758e-02, -1.2695e-03, -1.3122e-02,\n",
      "        -5.0221e-02,  2.3869e-02,  5.0954e-02,  7.0282e-04, -3.3970e-02,\n",
      "        -2.8963e-02, -8.4868e-02, -2.6569e-02, -6.5083e-02,  8.5820e-03,\n",
      "        -4.4336e-03,  5.8201e-03,  2.1587e-02,  7.3191e-03,  4.7043e-03,\n",
      "        -5.8309e-02,  2.1552e-02, -2.5648e-02, -2.2331e-02, -1.0112e-01,\n",
      "        -3.7041e-02, -4.1032e-02, -6.8042e-02,  1.7894e-02, -2.6997e-02,\n",
      "        -2.7584e-02,  1.7612e-02, -1.9444e-03,  5.9923e-02,  6.8182e-02,\n",
      "         2.6522e-02, -6.7600e-02,  3.6002e-02, -1.6933e-02,  9.7652e-03,\n",
      "        -1.0266e-01, -3.6495e-03,  1.1981e-01, -3.1746e-02, -2.1659e-02,\n",
      "        -4.1714e-02,  7.0952e-02, -8.4005e-02,  3.2536e-03, -2.2566e-02,\n",
      "        -3.9273e-02,  3.3117e-03, -8.4515e-02,  5.7761e-02,  9.1372e-02,\n",
      "         9.6171e-03, -1.2380e-01, -8.3872e-04, -1.1604e-02, -2.1467e-02,\n",
      "         3.9992e-02,  8.3243e-04, -5.9930e-03, -2.2868e-02,  2.3452e-02,\n",
      "         1.2934e-02,  1.4610e-01,  6.3666e-04, -4.7834e-02, -1.6290e-02,\n",
      "         6.7797e-02,  3.1905e-02, -6.1453e-02,  4.7708e-02,  4.9836e-02,\n",
      "        -3.2332e-02,  1.4693e-02, -8.0379e-02,  5.6533e-02,  6.9687e-02,\n",
      "         6.2967e-02, -3.5479e-02, -9.2222e-03, -6.3729e-03,  8.0024e-02,\n",
      "         1.0684e-02,  5.5488e-02, -5.7777e-03,  1.2793e-01,  2.4388e-02,\n",
      "         6.8428e-02, -2.1748e-03, -4.4633e-02,  1.3514e-02,  2.4887e-03,\n",
      "        -1.9060e-02, -1.2467e-01, -4.7357e-02, -4.9894e-02,  9.8269e-02,\n",
      "        -6.8453e-03,  3.6830e-02, -3.3399e-02, -4.3410e-02, -9.6036e-02,\n",
      "         8.1545e-02, -3.5613e-02,  6.0910e-02, -5.0575e-02,  6.5858e-03,\n",
      "         5.8657e-02,  2.9649e-02, -5.0301e-02, -1.8220e-02, -7.9198e-02,\n",
      "         4.7839e-02,  3.2613e-02, -9.3417e-02,  6.7337e-02, -8.7942e-03,\n",
      "        -1.6459e-02,  2.7349e-02, -4.9454e-02,  6.1516e-02,  6.7670e-02,\n",
      "         4.5408e-03,  3.2664e-02,  3.3849e-02, -8.3817e-03,  2.9799e-02,\n",
      "        -6.4481e-02,  6.9932e-02,  1.3802e-02, -7.4295e-02,  2.8266e-03,\n",
      "         1.3482e-01,  1.6569e-02, -4.2818e-02,  5.2147e-02,  4.8331e-02,\n",
      "        -2.2739e-02, -1.8746e-02,  2.8624e-02, -8.2209e-02, -4.9650e-02,\n",
      "        -2.9904e-02, -3.1530e-02, -4.7788e-02, -4.7805e-02,  4.2077e-02,\n",
      "        -5.1374e-03,  9.3389e-02,  7.7671e-02, -1.0206e-02, -5.3528e-02,\n",
      "        -6.0535e-03,  2.0553e-02,  2.7381e-02,  8.1292e-03, -6.6471e-02,\n",
      "        -1.9595e-02,  2.1768e-02,  4.5958e-02,  5.7396e-02,  1.7548e-02,\n",
      "        -6.3863e-03, -1.7971e-01,  2.8201e-02,  1.6888e-02, -6.0088e-02,\n",
      "        -4.4732e-02,  5.1204e-04,  5.4047e-02,  1.5042e-02,  8.6862e-02,\n",
      "        -5.6149e-02, -8.0252e-02, -1.7712e-02, -3.3251e-02,  6.7082e-02,\n",
      "         5.7277e-02,  7.4467e-02,  1.3210e-02,  8.0749e-02, -4.9230e-02,\n",
      "         4.0126e-02,  6.4328e-02,  3.2686e-02,  5.5669e-02, -4.5429e-02,\n",
      "        -6.0456e-02,  5.9471e-03, -7.2037e-03, -6.6578e-02,  6.4264e-02,\n",
      "        -3.4567e-02,  1.8057e-01,  9.6095e-02,  1.7282e-02, -5.5573e-03,\n",
      "        -1.5813e-02,  7.3891e-02, -9.6589e-03, -5.6928e-02,  3.5197e-02,\n",
      "        -3.6848e-02,  3.3619e-02, -7.9201e-02, -1.0853e-03, -6.1366e-02,\n",
      "        -4.6373e-02, -2.3210e-02,  2.4530e-02, -2.9117e-02, -2.6862e-02,\n",
      "         2.0443e-02, -1.0311e-02, -4.5818e-02,  3.2928e-02, -1.4177e-01,\n",
      "        -3.3394e-02, -8.0657e-02, -1.1610e-01,  2.7471e-03, -1.1582e-02,\n",
      "         1.8751e-03, -3.5150e-02,  9.0628e-02, -1.1234e-02, -6.3072e-03,\n",
      "        -2.9522e-03, -2.5991e-02,  7.4267e-02,  5.3881e-02, -4.0242e-03,\n",
      "         7.6560e-03,  8.1244e-02, -1.5535e-02, -7.0901e-02,  4.0996e-03,\n",
      "        -1.9212e-02,  1.5392e-02, -4.2169e-02,  1.7310e-02, -7.4863e-02,\n",
      "        -5.8399e-02, -4.7026e-02,  1.1410e-01, -1.0140e-01, -9.5707e-02,\n",
      "         2.0097e-02, -1.0625e-01,  6.2864e-02, -1.0046e-01,  4.0808e-02,\n",
      "        -5.9520e-02, -5.2804e-02,  1.8317e-02, -1.1327e-01, -1.7123e-02,\n",
      "        -2.9642e-03, -1.2108e-02,  4.3250e-02, -6.8001e-02,  2.8993e-02,\n",
      "         2.3379e-03,  6.4308e-03, -5.0257e-02, -2.6099e-02, -9.2139e-03,\n",
      "         1.4326e-01, -3.5042e-02, -5.5747e-03,  1.4443e-01,  6.4646e-02,\n",
      "        -3.6846e-02, -3.1642e-02,  1.8773e-04, -6.0860e-02,  7.3784e-02,\n",
      "         3.4365e-02, -5.6993e-02,  4.9817e-02, -4.8040e-02,  7.2079e-02,\n",
      "         6.0582e-02,  1.5344e-03, -6.8195e-02,  2.4479e-02, -6.7752e-02,\n",
      "        -7.2611e-02, -2.7682e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0768, -0.0110,  0.0261, -0.0717,  0.0138, -0.0634, -0.0912,  0.0113,\n",
      "        -0.0347, -0.0304, -0.0077, -0.0341, -0.0804, -0.0470, -0.0264,  0.0091,\n",
      "         0.0322,  0.0482, -0.0405, -0.0913,  0.0352, -0.0308,  0.0159,  0.0034,\n",
      "         0.0155, -0.0147,  0.0697,  0.0984,  0.0066,  0.0651, -0.1385, -0.0525,\n",
      "        -0.0866,  0.0596, -0.0648,  0.0693,  0.0717,  0.0327, -0.0749,  0.1113,\n",
      "         0.0407,  0.0465,  0.1108,  0.0816, -0.0240,  0.0117,  0.0365, -0.0328,\n",
      "         0.0209, -0.0589,  0.0395, -0.0040,  0.0484,  0.0579,  0.0430,  0.0961,\n",
      "         0.0019, -0.0478, -0.0156,  0.0328, -0.0624,  0.0715,  0.0612, -0.0883,\n",
      "         0.0393, -0.0688, -0.0231, -0.0230, -0.0219,  0.0156, -0.0243, -0.1010,\n",
      "        -0.0313,  0.0016, -0.0020, -0.0170, -0.0236, -0.0161, -0.0517, -0.0867,\n",
      "        -0.0712, -0.0125, -0.0954, -0.0109,  0.1592,  0.0375, -0.0574,  0.0412,\n",
      "        -0.0757,  0.1175,  0.0951, -0.0161, -0.0222, -0.1225,  0.0901,  0.0392,\n",
      "        -0.0461, -0.0242,  0.0155, -0.0975, -0.0425, -0.0112,  0.0040,  0.0077,\n",
      "         0.0669, -0.0678, -0.0185, -0.0830, -0.0124,  0.0362, -0.0285,  0.1085,\n",
      "        -0.0133,  0.0715, -0.0329, -0.0025,  0.0326, -0.0271,  0.0487, -0.0552,\n",
      "        -0.0141,  0.0521, -0.0023, -0.0375, -0.1438,  0.0137,  0.0634, -0.0483,\n",
      "        -0.0128,  0.0103,  0.0111,  0.0511,  0.1563,  0.0164,  0.0060, -0.1368,\n",
      "        -0.1142, -0.0285, -0.0205,  0.0208,  0.0782,  0.0446,  0.0960, -0.0340,\n",
      "        -0.0171,  0.0837,  0.1210,  0.0210, -0.0156, -0.0047,  0.0567,  0.1111,\n",
      "        -0.0234, -0.0498, -0.0705, -0.0082,  0.1107,  0.0074,  0.0705, -0.0538,\n",
      "         0.0613, -0.1379,  0.0155, -0.0276,  0.0236, -0.0070, -0.0942, -0.0741,\n",
      "         0.0344,  0.0320, -0.0537, -0.1111, -0.0324,  0.1613,  0.0198,  0.1086,\n",
      "        -0.0317,  0.0004, -0.0473,  0.0628,  0.0596, -0.0103, -0.0568,  0.0624,\n",
      "        -0.0776, -0.1148, -0.0166,  0.0027,  0.0078, -0.0937, -0.0514, -0.0138,\n",
      "        -0.1482, -0.0669, -0.0712,  0.0135,  0.1173, -0.0033, -0.0064, -0.0263,\n",
      "        -0.0567,  0.0106,  0.0777, -0.0619, -0.0526,  0.0932, -0.0841, -0.0340,\n",
      "        -0.1270,  0.0130,  0.0067, -0.0860,  0.1337, -0.0305, -0.0314, -0.0653,\n",
      "         0.1493, -0.0126, -0.0196, -0.0949, -0.0565,  0.0440, -0.0889,  0.0118,\n",
      "        -0.0558, -0.0214, -0.0157, -0.0387, -0.0158,  0.0084, -0.0396, -0.0521,\n",
      "        -0.0809,  0.0183,  0.0045,  0.0053, -0.0093, -0.0678, -0.1156,  0.0174,\n",
      "         0.1187,  0.0416,  0.0693, -0.0025,  0.0486,  0.0294, -0.0075, -0.0575,\n",
      "         0.1809,  0.0164,  0.0446, -0.0271, -0.0230,  0.0786, -0.0114, -0.0058,\n",
      "         0.0358, -0.0731, -0.0365, -0.0286,  0.1120, -0.0882,  0.0127,  0.0710,\n",
      "         0.0003,  0.0062, -0.0400,  0.0463,  0.0816,  0.0720,  0.0084,  0.0478,\n",
      "         0.0634,  0.0475,  0.0025, -0.0680, -0.0101,  0.0497,  0.0274,  0.0548,\n",
      "         0.0372, -0.0325,  0.1441,  0.0648,  0.0218,  0.0187,  0.0017,  0.0058,\n",
      "         0.0606,  0.0349, -0.0842, -0.0129,  0.1517, -0.0832, -0.0344,  0.0722,\n",
      "         0.0201, -0.0085,  0.0686, -0.0399, -0.1319,  0.0208, -0.0094, -0.0035,\n",
      "         0.0502,  0.0415,  0.0268,  0.0031, -0.0782, -0.0470,  0.0647, -0.0245,\n",
      "        -0.0220,  0.0053, -0.0115,  0.0109,  0.0431,  0.0079, -0.0562, -0.0070,\n",
      "         0.0463, -0.0588,  0.0339,  0.0052, -0.0210,  0.1090,  0.0647, -0.0540,\n",
      "         0.0085,  0.0879, -0.0313,  0.0073,  0.0437,  0.0494,  0.0060,  0.1026,\n",
      "         0.0076,  0.0393, -0.0335, -0.0069, -0.1043,  0.0803, -0.0891,  0.1589,\n",
      "        -0.0709, -0.0418, -0.0459, -0.0026,  0.1630, -0.0228,  0.0362,  0.0665,\n",
      "         0.0199,  0.0311, -0.0793,  0.0584, -0.0846, -0.0298,  0.0471,  0.1816,\n",
      "         0.1290, -0.0308, -0.0354,  0.0684,  0.0022,  0.1397,  0.1273, -0.0121,\n",
      "        -0.0255,  0.1549, -0.1043,  0.0030, -0.0070, -0.0533, -0.1327, -0.0505,\n",
      "        -0.0394, -0.0871, -0.1559, -0.1013, -0.0389,  0.0533, -0.0024,  0.0499,\n",
      "         0.0578, -0.0086, -0.0890, -0.0100,  0.0792, -0.0145, -0.0229, -0.0173,\n",
      "        -0.0718,  0.0246, -0.0108, -0.0746, -0.1079, -0.1119, -0.0225,  0.0620,\n",
      "        -0.0441,  0.0702,  0.1055, -0.0187,  0.0807,  0.0159,  0.0401,  0.0435,\n",
      "        -0.0720, -0.1575, -0.0476, -0.0490, -0.0268,  0.1036,  0.0390,  0.0015,\n",
      "        -0.1407, -0.0818, -0.0521, -0.0193,  0.0634,  0.0762, -0.0572,  0.0335,\n",
      "        -0.0147,  0.0902, -0.0812,  0.0083, -0.1243, -0.0758,  0.1391,  0.0418,\n",
      "         0.0337, -0.0012,  0.0702, -0.0611,  0.0674,  0.0109,  0.0365, -0.0833,\n",
      "        -0.0679, -0.0756,  0.0385, -0.0285,  0.0510, -0.0359,  0.0606,  0.0541,\n",
      "         0.0934, -0.0538, -0.0293,  0.0203, -0.0051,  0.1183, -0.0098,  0.0472,\n",
      "         0.0742, -0.0267, -0.0643, -0.0058,  0.0205,  0.0397, -0.0012,  0.0355,\n",
      "         0.0729,  0.0082,  0.0999,  0.0031,  0.0537,  0.0390,  0.0033,  0.0092,\n",
      "         0.0299, -0.0649,  0.0372,  0.0805,  0.0463, -0.0983, -0.0180, -0.0175,\n",
      "         0.0584, -0.0766,  0.0062, -0.0004,  0.0233, -0.0832,  0.0306,  0.0634,\n",
      "         0.0414, -0.0457,  0.0292, -0.0461,  0.0299,  0.0362,  0.0514,  0.0055,\n",
      "        -0.0551, -0.0026, -0.0381, -0.0229, -0.0396, -0.0021,  0.1161, -0.0633,\n",
      "         0.0352, -0.0886,  0.1244, -0.0195,  0.0971,  0.0900, -0.1717, -0.0553],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-2.8312e-02,  4.9911e-02,  9.7769e-03, -1.7147e-02,  4.0901e-02,\n",
      "        -1.2317e-01, -1.1881e-01,  8.5501e-02,  1.1018e-01,  6.2696e-02,\n",
      "         3.1070e-02, -1.0946e-01,  7.7663e-02,  6.7539e-02, -1.3375e-04,\n",
      "        -1.2912e-02,  5.7624e-02, -7.1261e-02,  9.6846e-04, -4.5915e-03,\n",
      "         6.0058e-02,  2.9872e-02,  4.2197e-02,  3.8850e-02,  5.4885e-02,\n",
      "         4.4528e-02, -8.8942e-02,  1.1722e-01, -4.4009e-02,  3.8589e-02,\n",
      "        -7.9293e-02, -1.1473e-02, -2.3653e-02, -4.3948e-02, -2.1827e-02,\n",
      "        -4.3308e-04,  8.2051e-02,  6.2999e-02,  3.0414e-02,  1.3454e-02,\n",
      "         5.9846e-03,  1.5785e-02, -6.2734e-02,  7.9752e-02, -1.4402e-01,\n",
      "        -5.4157e-02,  8.3404e-02, -5.4182e-02, -3.7938e-02,  1.9626e-03,\n",
      "         6.2376e-02, -9.8665e-02,  1.1238e-01,  8.4942e-02, -5.1376e-02,\n",
      "        -4.4197e-03,  1.0537e-02,  7.6728e-02,  7.0679e-02,  7.5002e-02,\n",
      "         2.3206e-02,  2.2686e-02,  3.7321e-02,  3.3898e-02, -2.2739e-02,\n",
      "        -1.1890e-01,  7.7856e-02,  1.0845e-01,  6.1648e-02, -2.4917e-02,\n",
      "        -5.6272e-02, -2.0143e-04, -6.7984e-02, -5.5723e-02,  1.5601e-03,\n",
      "         9.5723e-02, -1.2334e-01,  2.3138e-02,  1.5915e-03,  1.7391e-02,\n",
      "         1.0060e-03, -5.5752e-02, -7.3283e-03,  7.8786e-02, -8.5108e-02,\n",
      "         5.5049e-02,  1.5016e-01, -3.1859e-02,  4.4934e-03, -5.7109e-02,\n",
      "         8.0624e-03,  1.0309e-01, -3.0260e-03, -1.8075e-02,  1.0297e-01,\n",
      "         1.8190e-02,  8.1257e-02, -1.0586e-01,  4.6859e-02,  8.7545e-03,\n",
      "        -1.8347e-02,  7.8826e-04,  3.4076e-02,  3.4202e-02, -4.6036e-02,\n",
      "         7.8401e-02,  1.2534e-02, -2.9604e-02, -1.4013e-01, -1.2220e-01,\n",
      "        -3.9575e-02,  4.2375e-02,  6.8481e-02, -1.1031e-01,  1.7292e-03,\n",
      "         5.6505e-03, -1.3347e-01,  5.8967e-02,  1.0500e-01,  2.8959e-02,\n",
      "        -1.3579e-01, -3.6767e-02, -6.5603e-03,  5.9650e-02,  3.4714e-02,\n",
      "         3.4603e-02,  6.3472e-02,  8.8572e-02, -3.0379e-02,  1.2246e-02,\n",
      "         3.0892e-02, -1.9900e-02, -2.0532e-02, -9.3364e-02,  2.0879e-02,\n",
      "        -3.1082e-02,  7.4723e-02,  3.4827e-02,  9.9355e-03,  4.0432e-02,\n",
      "         9.0674e-02, -6.2378e-02, -1.7440e-02,  1.5880e-02, -1.3521e-02,\n",
      "         6.1648e-02, -2.5270e-02, -1.0506e-02,  1.8069e-02, -5.2453e-02,\n",
      "         1.3252e-02,  6.9504e-03, -5.8516e-02,  4.6623e-02,  1.4739e-02,\n",
      "         6.7765e-03,  3.7023e-03,  3.7319e-02,  1.9224e-02,  2.6738e-02,\n",
      "         8.2818e-02, -1.2007e-04,  7.7645e-02,  9.2141e-03,  4.3738e-03,\n",
      "        -1.0779e-01,  8.4956e-02,  3.7886e-02, -1.3384e-01, -1.1208e-01,\n",
      "        -5.7828e-02, -9.7238e-02,  1.0206e-02,  6.5645e-03, -2.8718e-02,\n",
      "         1.5325e-02,  6.6613e-02,  2.6445e-02, -2.4962e-02, -4.9788e-02,\n",
      "        -4.3545e-03, -4.5150e-02, -1.4951e-02,  6.1688e-02, -9.0608e-03,\n",
      "        -8.5805e-02, -1.0172e-01, -9.2241e-02, -1.5714e-03, -2.6098e-02,\n",
      "        -2.3720e-02, -4.2816e-03, -4.2465e-02,  4.0990e-03,  5.9952e-02,\n",
      "        -8.0171e-02,  3.4743e-02, -5.9418e-02, -5.0707e-04, -1.7003e-02,\n",
      "        -3.6289e-02,  9.0298e-02, -2.5486e-02,  2.2962e-02,  8.9927e-03,\n",
      "         3.8505e-02,  5.5345e-02, -2.0447e-02, -3.3111e-02,  3.7436e-02,\n",
      "         6.5773e-02, -4.5183e-02,  4.1996e-02, -8.7999e-02, -1.1769e-02,\n",
      "        -4.3234e-02, -6.6346e-02, -3.5659e-02, -5.7530e-03,  3.8261e-02,\n",
      "         6.5813e-02, -2.6030e-02, -7.3186e-03, -6.0748e-02, -5.1565e-02,\n",
      "        -2.2371e-02,  1.2256e-02,  7.5072e-02,  1.9970e-02,  2.4642e-02,\n",
      "        -7.0200e-02,  3.6686e-02,  2.4515e-02,  3.2946e-03,  6.7995e-03,\n",
      "         8.7247e-02, -6.1754e-02,  2.3224e-02,  4.8788e-02, -3.7919e-02,\n",
      "        -4.5916e-02, -6.3038e-03, -6.4867e-02,  9.7451e-03, -2.9809e-02,\n",
      "         1.9220e-02,  4.9873e-02, -8.4751e-02, -3.8756e-02,  2.4613e-03,\n",
      "         1.2979e-02, -1.9546e-02, -1.7456e-03,  6.0348e-02,  3.5478e-02,\n",
      "         8.5359e-02,  4.5793e-02, -2.9652e-02, -1.9533e-02,  2.8801e-02,\n",
      "         2.0128e-02, -1.6773e-02, -2.2567e-02,  8.6599e-02,  7.6258e-02,\n",
      "        -1.3919e-02, -5.2701e-03,  1.5254e-02, -5.6596e-03,  1.2512e-02,\n",
      "        -1.1107e-01, -3.9220e-02, -4.3274e-02, -1.4759e-02,  6.3456e-02,\n",
      "        -3.9313e-02,  6.6304e-02, -2.5031e-02, -8.0906e-02, -9.2574e-02,\n",
      "         7.7114e-03, -3.8525e-02,  2.6354e-02,  6.7656e-02, -3.6397e-02,\n",
      "        -6.6598e-02,  4.9100e-02, -4.5302e-02, -9.6687e-02,  3.2252e-03,\n",
      "        -1.6827e-02,  9.3235e-02, -2.9695e-02,  8.8593e-02,  1.0684e-01,\n",
      "         1.0159e-01,  7.8147e-02, -2.3984e-02,  7.4527e-02,  9.7435e-02,\n",
      "         9.9969e-02,  4.1802e-02,  5.5769e-02,  4.1883e-02,  3.7363e-02,\n",
      "        -1.2641e-02,  3.1162e-02, -5.7425e-04,  5.6984e-02,  2.1873e-03,\n",
      "         3.2089e-02, -7.0392e-02,  2.0635e-02,  9.4762e-03, -1.5822e-02,\n",
      "         5.4450e-02, -2.8916e-02,  1.6877e-02, -7.8206e-03, -1.1922e-01,\n",
      "         2.3058e-02,  6.5806e-02,  9.5983e-03,  4.4597e-02,  1.8453e-02,\n",
      "         4.3058e-02,  6.1493e-02, -6.8039e-02, -3.5424e-02, -3.8730e-02,\n",
      "        -4.6403e-02,  2.2619e-03,  1.3438e-02,  3.6322e-02, -9.0361e-02,\n",
      "         2.3885e-02, -6.8223e-02, -2.8933e-02,  1.0164e-01,  1.5505e-02,\n",
      "        -7.0034e-02,  7.1678e-02, -6.8170e-02,  4.8597e-02,  8.5489e-02,\n",
      "         3.4030e-02, -1.1827e-02,  4.7249e-02, -5.7491e-02,  6.4812e-02,\n",
      "        -3.8081e-02,  3.1269e-02,  4.8112e-02, -2.2889e-02, -1.2078e-01,\n",
      "         8.6875e-03,  2.7524e-03, -5.2020e-02, -1.3657e-02, -3.4252e-02,\n",
      "         1.2507e-01,  6.4650e-02, -4.3744e-02,  2.1554e-02,  7.2027e-02,\n",
      "         4.6084e-02,  1.0100e-01,  7.4042e-02, -5.4211e-02, -1.1455e-01,\n",
      "         5.7521e-02, -4.2710e-02, -7.8814e-02, -1.8124e-02,  4.4737e-02,\n",
      "        -5.1269e-02, -6.7855e-02, -8.3722e-02, -6.4286e-02,  3.4506e-02,\n",
      "         8.8117e-02,  4.1227e-02, -1.0366e-01, -5.4640e-02, -3.3339e-03,\n",
      "         1.3867e-01, -5.8631e-02,  1.0841e-02, -9.4331e-02,  1.0992e-01,\n",
      "        -1.8052e-02,  5.6607e-02, -3.0553e-03, -9.7665e-02,  3.6189e-03,\n",
      "         3.8424e-02, -2.0226e-02, -1.0399e-01,  7.1986e-02, -8.7396e-02,\n",
      "        -2.1321e-02, -3.3681e-02, -4.8806e-02, -9.9724e-03,  3.4821e-02,\n",
      "        -3.6701e-02, -1.0064e-01, -4.4952e-02, -2.9649e-02,  6.7568e-02,\n",
      "         1.0062e-01,  1.5413e-02, -5.2982e-03, -8.1491e-02,  6.9497e-02,\n",
      "         7.5970e-03,  2.6650e-02, -7.8061e-02,  8.9628e-02,  5.9069e-02,\n",
      "        -2.8076e-03,  2.2840e-02,  4.9031e-02, -3.0829e-02, -1.4460e-01,\n",
      "         2.0347e-02,  3.0446e-02,  4.5471e-02,  8.5173e-02, -1.1764e-02,\n",
      "        -1.9823e-02, -1.1526e-02, -1.4037e-02, -5.7210e-03,  3.2612e-02,\n",
      "         8.8098e-02,  2.5476e-02,  5.3235e-02,  9.3301e-02,  6.9620e-02,\n",
      "        -6.3628e-02,  6.8000e-02,  1.4908e-01, -5.6959e-02,  5.9116e-02,\n",
      "         2.2112e-02, -2.4973e-02, -2.7610e-02,  4.1903e-02, -2.0115e-02,\n",
      "         5.7806e-02,  1.3158e-03, -8.3065e-02,  4.6314e-02, -9.3857e-02,\n",
      "        -9.9200e-03,  4.4497e-02, -1.1722e-02, -6.1344e-02, -1.3309e-01,\n",
      "         4.0768e-02, -2.1628e-02, -5.0834e-02,  1.0866e-01,  1.6634e-02,\n",
      "         7.5386e-02,  1.1037e-01, -3.8678e-02,  5.1629e-02,  3.5886e-02,\n",
      "         3.2558e-02,  1.4227e-03,  5.5960e-02,  1.0197e-03, -5.6617e-02,\n",
      "         2.2816e-02, -1.3664e-01,  1.3298e-01, -3.5689e-02,  1.8169e-02,\n",
      "        -3.9363e-02, -4.9693e-02,  8.3050e-02, -1.3196e-02, -4.6567e-02,\n",
      "         3.9041e-02,  2.8396e-02, -2.6041e-02,  6.8008e-02, -1.0233e-01,\n",
      "        -1.5822e-02, -3.0579e-02, -4.8071e-02, -6.4514e-02,  1.8201e-02,\n",
      "        -4.3278e-02, -4.3680e-03, -8.4785e-02, -5.5908e-02, -6.7275e-02,\n",
      "         8.3114e-02,  1.3823e-02,  4.9019e-02,  4.0267e-02, -5.4514e-02,\n",
      "         4.9135e-02, -4.8312e-02, -2.4285e-02, -9.7027e-02,  2.4834e-02,\n",
      "         1.4886e-02,  6.9949e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 3.0289e-02,  3.1503e-02,  3.9986e-02,  1.3083e-01, -5.3132e-02,\n",
      "         2.9113e-02,  4.7187e-03,  5.0454e-02,  1.0700e-01, -2.2314e-02,\n",
      "         2.6524e-02, -1.1840e-02,  5.0855e-03,  7.3779e-04, -1.1865e-03,\n",
      "        -4.7954e-02,  1.0474e-02,  2.8582e-02, -7.9896e-02,  7.6038e-02,\n",
      "         4.5977e-02, -1.4148e-02,  3.9841e-02,  1.8766e-02,  8.0392e-02,\n",
      "         2.6746e-02,  2.9566e-02, -2.5976e-02,  1.6478e-02, -5.0035e-02,\n",
      "         2.4266e-02,  4.7684e-03, -4.6095e-02,  5.4383e-02, -5.5842e-02,\n",
      "        -6.3235e-02,  1.0002e-01, -7.9192e-03,  4.9059e-02, -2.9653e-02,\n",
      "         7.4298e-02,  3.2793e-02,  8.6242e-02,  1.3700e-03,  1.4234e-02,\n",
      "         7.6310e-02,  3.2565e-02, -5.5205e-02, -2.8722e-02, -3.9794e-02,\n",
      "         8.0323e-02, -1.0903e-01, -4.8134e-04,  4.3818e-02, -3.0959e-02,\n",
      "        -5.7084e-02,  4.3061e-02,  4.2138e-02,  7.2363e-02,  4.3792e-02,\n",
      "        -7.2850e-02,  5.2529e-03,  4.6195e-03, -6.2514e-02,  8.1972e-02,\n",
      "        -1.2628e-02,  1.1640e-01, -7.5081e-02,  2.6473e-02, -6.2586e-02,\n",
      "        -6.8327e-02,  5.4805e-03, -8.0045e-02, -1.0655e-02, -7.7074e-03,\n",
      "        -8.1215e-02, -1.6442e-02,  6.8840e-03, -6.9273e-03, -4.1731e-02,\n",
      "        -6.2782e-02,  6.2828e-02, -8.7719e-02,  1.7283e-02, -5.3315e-02,\n",
      "        -9.8364e-02, -9.7457e-02,  8.1505e-02,  2.6662e-02,  5.2712e-02,\n",
      "         5.1618e-02, -3.9540e-02, -1.0101e-01, -2.3273e-02,  1.6070e-02,\n",
      "        -3.2476e-02, -3.7883e-02, -1.9677e-02, -3.3466e-02,  1.7523e-02,\n",
      "        -9.1086e-02, -4.3556e-02,  7.8876e-02, -4.1143e-02, -3.5400e-02,\n",
      "        -1.7865e-02,  1.7630e-01,  1.3965e-01, -5.0848e-02, -3.6669e-02,\n",
      "         2.1116e-02, -1.0324e-01, -1.7145e-02,  6.3624e-02, -7.2753e-02,\n",
      "         8.1110e-04,  7.7122e-02,  6.0167e-02,  9.4302e-02,  3.3645e-02,\n",
      "         5.1997e-02,  9.3938e-03,  1.5380e-02,  3.0624e-02,  1.8364e-02,\n",
      "         9.4459e-02, -5.3204e-02,  5.3909e-02,  8.4368e-02, -2.6575e-02,\n",
      "         5.8741e-03,  1.7135e-01,  3.8734e-02,  1.1533e-01, -3.4991e-02,\n",
      "        -1.3902e-01, -5.0564e-02,  2.5342e-02,  1.9510e-03, -4.5458e-02,\n",
      "        -7.6664e-02,  1.0237e-01,  7.7267e-03,  5.8986e-02, -1.9288e-02,\n",
      "         5.3286e-02,  3.6359e-02,  8.0501e-02, -8.3045e-02,  3.3307e-02,\n",
      "         1.5659e-03,  9.6013e-03, -1.5590e-02, -5.1359e-02, -7.0246e-02,\n",
      "        -1.1975e-02,  2.6491e-02, -3.2005e-02,  6.8249e-02,  4.7669e-02,\n",
      "         4.7641e-02, -2.1512e-02, -6.3295e-02, -4.1788e-02, -1.5279e-02,\n",
      "        -9.7037e-02,  2.2685e-02,  2.0949e-02,  3.3309e-02,  9.4829e-03,\n",
      "         5.6710e-02, -7.6783e-03, -1.3969e-01, -4.1760e-02,  8.8335e-03,\n",
      "         4.3914e-02, -1.1144e-02,  2.1213e-02,  5.0143e-02, -1.7819e-02,\n",
      "        -3.6000e-02, -9.8346e-02,  1.8010e-02,  1.1031e-02, -4.7298e-02,\n",
      "        -2.5419e-02, -4.0803e-02,  3.5511e-02,  9.2070e-03,  6.9367e-03,\n",
      "        -4.2061e-02, -1.0377e-02,  8.0876e-02, -5.6107e-02,  5.7277e-02,\n",
      "         8.7439e-03,  1.8353e-02, -4.1559e-02,  3.4507e-02, -1.0548e-01,\n",
      "        -4.0571e-02, -2.1289e-02,  3.0586e-02,  5.1678e-03,  8.7577e-04,\n",
      "         1.3942e-01, -1.1645e-02,  7.2364e-02,  6.5043e-02,  2.4132e-02,\n",
      "         1.1002e-01,  6.1222e-03,  6.6061e-03, -5.2206e-02, -1.3325e-02,\n",
      "        -8.5573e-03, -2.0275e-03,  1.6365e-03,  2.6494e-02,  7.1705e-02,\n",
      "        -7.1865e-02,  8.4742e-02,  6.0429e-02, -5.9917e-04, -5.1137e-02,\n",
      "        -5.9481e-02, -7.6383e-02,  4.8239e-02, -3.4069e-02, -9.6994e-02,\n",
      "         1.8230e-02,  8.8950e-02,  8.6447e-02, -2.9383e-02, -9.0702e-02,\n",
      "        -3.7237e-02, -3.5979e-02, -4.2816e-02, -7.7253e-02,  7.3348e-03,\n",
      "         4.4436e-02, -1.5954e-01,  1.2394e-01,  1.1889e-02,  1.5041e-02,\n",
      "        -6.7389e-02, -4.5964e-02,  2.0859e-02, -3.0347e-02, -2.0750e-02,\n",
      "         3.9519e-02, -2.8886e-02, -8.1723e-02, -2.2986e-02, -2.3117e-03,\n",
      "         7.9396e-02, -4.6225e-02,  5.9592e-02, -6.6315e-02, -4.8456e-02,\n",
      "        -4.7836e-03, -6.7407e-02,  4.6288e-02,  1.5025e-01,  3.1964e-02,\n",
      "        -1.0685e-01, -3.1458e-02, -4.1457e-02,  7.1839e-02, -9.0231e-02,\n",
      "         3.3797e-02, -2.6273e-02, -6.0258e-02, -3.0063e-02, -9.9684e-02,\n",
      "         8.9154e-02,  4.6204e-02,  1.0030e-02, -2.1860e-02, -9.5296e-03,\n",
      "        -2.6632e-02, -2.0542e-02, -8.8112e-02, -3.1891e-02,  8.1285e-02,\n",
      "         3.4284e-02,  9.3343e-02, -7.2938e-02,  4.2222e-02,  8.5092e-02,\n",
      "        -6.9859e-02, -1.1665e-01, -1.7408e-02, -1.5403e-02,  5.4243e-02,\n",
      "         9.8341e-03, -2.8077e-02, -2.9991e-02,  3.4399e-02,  1.4826e-02,\n",
      "         1.0260e-02,  8.0673e-02,  5.1878e-03, -8.1736e-02,  8.6033e-02,\n",
      "         8.2636e-02,  5.0595e-02, -1.1922e-01,  9.3888e-03,  2.7255e-02,\n",
      "         2.7873e-02,  2.2796e-02,  1.8762e-02,  1.4380e-01, -1.4723e-01,\n",
      "        -1.4255e-02, -3.0604e-02, -3.7668e-03,  1.1167e-02, -8.0839e-02,\n",
      "         1.4414e-02, -2.5007e-02, -2.3666e-02, -2.7692e-02, -1.6474e-02,\n",
      "         5.1326e-02, -6.8901e-03,  2.6673e-02, -1.9049e-02, -4.9653e-02,\n",
      "         1.1313e-01,  8.5847e-02,  1.3205e-01, -4.7806e-02, -9.3220e-02,\n",
      "         4.1846e-02, -4.5715e-02,  2.4093e-02, -3.6066e-02,  5.0121e-02,\n",
      "         2.4745e-02, -9.0033e-02,  5.9747e-02, -5.9992e-02, -2.5795e-02,\n",
      "        -3.5649e-02,  2.3503e-02,  1.4340e-01, -5.7906e-02, -8.6132e-03,\n",
      "        -6.0701e-03,  3.0256e-03, -6.0207e-02,  1.3398e-02, -3.4405e-03,\n",
      "         3.6077e-02, -7.9061e-02, -4.5184e-02, -6.7206e-02,  8.3835e-02,\n",
      "        -1.4701e-02,  2.4760e-02,  1.7550e-02,  5.2360e-02, -1.1143e-01,\n",
      "        -6.0042e-02, -2.1617e-02, -2.3820e-02, -1.9716e-02, -1.1295e-01,\n",
      "        -1.7096e-02, -5.0607e-02,  9.7075e-02,  2.0780e-02, -4.8206e-02,\n",
      "         4.0675e-02, -5.4123e-02,  2.6274e-02, -1.1451e-01,  5.9652e-02,\n",
      "        -2.4965e-02, -2.3823e-02,  5.4150e-03, -2.5337e-03, -5.9982e-02,\n",
      "        -3.6474e-02, -1.8158e-02, -1.5301e-02,  1.1725e-02,  2.3499e-02,\n",
      "         7.4033e-02, -4.0130e-02, -5.1274e-02,  9.0815e-02,  5.4975e-02,\n",
      "        -3.4270e-02,  4.5382e-02, -7.2244e-02, -7.0036e-02, -9.7178e-03,\n",
      "        -3.3955e-02, -3.5253e-02,  8.1896e-02,  7.5562e-03, -7.9211e-02,\n",
      "        -1.0875e-01,  1.2409e-03,  7.7800e-02,  1.0634e-02, -8.2665e-02,\n",
      "         1.3230e-02, -3.4552e-02,  9.1453e-02, -6.4865e-02,  4.5128e-02,\n",
      "        -1.1324e-01, -5.8086e-02,  4.5286e-02, -3.5615e-02,  1.1491e-03,\n",
      "         4.5156e-02,  2.6197e-02, -9.7915e-02, -8.8574e-02,  6.3982e-02,\n",
      "        -7.3688e-02,  3.8706e-02,  8.2396e-02,  7.6938e-02, -2.0139e-02,\n",
      "        -6.2673e-02, -8.2048e-02,  5.6388e-02,  1.7644e-02,  4.3307e-02,\n",
      "         8.2072e-03, -4.8394e-02,  7.1145e-03, -1.4995e-01,  6.3767e-02,\n",
      "        -1.7300e-02, -4.0330e-04,  2.5645e-02,  6.1843e-02, -5.0088e-03,\n",
      "         3.9473e-03,  8.7710e-02,  3.0694e-02, -1.5863e-02,  1.2367e-01,\n",
      "         5.8815e-02,  6.1809e-02,  1.1823e-01,  3.4193e-02, -1.3734e-01,\n",
      "        -8.3475e-03, -1.3101e-02,  1.7372e-01,  3.1849e-02,  5.8699e-02,\n",
      "        -8.2168e-02,  2.9679e-02,  2.9754e-02, -1.9589e-02, -2.3867e-05,\n",
      "         2.9229e-03, -5.9795e-02,  1.0513e-01, -2.3250e-02,  1.5259e-02,\n",
      "        -9.9677e-04,  5.2436e-02,  4.5202e-02, -5.3536e-02, -3.1198e-02,\n",
      "         1.1600e-01,  8.2992e-02, -6.0462e-02, -6.9867e-02, -2.0561e-03,\n",
      "         6.2426e-02,  3.0686e-02,  7.3595e-03, -5.2512e-03, -8.7785e-02,\n",
      "         7.2232e-02, -5.5166e-02,  5.2830e-02, -3.4109e-02, -3.5072e-02,\n",
      "        -7.8913e-02,  3.6241e-02,  4.8680e-02, -2.4749e-02,  9.5748e-02,\n",
      "         1.1784e-01,  6.6303e-02, -3.3105e-02,  3.1397e-02,  4.8392e-02,\n",
      "        -9.6809e-02,  6.1331e-02,  3.0868e-02,  3.2937e-02,  1.4860e-02,\n",
      "        -8.8214e-02, -7.5167e-02, -2.6680e-02, -7.2619e-02, -3.8868e-02,\n",
      "         4.7005e-02, -1.5254e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-3.7034e-02, -4.5888e-02,  8.8781e-03,  2.7156e-02,  5.8858e-02,\n",
      "         1.2498e-03, -2.9473e-02, -2.4259e-02,  2.7695e-02,  4.8506e-02,\n",
      "        -1.3610e-02,  2.4264e-02, -1.0506e-02, -2.2343e-02, -1.2575e-02,\n",
      "        -2.7388e-02,  3.7047e-03, -9.8502e-02, -7.6187e-02, -1.3275e-02,\n",
      "         4.0868e-02,  3.1048e-02,  2.9744e-03, -3.4535e-02,  6.2692e-02,\n",
      "        -1.0555e-01, -1.8775e-03, -6.1323e-02,  1.1437e-02,  6.9841e-02,\n",
      "        -1.2952e-02,  7.9710e-02, -3.6756e-02,  1.2847e-02,  1.0407e-01,\n",
      "        -8.7324e-02, -1.0587e-01, -3.1902e-02, -8.2598e-03, -1.0516e-01,\n",
      "        -9.7262e-02,  1.1731e-02, -1.1542e-02, -1.0035e-01, -8.8628e-02,\n",
      "        -1.6604e-02, -6.6435e-04, -5.5660e-02, -5.8090e-03, -9.9288e-03,\n",
      "         2.7286e-02, -4.0562e-02, -1.3763e-02, -5.6210e-02, -8.2477e-03,\n",
      "         3.0968e-02, -2.2097e-02,  2.6884e-02, -4.4554e-03,  6.1624e-02,\n",
      "         5.7080e-02,  9.1388e-03, -2.2383e-02,  3.1594e-02,  9.0034e-02,\n",
      "         2.9283e-04, -2.4813e-03, -4.8279e-02,  2.9078e-02,  3.5868e-02,\n",
      "         4.1491e-02, -6.3660e-02, -8.4763e-02, -8.1597e-02, -5.1852e-02,\n",
      "         2.2601e-04,  1.0845e-01,  3.0973e-02, -1.5400e-01,  3.3164e-02,\n",
      "         7.9088e-02,  6.5250e-02,  5.1900e-02, -4.2283e-02, -1.1346e-01,\n",
      "        -9.0076e-03,  1.1980e-01, -1.1909e-02,  1.2310e-02,  1.8831e-02,\n",
      "        -4.9647e-02,  7.0969e-02, -2.3682e-02, -8.6618e-02,  5.2677e-02,\n",
      "         7.8079e-03, -1.0115e-01,  7.5915e-02, -4.8108e-02, -1.3128e-01,\n",
      "         6.4873e-02, -7.1029e-03, -1.4379e-01, -2.1432e-02, -5.3666e-02,\n",
      "         3.7874e-02, -8.1764e-02,  1.6618e-01,  7.1652e-02,  4.2189e-02,\n",
      "        -4.8112e-02,  5.0704e-02, -8.4332e-02,  2.3637e-02, -1.1713e-02,\n",
      "        -1.4738e-01, -5.6326e-02, -8.2328e-02, -6.9366e-03,  8.9393e-03,\n",
      "         9.0724e-02, -3.4346e-02, -1.7982e-02, -1.4817e-02, -9.2182e-02,\n",
      "         3.9414e-02, -1.3945e-02, -9.3391e-02,  1.0452e-01,  8.3443e-02,\n",
      "        -8.3101e-03,  5.8458e-02,  3.4724e-02, -9.1750e-02,  2.9846e-02,\n",
      "        -9.8895e-02, -2.4202e-02,  4.6580e-02,  4.4337e-02, -1.2447e-02,\n",
      "        -8.0480e-03, -5.6974e-03, -3.7265e-02,  7.7061e-02,  5.1464e-02,\n",
      "        -7.0224e-02, -4.4164e-02,  2.5564e-02,  1.2461e-02, -2.4537e-02,\n",
      "         2.2466e-02,  6.7765e-03, -2.1143e-02,  1.3173e-02, -4.8422e-02,\n",
      "        -2.4130e-02,  4.0795e-02, -6.9050e-02,  5.2960e-02,  2.9344e-02,\n",
      "         6.1323e-02,  2.6642e-02, -1.5501e-02,  1.1257e-02,  5.2199e-02,\n",
      "        -1.9131e-02, -7.1120e-02,  1.5206e-01, -5.5123e-02,  1.6600e-02,\n",
      "        -1.7471e-02,  5.4039e-02,  7.3465e-02, -1.4534e-02,  3.2988e-02,\n",
      "         1.0805e-01,  2.3235e-03,  2.6146e-02,  5.6207e-02,  2.4650e-02,\n",
      "         1.0190e-02, -4.5924e-03,  4.1432e-02, -4.8620e-02, -2.9034e-02,\n",
      "        -2.9012e-02,  1.4155e-02,  3.5942e-02, -9.4590e-03, -3.9627e-02,\n",
      "        -5.3268e-02,  1.3831e-01, -3.0257e-02, -5.7423e-03,  4.2466e-02,\n",
      "         1.2649e-01, -5.0767e-02, -1.1174e-02, -2.3112e-02,  3.8812e-02,\n",
      "        -6.3522e-02,  9.1453e-02,  2.6309e-02, -1.1686e-01, -3.9759e-02,\n",
      "         2.4578e-02, -4.7622e-03, -5.6869e-02,  9.6072e-02,  1.3556e-02,\n",
      "        -2.8459e-02, -4.5581e-02,  1.2914e-01, -1.1633e-02,  1.1193e-01,\n",
      "        -8.6753e-02, -8.5673e-03, -7.3127e-02, -3.6154e-02, -9.3040e-02,\n",
      "        -3.7462e-02,  1.2344e-01,  8.0146e-02, -1.7490e-02,  1.1924e-01,\n",
      "        -1.0738e-02,  6.7925e-02, -6.9445e-02, -2.5708e-02, -5.6665e-02,\n",
      "        -1.5419e-01,  1.2431e-01, -7.5615e-03, -1.0575e-01,  8.1955e-02,\n",
      "        -3.7937e-02,  8.6439e-02, -3.1533e-03,  1.4085e-01,  3.6980e-02,\n",
      "        -1.3440e-02, -5.1998e-02,  5.9634e-02, -4.4400e-02,  1.6468e-02,\n",
      "         3.7003e-02,  2.0843e-02,  4.8651e-02, -3.7829e-02,  1.0212e-01,\n",
      "        -1.8587e-02,  4.5990e-02, -4.5087e-03, -1.0517e-01, -7.8714e-02,\n",
      "        -2.2157e-02, -5.8386e-02,  7.0721e-02, -1.4240e-02, -1.0749e-01,\n",
      "        -6.8921e-02, -3.1443e-02, -3.2220e-02, -6.4972e-02,  1.1256e-02,\n",
      "         4.3494e-02,  1.8916e-02, -1.8547e-01, -2.1113e-02, -3.5792e-02,\n",
      "        -1.2145e-02,  4.6165e-02, -1.1010e-01,  3.3331e-04,  8.4547e-02,\n",
      "         5.4524e-02,  4.8118e-02, -9.5097e-02, -7.2445e-02, -6.6263e-05,\n",
      "         5.1787e-02,  4.9852e-02, -4.7932e-02, -1.2280e-02, -1.6250e-02,\n",
      "        -1.4342e-02, -1.1116e-01, -5.5778e-02, -7.7247e-03, -8.1662e-02,\n",
      "        -4.3206e-03,  6.6698e-02, -5.0373e-02, -1.2831e-01,  7.0735e-02,\n",
      "        -4.0484e-02, -2.6315e-02, -2.7391e-02, -8.0403e-02, -6.9732e-03,\n",
      "         5.4342e-02,  2.0656e-02,  1.5141e-01,  1.0275e-01,  1.5837e-03,\n",
      "        -1.4563e-01,  8.5911e-05,  4.7454e-03, -7.8300e-02,  4.8858e-02,\n",
      "        -2.1546e-02,  1.4427e-02,  4.6923e-02, -4.1582e-02,  3.4860e-02,\n",
      "         1.6094e-01, -2.8653e-02,  6.8671e-02,  3.9210e-02, -2.7989e-02,\n",
      "         1.2157e-01,  3.4874e-02,  1.0473e-01,  5.0698e-02, -6.6427e-02,\n",
      "        -8.5859e-02,  4.0868e-02, -8.1263e-02,  1.2227e-04, -4.1179e-02,\n",
      "         7.0834e-03,  8.5109e-02, -2.0567e-02,  6.0143e-03, -8.9583e-02,\n",
      "         6.3068e-02, -4.5089e-02,  2.6703e-02,  5.3511e-03,  9.8072e-03,\n",
      "         9.1949e-04,  4.8803e-02, -1.2944e-02, -1.6477e-02,  3.7466e-03,\n",
      "        -7.1968e-02, -6.9599e-02, -1.0072e-01, -7.0090e-02,  3.5817e-02,\n",
      "         6.2147e-02,  8.6350e-02,  8.2676e-02,  6.9734e-03, -1.6660e-01,\n",
      "         3.0636e-02, -7.5360e-02,  8.7070e-02,  4.6590e-02, -1.2240e-02,\n",
      "         4.7421e-02,  1.4499e-01, -3.2117e-02,  6.7256e-03, -9.1146e-03,\n",
      "         5.6627e-02,  3.4365e-02,  3.5674e-02,  1.1961e-03,  9.1195e-03,\n",
      "        -1.0258e-01, -2.6809e-02, -3.6439e-02, -5.3987e-02, -3.7285e-02,\n",
      "        -4.7299e-02,  2.0322e-02, -7.9408e-02, -7.7213e-02, -4.1219e-02,\n",
      "         1.1305e-01, -3.6860e-02,  3.4759e-02,  4.5197e-03, -1.8849e-02,\n",
      "        -1.1627e-02,  7.8283e-02, -5.6437e-02,  3.5024e-02,  6.2222e-02,\n",
      "        -8.2901e-02,  7.1049e-02,  9.9048e-03,  8.3881e-02,  3.7555e-03,\n",
      "         8.8532e-02,  9.2635e-02,  1.6246e-02, -3.0551e-02,  4.0173e-02,\n",
      "         3.9328e-02,  9.8969e-03,  7.2826e-04, -8.5527e-03,  1.9672e-02,\n",
      "         1.0268e-01, -4.0752e-03, -5.5843e-02,  1.5902e-02,  7.0855e-03,\n",
      "        -3.0325e-02,  2.9130e-02, -7.9757e-02,  2.0168e-02,  1.3599e-02,\n",
      "        -2.4822e-02, -8.0696e-03,  7.8805e-03,  3.1998e-04, -3.3752e-02,\n",
      "        -2.3653e-02,  7.4149e-02, -9.0394e-03, -6.5222e-03, -3.0573e-02,\n",
      "         1.1063e-01,  7.5828e-02,  4.1677e-02,  1.3911e-02, -7.0996e-03,\n",
      "         2.3597e-03,  2.6949e-03, -5.3042e-03,  7.1347e-02,  2.7978e-02,\n",
      "         9.5793e-04, -2.3873e-02, -7.2959e-02,  3.1148e-02, -6.5378e-02,\n",
      "         4.4773e-02, -4.6407e-02, -2.7808e-02,  6.0678e-02,  2.2824e-02,\n",
      "         1.2299e-02, -1.2252e-01, -9.4176e-02, -3.1335e-02,  6.1090e-02,\n",
      "        -8.9544e-02, -7.8463e-02, -1.0646e-01,  1.2856e-01,  5.3371e-02,\n",
      "        -3.5043e-02,  4.9204e-02, -2.7718e-02, -1.8169e-03, -3.2086e-02,\n",
      "         7.7823e-03,  6.8141e-03,  9.3693e-02,  1.6695e-02, -7.0995e-03,\n",
      "        -8.1406e-02, -1.0529e-02,  2.3930e-02, -2.4667e-02,  1.4599e-02,\n",
      "         2.2815e-02,  6.4431e-02, -8.6203e-02, -1.9157e-01,  3.7300e-02,\n",
      "        -2.8549e-02, -2.9900e-02,  2.0874e-02, -1.8929e-01,  6.7435e-02,\n",
      "        -4.1862e-02,  4.9628e-04,  7.5833e-03,  8.0471e-02, -1.7851e-02,\n",
      "        -4.5390e-02,  2.1833e-02, -1.6886e-02, -1.0043e-02, -7.4905e-02,\n",
      "        -9.9795e-04, -2.0626e-02,  8.3278e-02, -7.4464e-02,  3.2107e-02,\n",
      "         4.9412e-02, -5.9202e-02, -6.2015e-02,  1.0825e-02,  8.4142e-02,\n",
      "         6.0584e-02,  2.8453e-02, -6.4364e-02,  3.4312e-02, -3.1387e-02,\n",
      "        -1.0054e-02,  6.2364e-02,  9.9319e-02,  4.8268e-02,  3.6428e-02,\n",
      "         4.7602e-02, -2.9711e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 1.7651e-03,  3.1396e-03, -1.4551e-02, -3.5147e-03, -2.1400e-02,\n",
      "         1.1861e-01,  2.7863e-02, -5.9323e-02,  5.4408e-02, -3.9685e-02,\n",
      "        -2.9819e-02,  1.3290e-02, -3.3680e-02, -3.0514e-02, -9.5577e-02,\n",
      "        -2.7275e-02, -3.2411e-02,  1.0115e-01,  1.8278e-02,  5.4167e-02,\n",
      "        -6.5277e-02, -4.9623e-02,  3.8521e-02, -7.7113e-02,  2.3679e-02,\n",
      "        -2.1421e-02, -3.1328e-02, -3.8675e-02,  3.2301e-02,  8.0595e-02,\n",
      "        -1.7031e-01, -3.1086e-02,  7.4435e-02,  4.4086e-02, -5.2982e-02,\n",
      "        -3.7618e-02, -2.1757e-02,  2.6901e-02,  1.2416e-02, -6.1151e-02,\n",
      "        -5.6076e-03, -8.1322e-02,  1.2883e-01,  2.1244e-01,  6.3023e-03,\n",
      "        -6.4486e-02, -6.8903e-02, -5.2496e-02, -8.8419e-03, -4.8330e-03,\n",
      "        -9.8466e-02, -9.1724e-02, -4.6670e-03,  2.2265e-02,  7.4199e-03,\n",
      "        -6.7150e-03,  3.1992e-03, -1.9731e-02,  1.7806e-02, -7.7666e-03,\n",
      "         6.6665e-03, -4.9659e-03,  1.3266e-02, -3.1188e-02,  6.3222e-02,\n",
      "         2.4398e-02,  2.9437e-02, -7.7957e-04,  2.4054e-02,  1.6580e-01,\n",
      "        -7.9211e-02, -2.8934e-02,  3.4830e-02,  3.1386e-02,  1.2014e-03,\n",
      "         9.4098e-02, -5.5012e-03,  4.5756e-02,  3.2991e-02,  1.8693e-02,\n",
      "         4.4928e-02, -2.1605e-02,  4.0092e-02, -6.9511e-02, -8.6237e-02,\n",
      "        -1.2794e-01,  2.8559e-02, -3.4364e-02,  2.3834e-03,  9.2352e-03,\n",
      "        -2.0991e-03,  1.2794e-02,  1.0197e-02, -1.4751e-02, -5.3813e-03,\n",
      "         2.9286e-02,  8.8126e-02,  1.5448e-02, -1.4078e-02, -3.9143e-02,\n",
      "        -5.8560e-02,  5.4407e-02,  3.5490e-02, -7.9659e-02,  3.4453e-02,\n",
      "         2.5864e-02, -3.0899e-02, -1.5625e-02, -4.5447e-02,  4.7464e-02,\n",
      "        -3.1091e-02, -3.4445e-02, -5.4052e-02, -6.4918e-02,  4.4487e-02,\n",
      "         2.5045e-02, -1.1488e-02,  2.5262e-02, -1.2607e-02,  1.3235e-02,\n",
      "         2.8561e-02,  6.9778e-02,  3.5717e-02, -1.3796e-02, -1.6055e-01,\n",
      "        -6.3508e-02,  3.0388e-02,  3.6702e-02,  1.4510e-02,  8.2649e-02,\n",
      "        -9.6217e-03,  2.8959e-02,  3.8684e-02, -8.4300e-02, -1.5368e-01,\n",
      "         9.8709e-02, -7.2473e-02,  3.1997e-02,  1.1817e-01, -2.6140e-02,\n",
      "        -6.1742e-02, -1.6166e-02,  7.0216e-02, -1.2530e-01, -3.3601e-02,\n",
      "         1.8504e-02,  4.9253e-02,  1.5496e-01, -7.7431e-02, -1.4273e-02,\n",
      "        -1.3381e-02,  1.0467e-01, -7.3973e-02, -9.8395e-02, -2.9553e-02,\n",
      "         4.8231e-02,  6.4982e-02, -5.0469e-02,  3.5893e-02,  9.4489e-02,\n",
      "         6.2196e-02, -9.2381e-02, -8.7598e-02,  7.9401e-02, -6.6444e-02,\n",
      "        -1.0009e-02, -3.8275e-02, -2.5270e-02, -1.7952e-01, -9.5267e-03,\n",
      "        -1.3783e-01,  2.1312e-01, -1.1740e-02, -8.2986e-02,  3.5087e-02,\n",
      "        -1.9155e-02, -2.4328e-02, -4.0487e-02,  3.3686e-02, -1.7021e-02,\n",
      "        -5.0354e-02, -1.5596e-01, -1.7125e-03,  5.6674e-02,  6.6230e-03,\n",
      "         6.4058e-03, -3.7337e-03,  1.1259e-02, -2.4012e-02,  8.4532e-02,\n",
      "        -2.1994e-02,  3.6341e-03,  8.1102e-02, -5.8442e-02,  9.7022e-02,\n",
      "        -6.0901e-02,  5.0808e-02,  1.3352e-01,  1.6406e-02,  1.3148e-02,\n",
      "         2.8686e-02, -3.0704e-02, -4.3113e-02,  5.2098e-02, -5.5051e-02,\n",
      "        -1.1791e-01,  5.0002e-02,  2.3706e-03, -6.4074e-02,  5.0139e-02,\n",
      "        -3.7592e-02,  5.3099e-02,  3.9144e-02,  4.3691e-03,  1.4775e-02,\n",
      "        -7.3321e-02, -4.6698e-02,  1.2764e-01, -6.2895e-02, -2.6595e-02,\n",
      "         7.9530e-02,  3.6950e-02, -4.7796e-03,  3.2136e-02, -4.4875e-02,\n",
      "        -3.2131e-02,  8.3086e-02,  8.9513e-02, -6.2051e-03, -1.2118e-01,\n",
      "         2.6485e-02, -3.3139e-02,  4.4756e-02,  7.8008e-04,  7.1055e-02,\n",
      "         3.0050e-02,  8.2575e-03, -2.6538e-02, -3.9907e-02, -2.5800e-02,\n",
      "        -3.3800e-02,  1.8517e-02, -7.0688e-02, -1.3011e-01, -3.3101e-02,\n",
      "        -5.4424e-02,  3.0215e-02, -6.2839e-02,  2.4651e-02, -1.8812e-03,\n",
      "        -1.3442e-01,  1.2847e-02,  7.9453e-02,  8.0802e-02, -9.5993e-02,\n",
      "         3.4160e-02,  2.6102e-02, -8.6553e-03,  5.7268e-02,  8.5350e-02,\n",
      "         1.3918e-02,  1.1504e-02,  2.9779e-03,  1.0623e-02,  5.5536e-02,\n",
      "        -4.1146e-02, -9.3039e-02, -3.3455e-03,  1.5882e-02, -1.5050e-01,\n",
      "         7.5856e-03,  2.2823e-02, -3.8871e-02,  5.5844e-02,  5.4641e-03,\n",
      "        -2.4733e-02, -5.1179e-02, -1.8616e-02,  5.5658e-02, -6.9583e-02,\n",
      "        -6.0925e-02, -8.0161e-02, -1.0143e-01,  4.3837e-02,  1.3554e-01,\n",
      "         8.7156e-02,  2.5922e-02, -7.2726e-02, -1.8920e-02,  9.7482e-02,\n",
      "         2.0591e-02, -6.2224e-02,  5.4904e-02, -1.3960e-01, -7.6254e-02,\n",
      "         8.3799e-02, -3.9226e-02, -4.3723e-02, -3.3469e-02,  9.1810e-03,\n",
      "         4.9622e-02,  6.3080e-02, -2.8480e-02, -1.8700e-02,  6.6885e-02,\n",
      "        -6.8625e-03,  7.1043e-02,  7.1088e-02, -9.2783e-02,  9.1262e-02,\n",
      "         4.6247e-02, -2.9005e-02,  2.8690e-02,  1.9394e-02,  5.7164e-05,\n",
      "         2.2624e-02,  3.3163e-02,  1.7700e-02,  3.4232e-02, -2.9858e-02,\n",
      "        -7.4267e-02,  3.6014e-02, -4.4552e-02,  3.5258e-02, -1.0101e-01,\n",
      "        -6.7129e-03,  1.4119e-02,  2.7532e-02,  1.8333e-02,  1.0998e-01,\n",
      "        -4.3879e-04,  6.3078e-02,  1.9749e-02,  4.5188e-02,  1.7698e-02,\n",
      "        -1.6677e-02,  8.2497e-02, -7.5923e-02,  6.3407e-02,  6.3229e-02,\n",
      "         1.7209e-02,  8.9937e-02, -3.1758e-02,  2.4061e-02, -7.6937e-02,\n",
      "         2.9163e-03, -6.6448e-02, -1.3663e-02, -3.8498e-02, -6.1970e-02,\n",
      "        -5.3004e-02,  2.5560e-02,  1.7372e-01,  1.9347e-02,  7.7611e-02,\n",
      "         1.2019e-01, -1.5177e-01, -1.0369e-02, -3.0696e-02,  6.5096e-02,\n",
      "         1.3015e-02,  5.4550e-02, -5.5283e-02,  7.5891e-03, -2.0863e-02,\n",
      "        -2.2272e-02,  1.8210e-02, -6.6587e-03, -1.3865e-02,  5.7003e-02,\n",
      "        -1.9093e-02,  9.1872e-03,  9.9067e-02,  3.3590e-04,  4.0905e-02,\n",
      "         2.1044e-03, -6.7002e-03,  2.9374e-02, -1.1736e-02,  3.6019e-03,\n",
      "        -2.4367e-02, -3.7626e-02, -1.1231e-01,  1.7375e-02, -6.0035e-03,\n",
      "         5.7686e-02, -2.7193e-02,  1.9783e-02, -6.3263e-02,  2.2237e-02,\n",
      "         7.3779e-03, -2.8759e-03, -1.5603e-02,  5.7662e-02,  8.7457e-03,\n",
      "         1.0018e-02, -5.0072e-02, -3.7638e-02,  2.4585e-02, -9.2793e-02,\n",
      "        -1.1872e-01, -2.2116e-02, -1.0956e-01, -1.0836e-01,  8.6403e-02,\n",
      "         4.3467e-02,  2.0700e-02,  5.1945e-02,  3.0060e-02,  2.7744e-02,\n",
      "        -9.2273e-03,  7.5827e-02, -4.5446e-02,  8.2869e-02, -9.2931e-02,\n",
      "         1.2670e-02, -4.8179e-02, -1.5450e-01, -1.6038e-03, -2.9253e-02,\n",
      "        -2.7980e-02, -1.0475e-02,  2.7516e-02,  1.6998e-01,  2.4017e-02,\n",
      "         8.4535e-02, -2.9163e-04,  3.1187e-02,  5.4309e-02, -3.0479e-02,\n",
      "        -8.0611e-02, -6.6498e-02, -1.4551e-01,  2.1430e-03, -3.7552e-02,\n",
      "         7.6690e-02,  7.4113e-02, -1.0557e-01, -7.4909e-02,  6.7211e-02,\n",
      "        -7.8306e-02,  4.8829e-02, -4.6191e-02,  1.3408e-02,  2.7609e-02,\n",
      "        -7.1487e-03, -3.2693e-03,  6.9174e-02,  1.8630e-01,  8.2350e-02,\n",
      "        -7.1999e-02, -5.7636e-04, -1.0190e-01, -2.1849e-02, -2.4579e-02,\n",
      "         8.7751e-02, -3.5942e-02, -2.4704e-03, -1.1202e-01, -7.7516e-02,\n",
      "        -3.0877e-02,  5.2970e-02, -1.0476e-02,  9.5842e-03, -7.3720e-02,\n",
      "         4.0108e-02, -1.2442e-02,  3.8677e-02, -4.2649e-02,  3.2528e-02,\n",
      "         4.7383e-02, -1.2851e-03, -3.1630e-02,  9.8758e-02, -4.5205e-02,\n",
      "         9.8402e-02, -9.2297e-02, -1.9997e-02, -1.7744e-02, -2.2326e-02,\n",
      "         8.0307e-02, -1.7815e-02,  1.9394e-02, -5.2028e-02, -5.1993e-02,\n",
      "         5.9033e-03,  1.0825e-02, -1.7139e-02, -1.4043e-01,  3.3729e-02,\n",
      "         2.4079e-02,  3.1476e-02, -7.7750e-02,  4.4037e-03, -7.0054e-02,\n",
      "        -1.0412e-02,  8.7667e-03,  6.4475e-02, -6.7967e-02,  4.3379e-02,\n",
      "        -8.0798e-02,  1.3300e-01, -2.5715e-02,  4.1997e-02,  1.5607e-02,\n",
      "        -1.8457e-02, -1.4307e-02, -1.3592e-02, -8.4850e-04,  6.9601e-03,\n",
      "         1.7143e-02, -7.7591e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 7.0232e-02, -3.9177e-02, -5.6618e-02,  2.4083e-02, -1.1208e-01,\n",
      "        -8.1959e-02, -1.0443e-02,  3.7170e-02, -5.0199e-02,  3.4944e-02,\n",
      "        -1.1758e-01,  2.1176e-02, -1.3711e-02,  1.7198e-03,  5.5355e-03,\n",
      "        -2.0753e-02, -4.9122e-02, -1.6634e-02,  8.6761e-04,  5.3412e-02,\n",
      "        -5.2480e-02,  3.0198e-03, -2.5701e-02, -1.2355e-01,  5.1328e-02,\n",
      "        -6.1343e-02, -4.0117e-02,  6.6909e-02,  5.3694e-03,  2.3511e-02,\n",
      "        -8.8978e-03, -1.3987e-02,  5.8384e-02, -7.4027e-02,  7.0214e-03,\n",
      "        -2.3619e-02, -1.1647e-02,  5.0248e-03, -1.0472e-01,  5.0924e-02,\n",
      "         9.6729e-03,  4.4740e-02, -8.4304e-03,  3.1834e-02, -5.6028e-02,\n",
      "         1.2579e-02,  4.1046e-02, -7.6398e-02, -8.2215e-02, -7.4826e-02,\n",
      "        -5.8703e-03,  2.1899e-02, -3.4924e-02, -9.3841e-02,  9.2489e-02,\n",
      "        -3.9724e-02,  6.8778e-02,  6.2910e-02,  1.5035e-01, -8.5688e-02,\n",
      "         5.1889e-02, -9.3605e-02, -7.0402e-02,  4.7219e-02,  5.9798e-02,\n",
      "        -3.6312e-03, -1.3177e-02, -4.6579e-02,  2.6072e-02, -1.8031e-02,\n",
      "        -1.5455e-01,  1.6608e-01, -1.5167e-03, -2.2081e-02, -3.3239e-02,\n",
      "         7.1615e-03,  5.0772e-02,  6.4464e-03, -1.0717e-03,  1.1329e-01,\n",
      "        -4.0795e-03,  7.9883e-02, -4.3044e-02,  1.3580e-01, -1.0705e-02,\n",
      "         7.0666e-03, -7.1443e-03,  9.1426e-02, -1.3554e-03, -9.4658e-02,\n",
      "        -4.0040e-02,  5.9643e-02,  1.8720e-02, -6.1085e-03, -5.1143e-03,\n",
      "         5.2426e-03,  3.9795e-02,  5.7733e-02,  9.3336e-02,  4.6847e-03,\n",
      "         5.8702e-02,  2.5341e-02,  4.2893e-02,  7.5947e-02,  2.8520e-04,\n",
      "         7.1536e-03, -4.3884e-03, -4.4555e-02, -4.4503e-02,  5.6192e-02,\n",
      "        -5.1656e-02, -1.2393e-01,  4.3672e-02,  4.7996e-02, -1.0800e-02,\n",
      "         5.6755e-02, -8.3568e-02, -1.3538e-02, -5.6153e-02, -5.5316e-02,\n",
      "        -2.3975e-02, -1.1282e-01, -2.8566e-02,  7.2766e-02, -3.8624e-02,\n",
      "         7.6615e-02,  3.6164e-02,  1.0354e-01,  4.9160e-02,  1.9378e-02,\n",
      "        -2.2329e-02, -1.2350e-01,  1.2831e-01,  9.7161e-03,  8.3806e-02,\n",
      "        -5.0945e-02, -2.3909e-02, -2.4867e-02,  5.3618e-02,  4.3033e-02,\n",
      "        -8.6281e-03, -3.7764e-02, -1.2432e-01,  1.3901e-02, -8.2746e-02,\n",
      "         1.5292e-02, -1.0102e-01, -2.1163e-03,  2.4047e-02, -3.3842e-02,\n",
      "         1.7279e-01, -2.0493e-03, -1.4493e-02,  5.7667e-02, -2.8942e-02,\n",
      "        -3.2882e-03,  7.1961e-02,  1.5763e-02, -1.0857e-01,  3.1682e-02,\n",
      "        -1.5458e-02,  2.3903e-02, -7.8493e-02,  3.3385e-02, -1.1762e-02,\n",
      "         5.4726e-02, -1.0496e-01, -1.9116e-02,  4.4039e-02, -4.5159e-02,\n",
      "         1.1691e-01, -7.5459e-02, -3.4751e-02, -7.0932e-05, -5.4284e-03,\n",
      "        -3.1645e-02,  7.8052e-02, -1.3927e-02, -3.9138e-02, -6.9432e-02,\n",
      "        -5.6814e-02,  4.7092e-02, -9.7913e-02, -7.1706e-02, -7.4354e-02,\n",
      "         2.9061e-02,  1.2788e-01,  3.2878e-02,  6.8620e-02,  7.8050e-03,\n",
      "        -8.1034e-03,  1.2591e-01, -2.5306e-02,  2.3245e-02,  6.0525e-03,\n",
      "         5.1102e-02,  2.6583e-02, -2.1282e-03, -5.5411e-02,  4.6495e-02,\n",
      "        -2.4725e-02,  2.2852e-02, -1.2736e-02,  1.6637e-01, -5.4719e-02,\n",
      "         8.6107e-02, -5.4407e-02,  6.8237e-02, -6.1891e-02, -5.5849e-02,\n",
      "         7.3760e-03, -3.0345e-02, -3.1600e-02,  3.3583e-02,  2.8570e-02,\n",
      "         8.2200e-02,  2.6655e-02,  2.6249e-02, -1.2001e-02,  7.8356e-02,\n",
      "        -1.6183e-02, -1.4890e-02,  1.2511e-02,  3.7454e-02,  2.5717e-02,\n",
      "         2.4392e-03,  1.9375e-02,  6.4533e-02,  3.3817e-02, -6.6789e-02,\n",
      "        -8.1340e-02, -3.5166e-02, -2.8866e-02, -7.5490e-02,  3.9034e-02,\n",
      "        -4.9257e-02,  1.5981e-02,  1.2176e-02,  2.2973e-02,  1.0207e-02,\n",
      "        -1.0285e-03,  1.7862e-01,  6.4228e-02, -3.5339e-02,  8.1926e-02,\n",
      "         7.1711e-02, -1.0528e-02,  3.6034e-02, -2.3140e-02,  4.6343e-02,\n",
      "        -3.3368e-02, -4.6355e-02,  5.1168e-02,  1.8313e-02, -3.8195e-03,\n",
      "         1.0237e-01, -4.0303e-02,  3.3172e-02, -4.5773e-02, -1.4106e-02,\n",
      "        -3.1364e-02,  5.1665e-02, -3.1724e-02,  3.0433e-02, -3.9412e-02,\n",
      "        -3.3040e-02,  2.1146e-02, -1.1771e-01, -6.6739e-02, -3.3981e-02,\n",
      "        -4.4390e-03,  2.8506e-02,  1.9362e-02,  1.0839e-01, -1.5109e-02,\n",
      "         1.5135e-01, -2.9912e-02,  7.0132e-02, -6.2905e-02, -9.2045e-02,\n",
      "        -1.0811e-01, -6.4596e-02,  1.1569e-01,  4.2324e-02, -5.3588e-02,\n",
      "         2.2440e-02, -2.2649e-02, -7.6581e-02, -6.1811e-02,  6.7117e-02,\n",
      "        -8.8734e-02,  1.1926e-02, -1.0264e-02,  2.1893e-02,  6.1756e-02,\n",
      "         1.1959e-01, -9.6380e-02,  1.3470e-02, -7.0965e-02,  2.2478e-02,\n",
      "         5.0166e-02, -4.6788e-03,  9.3105e-02,  1.2183e-01, -1.0024e-01,\n",
      "         1.9777e-04,  8.1114e-02, -2.6921e-02,  1.0334e-01, -3.6504e-02,\n",
      "         1.0802e-02, -2.5081e-02, -6.5181e-02,  8.6339e-02,  3.7305e-02,\n",
      "        -1.2546e-01, -2.3171e-02, -5.1505e-02,  8.1840e-02,  4.9002e-02,\n",
      "         1.8363e-02,  2.9693e-02, -3.8902e-03, -4.1257e-02, -2.2935e-02,\n",
      "         8.3203e-02,  5.9329e-02,  7.7033e-03,  4.9673e-02, -3.4751e-02,\n",
      "        -3.4831e-03, -7.7208e-03,  1.0457e-01, -2.1170e-02,  6.3125e-02,\n",
      "        -1.2047e-02,  1.4499e-02, -5.0847e-02,  2.7684e-02,  8.1270e-02,\n",
      "        -2.4067e-02,  1.6061e-04,  4.5172e-02,  8.9830e-02,  5.0638e-03,\n",
      "        -2.7056e-02,  1.6215e-02, -1.2409e-01,  2.7129e-02, -3.0758e-02,\n",
      "        -4.1683e-02,  7.6068e-03,  1.4988e-02, -2.3955e-02, -8.0970e-02,\n",
      "        -1.0192e-01,  3.6965e-02, -2.6476e-02,  5.7144e-03,  7.6527e-02,\n",
      "         9.9065e-02,  4.3809e-02,  5.6087e-02, -6.8878e-02,  6.4834e-02,\n",
      "         2.2787e-02,  7.3976e-02,  9.9496e-03, -1.7695e-02,  8.8900e-02,\n",
      "        -9.6980e-02, -7.0818e-02,  4.9335e-02, -5.5873e-02, -6.7333e-03,\n",
      "        -1.0160e-02,  2.7102e-02, -2.2473e-02, -4.0724e-02, -4.3555e-02,\n",
      "        -4.3084e-02, -8.1544e-02, -2.8473e-02,  1.8932e-02,  2.8450e-02,\n",
      "        -8.1699e-02, -8.1030e-02,  4.9583e-02,  3.5871e-02, -1.5891e-02,\n",
      "        -8.8298e-03, -1.2130e-02, -8.1447e-02,  4.5123e-02,  6.6769e-02,\n",
      "         4.5007e-02, -1.0901e-02, -1.5257e-01,  1.2816e-02,  6.9188e-02,\n",
      "        -1.3537e-02,  1.0406e-01,  2.4015e-02,  4.3749e-02, -2.3074e-02,\n",
      "         6.2925e-02, -3.2508e-02, -5.4690e-02,  1.6847e-02, -5.7822e-02,\n",
      "         6.0195e-02, -1.6088e-02, -3.6611e-02,  4.3164e-03, -1.6129e-02,\n",
      "        -2.0027e-02,  2.7187e-02, -5.7546e-02,  1.6556e-02, -4.5320e-04,\n",
      "         2.4637e-02, -5.7647e-02, -4.5837e-02,  1.4810e-02,  1.4818e-02,\n",
      "        -9.2751e-03,  3.1316e-02,  4.6298e-02,  1.5679e-02,  2.5335e-02,\n",
      "         1.5162e-02, -6.5274e-02, -1.1448e-01,  3.5900e-02, -1.1034e-01,\n",
      "        -9.4011e-02,  3.3696e-02, -6.7059e-03,  1.4441e-02,  1.3973e-01,\n",
      "         7.2340e-02, -5.2067e-02, -1.5580e-02,  4.3312e-02, -6.7398e-02,\n",
      "         7.6808e-02, -4.1142e-02,  3.2319e-02,  1.2461e-01,  1.5610e-02,\n",
      "         7.3369e-02, -1.0851e-01, -4.5686e-02, -6.5544e-02,  7.0161e-02,\n",
      "        -4.9590e-03,  4.6399e-02,  4.5816e-02, -7.6833e-02,  5.7388e-02,\n",
      "         5.6216e-02,  1.7794e-02, -1.8920e-02, -4.4150e-02,  2.6347e-02,\n",
      "         8.7239e-02, -2.0536e-02, -1.2006e-02, -5.0354e-03,  3.5649e-02,\n",
      "        -8.1056e-02,  5.1311e-02,  1.9925e-02, -4.3425e-02,  2.6601e-02,\n",
      "        -7.5502e-02, -3.4638e-02, -7.5277e-02, -5.1211e-02, -4.9907e-02,\n",
      "         1.9271e-02,  2.3710e-02,  1.7192e-02, -7.7708e-02,  2.5729e-02,\n",
      "         5.5325e-02,  1.0182e-01, -9.2568e-02, -4.8824e-02,  2.3749e-02,\n",
      "         3.6623e-02, -1.6246e-02, -2.5600e-02, -8.7405e-02,  1.7550e-02,\n",
      "        -6.1699e-03, -4.0138e-02, -3.5954e-02, -6.4890e-02,  4.1684e-03,\n",
      "        -8.0014e-02, -7.6652e-02,  9.0478e-02,  2.2696e-03,  4.3178e-03,\n",
      "         1.3625e-01, -4.3848e-02,  3.4243e-02,  1.0695e-01,  2.2553e-02,\n",
      "        -1.3336e-02, -3.6943e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-6.3037e-02,  4.0266e-02, -4.1982e-02, -1.3677e-01, -2.8125e-02,\n",
      "        -3.5907e-02, -1.2441e-01,  3.9413e-02,  7.0257e-02,  2.5938e-02,\n",
      "         1.6168e-02, -4.9772e-03, -8.1783e-02,  5.8682e-02, -5.5678e-03,\n",
      "        -1.6609e-02, -7.0250e-02,  2.3007e-02, -1.1237e-01, -1.1362e-03,\n",
      "         2.6813e-02,  5.1206e-02, -1.0730e-01, -7.5571e-03,  6.9360e-02,\n",
      "        -3.0615e-02, -1.3997e-01, -1.6990e-02, -6.2863e-03,  3.9499e-02,\n",
      "         1.1053e-01,  7.3368e-02, -4.2755e-03, -6.1965e-02,  2.3791e-02,\n",
      "        -5.2535e-02, -2.8180e-02, -1.5272e-02, -2.4229e-02,  8.8722e-02,\n",
      "        -9.6024e-02,  4.9923e-02, -1.7209e-02,  1.6388e-02, -7.5840e-03,\n",
      "        -8.1901e-02,  6.1073e-02, -4.8348e-02, -1.0459e-02, -6.6470e-02,\n",
      "        -2.4781e-02,  5.3203e-02,  2.4020e-02,  4.9423e-02,  3.7947e-02,\n",
      "         2.0673e-01,  6.0895e-02, -4.0758e-02,  2.0071e-02,  1.0387e-01,\n",
      "         1.6163e-02, -1.5090e-02,  6.2854e-02,  2.6011e-02,  3.8561e-02,\n",
      "        -5.5162e-02, -7.5800e-02,  6.2836e-02,  1.3994e-02,  1.1939e-01,\n",
      "        -1.5908e-02,  4.2999e-02, -2.3383e-02,  1.5983e-02, -1.3671e-02,\n",
      "         9.9919e-02,  6.4056e-02, -7.5082e-02, -1.2190e-02, -1.4694e-02,\n",
      "         5.8069e-02,  5.5209e-02,  1.2079e-02,  5.1134e-02,  4.5578e-02,\n",
      "         5.2929e-02, -6.7412e-03,  9.7755e-02, -4.7786e-02, -1.6850e-02,\n",
      "        -5.9766e-02,  9.2122e-02, -2.9754e-02, -1.1698e-01,  2.3706e-02,\n",
      "        -2.3814e-02,  3.3031e-02, -1.1580e-01,  3.8596e-02, -3.3136e-03,\n",
      "        -1.2250e-02,  3.4611e-02, -8.4193e-02, -7.5750e-02, -3.5521e-02,\n",
      "        -5.1473e-03,  3.9007e-02,  6.4325e-03, -5.9280e-02, -1.3100e-02,\n",
      "        -4.1139e-02,  4.7848e-02,  8.4264e-03, -1.0753e-01, -4.3760e-02,\n",
      "         9.4994e-02, -1.7219e-02,  3.9596e-02, -4.1659e-02,  1.2531e-01,\n",
      "        -4.9070e-02,  1.2569e-02, -3.4510e-02,  5.6004e-02, -2.7773e-02,\n",
      "        -8.0413e-02,  7.7013e-02, -3.7365e-02, -7.8601e-02, -4.4590e-02,\n",
      "         1.6158e-02,  2.7064e-02,  1.0510e-01, -1.2408e-02,  1.6963e-02,\n",
      "        -9.1978e-03,  5.7486e-02, -4.6821e-02, -3.0573e-03, -1.0964e-02,\n",
      "        -8.9452e-02,  4.2682e-02, -1.1941e-02,  2.5132e-02, -3.7705e-02,\n",
      "         5.4186e-02, -7.1975e-02, -4.9173e-02, -6.7192e-02,  2.7494e-02,\n",
      "         2.4167e-03,  3.7371e-02,  4.2284e-02,  3.3118e-02,  5.1909e-02,\n",
      "        -6.6921e-02, -5.8869e-02, -6.1932e-02,  3.1455e-02, -2.2885e-02,\n",
      "        -9.3647e-02, -1.9637e-02,  5.1098e-02,  4.5610e-02, -4.1068e-02,\n",
      "         5.7816e-02, -8.5963e-04,  2.2186e-02, -1.8173e-02,  4.3025e-02,\n",
      "        -3.6500e-02,  4.6611e-02,  1.1417e-01, -6.0109e-02, -6.6532e-02,\n",
      "         9.2543e-02,  1.5739e-02, -7.0260e-03, -4.5298e-02, -4.6085e-02,\n",
      "        -1.7641e-02, -3.4245e-02, -2.9982e-02, -3.3564e-02, -2.3251e-02,\n",
      "        -9.0132e-02, -4.9113e-02, -1.5003e-02, -3.4544e-02, -1.2240e-02,\n",
      "        -6.6013e-02, -1.2225e-01,  2.1974e-02, -7.2869e-02,  7.3213e-02,\n",
      "         7.8171e-02, -1.1407e-02,  1.2900e-02,  1.3423e-02,  6.1885e-02,\n",
      "         8.2777e-02,  5.9639e-03, -2.9608e-02,  1.4335e-02, -3.0911e-02,\n",
      "        -2.6568e-02, -7.7970e-02,  5.7262e-02,  7.5148e-03, -8.3736e-02,\n",
      "         1.1164e-01, -3.6595e-02, -3.5647e-02, -2.2155e-02,  3.7071e-02,\n",
      "         5.2191e-03, -5.0187e-02, -1.0465e-02, -2.7389e-02,  2.4710e-02,\n",
      "         3.4442e-02, -3.3596e-02,  5.7857e-02,  4.2296e-02, -2.8121e-02,\n",
      "         3.7366e-02, -5.9914e-02,  1.6653e-02,  3.8050e-02,  5.3976e-02,\n",
      "         1.6561e-02,  5.0949e-02,  7.7352e-02,  8.3561e-02, -4.3670e-02,\n",
      "        -8.8957e-03,  2.0743e-03,  3.0768e-02, -3.4656e-02,  1.0132e-01,\n",
      "         2.0802e-02, -1.4734e-01, -1.1625e-02, -4.6762e-03,  1.0868e-01,\n",
      "         8.2071e-02, -1.4927e-02, -1.5449e-01, -7.1360e-02,  6.3504e-02,\n",
      "        -1.3678e-02, -3.2650e-02,  8.5200e-02, -4.5086e-02,  2.2611e-02,\n",
      "        -1.0392e-01, -6.0944e-02,  1.4738e-02,  3.9227e-02, -8.7592e-03,\n",
      "        -2.2234e-02, -5.5263e-03, -3.3027e-02,  3.9625e-03,  1.5417e-02,\n",
      "         1.2909e-02,  1.0592e-01, -5.5637e-02,  1.6255e-01, -8.2178e-02,\n",
      "         9.2043e-02,  1.9381e-03,  2.2714e-02,  3.5822e-02, -1.0901e-03,\n",
      "         1.2325e-02, -6.4859e-02, -2.5885e-02,  5.1314e-02, -4.6941e-04,\n",
      "        -2.8895e-03,  1.1293e-02, -1.7513e-02, -6.6949e-02,  6.9416e-02,\n",
      "         7.1142e-03, -1.4641e-03, -3.6779e-02,  1.1385e-01, -4.7641e-02,\n",
      "         1.4738e-02, -6.2718e-02,  8.7415e-02, -5.5629e-03,  2.7129e-02,\n",
      "         6.3722e-03,  3.4799e-02,  2.5760e-02, -7.6286e-02, -6.1321e-02,\n",
      "        -5.3081e-02, -1.3048e-03, -1.7442e-02, -1.6667e-01,  3.7299e-03,\n",
      "         1.3328e-02,  6.2362e-02,  1.6265e-02,  5.9280e-02, -9.6899e-02,\n",
      "        -9.9530e-03,  3.4732e-02,  5.4185e-03, -4.3835e-03,  3.4801e-02,\n",
      "         4.0341e-02, -1.1303e-02, -2.8805e-02,  2.6510e-02, -4.8988e-02,\n",
      "        -1.4906e-02, -8.7503e-02, -3.8591e-03,  3.9093e-02,  2.1345e-02,\n",
      "         4.3803e-02, -4.8825e-02, -3.8691e-02, -7.1864e-02, -5.9994e-02,\n",
      "         2.5898e-02, -4.4769e-02,  8.8324e-02, -7.2772e-02,  1.5155e-02,\n",
      "        -5.5817e-02,  5.3736e-02, -2.9101e-02,  1.5793e-03, -1.7930e-01,\n",
      "        -1.7445e-02, -6.8678e-02, -2.1378e-02, -4.4950e-02, -1.7106e-02,\n",
      "         1.5411e-01,  7.0336e-02,  3.1394e-02,  9.1400e-02, -6.3379e-02,\n",
      "         9.3097e-02,  6.2873e-02, -2.3895e-02,  4.8823e-02,  1.5050e-02,\n",
      "         1.5749e-01,  2.0483e-02,  2.5478e-02,  1.2565e-01,  6.4963e-02,\n",
      "        -3.3720e-02,  3.8453e-02, -6.7775e-02, -1.1753e-01,  6.8093e-02,\n",
      "         5.1249e-02, -1.5064e-01, -6.5369e-02,  4.8224e-02, -8.1458e-03,\n",
      "        -2.7762e-02, -2.5249e-02, -1.0149e-02, -1.9384e-02,  4.1005e-02,\n",
      "        -2.7609e-02, -9.2976e-02,  3.8276e-02,  7.2089e-02, -1.2936e-01,\n",
      "        -1.1778e-01, -6.5505e-02,  1.7166e-02,  1.5751e-02, -1.9162e-02,\n",
      "         5.5185e-03, -1.0558e-01, -2.3025e-02, -1.4394e-01,  1.1885e-01,\n",
      "         9.7875e-03, -9.7859e-02, -3.9622e-02, -4.5969e-02, -4.3369e-02,\n",
      "        -2.5617e-02, -5.2712e-02,  3.9468e-02,  1.0800e-01,  5.3185e-02,\n",
      "        -5.0451e-02,  5.3125e-02,  1.4214e-01,  1.0340e-01, -1.7702e-02,\n",
      "        -6.3901e-02,  3.0720e-02, -7.3908e-02,  9.5226e-02,  6.2002e-03,\n",
      "         5.0914e-02, -6.5561e-02,  5.4568e-02,  5.1027e-02, -4.2785e-02,\n",
      "        -7.9318e-02,  6.1157e-02,  6.2453e-02, -4.5603e-02, -2.7345e-02,\n",
      "        -5.6974e-02,  1.2981e-01,  1.0213e-01,  4.7302e-02, -2.4651e-02,\n",
      "        -3.3669e-02, -4.9926e-02,  7.3012e-02, -3.4709e-02,  1.2907e-01,\n",
      "         6.1702e-02,  3.1375e-02,  1.9113e-02, -9.1100e-02,  7.3931e-03,\n",
      "        -8.0293e-02, -3.6101e-02,  5.1210e-02, -2.9621e-02,  5.9973e-03,\n",
      "         9.6392e-02,  4.1492e-03,  2.3054e-02, -8.5028e-02,  1.3075e-03,\n",
      "        -8.0786e-02,  7.1889e-02, -3.7784e-02,  2.0823e-02, -5.1179e-02,\n",
      "         1.3547e-01,  4.0677e-02,  6.0206e-02, -4.4987e-03, -1.4705e-02,\n",
      "        -4.3924e-05,  2.2686e-02, -3.4385e-02, -3.4656e-02, -1.7687e-01,\n",
      "         4.2150e-02, -3.2622e-03, -4.4221e-02, -5.9327e-02, -1.2178e-01,\n",
      "        -9.8243e-02,  2.9285e-02,  1.0800e-01, -2.9136e-02, -1.2633e-02,\n",
      "        -8.9605e-02,  1.0191e-02,  2.9528e-02, -1.6184e-02, -2.1323e-02,\n",
      "         4.3191e-02, -5.9493e-02, -9.2964e-02, -2.2478e-02,  1.4769e-02,\n",
      "        -2.1768e-02,  4.5379e-02,  7.2459e-02,  6.8969e-02, -3.1864e-02,\n",
      "         9.2427e-03,  1.1675e-01, -1.2651e-02, -8.6167e-02, -7.0927e-02,\n",
      "        -3.2216e-02, -3.6091e-02,  1.1292e-02, -2.3667e-02,  1.0530e-01,\n",
      "        -2.7349e-02,  3.5006e-02, -8.4804e-02,  5.3443e-02, -3.9848e-02,\n",
      "        -1.8628e-02, -9.9607e-02, -1.0862e-01,  3.0266e-03,  6.8604e-02,\n",
      "        -2.6277e-02, -1.4869e-01, -4.6595e-02, -9.2243e-02,  5.9800e-02,\n",
      "         1.6846e-02, -5.4605e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss: 62.738426/1821.375000, val:  45.42%, val_best:  45.42%, tr:  37.49%, tr_best:  37.49%, epoch time: 28.88 seconds, 0.48 minutes\n",
      "[module.layers.3] weight_fb parameter count: 5,120\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss: 65.383629/2452.451416, val:  33.33%, val_best:  45.42%, tr:  47.50%, tr_best:  47.50%, epoch time: 28.99 seconds, 0.48 minutes\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss: 67.537392/2209.712158, val:  45.83%, val_best:  45.83%, tr:  51.28%, tr_best:  51.28%, epoch time: 29.68 seconds, 0.49 minutes\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss: 61.489803/1610.889648, val:  47.50%, val_best:  47.50%, tr:  54.55%, tr_best:  54.55%, epoch time: 30.04 seconds, 0.50 minutes\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss: 63.533501/1934.234985, val:  55.00%, val_best:  55.00%, tr:  54.95%, tr_best:  54.95%, epoch time: 28.68 seconds, 0.48 minutes\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss: 61.991997/1854.006470, val:  50.83%, val_best:  55.00%, tr:  56.69%, tr_best:  56.69%, epoch time: 29.48 seconds, 0.49 minutes\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss: 62.164146/3012.527832, val:  45.42%, val_best:  55.00%, tr:  56.59%, tr_best:  56.69%, epoch time: 29.90 seconds, 0.50 minutes\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss: 56.692177/2622.473145, val:  50.00%, val_best:  55.00%, tr:  59.55%, tr_best:  59.55%, epoch time: 29.83 seconds, 0.50 minutes\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss: 60.951477/1966.080811, val:  52.92%, val_best:  55.00%, tr:  56.49%, tr_best:  59.55%, epoch time: 28.68 seconds, 0.48 minutes\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss: 55.857075/1514.692383, val:  50.42%, val_best:  55.00%, tr:  61.59%, tr_best:  61.59%, epoch time: 29.66 seconds, 0.49 minutes\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss: 54.648094/3368.646729, val:  44.17%, val_best:  55.00%, tr:  62.31%, tr_best:  62.31%, epoch time: 29.45 seconds, 0.49 minutes\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss: 55.280163/2204.046875, val:  61.67%, val_best:  61.67%, tr:  58.32%, tr_best:  62.31%, epoch time: 29.41 seconds, 0.49 minutes\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss: 50.560326/1937.330444, val:  55.83%, val_best:  61.67%, tr:  64.04%, tr_best:  64.04%, epoch time: 29.61 seconds, 0.49 minutes\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss: 52.838577/2325.660156, val:  51.67%, val_best:  61.67%, tr:  63.13%, tr_best:  64.04%, epoch time: 29.66 seconds, 0.49 minutes\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss: 48.892048/2019.259277, val:  58.75%, val_best:  61.67%, tr:  64.76%, tr_best:  64.76%, epoch time: 29.36 seconds, 0.49 minutes\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss: 51.464512/2211.790771, val:  50.00%, val_best:  61.67%, tr:  63.94%, tr_best:  64.76%, epoch time: 29.56 seconds, 0.49 minutes\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss: 50.387596/1878.547852, val:  62.50%, val_best:  62.50%, tr:  64.76%, tr_best:  64.76%, epoch time: 29.35 seconds, 0.49 minutes\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss: 50.520546/2049.309082, val:  57.08%, val_best:  62.50%, tr:  65.47%, tr_best:  65.47%, epoch time: 30.37 seconds, 0.51 minutes\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss: 50.387436/2086.796387, val:  57.08%, val_best:  62.50%, tr:  63.43%, tr_best:  65.47%, epoch time: 29.82 seconds, 0.50 minutes\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss: 48.384407/2047.314697, val:  56.67%, val_best:  62.50%, tr:  65.47%, tr_best:  65.47%, epoch time: 28.57 seconds, 0.48 minutes\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss: 44.184135/2416.333252, val:  56.25%, val_best:  62.50%, tr:  67.42%, tr_best:  67.42%, epoch time: 29.76 seconds, 0.50 minutes\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss: 47.726696/1253.427612, val:  57.08%, val_best:  62.50%, tr:  66.80%, tr_best:  67.42%, epoch time: 29.28 seconds, 0.49 minutes\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss: 49.224400/1836.284546, val:  64.58%, val_best:  64.58%, tr:  65.68%, tr_best:  67.42%, epoch time: 29.49 seconds, 0.49 minutes\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss: 43.962074/2153.920898, val:  59.58%, val_best:  64.58%, tr:  68.03%, tr_best:  68.03%, epoch time: 28.73 seconds, 0.48 minutes\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss: 47.259392/1851.988281, val:  60.42%, val_best:  64.58%, tr:  66.39%, tr_best:  68.03%, epoch time: 30.33 seconds, 0.51 minutes\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss: 46.201466/1631.649048, val:  57.50%, val_best:  64.58%, tr:  66.39%, tr_best:  68.03%, epoch time: 29.08 seconds, 0.48 minutes\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss: 41.765583/1988.942139, val:  57.08%, val_best:  64.58%, tr:  68.95%, tr_best:  68.95%, epoch time: 28.73 seconds, 0.48 minutes\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss: 44.914921/1527.738892, val:  61.67%, val_best:  64.58%, tr:  67.72%, tr_best:  68.95%, epoch time: 29.12 seconds, 0.49 minutes\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss: 42.941891/2200.545410, val:  60.00%, val_best:  64.58%, tr:  69.77%, tr_best:  69.77%, epoch time: 28.70 seconds, 0.48 minutes\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss: 39.506836/1997.217651, val:  59.58%, val_best:  64.58%, tr:  69.66%, tr_best:  69.77%, epoch time: 29.42 seconds, 0.49 minutes\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss: 42.568584/2265.639404, val:  54.58%, val_best:  64.58%, tr:  67.82%, tr_best:  69.77%, epoch time: 29.04 seconds, 0.48 minutes\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss: 38.847305/2553.602783, val:  51.25%, val_best:  64.58%, tr:  69.56%, tr_best:  69.77%, epoch time: 29.05 seconds, 0.48 minutes\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss: 39.988392/2557.669189, val:  57.92%, val_best:  64.58%, tr:  69.97%, tr_best:  69.97%, epoch time: 29.54 seconds, 0.49 minutes\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss: 38.278156/2213.696777, val:  56.67%, val_best:  64.58%, tr:  72.11%, tr_best:  72.11%, epoch time: 29.60 seconds, 0.49 minutes\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss: 39.308372/2017.687866, val:  54.58%, val_best:  64.58%, tr:  70.38%, tr_best:  72.11%, epoch time: 29.24 seconds, 0.49 minutes\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss: 39.533852/1523.710693, val:  59.58%, val_best:  64.58%, tr:  69.87%, tr_best:  72.11%, epoch time: 28.49 seconds, 0.47 minutes\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss: 37.675900/1672.650513, val:  59.17%, val_best:  64.58%, tr:  70.28%, tr_best:  72.11%, epoch time: 29.51 seconds, 0.49 minutes\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss: 38.245682/2155.488281, val:  58.33%, val_best:  64.58%, tr:  71.40%, tr_best:  72.11%, epoch time: 28.85 seconds, 0.48 minutes\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss: 41.092110/1540.684448, val:  62.08%, val_best:  64.58%, tr:  69.97%, tr_best:  72.11%, epoch time: 28.68 seconds, 0.48 minutes\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss: 37.335880/1841.040771, val:  53.33%, val_best:  64.58%, tr:  70.58%, tr_best:  72.11%, epoch time: 30.04 seconds, 0.50 minutes\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss: 35.194069/2877.303711, val:  57.50%, val_best:  64.58%, tr:  73.44%, tr_best:  73.44%, epoch time: 28.85 seconds, 0.48 minutes\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss: 37.954319/2452.258057, val:  54.17%, val_best:  64.58%, tr:  71.30%, tr_best:  73.44%, epoch time: 29.83 seconds, 0.50 minutes\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss: 34.659657/2677.537842, val:  60.42%, val_best:  64.58%, tr:  74.26%, tr_best:  74.26%, epoch time: 29.17 seconds, 0.49 minutes\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss: 36.585236/1970.272705, val:  57.92%, val_best:  64.58%, tr:  72.73%, tr_best:  74.26%, epoch time: 28.77 seconds, 0.48 minutes\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss: 34.358910/1640.855835, val:  61.67%, val_best:  64.58%, tr:  73.85%, tr_best:  74.26%, epoch time: 29.28 seconds, 0.49 minutes\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss: 38.677147/1022.340759, val:  69.17%, val_best:  69.17%, tr:  72.83%, tr_best:  74.26%, epoch time: 28.97 seconds, 0.48 minutes\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss: 34.194283/1570.265015, val:  58.75%, val_best:  69.17%, tr:  73.54%, tr_best:  74.26%, epoch time: 29.79 seconds, 0.50 minutes\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss: 35.513702/1425.799438, val:  64.17%, val_best:  69.17%, tr:  74.26%, tr_best:  74.26%, epoch time: 29.43 seconds, 0.49 minutes\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss: 36.302750/995.854919, val:  69.17%, val_best:  69.17%, tr:  74.26%, tr_best:  74.26%, epoch time: 30.47 seconds, 0.51 minutes\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss: 32.748539/1268.621948, val:  62.08%, val_best:  69.17%, tr:  73.95%, tr_best:  74.26%, epoch time: 29.55 seconds, 0.49 minutes\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss: 34.112335/2061.182617, val:  60.00%, val_best:  69.17%, tr:  74.06%, tr_best:  74.26%, epoch time: 28.28 seconds, 0.47 minutes\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss: 29.437616/1724.861938, val:  60.83%, val_best:  69.17%, tr:  75.89%, tr_best:  75.89%, epoch time: 29.01 seconds, 0.48 minutes\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss: 30.649502/1858.099609, val:  54.17%, val_best:  69.17%, tr:  74.26%, tr_best:  75.89%, epoch time: 28.91 seconds, 0.48 minutes\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss: 30.224201/1601.558105, val:  60.42%, val_best:  69.17%, tr:  74.97%, tr_best:  75.89%, epoch time: 30.44 seconds, 0.51 minutes\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss: 30.686001/2046.395386, val:  52.92%, val_best:  69.17%, tr:  76.71%, tr_best:  76.71%, epoch time: 29.78 seconds, 0.50 minutes\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss: 35.593548/1548.830444, val:  60.83%, val_best:  69.17%, tr:  71.91%, tr_best:  76.71%, epoch time: 30.18 seconds, 0.50 minutes\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss: 34.271515/2332.460938, val:  61.25%, val_best:  69.17%, tr:  73.95%, tr_best:  76.71%, epoch time: 28.64 seconds, 0.48 minutes\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss: 32.146694/1934.928711, val:  61.67%, val_best:  69.17%, tr:  74.67%, tr_best:  76.71%, epoch time: 28.85 seconds, 0.48 minutes\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss: 30.650921/1247.759399, val:  65.00%, val_best:  69.17%, tr:  76.20%, tr_best:  76.71%, epoch time: 28.94 seconds, 0.48 minutes\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss: 32.472584/1628.963257, val:  64.17%, val_best:  69.17%, tr:  75.18%, tr_best:  76.71%, epoch time: 29.59 seconds, 0.49 minutes\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss: 32.152359/2212.715088, val:  55.00%, val_best:  69.17%, tr:  76.20%, tr_best:  76.71%, epoch time: 28.73 seconds, 0.48 minutes\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss: 31.029568/1767.342407, val:  65.83%, val_best:  69.17%, tr:  74.77%, tr_best:  76.71%, epoch time: 29.69 seconds, 0.49 minutes\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss: 33.490993/1065.780029, val:  66.25%, val_best:  69.17%, tr:  74.16%, tr_best:  76.71%, epoch time: 29.38 seconds, 0.49 minutes\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss: 29.840893/1049.802734, val:  67.08%, val_best:  69.17%, tr:  77.02%, tr_best:  77.02%, epoch time: 29.07 seconds, 0.48 minutes\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss: 28.814636/1602.947510, val:  59.58%, val_best:  69.17%, tr:  76.71%, tr_best:  77.02%, epoch time: 29.65 seconds, 0.49 minutes\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss: 28.384815/1078.512207, val:  66.25%, val_best:  69.17%, tr:  76.30%, tr_best:  77.02%, epoch time: 29.15 seconds, 0.49 minutes\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss: 31.182283/1397.680786, val:  62.50%, val_best:  69.17%, tr:  74.97%, tr_best:  77.02%, epoch time: 29.18 seconds, 0.49 minutes\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss: 30.443226/1664.078979, val:  56.67%, val_best:  69.17%, tr:  77.22%, tr_best:  77.22%, epoch time: 30.21 seconds, 0.50 minutes\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss: 28.631208/1669.115723, val:  63.33%, val_best:  69.17%, tr:  77.53%, tr_best:  77.53%, epoch time: 29.53 seconds, 0.49 minutes\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss: 31.487890/1287.549805, val:  62.50%, val_best:  69.17%, tr:  76.92%, tr_best:  77.53%, epoch time: 29.43 seconds, 0.49 minutes\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss: 29.019966/1455.781128, val:  62.92%, val_best:  69.17%, tr:  77.43%, tr_best:  77.53%, epoch time: 28.88 seconds, 0.48 minutes\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss: 28.153267/1899.725586, val:  58.33%, val_best:  69.17%, tr:  77.53%, tr_best:  77.53%, epoch time: 31.02 seconds, 0.52 minutes\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss: 29.614567/1366.218018, val:  65.83%, val_best:  69.17%, tr:  77.32%, tr_best:  77.53%, epoch time: 29.34 seconds, 0.49 minutes\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss: 29.562452/1739.588501, val:  62.08%, val_best:  69.17%, tr:  76.81%, tr_best:  77.53%, epoch time: 28.46 seconds, 0.47 minutes\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss: 30.868223/975.673279, val:  68.75%, val_best:  69.17%, tr:  76.00%, tr_best:  77.53%, epoch time: 29.66 seconds, 0.49 minutes\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss: 30.759277/1651.109863, val:  65.42%, val_best:  69.17%, tr:  76.92%, tr_best:  77.53%, epoch time: 29.04 seconds, 0.48 minutes\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss: 27.136562/2216.214844, val:  60.83%, val_best:  69.17%, tr:  79.47%, tr_best:  79.47%, epoch time: 28.88 seconds, 0.48 minutes\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss: 26.780602/3292.028076, val:  57.50%, val_best:  69.17%, tr:  77.32%, tr_best:  79.47%, epoch time: 29.04 seconds, 0.48 minutes\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss: 25.593672/1377.848389, val:  66.67%, val_best:  69.17%, tr:  77.83%, tr_best:  79.47%, epoch time: 29.62 seconds, 0.49 minutes\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss: 25.975748/1360.610474, val:  67.50%, val_best:  69.17%, tr:  77.43%, tr_best:  79.47%, epoch time: 29.43 seconds, 0.49 minutes\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss: 28.062183/1305.746582, val:  65.83%, val_best:  69.17%, tr:  78.45%, tr_best:  79.47%, epoch time: 28.73 seconds, 0.48 minutes\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss: 28.198851/1906.161499, val:  62.50%, val_best:  69.17%, tr:  78.14%, tr_best:  79.47%, epoch time: 30.54 seconds, 0.51 minutes\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss: 24.926283/2130.319580, val:  54.17%, val_best:  69.17%, tr:  79.26%, tr_best:  79.47%, epoch time: 28.90 seconds, 0.48 minutes\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss: 30.077219/1946.072998, val:  62.08%, val_best:  69.17%, tr:  76.92%, tr_best:  79.47%, epoch time: 29.80 seconds, 0.50 minutes\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss: 27.113815/2149.594482, val:  60.42%, val_best:  69.17%, tr:  78.45%, tr_best:  79.47%, epoch time: 29.39 seconds, 0.49 minutes\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss: 25.939173/1641.203613, val:  55.42%, val_best:  69.17%, tr:  78.96%, tr_best:  79.47%, epoch time: 29.18 seconds, 0.49 minutes\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss: 24.804625/1447.464722, val:  61.25%, val_best:  69.17%, tr:  80.39%, tr_best:  80.39%, epoch time: 29.78 seconds, 0.50 minutes\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss: 26.556614/2004.742554, val:  56.67%, val_best:  69.17%, tr:  78.45%, tr_best:  80.39%, epoch time: 29.52 seconds, 0.49 minutes\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss: 29.185968/1824.821899, val:  60.83%, val_best:  69.17%, tr:  77.22%, tr_best:  80.39%, epoch time: 29.42 seconds, 0.49 minutes\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss: 26.932308/1796.901367, val:  60.00%, val_best:  69.17%, tr:  76.51%, tr_best:  80.39%, epoch time: 29.30 seconds, 0.49 minutes\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss: 25.417091/1239.052734, val:  65.42%, val_best:  69.17%, tr:  78.45%, tr_best:  80.39%, epoch time: 30.47 seconds, 0.51 minutes\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss: 25.613789/1608.067139, val:  64.58%, val_best:  69.17%, tr:  79.98%, tr_best:  80.39%, epoch time: 30.03 seconds, 0.50 minutes\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss: 26.843170/1129.035156, val:  70.83%, val_best:  70.83%, tr:  79.47%, tr_best:  80.39%, epoch time: 28.84 seconds, 0.48 minutes\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss: 22.935125/1606.246094, val:  62.08%, val_best:  70.83%, tr:  79.57%, tr_best:  80.39%, epoch time: 29.97 seconds, 0.50 minutes\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss: 26.638742/2318.723389, val:  58.75%, val_best:  70.83%, tr:  77.94%, tr_best:  80.39%, epoch time: 29.23 seconds, 0.49 minutes\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss: 27.371822/1795.589844, val:  60.83%, val_best:  70.83%, tr:  78.65%, tr_best:  80.39%, epoch time: 28.67 seconds, 0.48 minutes\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss: 26.758530/2878.204102, val:  56.25%, val_best:  70.83%, tr:  78.45%, tr_best:  80.39%, epoch time: 29.49 seconds, 0.49 minutes\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss: 23.817188/1172.752075, val:  71.67%, val_best:  71.67%, tr:  80.18%, tr_best:  80.39%, epoch time: 28.79 seconds, 0.48 minutes\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss: 25.993546/1575.387451, val:  65.00%, val_best:  71.67%, tr:  79.88%, tr_best:  80.39%, epoch time: 30.41 seconds, 0.51 minutes\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss: 24.801949/1643.445923, val:  60.83%, val_best:  71.67%, tr:  79.57%, tr_best:  80.39%, epoch time: 29.51 seconds, 0.49 minutes\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss: 24.580864/1343.868774, val:  63.33%, val_best:  71.67%, tr:  78.86%, tr_best:  80.39%, epoch time: 29.28 seconds, 0.49 minutes\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss: 23.438595/1846.352783, val:  62.08%, val_best:  71.67%, tr:  79.98%, tr_best:  80.39%, epoch time: 29.73 seconds, 0.50 minutes\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss: 24.747389/1593.267578, val:  62.08%, val_best:  71.67%, tr:  79.06%, tr_best:  80.39%, epoch time: 29.17 seconds, 0.49 minutes\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss: 20.729649/1832.670288, val:  61.25%, val_best:  71.67%, tr:  83.15%, tr_best:  83.15%, epoch time: 30.42 seconds, 0.51 minutes\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss: 25.229685/1225.164429, val:  64.17%, val_best:  71.67%, tr:  79.16%, tr_best:  83.15%, epoch time: 29.87 seconds, 0.50 minutes\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss: 23.631556/1552.226196, val:  66.25%, val_best:  71.67%, tr:  79.78%, tr_best:  83.15%, epoch time: 29.76 seconds, 0.50 minutes\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss: 25.634335/1435.449463, val:  60.42%, val_best:  71.67%, tr:  79.88%, tr_best:  83.15%, epoch time: 29.95 seconds, 0.50 minutes\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss: 22.838112/1515.535400, val:  66.67%, val_best:  71.67%, tr:  81.31%, tr_best:  83.15%, epoch time: 28.74 seconds, 0.48 minutes\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss: 21.651827/1991.701172, val:  58.75%, val_best:  71.67%, tr:  81.72%, tr_best:  83.15%, epoch time: 29.89 seconds, 0.50 minutes\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss: 20.715473/1905.011963, val:  62.50%, val_best:  71.67%, tr:  81.61%, tr_best:  83.15%, epoch time: 28.76 seconds, 0.48 minutes\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss: 22.814140/1806.574097, val:  65.42%, val_best:  71.67%, tr:  79.26%, tr_best:  83.15%, epoch time: 29.07 seconds, 0.48 minutes\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss: 23.826384/2077.449707, val:  59.17%, val_best:  71.67%, tr:  80.08%, tr_best:  83.15%, epoch time: 29.86 seconds, 0.50 minutes\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss: 24.354691/1203.536865, val:  64.58%, val_best:  71.67%, tr:  79.67%, tr_best:  83.15%, epoch time: 29.11 seconds, 0.49 minutes\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss: 22.497766/1898.868652, val:  62.92%, val_best:  71.67%, tr:  81.41%, tr_best:  83.15%, epoch time: 28.81 seconds, 0.48 minutes\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss: 20.257427/1139.503662, val:  68.75%, val_best:  71.67%, tr:  82.43%, tr_best:  83.15%, epoch time: 28.72 seconds, 0.48 minutes\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss: 20.458820/916.490967, val:  69.17%, val_best:  71.67%, tr:  80.80%, tr_best:  83.15%, epoch time: 30.25 seconds, 0.50 minutes\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss: 21.511269/1946.723633, val:  60.83%, val_best:  71.67%, tr:  81.31%, tr_best:  83.15%, epoch time: 29.25 seconds, 0.49 minutes\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss: 21.765465/1146.125244, val:  75.00%, val_best:  75.00%, tr:  81.92%, tr_best:  83.15%, epoch time: 29.44 seconds, 0.49 minutes\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss: 26.495592/1598.684937, val:  68.75%, val_best:  75.00%, tr:  78.75%, tr_best:  83.15%, epoch time: 30.11 seconds, 0.50 minutes\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss: 24.357611/1525.757812, val:  61.67%, val_best:  75.00%, tr:  79.67%, tr_best:  83.15%, epoch time: 29.64 seconds, 0.49 minutes\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss: 25.214211/1868.565186, val:  59.58%, val_best:  75.00%, tr:  80.08%, tr_best:  83.15%, epoch time: 29.86 seconds, 0.50 minutes\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss: 20.369425/1726.503296, val:  62.50%, val_best:  75.00%, tr:  82.74%, tr_best:  83.15%, epoch time: 28.81 seconds, 0.48 minutes\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss: 22.026243/1298.374878, val:  67.50%, val_best:  75.00%, tr:  80.08%, tr_best:  83.15%, epoch time: 29.82 seconds, 0.50 minutes\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss: 24.511032/951.273193, val:  74.17%, val_best:  75.00%, tr:  79.78%, tr_best:  83.15%, epoch time: 29.19 seconds, 0.49 minutes\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss: 19.924585/1404.816895, val:  64.17%, val_best:  75.00%, tr:  82.43%, tr_best:  83.15%, epoch time: 28.23 seconds, 0.47 minutes\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss: 20.720579/1395.915405, val:  65.42%, val_best:  75.00%, tr:  81.21%, tr_best:  83.15%, epoch time: 29.37 seconds, 0.49 minutes\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss: 24.163549/1544.368774, val:  60.00%, val_best:  75.00%, tr:  79.37%, tr_best:  83.15%, epoch time: 29.33 seconds, 0.49 minutes\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss: 19.706446/1534.654541, val:  62.92%, val_best:  75.00%, tr:  83.04%, tr_best:  83.15%, epoch time: 30.05 seconds, 0.50 minutes\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss: 21.156828/2378.492432, val:  62.92%, val_best:  75.00%, tr:  82.64%, tr_best:  83.15%, epoch time: 29.41 seconds, 0.49 minutes\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss: 19.431097/1341.513916, val:  70.00%, val_best:  75.00%, tr:  82.43%, tr_best:  83.15%, epoch time: 30.75 seconds, 0.51 minutes\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss: 21.132486/1579.410889, val:  63.75%, val_best:  75.00%, tr:  82.33%, tr_best:  83.15%, epoch time: 29.91 seconds, 0.50 minutes\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss: 18.262993/1507.490723, val:  62.92%, val_best:  75.00%, tr:  83.45%, tr_best:  83.45%, epoch time: 28.67 seconds, 0.48 minutes\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss: 20.284935/1865.644043, val:  62.92%, val_best:  75.00%, tr:  81.31%, tr_best:  83.45%, epoch time: 29.53 seconds, 0.49 minutes\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss: 20.244787/1497.681396, val:  66.67%, val_best:  75.00%, tr:  81.51%, tr_best:  83.45%, epoch time: 29.44 seconds, 0.49 minutes\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss: 20.128117/2132.556396, val:  56.67%, val_best:  75.00%, tr:  82.23%, tr_best:  83.45%, epoch time: 28.69 seconds, 0.48 minutes\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss: 20.030878/2403.223877, val:  62.50%, val_best:  75.00%, tr:  81.21%, tr_best:  83.45%, epoch time: 29.55 seconds, 0.49 minutes\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss: 21.356314/1347.734863, val:  62.92%, val_best:  75.00%, tr:  82.12%, tr_best:  83.45%, epoch time: 29.38 seconds, 0.49 minutes\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss: 19.129536/2122.090820, val:  61.25%, val_best:  75.00%, tr:  83.35%, tr_best:  83.45%, epoch time: 30.41 seconds, 0.51 minutes\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss: 21.896482/1231.196533, val:  67.50%, val_best:  75.00%, tr:  81.10%, tr_best:  83.45%, epoch time: 29.72 seconds, 0.50 minutes\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss: 20.033350/1291.860718, val:  68.33%, val_best:  75.00%, tr:  79.88%, tr_best:  83.45%, epoch time: 28.78 seconds, 0.48 minutes\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss: 22.863811/1313.269165, val:  68.75%, val_best:  75.00%, tr:  81.82%, tr_best:  83.45%, epoch time: 29.67 seconds, 0.49 minutes\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss: 19.382874/1816.458496, val:  61.25%, val_best:  75.00%, tr:  84.07%, tr_best:  84.07%, epoch time: 28.54 seconds, 0.48 minutes\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss: 20.493032/1289.666382, val:  71.67%, val_best:  75.00%, tr:  83.04%, tr_best:  84.07%, epoch time: 29.56 seconds, 0.49 minutes\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss: 18.480076/1688.119141, val:  60.42%, val_best:  75.00%, tr:  82.64%, tr_best:  84.07%, epoch time: 29.63 seconds, 0.49 minutes\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss: 18.011177/1600.423462, val:  67.92%, val_best:  75.00%, tr:  83.66%, tr_best:  84.07%, epoch time: 28.53 seconds, 0.48 minutes\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss: 19.015991/1171.041748, val:  65.83%, val_best:  75.00%, tr:  82.64%, tr_best:  84.07%, epoch time: 29.35 seconds, 0.49 minutes\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss: 17.636641/1800.824341, val:  61.67%, val_best:  75.00%, tr:  85.70%, tr_best:  85.70%, epoch time: 29.43 seconds, 0.49 minutes\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss: 18.525743/1772.926392, val:  61.25%, val_best:  75.00%, tr:  84.07%, tr_best:  85.70%, epoch time: 29.45 seconds, 0.49 minutes\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss: 19.182467/1879.401123, val:  63.33%, val_best:  75.00%, tr:  83.15%, tr_best:  85.70%, epoch time: 29.62 seconds, 0.49 minutes\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss: 17.897459/1146.127319, val:  68.33%, val_best:  75.00%, tr:  84.68%, tr_best:  85.70%, epoch time: 29.17 seconds, 0.49 minutes\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss: 18.339731/1441.684570, val:  70.00%, val_best:  75.00%, tr:  83.86%, tr_best:  85.70%, epoch time: 29.70 seconds, 0.50 minutes\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss: 19.609455/1561.845459, val:  63.75%, val_best:  75.00%, tr:  83.96%, tr_best:  85.70%, epoch time: 28.73 seconds, 0.48 minutes\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss: 17.643801/2295.583496, val:  62.50%, val_best:  75.00%, tr:  83.04%, tr_best:  85.70%, epoch time: 29.19 seconds, 0.49 minutes\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss: 19.322687/1953.591064, val:  65.83%, val_best:  75.00%, tr:  82.94%, tr_best:  85.70%, epoch time: 31.08 seconds, 0.52 minutes\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss: 18.662014/1593.816895, val:  70.42%, val_best:  75.00%, tr:  83.76%, tr_best:  85.70%, epoch time: 28.96 seconds, 0.48 minutes\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss: 20.984478/1350.696045, val:  68.33%, val_best:  75.00%, tr:  81.61%, tr_best:  85.70%, epoch time: 30.23 seconds, 0.50 minutes\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss: 17.278103/1243.890137, val:  72.08%, val_best:  75.00%, tr:  84.27%, tr_best:  85.70%, epoch time: 29.83 seconds, 0.50 minutes\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss: 18.965460/1336.876465, val:  68.75%, val_best:  75.00%, tr:  83.04%, tr_best:  85.70%, epoch time: 29.59 seconds, 0.49 minutes\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss: 20.446590/1558.264038, val:  66.25%, val_best:  75.00%, tr:  82.02%, tr_best:  85.70%, epoch time: 28.71 seconds, 0.48 minutes\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss: 17.794327/1097.744873, val:  71.67%, val_best:  75.00%, tr:  83.45%, tr_best:  85.70%, epoch time: 29.41 seconds, 0.49 minutes\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss: 17.572765/1053.714355, val:  71.67%, val_best:  75.00%, tr:  86.01%, tr_best:  86.01%, epoch time: 29.73 seconds, 0.50 minutes\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss: 19.604322/2094.887207, val:  63.33%, val_best:  75.00%, tr:  82.02%, tr_best:  86.01%, epoch time: 29.39 seconds, 0.49 minutes\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss: 16.876902/1217.762085, val:  69.17%, val_best:  75.00%, tr:  83.76%, tr_best:  86.01%, epoch time: 29.39 seconds, 0.49 minutes\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss: 18.688416/1412.435303, val:  70.42%, val_best:  75.00%, tr:  84.68%, tr_best:  86.01%, epoch time: 29.74 seconds, 0.50 minutes\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss: 17.638630/1955.925781, val:  62.92%, val_best:  75.00%, tr:  84.37%, tr_best:  86.01%, epoch time: 29.70 seconds, 0.50 minutes\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss: 18.656828/1585.441406, val:  63.33%, val_best:  75.00%, tr:  83.15%, tr_best:  86.01%, epoch time: 29.05 seconds, 0.48 minutes\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss: 15.640604/1301.429810, val:  72.08%, val_best:  75.00%, tr:  84.58%, tr_best:  86.01%, epoch time: 29.33 seconds, 0.49 minutes\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss: 18.352558/1423.116455, val:  64.17%, val_best:  75.00%, tr:  84.78%, tr_best:  86.01%, epoch time: 28.95 seconds, 0.48 minutes\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss: 20.777308/1392.937866, val:  67.92%, val_best:  75.00%, tr:  81.82%, tr_best:  86.01%, epoch time: 30.17 seconds, 0.50 minutes\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss: 18.880424/1308.099731, val:  68.33%, val_best:  75.00%, tr:  84.07%, tr_best:  86.01%, epoch time: 28.86 seconds, 0.48 minutes\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss: 16.297609/2715.012939, val:  61.67%, val_best:  75.00%, tr:  84.58%, tr_best:  86.01%, epoch time: 29.27 seconds, 0.49 minutes\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss: 18.211456/1634.394043, val:  63.33%, val_best:  75.00%, tr:  84.88%, tr_best:  86.01%, epoch time: 29.93 seconds, 0.50 minutes\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss: 21.028692/1901.843750, val:  60.00%, val_best:  75.00%, tr:  82.43%, tr_best:  86.01%, epoch time: 29.86 seconds, 0.50 minutes\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss: 19.302595/1104.313721, val:  70.00%, val_best:  75.00%, tr:  83.76%, tr_best:  86.01%, epoch time: 29.20 seconds, 0.49 minutes\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss: 17.514154/1186.813843, val:  70.00%, val_best:  75.00%, tr:  84.37%, tr_best:  86.01%, epoch time: 28.94 seconds, 0.48 minutes\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss: 17.258144/1169.565552, val:  69.58%, val_best:  75.00%, tr:  85.39%, tr_best:  86.01%, epoch time: 30.01 seconds, 0.50 minutes\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss: 15.002714/1121.841187, val:  72.92%, val_best:  75.00%, tr:  86.11%, tr_best:  86.11%, epoch time: 29.14 seconds, 0.49 minutes\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss: 14.757095/1148.076416, val:  70.00%, val_best:  75.00%, tr:  86.31%, tr_best:  86.31%, epoch time: 29.86 seconds, 0.50 minutes\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss: 19.192139/1925.351685, val:  58.75%, val_best:  75.00%, tr:  82.64%, tr_best:  86.31%, epoch time: 29.86 seconds, 0.50 minutes\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss: 18.796278/1289.487793, val:  67.92%, val_best:  75.00%, tr:  83.66%, tr_best:  86.31%, epoch time: 29.15 seconds, 0.49 minutes\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss: 18.583836/2137.870361, val:  59.58%, val_best:  75.00%, tr:  84.37%, tr_best:  86.31%, epoch time: 29.29 seconds, 0.49 minutes\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss: 18.105446/1093.654175, val:  72.08%, val_best:  75.00%, tr:  83.55%, tr_best:  86.31%, epoch time: 29.28 seconds, 0.49 minutes\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss: 17.387854/1174.965576, val:  70.00%, val_best:  75.00%, tr:  84.27%, tr_best:  86.31%, epoch time: 29.71 seconds, 0.50 minutes\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss: 16.609886/1468.787720, val:  67.50%, val_best:  75.00%, tr:  85.90%, tr_best:  86.31%, epoch time: 29.04 seconds, 0.48 minutes\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss: 17.444580/1186.925293, val:  66.67%, val_best:  75.00%, tr:  83.35%, tr_best:  86.31%, epoch time: 28.74 seconds, 0.48 minutes\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss: 15.619016/1555.662476, val:  65.83%, val_best:  75.00%, tr:  86.31%, tr_best:  86.31%, epoch time: 29.92 seconds, 0.50 minutes\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss: 16.304197/1690.311646, val:  61.67%, val_best:  75.00%, tr:  85.80%, tr_best:  86.31%, epoch time: 29.85 seconds, 0.50 minutes\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss: 15.439041/1450.162231, val:  66.67%, val_best:  75.00%, tr:  84.47%, tr_best:  86.31%, epoch time: 30.01 seconds, 0.50 minutes\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss: 15.059763/1150.732666, val:  72.92%, val_best:  75.00%, tr:  86.52%, tr_best:  86.52%, epoch time: 30.39 seconds, 0.51 minutes\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss: 15.629560/1275.203857, val:  69.17%, val_best:  75.00%, tr:  84.58%, tr_best:  86.52%, epoch time: 30.55 seconds, 0.51 minutes\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss: 17.164480/1011.788330, val:  72.08%, val_best:  75.00%, tr:  84.47%, tr_best:  86.52%, epoch time: 29.48 seconds, 0.49 minutes\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss: 19.499784/1619.958740, val:  64.58%, val_best:  75.00%, tr:  83.96%, tr_best:  86.52%, epoch time: 30.47 seconds, 0.51 minutes\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss: 16.876333/1813.479370, val:  60.42%, val_best:  75.00%, tr:  85.39%, tr_best:  86.52%, epoch time: 29.08 seconds, 0.48 minutes\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss: 13.774090/1884.234985, val:  64.17%, val_best:  75.00%, tr:  87.13%, tr_best:  87.13%, epoch time: 28.77 seconds, 0.48 minutes\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss: 18.058750/1397.280029, val:  67.92%, val_best:  75.00%, tr:  84.27%, tr_best:  87.13%, epoch time: 28.45 seconds, 0.47 minutes\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss: 15.392564/1446.164062, val:  68.33%, val_best:  75.00%, tr:  86.21%, tr_best:  87.13%, epoch time: 30.21 seconds, 0.50 minutes\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss: 18.399149/1170.784668, val:  73.33%, val_best:  75.00%, tr:  83.86%, tr_best:  87.13%, epoch time: 28.77 seconds, 0.48 minutes\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss: 15.165048/1672.720215, val:  64.58%, val_best:  75.00%, tr:  85.80%, tr_best:  87.13%, epoch time: 29.67 seconds, 0.49 minutes\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss: 16.241423/1818.486328, val:  61.67%, val_best:  75.00%, tr:  86.31%, tr_best:  87.13%, epoch time: 29.83 seconds, 0.50 minutes\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss: 14.795706/1583.552734, val:  63.75%, val_best:  75.00%, tr:  86.11%, tr_best:  87.13%, epoch time: 29.08 seconds, 0.48 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a9c31264e0485382fed2188cba26d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.86108</td></tr><tr><td>tr_epoch_loss</td><td>14.79571</td></tr><tr><td>val_acc_best</td><td>0.75</td></tr><tr><td>val_acc_now</td><td>0.6375</td></tr><tr><td>val_loss</td><td>1583.55273</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-sweep-286</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8wh4p60e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8wh4p60e</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251111_034726-8wh4p60e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3wfe8y34 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 5000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_threshold: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: one\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251111_052614-3wfe8y34</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3wfe8y34' target=\"_blank\">bright-sweep-291</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3wfe8y34' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3wfe8y34</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_threshold' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': '20251111_052622_163', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.75, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 0.25, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.75, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0005, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'one', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'output_threshold': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 7a22c8a0ef5b9b252dbf98632e270efd\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.75, v_threshold=1, v_reset=10000, sg_width=0.25, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=False, ANPI_MODE=True)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.75, v_threshold=1, v_reset=10000, sg_width=0.25, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=False, ANPI_MODE=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient(single_step=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.0005\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "inFeed spike.shape torch.Size([10, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([ 1.8934e-02,  2.1093e-01, -1.6790e-02,  8.2860e-02, -1.8780e-01,\n",
      "         6.3725e-02, -6.3379e-02, -7.9782e-02,  5.1524e-02, -1.2446e-01,\n",
      "        -1.6221e-01, -2.9600e-02, -9.0339e-03,  1.9444e-02, -1.0934e-01,\n",
      "         1.8129e-01, -6.9730e-02,  6.7152e-02,  7.7763e-02, -3.2597e-03,\n",
      "         1.4558e-01, -5.0406e-02, -2.4797e-02,  1.4391e-01, -3.1818e-02,\n",
      "        -1.1320e-01,  2.2984e-01, -6.7576e-02,  1.7931e-02, -1.1550e-01,\n",
      "        -1.7594e-01, -1.5427e-01,  8.1846e-02,  1.3850e-01,  6.3135e-02,\n",
      "         4.1502e-02, -1.5509e-01,  6.0735e-02,  1.6491e-01, -6.4878e-02,\n",
      "         9.1983e-02,  7.6438e-03,  8.2616e-03, -1.3744e-02,  3.2357e-02,\n",
      "        -5.7478e-02, -1.0464e-01,  9.3097e-03, -3.2663e-02, -5.1313e-02,\n",
      "        -8.5647e-02,  3.8434e-02,  1.6001e-02, -1.9292e-02,  9.8606e-02,\n",
      "        -1.3158e-01, -3.4134e-02, -6.2874e-02,  4.3602e-02, -5.2417e-02,\n",
      "         1.2124e-01, -7.9496e-02,  2.4412e-02, -4.1696e-02,  1.0778e-01,\n",
      "        -1.0762e-01,  5.4097e-02, -1.2537e-01, -3.7238e-02,  5.0156e-02,\n",
      "         9.7776e-03,  2.5239e-02,  3.5296e-02,  2.2238e-01,  2.2782e-03,\n",
      "         1.5446e-01, -1.1312e-01,  9.2554e-02, -4.4633e-02,  7.4222e-02,\n",
      "        -5.6474e-02, -6.8803e-02, -7.0595e-02, -4.9484e-02, -4.2925e-02,\n",
      "        -4.0809e-02,  1.6994e-02,  4.3201e-02,  4.9468e-02, -1.1875e-01,\n",
      "        -2.6532e-02,  2.6988e-02, -1.4051e-01, -6.3074e-02,  7.3065e-03,\n",
      "         1.8921e-02,  5.8165e-02,  2.2661e-02,  1.1140e-01, -6.6528e-02,\n",
      "        -1.6133e-01,  5.8902e-04,  1.3482e-01,  1.2398e-01,  2.2678e-03,\n",
      "        -1.2688e-01, -7.3284e-02,  3.6657e-02, -5.3425e-02, -3.8686e-03,\n",
      "        -7.5912e-02, -2.4416e-01,  6.8315e-02, -9.1515e-03, -2.1105e-02,\n",
      "         4.3759e-02, -3.0760e-02,  2.1116e-03,  6.1028e-02,  2.4064e-02,\n",
      "         7.3052e-02, -1.1411e-02, -9.9703e-03, -4.8900e-02, -4.9272e-02,\n",
      "        -1.1781e-01, -2.3789e-02, -6.6208e-02,  1.9253e-02,  9.5465e-02,\n",
      "        -2.7977e-03,  1.6420e-01,  1.0646e-01, -9.6819e-02, -6.5508e-02,\n",
      "         1.6782e-01,  2.4013e-01, -6.0490e-02,  1.2407e-01, -2.7312e-02,\n",
      "         4.2546e-02,  4.1576e-02,  1.0389e-01, -1.9791e-01, -6.1734e-02,\n",
      "         2.0598e-01, -9.2463e-03,  2.3019e-02, -7.1248e-02, -1.6451e-01,\n",
      "         8.8945e-02,  7.6954e-02, -6.1358e-02,  2.1075e-01,  1.1362e-01,\n",
      "        -4.1540e-02,  2.3355e-02, -1.2469e-01, -1.1774e-02, -5.9197e-02,\n",
      "        -7.8824e-02,  2.0957e-04, -1.8973e-02,  7.3129e-02,  7.9223e-02,\n",
      "         8.0468e-02, -5.9513e-02,  1.2675e-01, -1.0473e-01, -2.2743e-03,\n",
      "        -3.5689e-02, -4.7485e-02,  1.4612e-02, -1.4731e-01,  3.0283e-02,\n",
      "        -4.3783e-02, -1.0702e-01, -1.2393e-01, -1.5395e-01, -1.5502e-01,\n",
      "        -6.4030e-02,  7.4169e-02, -5.9266e-02,  2.9501e-02, -1.3114e-01,\n",
      "        -3.1084e-02,  5.7895e-02,  5.5842e-02, -7.3755e-02, -1.2356e-02,\n",
      "        -4.2171e-02, -1.3607e-01,  3.7821e-02, -2.0038e-02,  5.8521e-02,\n",
      "        -9.7889e-02, -3.6203e-04, -7.8263e-02,  3.8445e-02,  2.4739e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 0.0243,  0.0875,  0.0762,  0.0893, -0.0358,  0.0084, -0.0823,  0.0415,\n",
      "         0.1093,  0.0390, -0.0084,  0.0824, -0.0388, -0.0084,  0.1905, -0.0412,\n",
      "         0.0535,  0.0146, -0.1927, -0.0034, -0.1234, -0.0248,  0.0023,  0.1960,\n",
      "         0.0956,  0.1355, -0.0962, -0.0471, -0.0019, -0.1697,  0.0164,  0.0255,\n",
      "        -0.0218, -0.0832,  0.0267,  0.0968,  0.1448,  0.0340,  0.0123, -0.0644,\n",
      "        -0.1539, -0.0648,  0.1434,  0.0292, -0.0543,  0.0045, -0.1050, -0.0141,\n",
      "        -0.0106, -0.0727, -0.0340,  0.0092,  0.0063, -0.0900,  0.0770, -0.1758,\n",
      "        -0.1888,  0.1074,  0.0024,  0.0566,  0.0525,  0.0500, -0.1397,  0.2322,\n",
      "        -0.1507, -0.0170, -0.0022,  0.0212,  0.0636,  0.1522,  0.0262,  0.0489,\n",
      "        -0.0238, -0.0843,  0.0417, -0.0206, -0.1503, -0.0105, -0.0025, -0.0157,\n",
      "         0.0537,  0.2482, -0.0157, -0.0847,  0.1378,  0.0673,  0.0827,  0.0278,\n",
      "         0.0017,  0.0711,  0.1171, -0.0113, -0.1379, -0.0627,  0.0863,  0.0732,\n",
      "         0.0663, -0.0518, -0.1748, -0.0279,  0.0079, -0.1269,  0.1308, -0.1293,\n",
      "         0.0255,  0.0589, -0.1456, -0.1754,  0.1100,  0.1369, -0.0427, -0.0364,\n",
      "        -0.1153, -0.2599, -0.0316,  0.0100,  0.1226, -0.1337,  0.0349,  0.1675,\n",
      "        -0.2253,  0.1285, -0.1121, -0.1268, -0.0064,  0.1009, -0.2251, -0.0125,\n",
      "         0.0833, -0.0059, -0.0931, -0.1747, -0.0026,  0.0089, -0.1154, -0.1073,\n",
      "         0.2383,  0.0302, -0.1482, -0.0759,  0.0708,  0.0868,  0.1001,  0.0796,\n",
      "         0.2262, -0.0222,  0.2962,  0.0536,  0.0022, -0.0399, -0.0256,  0.0852,\n",
      "        -0.1183, -0.1185, -0.0293, -0.0370,  0.0309, -0.0984,  0.1941,  0.0216,\n",
      "        -0.0090,  0.1404,  0.0598,  0.0650,  0.0744,  0.0426, -0.0094, -0.0445,\n",
      "        -0.1149,  0.0229,  0.1572,  0.1014,  0.1170, -0.1135,  0.0229,  0.1222,\n",
      "        -0.0146, -0.0612,  0.1128, -0.2361,  0.0667, -0.0671, -0.0090, -0.0696,\n",
      "        -0.1264,  0.1187, -0.0047, -0.0778,  0.0414, -0.0189, -0.0153, -0.0139,\n",
      "        -0.0278, -0.1414, -0.0518, -0.0080, -0.1608, -0.0265,  0.1849, -0.0543],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([-4.4951e-02, -3.0269e-02,  5.7666e-03,  5.0608e-02,  7.1861e-03,\n",
      "         1.3723e-01, -4.7515e-02,  2.8713e-02,  1.1686e-01, -2.1819e-02,\n",
      "         8.4215e-02,  3.5925e-02,  1.6644e-01, -8.9751e-02,  5.2478e-02,\n",
      "         1.1553e-01,  1.6151e-01, -2.2964e-02, -1.6174e-01,  1.9235e-01,\n",
      "         9.7453e-02,  7.7079e-02, -8.9056e-02,  8.0481e-02,  1.4806e-01,\n",
      "        -1.6083e-02,  1.0203e-01, -3.7285e-02,  6.1060e-02,  9.6561e-03,\n",
      "        -6.8472e-02,  1.0096e-02, -1.4065e-01,  2.3837e-01,  1.1363e-01,\n",
      "         2.4803e-02, -3.5905e-02,  8.2499e-02,  4.0047e-02,  4.7051e-02,\n",
      "        -1.8222e-01,  9.4498e-02, -9.1961e-02,  1.1252e-01,  7.8541e-02,\n",
      "         1.0352e-01,  5.1129e-02, -1.3695e-02,  1.3555e-01,  3.0085e-02,\n",
      "         4.2894e-02, -3.0123e-03, -1.0391e-01,  6.3762e-03,  2.7413e-02,\n",
      "         2.0599e-01, -1.3469e-01, -4.1689e-02,  8.9827e-02, -1.3860e-01,\n",
      "         5.3680e-02, -9.2493e-02,  2.8438e-02, -9.8101e-02, -2.9716e-02,\n",
      "         1.5025e-02, -8.1340e-03,  8.0789e-03,  1.5008e-01,  1.7955e-02,\n",
      "        -9.7189e-02,  4.2880e-02,  3.5098e-02, -7.5292e-02,  1.0018e-02,\n",
      "        -3.8158e-02,  2.1247e-02,  8.8000e-02,  5.3422e-02,  8.5364e-02,\n",
      "        -3.1726e-02, -4.0565e-02,  5.0597e-02, -5.4060e-02, -1.9612e-03,\n",
      "         2.9600e-01,  9.2132e-02,  2.9507e-02, -9.2795e-02, -1.0727e-01,\n",
      "        -9.4369e-04,  1.6943e-01, -1.1252e-01,  2.0963e-03, -4.7562e-02,\n",
      "        -8.9568e-02, -1.6470e-01, -1.3752e-02, -4.9301e-03,  1.9867e-02,\n",
      "         2.8623e-02, -1.4914e-01, -7.4637e-02,  4.3263e-02, -5.4997e-02,\n",
      "        -5.1978e-02, -9.9176e-02, -1.9955e-02,  5.1099e-02,  1.8961e-02,\n",
      "         3.6069e-02, -8.3924e-02,  1.0509e-01, -1.2345e-01, -6.2687e-02,\n",
      "        -6.5892e-02,  7.2257e-02,  7.6160e-02, -1.0679e-02,  1.1915e-01,\n",
      "        -1.1745e-01, -4.7364e-02,  8.2369e-03, -2.0607e-02, -8.7966e-03,\n",
      "        -1.3239e-01, -1.0953e-02, -3.8245e-02,  1.7113e-01, -9.6756e-02,\n",
      "        -2.3135e-01,  1.7700e-01, -9.4699e-02,  8.4271e-02,  1.7756e-01,\n",
      "        -7.7262e-03,  2.4065e-01,  1.2335e-01,  5.0242e-02,  1.1121e-02,\n",
      "        -1.3971e-01, -2.5510e-02,  1.1339e-02,  5.6864e-02, -2.9294e-02,\n",
      "        -1.0927e-01,  9.0566e-02, -1.4698e-01,  1.0142e-01, -2.0078e-01,\n",
      "        -2.5668e-02, -8.1559e-02, -2.6427e-02,  2.6780e-01,  4.4975e-02,\n",
      "         1.1964e-01,  6.6056e-03,  8.9370e-02,  7.3522e-02, -5.8117e-02,\n",
      "        -6.1687e-02, -3.5208e-02,  1.4783e-01, -1.6733e-02,  1.8550e-01,\n",
      "        -5.9637e-02,  1.0120e-01,  3.3498e-02, -1.4412e-02,  1.4279e-01,\n",
      "        -1.7611e-01,  2.3674e-02, -2.6663e-02,  2.8803e-02, -1.0240e-01,\n",
      "        -8.6560e-02, -1.3708e-02,  2.1908e-01,  1.7359e-01,  1.6947e-02,\n",
      "         1.3747e-01, -1.0779e-02, -5.6662e-02,  2.1867e-02,  9.4120e-02,\n",
      "        -1.4148e-04,  1.3205e-01, -7.6242e-03, -6.1247e-02, -1.5935e-01,\n",
      "         1.1932e-01, -1.7626e-01,  4.7519e-02, -6.7935e-02, -3.5344e-02,\n",
      "        -5.6960e-02, -1.6597e-01,  3.6102e-02,  7.2538e-02, -1.1702e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0897,  0.1346,  0.0082,  0.0404, -0.0141, -0.0915,  0.1112,  0.0613,\n",
      "         0.0437,  0.2269, -0.0635,  0.0330, -0.0388,  0.0970,  0.1340, -0.0103,\n",
      "        -0.0180, -0.2479, -0.1925, -0.0520, -0.0167,  0.2512,  0.2248,  0.0406,\n",
      "        -0.0638, -0.1051,  0.0551,  0.1565, -0.0160, -0.0229,  0.0975,  0.0753,\n",
      "        -0.0261,  0.0484,  0.0233,  0.1358, -0.0195,  0.1246, -0.1235, -0.0948,\n",
      "         0.1241,  0.0367, -0.0132,  0.0701, -0.0866,  0.0677,  0.0747, -0.0005,\n",
      "         0.0291, -0.0934,  0.0961,  0.0369,  0.1113, -0.0287,  0.0090,  0.2001,\n",
      "        -0.0262,  0.0943, -0.1185, -0.1080, -0.1874, -0.2298, -0.0099,  0.1366,\n",
      "        -0.0031, -0.2031,  0.0642, -0.0495, -0.0109,  0.0107, -0.0874, -0.0920,\n",
      "        -0.0740, -0.0029, -0.0110, -0.0867,  0.0235, -0.0947,  0.0334,  0.0025,\n",
      "        -0.1047, -0.1597,  0.0005, -0.1529,  0.0354,  0.1324, -0.1721, -0.0826,\n",
      "         0.1246,  0.1224,  0.1146,  0.0836,  0.0127,  0.1906,  0.0699, -0.0207,\n",
      "         0.0435, -0.0039, -0.1358, -0.0330, -0.1266,  0.1363,  0.0505, -0.1182,\n",
      "        -0.0723, -0.1437,  0.1342,  0.0754, -0.0922,  0.1479, -0.1418, -0.0035,\n",
      "         0.0841,  0.0382,  0.1203, -0.1159,  0.0937,  0.0410, -0.0786,  0.0332,\n",
      "         0.0585,  0.0918,  0.0264,  0.0213, -0.0575,  0.1411, -0.0949,  0.0688,\n",
      "         0.0401,  0.0027, -0.0604,  0.0814, -0.0098, -0.1172,  0.0241, -0.0613,\n",
      "        -0.0740, -0.0775,  0.0796, -0.1757,  0.0731,  0.0651,  0.0684, -0.1016,\n",
      "         0.2258,  0.0089,  0.0729, -0.1231,  0.0348,  0.0380, -0.1748, -0.0902,\n",
      "         0.2137,  0.0032, -0.1064, -0.0598,  0.1530,  0.0615,  0.0364,  0.0570,\n",
      "        -0.0708, -0.0792, -0.1045,  0.0952, -0.0262, -0.0666,  0.0778, -0.0828,\n",
      "         0.0837,  0.0487,  0.0133,  0.0663,  0.0131,  0.1229,  0.1182,  0.0589,\n",
      "        -0.0275, -0.0269, -0.0815, -0.1605,  0.0128, -0.0175,  0.0951, -0.1965,\n",
      "        -0.1307, -0.0327,  0.2386, -0.0120, -0.0878,  0.2075,  0.1736,  0.1385,\n",
      "        -0.0182, -0.1107, -0.2712,  0.1544,  0.2334, -0.0716, -0.0042, -0.0741],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-0.1114, -0.1260, -0.1119, -0.1797, -0.0123, -0.1354, -0.0627,  0.0113,\n",
      "        -0.2121, -0.0449, -0.1316,  0.0974, -0.0196, -0.2268, -0.0866,  0.0271,\n",
      "        -0.0689,  0.0083, -0.1552,  0.1024,  0.0478, -0.0129, -0.0338,  0.0313,\n",
      "        -0.0618,  0.0572, -0.0461,  0.0063,  0.1558, -0.0055, -0.2186,  0.1069,\n",
      "        -0.0180,  0.0722, -0.0741, -0.0951,  0.1404, -0.0053,  0.0869,  0.1728,\n",
      "         0.1237, -0.0906,  0.1067, -0.0791, -0.1660,  0.0487,  0.0635,  0.0555,\n",
      "         0.0562, -0.0208, -0.0247, -0.0210, -0.1192,  0.0735, -0.0960,  0.0183,\n",
      "         0.0923,  0.0311,  0.0228,  0.0739,  0.0312,  0.0293,  0.0242,  0.1432,\n",
      "        -0.1495,  0.0705,  0.1607,  0.1131,  0.0455, -0.1201,  0.1743,  0.0608,\n",
      "        -0.1190, -0.1751,  0.0174,  0.1122,  0.0615,  0.1286,  0.2249, -0.0399,\n",
      "         0.1110, -0.3303, -0.1719, -0.1252,  0.1496, -0.0990, -0.0732,  0.1068,\n",
      "        -0.0648, -0.0481, -0.1068, -0.0256,  0.1217, -0.1511,  0.0120, -0.0370,\n",
      "        -0.0630,  0.0352, -0.0881, -0.0354, -0.2016,  0.0181, -0.0575, -0.0375,\n",
      "         0.1403,  0.0006,  0.2255, -0.0530, -0.0560, -0.2372,  0.1225,  0.2346,\n",
      "         0.1047, -0.0298,  0.0164, -0.0071, -0.0364,  0.1334,  0.1921,  0.0409,\n",
      "         0.0370, -0.2028,  0.0428,  0.0574,  0.1313,  0.1827,  0.1051,  0.0931,\n",
      "         0.1609, -0.1042, -0.0180,  0.1062, -0.2921,  0.1134, -0.0590, -0.1621,\n",
      "         0.0792,  0.0747,  0.0440,  0.1033, -0.0561, -0.0242, -0.0610, -0.0865,\n",
      "        -0.0366,  0.0475, -0.0406, -0.0448,  0.0063,  0.1424,  0.0130,  0.0362,\n",
      "        -0.0880, -0.0607, -0.0947,  0.1126, -0.1080, -0.1318, -0.1815,  0.0529,\n",
      "        -0.0257, -0.0751, -0.2009, -0.0415, -0.0961, -0.0399,  0.0027, -0.0857,\n",
      "        -0.0904, -0.0699, -0.0287, -0.0942,  0.0285, -0.0968,  0.2108,  0.0588,\n",
      "        -0.0367, -0.0581,  0.0109,  0.0122,  0.0639, -0.0397, -0.1301,  0.1894,\n",
      "        -0.1012,  0.1052,  0.0252,  0.1616,  0.1519,  0.0793, -0.1125, -0.0098,\n",
      "        -0.2642,  0.0059, -0.1486,  0.1224, -0.1979, -0.0971, -0.0462,  0.0212],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([-2.3097e-02, -1.4788e-01,  1.0210e-01,  3.3557e-02,  2.4169e-01,\n",
      "        -1.3634e-01, -8.5383e-03, -3.8043e-02,  1.7316e-01, -4.1206e-02,\n",
      "         2.8185e-02,  1.1411e-01,  3.9203e-02,  8.1501e-02,  9.3352e-02,\n",
      "         2.0928e-01, -6.4334e-02,  3.8064e-02,  1.0619e-01,  9.6020e-02,\n",
      "         1.0335e-01,  2.7876e-01,  6.5856e-02,  2.0067e-01,  8.0354e-02,\n",
      "        -2.3247e-01,  1.2218e-01,  2.3015e-02,  1.7446e-01, -5.6854e-02,\n",
      "        -5.4324e-02,  3.6970e-03,  5.6947e-02, -3.3943e-02,  1.6095e-01,\n",
      "         1.9372e-03, -7.9038e-02,  9.8647e-02, -3.2229e-02,  5.0507e-02,\n",
      "         9.7127e-02, -1.5709e-01,  2.4332e-02,  6.0119e-02, -1.0542e-01,\n",
      "        -9.9471e-02, -9.7351e-02,  1.0731e-01,  1.3880e-01, -1.6828e-01,\n",
      "         8.0962e-02, -5.6699e-02, -7.4759e-03, -6.4011e-03, -1.6884e-01,\n",
      "        -1.3898e-01,  1.7222e-01, -8.1651e-02, -1.8829e-01, -5.1125e-02,\n",
      "         1.3743e-01,  1.8289e-01, -4.9558e-02,  3.3120e-02, -3.1865e-02,\n",
      "         8.0625e-02, -4.9182e-02, -2.8268e-02,  2.6301e-02, -9.2309e-03,\n",
      "        -4.7385e-02, -1.8954e-01,  6.7699e-02,  7.9177e-03, -1.2038e-01,\n",
      "         1.9006e-02,  1.7940e-02,  1.2187e-01, -5.8642e-02,  9.8349e-02,\n",
      "        -1.5748e-01,  2.1717e-02, -5.9567e-02, -1.2667e-02,  1.7577e-02,\n",
      "        -2.7029e-02, -1.3078e-01,  9.7926e-02,  2.2109e-02,  1.4673e-01,\n",
      "        -2.8729e-02,  1.4028e-01, -1.5075e-01,  8.0415e-02,  1.0154e-01,\n",
      "        -2.4165e-02, -2.6159e-02, -7.1517e-02, -4.9824e-02, -7.3641e-02,\n",
      "        -6.3665e-02,  1.4753e-01,  2.2992e-02,  2.5068e-02,  8.3415e-02,\n",
      "        -9.7786e-02,  1.5092e-01, -3.5185e-02, -1.6092e-02, -1.3254e-01,\n",
      "        -1.3460e-01,  2.2992e-02,  1.0348e-01,  1.3005e-01,  1.0918e-01,\n",
      "        -5.3370e-03,  3.2445e-02, -1.0142e-02, -9.9965e-03,  4.4736e-02,\n",
      "        -8.6016e-02,  7.8391e-02, -4.4661e-02,  1.1245e-01, -4.3102e-02,\n",
      "         1.1311e-01,  3.2954e-02, -2.3165e-02, -1.0094e-02, -5.7948e-02,\n",
      "         3.6766e-02, -2.9717e-02, -1.2952e-02, -1.1631e-01, -1.0180e-01,\n",
      "        -5.3192e-02,  3.6347e-02, -1.7114e-02,  5.3002e-02,  8.4525e-02,\n",
      "        -1.0713e-01, -9.2328e-02, -7.2179e-02,  3.7902e-02, -1.4022e-01,\n",
      "         5.5914e-02,  7.9675e-02,  3.9484e-02, -6.8603e-03, -5.1982e-02,\n",
      "        -2.1808e-01, -1.2660e-01,  2.4476e-01, -1.2630e-01,  3.3029e-02,\n",
      "         1.7699e-02,  9.0871e-02,  1.5078e-01, -7.2952e-02,  9.8937e-02,\n",
      "        -7.6370e-02,  8.8079e-02, -8.8526e-02,  1.4171e-02, -3.5435e-02,\n",
      "         4.1153e-03, -1.1882e-01, -5.6413e-02,  1.3846e-02, -4.2599e-02,\n",
      "         8.8813e-02,  2.6194e-03, -5.3535e-02, -1.1002e-01,  1.8341e-01,\n",
      "         9.4184e-02, -1.3347e-01,  5.6069e-02,  1.6303e-01,  1.1132e-04,\n",
      "        -6.4000e-02,  6.0648e-02,  1.7026e-01,  2.7840e-02, -1.4208e-01,\n",
      "        -1.4573e-01, -1.1765e-01,  4.9697e-02,  6.4271e-02, -1.9731e-01,\n",
      "         4.8141e-02,  1.8684e-02, -6.3554e-02,  3.6129e-02,  1.9314e-01,\n",
      "         1.0903e-01, -9.7833e-03,  6.3571e-02,  2.5554e-02, -1.0850e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([ 0.0344, -0.0583, -0.0703, -0.2514, -0.0263, -0.1517, -0.0595,  0.0852,\n",
      "        -0.0268,  0.1277,  0.0393,  0.0879, -0.1254, -0.3348, -0.0745, -0.0879,\n",
      "         0.0474,  0.2322, -0.0987,  0.0354, -0.1064,  0.2409,  0.0734,  0.1306,\n",
      "        -0.0146,  0.1136, -0.1571,  0.0971, -0.0936, -0.0309, -0.0165, -0.0660,\n",
      "        -0.0744, -0.0196, -0.0644,  0.0964, -0.0135,  0.1379,  0.0341, -0.0277,\n",
      "        -0.1563,  0.0474,  0.1149, -0.0199,  0.0589, -0.0346, -0.1100, -0.0020,\n",
      "        -0.0207, -0.0792,  0.0376,  0.0803,  0.0011, -0.0536, -0.0457, -0.1338,\n",
      "        -0.0419, -0.1026,  0.0135, -0.0070,  0.0092,  0.0340,  0.0115,  0.0074,\n",
      "        -0.0919,  0.0340, -0.0404, -0.0352, -0.1594, -0.0584, -0.0647, -0.1073,\n",
      "         0.0282, -0.0426, -0.0435,  0.0278, -0.0031,  0.0945,  0.1075,  0.0418,\n",
      "        -0.1066,  0.0568, -0.0267,  0.0154, -0.1619, -0.0058,  0.1889, -0.0501,\n",
      "        -0.0341, -0.0658,  0.1119, -0.1324,  0.0051, -0.0356, -0.0619,  0.0052,\n",
      "        -0.1332,  0.0911,  0.1441,  0.0152, -0.1952, -0.0013, -0.0183, -0.0338,\n",
      "         0.0631,  0.0013, -0.0094, -0.0361,  0.0370,  0.0204,  0.2303,  0.0010,\n",
      "        -0.0754, -0.0257,  0.1069,  0.0503, -0.0969,  0.0752,  0.0786, -0.0510,\n",
      "         0.0232, -0.1267,  0.0891,  0.1099,  0.0993, -0.0559, -0.0145, -0.0100,\n",
      "         0.1262,  0.0168,  0.0875, -0.0091,  0.2017,  0.0385,  0.1079, -0.0034,\n",
      "        -0.0704,  0.0213,  0.0039, -0.0300, -0.1966, -0.0747, -0.0787,  0.1549,\n",
      "        -0.0108,  0.0581, -0.0527, -0.0684, -0.1514,  0.1286, -0.0561,  0.0960,\n",
      "        -0.0797,  0.0104,  0.0925,  0.0467, -0.0793, -0.0287, -0.1249,  0.0754,\n",
      "         0.0514, -0.1473,  0.1062, -0.0139, -0.0259,  0.0431, -0.0780,  0.0970,\n",
      "         0.1067,  0.0072,  0.0515,  0.0534, -0.0132,  0.0470, -0.1017,  0.1103,\n",
      "         0.0218, -0.1171,  0.0045,  0.2126,  0.0261, -0.0675,  0.0822,  0.0762,\n",
      "        -0.0359, -0.0296,  0.0451, -0.1296, -0.0783, -0.0471, -0.0497, -0.0753,\n",
      "        -0.0754,  0.0663, -0.0081,  0.1472,  0.1225, -0.0161, -0.0844, -0.0095],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 0.0324,  0.0432,  0.0128, -0.1048, -0.0309,  0.0343,  0.0725,  0.0905,\n",
      "         0.0277, -0.0101, -0.2833,  0.0445,  0.0266, -0.0947, -0.0705,  0.0008,\n",
      "         0.0852,  0.0237,  0.1369, -0.0885, -0.1265, -0.0279, -0.0524,  0.1058,\n",
      "         0.0903,  0.1174,  0.0208,  0.1273, -0.0776,  0.0633,  0.1014,  0.0515,\n",
      "         0.0878, -0.0716, -0.0953,  0.0094, -0.0114, -0.1050,  0.1013, -0.0545,\n",
      "         0.2847,  0.1515,  0.0272, -0.0088, -0.0249,  0.1165, -0.0152, -0.0898,\n",
      "         0.0555, -0.0581,  0.0530, -0.1249, -0.0017, -0.0968, -0.0731, -0.0366,\n",
      "         0.0387, -0.0459, -0.0424,  0.0322, -0.0163, -0.0722,  0.0519, -0.2235,\n",
      "        -0.0526, -0.1272, -0.1831,  0.0043, -0.0183,  0.0030, -0.0554,  0.1429,\n",
      "        -0.0177, -0.0099, -0.0047, -0.0410,  0.1171,  0.0849, -0.0063,  0.0121,\n",
      "         0.1281, -0.0245, -0.1118,  0.0065, -0.0303,  0.0243, -0.0665,  0.0273,\n",
      "        -0.1180, -0.0921, -0.0741,  0.1799, -0.1599, -0.1509,  0.0317, -0.1675,\n",
      "         0.0991, -0.1584,  0.0643, -0.0938, -0.0833,  0.0289, -0.1786, -0.0270,\n",
      "        -0.0047, -0.0191,  0.0682, -0.1072,  0.0457,  0.0037,  0.0101, -0.0792,\n",
      "        -0.0411, -0.0145,  0.2259, -0.0552, -0.0088,  0.2277,  0.1019, -0.0581,\n",
      "        -0.0499,  0.0003, -0.0960,  0.1163,  0.0542, -0.0899,  0.0785, -0.0757,\n",
      "         0.1136,  0.0955,  0.0024, -0.1075,  0.0386, -0.1068, -0.1145, -0.0436,\n",
      "        -0.1211, -0.0174,  0.0412, -0.1130,  0.0218, -0.0999, -0.1438,  0.0179,\n",
      "        -0.0548, -0.0479, -0.0121, -0.0538, -0.1268, -0.0741, -0.0415,  0.0144,\n",
      "         0.0508,  0.0760, -0.0638, -0.1439,  0.0555, -0.0486,  0.0251,  0.0053,\n",
      "         0.0245, -0.0231,  0.1099,  0.1551,  0.0105,  0.1027, -0.2184, -0.0827,\n",
      "        -0.1366,  0.0940, -0.1021,  0.1093,  0.1131,  0.0516, -0.1181,  0.1755,\n",
      "         0.0641,  0.0734,  0.1746,  0.1287, -0.0379,  0.0184,  0.0576, -0.0517,\n",
      "         0.0330, -0.0928,  0.0623, -0.0064,  0.0763,  0.0913,  0.0678,  0.1516,\n",
      "         0.0029, -0.0753, -0.0246,  0.0518, -0.0983,  0.1127,  0.0964, -0.1392],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 0.0620, -0.1085, -0.0364, -0.0362, -0.0345,  0.0245, -0.0383, -0.1592,\n",
      "        -0.0493,  0.0025, -0.0031, -0.0267, -0.0372, -0.0254, -0.0815, -0.1367,\n",
      "        -0.1122, -0.0198, -0.1505, -0.0171,  0.2510,  0.0592, -0.0906,  0.0650,\n",
      "        -0.1193,  0.1853,  0.1500, -0.0254, -0.0351, -0.1932,  0.1420,  0.0618,\n",
      "        -0.0726, -0.0382,  0.0244, -0.1537, -0.0670, -0.0176,  0.0063,  0.0121,\n",
      "         0.1055, -0.1070, -0.0291, -0.1309, -0.0195,  0.0570, -0.0449,  0.1710,\n",
      "        -0.0209,  0.1127, -0.0518, -0.0039,  0.0513, -0.0427,  0.0767, -0.0871,\n",
      "        -0.0222,  0.0821, -0.0037, -0.0591, -0.2268,  0.0216,  0.0999, -0.0761,\n",
      "        -0.0202,  0.0163,  0.0175,  0.0805,  0.2464,  0.0258,  0.0094, -0.2156,\n",
      "        -0.1800, -0.0450, -0.0323,  0.0328,  0.1234,  0.0703,  0.1513, -0.0536,\n",
      "        -0.0270,  0.1320,  0.1908,  0.0332, -0.0246, -0.0074,  0.0894,  0.1751,\n",
      "        -0.0369, -0.0786, -0.1112, -0.0129,  0.1746,  0.0117,  0.1111, -0.0848,\n",
      "         0.0966, -0.2175,  0.0244, -0.0435,  0.0372, -0.0111, -0.1484, -0.1168,\n",
      "         0.0543,  0.0504, -0.0847, -0.1751, -0.0511,  0.2542,  0.0312,  0.1713,\n",
      "        -0.0500,  0.0007, -0.0746,  0.0990,  0.0940, -0.0162, -0.0896,  0.0984,\n",
      "        -0.1223, -0.1810, -0.0262,  0.0043,  0.0123, -0.1478, -0.0811, -0.0217,\n",
      "        -0.2336, -0.1054, -0.1123,  0.0213,  0.1850, -0.0052, -0.0100, -0.0414,\n",
      "        -0.0894,  0.0167,  0.1224, -0.0976, -0.0830,  0.1470, -0.1326, -0.0536,\n",
      "        -0.2003,  0.0206,  0.0106, -0.1356,  0.2107, -0.0481, -0.0495, -0.1029,\n",
      "         0.2354, -0.0199, -0.0308, -0.1495, -0.0890,  0.0694, -0.1402,  0.0187,\n",
      "        -0.0880, -0.0337, -0.0247, -0.0611, -0.0248,  0.0132, -0.0624, -0.0821,\n",
      "        -0.1276,  0.0288,  0.0071,  0.0083, -0.0146, -0.1069, -0.1822,  0.0275,\n",
      "         0.1872,  0.0655,  0.1092, -0.0039,  0.0767,  0.0464, -0.0118, -0.0907,\n",
      "         0.2852,  0.0258,  0.0703, -0.0427, -0.0363,  0.1239, -0.0180, -0.0091,\n",
      "         0.0565, -0.1153, -0.0575, -0.0451,  0.1766, -0.1391,  0.0200,  0.1119],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([ 0.0004,  0.0098, -0.0630,  0.0729,  0.1286,  0.1135,  0.0132,  0.0753,\n",
      "         0.1000,  0.0748,  0.0039, -0.1073, -0.0160,  0.0784,  0.0432,  0.0864,\n",
      "         0.0586, -0.0512,  0.2272,  0.1022,  0.0344,  0.0295,  0.0027,  0.0091,\n",
      "         0.0955,  0.0551, -0.1328, -0.0203,  0.2392, -0.1312, -0.0542,  0.1138,\n",
      "         0.0317, -0.0134,  0.1082, -0.0630, -0.2079,  0.0328, -0.0148, -0.0055,\n",
      "         0.0792,  0.0654,  0.0422,  0.0049, -0.1233, -0.0741,  0.1021, -0.0386,\n",
      "        -0.0348,  0.0083, -0.0181,  0.0171,  0.0679,  0.0125, -0.0887, -0.0110,\n",
      "         0.0730, -0.0927,  0.0534,  0.0082, -0.0331,  0.1718,  0.1019, -0.0851,\n",
      "         0.0134,  0.1386, -0.0494,  0.0115,  0.0689,  0.0778,  0.0095,  0.1618,\n",
      "         0.0119,  0.0620, -0.0528, -0.0109, -0.1644,  0.1267, -0.1405,  0.2506,\n",
      "        -0.1118, -0.0659, -0.0724, -0.0041,  0.2570, -0.0359,  0.0570,  0.1049,\n",
      "         0.0314,  0.0490, -0.1250,  0.0921, -0.1334, -0.0470,  0.0743,  0.2864,\n",
      "         0.2034, -0.0486, -0.0558,  0.1079,  0.0035,  0.2203,  0.2007, -0.0190,\n",
      "        -0.0402,  0.2442, -0.1645,  0.0048, -0.0110, -0.0840, -0.2092, -0.0796,\n",
      "        -0.0620, -0.1374, -0.2458, -0.1597, -0.0614,  0.0840, -0.0038,  0.0786,\n",
      "         0.0911, -0.0136, -0.1404, -0.0157,  0.1248, -0.0229, -0.0362, -0.0273,\n",
      "        -0.1132,  0.0388, -0.0171, -0.1177, -0.1702, -0.1764, -0.0355,  0.0977,\n",
      "        -0.0696,  0.1107,  0.1663, -0.0294,  0.1273,  0.0251,  0.0632,  0.0685,\n",
      "        -0.1134, -0.2482, -0.0751, -0.0773, -0.0422,  0.1633,  0.0615,  0.0023,\n",
      "        -0.2218, -0.1290, -0.0822, -0.0304,  0.0999,  0.1201, -0.0902,  0.0529,\n",
      "        -0.0231,  0.1423, -0.1279,  0.0132, -0.1959, -0.1194,  0.2194,  0.0659,\n",
      "         0.0531, -0.0018,  0.1107, -0.0963,  0.1062,  0.0171,  0.0575, -0.1313,\n",
      "        -0.1070, -0.1193,  0.0607, -0.0449,  0.0803, -0.0565,  0.0955,  0.0853,\n",
      "         0.1472, -0.0848, -0.0461,  0.0320, -0.0080,  0.1865, -0.0155,  0.0745,\n",
      "         0.1170, -0.0421, -0.1014, -0.0092,  0.0323,  0.0626, -0.0019,  0.0560],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([10, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([ 0.0136, -0.0106, -0.0700,  0.0738,  0.0363, -0.0981,  0.0008,  0.0320,\n",
      "         0.0276, -0.0871, -0.0159, -0.0787, -0.0114, -0.1574, -0.0150, -0.0063,\n",
      "        -0.0520,  0.0052, -0.0031, -0.0725, -0.1130, -0.0024, -0.0740, -0.0406,\n",
      "         0.0623, -0.0248, -0.1201, -0.1544,  0.0628,  0.0948, -0.1293, -0.0977,\n",
      "        -0.0445,  0.0393, -0.0752, -0.1147, -0.0152, -0.0637,  0.0246,  0.0448,\n",
      "         0.1848,  0.0159, -0.0257, -0.1081,  0.0749, -0.1096,  0.0297,  0.0380,\n",
      "        -0.0246, -0.0982,  0.0286, -0.1074,  0.1856, -0.1482,  0.1644, -0.1338,\n",
      "         0.0282,  0.0384,  0.0637, -0.0539,  0.0754,  0.0908,  0.0045,  0.0340,\n",
      "         0.1005,  0.0013,  0.0536,  0.0342, -0.0304, -0.0040, -0.0625,  0.1837,\n",
      "        -0.0095, -0.0813, -0.0995, -0.0855, -0.0032, -0.0013, -0.0328,  0.0576,\n",
      "         0.0864,  0.0268,  0.0238,  0.0534,  0.0807, -0.0262, -0.0349,  0.0197,\n",
      "        -0.1325,  0.0013,  0.0123, -0.1658, -0.0778, -0.0948, -0.1309,  0.1143,\n",
      "         0.0970,  0.1279,  0.1991, -0.1371, -0.0006,  0.0825, -0.1059,  0.0706,\n",
      "        -0.0534,  0.0225,  0.1133,  0.1787, -0.1257, -0.1148,  0.1220,  0.0250,\n",
      "        -0.1914, -0.2004,  0.0467,  0.0144,  0.0167, -0.0004, -0.1363, -0.0944,\n",
      "        -0.0846, -0.1734,  0.0105,  0.1364, -0.1122, -0.0127,  0.1167, -0.0659,\n",
      "        -0.0496,  0.0946,  0.0137,  0.1505, -0.0582,  0.0473, -0.2295, -0.1048,\n",
      "        -0.0118, -0.1000, -0.0767,  0.0394, -0.0572,  0.0360, -0.1225,  0.1299,\n",
      "        -0.0898,  0.0036, -0.1052,  0.1738, -0.1547, -0.0046, -0.0737,  0.0932,\n",
      "        -0.1146, -0.0283,  0.0006,  0.0109,  0.1251,  0.1539, -0.1232,  0.0430,\n",
      "        -0.1511,  0.0493, -0.0529,  0.0596,  0.0372,  0.1905, -0.1539,  0.0558,\n",
      "         0.1288,  0.1966,  0.0664, -0.0392,  0.0058, -0.1144,  0.0046, -0.0110,\n",
      "         0.0415,  0.1495,  0.1298, -0.0630, -0.1861, -0.0034, -0.0232, -0.1339,\n",
      "         0.0198, -0.0104,  0.0531,  0.0360,  0.0969, -0.0470, -0.1119,  0.1577,\n",
      "         0.0795, -0.0187, -0.0610, -0.0518,  0.0902, -0.1077,  0.0492, -0.0430],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 3.9019e-02,  2.5154e-02,  9.0798e-02, -9.7679e-02,  2.8764e-02,\n",
      "         7.0434e-02,  8.4063e-02,  2.5928e-02,  9.7380e-02,  1.6275e-02,\n",
      "         8.7966e-02,  1.8267e-02, -2.7582e-02,  3.0036e-02, -1.6191e-01,\n",
      "        -8.6450e-02, -1.8375e-03,  8.3683e-02,  1.2906e-02, -1.4225e-02,\n",
      "         1.1338e-01,  3.5779e-02,  1.1134e-01,  3.2772e-02,  3.0779e-02,\n",
      "         4.5491e-02,  6.9465e-02, -7.9200e-02,  1.2984e-01,  1.1974e-01,\n",
      "         5.7609e-02, -1.1568e-01, -1.6414e-01,  3.0509e-02,  6.8447e-02,\n",
      "        -6.2259e-02,  1.0754e-01, -2.4923e-02,  4.4897e-02,  5.2062e-02,\n",
      "         1.1688e-01,  1.9003e-01,  2.0063e-02,  6.7085e-02,  1.0077e-01,\n",
      "         5.4456e-02, -6.7470e-02,  1.3327e-01, -1.5840e-03, -6.3881e-02,\n",
      "         3.4684e-02,  3.7240e-02,  6.3400e-02, -2.8443e-02, -9.6012e-02,\n",
      "        -6.2349e-02, -1.1489e-01,  7.2854e-02, -1.0937e-01,  9.2610e-02,\n",
      "        -2.1004e-04, -3.7679e-02, -9.4393e-03,  2.1505e-01,  1.5926e-01,\n",
      "        -7.5369e-02,  1.9417e-01,  2.1636e-02, -8.1999e-02, -1.8520e-01,\n",
      "         1.7142e-02, -2.9865e-02, -4.0532e-02, -9.7150e-02,  1.5350e-01,\n",
      "        -1.8486e-01, -5.9385e-03,  1.1371e-02, -3.8125e-02, -1.3841e-02,\n",
      "        -2.0428e-02, -1.0368e-01, -4.4102e-02, -2.1970e-02,  1.0765e-02,\n",
      "        -5.5344e-02,  7.0728e-02,  4.6190e-02,  6.2990e-02, -5.4066e-02,\n",
      "        -1.6213e-01, -3.9405e-02,  1.3400e-01,  8.2171e-02, -2.3741e-01,\n",
      "         1.0548e-01, -9.5347e-02, -3.4751e-02, -3.8221e-02,  9.9864e-02,\n",
      "        -1.1664e-01,  1.0195e-01,  3.3650e-02,  8.7214e-03, -5.3215e-02,\n",
      "         7.9702e-02, -9.8969e-02,  1.6521e-01,  4.4730e-02, -2.8468e-02,\n",
      "        -7.8716e-02, -6.4015e-03,  1.3884e-01, -2.8019e-02,  2.1341e-01,\n",
      "        -9.0670e-02,  9.4125e-02, -6.3092e-02,  5.8177e-02,  6.7590e-02,\n",
      "         4.8755e-02, -9.2646e-02, -1.4212e-01, -6.4560e-02, -2.2409e-01,\n",
      "         1.1573e-03,  1.1887e-01, -7.8905e-02, -7.3512e-02, -3.9799e-02,\n",
      "         1.3260e-01,  4.2742e-02,  7.5729e-02,  3.1080e-02, -6.4224e-02,\n",
      "        -1.1484e-01,  1.0520e-01,  7.3674e-02, -1.0186e-01, -3.0466e-02,\n",
      "        -4.0072e-02,  3.1730e-02,  9.9246e-02, -7.0664e-02,  2.0741e-02,\n",
      "        -1.1007e-03, -2.3973e-01,  9.8560e-02,  4.4328e-02,  1.3719e-01,\n",
      "         1.7300e-01,  5.2069e-02, -1.3163e-02, -4.8457e-03,  1.1188e-01,\n",
      "        -4.3641e-02,  9.1629e-02, -7.9687e-02, -3.0985e-02, -5.0387e-02,\n",
      "        -1.0267e-02, -8.4093e-02, -1.3090e-01, -9.6986e-02, -1.1397e-01,\n",
      "         1.7749e-01, -7.0455e-03,  8.1856e-02, -3.4062e-02,  5.2526e-02,\n",
      "        -2.2659e-01,  1.9467e-01,  7.6326e-02,  2.5363e-03,  6.2256e-02,\n",
      "         9.6879e-02,  1.5179e-01, -1.3537e-01, -1.3574e-01, -2.1640e-03,\n",
      "        -1.6477e-01,  1.0380e-01,  6.0904e-02,  1.8073e-02, -5.3058e-02,\n",
      "         4.4933e-02,  2.5375e-01,  2.1265e-03, -2.3270e-02, -1.6723e-02,\n",
      "        -3.1699e-02, -1.4938e-01, -2.2361e-02,  2.1272e-03, -7.7590e-02,\n",
      "        -1.0800e-01, -2.0303e-02,  1.5057e-01, -2.4740e-02,  8.9488e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([-0.1176, -0.0570, -0.0881,  0.0382, -0.0470, -0.0022, -0.0419,  0.0347,\n",
      "        -0.0455,  0.1512, -0.2212,  0.0205,  0.0492,  0.0188,  0.1280,  0.0916,\n",
      "        -0.2246, -0.0407, -0.0579, -0.0697, -0.0332, -0.0426, -0.0589, -0.0386,\n",
      "         0.1296, -0.1049,  0.0089, -0.1006,  0.0838,  0.0634, -0.0685,  0.0280,\n",
      "         0.0230, -0.0933, -0.0234,  0.0639,  0.0010,  0.0445,  0.0817, -0.1759,\n",
      "         0.0889, -0.1021,  0.0453,  0.0165, -0.1924,  0.0662,  0.0422, -0.1617,\n",
      "         0.0394, -0.0525, -0.1020,  0.0969,  0.1674, -0.0956,  0.0062, -0.2188,\n",
      "        -0.1477, -0.0795, -0.0521,  0.0115, -0.0537, -0.0015,  0.0250,  0.0240,\n",
      "        -0.2109, -0.0307, -0.1652, -0.1158,  0.0194, -0.1232, -0.0790,  0.0154,\n",
      "        -0.1488, -0.0429, -0.0399,  0.0404, -0.0934,  0.0394, -0.0222,  0.0784,\n",
      "        -0.0192,  0.0321,  0.1407,  0.0594,  0.0645,  0.2660,  0.1538, -0.0809,\n",
      "         0.0616,  0.0261,  0.0989, -0.0503, -0.0319,  0.0858, -0.1420, -0.0518,\n",
      "         0.1445, -0.0124, -0.0205, -0.1577, -0.0543, -0.0750,  0.0879,  0.0205,\n",
      "        -0.0879,  0.1437, -0.1579, -0.1448, -0.1457, -0.0660, -0.0659, -0.0375,\n",
      "         0.0527, -0.0404,  0.1653,  0.0289,  0.1094, -0.1015,  0.1626, -0.0075,\n",
      "         0.1662,  0.2571,  0.0893,  0.0890, -0.0148, -0.0483, -0.0005,  0.0456,\n",
      "         0.0326, -0.0538, -0.2244,  0.0251, -0.0627,  0.0730, -0.0966, -0.1493,\n",
      "        -0.0732,  0.0858,  0.1233,  0.2035,  0.0183,  0.1147,  0.1068,  0.0161,\n",
      "         0.0902, -0.0651, -0.0715,  0.0183,  0.0563,  0.1722,  0.0345, -0.0602,\n",
      "        -0.1160,  0.0269, -0.1717,  0.0408,  0.0133,  0.1677,  0.0753,  0.2172,\n",
      "        -0.0146,  0.0282, -0.0693,  0.1199, -0.0784, -0.0165,  0.1006,  0.0451,\n",
      "        -0.1145,  0.1461,  0.0647, -0.0945,  0.0623, -0.1480, -0.0218, -0.1240,\n",
      "        -0.0219,  0.0630, -0.0281, -0.0556,  0.0266,  0.0834, -0.0797, -0.1292,\n",
      "         0.0918,  0.0871,  0.0766, -0.0546, -0.1768, -0.0866, -0.0672,  0.0826,\n",
      "        -0.1746,  0.0408, -0.1357, -0.1097, -0.0017,  0.0469, -0.0158, -0.1136],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0365,  0.1492,  0.1437,  0.1214, -0.1905,  0.0414,  0.0920, -0.1194,\n",
      "        -0.0793,  0.0523, -0.0206, -0.0877, -0.1264, -0.0247,  0.0060,  0.1693,\n",
      "        -0.1031,  0.0021,  0.1151,  0.0680, -0.0114, -0.1083,  0.0192, -0.1049,\n",
      "         0.0003,  0.1907, -0.1256,  0.0967,  0.0163,  0.0660,  0.1730, -0.0695,\n",
      "         0.1811,  0.0153, -0.0434, -0.0872,  0.0316, -0.0302, -0.0075, -0.0259,\n",
      "        -0.0154,  0.0773, -0.1435,  0.1663, -0.0149, -0.0396,  0.1477,  0.2279,\n",
      "         0.0989,  0.0160,  0.0486,  0.0256,  0.0010,  0.0822,  0.0260,  0.0013,\n",
      "        -0.0255, -0.0615,  0.0043, -0.1938, -0.0396, -0.0095,  0.0934,  0.0380,\n",
      "        -0.0638,  0.1835, -0.0801, -0.1317,  0.2654,  0.0595, -0.0071, -0.1468,\n",
      "        -0.0288, -0.1604,  0.0815, -0.0099, -0.0089, -0.0636,  0.0295, -0.0150,\n",
      "        -0.1872, -0.0362,  0.1606, -0.1631, -0.0683,  0.1717, -0.1120, -0.0207,\n",
      "        -0.0738, -0.0534,  0.1167,  0.0465, -0.1569,  0.0623,  0.1992, -0.0154,\n",
      "        -0.0664,  0.0287,  0.0269, -0.0208,  0.0981,  0.1508,  0.0021,  0.0982,\n",
      "         0.0723,  0.0228,  0.0112,  0.0613,  0.1816, -0.1070,  0.0553,  0.0236,\n",
      "        -0.0179, -0.0131,  0.0833, -0.0739,  0.0031,  0.0167,  0.0272, -0.1319,\n",
      "         0.0396,  0.0491,  0.0588,  0.0015, -0.0071,  0.1565, -0.0120, -0.1833,\n",
      "         0.1558,  0.1691,  0.0405, -0.1244, -0.0768, -0.0768, -0.1019,  0.1306,\n",
      "         0.0825, -0.0200, -0.0242,  0.0078,  0.0199, -0.1735, -0.1019, -0.0651,\n",
      "        -0.1056, -0.0799,  0.1560,  0.1587, -0.0255,  0.1213, -0.0596,  0.1384,\n",
      "        -0.0424,  0.0252, -0.1582,  0.1200, -0.0028, -0.1406, -0.2185,  0.1694,\n",
      "         0.0334,  0.1966, -0.0360,  0.1704,  0.2084, -0.1135, -0.0869,  0.1150,\n",
      "         0.0498,  0.0928, -0.0948, -0.0336,  0.1267,  0.0648, -0.0360,  0.0417,\n",
      "        -0.0814, -0.0225,  0.2085, -0.0853,  0.0700,  0.1192, -0.0077,  0.0075,\n",
      "        -0.0558, -0.1166,  0.0397,  0.0466,  0.0524, -0.0005,  0.0760,  0.0648,\n",
      "        -0.0572, -0.0123, -0.1625, -0.2079,  0.0009,  0.1398, -0.1563, -0.0761],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-0.0874, -0.2878,  0.0162,  0.1137, -0.0216, -0.0095, -0.0791, -0.0112,\n",
      "         0.1378,  0.0276, -0.0191,  0.0996, -0.0046,  0.0440, -0.1734, -0.1609,\n",
      "        -0.0709, -0.2547,  0.0804,  0.1218,  0.2098, -0.1937, -0.0028, -0.1298,\n",
      "        -0.0120,  0.0623, -0.1709, -0.0088,  0.0245, -0.0329, -0.1025, -0.0525,\n",
      "        -0.0827,  0.2040, -0.0738, -0.0449, -0.0073,  0.0925,  0.0297, -0.0470,\n",
      "        -0.0164,  0.1825, -0.2102, -0.0393, -0.0126,  0.0600, -0.0207, -0.1989,\n",
      "         0.0681,  0.0819, -0.0137,  0.0233,  0.0062, -0.0912, -0.0540, -0.0826,\n",
      "         0.0754,  0.1373,  0.1662, -0.0536, -0.0954, -0.1096, -0.1003, -0.0158,\n",
      "        -0.2121,  0.0899, -0.2017,  0.0826,  0.1063,  0.1378, -0.0192,  0.1304,\n",
      "         0.2962, -0.0886,  0.0716,  0.2377,  0.0810,  0.0063,  0.0698,  0.1793,\n",
      "        -0.0389,  0.0985, -0.1307, -0.2917, -0.1084, -0.1743,  0.0437,  0.0713,\n",
      "        -0.0190, -0.1089, -0.1086,  0.0735,  0.0281,  0.0158,  0.0761, -0.2097,\n",
      "         0.1789, -0.1032,  0.0685, -0.1225,  0.0463,  0.2206,  0.0182,  0.0383,\n",
      "        -0.0163,  0.0126, -0.0634,  0.0739, -0.1272, -0.0418,  0.0581, -0.0152,\n",
      "        -0.1131,  0.0571,  0.0255, -0.0762, -0.1088, -0.0394,  0.0134, -0.1451,\n",
      "        -0.0832, -0.0144,  0.1111, -0.0474,  0.1000,  0.0990,  0.0673, -0.0446,\n",
      "        -0.0586,  0.0127,  0.0218, -0.0582,  0.2106,  0.0093, -0.1277, -0.0548,\n",
      "        -0.0257,  0.2035, -0.1348, -0.0572, -0.0537, -0.0828, -0.1483,  0.0311,\n",
      "        -0.0788,  0.0215, -0.1438, -0.0198, -0.1424,  0.1543, -0.1733, -0.0286,\n",
      "        -0.1172, -0.0252,  0.0334, -0.0683,  0.1256, -0.0856, -0.0397, -0.0185,\n",
      "        -0.1631, -0.1860,  0.0950, -0.1286, -0.0509,  0.0371,  0.0340, -0.0386,\n",
      "        -0.0047,  0.0548, -0.0648, -0.0035,  0.2056, -0.1304,  0.0738,  0.0144,\n",
      "         0.0309, -0.1177, -0.1432,  0.0321,  0.0579, -0.1703, -0.0908,  0.0473,\n",
      "         0.1625, -0.0337,  0.0552,  0.0707,  0.0216, -0.0073,  0.0840, -0.0357,\n",
      "        -0.0778,  0.0333,  0.0204,  0.1302, -0.0612, -0.0760,  0.0045, -0.0715],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 0.1300, -0.0290,  0.0912, -0.1956,  0.0597,  0.0285, -0.1028,  0.0317,\n",
      "        -0.0103,  0.1087,  0.0391,  0.0249,  0.1348,  0.0188,  0.2061, -0.0331,\n",
      "        -0.0511,  0.1448, -0.0687, -0.0512,  0.1953, -0.1046, -0.1168,  0.0371,\n",
      "        -0.0454,  0.1605,  0.2064,  0.0105,  0.0217, -0.1555,  0.0123, -0.0809,\n",
      "         0.0847,  0.0267, -0.2364, -0.1307,  0.0148, -0.1221,  0.1381, -0.0256,\n",
      "         0.0939,  0.0111, -0.0287, -0.2360,  0.0629,  0.1009,  0.0447,  0.1768,\n",
      "        -0.0366, -0.0532,  0.2044,  0.1877, -0.0016,  0.1828, -0.1052, -0.1184,\n",
      "        -0.2589,  0.1056,  0.2014, -0.0456,  0.0197, -0.0621,  0.0749, -0.0556,\n",
      "        -0.1770,  0.1183,  0.2220, -0.0243, -0.0030,  0.0711,  0.0164, -0.0922,\n",
      "         0.0700, -0.0595,  0.1727,  0.0145,  0.0382, -0.3578, -0.0543,  0.0084,\n",
      "         0.0855, -0.0105, -0.0112,  0.0679, -0.1331,  0.0217,  0.0218,  0.0706,\n",
      "        -0.1121, -0.0037,  0.0218, -0.0264, -0.0442,  0.0885, -0.0255, -0.0512,\n",
      "        -0.0117, -0.0858, -0.1294, -0.0291, -0.0313,  0.1300, -0.1170,  0.0544,\n",
      "         0.0718,  0.1288, -0.1224,  0.1531, -0.0877,  0.0856, -0.0822, -0.0295,\n",
      "         0.0070, -0.0089, -0.1059, -0.1029,  0.1119, -0.1977,  0.1345, -0.1227,\n",
      "        -0.1313, -0.0701,  0.0925,  0.0867, -0.0602, -0.0454, -0.1162,  0.1331,\n",
      "        -0.0070,  0.0286,  0.1760, -0.1712,  0.0136,  0.0362,  0.0169, -0.0475,\n",
      "        -0.0543, -0.1353, -0.0515,  0.0787, -0.0935, -0.2302,  0.0657, -0.0193,\n",
      "        -0.0741, -0.1347, -0.1320, -0.0633,  0.0482,  0.0237,  0.0235,  0.1115,\n",
      "         0.0948,  0.2133,  0.0239,  0.0536,  0.0297,  0.1637, -0.1271,  0.0527,\n",
      "         0.0373, -0.0568,  0.0910, -0.0004, -0.1461,  0.1952, -0.0235,  0.0972,\n",
      "         0.0026,  0.0695,  0.1002,  0.0492, -0.0383, -0.0427, -0.0296,  0.0046,\n",
      "         0.0502,  0.0025,  0.0582,  0.1880, -0.0915, -0.0357,  0.0783,  0.0295,\n",
      "        -0.0559, -0.1317,  0.0729, -0.0005,  0.0163, -0.0260, -0.0636, -0.0965,\n",
      "        -0.0161,  0.0863,  0.0049, -0.0130, -0.0310,  0.0269,  0.0490, -0.0194],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-0.0755,  0.1157,  0.1487,  0.0312,  0.0088, -0.0934,  0.1386,  0.0494,\n",
      "         0.1581,  0.0538, -0.1382,  0.1405, -0.0518, -0.1131, -0.0435,  0.0108,\n",
      "        -0.0136, -0.0261,  0.0036, -0.0735,  0.0057,  0.1861, -0.0205, -0.0147,\n",
      "         0.1115, -0.1055, -0.0333, -0.0581,  0.0298, -0.1677,  0.0853, -0.1542,\n",
      "         0.0841, -0.0789,  0.1823,  0.0546, -0.1157, -0.0021, -0.0624, -0.0842,\n",
      "        -0.0643,  0.0172, -0.0460, -0.0905, -0.1069,  0.0294,  0.1544, -0.0694,\n",
      "         0.0053,  0.0024,  0.1008,  0.1026,  0.2395,  0.0446, -0.0760,  0.0411,\n",
      "        -0.0543,  0.0697, -0.0095,  0.2348, -0.0666,  0.0692, -0.0046,  0.0664,\n",
      "        -0.0122, -0.0853,  0.0614, -0.0631,  0.1126, -0.1014, -0.0805,  0.0063,\n",
      "         0.0694, -0.0091, -0.1463,  0.0367, -0.0787,  0.1328, -0.1187,  0.0344,\n",
      "         0.1351, -0.1024,  0.0370, -0.0922, -0.0747,  0.0126,  0.0906, -0.0895,\n",
      "        -0.1623,  0.0673,  0.0384, -0.0280,  0.0264,  0.0056,  0.0317, -0.0555,\n",
      "         0.0636, -0.2060,  0.0456,  0.0843, -0.0197,  0.0903,  0.2240,  0.1724,\n",
      "         0.0046,  0.0300,  0.0270,  0.0626,  0.1121,  0.0250, -0.0804,  0.0623,\n",
      "         0.0696,  0.0334,  0.0043, -0.2056,  0.1308,  0.0856, -0.0198, -0.0268,\n",
      "         0.1300, -0.0193,  0.0783,  0.0175, -0.0085,  0.0610, -0.0509,  0.0733,\n",
      "         0.0186, -0.0053,  0.0114, -0.0056, -0.0509,  0.0588,  0.1049,  0.0542,\n",
      "         0.1166, -0.1209,  0.0434,  0.1118,  0.0814, -0.0112,  0.0703, -0.0407,\n",
      "        -0.0151, -0.0558,  0.0140,  0.1696, -0.0412,  0.1057,  0.0986, -0.0241,\n",
      "        -0.0095, -0.0080, -0.0638, -0.0291,  0.0576, -0.0776, -0.1520, -0.1818,\n",
      "         0.0851, -0.0793,  0.1604, -0.0665,  0.0501,  0.1502, -0.1287,  0.0479,\n",
      "        -0.1453,  0.0263, -0.0035,  0.0960,  0.0638, -0.1566,  0.0418,  0.0929,\n",
      "         0.1142, -0.0140,  0.0344,  0.1191, -0.1846, -0.0907,  0.1322, -0.0362,\n",
      "         0.0544, -0.1205, -0.0043,  0.0934, -0.0007,  0.0405,  0.1871, -0.0829,\n",
      "        -0.0702,  0.1021,  0.0760,  0.0168, -0.0093, -0.1217,  0.0335,  0.1808],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 0.0619,  0.1403, -0.1077, -0.2767,  0.2080,  0.0575, -0.0134, -0.1055,\n",
      "         0.1499,  0.1858,  0.0217,  0.1187, -0.0649,  0.2870, -0.1568,  0.1042,\n",
      "        -0.1069, -0.1348,  0.0584, -0.1022, -0.0325, -0.0836,  0.0143,  0.0472,\n",
      "         0.0017, -0.0113, -0.0767, -0.1494,  0.0455, -0.1245, -0.0429,  0.0766,\n",
      "         0.0107, -0.0946, -0.1371,  0.1191,  0.1512,  0.0053,  0.0394, -0.0465,\n",
      "         0.0175, -0.1627,  0.0771,  0.0206,  0.1903, -0.1087,  0.1277,  0.0255,\n",
      "         0.1399,  0.1205,  0.1156, -0.0843,  0.0157,  0.0475, -0.1077, -0.0837,\n",
      "        -0.0339, -0.1300, -0.0038, -0.0063, -0.1576,  0.0628,  0.1015, -0.0629,\n",
      "        -0.0113,  0.1316,  0.1097,  0.0832,  0.0638, -0.0506, -0.0556, -0.0724,\n",
      "        -0.0188, -0.0401, -0.1590,  0.0450,  0.0624, -0.0515, -0.0752, -0.0796,\n",
      "        -0.0007, -0.3024, -0.2144, -0.0835, -0.0758, -0.1519, -0.1939,  0.1572,\n",
      "        -0.1857,  0.1345, -0.1711, -0.0674,  0.0412, -0.1714, -0.0072, -0.2019,\n",
      "         0.0787,  0.0984,  0.0226, -0.0245, -0.0423,  0.1742, -0.2161, -0.0183,\n",
      "        -0.0029, -0.0044,  0.1192,  0.0439,  0.1932,  0.0082,  0.1160, -0.1654,\n",
      "        -0.0213, -0.1270, -0.0089, -0.0236, -0.0589,  0.1086,  0.1448, -0.0744,\n",
      "        -0.0636, -0.0634,  0.0920, -0.0227, -0.0138, -0.2378,  0.0915,  0.0410,\n",
      "        -0.0771, -0.2554, -0.0056, -0.0511,  0.0792, -0.1065,  0.0931,  0.0945,\n",
      "         0.1621, -0.0576,  0.0169,  0.0327,  0.0696,  0.0691, -0.0391,  0.1367,\n",
      "        -0.0409, -0.0418,  0.0063, -0.0193, -0.0210, -0.0267,  0.1050, -0.0354,\n",
      "         0.1055,  0.0792,  0.0318, -0.0377,  0.0276, -0.1005, -0.1378, -0.0189,\n",
      "         0.0850,  0.1300, -0.0152, -0.1383,  0.0798, -0.1142, -0.0370, -0.0458,\n",
      "        -0.1406,  0.0460, -0.0168,  0.1019,  0.0849,  0.0184,  0.0901, -0.1363,\n",
      "         0.2124,  0.0447, -0.1437,  0.0413, -0.3102,  0.0478,  0.0038, -0.0860,\n",
      "        -0.1465, -0.0472, -0.1059,  0.2646,  0.0621, -0.1125,  0.0021,  0.0192,\n",
      "        -0.0153, -0.1243, -0.0433, -0.1272, -0.0750,  0.0914, -0.1973,  0.0438],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([-4.2623e-02, -3.6569e-02,  2.1554e-01,  7.3948e-02,  4.5928e-02,\n",
      "         8.9498e-02,  5.3931e-02,  7.0713e-02, -1.2092e-01,  1.1116e-01,\n",
      "        -3.0323e-02, -1.2760e-01, -8.4349e-02,  8.3266e-02, -1.4023e-01,\n",
      "        -1.5514e-02, -1.0077e-01,  1.6821e-02,  1.4161e-01,  4.8444e-02,\n",
      "         2.6380e-01,  1.6819e-02,  2.7638e-02,  1.6771e-01, -4.9164e-02,\n",
      "        -5.8186e-02,  3.7733e-02, -7.5490e-02, -1.2715e-01,  6.3545e-03,\n",
      "         4.5001e-02, -1.1637e-01,  4.3808e-02, -6.4871e-02, -1.9854e-01,\n",
      "        -8.2011e-02,  4.6362e-02,  4.2026e-02,  3.5503e-02, -9.2668e-03,\n",
      "        -6.1567e-02, -7.3332e-02,  1.2077e-01,  9.6574e-03, -1.1398e-01,\n",
      "        -8.3217e-02,  9.7229e-02, -1.8468e-01,  1.0019e-01,  1.1664e-01,\n",
      "        -4.1808e-03, -1.5724e-01, -1.2025e-01, -4.9540e-02,  8.5170e-02,\n",
      "         7.6881e-02,  1.2248e-01, -4.9930e-02, -1.4443e-01,  1.7782e-01,\n",
      "         4.8178e-02,  5.4946e-02, -4.6922e-02,  2.5569e-02,  6.4497e-02,\n",
      "         6.3852e-03, -2.8890e-02, -2.0665e-02,  6.1656e-02, -8.2958e-03,\n",
      "        -1.1087e-01, -4.8137e-02,  1.0483e-01,  6.2579e-02, -1.6083e-04,\n",
      "        -1.1344e-01, -8.9015e-02, -1.7166e-02, -4.1521e-02, -1.5454e-01,\n",
      "        -1.4731e-01,  8.7373e-02, -2.9756e-02,  2.0193e-02, -1.1296e-01,\n",
      "        -3.1650e-02,  4.1894e-03, -1.8676e-01,  6.8287e-02, -2.2091e-02,\n",
      "        -1.2922e-01,  2.7839e-02, -1.4709e-01, -4.7553e-02, -3.7346e-03,\n",
      "         2.2710e-02,  2.5883e-02, -2.9356e-02,  1.6152e-01,  1.6246e-01,\n",
      "        -1.3543e-01,  8.9756e-02, -4.2627e-02, -8.3722e-02, -1.5255e-01,\n",
      "         2.2945e-01, -1.2727e-01, -2.3556e-01, -2.0420e-02,  1.3193e-01,\n",
      "         4.7064e-02,  1.3503e-01, -1.1775e-01,  4.4378e-02, -3.4163e-02,\n",
      "        -1.9214e-01,  1.5731e-01, -3.1321e-02,  6.0982e-02, -5.8376e-03,\n",
      "         1.5299e-01, -5.1013e-02, -1.3059e-01, -4.6080e-02, -4.5595e-02,\n",
      "         1.3429e-02, -6.2867e-02, -9.5215e-02, -1.7407e-02,  7.2585e-02,\n",
      "         3.3903e-02, -8.6348e-02, -1.3825e-01,  4.7451e-02, -5.0096e-02,\n",
      "        -3.1774e-02,  6.5457e-02,  1.4885e-01, -2.2025e-01, -4.0868e-02,\n",
      "         9.5003e-02, -2.9685e-02, -9.5620e-03, -1.2427e-01,  1.7506e-02,\n",
      "        -6.4477e-02,  7.1862e-02,  7.0507e-03, -5.8418e-02,  1.5294e-03,\n",
      "         3.2851e-02,  1.9095e-02, -8.0235e-02,  6.8884e-02,  6.0092e-02,\n",
      "         1.5271e-01,  1.9154e-01, -1.1346e-01,  6.4830e-02,  3.5148e-02,\n",
      "         1.3541e-01, -3.6145e-02,  7.7164e-02,  6.6006e-02, -9.8362e-02,\n",
      "        -5.8074e-02, -6.8333e-02, -6.2350e-02, -1.1440e-01, -7.7808e-02,\n",
      "        -5.0657e-02, -1.3394e-01,  7.3470e-03,  2.3200e-02,  5.6048e-02,\n",
      "        -1.1629e-01,  1.7295e-02, -3.8359e-02, -9.3535e-02,  5.3482e-02,\n",
      "         9.3346e-02, -5.7120e-03,  4.7397e-02, -1.3661e-01,  4.1543e-02,\n",
      "        -3.0960e-02, -2.6104e-02,  3.4355e-02, -8.6857e-02,  9.2401e-03,\n",
      "        -1.4995e-01,  1.1801e-01, -5.6550e-03, -9.9898e-02,  1.3610e-02,\n",
      "         1.8182e-02, -1.3491e-01, -8.2483e-02, -6.1044e-02, -5.1449e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-0.0404,  0.0334, -0.0873, -0.0599, -0.0711,  0.0188,  0.0374,  0.0498,\n",
      "         0.0180,  0.0348, -0.1043,  0.1819, -0.0947,  0.0222,  0.0732,  0.0492,\n",
      "         0.1486,  0.1599, -0.0004,  0.1500,  0.0163, -0.0051,  0.1059,  0.0772,\n",
      "         0.0216,  0.1549, -0.0047,  0.0367,  0.0610, -0.1326,  0.2364, -0.0735,\n",
      "        -0.1667,  0.0856,  0.0432, -0.1610, -0.1542,  0.0944, -0.0258,  0.0386,\n",
      "         0.0133,  0.0888,  0.0012, -0.1798,  0.0380, -0.0701,  0.0899, -0.0979,\n",
      "         0.1967, -0.0406, -0.0714, -0.1593,  0.0574,  0.0158, -0.1353, -0.0116,\n",
      "        -0.0659, -0.0594, -0.0836,  0.1687,  0.1432, -0.1235, -0.1411, -0.1314,\n",
      "        -0.0911,  0.2131, -0.1926, -0.0544, -0.1664,  0.0304,  0.1048, -0.0392,\n",
      "        -0.0422, -0.1233,  0.0076, -0.0738, -0.1557,  0.1152,  0.0373, -0.0577,\n",
      "         0.0090, -0.0523,  0.0164,  0.0149, -0.0101, -0.0796,  0.0258, -0.1029,\n",
      "         0.0345, -0.0131, -0.0666, -0.0779,  0.0692, -0.0341, -0.0940, -0.1161,\n",
      "        -0.0177,  0.1019, -0.1398,  0.0373, -0.1376, -0.0576,  0.0860, -0.0356,\n",
      "         0.1078,  0.0419,  0.2311, -0.0366,  0.1426, -0.1082, -0.0577, -0.0187,\n",
      "         0.0658, -0.1028, -0.0874, -0.0337,  0.1840,  0.0999,  0.2344,  0.0839,\n",
      "         0.1116,  0.0364,  0.0575,  0.0951,  0.0945,  0.0339, -0.0087,  0.0691,\n",
      "         0.0113,  0.0132, -0.0651,  0.0311, -0.0628,  0.0944, -0.0417,  0.1208,\n",
      "         0.0239,  0.1379,  0.1112,  0.1446, -0.1183, -0.0495, -0.0503, -0.1729,\n",
      "         0.0460, -0.1241, -0.0561,  0.0750, -0.1225, -0.0539,  0.2356,  0.0499,\n",
      "        -0.0831,  0.0315,  0.1233,  0.1682, -0.1815,  0.1077,  0.1187,  0.1555,\n",
      "         0.1233, -0.0816, -0.0952, -0.1361,  0.0105,  0.0832,  0.0652,  0.0702,\n",
      "         0.0459,  0.0453, -0.0204, -0.0968, -0.0660,  0.0714, -0.0433,  0.0744,\n",
      "        -0.0709,  0.0625,  0.0672, -0.1082, -0.0944,  0.1826,  0.0588,  0.0226,\n",
      "         0.1245, -0.0560,  0.1531,  0.0472,  0.0311, -0.0327, -0.1191,  0.0361,\n",
      "        -0.1240,  0.0982,  0.0693,  0.0030, -0.1002,  0.2047,  0.0471, -0.0009],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0005000'], tr/val_loss:  2.280721/  4.389807, val:  20.00%, val_best:  20.00%, tr:  13.48%, tr_best:  13.48%, epoch time: 65.31 seconds, 1.09 minutes\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0005000'], tr/val_loss:  2.084042/  3.421953, val:  38.75%, val_best:  38.75%, tr:  29.93%, tr_best:  29.93%, epoch time: 65.42 seconds, 1.09 minutes\n",
      "epoch-2   lr=['0.0005000'], tr/val_loss:  1.989210/  4.148986, val:  47.50%, val_best:  47.50%, tr:  43.21%, tr_best:  43.21%, epoch time: 65.09 seconds, 1.08 minutes\n",
      "epoch-3   lr=['0.0005000'], tr/val_loss:  1.963167/  5.926650, val:  36.67%, val_best:  47.50%, tr:  48.83%, tr_best:  48.83%, epoch time: 66.16 seconds, 1.10 minutes\n",
      "epoch-4   lr=['0.0005000'], tr/val_loss:  1.920007/  4.299273, val:  49.58%, val_best:  49.58%, tr:  48.62%, tr_best:  48.83%, epoch time: 66.64 seconds, 1.11 minutes\n",
      "epoch-5   lr=['0.0005000'], tr/val_loss:  1.896321/  3.481785, val:  50.83%, val_best:  50.83%, tr:  51.38%, tr_best:  51.38%, epoch time: 66.05 seconds, 1.10 minutes\n",
      "epoch-6   lr=['0.0005000'], tr/val_loss:  1.866891/  5.502759, val:  48.75%, val_best:  50.83%, tr:  52.81%, tr_best:  52.81%, epoch time: 66.02 seconds, 1.10 minutes\n",
      "epoch-7   lr=['0.0005000'], tr/val_loss:  1.848471/  3.760907, val:  47.08%, val_best:  50.83%, tr:  54.95%, tr_best:  54.95%, epoch time: 66.34 seconds, 1.11 minutes\n",
      "epoch-8   lr=['0.0005000'], tr/val_loss:  1.840640/  2.796382, val:  57.08%, val_best:  57.08%, tr:  53.73%, tr_best:  54.95%, epoch time: 67.06 seconds, 1.12 minutes\n",
      "epoch-9   lr=['0.0005000'], tr/val_loss:  1.812426/  3.718451, val:  46.25%, val_best:  57.08%, tr:  53.73%, tr_best:  54.95%, epoch time: 65.44 seconds, 1.09 minutes\n",
      "epoch-10  lr=['0.0005000'], tr/val_loss:  1.817642/  4.794011, val:  52.92%, val_best:  57.08%, tr:  55.57%, tr_best:  55.57%, epoch time: 65.39 seconds, 1.09 minutes\n",
      "epoch-11  lr=['0.0005000'], tr/val_loss:  1.820210/  3.964753, val:  55.83%, val_best:  57.08%, tr:  54.14%, tr_best:  55.57%, epoch time: 67.00 seconds, 1.12 minutes\n",
      "epoch-12  lr=['0.0005000'], tr/val_loss:  1.777743/  4.357707, val:  42.50%, val_best:  57.08%, tr:  57.81%, tr_best:  57.81%, epoch time: 64.27 seconds, 1.07 minutes\n",
      "epoch-13  lr=['0.0005000'], tr/val_loss:  1.762822/  7.662119, val:  36.25%, val_best:  57.08%, tr:  56.18%, tr_best:  57.81%, epoch time: 66.43 seconds, 1.11 minutes\n",
      "epoch-14  lr=['0.0005000'], tr/val_loss:  1.737342/  5.926049, val:  49.17%, val_best:  57.08%, tr:  57.71%, tr_best:  57.81%, epoch time: 65.26 seconds, 1.09 minutes\n",
      "epoch-15  lr=['0.0005000'], tr/val_loss:  1.732159/  4.924495, val:  55.83%, val_best:  57.08%, tr:  57.41%, tr_best:  57.81%, epoch time: 65.15 seconds, 1.09 minutes\n",
      "epoch-16  lr=['0.0005000'], tr/val_loss:  1.667128/  6.596220, val:  49.17%, val_best:  57.08%, tr:  59.35%, tr_best:  59.35%, epoch time: 66.57 seconds, 1.11 minutes\n",
      "epoch-17  lr=['0.0005000'], tr/val_loss:  1.667787/  6.502138, val:  45.83%, val_best:  57.08%, tr:  59.04%, tr_best:  59.35%, epoch time: 66.52 seconds, 1.11 minutes\n",
      "epoch-18  lr=['0.0005000'], tr/val_loss:  1.681963/  7.332327, val:  47.50%, val_best:  57.08%, tr:  57.92%, tr_best:  59.35%, epoch time: 65.66 seconds, 1.09 minutes\n",
      "epoch-19  lr=['0.0005000'], tr/val_loss:  1.655699/  5.912611, val:  54.17%, val_best:  57.08%, tr:  59.86%, tr_best:  59.86%, epoch time: 65.83 seconds, 1.10 minutes\n",
      "epoch-20  lr=['0.0005000'], tr/val_loss:  1.641177/  8.897388, val:  45.42%, val_best:  57.08%, tr:  59.04%, tr_best:  59.86%, epoch time: 66.46 seconds, 1.11 minutes\n",
      "epoch-21  lr=['0.0005000'], tr/val_loss:  1.636952/  3.781071, val:  53.75%, val_best:  57.08%, tr:  56.38%, tr_best:  59.86%, epoch time: 67.04 seconds, 1.12 minutes\n",
      "epoch-22  lr=['0.0005000'], tr/val_loss:  1.632482/  6.002269, val:  54.58%, val_best:  57.08%, tr:  56.89%, tr_best:  59.86%, epoch time: 66.09 seconds, 1.10 minutes\n",
      "epoch-23  lr=['0.0005000'], tr/val_loss:  1.599182/ 10.171082, val:  36.67%, val_best:  57.08%, tr:  58.32%, tr_best:  59.86%, epoch time: 64.89 seconds, 1.08 minutes\n",
      "epoch-24  lr=['0.0005000'], tr/val_loss:  1.606575/ 12.918462, val:  42.92%, val_best:  57.08%, tr:  60.16%, tr_best:  60.16%, epoch time: 66.80 seconds, 1.11 minutes\n",
      "epoch-25  lr=['0.0005000'], tr/val_loss:  1.617902/  4.700231, val:  64.17%, val_best:  64.17%, tr:  56.49%, tr_best:  60.16%, epoch time: 65.48 seconds, 1.09 minutes\n",
      "epoch-26  lr=['0.0005000'], tr/val_loss:  1.555776/  5.379253, val:  49.17%, val_best:  64.17%, tr:  59.96%, tr_best:  60.16%, epoch time: 65.47 seconds, 1.09 minutes\n",
      "epoch-27  lr=['0.0005000'], tr/val_loss:  1.520230/  7.001034, val:  52.92%, val_best:  64.17%, tr:  60.37%, tr_best:  60.37%, epoch time: 65.02 seconds, 1.08 minutes\n",
      "epoch-28  lr=['0.0005000'], tr/val_loss:  1.492591/  7.336924, val:  57.50%, val_best:  64.17%, tr:  62.00%, tr_best:  62.00%, epoch time: 66.18 seconds, 1.10 minutes\n",
      "epoch-29  lr=['0.0005000'], tr/val_loss:  1.497365/  7.111561, val:  53.33%, val_best:  64.17%, tr:  63.02%, tr_best:  63.02%, epoch time: 66.03 seconds, 1.10 minutes\n",
      "epoch-30  lr=['0.0005000'], tr/val_loss:  1.448564/  8.505878, val:  56.25%, val_best:  64.17%, tr:  64.45%, tr_best:  64.45%, epoch time: 65.87 seconds, 1.10 minutes\n",
      "epoch-31  lr=['0.0005000'], tr/val_loss:  1.475157/  7.082075, val:  52.08%, val_best:  64.17%, tr:  63.64%, tr_best:  64.45%, epoch time: 66.16 seconds, 1.10 minutes\n",
      "epoch-32  lr=['0.0005000'], tr/val_loss:  1.469324/  7.147244, val:  55.42%, val_best:  64.17%, tr:  65.17%, tr_best:  65.17%, epoch time: 66.06 seconds, 1.10 minutes\n",
      "epoch-33  lr=['0.0005000'], tr/val_loss:  1.461354/  5.056909, val:  64.17%, val_best:  64.17%, tr:  62.51%, tr_best:  65.17%, epoch time: 65.20 seconds, 1.09 minutes\n",
      "epoch-34  lr=['0.0005000'], tr/val_loss:  1.434991/  6.578032, val:  53.75%, val_best:  64.17%, tr:  66.39%, tr_best:  66.39%, epoch time: 64.89 seconds, 1.08 minutes\n",
      "epoch-35  lr=['0.0005000'], tr/val_loss:  1.425247/  5.085095, val:  62.08%, val_best:  64.17%, tr:  65.68%, tr_best:  66.39%, epoch time: 65.74 seconds, 1.10 minutes\n",
      "epoch-36  lr=['0.0005000'], tr/val_loss:  1.421363/  4.191884, val:  63.33%, val_best:  64.17%, tr:  64.15%, tr_best:  66.39%, epoch time: 65.99 seconds, 1.10 minutes\n",
      "epoch-37  lr=['0.0005000'], tr/val_loss:  1.389517/  8.934866, val:  55.00%, val_best:  64.17%, tr:  68.03%, tr_best:  68.03%, epoch time: 66.03 seconds, 1.10 minutes\n",
      "epoch-38  lr=['0.0005000'], tr/val_loss:  1.409237/  7.160719, val:  55.00%, val_best:  64.17%, tr:  63.13%, tr_best:  68.03%, epoch time: 65.02 seconds, 1.08 minutes\n",
      "epoch-39  lr=['0.0005000'], tr/val_loss:  1.360351/  8.706309, val:  53.75%, val_best:  64.17%, tr:  65.17%, tr_best:  68.03%, epoch time: 65.40 seconds, 1.09 minutes\n",
      "epoch-40  lr=['0.0005000'], tr/val_loss:  1.378801/  6.360953, val:  57.50%, val_best:  64.17%, tr:  66.19%, tr_best:  68.03%, epoch time: 66.62 seconds, 1.11 minutes\n",
      "epoch-41  lr=['0.0005000'], tr/val_loss:  1.362507/  6.910841, val:  59.17%, val_best:  64.17%, tr:  66.39%, tr_best:  68.03%, epoch time: 64.76 seconds, 1.08 minutes\n",
      "epoch-42  lr=['0.0005000'], tr/val_loss:  1.376666/  6.852860, val:  57.92%, val_best:  64.17%, tr:  65.17%, tr_best:  68.03%, epoch time: 66.16 seconds, 1.10 minutes\n",
      "epoch-43  lr=['0.0005000'], tr/val_loss:  1.352509/  4.675076, val:  63.75%, val_best:  64.17%, tr:  66.29%, tr_best:  68.03%, epoch time: 66.74 seconds, 1.11 minutes\n",
      "epoch-44  lr=['0.0005000'], tr/val_loss:  1.336816/  7.933210, val:  60.83%, val_best:  64.17%, tr:  65.99%, tr_best:  68.03%, epoch time: 65.26 seconds, 1.09 minutes\n",
      "epoch-45  lr=['0.0005000'], tr/val_loss:  1.334416/  6.716795, val:  58.75%, val_best:  64.17%, tr:  66.80%, tr_best:  68.03%, epoch time: 67.56 seconds, 1.13 minutes\n",
      "epoch-46  lr=['0.0005000'], tr/val_loss:  1.331329/  7.560236, val:  58.33%, val_best:  64.17%, tr:  67.31%, tr_best:  68.03%, epoch time: 67.09 seconds, 1.12 minutes\n",
      "epoch-47  lr=['0.0005000'], tr/val_loss:  1.336823/  8.923028, val:  58.75%, val_best:  64.17%, tr:  67.72%, tr_best:  68.03%, epoch time: 64.98 seconds, 1.08 minutes\n",
      "epoch-48  lr=['0.0005000'], tr/val_loss:  1.314535/  6.314840, val:  59.58%, val_best:  64.17%, tr:  68.74%, tr_best:  68.74%, epoch time: 65.39 seconds, 1.09 minutes\n",
      "epoch-49  lr=['0.0005000'], tr/val_loss:  1.311360/  6.608384, val:  60.42%, val_best:  64.17%, tr:  67.01%, tr_best:  68.74%, epoch time: 65.05 seconds, 1.08 minutes\n",
      "epoch-50  lr=['0.0005000'], tr/val_loss:  1.281527/  9.707151, val:  55.00%, val_best:  64.17%, tr:  68.44%, tr_best:  68.74%, epoch time: 65.39 seconds, 1.09 minutes\n",
      "epoch-51  lr=['0.0005000'], tr/val_loss:  1.274302/  6.013309, val:  56.67%, val_best:  64.17%, tr:  68.95%, tr_best:  68.95%, epoch time: 65.43 seconds, 1.09 minutes\n",
      "epoch-52  lr=['0.0005000'], tr/val_loss:  1.292743/  8.433098, val:  54.17%, val_best:  64.17%, tr:  68.03%, tr_best:  68.95%, epoch time: 65.77 seconds, 1.10 minutes\n",
      "epoch-53  lr=['0.0005000'], tr/val_loss:  1.260763/  6.654589, val:  59.58%, val_best:  64.17%, tr:  67.72%, tr_best:  68.95%, epoch time: 64.87 seconds, 1.08 minutes\n",
      "epoch-54  lr=['0.0005000'], tr/val_loss:  1.234894/  7.096353, val:  60.83%, val_best:  64.17%, tr:  68.74%, tr_best:  68.95%, epoch time: 66.47 seconds, 1.11 minutes\n",
      "epoch-55  lr=['0.0005000'], tr/val_loss:  1.253675/  6.235535, val:  61.67%, val_best:  64.17%, tr:  67.52%, tr_best:  68.95%, epoch time: 66.46 seconds, 1.11 minutes\n",
      "epoch-56  lr=['0.0005000'], tr/val_loss:  1.254817/  5.623028, val:  62.08%, val_best:  64.17%, tr:  68.85%, tr_best:  68.95%, epoch time: 64.88 seconds, 1.08 minutes\n",
      "epoch-57  lr=['0.0005000'], tr/val_loss:  1.226133/  6.392003, val:  57.92%, val_best:  64.17%, tr:  68.23%, tr_best:  68.95%, epoch time: 65.31 seconds, 1.09 minutes\n",
      "epoch-58  lr=['0.0005000'], tr/val_loss:  1.245351/  7.352469, val:  57.50%, val_best:  64.17%, tr:  69.56%, tr_best:  69.56%, epoch time: 66.09 seconds, 1.10 minutes\n",
      "epoch-59  lr=['0.0005000'], tr/val_loss:  1.211186/  9.079722, val:  51.67%, val_best:  64.17%, tr:  68.13%, tr_best:  69.56%, epoch time: 65.64 seconds, 1.09 minutes\n",
      "epoch-60  lr=['0.0005000'], tr/val_loss:  1.171183/ 10.214548, val:  55.00%, val_best:  64.17%, tr:  70.68%, tr_best:  70.68%, epoch time: 66.58 seconds, 1.11 minutes\n",
      "epoch-61  lr=['0.0005000'], tr/val_loss:  1.202736/  9.401555, val:  51.25%, val_best:  64.17%, tr:  68.34%, tr_best:  70.68%, epoch time: 65.99 seconds, 1.10 minutes\n",
      "epoch-62  lr=['0.0005000'], tr/val_loss:  1.238997/  7.690936, val:  59.17%, val_best:  64.17%, tr:  66.50%, tr_best:  70.68%, epoch time: 65.55 seconds, 1.09 minutes\n",
      "epoch-63  lr=['0.0005000'], tr/val_loss:  1.221063/  8.440593, val:  57.92%, val_best:  64.17%, tr:  72.32%, tr_best:  72.32%, epoch time: 65.46 seconds, 1.09 minutes\n",
      "epoch-64  lr=['0.0005000'], tr/val_loss:  1.202403/ 11.942384, val:  57.50%, val_best:  64.17%, tr:  70.79%, tr_best:  72.32%, epoch time: 66.34 seconds, 1.11 minutes\n",
      "epoch-65  lr=['0.0005000'], tr/val_loss:  1.170574/  9.305732, val:  52.50%, val_best:  64.17%, tr:  69.77%, tr_best:  72.32%, epoch time: 66.56 seconds, 1.11 minutes\n",
      "epoch-66  lr=['0.0005000'], tr/val_loss:  1.184875/  8.086272, val:  54.58%, val_best:  64.17%, tr:  70.58%, tr_best:  72.32%, epoch time: 65.30 seconds, 1.09 minutes\n",
      "epoch-67  lr=['0.0005000'], tr/val_loss:  1.172572/ 11.311647, val:  55.42%, val_best:  64.17%, tr:  69.87%, tr_best:  72.32%, epoch time: 64.72 seconds, 1.08 minutes\n",
      "epoch-68  lr=['0.0005000'], tr/val_loss:  1.150712/  9.418085, val:  55.42%, val_best:  64.17%, tr:  71.09%, tr_best:  72.32%, epoch time: 65.09 seconds, 1.08 minutes\n",
      "epoch-69  lr=['0.0005000'], tr/val_loss:  1.158765/ 10.159642, val:  53.75%, val_best:  64.17%, tr:  70.07%, tr_best:  72.32%, epoch time: 65.82 seconds, 1.10 minutes\n",
      "epoch-70  lr=['0.0005000'], tr/val_loss:  1.150823/ 10.633502, val:  50.42%, val_best:  64.17%, tr:  70.68%, tr_best:  72.32%, epoch time: 66.74 seconds, 1.11 minutes\n",
      "epoch-71  lr=['0.0005000'], tr/val_loss:  1.159689/ 11.850266, val:  52.50%, val_best:  64.17%, tr:  69.97%, tr_best:  72.32%, epoch time: 66.90 seconds, 1.12 minutes\n",
      "epoch-72  lr=['0.0005000'], tr/val_loss:  1.190300/  9.957916, val:  53.75%, val_best:  64.17%, tr:  68.13%, tr_best:  72.32%, epoch time: 66.71 seconds, 1.11 minutes\n",
      "epoch-73  lr=['0.0005000'], tr/val_loss:  1.130468/  9.370748, val:  56.67%, val_best:  64.17%, tr:  71.81%, tr_best:  72.32%, epoch time: 67.45 seconds, 1.12 minutes\n",
      "epoch-74  lr=['0.0005000'], tr/val_loss:  1.141032/ 10.052430, val:  60.00%, val_best:  64.17%, tr:  69.46%, tr_best:  72.32%, epoch time: 67.22 seconds, 1.12 minutes\n",
      "epoch-75  lr=['0.0005000'], tr/val_loss:  1.141150/  7.811800, val:  58.33%, val_best:  64.17%, tr:  72.42%, tr_best:  72.42%, epoch time: 66.37 seconds, 1.11 minutes\n",
      "epoch-76  lr=['0.0005000'], tr/val_loss:  1.162192/  7.387806, val:  61.25%, val_best:  64.17%, tr:  69.66%, tr_best:  72.42%, epoch time: 67.74 seconds, 1.13 minutes\n",
      "epoch-77  lr=['0.0005000'], tr/val_loss:  1.143384/ 10.892923, val:  55.42%, val_best:  64.17%, tr:  72.22%, tr_best:  72.42%, epoch time: 66.29 seconds, 1.10 minutes\n",
      "epoch-78  lr=['0.0005000'], tr/val_loss:  1.093123/ 10.533727, val:  52.08%, val_best:  64.17%, tr:  73.24%, tr_best:  73.24%, epoch time: 66.42 seconds, 1.11 minutes\n",
      "epoch-79  lr=['0.0005000'], tr/val_loss:  1.121084/  9.191558, val:  58.75%, val_best:  64.17%, tr:  71.09%, tr_best:  73.24%, epoch time: 66.05 seconds, 1.10 minutes\n",
      "epoch-80  lr=['0.0005000'], tr/val_loss:  1.100054/  7.516139, val:  62.92%, val_best:  64.17%, tr:  72.11%, tr_best:  73.24%, epoch time: 66.13 seconds, 1.10 minutes\n",
      "epoch-81  lr=['0.0005000'], tr/val_loss:  1.080169/ 11.346286, val:  50.42%, val_best:  64.17%, tr:  72.42%, tr_best:  73.24%, epoch time: 68.19 seconds, 1.14 minutes\n",
      "epoch-82  lr=['0.0005000'], tr/val_loss:  1.093136/  8.132195, val:  60.42%, val_best:  64.17%, tr:  71.09%, tr_best:  73.24%, epoch time: 66.93 seconds, 1.12 minutes\n",
      "epoch-83  lr=['0.0005000'], tr/val_loss:  1.122237/ 10.996050, val:  53.33%, val_best:  64.17%, tr:  70.58%, tr_best:  73.24%, epoch time: 65.55 seconds, 1.09 minutes\n",
      "epoch-84  lr=['0.0005000'], tr/val_loss:  1.151754/ 10.273059, val:  55.00%, val_best:  64.17%, tr:  69.46%, tr_best:  73.24%, epoch time: 64.91 seconds, 1.08 minutes\n",
      "epoch-85  lr=['0.0005000'], tr/val_loss:  1.118043/  6.732955, val:  58.33%, val_best:  64.17%, tr:  71.30%, tr_best:  73.24%, epoch time: 68.14 seconds, 1.14 minutes\n",
      "epoch-86  lr=['0.0005000'], tr/val_loss:  1.143110/  9.862742, val:  58.33%, val_best:  64.17%, tr:  71.91%, tr_best:  73.24%, epoch time: 65.80 seconds, 1.10 minutes\n",
      "epoch-87  lr=['0.0005000'], tr/val_loss:  1.114151/ 10.426276, val:  57.50%, val_best:  64.17%, tr:  71.09%, tr_best:  73.24%, epoch time: 65.02 seconds, 1.08 minutes\n",
      "epoch-88  lr=['0.0005000'], tr/val_loss:  1.124367/  8.191144, val:  59.17%, val_best:  64.17%, tr:  70.17%, tr_best:  73.24%, epoch time: 67.18 seconds, 1.12 minutes\n",
      "epoch-89  lr=['0.0005000'], tr/val_loss:  1.105202/  7.209976, val:  62.08%, val_best:  64.17%, tr:  74.46%, tr_best:  74.46%, epoch time: 66.41 seconds, 1.11 minutes\n",
      "epoch-90  lr=['0.0005000'], tr/val_loss:  1.122634/  8.800966, val:  61.25%, val_best:  64.17%, tr:  71.50%, tr_best:  74.46%, epoch time: 66.78 seconds, 1.11 minutes\n",
      "epoch-91  lr=['0.0005000'], tr/val_loss:  1.104112/  7.794798, val:  59.58%, val_best:  64.17%, tr:  73.03%, tr_best:  74.46%, epoch time: 66.22 seconds, 1.10 minutes\n",
      "epoch-92  lr=['0.0005000'], tr/val_loss:  1.112332/  8.115844, val:  62.08%, val_best:  64.17%, tr:  72.42%, tr_best:  74.46%, epoch time: 66.55 seconds, 1.11 minutes\n",
      "epoch-93  lr=['0.0005000'], tr/val_loss:  1.110630/  6.516997, val:  58.33%, val_best:  64.17%, tr:  73.34%, tr_best:  74.46%, epoch time: 66.38 seconds, 1.11 minutes\n",
      "epoch-94  lr=['0.0005000'], tr/val_loss:  1.096663/  8.769288, val:  53.33%, val_best:  64.17%, tr:  72.22%, tr_best:  74.46%, epoch time: 66.02 seconds, 1.10 minutes\n",
      "epoch-95  lr=['0.0005000'], tr/val_loss:  1.058180/  8.544480, val:  57.92%, val_best:  64.17%, tr:  72.22%, tr_best:  74.46%, epoch time: 66.94 seconds, 1.12 minutes\n",
      "epoch-96  lr=['0.0005000'], tr/val_loss:  1.042304/  9.014621, val:  59.58%, val_best:  64.17%, tr:  74.87%, tr_best:  74.87%, epoch time: 65.24 seconds, 1.09 minutes\n",
      "epoch-97  lr=['0.0005000'], tr/val_loss:  1.036449/  6.844518, val:  57.08%, val_best:  64.17%, tr:  74.26%, tr_best:  74.87%, epoch time: 66.77 seconds, 1.11 minutes\n",
      "epoch-98  lr=['0.0005000'], tr/val_loss:  1.076535/  7.173059, val:  62.50%, val_best:  64.17%, tr:  71.71%, tr_best:  74.87%, epoch time: 66.86 seconds, 1.11 minutes\n",
      "epoch-99  lr=['0.0005000'], tr/val_loss:  1.071823/  9.485410, val:  57.08%, val_best:  64.17%, tr:  72.73%, tr_best:  74.87%, epoch time: 66.42 seconds, 1.11 minutes\n",
      "epoch-100 lr=['0.0005000'], tr/val_loss:  1.066250/  9.413435, val:  60.83%, val_best:  64.17%, tr:  72.83%, tr_best:  74.87%, epoch time: 66.92 seconds, 1.12 minutes\n",
      "epoch-101 lr=['0.0005000'], tr/val_loss:  1.055606/  9.861645, val:  60.42%, val_best:  64.17%, tr:  72.93%, tr_best:  74.87%, epoch time: 65.81 seconds, 1.10 minutes\n",
      "epoch-102 lr=['0.0005000'], tr/val_loss:  1.041980/ 10.041780, val:  62.08%, val_best:  64.17%, tr:  73.85%, tr_best:  74.87%, epoch time: 66.76 seconds, 1.11 minutes\n",
      "epoch-103 lr=['0.0005000'], tr/val_loss:  1.040814/  6.744092, val:  62.92%, val_best:  64.17%, tr:  73.65%, tr_best:  74.87%, epoch time: 67.02 seconds, 1.12 minutes\n",
      "epoch-104 lr=['0.0005000'], tr/val_loss:  1.032060/  8.097218, val:  55.42%, val_best:  64.17%, tr:  74.26%, tr_best:  74.87%, epoch time: 65.92 seconds, 1.10 minutes\n",
      "epoch-105 lr=['0.0005000'], tr/val_loss:  1.046271/ 11.313410, val:  59.58%, val_best:  64.17%, tr:  74.36%, tr_best:  74.87%, epoch time: 66.50 seconds, 1.11 minutes\n",
      "epoch-106 lr=['0.0005000'], tr/val_loss:  1.022864/  9.662768, val:  58.75%, val_best:  64.17%, tr:  74.46%, tr_best:  74.87%, epoch time: 66.57 seconds, 1.11 minutes\n",
      "epoch-107 lr=['0.0005000'], tr/val_loss:  1.055352/  5.191687, val:  69.58%, val_best:  69.58%, tr:  72.01%, tr_best:  74.87%, epoch time: 65.73 seconds, 1.10 minutes\n",
      "epoch-108 lr=['0.0005000'], tr/val_loss:  1.049538/  9.262897, val:  57.92%, val_best:  69.58%, tr:  71.71%, tr_best:  74.87%, epoch time: 66.26 seconds, 1.10 minutes\n",
      "epoch-109 lr=['0.0005000'], tr/val_loss:  1.019250/  7.453602, val:  60.42%, val_best:  69.58%, tr:  73.34%, tr_best:  74.87%, epoch time: 66.40 seconds, 1.11 minutes\n",
      "epoch-110 lr=['0.0005000'], tr/val_loss:  0.995054/ 10.573130, val:  57.92%, val_best:  69.58%, tr:  74.36%, tr_best:  74.87%, epoch time: 67.20 seconds, 1.12 minutes\n",
      "epoch-111 lr=['0.0005000'], tr/val_loss:  1.006270/  8.810414, val:  61.67%, val_best:  69.58%, tr:  75.18%, tr_best:  75.18%, epoch time: 66.72 seconds, 1.11 minutes\n",
      "epoch-112 lr=['0.0005000'], tr/val_loss:  1.033587/ 10.407065, val:  59.58%, val_best:  69.58%, tr:  74.97%, tr_best:  75.18%, epoch time: 66.63 seconds, 1.11 minutes\n",
      "epoch-113 lr=['0.0005000'], tr/val_loss:  1.070391/ 10.017165, val:  55.42%, val_best:  69.58%, tr:  73.34%, tr_best:  75.18%, epoch time: 65.98 seconds, 1.10 minutes\n",
      "epoch-114 lr=['0.0005000'], tr/val_loss:  1.050913/  7.901789, val:  60.00%, val_best:  69.58%, tr:  74.16%, tr_best:  75.18%, epoch time: 65.71 seconds, 1.10 minutes\n",
      "epoch-115 lr=['0.0005000'], tr/val_loss:  1.036777/  7.891989, val:  62.92%, val_best:  69.58%, tr:  74.77%, tr_best:  75.18%, epoch time: 65.68 seconds, 1.09 minutes\n",
      "epoch-116 lr=['0.0005000'], tr/val_loss:  1.097342/  8.346126, val:  61.25%, val_best:  69.58%, tr:  71.81%, tr_best:  75.18%, epoch time: 67.82 seconds, 1.13 minutes\n",
      "epoch-117 lr=['0.0005000'], tr/val_loss:  1.034121/ 11.576290, val:  56.67%, val_best:  69.58%, tr:  74.67%, tr_best:  75.18%, epoch time: 65.55 seconds, 1.09 minutes\n",
      "epoch-118 lr=['0.0005000'], tr/val_loss:  1.020347/  7.373446, val:  58.33%, val_best:  69.58%, tr:  76.40%, tr_best:  76.40%, epoch time: 66.23 seconds, 1.10 minutes\n",
      "epoch-119 lr=['0.0005000'], tr/val_loss:  1.024903/  8.219254, val:  60.83%, val_best:  69.58%, tr:  72.93%, tr_best:  76.40%, epoch time: 66.45 seconds, 1.11 minutes\n",
      "epoch-120 lr=['0.0005000'], tr/val_loss:  1.026246/ 10.795204, val:  57.08%, val_best:  69.58%, tr:  75.79%, tr_best:  76.40%, epoch time: 66.57 seconds, 1.11 minutes\n",
      "epoch-121 lr=['0.0005000'], tr/val_loss:  1.015901/ 10.133340, val:  58.33%, val_best:  69.58%, tr:  74.97%, tr_best:  76.40%, epoch time: 66.60 seconds, 1.11 minutes\n",
      "epoch-122 lr=['0.0005000'], tr/val_loss:  1.010676/  7.691530, val:  57.92%, val_best:  69.58%, tr:  75.08%, tr_best:  76.40%, epoch time: 66.21 seconds, 1.10 minutes\n",
      "epoch-123 lr=['0.0005000'], tr/val_loss:  1.067072/  8.419945, val:  59.58%, val_best:  69.58%, tr:  73.54%, tr_best:  76.40%, epoch time: 67.55 seconds, 1.13 minutes\n",
      "epoch-124 lr=['0.0005000'], tr/val_loss:  1.019768/  7.241560, val:  65.83%, val_best:  69.58%, tr:  75.49%, tr_best:  76.40%, epoch time: 65.77 seconds, 1.10 minutes\n",
      "epoch-125 lr=['0.0005000'], tr/val_loss:  1.042609/ 10.369636, val:  60.42%, val_best:  69.58%, tr:  73.85%, tr_best:  76.40%, epoch time: 65.75 seconds, 1.10 minutes\n",
      "epoch-126 lr=['0.0005000'], tr/val_loss:  1.025968/  9.613731, val:  59.17%, val_best:  69.58%, tr:  75.28%, tr_best:  76.40%, epoch time: 65.69 seconds, 1.09 minutes\n",
      "epoch-127 lr=['0.0005000'], tr/val_loss:  1.061226/  7.817229, val:  60.83%, val_best:  69.58%, tr:  73.44%, tr_best:  76.40%, epoch time: 65.99 seconds, 1.10 minutes\n",
      "epoch-128 lr=['0.0005000'], tr/val_loss:  1.037713/  6.174396, val:  62.50%, val_best:  69.58%, tr:  73.85%, tr_best:  76.40%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "epoch-129 lr=['0.0005000'], tr/val_loss:  1.008787/  8.837966, val:  60.83%, val_best:  69.58%, tr:  76.40%, tr_best:  76.40%, epoch time: 67.73 seconds, 1.13 minutes\n",
      "epoch-130 lr=['0.0005000'], tr/val_loss:  1.054992/ 12.652507, val:  57.92%, val_best:  69.58%, tr:  74.97%, tr_best:  76.40%, epoch time: 67.39 seconds, 1.12 minutes\n",
      "epoch-131 lr=['0.0005000'], tr/val_loss:  1.052942/ 11.330295, val:  55.42%, val_best:  69.58%, tr:  72.93%, tr_best:  76.40%, epoch time: 67.16 seconds, 1.12 minutes\n",
      "epoch-132 lr=['0.0005000'], tr/val_loss:  1.010523/  9.665224, val:  56.25%, val_best:  69.58%, tr:  75.69%, tr_best:  76.40%, epoch time: 68.15 seconds, 1.14 minutes\n",
      "epoch-133 lr=['0.0005000'], tr/val_loss:  1.028676/  7.168559, val:  63.33%, val_best:  69.58%, tr:  73.54%, tr_best:  76.40%, epoch time: 66.91 seconds, 1.12 minutes\n",
      "epoch-134 lr=['0.0005000'], tr/val_loss:  1.032980/  8.977610, val:  60.42%, val_best:  69.58%, tr:  73.03%, tr_best:  76.40%, epoch time: 67.85 seconds, 1.13 minutes\n",
      "epoch-135 lr=['0.0005000'], tr/val_loss:  0.980623/  8.908985, val:  62.08%, val_best:  69.58%, tr:  73.95%, tr_best:  76.40%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "epoch-136 lr=['0.0005000'], tr/val_loss:  0.988230/  7.530697, val:  58.75%, val_best:  69.58%, tr:  73.75%, tr_best:  76.40%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "epoch-137 lr=['0.0005000'], tr/val_loss:  1.009130/  7.466257, val:  63.75%, val_best:  69.58%, tr:  76.92%, tr_best:  76.92%, epoch time: 66.10 seconds, 1.10 minutes\n",
      "epoch-138 lr=['0.0005000'], tr/val_loss:  0.981828/  8.520227, val:  61.25%, val_best:  69.58%, tr:  76.81%, tr_best:  76.92%, epoch time: 66.59 seconds, 1.11 minutes\n",
      "epoch-139 lr=['0.0005000'], tr/val_loss:  1.017110/  7.648599, val:  65.00%, val_best:  69.58%, tr:  74.26%, tr_best:  76.92%, epoch time: 67.30 seconds, 1.12 minutes\n",
      "epoch-140 lr=['0.0005000'], tr/val_loss:  1.047867/ 14.399397, val:  52.08%, val_best:  69.58%, tr:  74.36%, tr_best:  76.92%, epoch time: 66.26 seconds, 1.10 minutes\n",
      "epoch-141 lr=['0.0005000'], tr/val_loss:  0.996897/  7.592550, val:  60.42%, val_best:  69.58%, tr:  77.83%, tr_best:  77.83%, epoch time: 66.46 seconds, 1.11 minutes\n",
      "epoch-142 lr=['0.0005000'], tr/val_loss:  1.020946/  9.290701, val:  60.42%, val_best:  69.58%, tr:  73.75%, tr_best:  77.83%, epoch time: 66.61 seconds, 1.11 minutes\n",
      "epoch-143 lr=['0.0005000'], tr/val_loss:  0.984711/  7.103067, val:  63.75%, val_best:  69.58%, tr:  75.79%, tr_best:  77.83%, epoch time: 65.94 seconds, 1.10 minutes\n",
      "epoch-144 lr=['0.0005000'], tr/val_loss:  0.973423/ 11.347228, val:  60.00%, val_best:  69.58%, tr:  76.10%, tr_best:  77.83%, epoch time: 66.93 seconds, 1.12 minutes\n",
      "epoch-145 lr=['0.0005000'], tr/val_loss:  0.981624/  9.416159, val:  62.92%, val_best:  69.58%, tr:  76.81%, tr_best:  77.83%, epoch time: 66.80 seconds, 1.11 minutes\n",
      "epoch-146 lr=['0.0005000'], tr/val_loss:  0.971844/  6.812705, val:  67.92%, val_best:  69.58%, tr:  75.59%, tr_best:  77.83%, epoch time: 68.33 seconds, 1.14 minutes\n",
      "epoch-147 lr=['0.0005000'], tr/val_loss:  0.978987/ 12.231829, val:  52.08%, val_best:  69.58%, tr:  77.94%, tr_best:  77.94%, epoch time: 66.59 seconds, 1.11 minutes\n",
      "epoch-148 lr=['0.0005000'], tr/val_loss:  0.989911/  9.617283, val:  61.25%, val_best:  69.58%, tr:  74.97%, tr_best:  77.94%, epoch time: 67.47 seconds, 1.12 minutes\n",
      "epoch-149 lr=['0.0005000'], tr/val_loss:  1.027710/  6.503455, val:  65.00%, val_best:  69.58%, tr:  76.20%, tr_best:  77.94%, epoch time: 66.85 seconds, 1.11 minutes\n",
      "epoch-150 lr=['0.0005000'], tr/val_loss:  0.993527/  6.928044, val:  64.58%, val_best:  69.58%, tr:  77.32%, tr_best:  77.94%, epoch time: 66.40 seconds, 1.11 minutes\n",
      "epoch-151 lr=['0.0005000'], tr/val_loss:  1.038052/ 11.241552, val:  54.58%, val_best:  69.58%, tr:  72.93%, tr_best:  77.94%, epoch time: 67.25 seconds, 1.12 minutes\n",
      "epoch-152 lr=['0.0005000'], tr/val_loss:  1.034083/  8.341680, val:  65.83%, val_best:  69.58%, tr:  76.20%, tr_best:  77.94%, epoch time: 66.96 seconds, 1.12 minutes\n",
      "epoch-153 lr=['0.0005000'], tr/val_loss:  1.034116/  6.420529, val:  65.42%, val_best:  69.58%, tr:  74.87%, tr_best:  77.94%, epoch time: 68.61 seconds, 1.14 minutes\n",
      "epoch-154 lr=['0.0005000'], tr/val_loss:  1.018066/  7.098150, val:  61.25%, val_best:  69.58%, tr:  76.30%, tr_best:  77.94%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "epoch-155 lr=['0.0005000'], tr/val_loss:  1.034372/ 10.401955, val:  57.50%, val_best:  69.58%, tr:  75.79%, tr_best:  77.94%, epoch time: 70.02 seconds, 1.17 minutes\n",
      "epoch-156 lr=['0.0005000'], tr/val_loss:  1.019869/  6.995081, val:  65.83%, val_best:  69.58%, tr:  77.32%, tr_best:  77.94%, epoch time: 67.90 seconds, 1.13 minutes\n",
      "epoch-157 lr=['0.0005000'], tr/val_loss:  1.067905/  6.370001, val:  67.92%, val_best:  69.58%, tr:  76.00%, tr_best:  77.94%, epoch time: 67.40 seconds, 1.12 minutes\n",
      "epoch-158 lr=['0.0005000'], tr/val_loss:  1.041911/  8.933698, val:  61.67%, val_best:  69.58%, tr:  76.30%, tr_best:  77.94%, epoch time: 66.79 seconds, 1.11 minutes\n",
      "epoch-159 lr=['0.0005000'], tr/val_loss:  1.002521/  7.482043, val:  61.67%, val_best:  69.58%, tr:  78.35%, tr_best:  78.35%, epoch time: 65.56 seconds, 1.09 minutes\n",
      "epoch-160 lr=['0.0005000'], tr/val_loss:  1.011962/  7.389409, val:  62.08%, val_best:  69.58%, tr:  77.22%, tr_best:  78.35%, epoch time: 66.65 seconds, 1.11 minutes\n",
      "epoch-161 lr=['0.0005000'], tr/val_loss:  1.027701/  9.393517, val:  65.00%, val_best:  69.58%, tr:  75.49%, tr_best:  78.35%, epoch time: 65.49 seconds, 1.09 minutes\n",
      "epoch-162 lr=['0.0005000'], tr/val_loss:  1.031248/  6.841924, val:  62.92%, val_best:  69.58%, tr:  76.30%, tr_best:  78.35%, epoch time: 68.77 seconds, 1.15 minutes\n",
      "epoch-163 lr=['0.0005000'], tr/val_loss:  1.042017/  8.568785, val:  59.58%, val_best:  69.58%, tr:  77.02%, tr_best:  78.35%, epoch time: 65.68 seconds, 1.09 minutes\n",
      "epoch-164 lr=['0.0005000'], tr/val_loss:  1.054182/  8.269231, val:  61.25%, val_best:  69.58%, tr:  76.81%, tr_best:  78.35%, epoch time: 66.92 seconds, 1.12 minutes\n",
      "epoch-165 lr=['0.0005000'], tr/val_loss:  1.029472/ 10.082157, val:  57.08%, val_best:  69.58%, tr:  76.92%, tr_best:  78.35%, epoch time: 66.15 seconds, 1.10 minutes\n",
      "epoch-166 lr=['0.0005000'], tr/val_loss:  1.050115/  9.312084, val:  60.42%, val_best:  69.58%, tr:  74.97%, tr_best:  78.35%, epoch time: 66.21 seconds, 1.10 minutes\n",
      "epoch-167 lr=['0.0005000'], tr/val_loss:  1.042887/ 11.520679, val:  61.25%, val_best:  69.58%, tr:  75.89%, tr_best:  78.35%, epoch time: 65.04 seconds, 1.08 minutes\n",
      "epoch-168 lr=['0.0005000'], tr/val_loss:  1.016006/  7.874322, val:  59.58%, val_best:  69.58%, tr:  78.04%, tr_best:  78.35%, epoch time: 66.20 seconds, 1.10 minutes\n",
      "epoch-169 lr=['0.0005000'], tr/val_loss:  1.053198/ 11.342418, val:  60.83%, val_best:  69.58%, tr:  75.28%, tr_best:  78.35%, epoch time: 64.80 seconds, 1.08 minutes\n",
      "epoch-170 lr=['0.0005000'], tr/val_loss:  1.034070/ 11.587781, val:  47.08%, val_best:  69.58%, tr:  77.32%, tr_best:  78.35%, epoch time: 65.10 seconds, 1.09 minutes\n",
      "epoch-171 lr=['0.0005000'], tr/val_loss:  1.030462/ 10.238638, val:  57.50%, val_best:  69.58%, tr:  77.83%, tr_best:  78.35%, epoch time: 66.98 seconds, 1.12 minutes\n",
      "epoch-172 lr=['0.0005000'], tr/val_loss:  1.031711/  6.935276, val:  63.33%, val_best:  69.58%, tr:  71.81%, tr_best:  78.35%, epoch time: 66.48 seconds, 1.11 minutes\n",
      "epoch-173 lr=['0.0005000'], tr/val_loss:  1.017296/  8.672634, val:  65.00%, val_best:  69.58%, tr:  76.51%, tr_best:  78.35%, epoch time: 65.62 seconds, 1.09 minutes\n",
      "epoch-174 lr=['0.0005000'], tr/val_loss:  0.959452/  7.053447, val:  63.33%, val_best:  69.58%, tr:  76.81%, tr_best:  78.35%, epoch time: 66.26 seconds, 1.10 minutes\n",
      "epoch-175 lr=['0.0005000'], tr/val_loss:  0.989941/  5.756121, val:  66.25%, val_best:  69.58%, tr:  76.40%, tr_best:  78.35%, epoch time: 65.48 seconds, 1.09 minutes\n",
      "epoch-176 lr=['0.0005000'], tr/val_loss:  0.963802/  6.454265, val:  64.17%, val_best:  69.58%, tr:  78.55%, tr_best:  78.55%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "epoch-177 lr=['0.0005000'], tr/val_loss:  0.953431/ 10.736478, val:  55.83%, val_best:  69.58%, tr:  80.29%, tr_best:  80.29%, epoch time: 66.39 seconds, 1.11 minutes\n",
      "epoch-178 lr=['0.0005000'], tr/val_loss:  0.940315/  7.284163, val:  60.42%, val_best:  69.58%, tr:  78.86%, tr_best:  80.29%, epoch time: 64.40 seconds, 1.07 minutes\n",
      "epoch-179 lr=['0.0005000'], tr/val_loss:  0.948797/  8.715778, val:  59.17%, val_best:  69.58%, tr:  75.79%, tr_best:  80.29%, epoch time: 66.98 seconds, 1.12 minutes\n",
      "epoch-180 lr=['0.0005000'], tr/val_loss:  0.954784/  5.358933, val:  62.08%, val_best:  69.58%, tr:  74.97%, tr_best:  80.29%, epoch time: 66.07 seconds, 1.10 minutes\n",
      "epoch-181 lr=['0.0005000'], tr/val_loss:  0.951304/  6.900479, val:  65.00%, val_best:  69.58%, tr:  76.20%, tr_best:  80.29%, epoch time: 67.56 seconds, 1.13 minutes\n",
      "epoch-182 lr=['0.0005000'], tr/val_loss:  0.996984/  6.716804, val:  63.75%, val_best:  69.58%, tr:  76.61%, tr_best:  80.29%, epoch time: 65.85 seconds, 1.10 minutes\n",
      "epoch-183 lr=['0.0005000'], tr/val_loss:  1.025814/ 10.045570, val:  50.83%, val_best:  69.58%, tr:  77.22%, tr_best:  80.29%, epoch time: 68.60 seconds, 1.14 minutes\n",
      "epoch-184 lr=['0.0005000'], tr/val_loss:  0.974841/  7.541860, val:  62.50%, val_best:  69.58%, tr:  78.75%, tr_best:  80.29%, epoch time: 65.58 seconds, 1.09 minutes\n",
      "epoch-185 lr=['0.0005000'], tr/val_loss:  1.001376/  5.522709, val:  67.08%, val_best:  69.58%, tr:  74.87%, tr_best:  80.29%, epoch time: 67.29 seconds, 1.12 minutes\n",
      "epoch-186 lr=['0.0005000'], tr/val_loss:  1.003734/  8.453575, val:  64.58%, val_best:  69.58%, tr:  76.71%, tr_best:  80.29%, epoch time: 66.36 seconds, 1.11 minutes\n",
      "epoch-187 lr=['0.0005000'], tr/val_loss:  0.972419/  7.109062, val:  63.75%, val_best:  69.58%, tr:  76.20%, tr_best:  80.29%, epoch time: 66.11 seconds, 1.10 minutes\n",
      "epoch-188 lr=['0.0005000'], tr/val_loss:  0.982343/  7.476267, val:  65.83%, val_best:  69.58%, tr:  75.28%, tr_best:  80.29%, epoch time: 66.13 seconds, 1.10 minutes\n",
      "epoch-189 lr=['0.0005000'], tr/val_loss:  0.965342/  7.318150, val:  65.00%, val_best:  69.58%, tr:  76.40%, tr_best:  80.29%, epoch time: 67.16 seconds, 1.12 minutes\n",
      "epoch-190 lr=['0.0005000'], tr/val_loss:  0.947668/  9.039598, val:  63.33%, val_best:  69.58%, tr:  76.92%, tr_best:  80.29%, epoch time: 66.02 seconds, 1.10 minutes\n",
      "epoch-191 lr=['0.0005000'], tr/val_loss:  0.951179/  8.315219, val:  59.58%, val_best:  69.58%, tr:  77.02%, tr_best:  80.29%, epoch time: 66.51 seconds, 1.11 minutes\n",
      "epoch-192 lr=['0.0005000'], tr/val_loss:  0.938703/  9.184944, val:  62.08%, val_best:  69.58%, tr:  78.14%, tr_best:  80.29%, epoch time: 65.67 seconds, 1.09 minutes\n",
      "epoch-193 lr=['0.0005000'], tr/val_loss:  0.947602/ 11.057220, val:  57.08%, val_best:  69.58%, tr:  77.94%, tr_best:  80.29%, epoch time: 67.27 seconds, 1.12 minutes\n",
      "epoch-194 lr=['0.0005000'], tr/val_loss:  0.942622/ 10.909851, val:  62.50%, val_best:  69.58%, tr:  77.53%, tr_best:  80.29%, epoch time: 65.37 seconds, 1.09 minutes\n",
      "epoch-195 lr=['0.0005000'], tr/val_loss:  0.935096/  7.743964, val:  62.92%, val_best:  69.58%, tr:  77.73%, tr_best:  80.29%, epoch time: 68.15 seconds, 1.14 minutes\n",
      "epoch-196 lr=['0.0005000'], tr/val_loss:  0.937881/  9.359501, val:  62.08%, val_best:  69.58%, tr:  78.04%, tr_best:  80.29%, epoch time: 66.07 seconds, 1.10 minutes\n",
      "epoch-197 lr=['0.0005000'], tr/val_loss:  0.919515/  9.429242, val:  61.25%, val_best:  69.58%, tr:  78.24%, tr_best:  80.29%, epoch time: 66.74 seconds, 1.11 minutes\n",
      "epoch-198 lr=['0.0005000'], tr/val_loss:  0.907101/  9.156796, val:  65.00%, val_best:  69.58%, tr:  78.04%, tr_best:  80.29%, epoch time: 66.37 seconds, 1.11 minutes\n",
      "epoch-199 lr=['0.0005000'], tr/val_loss:  0.964881/ 12.474287, val:  60.83%, val_best:  69.58%, tr:  75.18%, tr_best:  80.29%, epoch time: 67.61 seconds, 1.13 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbbaee9814645e98b3380ace89cdf8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÑ‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÑ‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÇ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñá‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.75179</td></tr><tr><td>tr_epoch_loss</td><td>0.96488</td></tr><tr><td>val_acc_best</td><td>0.69583</td></tr><tr><td>val_acc_now</td><td>0.60833</td></tr><tr><td>val_loss</td><td>12.47429</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bright-sweep-291</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3wfe8y34' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3wfe8y34</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251111_052614-3wfe8y34/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tkuubns9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 5000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_threshold: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: one\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251111_090808-tkuubns9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tkuubns9' target=\"_blank\">dazzling-sweep-300</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tkuubns9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tkuubns9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_threshold' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': '20251111_090816_243', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.005, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'one', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'output_threshold': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 7a22c8a0ef5b9b252dbf98632e270efd\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.5, v_reset=10000, sg_width=1, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=False, ANPI_MODE=True)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.5, v_reset=10000, sg_width=1, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=False, ANPI_MODE=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient(single_step=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.005\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "inFeed spike.shape torch.Size([10, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([ 1.8934e-02,  2.1093e-01, -1.6790e-02,  8.2860e-02, -1.8780e-01,\n",
      "         6.3725e-02, -6.3379e-02, -7.9782e-02,  5.1524e-02, -1.2446e-01,\n",
      "        -1.6221e-01, -2.9600e-02, -9.0339e-03,  1.9444e-02, -1.0934e-01,\n",
      "         1.8129e-01, -6.9730e-02,  6.7152e-02,  7.7763e-02, -3.2597e-03,\n",
      "         1.4558e-01, -5.0406e-02, -2.4797e-02,  1.4391e-01, -3.1818e-02,\n",
      "        -1.1320e-01,  2.2984e-01, -6.7576e-02,  1.7931e-02, -1.1550e-01,\n",
      "        -1.7594e-01, -1.5427e-01,  8.1846e-02,  1.3850e-01,  6.3135e-02,\n",
      "         4.1502e-02, -1.5509e-01,  6.0735e-02,  1.6491e-01, -6.4878e-02,\n",
      "         9.1983e-02,  7.6438e-03,  8.2616e-03, -1.3744e-02,  3.2357e-02,\n",
      "        -5.7478e-02, -1.0464e-01,  9.3097e-03, -3.2663e-02, -5.1313e-02,\n",
      "        -8.5647e-02,  3.8434e-02,  1.6001e-02, -1.9292e-02,  9.8606e-02,\n",
      "        -1.3158e-01, -3.4134e-02, -6.2874e-02,  4.3602e-02, -5.2417e-02,\n",
      "         1.2124e-01, -7.9496e-02,  2.4412e-02, -4.1696e-02,  1.0778e-01,\n",
      "        -1.0762e-01,  5.4097e-02, -1.2537e-01, -3.7238e-02,  5.0156e-02,\n",
      "         9.7776e-03,  2.5239e-02,  3.5296e-02,  2.2238e-01,  2.2782e-03,\n",
      "         1.5446e-01, -1.1312e-01,  9.2554e-02, -4.4633e-02,  7.4222e-02,\n",
      "        -5.6474e-02, -6.8803e-02, -7.0595e-02, -4.9484e-02, -4.2925e-02,\n",
      "        -4.0809e-02,  1.6994e-02,  4.3201e-02,  4.9468e-02, -1.1875e-01,\n",
      "        -2.6532e-02,  2.6988e-02, -1.4051e-01, -6.3074e-02,  7.3065e-03,\n",
      "         1.8921e-02,  5.8165e-02,  2.2661e-02,  1.1140e-01, -6.6528e-02,\n",
      "        -1.6133e-01,  5.8902e-04,  1.3482e-01,  1.2398e-01,  2.2678e-03,\n",
      "        -1.2688e-01, -7.3284e-02,  3.6657e-02, -5.3425e-02, -3.8686e-03,\n",
      "        -7.5912e-02, -2.4416e-01,  6.8315e-02, -9.1515e-03, -2.1105e-02,\n",
      "         4.3759e-02, -3.0760e-02,  2.1116e-03,  6.1028e-02,  2.4064e-02,\n",
      "         7.3052e-02, -1.1411e-02, -9.9703e-03, -4.8900e-02, -4.9272e-02,\n",
      "        -1.1781e-01, -2.3789e-02, -6.6208e-02,  1.9253e-02,  9.5465e-02,\n",
      "        -2.7977e-03,  1.6420e-01,  1.0646e-01, -9.6819e-02, -6.5508e-02,\n",
      "         1.6782e-01,  2.4013e-01, -6.0490e-02,  1.2407e-01, -2.7312e-02,\n",
      "         4.2546e-02,  4.1576e-02,  1.0389e-01, -1.9791e-01, -6.1734e-02,\n",
      "         2.0598e-01, -9.2463e-03,  2.3019e-02, -7.1248e-02, -1.6451e-01,\n",
      "         8.8945e-02,  7.6954e-02, -6.1358e-02,  2.1075e-01,  1.1362e-01,\n",
      "        -4.1540e-02,  2.3355e-02, -1.2469e-01, -1.1774e-02, -5.9197e-02,\n",
      "        -7.8824e-02,  2.0957e-04, -1.8973e-02,  7.3129e-02,  7.9223e-02,\n",
      "         8.0468e-02, -5.9513e-02,  1.2675e-01, -1.0473e-01, -2.2743e-03,\n",
      "        -3.5689e-02, -4.7485e-02,  1.4612e-02, -1.4731e-01,  3.0283e-02,\n",
      "        -4.3783e-02, -1.0702e-01, -1.2393e-01, -1.5395e-01, -1.5502e-01,\n",
      "        -6.4030e-02,  7.4169e-02, -5.9266e-02,  2.9501e-02, -1.3114e-01,\n",
      "        -3.1084e-02,  5.7895e-02,  5.5842e-02, -7.3755e-02, -1.2356e-02,\n",
      "        -4.2171e-02, -1.3607e-01,  3.7821e-02, -2.0038e-02,  5.8521e-02,\n",
      "        -9.7889e-02, -3.6203e-04, -7.8263e-02,  3.8445e-02,  2.4739e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 0.0243,  0.0875,  0.0762,  0.0893, -0.0358,  0.0084, -0.0823,  0.0415,\n",
      "         0.1093,  0.0390, -0.0084,  0.0824, -0.0388, -0.0084,  0.1905, -0.0412,\n",
      "         0.0535,  0.0146, -0.1927, -0.0034, -0.1234, -0.0248,  0.0023,  0.1960,\n",
      "         0.0956,  0.1355, -0.0962, -0.0471, -0.0019, -0.1697,  0.0164,  0.0255,\n",
      "        -0.0218, -0.0832,  0.0267,  0.0968,  0.1448,  0.0340,  0.0123, -0.0644,\n",
      "        -0.1539, -0.0648,  0.1434,  0.0292, -0.0543,  0.0045, -0.1050, -0.0141,\n",
      "        -0.0106, -0.0727, -0.0340,  0.0092,  0.0063, -0.0900,  0.0770, -0.1758,\n",
      "        -0.1888,  0.1074,  0.0024,  0.0566,  0.0525,  0.0500, -0.1397,  0.2322,\n",
      "        -0.1507, -0.0170, -0.0022,  0.0212,  0.0636,  0.1522,  0.0262,  0.0489,\n",
      "        -0.0238, -0.0843,  0.0417, -0.0206, -0.1503, -0.0105, -0.0025, -0.0157,\n",
      "         0.0537,  0.2482, -0.0157, -0.0847,  0.1378,  0.0673,  0.0827,  0.0278,\n",
      "         0.0017,  0.0711,  0.1171, -0.0113, -0.1379, -0.0627,  0.0863,  0.0732,\n",
      "         0.0663, -0.0518, -0.1748, -0.0279,  0.0079, -0.1269,  0.1308, -0.1293,\n",
      "         0.0255,  0.0589, -0.1456, -0.1754,  0.1100,  0.1369, -0.0427, -0.0364,\n",
      "        -0.1153, -0.2599, -0.0316,  0.0100,  0.1226, -0.1337,  0.0349,  0.1675,\n",
      "        -0.2253,  0.1285, -0.1121, -0.1268, -0.0064,  0.1009, -0.2251, -0.0125,\n",
      "         0.0833, -0.0059, -0.0931, -0.1747, -0.0026,  0.0089, -0.1154, -0.1073,\n",
      "         0.2383,  0.0302, -0.1482, -0.0759,  0.0708,  0.0868,  0.1001,  0.0796,\n",
      "         0.2262, -0.0222,  0.2962,  0.0536,  0.0022, -0.0399, -0.0256,  0.0852,\n",
      "        -0.1183, -0.1185, -0.0293, -0.0370,  0.0309, -0.0984,  0.1941,  0.0216,\n",
      "        -0.0090,  0.1404,  0.0598,  0.0650,  0.0744,  0.0426, -0.0094, -0.0445,\n",
      "        -0.1149,  0.0229,  0.1572,  0.1014,  0.1170, -0.1135,  0.0229,  0.1222,\n",
      "        -0.0146, -0.0612,  0.1128, -0.2361,  0.0667, -0.0671, -0.0090, -0.0696,\n",
      "        -0.1264,  0.1187, -0.0047, -0.0778,  0.0414, -0.0189, -0.0153, -0.0139,\n",
      "        -0.0278, -0.1414, -0.0518, -0.0080, -0.1608, -0.0265,  0.1849, -0.0543],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([-4.4951e-02, -3.0269e-02,  5.7666e-03,  5.0608e-02,  7.1861e-03,\n",
      "         1.3723e-01, -4.7515e-02,  2.8713e-02,  1.1686e-01, -2.1819e-02,\n",
      "         8.4215e-02,  3.5925e-02,  1.6644e-01, -8.9751e-02,  5.2478e-02,\n",
      "         1.1553e-01,  1.6151e-01, -2.2964e-02, -1.6174e-01,  1.9235e-01,\n",
      "         9.7453e-02,  7.7079e-02, -8.9056e-02,  8.0481e-02,  1.4806e-01,\n",
      "        -1.6083e-02,  1.0203e-01, -3.7285e-02,  6.1060e-02,  9.6561e-03,\n",
      "        -6.8472e-02,  1.0096e-02, -1.4065e-01,  2.3837e-01,  1.1363e-01,\n",
      "         2.4803e-02, -3.5905e-02,  8.2499e-02,  4.0047e-02,  4.7051e-02,\n",
      "        -1.8222e-01,  9.4498e-02, -9.1961e-02,  1.1252e-01,  7.8541e-02,\n",
      "         1.0352e-01,  5.1129e-02, -1.3695e-02,  1.3555e-01,  3.0085e-02,\n",
      "         4.2894e-02, -3.0123e-03, -1.0391e-01,  6.3762e-03,  2.7413e-02,\n",
      "         2.0599e-01, -1.3469e-01, -4.1689e-02,  8.9827e-02, -1.3860e-01,\n",
      "         5.3680e-02, -9.2493e-02,  2.8438e-02, -9.8101e-02, -2.9716e-02,\n",
      "         1.5025e-02, -8.1340e-03,  8.0789e-03,  1.5008e-01,  1.7955e-02,\n",
      "        -9.7189e-02,  4.2880e-02,  3.5098e-02, -7.5292e-02,  1.0018e-02,\n",
      "        -3.8158e-02,  2.1247e-02,  8.8000e-02,  5.3422e-02,  8.5364e-02,\n",
      "        -3.1726e-02, -4.0565e-02,  5.0597e-02, -5.4060e-02, -1.9612e-03,\n",
      "         2.9600e-01,  9.2132e-02,  2.9507e-02, -9.2795e-02, -1.0727e-01,\n",
      "        -9.4369e-04,  1.6943e-01, -1.1252e-01,  2.0963e-03, -4.7562e-02,\n",
      "        -8.9568e-02, -1.6470e-01, -1.3752e-02, -4.9301e-03,  1.9867e-02,\n",
      "         2.8623e-02, -1.4914e-01, -7.4637e-02,  4.3263e-02, -5.4997e-02,\n",
      "        -5.1978e-02, -9.9176e-02, -1.9955e-02,  5.1099e-02,  1.8961e-02,\n",
      "         3.6069e-02, -8.3924e-02,  1.0509e-01, -1.2345e-01, -6.2687e-02,\n",
      "        -6.5892e-02,  7.2257e-02,  7.6160e-02, -1.0679e-02,  1.1915e-01,\n",
      "        -1.1745e-01, -4.7364e-02,  8.2369e-03, -2.0607e-02, -8.7966e-03,\n",
      "        -1.3239e-01, -1.0953e-02, -3.8245e-02,  1.7113e-01, -9.6756e-02,\n",
      "        -2.3135e-01,  1.7700e-01, -9.4699e-02,  8.4271e-02,  1.7756e-01,\n",
      "        -7.7262e-03,  2.4065e-01,  1.2335e-01,  5.0242e-02,  1.1121e-02,\n",
      "        -1.3971e-01, -2.5510e-02,  1.1339e-02,  5.6864e-02, -2.9294e-02,\n",
      "        -1.0927e-01,  9.0566e-02, -1.4698e-01,  1.0142e-01, -2.0078e-01,\n",
      "        -2.5668e-02, -8.1559e-02, -2.6427e-02,  2.6780e-01,  4.4975e-02,\n",
      "         1.1964e-01,  6.6056e-03,  8.9370e-02,  7.3522e-02, -5.8117e-02,\n",
      "        -6.1687e-02, -3.5208e-02,  1.4783e-01, -1.6733e-02,  1.8550e-01,\n",
      "        -5.9637e-02,  1.0120e-01,  3.3498e-02, -1.4412e-02,  1.4279e-01,\n",
      "        -1.7611e-01,  2.3674e-02, -2.6663e-02,  2.8803e-02, -1.0240e-01,\n",
      "        -8.6560e-02, -1.3708e-02,  2.1908e-01,  1.7359e-01,  1.6947e-02,\n",
      "         1.3747e-01, -1.0779e-02, -5.6662e-02,  2.1867e-02,  9.4120e-02,\n",
      "        -1.4148e-04,  1.3205e-01, -7.6242e-03, -6.1247e-02, -1.5935e-01,\n",
      "         1.1932e-01, -1.7626e-01,  4.7519e-02, -6.7935e-02, -3.5344e-02,\n",
      "        -5.6960e-02, -1.6597e-01,  3.6102e-02,  7.2538e-02, -1.1702e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0897,  0.1346,  0.0082,  0.0404, -0.0141, -0.0915,  0.1112,  0.0613,\n",
      "         0.0437,  0.2269, -0.0635,  0.0330, -0.0388,  0.0970,  0.1340, -0.0103,\n",
      "        -0.0180, -0.2479, -0.1925, -0.0520, -0.0167,  0.2512,  0.2248,  0.0406,\n",
      "        -0.0638, -0.1051,  0.0551,  0.1565, -0.0160, -0.0229,  0.0975,  0.0753,\n",
      "        -0.0261,  0.0484,  0.0233,  0.1358, -0.0195,  0.1246, -0.1235, -0.0948,\n",
      "         0.1241,  0.0367, -0.0132,  0.0701, -0.0866,  0.0677,  0.0747, -0.0005,\n",
      "         0.0291, -0.0934,  0.0961,  0.0369,  0.1113, -0.0287,  0.0090,  0.2001,\n",
      "        -0.0262,  0.0943, -0.1185, -0.1080, -0.1874, -0.2298, -0.0099,  0.1366,\n",
      "        -0.0031, -0.2031,  0.0642, -0.0495, -0.0109,  0.0107, -0.0874, -0.0920,\n",
      "        -0.0740, -0.0029, -0.0110, -0.0867,  0.0235, -0.0947,  0.0334,  0.0025,\n",
      "        -0.1047, -0.1597,  0.0005, -0.1529,  0.0354,  0.1324, -0.1721, -0.0826,\n",
      "         0.1246,  0.1224,  0.1146,  0.0836,  0.0127,  0.1906,  0.0699, -0.0207,\n",
      "         0.0435, -0.0039, -0.1358, -0.0330, -0.1266,  0.1363,  0.0505, -0.1182,\n",
      "        -0.0723, -0.1437,  0.1342,  0.0754, -0.0922,  0.1479, -0.1418, -0.0035,\n",
      "         0.0841,  0.0382,  0.1203, -0.1159,  0.0937,  0.0410, -0.0786,  0.0332,\n",
      "         0.0585,  0.0918,  0.0264,  0.0213, -0.0575,  0.1411, -0.0949,  0.0688,\n",
      "         0.0401,  0.0027, -0.0604,  0.0814, -0.0098, -0.1172,  0.0241, -0.0613,\n",
      "        -0.0740, -0.0775,  0.0796, -0.1757,  0.0731,  0.0651,  0.0684, -0.1016,\n",
      "         0.2258,  0.0089,  0.0729, -0.1231,  0.0348,  0.0380, -0.1748, -0.0902,\n",
      "         0.2137,  0.0032, -0.1064, -0.0598,  0.1530,  0.0615,  0.0364,  0.0570,\n",
      "        -0.0708, -0.0792, -0.1045,  0.0952, -0.0262, -0.0666,  0.0778, -0.0828,\n",
      "         0.0837,  0.0487,  0.0133,  0.0663,  0.0131,  0.1229,  0.1182,  0.0589,\n",
      "        -0.0275, -0.0269, -0.0815, -0.1605,  0.0128, -0.0175,  0.0951, -0.1965,\n",
      "        -0.1307, -0.0327,  0.2386, -0.0120, -0.0878,  0.2075,  0.1736,  0.1385,\n",
      "        -0.0182, -0.1107, -0.2712,  0.1544,  0.2334, -0.0716, -0.0042, -0.0741],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-0.1114, -0.1260, -0.1119, -0.1797, -0.0123, -0.1354, -0.0627,  0.0113,\n",
      "        -0.2121, -0.0449, -0.1316,  0.0974, -0.0196, -0.2268, -0.0866,  0.0271,\n",
      "        -0.0689,  0.0083, -0.1552,  0.1024,  0.0478, -0.0129, -0.0338,  0.0313,\n",
      "        -0.0618,  0.0572, -0.0461,  0.0063,  0.1558, -0.0055, -0.2186,  0.1069,\n",
      "        -0.0180,  0.0722, -0.0741, -0.0951,  0.1404, -0.0053,  0.0869,  0.1728,\n",
      "         0.1237, -0.0906,  0.1067, -0.0791, -0.1660,  0.0487,  0.0635,  0.0555,\n",
      "         0.0562, -0.0208, -0.0247, -0.0210, -0.1192,  0.0735, -0.0960,  0.0183,\n",
      "         0.0923,  0.0311,  0.0228,  0.0739,  0.0312,  0.0293,  0.0242,  0.1432,\n",
      "        -0.1495,  0.0705,  0.1607,  0.1131,  0.0455, -0.1201,  0.1743,  0.0608,\n",
      "        -0.1190, -0.1751,  0.0174,  0.1122,  0.0615,  0.1286,  0.2249, -0.0399,\n",
      "         0.1110, -0.3303, -0.1719, -0.1252,  0.1496, -0.0990, -0.0732,  0.1068,\n",
      "        -0.0648, -0.0481, -0.1068, -0.0256,  0.1217, -0.1511,  0.0120, -0.0370,\n",
      "        -0.0630,  0.0352, -0.0881, -0.0354, -0.2016,  0.0181, -0.0575, -0.0375,\n",
      "         0.1403,  0.0006,  0.2255, -0.0530, -0.0560, -0.2372,  0.1225,  0.2346,\n",
      "         0.1047, -0.0298,  0.0164, -0.0071, -0.0364,  0.1334,  0.1921,  0.0409,\n",
      "         0.0370, -0.2028,  0.0428,  0.0574,  0.1313,  0.1827,  0.1051,  0.0931,\n",
      "         0.1609, -0.1042, -0.0180,  0.1062, -0.2921,  0.1134, -0.0590, -0.1621,\n",
      "         0.0792,  0.0747,  0.0440,  0.1033, -0.0561, -0.0242, -0.0610, -0.0865,\n",
      "        -0.0366,  0.0475, -0.0406, -0.0448,  0.0063,  0.1424,  0.0130,  0.0362,\n",
      "        -0.0880, -0.0607, -0.0947,  0.1126, -0.1080, -0.1318, -0.1815,  0.0529,\n",
      "        -0.0257, -0.0751, -0.2009, -0.0415, -0.0961, -0.0399,  0.0027, -0.0857,\n",
      "        -0.0904, -0.0699, -0.0287, -0.0942,  0.0285, -0.0968,  0.2108,  0.0588,\n",
      "        -0.0367, -0.0581,  0.0109,  0.0122,  0.0639, -0.0397, -0.1301,  0.1894,\n",
      "        -0.1012,  0.1052,  0.0252,  0.1616,  0.1519,  0.0793, -0.1125, -0.0098,\n",
      "        -0.2642,  0.0059, -0.1486,  0.1224, -0.1979, -0.0971, -0.0462,  0.0212],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([-2.3097e-02, -1.4788e-01,  1.0210e-01,  3.3557e-02,  2.4169e-01,\n",
      "        -1.3634e-01, -8.5383e-03, -3.8043e-02,  1.7316e-01, -4.1206e-02,\n",
      "         2.8185e-02,  1.1411e-01,  3.9203e-02,  8.1501e-02,  9.3352e-02,\n",
      "         2.0928e-01, -6.4334e-02,  3.8064e-02,  1.0619e-01,  9.6020e-02,\n",
      "         1.0335e-01,  2.7876e-01,  6.5856e-02,  2.0067e-01,  8.0354e-02,\n",
      "        -2.3247e-01,  1.2218e-01,  2.3015e-02,  1.7446e-01, -5.6854e-02,\n",
      "        -5.4324e-02,  3.6970e-03,  5.6947e-02, -3.3943e-02,  1.6095e-01,\n",
      "         1.9372e-03, -7.9038e-02,  9.8647e-02, -3.2229e-02,  5.0507e-02,\n",
      "         9.7127e-02, -1.5709e-01,  2.4332e-02,  6.0119e-02, -1.0542e-01,\n",
      "        -9.9471e-02, -9.7351e-02,  1.0731e-01,  1.3880e-01, -1.6828e-01,\n",
      "         8.0962e-02, -5.6699e-02, -7.4759e-03, -6.4011e-03, -1.6884e-01,\n",
      "        -1.3898e-01,  1.7222e-01, -8.1651e-02, -1.8829e-01, -5.1125e-02,\n",
      "         1.3743e-01,  1.8289e-01, -4.9558e-02,  3.3120e-02, -3.1865e-02,\n",
      "         8.0625e-02, -4.9182e-02, -2.8268e-02,  2.6301e-02, -9.2309e-03,\n",
      "        -4.7385e-02, -1.8954e-01,  6.7699e-02,  7.9177e-03, -1.2038e-01,\n",
      "         1.9006e-02,  1.7940e-02,  1.2187e-01, -5.8642e-02,  9.8349e-02,\n",
      "        -1.5748e-01,  2.1717e-02, -5.9567e-02, -1.2667e-02,  1.7577e-02,\n",
      "        -2.7029e-02, -1.3078e-01,  9.7926e-02,  2.2109e-02,  1.4673e-01,\n",
      "        -2.8729e-02,  1.4028e-01, -1.5075e-01,  8.0415e-02,  1.0154e-01,\n",
      "        -2.4165e-02, -2.6159e-02, -7.1517e-02, -4.9824e-02, -7.3641e-02,\n",
      "        -6.3665e-02,  1.4753e-01,  2.2992e-02,  2.5068e-02,  8.3415e-02,\n",
      "        -9.7786e-02,  1.5092e-01, -3.5185e-02, -1.6092e-02, -1.3254e-01,\n",
      "        -1.3460e-01,  2.2992e-02,  1.0348e-01,  1.3005e-01,  1.0918e-01,\n",
      "        -5.3370e-03,  3.2445e-02, -1.0142e-02, -9.9965e-03,  4.4736e-02,\n",
      "        -8.6016e-02,  7.8391e-02, -4.4661e-02,  1.1245e-01, -4.3102e-02,\n",
      "         1.1311e-01,  3.2954e-02, -2.3165e-02, -1.0094e-02, -5.7948e-02,\n",
      "         3.6766e-02, -2.9717e-02, -1.2952e-02, -1.1631e-01, -1.0180e-01,\n",
      "        -5.3192e-02,  3.6347e-02, -1.7114e-02,  5.3002e-02,  8.4525e-02,\n",
      "        -1.0713e-01, -9.2328e-02, -7.2179e-02,  3.7902e-02, -1.4022e-01,\n",
      "         5.5914e-02,  7.9675e-02,  3.9484e-02, -6.8603e-03, -5.1982e-02,\n",
      "        -2.1808e-01, -1.2660e-01,  2.4476e-01, -1.2630e-01,  3.3029e-02,\n",
      "         1.7699e-02,  9.0871e-02,  1.5078e-01, -7.2952e-02,  9.8937e-02,\n",
      "        -7.6370e-02,  8.8079e-02, -8.8526e-02,  1.4171e-02, -3.5435e-02,\n",
      "         4.1153e-03, -1.1882e-01, -5.6413e-02,  1.3846e-02, -4.2599e-02,\n",
      "         8.8813e-02,  2.6194e-03, -5.3535e-02, -1.1002e-01,  1.8341e-01,\n",
      "         9.4184e-02, -1.3347e-01,  5.6069e-02,  1.6303e-01,  1.1132e-04,\n",
      "        -6.4000e-02,  6.0648e-02,  1.7026e-01,  2.7840e-02, -1.4208e-01,\n",
      "        -1.4573e-01, -1.1765e-01,  4.9697e-02,  6.4271e-02, -1.9731e-01,\n",
      "         4.8141e-02,  1.8684e-02, -6.3554e-02,  3.6129e-02,  1.9314e-01,\n",
      "         1.0903e-01, -9.7833e-03,  6.3571e-02,  2.5554e-02, -1.0850e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([ 0.0344, -0.0583, -0.0703, -0.2514, -0.0263, -0.1517, -0.0595,  0.0852,\n",
      "        -0.0268,  0.1277,  0.0393,  0.0879, -0.1254, -0.3348, -0.0745, -0.0879,\n",
      "         0.0474,  0.2322, -0.0987,  0.0354, -0.1064,  0.2409,  0.0734,  0.1306,\n",
      "        -0.0146,  0.1136, -0.1571,  0.0971, -0.0936, -0.0309, -0.0165, -0.0660,\n",
      "        -0.0744, -0.0196, -0.0644,  0.0964, -0.0135,  0.1379,  0.0341, -0.0277,\n",
      "        -0.1563,  0.0474,  0.1149, -0.0199,  0.0589, -0.0346, -0.1100, -0.0020,\n",
      "        -0.0207, -0.0792,  0.0376,  0.0803,  0.0011, -0.0536, -0.0457, -0.1338,\n",
      "        -0.0419, -0.1026,  0.0135, -0.0070,  0.0092,  0.0340,  0.0115,  0.0074,\n",
      "        -0.0919,  0.0340, -0.0404, -0.0352, -0.1594, -0.0584, -0.0647, -0.1073,\n",
      "         0.0282, -0.0426, -0.0435,  0.0278, -0.0031,  0.0945,  0.1075,  0.0418,\n",
      "        -0.1066,  0.0568, -0.0267,  0.0154, -0.1619, -0.0058,  0.1889, -0.0501,\n",
      "        -0.0341, -0.0658,  0.1119, -0.1324,  0.0051, -0.0356, -0.0619,  0.0052,\n",
      "        -0.1332,  0.0911,  0.1441,  0.0152, -0.1952, -0.0013, -0.0183, -0.0338,\n",
      "         0.0631,  0.0013, -0.0094, -0.0361,  0.0370,  0.0204,  0.2303,  0.0010,\n",
      "        -0.0754, -0.0257,  0.1069,  0.0503, -0.0969,  0.0752,  0.0786, -0.0510,\n",
      "         0.0232, -0.1267,  0.0891,  0.1099,  0.0993, -0.0559, -0.0145, -0.0100,\n",
      "         0.1262,  0.0168,  0.0875, -0.0091,  0.2017,  0.0385,  0.1079, -0.0034,\n",
      "        -0.0704,  0.0213,  0.0039, -0.0300, -0.1966, -0.0747, -0.0787,  0.1549,\n",
      "        -0.0108,  0.0581, -0.0527, -0.0684, -0.1514,  0.1286, -0.0561,  0.0960,\n",
      "        -0.0797,  0.0104,  0.0925,  0.0467, -0.0793, -0.0287, -0.1249,  0.0754,\n",
      "         0.0514, -0.1473,  0.1062, -0.0139, -0.0259,  0.0431, -0.0780,  0.0970,\n",
      "         0.1067,  0.0072,  0.0515,  0.0534, -0.0132,  0.0470, -0.1017,  0.1103,\n",
      "         0.0218, -0.1171,  0.0045,  0.2126,  0.0261, -0.0675,  0.0822,  0.0762,\n",
      "        -0.0359, -0.0296,  0.0451, -0.1296, -0.0783, -0.0471, -0.0497, -0.0753,\n",
      "        -0.0754,  0.0663, -0.0081,  0.1472,  0.1225, -0.0161, -0.0844, -0.0095],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 0.0324,  0.0432,  0.0128, -0.1048, -0.0309,  0.0343,  0.0725,  0.0905,\n",
      "         0.0277, -0.0101, -0.2833,  0.0445,  0.0266, -0.0947, -0.0705,  0.0008,\n",
      "         0.0852,  0.0237,  0.1369, -0.0885, -0.1265, -0.0279, -0.0524,  0.1058,\n",
      "         0.0903,  0.1174,  0.0208,  0.1273, -0.0776,  0.0633,  0.1014,  0.0515,\n",
      "         0.0878, -0.0716, -0.0953,  0.0094, -0.0114, -0.1050,  0.1013, -0.0545,\n",
      "         0.2847,  0.1515,  0.0272, -0.0088, -0.0249,  0.1165, -0.0152, -0.0898,\n",
      "         0.0555, -0.0581,  0.0530, -0.1249, -0.0017, -0.0968, -0.0731, -0.0366,\n",
      "         0.0387, -0.0459, -0.0424,  0.0322, -0.0163, -0.0722,  0.0519, -0.2235,\n",
      "        -0.0526, -0.1272, -0.1831,  0.0043, -0.0183,  0.0030, -0.0554,  0.1429,\n",
      "        -0.0177, -0.0099, -0.0047, -0.0410,  0.1171,  0.0849, -0.0063,  0.0121,\n",
      "         0.1281, -0.0245, -0.1118,  0.0065, -0.0303,  0.0243, -0.0665,  0.0273,\n",
      "        -0.1180, -0.0921, -0.0741,  0.1799, -0.1599, -0.1509,  0.0317, -0.1675,\n",
      "         0.0991, -0.1584,  0.0643, -0.0938, -0.0833,  0.0289, -0.1786, -0.0270,\n",
      "        -0.0047, -0.0191,  0.0682, -0.1072,  0.0457,  0.0037,  0.0101, -0.0792,\n",
      "        -0.0411, -0.0145,  0.2259, -0.0552, -0.0088,  0.2277,  0.1019, -0.0581,\n",
      "        -0.0499,  0.0003, -0.0960,  0.1163,  0.0542, -0.0899,  0.0785, -0.0757,\n",
      "         0.1136,  0.0955,  0.0024, -0.1075,  0.0386, -0.1068, -0.1145, -0.0436,\n",
      "        -0.1211, -0.0174,  0.0412, -0.1130,  0.0218, -0.0999, -0.1438,  0.0179,\n",
      "        -0.0548, -0.0479, -0.0121, -0.0538, -0.1268, -0.0741, -0.0415,  0.0144,\n",
      "         0.0508,  0.0760, -0.0638, -0.1439,  0.0555, -0.0486,  0.0251,  0.0053,\n",
      "         0.0245, -0.0231,  0.1099,  0.1551,  0.0105,  0.1027, -0.2184, -0.0827,\n",
      "        -0.1366,  0.0940, -0.1021,  0.1093,  0.1131,  0.0516, -0.1181,  0.1755,\n",
      "         0.0641,  0.0734,  0.1746,  0.1287, -0.0379,  0.0184,  0.0576, -0.0517,\n",
      "         0.0330, -0.0928,  0.0623, -0.0064,  0.0763,  0.0913,  0.0678,  0.1516,\n",
      "         0.0029, -0.0753, -0.0246,  0.0518, -0.0983,  0.1127,  0.0964, -0.1392],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 0.0620, -0.1085, -0.0364, -0.0362, -0.0345,  0.0245, -0.0383, -0.1592,\n",
      "        -0.0493,  0.0025, -0.0031, -0.0267, -0.0372, -0.0254, -0.0815, -0.1367,\n",
      "        -0.1122, -0.0198, -0.1505, -0.0171,  0.2510,  0.0592, -0.0906,  0.0650,\n",
      "        -0.1193,  0.1853,  0.1500, -0.0254, -0.0351, -0.1932,  0.1420,  0.0618,\n",
      "        -0.0726, -0.0382,  0.0244, -0.1537, -0.0670, -0.0176,  0.0063,  0.0121,\n",
      "         0.1055, -0.1070, -0.0291, -0.1309, -0.0195,  0.0570, -0.0449,  0.1710,\n",
      "        -0.0209,  0.1127, -0.0518, -0.0039,  0.0513, -0.0427,  0.0767, -0.0871,\n",
      "        -0.0222,  0.0821, -0.0037, -0.0591, -0.2268,  0.0216,  0.0999, -0.0761,\n",
      "        -0.0202,  0.0163,  0.0175,  0.0805,  0.2464,  0.0258,  0.0094, -0.2156,\n",
      "        -0.1800, -0.0450, -0.0323,  0.0328,  0.1234,  0.0703,  0.1513, -0.0536,\n",
      "        -0.0270,  0.1320,  0.1908,  0.0332, -0.0246, -0.0074,  0.0894,  0.1751,\n",
      "        -0.0369, -0.0786, -0.1112, -0.0129,  0.1746,  0.0117,  0.1111, -0.0848,\n",
      "         0.0966, -0.2175,  0.0244, -0.0435,  0.0372, -0.0111, -0.1484, -0.1168,\n",
      "         0.0543,  0.0504, -0.0847, -0.1751, -0.0511,  0.2542,  0.0312,  0.1713,\n",
      "        -0.0500,  0.0007, -0.0746,  0.0990,  0.0940, -0.0162, -0.0896,  0.0984,\n",
      "        -0.1223, -0.1810, -0.0262,  0.0043,  0.0123, -0.1478, -0.0811, -0.0217,\n",
      "        -0.2336, -0.1054, -0.1123,  0.0213,  0.1850, -0.0052, -0.0100, -0.0414,\n",
      "        -0.0894,  0.0167,  0.1224, -0.0976, -0.0830,  0.1470, -0.1326, -0.0536,\n",
      "        -0.2003,  0.0206,  0.0106, -0.1356,  0.2107, -0.0481, -0.0495, -0.1029,\n",
      "         0.2354, -0.0199, -0.0308, -0.1495, -0.0890,  0.0694, -0.1402,  0.0187,\n",
      "        -0.0880, -0.0337, -0.0247, -0.0611, -0.0248,  0.0132, -0.0624, -0.0821,\n",
      "        -0.1276,  0.0288,  0.0071,  0.0083, -0.0146, -0.1069, -0.1822,  0.0275,\n",
      "         0.1872,  0.0655,  0.1092, -0.0039,  0.0767,  0.0464, -0.0118, -0.0907,\n",
      "         0.2852,  0.0258,  0.0703, -0.0427, -0.0363,  0.1239, -0.0180, -0.0091,\n",
      "         0.0565, -0.1153, -0.0575, -0.0451,  0.1766, -0.1391,  0.0200,  0.1119],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([ 0.0004,  0.0098, -0.0630,  0.0729,  0.1286,  0.1135,  0.0132,  0.0753,\n",
      "         0.1000,  0.0748,  0.0039, -0.1073, -0.0160,  0.0784,  0.0432,  0.0864,\n",
      "         0.0586, -0.0512,  0.2272,  0.1022,  0.0344,  0.0295,  0.0027,  0.0091,\n",
      "         0.0955,  0.0551, -0.1328, -0.0203,  0.2392, -0.1312, -0.0542,  0.1138,\n",
      "         0.0317, -0.0134,  0.1082, -0.0630, -0.2079,  0.0328, -0.0148, -0.0055,\n",
      "         0.0792,  0.0654,  0.0422,  0.0049, -0.1233, -0.0741,  0.1021, -0.0386,\n",
      "        -0.0348,  0.0083, -0.0181,  0.0171,  0.0679,  0.0125, -0.0887, -0.0110,\n",
      "         0.0730, -0.0927,  0.0534,  0.0082, -0.0331,  0.1718,  0.1019, -0.0851,\n",
      "         0.0134,  0.1386, -0.0494,  0.0115,  0.0689,  0.0778,  0.0095,  0.1618,\n",
      "         0.0119,  0.0620, -0.0528, -0.0109, -0.1644,  0.1267, -0.1405,  0.2506,\n",
      "        -0.1118, -0.0659, -0.0724, -0.0041,  0.2570, -0.0359,  0.0570,  0.1049,\n",
      "         0.0314,  0.0490, -0.1250,  0.0921, -0.1334, -0.0470,  0.0743,  0.2864,\n",
      "         0.2034, -0.0486, -0.0558,  0.1079,  0.0035,  0.2203,  0.2007, -0.0190,\n",
      "        -0.0402,  0.2442, -0.1645,  0.0048, -0.0110, -0.0840, -0.2092, -0.0796,\n",
      "        -0.0620, -0.1374, -0.2458, -0.1597, -0.0614,  0.0840, -0.0038,  0.0786,\n",
      "         0.0911, -0.0136, -0.1404, -0.0157,  0.1248, -0.0229, -0.0362, -0.0273,\n",
      "        -0.1132,  0.0388, -0.0171, -0.1177, -0.1702, -0.1764, -0.0355,  0.0977,\n",
      "        -0.0696,  0.1107,  0.1663, -0.0294,  0.1273,  0.0251,  0.0632,  0.0685,\n",
      "        -0.1134, -0.2482, -0.0751, -0.0773, -0.0422,  0.1633,  0.0615,  0.0023,\n",
      "        -0.2218, -0.1290, -0.0822, -0.0304,  0.0999,  0.1201, -0.0902,  0.0529,\n",
      "        -0.0231,  0.1423, -0.1279,  0.0132, -0.1959, -0.1194,  0.2194,  0.0659,\n",
      "         0.0531, -0.0018,  0.1107, -0.0963,  0.1062,  0.0171,  0.0575, -0.1313,\n",
      "        -0.1070, -0.1193,  0.0607, -0.0449,  0.0803, -0.0565,  0.0955,  0.0853,\n",
      "         0.1472, -0.0848, -0.0461,  0.0320, -0.0080,  0.1865, -0.0155,  0.0745,\n",
      "         0.1170, -0.0421, -0.1014, -0.0092,  0.0323,  0.0626, -0.0019,  0.0560],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([10, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([ 0.0136, -0.0106, -0.0700,  0.0738,  0.0363, -0.0981,  0.0008,  0.0320,\n",
      "         0.0276, -0.0871, -0.0159, -0.0787, -0.0114, -0.1574, -0.0150, -0.0063,\n",
      "        -0.0520,  0.0052, -0.0031, -0.0725, -0.1130, -0.0024, -0.0740, -0.0406,\n",
      "         0.0623, -0.0248, -0.1201, -0.1544,  0.0628,  0.0948, -0.1293, -0.0977,\n",
      "        -0.0445,  0.0393, -0.0752, -0.1147, -0.0152, -0.0637,  0.0246,  0.0448,\n",
      "         0.1848,  0.0159, -0.0257, -0.1081,  0.0749, -0.1096,  0.0297,  0.0380,\n",
      "        -0.0246, -0.0982,  0.0286, -0.1074,  0.1856, -0.1482,  0.1644, -0.1338,\n",
      "         0.0282,  0.0384,  0.0637, -0.0539,  0.0754,  0.0908,  0.0045,  0.0340,\n",
      "         0.1005,  0.0013,  0.0536,  0.0342, -0.0304, -0.0040, -0.0625,  0.1837,\n",
      "        -0.0095, -0.0813, -0.0995, -0.0855, -0.0032, -0.0013, -0.0328,  0.0576,\n",
      "         0.0864,  0.0268,  0.0238,  0.0534,  0.0807, -0.0262, -0.0349,  0.0197,\n",
      "        -0.1325,  0.0013,  0.0123, -0.1658, -0.0778, -0.0948, -0.1309,  0.1143,\n",
      "         0.0970,  0.1279,  0.1991, -0.1371, -0.0006,  0.0825, -0.1059,  0.0706,\n",
      "        -0.0534,  0.0225,  0.1133,  0.1787, -0.1257, -0.1148,  0.1220,  0.0250,\n",
      "        -0.1914, -0.2004,  0.0467,  0.0144,  0.0167, -0.0004, -0.1363, -0.0944,\n",
      "        -0.0846, -0.1734,  0.0105,  0.1364, -0.1122, -0.0127,  0.1167, -0.0659,\n",
      "        -0.0496,  0.0946,  0.0137,  0.1505, -0.0582,  0.0473, -0.2295, -0.1048,\n",
      "        -0.0118, -0.1000, -0.0767,  0.0394, -0.0572,  0.0360, -0.1225,  0.1299,\n",
      "        -0.0898,  0.0036, -0.1052,  0.1738, -0.1547, -0.0046, -0.0737,  0.0932,\n",
      "        -0.1146, -0.0283,  0.0006,  0.0109,  0.1251,  0.1539, -0.1232,  0.0430,\n",
      "        -0.1511,  0.0493, -0.0529,  0.0596,  0.0372,  0.1905, -0.1539,  0.0558,\n",
      "         0.1288,  0.1966,  0.0664, -0.0392,  0.0058, -0.1144,  0.0046, -0.0110,\n",
      "         0.0415,  0.1495,  0.1298, -0.0630, -0.1861, -0.0034, -0.0232, -0.1339,\n",
      "         0.0198, -0.0104,  0.0531,  0.0360,  0.0969, -0.0470, -0.1119,  0.1577,\n",
      "         0.0795, -0.0187, -0.0610, -0.0518,  0.0902, -0.1077,  0.0492, -0.0430],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 3.9019e-02,  2.5154e-02,  9.0798e-02, -9.7679e-02,  2.8764e-02,\n",
      "         7.0434e-02,  8.4063e-02,  2.5928e-02,  9.7380e-02,  1.6275e-02,\n",
      "         8.7966e-02,  1.8267e-02, -2.7582e-02,  3.0036e-02, -1.6191e-01,\n",
      "        -8.6450e-02, -1.8375e-03,  8.3683e-02,  1.2906e-02, -1.4225e-02,\n",
      "         1.1338e-01,  3.5779e-02,  1.1134e-01,  3.2772e-02,  3.0779e-02,\n",
      "         4.5491e-02,  6.9465e-02, -7.9200e-02,  1.2984e-01,  1.1974e-01,\n",
      "         5.7609e-02, -1.1568e-01, -1.6414e-01,  3.0509e-02,  6.8447e-02,\n",
      "        -6.2259e-02,  1.0754e-01, -2.4923e-02,  4.4897e-02,  5.2062e-02,\n",
      "         1.1688e-01,  1.9003e-01,  2.0063e-02,  6.7085e-02,  1.0077e-01,\n",
      "         5.4456e-02, -6.7470e-02,  1.3327e-01, -1.5840e-03, -6.3881e-02,\n",
      "         3.4684e-02,  3.7240e-02,  6.3400e-02, -2.8443e-02, -9.6012e-02,\n",
      "        -6.2349e-02, -1.1489e-01,  7.2854e-02, -1.0937e-01,  9.2610e-02,\n",
      "        -2.1004e-04, -3.7679e-02, -9.4393e-03,  2.1505e-01,  1.5926e-01,\n",
      "        -7.5369e-02,  1.9417e-01,  2.1636e-02, -8.1999e-02, -1.8520e-01,\n",
      "         1.7142e-02, -2.9865e-02, -4.0532e-02, -9.7150e-02,  1.5350e-01,\n",
      "        -1.8486e-01, -5.9385e-03,  1.1371e-02, -3.8125e-02, -1.3841e-02,\n",
      "        -2.0428e-02, -1.0368e-01, -4.4102e-02, -2.1970e-02,  1.0765e-02,\n",
      "        -5.5344e-02,  7.0728e-02,  4.6190e-02,  6.2990e-02, -5.4066e-02,\n",
      "        -1.6213e-01, -3.9405e-02,  1.3400e-01,  8.2171e-02, -2.3741e-01,\n",
      "         1.0548e-01, -9.5347e-02, -3.4751e-02, -3.8221e-02,  9.9864e-02,\n",
      "        -1.1664e-01,  1.0195e-01,  3.3650e-02,  8.7214e-03, -5.3215e-02,\n",
      "         7.9702e-02, -9.8969e-02,  1.6521e-01,  4.4730e-02, -2.8468e-02,\n",
      "        -7.8716e-02, -6.4015e-03,  1.3884e-01, -2.8019e-02,  2.1341e-01,\n",
      "        -9.0670e-02,  9.4125e-02, -6.3092e-02,  5.8177e-02,  6.7590e-02,\n",
      "         4.8755e-02, -9.2646e-02, -1.4212e-01, -6.4560e-02, -2.2409e-01,\n",
      "         1.1573e-03,  1.1887e-01, -7.8905e-02, -7.3512e-02, -3.9799e-02,\n",
      "         1.3260e-01,  4.2742e-02,  7.5729e-02,  3.1080e-02, -6.4224e-02,\n",
      "        -1.1484e-01,  1.0520e-01,  7.3674e-02, -1.0186e-01, -3.0466e-02,\n",
      "        -4.0072e-02,  3.1730e-02,  9.9246e-02, -7.0664e-02,  2.0741e-02,\n",
      "        -1.1007e-03, -2.3973e-01,  9.8560e-02,  4.4328e-02,  1.3719e-01,\n",
      "         1.7300e-01,  5.2069e-02, -1.3163e-02, -4.8457e-03,  1.1188e-01,\n",
      "        -4.3641e-02,  9.1629e-02, -7.9687e-02, -3.0985e-02, -5.0387e-02,\n",
      "        -1.0267e-02, -8.4093e-02, -1.3090e-01, -9.6986e-02, -1.1397e-01,\n",
      "         1.7749e-01, -7.0455e-03,  8.1856e-02, -3.4062e-02,  5.2526e-02,\n",
      "        -2.2659e-01,  1.9467e-01,  7.6326e-02,  2.5363e-03,  6.2256e-02,\n",
      "         9.6879e-02,  1.5179e-01, -1.3537e-01, -1.3574e-01, -2.1640e-03,\n",
      "        -1.6477e-01,  1.0380e-01,  6.0904e-02,  1.8073e-02, -5.3058e-02,\n",
      "         4.4933e-02,  2.5375e-01,  2.1265e-03, -2.3270e-02, -1.6723e-02,\n",
      "        -3.1699e-02, -1.4938e-01, -2.2361e-02,  2.1272e-03, -7.7590e-02,\n",
      "        -1.0800e-01, -2.0303e-02,  1.5057e-01, -2.4740e-02,  8.9488e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([-0.1176, -0.0570, -0.0881,  0.0382, -0.0470, -0.0022, -0.0419,  0.0347,\n",
      "        -0.0455,  0.1512, -0.2212,  0.0205,  0.0492,  0.0188,  0.1280,  0.0916,\n",
      "        -0.2246, -0.0407, -0.0579, -0.0697, -0.0332, -0.0426, -0.0589, -0.0386,\n",
      "         0.1296, -0.1049,  0.0089, -0.1006,  0.0838,  0.0634, -0.0685,  0.0280,\n",
      "         0.0230, -0.0933, -0.0234,  0.0639,  0.0010,  0.0445,  0.0817, -0.1759,\n",
      "         0.0889, -0.1021,  0.0453,  0.0165, -0.1924,  0.0662,  0.0422, -0.1617,\n",
      "         0.0394, -0.0525, -0.1020,  0.0969,  0.1674, -0.0956,  0.0062, -0.2188,\n",
      "        -0.1477, -0.0795, -0.0521,  0.0115, -0.0537, -0.0015,  0.0250,  0.0240,\n",
      "        -0.2109, -0.0307, -0.1652, -0.1158,  0.0194, -0.1232, -0.0790,  0.0154,\n",
      "        -0.1488, -0.0429, -0.0399,  0.0404, -0.0934,  0.0394, -0.0222,  0.0784,\n",
      "        -0.0192,  0.0321,  0.1407,  0.0594,  0.0645,  0.2660,  0.1538, -0.0809,\n",
      "         0.0616,  0.0261,  0.0989, -0.0503, -0.0319,  0.0858, -0.1420, -0.0518,\n",
      "         0.1445, -0.0124, -0.0205, -0.1577, -0.0543, -0.0750,  0.0879,  0.0205,\n",
      "        -0.0879,  0.1437, -0.1579, -0.1448, -0.1457, -0.0660, -0.0659, -0.0375,\n",
      "         0.0527, -0.0404,  0.1653,  0.0289,  0.1094, -0.1015,  0.1626, -0.0075,\n",
      "         0.1662,  0.2571,  0.0893,  0.0890, -0.0148, -0.0483, -0.0005,  0.0456,\n",
      "         0.0326, -0.0538, -0.2244,  0.0251, -0.0627,  0.0730, -0.0966, -0.1493,\n",
      "        -0.0732,  0.0858,  0.1233,  0.2035,  0.0183,  0.1147,  0.1068,  0.0161,\n",
      "         0.0902, -0.0651, -0.0715,  0.0183,  0.0563,  0.1722,  0.0345, -0.0602,\n",
      "        -0.1160,  0.0269, -0.1717,  0.0408,  0.0133,  0.1677,  0.0753,  0.2172,\n",
      "        -0.0146,  0.0282, -0.0693,  0.1199, -0.0784, -0.0165,  0.1006,  0.0451,\n",
      "        -0.1145,  0.1461,  0.0647, -0.0945,  0.0623, -0.1480, -0.0218, -0.1240,\n",
      "        -0.0219,  0.0630, -0.0281, -0.0556,  0.0266,  0.0834, -0.0797, -0.1292,\n",
      "         0.0918,  0.0871,  0.0766, -0.0546, -0.1768, -0.0866, -0.0672,  0.0826,\n",
      "        -0.1746,  0.0408, -0.1357, -0.1097, -0.0017,  0.0469, -0.0158, -0.1136],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0365,  0.1492,  0.1437,  0.1214, -0.1905,  0.0414,  0.0920, -0.1194,\n",
      "        -0.0793,  0.0523, -0.0206, -0.0877, -0.1264, -0.0247,  0.0060,  0.1693,\n",
      "        -0.1031,  0.0021,  0.1151,  0.0680, -0.0114, -0.1083,  0.0192, -0.1049,\n",
      "         0.0003,  0.1907, -0.1256,  0.0967,  0.0163,  0.0660,  0.1730, -0.0695,\n",
      "         0.1811,  0.0153, -0.0434, -0.0872,  0.0316, -0.0302, -0.0075, -0.0259,\n",
      "        -0.0154,  0.0773, -0.1435,  0.1663, -0.0149, -0.0396,  0.1477,  0.2279,\n",
      "         0.0989,  0.0160,  0.0486,  0.0256,  0.0010,  0.0822,  0.0260,  0.0013,\n",
      "        -0.0255, -0.0615,  0.0043, -0.1938, -0.0396, -0.0095,  0.0934,  0.0380,\n",
      "        -0.0638,  0.1835, -0.0801, -0.1317,  0.2654,  0.0595, -0.0071, -0.1468,\n",
      "        -0.0288, -0.1604,  0.0815, -0.0099, -0.0089, -0.0636,  0.0295, -0.0150,\n",
      "        -0.1872, -0.0362,  0.1606, -0.1631, -0.0683,  0.1717, -0.1120, -0.0207,\n",
      "        -0.0738, -0.0534,  0.1167,  0.0465, -0.1569,  0.0623,  0.1992, -0.0154,\n",
      "        -0.0664,  0.0287,  0.0269, -0.0208,  0.0981,  0.1508,  0.0021,  0.0982,\n",
      "         0.0723,  0.0228,  0.0112,  0.0613,  0.1816, -0.1070,  0.0553,  0.0236,\n",
      "        -0.0179, -0.0131,  0.0833, -0.0739,  0.0031,  0.0167,  0.0272, -0.1319,\n",
      "         0.0396,  0.0491,  0.0588,  0.0015, -0.0071,  0.1565, -0.0120, -0.1833,\n",
      "         0.1558,  0.1691,  0.0405, -0.1244, -0.0768, -0.0768, -0.1019,  0.1306,\n",
      "         0.0825, -0.0200, -0.0242,  0.0078,  0.0199, -0.1735, -0.1019, -0.0651,\n",
      "        -0.1056, -0.0799,  0.1560,  0.1587, -0.0255,  0.1213, -0.0596,  0.1384,\n",
      "        -0.0424,  0.0252, -0.1582,  0.1200, -0.0028, -0.1406, -0.2185,  0.1694,\n",
      "         0.0334,  0.1966, -0.0360,  0.1704,  0.2084, -0.1135, -0.0869,  0.1150,\n",
      "         0.0498,  0.0928, -0.0948, -0.0336,  0.1267,  0.0648, -0.0360,  0.0417,\n",
      "        -0.0814, -0.0225,  0.2085, -0.0853,  0.0700,  0.1192, -0.0077,  0.0075,\n",
      "        -0.0558, -0.1166,  0.0397,  0.0466,  0.0524, -0.0005,  0.0760,  0.0648,\n",
      "        -0.0572, -0.0123, -0.1625, -0.2079,  0.0009,  0.1398, -0.1563, -0.0761],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-0.0874, -0.2878,  0.0162,  0.1137, -0.0216, -0.0095, -0.0791, -0.0112,\n",
      "         0.1378,  0.0276, -0.0191,  0.0996, -0.0046,  0.0440, -0.1734, -0.1609,\n",
      "        -0.0709, -0.2547,  0.0804,  0.1218,  0.2098, -0.1937, -0.0028, -0.1298,\n",
      "        -0.0120,  0.0623, -0.1709, -0.0088,  0.0245, -0.0329, -0.1025, -0.0525,\n",
      "        -0.0827,  0.2040, -0.0738, -0.0449, -0.0073,  0.0925,  0.0297, -0.0470,\n",
      "        -0.0164,  0.1825, -0.2102, -0.0393, -0.0126,  0.0600, -0.0207, -0.1989,\n",
      "         0.0681,  0.0819, -0.0137,  0.0233,  0.0062, -0.0912, -0.0540, -0.0826,\n",
      "         0.0754,  0.1373,  0.1662, -0.0536, -0.0954, -0.1096, -0.1003, -0.0158,\n",
      "        -0.2121,  0.0899, -0.2017,  0.0826,  0.1063,  0.1378, -0.0192,  0.1304,\n",
      "         0.2962, -0.0886,  0.0716,  0.2377,  0.0810,  0.0063,  0.0698,  0.1793,\n",
      "        -0.0389,  0.0985, -0.1307, -0.2917, -0.1084, -0.1743,  0.0437,  0.0713,\n",
      "        -0.0190, -0.1089, -0.1086,  0.0735,  0.0281,  0.0158,  0.0761, -0.2097,\n",
      "         0.1789, -0.1032,  0.0685, -0.1225,  0.0463,  0.2206,  0.0182,  0.0383,\n",
      "        -0.0163,  0.0126, -0.0634,  0.0739, -0.1272, -0.0418,  0.0581, -0.0152,\n",
      "        -0.1131,  0.0571,  0.0255, -0.0762, -0.1088, -0.0394,  0.0134, -0.1451,\n",
      "        -0.0832, -0.0144,  0.1111, -0.0474,  0.1000,  0.0990,  0.0673, -0.0446,\n",
      "        -0.0586,  0.0127,  0.0218, -0.0582,  0.2106,  0.0093, -0.1277, -0.0548,\n",
      "        -0.0257,  0.2035, -0.1348, -0.0572, -0.0537, -0.0828, -0.1483,  0.0311,\n",
      "        -0.0788,  0.0215, -0.1438, -0.0198, -0.1424,  0.1543, -0.1733, -0.0286,\n",
      "        -0.1172, -0.0252,  0.0334, -0.0683,  0.1256, -0.0856, -0.0397, -0.0185,\n",
      "        -0.1631, -0.1860,  0.0950, -0.1286, -0.0509,  0.0371,  0.0340, -0.0386,\n",
      "        -0.0047,  0.0548, -0.0648, -0.0035,  0.2056, -0.1304,  0.0738,  0.0144,\n",
      "         0.0309, -0.1177, -0.1432,  0.0321,  0.0579, -0.1703, -0.0908,  0.0473,\n",
      "         0.1625, -0.0337,  0.0552,  0.0707,  0.0216, -0.0073,  0.0840, -0.0357,\n",
      "        -0.0778,  0.0333,  0.0204,  0.1302, -0.0612, -0.0760,  0.0045, -0.0715],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 0.1300, -0.0290,  0.0912, -0.1956,  0.0597,  0.0285, -0.1028,  0.0317,\n",
      "        -0.0103,  0.1087,  0.0391,  0.0249,  0.1348,  0.0188,  0.2061, -0.0331,\n",
      "        -0.0511,  0.1448, -0.0687, -0.0512,  0.1953, -0.1046, -0.1168,  0.0371,\n",
      "        -0.0454,  0.1605,  0.2064,  0.0105,  0.0217, -0.1555,  0.0123, -0.0809,\n",
      "         0.0847,  0.0267, -0.2364, -0.1307,  0.0148, -0.1221,  0.1381, -0.0256,\n",
      "         0.0939,  0.0111, -0.0287, -0.2360,  0.0629,  0.1009,  0.0447,  0.1768,\n",
      "        -0.0366, -0.0532,  0.2044,  0.1877, -0.0016,  0.1828, -0.1052, -0.1184,\n",
      "        -0.2589,  0.1056,  0.2014, -0.0456,  0.0197, -0.0621,  0.0749, -0.0556,\n",
      "        -0.1770,  0.1183,  0.2220, -0.0243, -0.0030,  0.0711,  0.0164, -0.0922,\n",
      "         0.0700, -0.0595,  0.1727,  0.0145,  0.0382, -0.3578, -0.0543,  0.0084,\n",
      "         0.0855, -0.0105, -0.0112,  0.0679, -0.1331,  0.0217,  0.0218,  0.0706,\n",
      "        -0.1121, -0.0037,  0.0218, -0.0264, -0.0442,  0.0885, -0.0255, -0.0512,\n",
      "        -0.0117, -0.0858, -0.1294, -0.0291, -0.0313,  0.1300, -0.1170,  0.0544,\n",
      "         0.0718,  0.1288, -0.1224,  0.1531, -0.0877,  0.0856, -0.0822, -0.0295,\n",
      "         0.0070, -0.0089, -0.1059, -0.1029,  0.1119, -0.1977,  0.1345, -0.1227,\n",
      "        -0.1313, -0.0701,  0.0925,  0.0867, -0.0602, -0.0454, -0.1162,  0.1331,\n",
      "        -0.0070,  0.0286,  0.1760, -0.1712,  0.0136,  0.0362,  0.0169, -0.0475,\n",
      "        -0.0543, -0.1353, -0.0515,  0.0787, -0.0935, -0.2302,  0.0657, -0.0193,\n",
      "        -0.0741, -0.1347, -0.1320, -0.0633,  0.0482,  0.0237,  0.0235,  0.1115,\n",
      "         0.0948,  0.2133,  0.0239,  0.0536,  0.0297,  0.1637, -0.1271,  0.0527,\n",
      "         0.0373, -0.0568,  0.0910, -0.0004, -0.1461,  0.1952, -0.0235,  0.0972,\n",
      "         0.0026,  0.0695,  0.1002,  0.0492, -0.0383, -0.0427, -0.0296,  0.0046,\n",
      "         0.0502,  0.0025,  0.0582,  0.1880, -0.0915, -0.0357,  0.0783,  0.0295,\n",
      "        -0.0559, -0.1317,  0.0729, -0.0005,  0.0163, -0.0260, -0.0636, -0.0965,\n",
      "        -0.0161,  0.0863,  0.0049, -0.0130, -0.0310,  0.0269,  0.0490, -0.0194],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-0.0755,  0.1157,  0.1487,  0.0312,  0.0088, -0.0934,  0.1386,  0.0494,\n",
      "         0.1581,  0.0538, -0.1382,  0.1405, -0.0518, -0.1131, -0.0435,  0.0108,\n",
      "        -0.0136, -0.0261,  0.0036, -0.0735,  0.0057,  0.1861, -0.0205, -0.0147,\n",
      "         0.1115, -0.1055, -0.0333, -0.0581,  0.0298, -0.1677,  0.0853, -0.1542,\n",
      "         0.0841, -0.0789,  0.1823,  0.0546, -0.1157, -0.0021, -0.0624, -0.0842,\n",
      "        -0.0643,  0.0172, -0.0460, -0.0905, -0.1069,  0.0294,  0.1544, -0.0694,\n",
      "         0.0053,  0.0024,  0.1008,  0.1026,  0.2395,  0.0446, -0.0760,  0.0411,\n",
      "        -0.0543,  0.0697, -0.0095,  0.2348, -0.0666,  0.0692, -0.0046,  0.0664,\n",
      "        -0.0122, -0.0853,  0.0614, -0.0631,  0.1126, -0.1014, -0.0805,  0.0063,\n",
      "         0.0694, -0.0091, -0.1463,  0.0367, -0.0787,  0.1328, -0.1187,  0.0344,\n",
      "         0.1351, -0.1024,  0.0370, -0.0922, -0.0747,  0.0126,  0.0906, -0.0895,\n",
      "        -0.1623,  0.0673,  0.0384, -0.0280,  0.0264,  0.0056,  0.0317, -0.0555,\n",
      "         0.0636, -0.2060,  0.0456,  0.0843, -0.0197,  0.0903,  0.2240,  0.1724,\n",
      "         0.0046,  0.0300,  0.0270,  0.0626,  0.1121,  0.0250, -0.0804,  0.0623,\n",
      "         0.0696,  0.0334,  0.0043, -0.2056,  0.1308,  0.0856, -0.0198, -0.0268,\n",
      "         0.1300, -0.0193,  0.0783,  0.0175, -0.0085,  0.0610, -0.0509,  0.0733,\n",
      "         0.0186, -0.0053,  0.0114, -0.0056, -0.0509,  0.0588,  0.1049,  0.0542,\n",
      "         0.1166, -0.1209,  0.0434,  0.1118,  0.0814, -0.0112,  0.0703, -0.0407,\n",
      "        -0.0151, -0.0558,  0.0140,  0.1696, -0.0412,  0.1057,  0.0986, -0.0241,\n",
      "        -0.0095, -0.0080, -0.0638, -0.0291,  0.0576, -0.0776, -0.1520, -0.1818,\n",
      "         0.0851, -0.0793,  0.1604, -0.0665,  0.0501,  0.1502, -0.1287,  0.0479,\n",
      "        -0.1453,  0.0263, -0.0035,  0.0960,  0.0638, -0.1566,  0.0418,  0.0929,\n",
      "         0.1142, -0.0140,  0.0344,  0.1191, -0.1846, -0.0907,  0.1322, -0.0362,\n",
      "         0.0544, -0.1205, -0.0043,  0.0934, -0.0007,  0.0405,  0.1871, -0.0829,\n",
      "        -0.0702,  0.1021,  0.0760,  0.0168, -0.0093, -0.1217,  0.0335,  0.1808],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 0.0619,  0.1403, -0.1077, -0.2767,  0.2080,  0.0575, -0.0134, -0.1055,\n",
      "         0.1499,  0.1858,  0.0217,  0.1187, -0.0649,  0.2870, -0.1568,  0.1042,\n",
      "        -0.1069, -0.1348,  0.0584, -0.1022, -0.0325, -0.0836,  0.0143,  0.0472,\n",
      "         0.0017, -0.0113, -0.0767, -0.1494,  0.0455, -0.1245, -0.0429,  0.0766,\n",
      "         0.0107, -0.0946, -0.1371,  0.1191,  0.1512,  0.0053,  0.0394, -0.0465,\n",
      "         0.0175, -0.1627,  0.0771,  0.0206,  0.1903, -0.1087,  0.1277,  0.0255,\n",
      "         0.1399,  0.1205,  0.1156, -0.0843,  0.0157,  0.0475, -0.1077, -0.0837,\n",
      "        -0.0339, -0.1300, -0.0038, -0.0063, -0.1576,  0.0628,  0.1015, -0.0629,\n",
      "        -0.0113,  0.1316,  0.1097,  0.0832,  0.0638, -0.0506, -0.0556, -0.0724,\n",
      "        -0.0188, -0.0401, -0.1590,  0.0450,  0.0624, -0.0515, -0.0752, -0.0796,\n",
      "        -0.0007, -0.3024, -0.2144, -0.0835, -0.0758, -0.1519, -0.1939,  0.1572,\n",
      "        -0.1857,  0.1345, -0.1711, -0.0674,  0.0412, -0.1714, -0.0072, -0.2019,\n",
      "         0.0787,  0.0984,  0.0226, -0.0245, -0.0423,  0.1742, -0.2161, -0.0183,\n",
      "        -0.0029, -0.0044,  0.1192,  0.0439,  0.1932,  0.0082,  0.1160, -0.1654,\n",
      "        -0.0213, -0.1270, -0.0089, -0.0236, -0.0589,  0.1086,  0.1448, -0.0744,\n",
      "        -0.0636, -0.0634,  0.0920, -0.0227, -0.0138, -0.2378,  0.0915,  0.0410,\n",
      "        -0.0771, -0.2554, -0.0056, -0.0511,  0.0792, -0.1065,  0.0931,  0.0945,\n",
      "         0.1621, -0.0576,  0.0169,  0.0327,  0.0696,  0.0691, -0.0391,  0.1367,\n",
      "        -0.0409, -0.0418,  0.0063, -0.0193, -0.0210, -0.0267,  0.1050, -0.0354,\n",
      "         0.1055,  0.0792,  0.0318, -0.0377,  0.0276, -0.1005, -0.1378, -0.0189,\n",
      "         0.0850,  0.1300, -0.0152, -0.1383,  0.0798, -0.1142, -0.0370, -0.0458,\n",
      "        -0.1406,  0.0460, -0.0168,  0.1019,  0.0849,  0.0184,  0.0901, -0.1363,\n",
      "         0.2124,  0.0447, -0.1437,  0.0413, -0.3102,  0.0478,  0.0038, -0.0860,\n",
      "        -0.1465, -0.0472, -0.1059,  0.2646,  0.0621, -0.1125,  0.0021,  0.0192,\n",
      "        -0.0153, -0.1243, -0.0433, -0.1272, -0.0750,  0.0914, -0.1973,  0.0438],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([-4.2623e-02, -3.6569e-02,  2.1554e-01,  7.3948e-02,  4.5928e-02,\n",
      "         8.9498e-02,  5.3931e-02,  7.0713e-02, -1.2092e-01,  1.1116e-01,\n",
      "        -3.0323e-02, -1.2760e-01, -8.4349e-02,  8.3266e-02, -1.4023e-01,\n",
      "        -1.5514e-02, -1.0077e-01,  1.6821e-02,  1.4161e-01,  4.8444e-02,\n",
      "         2.6380e-01,  1.6819e-02,  2.7638e-02,  1.6771e-01, -4.9164e-02,\n",
      "        -5.8186e-02,  3.7733e-02, -7.5490e-02, -1.2715e-01,  6.3545e-03,\n",
      "         4.5001e-02, -1.1637e-01,  4.3808e-02, -6.4871e-02, -1.9854e-01,\n",
      "        -8.2011e-02,  4.6362e-02,  4.2026e-02,  3.5503e-02, -9.2668e-03,\n",
      "        -6.1567e-02, -7.3332e-02,  1.2077e-01,  9.6574e-03, -1.1398e-01,\n",
      "        -8.3217e-02,  9.7229e-02, -1.8468e-01,  1.0019e-01,  1.1664e-01,\n",
      "        -4.1808e-03, -1.5724e-01, -1.2025e-01, -4.9540e-02,  8.5170e-02,\n",
      "         7.6881e-02,  1.2248e-01, -4.9930e-02, -1.4443e-01,  1.7782e-01,\n",
      "         4.8178e-02,  5.4946e-02, -4.6922e-02,  2.5569e-02,  6.4497e-02,\n",
      "         6.3852e-03, -2.8890e-02, -2.0665e-02,  6.1656e-02, -8.2958e-03,\n",
      "        -1.1087e-01, -4.8137e-02,  1.0483e-01,  6.2579e-02, -1.6083e-04,\n",
      "        -1.1344e-01, -8.9015e-02, -1.7166e-02, -4.1521e-02, -1.5454e-01,\n",
      "        -1.4731e-01,  8.7373e-02, -2.9756e-02,  2.0193e-02, -1.1296e-01,\n",
      "        -3.1650e-02,  4.1894e-03, -1.8676e-01,  6.8287e-02, -2.2091e-02,\n",
      "        -1.2922e-01,  2.7839e-02, -1.4709e-01, -4.7553e-02, -3.7346e-03,\n",
      "         2.2710e-02,  2.5883e-02, -2.9356e-02,  1.6152e-01,  1.6246e-01,\n",
      "        -1.3543e-01,  8.9756e-02, -4.2627e-02, -8.3722e-02, -1.5255e-01,\n",
      "         2.2945e-01, -1.2727e-01, -2.3556e-01, -2.0420e-02,  1.3193e-01,\n",
      "         4.7064e-02,  1.3503e-01, -1.1775e-01,  4.4378e-02, -3.4163e-02,\n",
      "        -1.9214e-01,  1.5731e-01, -3.1321e-02,  6.0982e-02, -5.8376e-03,\n",
      "         1.5299e-01, -5.1013e-02, -1.3059e-01, -4.6080e-02, -4.5595e-02,\n",
      "         1.3429e-02, -6.2867e-02, -9.5215e-02, -1.7407e-02,  7.2585e-02,\n",
      "         3.3903e-02, -8.6348e-02, -1.3825e-01,  4.7451e-02, -5.0096e-02,\n",
      "        -3.1774e-02,  6.5457e-02,  1.4885e-01, -2.2025e-01, -4.0868e-02,\n",
      "         9.5003e-02, -2.9685e-02, -9.5620e-03, -1.2427e-01,  1.7506e-02,\n",
      "        -6.4477e-02,  7.1862e-02,  7.0507e-03, -5.8418e-02,  1.5294e-03,\n",
      "         3.2851e-02,  1.9095e-02, -8.0235e-02,  6.8884e-02,  6.0092e-02,\n",
      "         1.5271e-01,  1.9154e-01, -1.1346e-01,  6.4830e-02,  3.5148e-02,\n",
      "         1.3541e-01, -3.6145e-02,  7.7164e-02,  6.6006e-02, -9.8362e-02,\n",
      "        -5.8074e-02, -6.8333e-02, -6.2350e-02, -1.1440e-01, -7.7808e-02,\n",
      "        -5.0657e-02, -1.3394e-01,  7.3470e-03,  2.3200e-02,  5.6048e-02,\n",
      "        -1.1629e-01,  1.7295e-02, -3.8359e-02, -9.3535e-02,  5.3482e-02,\n",
      "         9.3346e-02, -5.7120e-03,  4.7397e-02, -1.3661e-01,  4.1543e-02,\n",
      "        -3.0960e-02, -2.6104e-02,  3.4355e-02, -8.6857e-02,  9.2401e-03,\n",
      "        -1.4995e-01,  1.1801e-01, -5.6550e-03, -9.9898e-02,  1.3610e-02,\n",
      "         1.8182e-02, -1.3491e-01, -8.2483e-02, -6.1044e-02, -5.1449e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-0.0404,  0.0334, -0.0873, -0.0599, -0.0711,  0.0188,  0.0374,  0.0498,\n",
      "         0.0180,  0.0348, -0.1043,  0.1819, -0.0947,  0.0222,  0.0732,  0.0492,\n",
      "         0.1486,  0.1599, -0.0004,  0.1500,  0.0163, -0.0051,  0.1059,  0.0772,\n",
      "         0.0216,  0.1549, -0.0047,  0.0367,  0.0610, -0.1326,  0.2364, -0.0735,\n",
      "        -0.1667,  0.0856,  0.0432, -0.1610, -0.1542,  0.0944, -0.0258,  0.0386,\n",
      "         0.0133,  0.0888,  0.0012, -0.1798,  0.0380, -0.0701,  0.0899, -0.0979,\n",
      "         0.1967, -0.0406, -0.0714, -0.1593,  0.0574,  0.0158, -0.1353, -0.0116,\n",
      "        -0.0659, -0.0594, -0.0836,  0.1687,  0.1432, -0.1235, -0.1411, -0.1314,\n",
      "        -0.0911,  0.2131, -0.1926, -0.0544, -0.1664,  0.0304,  0.1048, -0.0392,\n",
      "        -0.0422, -0.1233,  0.0076, -0.0738, -0.1557,  0.1152,  0.0373, -0.0577,\n",
      "         0.0090, -0.0523,  0.0164,  0.0149, -0.0101, -0.0796,  0.0258, -0.1029,\n",
      "         0.0345, -0.0131, -0.0666, -0.0779,  0.0692, -0.0341, -0.0940, -0.1161,\n",
      "        -0.0177,  0.1019, -0.1398,  0.0373, -0.1376, -0.0576,  0.0860, -0.0356,\n",
      "         0.1078,  0.0419,  0.2311, -0.0366,  0.1426, -0.1082, -0.0577, -0.0187,\n",
      "         0.0658, -0.1028, -0.0874, -0.0337,  0.1840,  0.0999,  0.2344,  0.0839,\n",
      "         0.1116,  0.0364,  0.0575,  0.0951,  0.0945,  0.0339, -0.0087,  0.0691,\n",
      "         0.0113,  0.0132, -0.0651,  0.0311, -0.0628,  0.0944, -0.0417,  0.1208,\n",
      "         0.0239,  0.1379,  0.1112,  0.1446, -0.1183, -0.0495, -0.0503, -0.1729,\n",
      "         0.0460, -0.1241, -0.0561,  0.0750, -0.1225, -0.0539,  0.2356,  0.0499,\n",
      "        -0.0831,  0.0315,  0.1233,  0.1682, -0.1815,  0.1077,  0.1187,  0.1555,\n",
      "         0.1233, -0.0816, -0.0952, -0.1361,  0.0105,  0.0832,  0.0652,  0.0702,\n",
      "         0.0459,  0.0453, -0.0204, -0.0968, -0.0660,  0.0714, -0.0433,  0.0744,\n",
      "        -0.0709,  0.0625,  0.0672, -0.1082, -0.0944,  0.1826,  0.0588,  0.0226,\n",
      "         0.1245, -0.0560,  0.1531,  0.0472,  0.0311, -0.0327, -0.1191,  0.0361,\n",
      "        -0.1240,  0.0982,  0.0693,  0.0030, -0.1002,  0.2047,  0.0471, -0.0009],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0050000'], tr/val_loss:  2.451432/ 57.374870, val:  32.08%, val_best:  32.08%, tr:  22.68%, tr_best:  22.68%, epoch time: 67.77 seconds, 1.13 minutes\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0050000'], tr/val_loss:  2.471321/ 51.672382, val:  35.42%, val_best:  35.42%, tr:  32.99%, tr_best:  32.99%, epoch time: 65.57 seconds, 1.09 minutes\n",
      "epoch-2   lr=['0.0050000'], tr/val_loss:  2.332162/ 47.181427, val:  37.92%, val_best:  37.92%, tr:  36.98%, tr_best:  36.98%, epoch time: 66.31 seconds, 1.11 minutes\n",
      "epoch-3   lr=['0.0050000'], tr/val_loss:  2.048363/ 37.962055, val:  43.33%, val_best:  43.33%, tr:  40.86%, tr_best:  40.86%, epoch time: 65.36 seconds, 1.09 minutes\n",
      "epoch-4   lr=['0.0050000'], tr/val_loss:  2.030159/ 44.188152, val:  40.00%, val_best:  43.33%, tr:  43.00%, tr_best:  43.00%, epoch time: 67.64 seconds, 1.13 minutes\n",
      "epoch-5   lr=['0.0050000'], tr/val_loss:  2.085287/ 49.295368, val:  39.58%, val_best:  43.33%, tr:  44.23%, tr_best:  44.23%, epoch time: 66.74 seconds, 1.11 minutes\n",
      "epoch-6   lr=['0.0050000'], tr/val_loss:  2.147457/ 57.513889, val:  42.08%, val_best:  43.33%, tr:  44.64%, tr_best:  44.64%, epoch time: 66.36 seconds, 1.11 minutes\n",
      "epoch-7   lr=['0.0050000'], tr/val_loss:  2.047343/ 37.063492, val:  42.92%, val_best:  43.33%, tr:  48.42%, tr_best:  48.42%, epoch time: 66.59 seconds, 1.11 minutes\n",
      "epoch-8   lr=['0.0050000'], tr/val_loss:  2.001147/ 47.617195, val:  37.50%, val_best:  43.33%, tr:  48.11%, tr_best:  48.42%, epoch time: 65.95 seconds, 1.10 minutes\n",
      "epoch-9   lr=['0.0050000'], tr/val_loss:  2.006809/ 42.605659, val:  46.67%, val_best:  46.67%, tr:  47.29%, tr_best:  48.42%, epoch time: 66.26 seconds, 1.10 minutes\n",
      "epoch-10  lr=['0.0050000'], tr/val_loss:  1.919999/ 69.754768, val:  39.58%, val_best:  46.67%, tr:  51.17%, tr_best:  51.17%, epoch time: 66.30 seconds, 1.11 minutes\n",
      "epoch-11  lr=['0.0050000'], tr/val_loss:  2.099768/ 50.717800, val:  45.00%, val_best:  46.67%, tr:  49.13%, tr_best:  51.17%, epoch time: 66.90 seconds, 1.11 minutes\n",
      "epoch-12  lr=['0.0050000'], tr/val_loss:  2.030907/ 45.719982, val:  42.50%, val_best:  46.67%, tr:  50.77%, tr_best:  51.17%, epoch time: 67.53 seconds, 1.13 minutes\n",
      "epoch-13  lr=['0.0050000'], tr/val_loss:  1.879328/ 97.194733, val:  26.67%, val_best:  46.67%, tr:  53.83%, tr_best:  53.83%, epoch time: 65.59 seconds, 1.09 minutes\n",
      "epoch-14  lr=['0.0050000'], tr/val_loss:  1.844549/ 69.444443, val:  41.25%, val_best:  46.67%, tr:  54.44%, tr_best:  54.44%, epoch time: 65.57 seconds, 1.09 minutes\n",
      "epoch-15  lr=['0.0050000'], tr/val_loss:  1.924921/ 48.668419, val:  51.67%, val_best:  51.67%, tr:  53.83%, tr_best:  54.44%, epoch time: 66.32 seconds, 1.11 minutes\n",
      "epoch-16  lr=['0.0050000'], tr/val_loss:  1.724756/ 55.091415, val:  45.42%, val_best:  51.67%, tr:  55.06%, tr_best:  55.06%, epoch time: 66.93 seconds, 1.12 minutes\n",
      "epoch-17  lr=['0.0050000'], tr/val_loss:  1.817694/ 34.605526, val:  50.83%, val_best:  51.67%, tr:  54.65%, tr_best:  55.06%, epoch time: 65.96 seconds, 1.10 minutes\n",
      "epoch-18  lr=['0.0050000'], tr/val_loss:  1.827630/ 74.080849, val:  40.00%, val_best:  51.67%, tr:  53.93%, tr_best:  55.06%, epoch time: 65.05 seconds, 1.08 minutes\n",
      "epoch-19  lr=['0.0050000'], tr/val_loss:  1.763952/ 64.903511, val:  48.33%, val_best:  51.67%, tr:  56.49%, tr_best:  56.49%, epoch time: 65.62 seconds, 1.09 minutes\n",
      "epoch-20  lr=['0.0050000'], tr/val_loss:  1.829478/ 82.297623, val:  47.92%, val_best:  51.67%, tr:  54.65%, tr_best:  56.49%, epoch time: 64.75 seconds, 1.08 minutes\n",
      "epoch-21  lr=['0.0050000'], tr/val_loss:  1.948868/ 55.399570, val:  46.25%, val_best:  51.67%, tr:  52.60%, tr_best:  56.49%, epoch time: 65.80 seconds, 1.10 minutes\n",
      "epoch-22  lr=['0.0050000'], tr/val_loss:  1.896258/ 71.649757, val:  52.92%, val_best:  52.92%, tr:  53.93%, tr_best:  56.49%, epoch time: 65.41 seconds, 1.09 minutes\n",
      "epoch-23  lr=['0.0050000'], tr/val_loss:  1.834950/ 63.486397, val:  50.42%, val_best:  52.92%, tr:  55.46%, tr_best:  56.49%, epoch time: 65.72 seconds, 1.10 minutes\n",
      "epoch-24  lr=['0.0050000'], tr/val_loss:  1.862932/ 64.144104, val:  40.42%, val_best:  52.92%, tr:  56.59%, tr_best:  56.59%, epoch time: 65.10 seconds, 1.08 minutes\n",
      "epoch-25  lr=['0.0050000'], tr/val_loss:  2.042479/ 54.583061, val:  54.17%, val_best:  54.17%, tr:  54.14%, tr_best:  56.59%, epoch time: 65.90 seconds, 1.10 minutes\n",
      "epoch-26  lr=['0.0050000'], tr/val_loss:  1.880007/ 42.769276, val:  56.67%, val_best:  56.67%, tr:  55.46%, tr_best:  56.59%, epoch time: 64.17 seconds, 1.07 minutes\n",
      "epoch-27  lr=['0.0050000'], tr/val_loss:  1.743920/ 51.973095, val:  51.25%, val_best:  56.67%, tr:  57.51%, tr_best:  57.51%, epoch time: 66.17 seconds, 1.10 minutes\n",
      "epoch-28  lr=['0.0050000'], tr/val_loss:  1.703927/ 47.692963, val:  50.42%, val_best:  56.67%, tr:  57.20%, tr_best:  57.51%, epoch time: 65.73 seconds, 1.10 minutes\n",
      "epoch-29  lr=['0.0050000'], tr/val_loss:  1.786374/ 60.983147, val:  45.83%, val_best:  56.67%, tr:  56.49%, tr_best:  57.51%, epoch time: 65.89 seconds, 1.10 minutes\n",
      "epoch-30  lr=['0.0050000'], tr/val_loss:  1.750815/ 41.205074, val:  58.75%, val_best:  58.75%, tr:  57.30%, tr_best:  57.51%, epoch time: 65.94 seconds, 1.10 minutes\n",
      "epoch-31  lr=['0.0050000'], tr/val_loss:  1.599763/ 47.768635, val:  57.08%, val_best:  58.75%, tr:  58.43%, tr_best:  58.43%, epoch time: 65.90 seconds, 1.10 minutes\n",
      "epoch-32  lr=['0.0050000'], tr/val_loss:  1.567716/ 50.460079, val:  52.08%, val_best:  58.75%, tr:  59.04%, tr_best:  59.04%, epoch time: 66.37 seconds, 1.11 minutes\n",
      "epoch-33  lr=['0.0050000'], tr/val_loss:  1.612382/ 46.431767, val:  47.08%, val_best:  58.75%, tr:  58.84%, tr_best:  59.04%, epoch time: 66.01 seconds, 1.10 minutes\n",
      "epoch-34  lr=['0.0050000'], tr/val_loss:  1.527555/ 63.002827, val:  45.00%, val_best:  58.75%, tr:  59.65%, tr_best:  59.65%, epoch time: 66.30 seconds, 1.10 minutes\n",
      "epoch-35  lr=['0.0050000'], tr/val_loss:  1.409579/ 35.192924, val:  56.67%, val_best:  58.75%, tr:  62.10%, tr_best:  62.10%, epoch time: 65.18 seconds, 1.09 minutes\n",
      "epoch-36  lr=['0.0050000'], tr/val_loss:  1.518343/ 48.940941, val:  49.58%, val_best:  58.75%, tr:  60.16%, tr_best:  62.10%, epoch time: 64.72 seconds, 1.08 minutes\n",
      "epoch-37  lr=['0.0050000'], tr/val_loss:  1.310642/ 40.120384, val:  53.33%, val_best:  58.75%, tr:  64.56%, tr_best:  64.56%, epoch time: 64.84 seconds, 1.08 minutes\n",
      "epoch-38  lr=['0.0050000'], tr/val_loss:  1.457624/ 32.385571, val:  57.50%, val_best:  58.75%, tr:  60.88%, tr_best:  64.56%, epoch time: 65.64 seconds, 1.09 minutes\n",
      "epoch-39  lr=['0.0050000'], tr/val_loss:  1.343325/ 62.052464, val:  45.00%, val_best:  58.75%, tr:  62.92%, tr_best:  64.56%, epoch time: 64.63 seconds, 1.08 minutes\n",
      "epoch-40  lr=['0.0050000'], tr/val_loss:  1.362396/ 50.852329, val:  46.25%, val_best:  58.75%, tr:  62.61%, tr_best:  64.56%, epoch time: 65.87 seconds, 1.10 minutes\n",
      "epoch-41  lr=['0.0050000'], tr/val_loss:  1.385601/ 35.175964, val:  54.58%, val_best:  58.75%, tr:  63.13%, tr_best:  64.56%, epoch time: 64.25 seconds, 1.07 minutes\n",
      "epoch-42  lr=['0.0050000'], tr/val_loss:  1.330709/ 51.275887, val:  53.33%, val_best:  58.75%, tr:  62.31%, tr_best:  64.56%, epoch time: 66.15 seconds, 1.10 minutes\n",
      "epoch-43  lr=['0.0050000'], tr/val_loss:  1.334169/ 35.660137, val:  54.58%, val_best:  58.75%, tr:  61.18%, tr_best:  64.56%, epoch time: 67.73 seconds, 1.13 minutes\n",
      "epoch-44  lr=['0.0050000'], tr/val_loss:  1.297968/ 32.464752, val:  51.25%, val_best:  58.75%, tr:  62.21%, tr_best:  64.56%, epoch time: 64.37 seconds, 1.07 minutes\n",
      "epoch-45  lr=['0.0050000'], tr/val_loss:  1.352834/ 37.392578, val:  59.17%, val_best:  59.17%, tr:  62.10%, tr_best:  64.56%, epoch time: 66.22 seconds, 1.10 minutes\n",
      "epoch-46  lr=['0.0050000'], tr/val_loss:  1.315320/ 52.417358, val:  51.67%, val_best:  59.17%, tr:  61.59%, tr_best:  64.56%, epoch time: 63.95 seconds, 1.07 minutes\n",
      "epoch-47  lr=['0.0050000'], tr/val_loss:  1.212281/ 25.486504, val:  57.50%, val_best:  59.17%, tr:  64.25%, tr_best:  64.56%, epoch time: 65.92 seconds, 1.10 minutes\n",
      "epoch-48  lr=['0.0050000'], tr/val_loss:  1.216808/ 31.538794, val:  54.58%, val_best:  59.17%, tr:  63.13%, tr_best:  64.56%, epoch time: 64.14 seconds, 1.07 minutes\n",
      "epoch-49  lr=['0.0050000'], tr/val_loss:  1.264188/ 31.454033, val:  58.75%, val_best:  59.17%, tr:  62.21%, tr_best:  64.56%, epoch time: 66.55 seconds, 1.11 minutes\n",
      "epoch-50  lr=['0.0050000'], tr/val_loss:  1.295670/ 48.920452, val:  54.58%, val_best:  59.17%, tr:  63.74%, tr_best:  64.56%, epoch time: 65.01 seconds, 1.08 minutes\n",
      "epoch-51  lr=['0.0050000'], tr/val_loss:  1.157111/ 40.355259, val:  55.83%, val_best:  59.17%, tr:  64.76%, tr_best:  64.76%, epoch time: 66.81 seconds, 1.11 minutes\n",
      "epoch-52  lr=['0.0050000'], tr/val_loss:  1.267447/ 36.607059, val:  54.58%, val_best:  59.17%, tr:  64.04%, tr_best:  64.76%, epoch time: 64.12 seconds, 1.07 minutes\n",
      "epoch-53  lr=['0.0050000'], tr/val_loss:  1.181932/ 39.951981, val:  57.08%, val_best:  59.17%, tr:  64.76%, tr_best:  64.76%, epoch time: 65.87 seconds, 1.10 minutes\n",
      "epoch-54  lr=['0.0050000'], tr/val_loss:  1.262858/ 36.436192, val:  52.50%, val_best:  59.17%, tr:  64.76%, tr_best:  64.76%, epoch time: 64.12 seconds, 1.07 minutes\n",
      "epoch-55  lr=['0.0050000'], tr/val_loss:  1.227876/ 42.289841, val:  53.75%, val_best:  59.17%, tr:  64.04%, tr_best:  64.76%, epoch time: 66.29 seconds, 1.10 minutes\n",
      "epoch-56  lr=['0.0050000'], tr/val_loss:  1.273387/ 29.579905, val:  57.92%, val_best:  59.17%, tr:  62.82%, tr_best:  64.76%, epoch time: 64.86 seconds, 1.08 minutes\n",
      "epoch-57  lr=['0.0050000'], tr/val_loss:  1.225106/ 46.566452, val:  50.83%, val_best:  59.17%, tr:  65.07%, tr_best:  65.07%, epoch time: 65.91 seconds, 1.10 minutes\n",
      "epoch-58  lr=['0.0050000'], tr/val_loss:  1.207667/ 30.082861, val:  52.92%, val_best:  59.17%, tr:  64.66%, tr_best:  65.07%, epoch time: 66.44 seconds, 1.11 minutes\n",
      "epoch-59  lr=['0.0050000'], tr/val_loss:  1.247463/ 46.478176, val:  52.92%, val_best:  59.17%, tr:  65.68%, tr_best:  65.68%, epoch time: 65.12 seconds, 1.09 minutes\n",
      "epoch-60  lr=['0.0050000'], tr/val_loss:  1.209678/ 61.806271, val:  46.25%, val_best:  59.17%, tr:  65.07%, tr_best:  65.68%, epoch time: 64.67 seconds, 1.08 minutes\n",
      "epoch-61  lr=['0.0050000'], tr/val_loss:  1.323344/ 40.012093, val:  53.75%, val_best:  59.17%, tr:  62.51%, tr_best:  65.68%, epoch time: 65.36 seconds, 1.09 minutes\n",
      "epoch-62  lr=['0.0050000'], tr/val_loss:  1.223153/ 44.143299, val:  56.67%, val_best:  59.17%, tr:  63.84%, tr_best:  65.68%, epoch time: 64.19 seconds, 1.07 minutes\n",
      "epoch-63  lr=['0.0050000'], tr/val_loss:  1.169462/ 44.721066, val:  57.50%, val_best:  59.17%, tr:  65.99%, tr_best:  65.99%, epoch time: 67.50 seconds, 1.12 minutes\n",
      "epoch-64  lr=['0.0050000'], tr/val_loss:  1.183450/ 52.934811, val:  52.08%, val_best:  59.17%, tr:  65.88%, tr_best:  65.99%, epoch time: 64.00 seconds, 1.07 minutes\n",
      "epoch-65  lr=['0.0050000'], tr/val_loss:  1.238765/ 37.461712, val:  56.25%, val_best:  59.17%, tr:  66.70%, tr_best:  66.70%, epoch time: 65.98 seconds, 1.10 minutes\n",
      "epoch-66  lr=['0.0050000'], tr/val_loss:  1.207367/ 32.552849, val:  56.67%, val_best:  59.17%, tr:  65.37%, tr_best:  66.70%, epoch time: 65.18 seconds, 1.09 minutes\n",
      "epoch-67  lr=['0.0050000'], tr/val_loss:  1.195309/ 66.005936, val:  47.92%, val_best:  59.17%, tr:  64.76%, tr_best:  66.70%, epoch time: 65.60 seconds, 1.09 minutes\n",
      "epoch-68  lr=['0.0050000'], tr/val_loss:  1.225048/ 41.135307, val:  57.50%, val_best:  59.17%, tr:  67.11%, tr_best:  67.11%, epoch time: 64.72 seconds, 1.08 minutes\n",
      "epoch-69  lr=['0.0050000'], tr/val_loss:  1.280606/ 55.784050, val:  52.08%, val_best:  59.17%, tr:  65.47%, tr_best:  67.11%, epoch time: 64.65 seconds, 1.08 minutes\n",
      "epoch-70  lr=['0.0050000'], tr/val_loss:  1.306789/ 43.028709, val:  53.75%, val_best:  59.17%, tr:  64.96%, tr_best:  67.11%, epoch time: 65.41 seconds, 1.09 minutes\n",
      "epoch-71  lr=['0.0050000'], tr/val_loss:  1.275363/ 66.014023, val:  42.50%, val_best:  59.17%, tr:  63.94%, tr_best:  67.11%, epoch time: 64.94 seconds, 1.08 minutes\n",
      "epoch-72  lr=['0.0050000'], tr/val_loss:  1.225921/ 36.127106, val:  57.50%, val_best:  59.17%, tr:  64.96%, tr_best:  67.11%, epoch time: 65.00 seconds, 1.08 minutes\n",
      "epoch-73  lr=['0.0050000'], tr/val_loss:  1.208396/ 38.844116, val:  52.92%, val_best:  59.17%, tr:  64.35%, tr_best:  67.11%, epoch time: 65.28 seconds, 1.09 minutes\n",
      "epoch-74  lr=['0.0050000'], tr/val_loss:  1.209000/ 34.468155, val:  57.92%, val_best:  59.17%, tr:  63.13%, tr_best:  67.11%, epoch time: 65.53 seconds, 1.09 minutes\n",
      "epoch-75  lr=['0.0050000'], tr/val_loss:  1.195065/ 30.378914, val:  54.58%, val_best:  59.17%, tr:  65.47%, tr_best:  67.11%, epoch time: 65.97 seconds, 1.10 minutes\n",
      "epoch-76  lr=['0.0050000'], tr/val_loss:  1.289745/ 32.548168, val:  58.75%, val_best:  59.17%, tr:  62.82%, tr_best:  67.11%, epoch time: 65.22 seconds, 1.09 minutes\n",
      "epoch-77  lr=['0.0050000'], tr/val_loss:  1.133368/ 57.691925, val:  51.25%, val_best:  59.17%, tr:  66.29%, tr_best:  67.11%, epoch time: 66.72 seconds, 1.11 minutes\n",
      "epoch-78  lr=['0.0050000'], tr/val_loss:  1.128187/ 38.375175, val:  54.17%, val_best:  59.17%, tr:  68.34%, tr_best:  68.34%, epoch time: 67.27 seconds, 1.12 minutes\n",
      "epoch-79  lr=['0.0050000'], tr/val_loss:  1.126934/ 45.747345, val:  56.25%, val_best:  59.17%, tr:  66.39%, tr_best:  68.34%, epoch time: 66.57 seconds, 1.11 minutes\n",
      "epoch-80  lr=['0.0050000'], tr/val_loss:  1.169243/ 26.233459, val:  64.58%, val_best:  64.58%, tr:  66.29%, tr_best:  68.34%, epoch time: 64.39 seconds, 1.07 minutes\n",
      "epoch-81  lr=['0.0050000'], tr/val_loss:  1.166263/ 50.634815, val:  49.17%, val_best:  64.58%, tr:  65.99%, tr_best:  68.34%, epoch time: 64.54 seconds, 1.08 minutes\n",
      "epoch-82  lr=['0.0050000'], tr/val_loss:  1.073598/ 38.162148, val:  54.17%, val_best:  64.58%, tr:  68.13%, tr_best:  68.34%, epoch time: 68.74 seconds, 1.15 minutes\n",
      "epoch-83  lr=['0.0050000'], tr/val_loss:  1.246867/ 52.975452, val:  48.75%, val_best:  64.58%, tr:  65.37%, tr_best:  68.34%, epoch time: 66.30 seconds, 1.10 minutes\n",
      "epoch-84  lr=['0.0050000'], tr/val_loss:  1.117674/ 35.351570, val:  55.00%, val_best:  64.58%, tr:  66.80%, tr_best:  68.34%, epoch time: 65.62 seconds, 1.09 minutes\n",
      "epoch-85  lr=['0.0050000'], tr/val_loss:  1.052386/ 51.261196, val:  47.08%, val_best:  64.58%, tr:  67.52%, tr_best:  68.34%, epoch time: 66.45 seconds, 1.11 minutes\n",
      "epoch-86  lr=['0.0050000'], tr/val_loss:  1.151014/ 39.200180, val:  55.83%, val_best:  64.58%, tr:  67.82%, tr_best:  68.34%, epoch time: 65.91 seconds, 1.10 minutes\n",
      "epoch-87  lr=['0.0050000'], tr/val_loss:  1.118658/ 40.135090, val:  57.08%, val_best:  64.58%, tr:  67.72%, tr_best:  68.34%, epoch time: 66.38 seconds, 1.11 minutes\n",
      "epoch-88  lr=['0.0050000'], tr/val_loss:  1.179611/ 54.516483, val:  55.00%, val_best:  64.58%, tr:  66.80%, tr_best:  68.34%, epoch time: 65.51 seconds, 1.09 minutes\n",
      "epoch-89  lr=['0.0050000'], tr/val_loss:  1.104547/ 26.296923, val:  56.67%, val_best:  64.58%, tr:  68.44%, tr_best:  68.44%, epoch time: 66.41 seconds, 1.11 minutes\n",
      "epoch-90  lr=['0.0050000'], tr/val_loss:  1.216716/ 40.211964, val:  57.08%, val_best:  64.58%, tr:  64.45%, tr_best:  68.44%, epoch time: 63.97 seconds, 1.07 minutes\n",
      "epoch-91  lr=['0.0050000'], tr/val_loss:  1.059065/ 31.771090, val:  60.00%, val_best:  64.58%, tr:  70.17%, tr_best:  70.17%, epoch time: 65.86 seconds, 1.10 minutes\n",
      "epoch-92  lr=['0.0050000'], tr/val_loss:  1.123869/ 32.193645, val:  53.33%, val_best:  64.58%, tr:  68.13%, tr_best:  70.17%, epoch time: 65.58 seconds, 1.09 minutes\n",
      "epoch-93  lr=['0.0050000'], tr/val_loss:  1.035060/ 40.701927, val:  51.25%, val_best:  64.58%, tr:  69.46%, tr_best:  70.17%, epoch time: 65.45 seconds, 1.09 minutes\n",
      "epoch-94  lr=['0.0050000'], tr/val_loss:  1.107160/ 33.111488, val:  53.75%, val_best:  64.58%, tr:  66.09%, tr_best:  70.17%, epoch time: 64.56 seconds, 1.08 minutes\n",
      "epoch-95  lr=['0.0050000'], tr/val_loss:  1.109923/ 34.875938, val:  58.33%, val_best:  64.58%, tr:  69.25%, tr_best:  70.17%, epoch time: 69.53 seconds, 1.16 minutes\n",
      "epoch-96  lr=['0.0050000'], tr/val_loss:  1.024209/ 55.606152, val:  50.42%, val_best:  64.58%, tr:  69.46%, tr_best:  70.17%, epoch time: 66.78 seconds, 1.11 minutes\n",
      "epoch-97  lr=['0.0050000'], tr/val_loss:  0.975249/ 35.278526, val:  60.42%, val_best:  64.58%, tr:  70.07%, tr_best:  70.17%, epoch time: 67.13 seconds, 1.12 minutes\n",
      "epoch-98  lr=['0.0050000'], tr/val_loss:  1.036915/ 40.289989, val:  51.67%, val_best:  64.58%, tr:  68.74%, tr_best:  70.17%, epoch time: 64.89 seconds, 1.08 minutes\n",
      "epoch-99  lr=['0.0050000'], tr/val_loss:  1.013361/ 39.769283, val:  58.33%, val_best:  64.58%, tr:  70.48%, tr_best:  70.48%, epoch time: 65.96 seconds, 1.10 minutes\n",
      "epoch-100 lr=['0.0050000'], tr/val_loss:  1.041863/ 36.622448, val:  54.17%, val_best:  64.58%, tr:  68.54%, tr_best:  70.48%, epoch time: 65.30 seconds, 1.09 minutes\n",
      "epoch-101 lr=['0.0050000'], tr/val_loss:  1.038125/ 59.721420, val:  52.50%, val_best:  64.58%, tr:  69.87%, tr_best:  70.48%, epoch time: 66.36 seconds, 1.11 minutes\n",
      "epoch-102 lr=['0.0050000'], tr/val_loss:  1.074933/ 54.646187, val:  49.58%, val_best:  64.58%, tr:  68.44%, tr_best:  70.48%, epoch time: 65.67 seconds, 1.09 minutes\n",
      "epoch-103 lr=['0.0050000'], tr/val_loss:  1.070272/ 31.686726, val:  62.92%, val_best:  64.58%, tr:  69.46%, tr_best:  70.48%, epoch time: 67.23 seconds, 1.12 minutes\n",
      "epoch-104 lr=['0.0050000'], tr/val_loss:  1.143202/ 29.635687, val:  64.17%, val_best:  64.58%, tr:  68.34%, tr_best:  70.48%, epoch time: 66.61 seconds, 1.11 minutes\n",
      "epoch-105 lr=['0.0050000'], tr/val_loss:  1.197083/ 30.487104, val:  53.33%, val_best:  64.58%, tr:  65.07%, tr_best:  70.48%, epoch time: 67.24 seconds, 1.12 minutes\n",
      "epoch-106 lr=['0.0050000'], tr/val_loss:  1.140930/ 44.865669, val:  57.92%, val_best:  64.58%, tr:  67.52%, tr_best:  70.48%, epoch time: 65.23 seconds, 1.09 minutes\n",
      "epoch-107 lr=['0.0050000'], tr/val_loss:  1.219040/ 40.773975, val:  53.75%, val_best:  64.58%, tr:  66.19%, tr_best:  70.48%, epoch time: 65.74 seconds, 1.10 minutes\n",
      "epoch-108 lr=['0.0050000'], tr/val_loss:  1.142836/ 39.141521, val:  63.75%, val_best:  64.58%, tr:  69.25%, tr_best:  70.48%, epoch time: 67.43 seconds, 1.12 minutes\n",
      "epoch-109 lr=['0.0050000'], tr/val_loss:  1.102106/ 33.993656, val:  56.67%, val_best:  64.58%, tr:  69.25%, tr_best:  70.48%, epoch time: 65.00 seconds, 1.08 minutes\n",
      "epoch-110 lr=['0.0050000'], tr/val_loss:  1.106927/ 40.523556, val:  58.75%, val_best:  64.58%, tr:  68.54%, tr_best:  70.48%, epoch time: 65.94 seconds, 1.10 minutes\n",
      "epoch-111 lr=['0.0050000'], tr/val_loss:  1.047473/ 61.928844, val:  53.33%, val_best:  64.58%, tr:  70.07%, tr_best:  70.48%, epoch time: 66.93 seconds, 1.12 minutes\n",
      "epoch-112 lr=['0.0050000'], tr/val_loss:  1.077819/ 49.586353, val:  51.67%, val_best:  64.58%, tr:  69.46%, tr_best:  70.48%, epoch time: 66.19 seconds, 1.10 minutes\n",
      "epoch-113 lr=['0.0050000'], tr/val_loss:  1.107993/ 38.540154, val:  55.00%, val_best:  64.58%, tr:  69.15%, tr_best:  70.48%, epoch time: 64.86 seconds, 1.08 minutes\n",
      "epoch-114 lr=['0.0050000'], tr/val_loss:  1.073712/ 48.585556, val:  57.08%, val_best:  64.58%, tr:  69.87%, tr_best:  70.48%, epoch time: 67.43 seconds, 1.12 minutes\n",
      "epoch-115 lr=['0.0050000'], tr/val_loss:  1.146466/ 48.823826, val:  59.58%, val_best:  64.58%, tr:  70.07%, tr_best:  70.48%, epoch time: 66.41 seconds, 1.11 minutes\n",
      "epoch-116 lr=['0.0050000'], tr/val_loss:  1.175620/ 47.368942, val:  55.83%, val_best:  64.58%, tr:  68.54%, tr_best:  70.48%, epoch time: 65.30 seconds, 1.09 minutes\n",
      "epoch-117 lr=['0.0050000'], tr/val_loss:  1.204113/ 35.595787, val:  59.58%, val_best:  64.58%, tr:  67.82%, tr_best:  70.48%, epoch time: 66.02 seconds, 1.10 minutes\n",
      "epoch-118 lr=['0.0050000'], tr/val_loss:  1.145689/ 54.273552, val:  59.17%, val_best:  64.58%, tr:  69.25%, tr_best:  70.48%, epoch time: 65.92 seconds, 1.10 minutes\n",
      "epoch-119 lr=['0.0050000'], tr/val_loss:  1.257703/ 47.361519, val:  52.92%, val_best:  64.58%, tr:  68.13%, tr_best:  70.48%, epoch time: 64.90 seconds, 1.08 minutes\n",
      "epoch-120 lr=['0.0050000'], tr/val_loss:  1.063829/ 37.909950, val:  60.42%, val_best:  64.58%, tr:  72.01%, tr_best:  72.01%, epoch time: 65.29 seconds, 1.09 minutes\n",
      "epoch-121 lr=['0.0050000'], tr/val_loss:  1.091188/ 66.427658, val:  54.58%, val_best:  64.58%, tr:  70.07%, tr_best:  72.01%, epoch time: 65.99 seconds, 1.10 minutes\n",
      "epoch-122 lr=['0.0050000'], tr/val_loss:  1.191301/ 26.882059, val:  68.33%, val_best:  68.33%, tr:  67.21%, tr_best:  72.01%, epoch time: 65.75 seconds, 1.10 minutes\n",
      "epoch-123 lr=['0.0050000'], tr/val_loss:  1.171540/ 44.347061, val:  62.08%, val_best:  68.33%, tr:  69.15%, tr_best:  72.01%, epoch time: 65.90 seconds, 1.10 minutes\n",
      "epoch-124 lr=['0.0050000'], tr/val_loss:  1.159921/ 35.058140, val:  61.67%, val_best:  68.33%, tr:  69.77%, tr_best:  72.01%, epoch time: 65.42 seconds, 1.09 minutes\n",
      "epoch-125 lr=['0.0050000'], tr/val_loss:  1.206786/ 47.270088, val:  58.75%, val_best:  68.33%, tr:  68.13%, tr_best:  72.01%, epoch time: 66.63 seconds, 1.11 minutes\n",
      "epoch-126 lr=['0.0050000'], tr/val_loss:  1.083390/ 37.694241, val:  58.33%, val_best:  68.33%, tr:  70.99%, tr_best:  72.01%, epoch time: 65.17 seconds, 1.09 minutes\n",
      "epoch-127 lr=['0.0050000'], tr/val_loss:  1.129413/ 46.249424, val:  59.17%, val_best:  68.33%, tr:  68.23%, tr_best:  72.01%, epoch time: 67.06 seconds, 1.12 minutes\n",
      "epoch-128 lr=['0.0050000'], tr/val_loss:  1.106435/ 48.432907, val:  56.25%, val_best:  68.33%, tr:  69.56%, tr_best:  72.01%, epoch time: 66.62 seconds, 1.11 minutes\n",
      "epoch-129 lr=['0.0050000'], tr/val_loss:  1.011394/ 42.689678, val:  54.58%, val_best:  68.33%, tr:  72.63%, tr_best:  72.63%, epoch time: 67.46 seconds, 1.12 minutes\n",
      "epoch-130 lr=['0.0050000'], tr/val_loss:  1.107086/ 77.141220, val:  53.75%, val_best:  68.33%, tr:  69.25%, tr_best:  72.63%, epoch time: 65.09 seconds, 1.08 minutes\n",
      "epoch-131 lr=['0.0050000'], tr/val_loss:  1.122620/ 46.292770, val:  60.42%, val_best:  68.33%, tr:  70.28%, tr_best:  72.63%, epoch time: 67.11 seconds, 1.12 minutes\n",
      "epoch-132 lr=['0.0050000'], tr/val_loss:  1.046705/ 52.460461, val:  55.42%, val_best:  68.33%, tr:  73.03%, tr_best:  73.03%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "epoch-133 lr=['0.0050000'], tr/val_loss:  1.127105/ 32.767254, val:  67.08%, val_best:  68.33%, tr:  69.36%, tr_best:  73.03%, epoch time: 65.60 seconds, 1.09 minutes\n",
      "epoch-134 lr=['0.0050000'], tr/val_loss:  1.079796/ 31.414862, val:  59.58%, val_best:  68.33%, tr:  69.56%, tr_best:  73.03%, epoch time: 66.09 seconds, 1.10 minutes\n",
      "epoch-135 lr=['0.0050000'], tr/val_loss:  1.001485/ 50.128471, val:  58.33%, val_best:  68.33%, tr:  72.01%, tr_best:  73.03%, epoch time: 66.18 seconds, 1.10 minutes\n",
      "epoch-136 lr=['0.0050000'], tr/val_loss:  1.117327/ 45.945812, val:  60.42%, val_best:  68.33%, tr:  69.77%, tr_best:  73.03%, epoch time: 67.34 seconds, 1.12 minutes\n",
      "epoch-137 lr=['0.0050000'], tr/val_loss:  1.036057/ 39.878632, val:  61.67%, val_best:  68.33%, tr:  71.81%, tr_best:  73.03%, epoch time: 65.93 seconds, 1.10 minutes\n",
      "epoch-138 lr=['0.0050000'], tr/val_loss:  1.029615/ 46.129124, val:  61.25%, val_best:  68.33%, tr:  70.89%, tr_best:  73.03%, epoch time: 65.65 seconds, 1.09 minutes\n",
      "epoch-139 lr=['0.0050000'], tr/val_loss:  1.084948/ 36.400803, val:  55.42%, val_best:  68.33%, tr:  69.77%, tr_best:  73.03%, epoch time: 66.58 seconds, 1.11 minutes\n",
      "epoch-140 lr=['0.0050000'], tr/val_loss:  1.093955/ 55.626244, val:  54.17%, val_best:  68.33%, tr:  69.56%, tr_best:  73.03%, epoch time: 66.02 seconds, 1.10 minutes\n",
      "epoch-141 lr=['0.0050000'], tr/val_loss:  1.086757/ 53.033806, val:  53.33%, val_best:  68.33%, tr:  69.66%, tr_best:  73.03%, epoch time: 64.60 seconds, 1.08 minutes\n",
      "epoch-142 lr=['0.0050000'], tr/val_loss:  1.094368/ 32.179008, val:  66.25%, val_best:  68.33%, tr:  68.54%, tr_best:  73.03%, epoch time: 66.32 seconds, 1.11 minutes\n",
      "epoch-143 lr=['0.0050000'], tr/val_loss:  1.038914/ 43.209183, val:  56.25%, val_best:  68.33%, tr:  71.91%, tr_best:  73.03%, epoch time: 65.31 seconds, 1.09 minutes\n",
      "epoch-144 lr=['0.0050000'], tr/val_loss:  1.057719/ 49.174976, val:  56.25%, val_best:  68.33%, tr:  70.99%, tr_best:  73.03%, epoch time: 66.05 seconds, 1.10 minutes\n",
      "epoch-145 lr=['0.0050000'], tr/val_loss:  1.125382/ 27.279993, val:  65.00%, val_best:  68.33%, tr:  69.56%, tr_best:  73.03%, epoch time: 65.46 seconds, 1.09 minutes\n",
      "epoch-146 lr=['0.0050000'], tr/val_loss:  1.153401/ 41.154938, val:  57.08%, val_best:  68.33%, tr:  68.64%, tr_best:  73.03%, epoch time: 66.21 seconds, 1.10 minutes\n",
      "epoch-147 lr=['0.0050000'], tr/val_loss:  1.009773/ 46.556049, val:  58.75%, val_best:  68.33%, tr:  73.65%, tr_best:  73.65%, epoch time: 64.47 seconds, 1.07 minutes\n",
      "epoch-148 lr=['0.0050000'], tr/val_loss:  1.037655/ 31.672539, val:  65.00%, val_best:  68.33%, tr:  70.89%, tr_best:  73.65%, epoch time: 64.66 seconds, 1.08 minutes\n",
      "epoch-149 lr=['0.0050000'], tr/val_loss:  1.080388/ 44.492722, val:  60.00%, val_best:  68.33%, tr:  69.87%, tr_best:  73.65%, epoch time: 66.78 seconds, 1.11 minutes\n",
      "epoch-150 lr=['0.0050000'], tr/val_loss:  0.961002/ 43.082981, val:  57.08%, val_best:  68.33%, tr:  72.32%, tr_best:  73.65%, epoch time: 64.58 seconds, 1.08 minutes\n",
      "epoch-151 lr=['0.0050000'], tr/val_loss:  1.075260/ 40.910515, val:  57.50%, val_best:  68.33%, tr:  69.77%, tr_best:  73.65%, epoch time: 66.22 seconds, 1.10 minutes\n",
      "epoch-152 lr=['0.0050000'], tr/val_loss:  0.956094/ 62.780533, val:  54.17%, val_best:  68.33%, tr:  73.03%, tr_best:  73.65%, epoch time: 64.81 seconds, 1.08 minutes\n",
      "epoch-153 lr=['0.0050000'], tr/val_loss:  1.024256/ 45.969616, val:  63.33%, val_best:  68.33%, tr:  70.28%, tr_best:  73.65%, epoch time: 65.53 seconds, 1.09 minutes\n",
      "epoch-154 lr=['0.0050000'], tr/val_loss:  0.898584/ 56.768982, val:  58.75%, val_best:  68.33%, tr:  74.36%, tr_best:  74.36%, epoch time: 65.83 seconds, 1.10 minutes\n",
      "epoch-155 lr=['0.0050000'], tr/val_loss:  0.948257/ 48.800186, val:  56.25%, val_best:  68.33%, tr:  72.93%, tr_best:  74.36%, epoch time: 66.66 seconds, 1.11 minutes\n",
      "epoch-156 lr=['0.0050000'], tr/val_loss:  0.945377/ 36.740952, val:  63.33%, val_best:  68.33%, tr:  73.44%, tr_best:  74.36%, epoch time: 65.56 seconds, 1.09 minutes\n",
      "epoch-157 lr=['0.0050000'], tr/val_loss:  1.040349/ 31.124590, val:  60.42%, val_best:  68.33%, tr:  71.60%, tr_best:  74.36%, epoch time: 65.29 seconds, 1.09 minutes\n",
      "epoch-158 lr=['0.0050000'], tr/val_loss:  1.045519/ 28.979189, val:  62.92%, val_best:  68.33%, tr:  71.20%, tr_best:  74.36%, epoch time: 64.89 seconds, 1.08 minutes\n",
      "epoch-159 lr=['0.0050000'], tr/val_loss:  0.965169/ 38.839062, val:  57.50%, val_best:  68.33%, tr:  71.81%, tr_best:  74.36%, epoch time: 64.91 seconds, 1.08 minutes\n",
      "epoch-160 lr=['0.0050000'], tr/val_loss:  1.045405/ 58.070320, val:  58.33%, val_best:  68.33%, tr:  72.11%, tr_best:  74.36%, epoch time: 66.20 seconds, 1.10 minutes\n",
      "epoch-161 lr=['0.0050000'], tr/val_loss:  1.065570/ 47.403404, val:  54.17%, val_best:  68.33%, tr:  70.89%, tr_best:  74.36%, epoch time: 64.50 seconds, 1.08 minutes\n",
      "epoch-162 lr=['0.0050000'], tr/val_loss:  1.102968/ 32.719288, val:  56.67%, val_best:  68.33%, tr:  69.97%, tr_best:  74.36%, epoch time: 65.22 seconds, 1.09 minutes\n",
      "epoch-163 lr=['0.0050000'], tr/val_loss:  1.025974/ 51.280674, val:  54.58%, val_best:  68.33%, tr:  71.40%, tr_best:  74.36%, epoch time: 66.29 seconds, 1.10 minutes\n",
      "epoch-164 lr=['0.0050000'], tr/val_loss:  0.982640/ 45.621933, val:  62.50%, val_best:  68.33%, tr:  72.93%, tr_best:  74.36%, epoch time: 65.11 seconds, 1.09 minutes\n",
      "epoch-165 lr=['0.0050000'], tr/val_loss:  1.009162/ 37.495308, val:  65.42%, val_best:  68.33%, tr:  71.30%, tr_best:  74.36%, epoch time: 66.65 seconds, 1.11 minutes\n",
      "epoch-166 lr=['0.0050000'], tr/val_loss:  0.889579/ 49.056137, val:  57.50%, val_best:  68.33%, tr:  74.06%, tr_best:  74.36%, epoch time: 64.99 seconds, 1.08 minutes\n",
      "epoch-167 lr=['0.0050000'], tr/val_loss:  0.995047/ 62.517368, val:  57.08%, val_best:  68.33%, tr:  70.99%, tr_best:  74.36%, epoch time: 65.37 seconds, 1.09 minutes\n",
      "epoch-168 lr=['0.0050000'], tr/val_loss:  0.997791/ 48.151321, val:  59.17%, val_best:  68.33%, tr:  72.73%, tr_best:  74.36%, epoch time: 65.35 seconds, 1.09 minutes\n",
      "epoch-169 lr=['0.0050000'], tr/val_loss:  1.014284/ 37.948315, val:  60.83%, val_best:  68.33%, tr:  72.01%, tr_best:  74.36%, epoch time: 65.22 seconds, 1.09 minutes\n",
      "epoch-170 lr=['0.0050000'], tr/val_loss:  0.985215/ 62.603653, val:  57.92%, val_best:  68.33%, tr:  73.95%, tr_best:  74.36%, epoch time: 66.23 seconds, 1.10 minutes\n",
      "epoch-171 lr=['0.0050000'], tr/val_loss:  0.924488/ 66.258492, val:  53.33%, val_best:  68.33%, tr:  73.34%, tr_best:  74.36%, epoch time: 67.64 seconds, 1.13 minutes\n",
      "epoch-172 lr=['0.0050000'], tr/val_loss:  1.120068/ 43.583794, val:  56.25%, val_best:  68.33%, tr:  69.77%, tr_best:  74.36%, epoch time: 65.68 seconds, 1.09 minutes\n",
      "epoch-173 lr=['0.0050000'], tr/val_loss:  1.024975/ 57.403034, val:  54.17%, val_best:  68.33%, tr:  72.63%, tr_best:  74.36%, epoch time: 64.45 seconds, 1.07 minutes\n",
      "epoch-174 lr=['0.0050000'], tr/val_loss:  0.908366/ 48.528122, val:  58.33%, val_best:  68.33%, tr:  73.85%, tr_best:  74.36%, epoch time: 65.48 seconds, 1.09 minutes\n",
      "epoch-175 lr=['0.0050000'], tr/val_loss:  1.032867/ 46.819164, val:  59.58%, val_best:  68.33%, tr:  72.42%, tr_best:  74.36%, epoch time: 65.17 seconds, 1.09 minutes\n",
      "epoch-176 lr=['0.0050000'], tr/val_loss:  0.886937/ 44.927803, val:  57.92%, val_best:  68.33%, tr:  73.03%, tr_best:  74.36%, epoch time: 65.75 seconds, 1.10 minutes\n",
      "epoch-177 lr=['0.0050000'], tr/val_loss:  0.979628/ 62.426895, val:  57.08%, val_best:  68.33%, tr:  73.14%, tr_best:  74.36%, epoch time: 67.17 seconds, 1.12 minutes\n",
      "epoch-178 lr=['0.0050000'], tr/val_loss:  0.989670/ 35.846573, val:  63.33%, val_best:  68.33%, tr:  72.11%, tr_best:  74.36%, epoch time: 65.45 seconds, 1.09 minutes\n",
      "epoch-179 lr=['0.0050000'], tr/val_loss:  1.061375/ 41.384178, val:  63.33%, val_best:  68.33%, tr:  69.25%, tr_best:  74.36%, epoch time: 66.22 seconds, 1.10 minutes\n",
      "epoch-180 lr=['0.0050000'], tr/val_loss:  0.932998/ 41.995907, val:  60.42%, val_best:  68.33%, tr:  72.32%, tr_best:  74.36%, epoch time: 64.93 seconds, 1.08 minutes\n",
      "epoch-181 lr=['0.0050000'], tr/val_loss:  0.913190/ 31.499554, val:  62.92%, val_best:  68.33%, tr:  75.69%, tr_best:  75.69%, epoch time: 66.11 seconds, 1.10 minutes\n",
      "epoch-182 lr=['0.0050000'], tr/val_loss:  0.895922/ 37.378956, val:  58.33%, val_best:  68.33%, tr:  73.03%, tr_best:  75.69%, epoch time: 66.87 seconds, 1.11 minutes\n",
      "epoch-183 lr=['0.0050000'], tr/val_loss:  0.950778/ 50.207237, val:  55.83%, val_best:  68.33%, tr:  72.73%, tr_best:  75.69%, epoch time: 64.25 seconds, 1.07 minutes\n",
      "epoch-184 lr=['0.0050000'], tr/val_loss:  0.888153/ 38.762348, val:  61.25%, val_best:  68.33%, tr:  73.95%, tr_best:  75.69%, epoch time: 65.86 seconds, 1.10 minutes\n",
      "epoch-185 lr=['0.0050000'], tr/val_loss:  0.895576/ 32.840317, val:  68.75%, val_best:  68.75%, tr:  72.73%, tr_best:  75.69%, epoch time: 66.20 seconds, 1.10 minutes\n",
      "epoch-186 lr=['0.0050000'], tr/val_loss:  0.956800/ 37.484154, val:  61.25%, val_best:  68.75%, tr:  73.44%, tr_best:  75.69%, epoch time: 65.97 seconds, 1.10 minutes\n",
      "epoch-187 lr=['0.0050000'], tr/val_loss:  0.967661/ 43.354176, val:  55.42%, val_best:  68.75%, tr:  70.89%, tr_best:  75.69%, epoch time: 65.18 seconds, 1.09 minutes\n",
      "epoch-188 lr=['0.0050000'], tr/val_loss:  1.049163/ 51.222725, val:  57.08%, val_best:  68.75%, tr:  70.58%, tr_best:  75.69%, epoch time: 65.92 seconds, 1.10 minutes\n",
      "epoch-189 lr=['0.0050000'], tr/val_loss:  1.013984/ 32.205719, val:  69.58%, val_best:  69.58%, tr:  70.28%, tr_best:  75.69%, epoch time: 66.34 seconds, 1.11 minutes\n",
      "epoch-190 lr=['0.0050000'], tr/val_loss:  0.909877/ 51.477901, val:  56.67%, val_best:  69.58%, tr:  74.87%, tr_best:  75.69%, epoch time: 64.96 seconds, 1.08 minutes\n",
      "epoch-191 lr=['0.0050000'], tr/val_loss:  0.930072/ 70.955978, val:  49.17%, val_best:  69.58%, tr:  73.75%, tr_best:  75.69%, epoch time: 65.88 seconds, 1.10 minutes\n",
      "epoch-192 lr=['0.0050000'], tr/val_loss:  0.973571/ 34.371635, val:  61.67%, val_best:  69.58%, tr:  73.44%, tr_best:  75.69%, epoch time: 66.35 seconds, 1.11 minutes\n",
      "epoch-193 lr=['0.0050000'], tr/val_loss:  0.981448/ 68.421135, val:  51.67%, val_best:  69.58%, tr:  73.34%, tr_best:  75.69%, epoch time: 65.87 seconds, 1.10 minutes\n",
      "epoch-194 lr=['0.0050000'], tr/val_loss:  0.980765/ 44.152840, val:  65.00%, val_best:  69.58%, tr:  74.46%, tr_best:  75.69%, epoch time: 67.04 seconds, 1.12 minutes\n",
      "epoch-195 lr=['0.0050000'], tr/val_loss:  0.941483/ 70.312164, val:  54.17%, val_best:  69.58%, tr:  73.03%, tr_best:  75.69%, epoch time: 65.97 seconds, 1.10 minutes\n",
      "epoch-196 lr=['0.0050000'], tr/val_loss:  0.941404/ 58.358913, val:  53.75%, val_best:  69.58%, tr:  73.95%, tr_best:  75.69%, epoch time: 66.24 seconds, 1.10 minutes\n",
      "epoch-197 lr=['0.0050000'], tr/val_loss:  1.026268/ 41.063374, val:  60.00%, val_best:  69.58%, tr:  70.99%, tr_best:  75.69%, epoch time: 66.47 seconds, 1.11 minutes\n",
      "epoch-198 lr=['0.0050000'], tr/val_loss:  1.000441/ 48.572071, val:  56.25%, val_best:  69.58%, tr:  72.63%, tr_best:  75.69%, epoch time: 65.18 seconds, 1.09 minutes\n",
      "epoch-199 lr=['0.0050000'], tr/val_loss:  0.904671/ 53.454334, val:  61.25%, val_best:  69.58%, tr:  72.73%, tr_best:  75.69%, epoch time: 65.62 seconds, 1.09 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72c81443e5b4993b1499707a9c29da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÜ‚ñÉ‚ñá‚ñÇ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÉ‚ñà‚ñÖ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.72727</td></tr><tr><td>tr_epoch_loss</td><td>0.90467</td></tr><tr><td>val_acc_best</td><td>0.69583</td></tr><tr><td>val_acc_now</td><td>0.6125</td></tr><tr><td>val_loss</td><td>53.45433</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dazzling-sweep-300</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tkuubns9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tkuubns9</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251111_090808-tkuubns9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xtm2ue6f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 5000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_threshold: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: one\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251111_124815-xtm2ue6f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xtm2ue6f' target=\"_blank\">ruby-sweep-309</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xtm2ue6f' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xtm2ue6f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_threshold' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': '20251111_124823_542', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'one', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'output_threshold': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 7a22c8a0ef5b9b252dbf98632e270efd\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=4, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=False, ANPI_MODE=True)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=4, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=False, ANPI_MODE=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient(single_step=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "inFeed spike.shape torch.Size([10, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([ 1.8934e-02,  2.1093e-01, -1.6790e-02,  8.2860e-02, -1.8780e-01,\n",
      "         6.3725e-02, -6.3379e-02, -7.9782e-02,  5.1524e-02, -1.2446e-01,\n",
      "        -1.6221e-01, -2.9600e-02, -9.0339e-03,  1.9444e-02, -1.0934e-01,\n",
      "         1.8129e-01, -6.9730e-02,  6.7152e-02,  7.7763e-02, -3.2597e-03,\n",
      "         1.4558e-01, -5.0406e-02, -2.4797e-02,  1.4391e-01, -3.1818e-02,\n",
      "        -1.1320e-01,  2.2984e-01, -6.7576e-02,  1.7931e-02, -1.1550e-01,\n",
      "        -1.7594e-01, -1.5427e-01,  8.1846e-02,  1.3850e-01,  6.3135e-02,\n",
      "         4.1502e-02, -1.5509e-01,  6.0735e-02,  1.6491e-01, -6.4878e-02,\n",
      "         9.1983e-02,  7.6438e-03,  8.2616e-03, -1.3744e-02,  3.2357e-02,\n",
      "        -5.7478e-02, -1.0464e-01,  9.3097e-03, -3.2663e-02, -5.1313e-02,\n",
      "        -8.5647e-02,  3.8434e-02,  1.6001e-02, -1.9292e-02,  9.8606e-02,\n",
      "        -1.3158e-01, -3.4134e-02, -6.2874e-02,  4.3602e-02, -5.2417e-02,\n",
      "         1.2124e-01, -7.9496e-02,  2.4412e-02, -4.1696e-02,  1.0778e-01,\n",
      "        -1.0762e-01,  5.4097e-02, -1.2537e-01, -3.7238e-02,  5.0156e-02,\n",
      "         9.7776e-03,  2.5239e-02,  3.5296e-02,  2.2238e-01,  2.2782e-03,\n",
      "         1.5446e-01, -1.1312e-01,  9.2554e-02, -4.4633e-02,  7.4222e-02,\n",
      "        -5.6474e-02, -6.8803e-02, -7.0595e-02, -4.9484e-02, -4.2925e-02,\n",
      "        -4.0809e-02,  1.6994e-02,  4.3201e-02,  4.9468e-02, -1.1875e-01,\n",
      "        -2.6532e-02,  2.6988e-02, -1.4051e-01, -6.3074e-02,  7.3065e-03,\n",
      "         1.8921e-02,  5.8165e-02,  2.2661e-02,  1.1140e-01, -6.6528e-02,\n",
      "        -1.6133e-01,  5.8902e-04,  1.3482e-01,  1.2398e-01,  2.2678e-03,\n",
      "        -1.2688e-01, -7.3284e-02,  3.6657e-02, -5.3425e-02, -3.8686e-03,\n",
      "        -7.5912e-02, -2.4416e-01,  6.8315e-02, -9.1515e-03, -2.1105e-02,\n",
      "         4.3759e-02, -3.0760e-02,  2.1116e-03,  6.1028e-02,  2.4064e-02,\n",
      "         7.3052e-02, -1.1411e-02, -9.9703e-03, -4.8900e-02, -4.9272e-02,\n",
      "        -1.1781e-01, -2.3789e-02, -6.6208e-02,  1.9253e-02,  9.5465e-02,\n",
      "        -2.7977e-03,  1.6420e-01,  1.0646e-01, -9.6819e-02, -6.5508e-02,\n",
      "         1.6782e-01,  2.4013e-01, -6.0490e-02,  1.2407e-01, -2.7312e-02,\n",
      "         4.2546e-02,  4.1576e-02,  1.0389e-01, -1.9791e-01, -6.1734e-02,\n",
      "         2.0598e-01, -9.2463e-03,  2.3019e-02, -7.1248e-02, -1.6451e-01,\n",
      "         8.8945e-02,  7.6954e-02, -6.1358e-02,  2.1075e-01,  1.1362e-01,\n",
      "        -4.1540e-02,  2.3355e-02, -1.2469e-01, -1.1774e-02, -5.9197e-02,\n",
      "        -7.8824e-02,  2.0957e-04, -1.8973e-02,  7.3129e-02,  7.9223e-02,\n",
      "         8.0468e-02, -5.9513e-02,  1.2675e-01, -1.0473e-01, -2.2743e-03,\n",
      "        -3.5689e-02, -4.7485e-02,  1.4612e-02, -1.4731e-01,  3.0283e-02,\n",
      "        -4.3783e-02, -1.0702e-01, -1.2393e-01, -1.5395e-01, -1.5502e-01,\n",
      "        -6.4030e-02,  7.4169e-02, -5.9266e-02,  2.9501e-02, -1.3114e-01,\n",
      "        -3.1084e-02,  5.7895e-02,  5.5842e-02, -7.3755e-02, -1.2356e-02,\n",
      "        -4.2171e-02, -1.3607e-01,  3.7821e-02, -2.0038e-02,  5.8521e-02,\n",
      "        -9.7889e-02, -3.6203e-04, -7.8263e-02,  3.8445e-02,  2.4739e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 0.0243,  0.0875,  0.0762,  0.0893, -0.0358,  0.0084, -0.0823,  0.0415,\n",
      "         0.1093,  0.0390, -0.0084,  0.0824, -0.0388, -0.0084,  0.1905, -0.0412,\n",
      "         0.0535,  0.0146, -0.1927, -0.0034, -0.1234, -0.0248,  0.0023,  0.1960,\n",
      "         0.0956,  0.1355, -0.0962, -0.0471, -0.0019, -0.1697,  0.0164,  0.0255,\n",
      "        -0.0218, -0.0832,  0.0267,  0.0968,  0.1448,  0.0340,  0.0123, -0.0644,\n",
      "        -0.1539, -0.0648,  0.1434,  0.0292, -0.0543,  0.0045, -0.1050, -0.0141,\n",
      "        -0.0106, -0.0727, -0.0340,  0.0092,  0.0063, -0.0900,  0.0770, -0.1758,\n",
      "        -0.1888,  0.1074,  0.0024,  0.0566,  0.0525,  0.0500, -0.1397,  0.2322,\n",
      "        -0.1507, -0.0170, -0.0022,  0.0212,  0.0636,  0.1522,  0.0262,  0.0489,\n",
      "        -0.0238, -0.0843,  0.0417, -0.0206, -0.1503, -0.0105, -0.0025, -0.0157,\n",
      "         0.0537,  0.2482, -0.0157, -0.0847,  0.1378,  0.0673,  0.0827,  0.0278,\n",
      "         0.0017,  0.0711,  0.1171, -0.0113, -0.1379, -0.0627,  0.0863,  0.0732,\n",
      "         0.0663, -0.0518, -0.1748, -0.0279,  0.0079, -0.1269,  0.1308, -0.1293,\n",
      "         0.0255,  0.0589, -0.1456, -0.1754,  0.1100,  0.1369, -0.0427, -0.0364,\n",
      "        -0.1153, -0.2599, -0.0316,  0.0100,  0.1226, -0.1337,  0.0349,  0.1675,\n",
      "        -0.2253,  0.1285, -0.1121, -0.1268, -0.0064,  0.1009, -0.2251, -0.0125,\n",
      "         0.0833, -0.0059, -0.0931, -0.1747, -0.0026,  0.0089, -0.1154, -0.1073,\n",
      "         0.2383,  0.0302, -0.1482, -0.0759,  0.0708,  0.0868,  0.1001,  0.0796,\n",
      "         0.2262, -0.0222,  0.2962,  0.0536,  0.0022, -0.0399, -0.0256,  0.0852,\n",
      "        -0.1183, -0.1185, -0.0293, -0.0370,  0.0309, -0.0984,  0.1941,  0.0216,\n",
      "        -0.0090,  0.1404,  0.0598,  0.0650,  0.0744,  0.0426, -0.0094, -0.0445,\n",
      "        -0.1149,  0.0229,  0.1572,  0.1014,  0.1170, -0.1135,  0.0229,  0.1222,\n",
      "        -0.0146, -0.0612,  0.1128, -0.2361,  0.0667, -0.0671, -0.0090, -0.0696,\n",
      "        -0.1264,  0.1187, -0.0047, -0.0778,  0.0414, -0.0189, -0.0153, -0.0139,\n",
      "        -0.0278, -0.1414, -0.0518, -0.0080, -0.1608, -0.0265,  0.1849, -0.0543],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([-4.4951e-02, -3.0269e-02,  5.7666e-03,  5.0608e-02,  7.1861e-03,\n",
      "         1.3723e-01, -4.7515e-02,  2.8713e-02,  1.1686e-01, -2.1819e-02,\n",
      "         8.4215e-02,  3.5925e-02,  1.6644e-01, -8.9751e-02,  5.2478e-02,\n",
      "         1.1553e-01,  1.6151e-01, -2.2964e-02, -1.6174e-01,  1.9235e-01,\n",
      "         9.7453e-02,  7.7079e-02, -8.9056e-02,  8.0481e-02,  1.4806e-01,\n",
      "        -1.6083e-02,  1.0203e-01, -3.7285e-02,  6.1060e-02,  9.6561e-03,\n",
      "        -6.8472e-02,  1.0096e-02, -1.4065e-01,  2.3837e-01,  1.1363e-01,\n",
      "         2.4803e-02, -3.5905e-02,  8.2499e-02,  4.0047e-02,  4.7051e-02,\n",
      "        -1.8222e-01,  9.4498e-02, -9.1961e-02,  1.1252e-01,  7.8541e-02,\n",
      "         1.0352e-01,  5.1129e-02, -1.3695e-02,  1.3555e-01,  3.0085e-02,\n",
      "         4.2894e-02, -3.0123e-03, -1.0391e-01,  6.3762e-03,  2.7413e-02,\n",
      "         2.0599e-01, -1.3469e-01, -4.1689e-02,  8.9827e-02, -1.3860e-01,\n",
      "         5.3680e-02, -9.2493e-02,  2.8438e-02, -9.8101e-02, -2.9716e-02,\n",
      "         1.5025e-02, -8.1340e-03,  8.0789e-03,  1.5008e-01,  1.7955e-02,\n",
      "        -9.7189e-02,  4.2880e-02,  3.5098e-02, -7.5292e-02,  1.0018e-02,\n",
      "        -3.8158e-02,  2.1247e-02,  8.8000e-02,  5.3422e-02,  8.5364e-02,\n",
      "        -3.1726e-02, -4.0565e-02,  5.0597e-02, -5.4060e-02, -1.9612e-03,\n",
      "         2.9600e-01,  9.2132e-02,  2.9507e-02, -9.2795e-02, -1.0727e-01,\n",
      "        -9.4369e-04,  1.6943e-01, -1.1252e-01,  2.0963e-03, -4.7562e-02,\n",
      "        -8.9568e-02, -1.6470e-01, -1.3752e-02, -4.9301e-03,  1.9867e-02,\n",
      "         2.8623e-02, -1.4914e-01, -7.4637e-02,  4.3263e-02, -5.4997e-02,\n",
      "        -5.1978e-02, -9.9176e-02, -1.9955e-02,  5.1099e-02,  1.8961e-02,\n",
      "         3.6069e-02, -8.3924e-02,  1.0509e-01, -1.2345e-01, -6.2687e-02,\n",
      "        -6.5892e-02,  7.2257e-02,  7.6160e-02, -1.0679e-02,  1.1915e-01,\n",
      "        -1.1745e-01, -4.7364e-02,  8.2369e-03, -2.0607e-02, -8.7966e-03,\n",
      "        -1.3239e-01, -1.0953e-02, -3.8245e-02,  1.7113e-01, -9.6756e-02,\n",
      "        -2.3135e-01,  1.7700e-01, -9.4699e-02,  8.4271e-02,  1.7756e-01,\n",
      "        -7.7262e-03,  2.4065e-01,  1.2335e-01,  5.0242e-02,  1.1121e-02,\n",
      "        -1.3971e-01, -2.5510e-02,  1.1339e-02,  5.6864e-02, -2.9294e-02,\n",
      "        -1.0927e-01,  9.0566e-02, -1.4698e-01,  1.0142e-01, -2.0078e-01,\n",
      "        -2.5668e-02, -8.1559e-02, -2.6427e-02,  2.6780e-01,  4.4975e-02,\n",
      "         1.1964e-01,  6.6056e-03,  8.9370e-02,  7.3522e-02, -5.8117e-02,\n",
      "        -6.1687e-02, -3.5208e-02,  1.4783e-01, -1.6733e-02,  1.8550e-01,\n",
      "        -5.9637e-02,  1.0120e-01,  3.3498e-02, -1.4412e-02,  1.4279e-01,\n",
      "        -1.7611e-01,  2.3674e-02, -2.6663e-02,  2.8803e-02, -1.0240e-01,\n",
      "        -8.6560e-02, -1.3708e-02,  2.1908e-01,  1.7359e-01,  1.6947e-02,\n",
      "         1.3747e-01, -1.0779e-02, -5.6662e-02,  2.1867e-02,  9.4120e-02,\n",
      "        -1.4148e-04,  1.3205e-01, -7.6242e-03, -6.1247e-02, -1.5935e-01,\n",
      "         1.1932e-01, -1.7626e-01,  4.7519e-02, -6.7935e-02, -3.5344e-02,\n",
      "        -5.6960e-02, -1.6597e-01,  3.6102e-02,  7.2538e-02, -1.1702e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0897,  0.1346,  0.0082,  0.0404, -0.0141, -0.0915,  0.1112,  0.0613,\n",
      "         0.0437,  0.2269, -0.0635,  0.0330, -0.0388,  0.0970,  0.1340, -0.0103,\n",
      "        -0.0180, -0.2479, -0.1925, -0.0520, -0.0167,  0.2512,  0.2248,  0.0406,\n",
      "        -0.0638, -0.1051,  0.0551,  0.1565, -0.0160, -0.0229,  0.0975,  0.0753,\n",
      "        -0.0261,  0.0484,  0.0233,  0.1358, -0.0195,  0.1246, -0.1235, -0.0948,\n",
      "         0.1241,  0.0367, -0.0132,  0.0701, -0.0866,  0.0677,  0.0747, -0.0005,\n",
      "         0.0291, -0.0934,  0.0961,  0.0369,  0.1113, -0.0287,  0.0090,  0.2001,\n",
      "        -0.0262,  0.0943, -0.1185, -0.1080, -0.1874, -0.2298, -0.0099,  0.1366,\n",
      "        -0.0031, -0.2031,  0.0642, -0.0495, -0.0109,  0.0107, -0.0874, -0.0920,\n",
      "        -0.0740, -0.0029, -0.0110, -0.0867,  0.0235, -0.0947,  0.0334,  0.0025,\n",
      "        -0.1047, -0.1597,  0.0005, -0.1529,  0.0354,  0.1324, -0.1721, -0.0826,\n",
      "         0.1246,  0.1224,  0.1146,  0.0836,  0.0127,  0.1906,  0.0699, -0.0207,\n",
      "         0.0435, -0.0039, -0.1358, -0.0330, -0.1266,  0.1363,  0.0505, -0.1182,\n",
      "        -0.0723, -0.1437,  0.1342,  0.0754, -0.0922,  0.1479, -0.1418, -0.0035,\n",
      "         0.0841,  0.0382,  0.1203, -0.1159,  0.0937,  0.0410, -0.0786,  0.0332,\n",
      "         0.0585,  0.0918,  0.0264,  0.0213, -0.0575,  0.1411, -0.0949,  0.0688,\n",
      "         0.0401,  0.0027, -0.0604,  0.0814, -0.0098, -0.1172,  0.0241, -0.0613,\n",
      "        -0.0740, -0.0775,  0.0796, -0.1757,  0.0731,  0.0651,  0.0684, -0.1016,\n",
      "         0.2258,  0.0089,  0.0729, -0.1231,  0.0348,  0.0380, -0.1748, -0.0902,\n",
      "         0.2137,  0.0032, -0.1064, -0.0598,  0.1530,  0.0615,  0.0364,  0.0570,\n",
      "        -0.0708, -0.0792, -0.1045,  0.0952, -0.0262, -0.0666,  0.0778, -0.0828,\n",
      "         0.0837,  0.0487,  0.0133,  0.0663,  0.0131,  0.1229,  0.1182,  0.0589,\n",
      "        -0.0275, -0.0269, -0.0815, -0.1605,  0.0128, -0.0175,  0.0951, -0.1965,\n",
      "        -0.1307, -0.0327,  0.2386, -0.0120, -0.0878,  0.2075,  0.1736,  0.1385,\n",
      "        -0.0182, -0.1107, -0.2712,  0.1544,  0.2334, -0.0716, -0.0042, -0.0741],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-0.1114, -0.1260, -0.1119, -0.1797, -0.0123, -0.1354, -0.0627,  0.0113,\n",
      "        -0.2121, -0.0449, -0.1316,  0.0974, -0.0196, -0.2268, -0.0866,  0.0271,\n",
      "        -0.0689,  0.0083, -0.1552,  0.1024,  0.0478, -0.0129, -0.0338,  0.0313,\n",
      "        -0.0618,  0.0572, -0.0461,  0.0063,  0.1558, -0.0055, -0.2186,  0.1069,\n",
      "        -0.0180,  0.0722, -0.0741, -0.0951,  0.1404, -0.0053,  0.0869,  0.1728,\n",
      "         0.1237, -0.0906,  0.1067, -0.0791, -0.1660,  0.0487,  0.0635,  0.0555,\n",
      "         0.0562, -0.0208, -0.0247, -0.0210, -0.1192,  0.0735, -0.0960,  0.0183,\n",
      "         0.0923,  0.0311,  0.0228,  0.0739,  0.0312,  0.0293,  0.0242,  0.1432,\n",
      "        -0.1495,  0.0705,  0.1607,  0.1131,  0.0455, -0.1201,  0.1743,  0.0608,\n",
      "        -0.1190, -0.1751,  0.0174,  0.1122,  0.0615,  0.1286,  0.2249, -0.0399,\n",
      "         0.1110, -0.3303, -0.1719, -0.1252,  0.1496, -0.0990, -0.0732,  0.1068,\n",
      "        -0.0648, -0.0481, -0.1068, -0.0256,  0.1217, -0.1511,  0.0120, -0.0370,\n",
      "        -0.0630,  0.0352, -0.0881, -0.0354, -0.2016,  0.0181, -0.0575, -0.0375,\n",
      "         0.1403,  0.0006,  0.2255, -0.0530, -0.0560, -0.2372,  0.1225,  0.2346,\n",
      "         0.1047, -0.0298,  0.0164, -0.0071, -0.0364,  0.1334,  0.1921,  0.0409,\n",
      "         0.0370, -0.2028,  0.0428,  0.0574,  0.1313,  0.1827,  0.1051,  0.0931,\n",
      "         0.1609, -0.1042, -0.0180,  0.1062, -0.2921,  0.1134, -0.0590, -0.1621,\n",
      "         0.0792,  0.0747,  0.0440,  0.1033, -0.0561, -0.0242, -0.0610, -0.0865,\n",
      "        -0.0366,  0.0475, -0.0406, -0.0448,  0.0063,  0.1424,  0.0130,  0.0362,\n",
      "        -0.0880, -0.0607, -0.0947,  0.1126, -0.1080, -0.1318, -0.1815,  0.0529,\n",
      "        -0.0257, -0.0751, -0.2009, -0.0415, -0.0961, -0.0399,  0.0027, -0.0857,\n",
      "        -0.0904, -0.0699, -0.0287, -0.0942,  0.0285, -0.0968,  0.2108,  0.0588,\n",
      "        -0.0367, -0.0581,  0.0109,  0.0122,  0.0639, -0.0397, -0.1301,  0.1894,\n",
      "        -0.1012,  0.1052,  0.0252,  0.1616,  0.1519,  0.0793, -0.1125, -0.0098,\n",
      "        -0.2642,  0.0059, -0.1486,  0.1224, -0.1979, -0.0971, -0.0462,  0.0212],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([-2.3097e-02, -1.4788e-01,  1.0210e-01,  3.3557e-02,  2.4169e-01,\n",
      "        -1.3634e-01, -8.5383e-03, -3.8043e-02,  1.7316e-01, -4.1206e-02,\n",
      "         2.8185e-02,  1.1411e-01,  3.9203e-02,  8.1501e-02,  9.3352e-02,\n",
      "         2.0928e-01, -6.4334e-02,  3.8064e-02,  1.0619e-01,  9.6020e-02,\n",
      "         1.0335e-01,  2.7876e-01,  6.5856e-02,  2.0067e-01,  8.0354e-02,\n",
      "        -2.3247e-01,  1.2218e-01,  2.3015e-02,  1.7446e-01, -5.6854e-02,\n",
      "        -5.4324e-02,  3.6970e-03,  5.6947e-02, -3.3943e-02,  1.6095e-01,\n",
      "         1.9372e-03, -7.9038e-02,  9.8647e-02, -3.2229e-02,  5.0507e-02,\n",
      "         9.7127e-02, -1.5709e-01,  2.4332e-02,  6.0119e-02, -1.0542e-01,\n",
      "        -9.9471e-02, -9.7351e-02,  1.0731e-01,  1.3880e-01, -1.6828e-01,\n",
      "         8.0962e-02, -5.6699e-02, -7.4759e-03, -6.4011e-03, -1.6884e-01,\n",
      "        -1.3898e-01,  1.7222e-01, -8.1651e-02, -1.8829e-01, -5.1125e-02,\n",
      "         1.3743e-01,  1.8289e-01, -4.9558e-02,  3.3120e-02, -3.1865e-02,\n",
      "         8.0625e-02, -4.9182e-02, -2.8268e-02,  2.6301e-02, -9.2309e-03,\n",
      "        -4.7385e-02, -1.8954e-01,  6.7699e-02,  7.9177e-03, -1.2038e-01,\n",
      "         1.9006e-02,  1.7940e-02,  1.2187e-01, -5.8642e-02,  9.8349e-02,\n",
      "        -1.5748e-01,  2.1717e-02, -5.9567e-02, -1.2667e-02,  1.7577e-02,\n",
      "        -2.7029e-02, -1.3078e-01,  9.7926e-02,  2.2109e-02,  1.4673e-01,\n",
      "        -2.8729e-02,  1.4028e-01, -1.5075e-01,  8.0415e-02,  1.0154e-01,\n",
      "        -2.4165e-02, -2.6159e-02, -7.1517e-02, -4.9824e-02, -7.3641e-02,\n",
      "        -6.3665e-02,  1.4753e-01,  2.2992e-02,  2.5068e-02,  8.3415e-02,\n",
      "        -9.7786e-02,  1.5092e-01, -3.5185e-02, -1.6092e-02, -1.3254e-01,\n",
      "        -1.3460e-01,  2.2992e-02,  1.0348e-01,  1.3005e-01,  1.0918e-01,\n",
      "        -5.3370e-03,  3.2445e-02, -1.0142e-02, -9.9965e-03,  4.4736e-02,\n",
      "        -8.6016e-02,  7.8391e-02, -4.4661e-02,  1.1245e-01, -4.3102e-02,\n",
      "         1.1311e-01,  3.2954e-02, -2.3165e-02, -1.0094e-02, -5.7948e-02,\n",
      "         3.6766e-02, -2.9717e-02, -1.2952e-02, -1.1631e-01, -1.0180e-01,\n",
      "        -5.3192e-02,  3.6347e-02, -1.7114e-02,  5.3002e-02,  8.4525e-02,\n",
      "        -1.0713e-01, -9.2328e-02, -7.2179e-02,  3.7902e-02, -1.4022e-01,\n",
      "         5.5914e-02,  7.9675e-02,  3.9484e-02, -6.8603e-03, -5.1982e-02,\n",
      "        -2.1808e-01, -1.2660e-01,  2.4476e-01, -1.2630e-01,  3.3029e-02,\n",
      "         1.7699e-02,  9.0871e-02,  1.5078e-01, -7.2952e-02,  9.8937e-02,\n",
      "        -7.6370e-02,  8.8079e-02, -8.8526e-02,  1.4171e-02, -3.5435e-02,\n",
      "         4.1153e-03, -1.1882e-01, -5.6413e-02,  1.3846e-02, -4.2599e-02,\n",
      "         8.8813e-02,  2.6194e-03, -5.3535e-02, -1.1002e-01,  1.8341e-01,\n",
      "         9.4184e-02, -1.3347e-01,  5.6069e-02,  1.6303e-01,  1.1132e-04,\n",
      "        -6.4000e-02,  6.0648e-02,  1.7026e-01,  2.7840e-02, -1.4208e-01,\n",
      "        -1.4573e-01, -1.1765e-01,  4.9697e-02,  6.4271e-02, -1.9731e-01,\n",
      "         4.8141e-02,  1.8684e-02, -6.3554e-02,  3.6129e-02,  1.9314e-01,\n",
      "         1.0903e-01, -9.7833e-03,  6.3571e-02,  2.5554e-02, -1.0850e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([ 0.0344, -0.0583, -0.0703, -0.2514, -0.0263, -0.1517, -0.0595,  0.0852,\n",
      "        -0.0268,  0.1277,  0.0393,  0.0879, -0.1254, -0.3348, -0.0745, -0.0879,\n",
      "         0.0474,  0.2322, -0.0987,  0.0354, -0.1064,  0.2409,  0.0734,  0.1306,\n",
      "        -0.0146,  0.1136, -0.1571,  0.0971, -0.0936, -0.0309, -0.0165, -0.0660,\n",
      "        -0.0744, -0.0196, -0.0644,  0.0964, -0.0135,  0.1379,  0.0341, -0.0277,\n",
      "        -0.1563,  0.0474,  0.1149, -0.0199,  0.0589, -0.0346, -0.1100, -0.0020,\n",
      "        -0.0207, -0.0792,  0.0376,  0.0803,  0.0011, -0.0536, -0.0457, -0.1338,\n",
      "        -0.0419, -0.1026,  0.0135, -0.0070,  0.0092,  0.0340,  0.0115,  0.0074,\n",
      "        -0.0919,  0.0340, -0.0404, -0.0352, -0.1594, -0.0584, -0.0647, -0.1073,\n",
      "         0.0282, -0.0426, -0.0435,  0.0278, -0.0031,  0.0945,  0.1075,  0.0418,\n",
      "        -0.1066,  0.0568, -0.0267,  0.0154, -0.1619, -0.0058,  0.1889, -0.0501,\n",
      "        -0.0341, -0.0658,  0.1119, -0.1324,  0.0051, -0.0356, -0.0619,  0.0052,\n",
      "        -0.1332,  0.0911,  0.1441,  0.0152, -0.1952, -0.0013, -0.0183, -0.0338,\n",
      "         0.0631,  0.0013, -0.0094, -0.0361,  0.0370,  0.0204,  0.2303,  0.0010,\n",
      "        -0.0754, -0.0257,  0.1069,  0.0503, -0.0969,  0.0752,  0.0786, -0.0510,\n",
      "         0.0232, -0.1267,  0.0891,  0.1099,  0.0993, -0.0559, -0.0145, -0.0100,\n",
      "         0.1262,  0.0168,  0.0875, -0.0091,  0.2017,  0.0385,  0.1079, -0.0034,\n",
      "        -0.0704,  0.0213,  0.0039, -0.0300, -0.1966, -0.0747, -0.0787,  0.1549,\n",
      "        -0.0108,  0.0581, -0.0527, -0.0684, -0.1514,  0.1286, -0.0561,  0.0960,\n",
      "        -0.0797,  0.0104,  0.0925,  0.0467, -0.0793, -0.0287, -0.1249,  0.0754,\n",
      "         0.0514, -0.1473,  0.1062, -0.0139, -0.0259,  0.0431, -0.0780,  0.0970,\n",
      "         0.1067,  0.0072,  0.0515,  0.0534, -0.0132,  0.0470, -0.1017,  0.1103,\n",
      "         0.0218, -0.1171,  0.0045,  0.2126,  0.0261, -0.0675,  0.0822,  0.0762,\n",
      "        -0.0359, -0.0296,  0.0451, -0.1296, -0.0783, -0.0471, -0.0497, -0.0753,\n",
      "        -0.0754,  0.0663, -0.0081,  0.1472,  0.1225, -0.0161, -0.0844, -0.0095],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 0.0324,  0.0432,  0.0128, -0.1048, -0.0309,  0.0343,  0.0725,  0.0905,\n",
      "         0.0277, -0.0101, -0.2833,  0.0445,  0.0266, -0.0947, -0.0705,  0.0008,\n",
      "         0.0852,  0.0237,  0.1369, -0.0885, -0.1265, -0.0279, -0.0524,  0.1058,\n",
      "         0.0903,  0.1174,  0.0208,  0.1273, -0.0776,  0.0633,  0.1014,  0.0515,\n",
      "         0.0878, -0.0716, -0.0953,  0.0094, -0.0114, -0.1050,  0.1013, -0.0545,\n",
      "         0.2847,  0.1515,  0.0272, -0.0088, -0.0249,  0.1165, -0.0152, -0.0898,\n",
      "         0.0555, -0.0581,  0.0530, -0.1249, -0.0017, -0.0968, -0.0731, -0.0366,\n",
      "         0.0387, -0.0459, -0.0424,  0.0322, -0.0163, -0.0722,  0.0519, -0.2235,\n",
      "        -0.0526, -0.1272, -0.1831,  0.0043, -0.0183,  0.0030, -0.0554,  0.1429,\n",
      "        -0.0177, -0.0099, -0.0047, -0.0410,  0.1171,  0.0849, -0.0063,  0.0121,\n",
      "         0.1281, -0.0245, -0.1118,  0.0065, -0.0303,  0.0243, -0.0665,  0.0273,\n",
      "        -0.1180, -0.0921, -0.0741,  0.1799, -0.1599, -0.1509,  0.0317, -0.1675,\n",
      "         0.0991, -0.1584,  0.0643, -0.0938, -0.0833,  0.0289, -0.1786, -0.0270,\n",
      "        -0.0047, -0.0191,  0.0682, -0.1072,  0.0457,  0.0037,  0.0101, -0.0792,\n",
      "        -0.0411, -0.0145,  0.2259, -0.0552, -0.0088,  0.2277,  0.1019, -0.0581,\n",
      "        -0.0499,  0.0003, -0.0960,  0.1163,  0.0542, -0.0899,  0.0785, -0.0757,\n",
      "         0.1136,  0.0955,  0.0024, -0.1075,  0.0386, -0.1068, -0.1145, -0.0436,\n",
      "        -0.1211, -0.0174,  0.0412, -0.1130,  0.0218, -0.0999, -0.1438,  0.0179,\n",
      "        -0.0548, -0.0479, -0.0121, -0.0538, -0.1268, -0.0741, -0.0415,  0.0144,\n",
      "         0.0508,  0.0760, -0.0638, -0.1439,  0.0555, -0.0486,  0.0251,  0.0053,\n",
      "         0.0245, -0.0231,  0.1099,  0.1551,  0.0105,  0.1027, -0.2184, -0.0827,\n",
      "        -0.1366,  0.0940, -0.1021,  0.1093,  0.1131,  0.0516, -0.1181,  0.1755,\n",
      "         0.0641,  0.0734,  0.1746,  0.1287, -0.0379,  0.0184,  0.0576, -0.0517,\n",
      "         0.0330, -0.0928,  0.0623, -0.0064,  0.0763,  0.0913,  0.0678,  0.1516,\n",
      "         0.0029, -0.0753, -0.0246,  0.0518, -0.0983,  0.1127,  0.0964, -0.1392],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 0.0620, -0.1085, -0.0364, -0.0362, -0.0345,  0.0245, -0.0383, -0.1592,\n",
      "        -0.0493,  0.0025, -0.0031, -0.0267, -0.0372, -0.0254, -0.0815, -0.1367,\n",
      "        -0.1122, -0.0198, -0.1505, -0.0171,  0.2510,  0.0592, -0.0906,  0.0650,\n",
      "        -0.1193,  0.1853,  0.1500, -0.0254, -0.0351, -0.1932,  0.1420,  0.0618,\n",
      "        -0.0726, -0.0382,  0.0244, -0.1537, -0.0670, -0.0176,  0.0063,  0.0121,\n",
      "         0.1055, -0.1070, -0.0291, -0.1309, -0.0195,  0.0570, -0.0449,  0.1710,\n",
      "        -0.0209,  0.1127, -0.0518, -0.0039,  0.0513, -0.0427,  0.0767, -0.0871,\n",
      "        -0.0222,  0.0821, -0.0037, -0.0591, -0.2268,  0.0216,  0.0999, -0.0761,\n",
      "        -0.0202,  0.0163,  0.0175,  0.0805,  0.2464,  0.0258,  0.0094, -0.2156,\n",
      "        -0.1800, -0.0450, -0.0323,  0.0328,  0.1234,  0.0703,  0.1513, -0.0536,\n",
      "        -0.0270,  0.1320,  0.1908,  0.0332, -0.0246, -0.0074,  0.0894,  0.1751,\n",
      "        -0.0369, -0.0786, -0.1112, -0.0129,  0.1746,  0.0117,  0.1111, -0.0848,\n",
      "         0.0966, -0.2175,  0.0244, -0.0435,  0.0372, -0.0111, -0.1484, -0.1168,\n",
      "         0.0543,  0.0504, -0.0847, -0.1751, -0.0511,  0.2542,  0.0312,  0.1713,\n",
      "        -0.0500,  0.0007, -0.0746,  0.0990,  0.0940, -0.0162, -0.0896,  0.0984,\n",
      "        -0.1223, -0.1810, -0.0262,  0.0043,  0.0123, -0.1478, -0.0811, -0.0217,\n",
      "        -0.2336, -0.1054, -0.1123,  0.0213,  0.1850, -0.0052, -0.0100, -0.0414,\n",
      "        -0.0894,  0.0167,  0.1224, -0.0976, -0.0830,  0.1470, -0.1326, -0.0536,\n",
      "        -0.2003,  0.0206,  0.0106, -0.1356,  0.2107, -0.0481, -0.0495, -0.1029,\n",
      "         0.2354, -0.0199, -0.0308, -0.1495, -0.0890,  0.0694, -0.1402,  0.0187,\n",
      "        -0.0880, -0.0337, -0.0247, -0.0611, -0.0248,  0.0132, -0.0624, -0.0821,\n",
      "        -0.1276,  0.0288,  0.0071,  0.0083, -0.0146, -0.1069, -0.1822,  0.0275,\n",
      "         0.1872,  0.0655,  0.1092, -0.0039,  0.0767,  0.0464, -0.0118, -0.0907,\n",
      "         0.2852,  0.0258,  0.0703, -0.0427, -0.0363,  0.1239, -0.0180, -0.0091,\n",
      "         0.0565, -0.1153, -0.0575, -0.0451,  0.1766, -0.1391,  0.0200,  0.1119],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([ 0.0004,  0.0098, -0.0630,  0.0729,  0.1286,  0.1135,  0.0132,  0.0753,\n",
      "         0.1000,  0.0748,  0.0039, -0.1073, -0.0160,  0.0784,  0.0432,  0.0864,\n",
      "         0.0586, -0.0512,  0.2272,  0.1022,  0.0344,  0.0295,  0.0027,  0.0091,\n",
      "         0.0955,  0.0551, -0.1328, -0.0203,  0.2392, -0.1312, -0.0542,  0.1138,\n",
      "         0.0317, -0.0134,  0.1082, -0.0630, -0.2079,  0.0328, -0.0148, -0.0055,\n",
      "         0.0792,  0.0654,  0.0422,  0.0049, -0.1233, -0.0741,  0.1021, -0.0386,\n",
      "        -0.0348,  0.0083, -0.0181,  0.0171,  0.0679,  0.0125, -0.0887, -0.0110,\n",
      "         0.0730, -0.0927,  0.0534,  0.0082, -0.0331,  0.1718,  0.1019, -0.0851,\n",
      "         0.0134,  0.1386, -0.0494,  0.0115,  0.0689,  0.0778,  0.0095,  0.1618,\n",
      "         0.0119,  0.0620, -0.0528, -0.0109, -0.1644,  0.1267, -0.1405,  0.2506,\n",
      "        -0.1118, -0.0659, -0.0724, -0.0041,  0.2570, -0.0359,  0.0570,  0.1049,\n",
      "         0.0314,  0.0490, -0.1250,  0.0921, -0.1334, -0.0470,  0.0743,  0.2864,\n",
      "         0.2034, -0.0486, -0.0558,  0.1079,  0.0035,  0.2203,  0.2007, -0.0190,\n",
      "        -0.0402,  0.2442, -0.1645,  0.0048, -0.0110, -0.0840, -0.2092, -0.0796,\n",
      "        -0.0620, -0.1374, -0.2458, -0.1597, -0.0614,  0.0840, -0.0038,  0.0786,\n",
      "         0.0911, -0.0136, -0.1404, -0.0157,  0.1248, -0.0229, -0.0362, -0.0273,\n",
      "        -0.1132,  0.0388, -0.0171, -0.1177, -0.1702, -0.1764, -0.0355,  0.0977,\n",
      "        -0.0696,  0.1107,  0.1663, -0.0294,  0.1273,  0.0251,  0.0632,  0.0685,\n",
      "        -0.1134, -0.2482, -0.0751, -0.0773, -0.0422,  0.1633,  0.0615,  0.0023,\n",
      "        -0.2218, -0.1290, -0.0822, -0.0304,  0.0999,  0.1201, -0.0902,  0.0529,\n",
      "        -0.0231,  0.1423, -0.1279,  0.0132, -0.1959, -0.1194,  0.2194,  0.0659,\n",
      "         0.0531, -0.0018,  0.1107, -0.0963,  0.1062,  0.0171,  0.0575, -0.1313,\n",
      "        -0.1070, -0.1193,  0.0607, -0.0449,  0.0803, -0.0565,  0.0955,  0.0853,\n",
      "         0.1472, -0.0848, -0.0461,  0.0320, -0.0080,  0.1865, -0.0155,  0.0745,\n",
      "         0.1170, -0.0421, -0.1014, -0.0092,  0.0323,  0.0626, -0.0019,  0.0560],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([10, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([ 0.0136, -0.0106, -0.0700,  0.0738,  0.0363, -0.0981,  0.0008,  0.0320,\n",
      "         0.0276, -0.0871, -0.0159, -0.0787, -0.0114, -0.1574, -0.0150, -0.0063,\n",
      "        -0.0520,  0.0052, -0.0031, -0.0725, -0.1130, -0.0024, -0.0740, -0.0406,\n",
      "         0.0623, -0.0248, -0.1201, -0.1544,  0.0628,  0.0948, -0.1293, -0.0977,\n",
      "        -0.0445,  0.0393, -0.0752, -0.1147, -0.0152, -0.0637,  0.0246,  0.0448,\n",
      "         0.1848,  0.0159, -0.0257, -0.1081,  0.0749, -0.1096,  0.0297,  0.0380,\n",
      "        -0.0246, -0.0982,  0.0286, -0.1074,  0.1856, -0.1482,  0.1644, -0.1338,\n",
      "         0.0282,  0.0384,  0.0637, -0.0539,  0.0754,  0.0908,  0.0045,  0.0340,\n",
      "         0.1005,  0.0013,  0.0536,  0.0342, -0.0304, -0.0040, -0.0625,  0.1837,\n",
      "        -0.0095, -0.0813, -0.0995, -0.0855, -0.0032, -0.0013, -0.0328,  0.0576,\n",
      "         0.0864,  0.0268,  0.0238,  0.0534,  0.0807, -0.0262, -0.0349,  0.0197,\n",
      "        -0.1325,  0.0013,  0.0123, -0.1658, -0.0778, -0.0948, -0.1309,  0.1143,\n",
      "         0.0970,  0.1279,  0.1991, -0.1371, -0.0006,  0.0825, -0.1059,  0.0706,\n",
      "        -0.0534,  0.0225,  0.1133,  0.1787, -0.1257, -0.1148,  0.1220,  0.0250,\n",
      "        -0.1914, -0.2004,  0.0467,  0.0144,  0.0167, -0.0004, -0.1363, -0.0944,\n",
      "        -0.0846, -0.1734,  0.0105,  0.1364, -0.1122, -0.0127,  0.1167, -0.0659,\n",
      "        -0.0496,  0.0946,  0.0137,  0.1505, -0.0582,  0.0473, -0.2295, -0.1048,\n",
      "        -0.0118, -0.1000, -0.0767,  0.0394, -0.0572,  0.0360, -0.1225,  0.1299,\n",
      "        -0.0898,  0.0036, -0.1052,  0.1738, -0.1547, -0.0046, -0.0737,  0.0932,\n",
      "        -0.1146, -0.0283,  0.0006,  0.0109,  0.1251,  0.1539, -0.1232,  0.0430,\n",
      "        -0.1511,  0.0493, -0.0529,  0.0596,  0.0372,  0.1905, -0.1539,  0.0558,\n",
      "         0.1288,  0.1966,  0.0664, -0.0392,  0.0058, -0.1144,  0.0046, -0.0110,\n",
      "         0.0415,  0.1495,  0.1298, -0.0630, -0.1861, -0.0034, -0.0232, -0.1339,\n",
      "         0.0198, -0.0104,  0.0531,  0.0360,  0.0969, -0.0470, -0.1119,  0.1577,\n",
      "         0.0795, -0.0187, -0.0610, -0.0518,  0.0902, -0.1077,  0.0492, -0.0430],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 3.9019e-02,  2.5154e-02,  9.0798e-02, -9.7679e-02,  2.8764e-02,\n",
      "         7.0434e-02,  8.4063e-02,  2.5928e-02,  9.7380e-02,  1.6275e-02,\n",
      "         8.7966e-02,  1.8267e-02, -2.7582e-02,  3.0036e-02, -1.6191e-01,\n",
      "        -8.6450e-02, -1.8375e-03,  8.3683e-02,  1.2906e-02, -1.4225e-02,\n",
      "         1.1338e-01,  3.5779e-02,  1.1134e-01,  3.2772e-02,  3.0779e-02,\n",
      "         4.5491e-02,  6.9465e-02, -7.9200e-02,  1.2984e-01,  1.1974e-01,\n",
      "         5.7609e-02, -1.1568e-01, -1.6414e-01,  3.0509e-02,  6.8447e-02,\n",
      "        -6.2259e-02,  1.0754e-01, -2.4923e-02,  4.4897e-02,  5.2062e-02,\n",
      "         1.1688e-01,  1.9003e-01,  2.0063e-02,  6.7085e-02,  1.0077e-01,\n",
      "         5.4456e-02, -6.7470e-02,  1.3327e-01, -1.5840e-03, -6.3881e-02,\n",
      "         3.4684e-02,  3.7240e-02,  6.3400e-02, -2.8443e-02, -9.6012e-02,\n",
      "        -6.2349e-02, -1.1489e-01,  7.2854e-02, -1.0937e-01,  9.2610e-02,\n",
      "        -2.1004e-04, -3.7679e-02, -9.4393e-03,  2.1505e-01,  1.5926e-01,\n",
      "        -7.5369e-02,  1.9417e-01,  2.1636e-02, -8.1999e-02, -1.8520e-01,\n",
      "         1.7142e-02, -2.9865e-02, -4.0532e-02, -9.7150e-02,  1.5350e-01,\n",
      "        -1.8486e-01, -5.9385e-03,  1.1371e-02, -3.8125e-02, -1.3841e-02,\n",
      "        -2.0428e-02, -1.0368e-01, -4.4102e-02, -2.1970e-02,  1.0765e-02,\n",
      "        -5.5344e-02,  7.0728e-02,  4.6190e-02,  6.2990e-02, -5.4066e-02,\n",
      "        -1.6213e-01, -3.9405e-02,  1.3400e-01,  8.2171e-02, -2.3741e-01,\n",
      "         1.0548e-01, -9.5347e-02, -3.4751e-02, -3.8221e-02,  9.9864e-02,\n",
      "        -1.1664e-01,  1.0195e-01,  3.3650e-02,  8.7214e-03, -5.3215e-02,\n",
      "         7.9702e-02, -9.8969e-02,  1.6521e-01,  4.4730e-02, -2.8468e-02,\n",
      "        -7.8716e-02, -6.4015e-03,  1.3884e-01, -2.8019e-02,  2.1341e-01,\n",
      "        -9.0670e-02,  9.4125e-02, -6.3092e-02,  5.8177e-02,  6.7590e-02,\n",
      "         4.8755e-02, -9.2646e-02, -1.4212e-01, -6.4560e-02, -2.2409e-01,\n",
      "         1.1573e-03,  1.1887e-01, -7.8905e-02, -7.3512e-02, -3.9799e-02,\n",
      "         1.3260e-01,  4.2742e-02,  7.5729e-02,  3.1080e-02, -6.4224e-02,\n",
      "        -1.1484e-01,  1.0520e-01,  7.3674e-02, -1.0186e-01, -3.0466e-02,\n",
      "        -4.0072e-02,  3.1730e-02,  9.9246e-02, -7.0664e-02,  2.0741e-02,\n",
      "        -1.1007e-03, -2.3973e-01,  9.8560e-02,  4.4328e-02,  1.3719e-01,\n",
      "         1.7300e-01,  5.2069e-02, -1.3163e-02, -4.8457e-03,  1.1188e-01,\n",
      "        -4.3641e-02,  9.1629e-02, -7.9687e-02, -3.0985e-02, -5.0387e-02,\n",
      "        -1.0267e-02, -8.4093e-02, -1.3090e-01, -9.6986e-02, -1.1397e-01,\n",
      "         1.7749e-01, -7.0455e-03,  8.1856e-02, -3.4062e-02,  5.2526e-02,\n",
      "        -2.2659e-01,  1.9467e-01,  7.6326e-02,  2.5363e-03,  6.2256e-02,\n",
      "         9.6879e-02,  1.5179e-01, -1.3537e-01, -1.3574e-01, -2.1640e-03,\n",
      "        -1.6477e-01,  1.0380e-01,  6.0904e-02,  1.8073e-02, -5.3058e-02,\n",
      "         4.4933e-02,  2.5375e-01,  2.1265e-03, -2.3270e-02, -1.6723e-02,\n",
      "        -3.1699e-02, -1.4938e-01, -2.2361e-02,  2.1272e-03, -7.7590e-02,\n",
      "        -1.0800e-01, -2.0303e-02,  1.5057e-01, -2.4740e-02,  8.9488e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([-0.1176, -0.0570, -0.0881,  0.0382, -0.0470, -0.0022, -0.0419,  0.0347,\n",
      "        -0.0455,  0.1512, -0.2212,  0.0205,  0.0492,  0.0188,  0.1280,  0.0916,\n",
      "        -0.2246, -0.0407, -0.0579, -0.0697, -0.0332, -0.0426, -0.0589, -0.0386,\n",
      "         0.1296, -0.1049,  0.0089, -0.1006,  0.0838,  0.0634, -0.0685,  0.0280,\n",
      "         0.0230, -0.0933, -0.0234,  0.0639,  0.0010,  0.0445,  0.0817, -0.1759,\n",
      "         0.0889, -0.1021,  0.0453,  0.0165, -0.1924,  0.0662,  0.0422, -0.1617,\n",
      "         0.0394, -0.0525, -0.1020,  0.0969,  0.1674, -0.0956,  0.0062, -0.2188,\n",
      "        -0.1477, -0.0795, -0.0521,  0.0115, -0.0537, -0.0015,  0.0250,  0.0240,\n",
      "        -0.2109, -0.0307, -0.1652, -0.1158,  0.0194, -0.1232, -0.0790,  0.0154,\n",
      "        -0.1488, -0.0429, -0.0399,  0.0404, -0.0934,  0.0394, -0.0222,  0.0784,\n",
      "        -0.0192,  0.0321,  0.1407,  0.0594,  0.0645,  0.2660,  0.1538, -0.0809,\n",
      "         0.0616,  0.0261,  0.0989, -0.0503, -0.0319,  0.0858, -0.1420, -0.0518,\n",
      "         0.1445, -0.0124, -0.0205, -0.1577, -0.0543, -0.0750,  0.0879,  0.0205,\n",
      "        -0.0879,  0.1437, -0.1579, -0.1448, -0.1457, -0.0660, -0.0659, -0.0375,\n",
      "         0.0527, -0.0404,  0.1653,  0.0289,  0.1094, -0.1015,  0.1626, -0.0075,\n",
      "         0.1662,  0.2571,  0.0893,  0.0890, -0.0148, -0.0483, -0.0005,  0.0456,\n",
      "         0.0326, -0.0538, -0.2244,  0.0251, -0.0627,  0.0730, -0.0966, -0.1493,\n",
      "        -0.0732,  0.0858,  0.1233,  0.2035,  0.0183,  0.1147,  0.1068,  0.0161,\n",
      "         0.0902, -0.0651, -0.0715,  0.0183,  0.0563,  0.1722,  0.0345, -0.0602,\n",
      "        -0.1160,  0.0269, -0.1717,  0.0408,  0.0133,  0.1677,  0.0753,  0.2172,\n",
      "        -0.0146,  0.0282, -0.0693,  0.1199, -0.0784, -0.0165,  0.1006,  0.0451,\n",
      "        -0.1145,  0.1461,  0.0647, -0.0945,  0.0623, -0.1480, -0.0218, -0.1240,\n",
      "        -0.0219,  0.0630, -0.0281, -0.0556,  0.0266,  0.0834, -0.0797, -0.1292,\n",
      "         0.0918,  0.0871,  0.0766, -0.0546, -0.1768, -0.0866, -0.0672,  0.0826,\n",
      "        -0.1746,  0.0408, -0.1357, -0.1097, -0.0017,  0.0469, -0.0158, -0.1136],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0365,  0.1492,  0.1437,  0.1214, -0.1905,  0.0414,  0.0920, -0.1194,\n",
      "        -0.0793,  0.0523, -0.0206, -0.0877, -0.1264, -0.0247,  0.0060,  0.1693,\n",
      "        -0.1031,  0.0021,  0.1151,  0.0680, -0.0114, -0.1083,  0.0192, -0.1049,\n",
      "         0.0003,  0.1907, -0.1256,  0.0967,  0.0163,  0.0660,  0.1730, -0.0695,\n",
      "         0.1811,  0.0153, -0.0434, -0.0872,  0.0316, -0.0302, -0.0075, -0.0259,\n",
      "        -0.0154,  0.0773, -0.1435,  0.1663, -0.0149, -0.0396,  0.1477,  0.2279,\n",
      "         0.0989,  0.0160,  0.0486,  0.0256,  0.0010,  0.0822,  0.0260,  0.0013,\n",
      "        -0.0255, -0.0615,  0.0043, -0.1938, -0.0396, -0.0095,  0.0934,  0.0380,\n",
      "        -0.0638,  0.1835, -0.0801, -0.1317,  0.2654,  0.0595, -0.0071, -0.1468,\n",
      "        -0.0288, -0.1604,  0.0815, -0.0099, -0.0089, -0.0636,  0.0295, -0.0150,\n",
      "        -0.1872, -0.0362,  0.1606, -0.1631, -0.0683,  0.1717, -0.1120, -0.0207,\n",
      "        -0.0738, -0.0534,  0.1167,  0.0465, -0.1569,  0.0623,  0.1992, -0.0154,\n",
      "        -0.0664,  0.0287,  0.0269, -0.0208,  0.0981,  0.1508,  0.0021,  0.0982,\n",
      "         0.0723,  0.0228,  0.0112,  0.0613,  0.1816, -0.1070,  0.0553,  0.0236,\n",
      "        -0.0179, -0.0131,  0.0833, -0.0739,  0.0031,  0.0167,  0.0272, -0.1319,\n",
      "         0.0396,  0.0491,  0.0588,  0.0015, -0.0071,  0.1565, -0.0120, -0.1833,\n",
      "         0.1558,  0.1691,  0.0405, -0.1244, -0.0768, -0.0768, -0.1019,  0.1306,\n",
      "         0.0825, -0.0200, -0.0242,  0.0078,  0.0199, -0.1735, -0.1019, -0.0651,\n",
      "        -0.1056, -0.0799,  0.1560,  0.1587, -0.0255,  0.1213, -0.0596,  0.1384,\n",
      "        -0.0424,  0.0252, -0.1582,  0.1200, -0.0028, -0.1406, -0.2185,  0.1694,\n",
      "         0.0334,  0.1966, -0.0360,  0.1704,  0.2084, -0.1135, -0.0869,  0.1150,\n",
      "         0.0498,  0.0928, -0.0948, -0.0336,  0.1267,  0.0648, -0.0360,  0.0417,\n",
      "        -0.0814, -0.0225,  0.2085, -0.0853,  0.0700,  0.1192, -0.0077,  0.0075,\n",
      "        -0.0558, -0.1166,  0.0397,  0.0466,  0.0524, -0.0005,  0.0760,  0.0648,\n",
      "        -0.0572, -0.0123, -0.1625, -0.2079,  0.0009,  0.1398, -0.1563, -0.0761],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-0.0874, -0.2878,  0.0162,  0.1137, -0.0216, -0.0095, -0.0791, -0.0112,\n",
      "         0.1378,  0.0276, -0.0191,  0.0996, -0.0046,  0.0440, -0.1734, -0.1609,\n",
      "        -0.0709, -0.2547,  0.0804,  0.1218,  0.2098, -0.1937, -0.0028, -0.1298,\n",
      "        -0.0120,  0.0623, -0.1709, -0.0088,  0.0245, -0.0329, -0.1025, -0.0525,\n",
      "        -0.0827,  0.2040, -0.0738, -0.0449, -0.0073,  0.0925,  0.0297, -0.0470,\n",
      "        -0.0164,  0.1825, -0.2102, -0.0393, -0.0126,  0.0600, -0.0207, -0.1989,\n",
      "         0.0681,  0.0819, -0.0137,  0.0233,  0.0062, -0.0912, -0.0540, -0.0826,\n",
      "         0.0754,  0.1373,  0.1662, -0.0536, -0.0954, -0.1096, -0.1003, -0.0158,\n",
      "        -0.2121,  0.0899, -0.2017,  0.0826,  0.1063,  0.1378, -0.0192,  0.1304,\n",
      "         0.2962, -0.0886,  0.0716,  0.2377,  0.0810,  0.0063,  0.0698,  0.1793,\n",
      "        -0.0389,  0.0985, -0.1307, -0.2917, -0.1084, -0.1743,  0.0437,  0.0713,\n",
      "        -0.0190, -0.1089, -0.1086,  0.0735,  0.0281,  0.0158,  0.0761, -0.2097,\n",
      "         0.1789, -0.1032,  0.0685, -0.1225,  0.0463,  0.2206,  0.0182,  0.0383,\n",
      "        -0.0163,  0.0126, -0.0634,  0.0739, -0.1272, -0.0418,  0.0581, -0.0152,\n",
      "        -0.1131,  0.0571,  0.0255, -0.0762, -0.1088, -0.0394,  0.0134, -0.1451,\n",
      "        -0.0832, -0.0144,  0.1111, -0.0474,  0.1000,  0.0990,  0.0673, -0.0446,\n",
      "        -0.0586,  0.0127,  0.0218, -0.0582,  0.2106,  0.0093, -0.1277, -0.0548,\n",
      "        -0.0257,  0.2035, -0.1348, -0.0572, -0.0537, -0.0828, -0.1483,  0.0311,\n",
      "        -0.0788,  0.0215, -0.1438, -0.0198, -0.1424,  0.1543, -0.1733, -0.0286,\n",
      "        -0.1172, -0.0252,  0.0334, -0.0683,  0.1256, -0.0856, -0.0397, -0.0185,\n",
      "        -0.1631, -0.1860,  0.0950, -0.1286, -0.0509,  0.0371,  0.0340, -0.0386,\n",
      "        -0.0047,  0.0548, -0.0648, -0.0035,  0.2056, -0.1304,  0.0738,  0.0144,\n",
      "         0.0309, -0.1177, -0.1432,  0.0321,  0.0579, -0.1703, -0.0908,  0.0473,\n",
      "         0.1625, -0.0337,  0.0552,  0.0707,  0.0216, -0.0073,  0.0840, -0.0357,\n",
      "        -0.0778,  0.0333,  0.0204,  0.1302, -0.0612, -0.0760,  0.0045, -0.0715],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 0.1300, -0.0290,  0.0912, -0.1956,  0.0597,  0.0285, -0.1028,  0.0317,\n",
      "        -0.0103,  0.1087,  0.0391,  0.0249,  0.1348,  0.0188,  0.2061, -0.0331,\n",
      "        -0.0511,  0.1448, -0.0687, -0.0512,  0.1953, -0.1046, -0.1168,  0.0371,\n",
      "        -0.0454,  0.1605,  0.2064,  0.0105,  0.0217, -0.1555,  0.0123, -0.0809,\n",
      "         0.0847,  0.0267, -0.2364, -0.1307,  0.0148, -0.1221,  0.1381, -0.0256,\n",
      "         0.0939,  0.0111, -0.0287, -0.2360,  0.0629,  0.1009,  0.0447,  0.1768,\n",
      "        -0.0366, -0.0532,  0.2044,  0.1877, -0.0016,  0.1828, -0.1052, -0.1184,\n",
      "        -0.2589,  0.1056,  0.2014, -0.0456,  0.0197, -0.0621,  0.0749, -0.0556,\n",
      "        -0.1770,  0.1183,  0.2220, -0.0243, -0.0030,  0.0711,  0.0164, -0.0922,\n",
      "         0.0700, -0.0595,  0.1727,  0.0145,  0.0382, -0.3578, -0.0543,  0.0084,\n",
      "         0.0855, -0.0105, -0.0112,  0.0679, -0.1331,  0.0217,  0.0218,  0.0706,\n",
      "        -0.1121, -0.0037,  0.0218, -0.0264, -0.0442,  0.0885, -0.0255, -0.0512,\n",
      "        -0.0117, -0.0858, -0.1294, -0.0291, -0.0313,  0.1300, -0.1170,  0.0544,\n",
      "         0.0718,  0.1288, -0.1224,  0.1531, -0.0877,  0.0856, -0.0822, -0.0295,\n",
      "         0.0070, -0.0089, -0.1059, -0.1029,  0.1119, -0.1977,  0.1345, -0.1227,\n",
      "        -0.1313, -0.0701,  0.0925,  0.0867, -0.0602, -0.0454, -0.1162,  0.1331,\n",
      "        -0.0070,  0.0286,  0.1760, -0.1712,  0.0136,  0.0362,  0.0169, -0.0475,\n",
      "        -0.0543, -0.1353, -0.0515,  0.0787, -0.0935, -0.2302,  0.0657, -0.0193,\n",
      "        -0.0741, -0.1347, -0.1320, -0.0633,  0.0482,  0.0237,  0.0235,  0.1115,\n",
      "         0.0948,  0.2133,  0.0239,  0.0536,  0.0297,  0.1637, -0.1271,  0.0527,\n",
      "         0.0373, -0.0568,  0.0910, -0.0004, -0.1461,  0.1952, -0.0235,  0.0972,\n",
      "         0.0026,  0.0695,  0.1002,  0.0492, -0.0383, -0.0427, -0.0296,  0.0046,\n",
      "         0.0502,  0.0025,  0.0582,  0.1880, -0.0915, -0.0357,  0.0783,  0.0295,\n",
      "        -0.0559, -0.1317,  0.0729, -0.0005,  0.0163, -0.0260, -0.0636, -0.0965,\n",
      "        -0.0161,  0.0863,  0.0049, -0.0130, -0.0310,  0.0269,  0.0490, -0.0194],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-0.0755,  0.1157,  0.1487,  0.0312,  0.0088, -0.0934,  0.1386,  0.0494,\n",
      "         0.1581,  0.0538, -0.1382,  0.1405, -0.0518, -0.1131, -0.0435,  0.0108,\n",
      "        -0.0136, -0.0261,  0.0036, -0.0735,  0.0057,  0.1861, -0.0205, -0.0147,\n",
      "         0.1115, -0.1055, -0.0333, -0.0581,  0.0298, -0.1677,  0.0853, -0.1542,\n",
      "         0.0841, -0.0789,  0.1823,  0.0546, -0.1157, -0.0021, -0.0624, -0.0842,\n",
      "        -0.0643,  0.0172, -0.0460, -0.0905, -0.1069,  0.0294,  0.1544, -0.0694,\n",
      "         0.0053,  0.0024,  0.1008,  0.1026,  0.2395,  0.0446, -0.0760,  0.0411,\n",
      "        -0.0543,  0.0697, -0.0095,  0.2348, -0.0666,  0.0692, -0.0046,  0.0664,\n",
      "        -0.0122, -0.0853,  0.0614, -0.0631,  0.1126, -0.1014, -0.0805,  0.0063,\n",
      "         0.0694, -0.0091, -0.1463,  0.0367, -0.0787,  0.1328, -0.1187,  0.0344,\n",
      "         0.1351, -0.1024,  0.0370, -0.0922, -0.0747,  0.0126,  0.0906, -0.0895,\n",
      "        -0.1623,  0.0673,  0.0384, -0.0280,  0.0264,  0.0056,  0.0317, -0.0555,\n",
      "         0.0636, -0.2060,  0.0456,  0.0843, -0.0197,  0.0903,  0.2240,  0.1724,\n",
      "         0.0046,  0.0300,  0.0270,  0.0626,  0.1121,  0.0250, -0.0804,  0.0623,\n",
      "         0.0696,  0.0334,  0.0043, -0.2056,  0.1308,  0.0856, -0.0198, -0.0268,\n",
      "         0.1300, -0.0193,  0.0783,  0.0175, -0.0085,  0.0610, -0.0509,  0.0733,\n",
      "         0.0186, -0.0053,  0.0114, -0.0056, -0.0509,  0.0588,  0.1049,  0.0542,\n",
      "         0.1166, -0.1209,  0.0434,  0.1118,  0.0814, -0.0112,  0.0703, -0.0407,\n",
      "        -0.0151, -0.0558,  0.0140,  0.1696, -0.0412,  0.1057,  0.0986, -0.0241,\n",
      "        -0.0095, -0.0080, -0.0638, -0.0291,  0.0576, -0.0776, -0.1520, -0.1818,\n",
      "         0.0851, -0.0793,  0.1604, -0.0665,  0.0501,  0.1502, -0.1287,  0.0479,\n",
      "        -0.1453,  0.0263, -0.0035,  0.0960,  0.0638, -0.1566,  0.0418,  0.0929,\n",
      "         0.1142, -0.0140,  0.0344,  0.1191, -0.1846, -0.0907,  0.1322, -0.0362,\n",
      "         0.0544, -0.1205, -0.0043,  0.0934, -0.0007,  0.0405,  0.1871, -0.0829,\n",
      "        -0.0702,  0.1021,  0.0760,  0.0168, -0.0093, -0.1217,  0.0335,  0.1808],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 0.0619,  0.1403, -0.1077, -0.2767,  0.2080,  0.0575, -0.0134, -0.1055,\n",
      "         0.1499,  0.1858,  0.0217,  0.1187, -0.0649,  0.2870, -0.1568,  0.1042,\n",
      "        -0.1069, -0.1348,  0.0584, -0.1022, -0.0325, -0.0836,  0.0143,  0.0472,\n",
      "         0.0017, -0.0113, -0.0767, -0.1494,  0.0455, -0.1245, -0.0429,  0.0766,\n",
      "         0.0107, -0.0946, -0.1371,  0.1191,  0.1512,  0.0053,  0.0394, -0.0465,\n",
      "         0.0175, -0.1627,  0.0771,  0.0206,  0.1903, -0.1087,  0.1277,  0.0255,\n",
      "         0.1399,  0.1205,  0.1156, -0.0843,  0.0157,  0.0475, -0.1077, -0.0837,\n",
      "        -0.0339, -0.1300, -0.0038, -0.0063, -0.1576,  0.0628,  0.1015, -0.0629,\n",
      "        -0.0113,  0.1316,  0.1097,  0.0832,  0.0638, -0.0506, -0.0556, -0.0724,\n",
      "        -0.0188, -0.0401, -0.1590,  0.0450,  0.0624, -0.0515, -0.0752, -0.0796,\n",
      "        -0.0007, -0.3024, -0.2144, -0.0835, -0.0758, -0.1519, -0.1939,  0.1572,\n",
      "        -0.1857,  0.1345, -0.1711, -0.0674,  0.0412, -0.1714, -0.0072, -0.2019,\n",
      "         0.0787,  0.0984,  0.0226, -0.0245, -0.0423,  0.1742, -0.2161, -0.0183,\n",
      "        -0.0029, -0.0044,  0.1192,  0.0439,  0.1932,  0.0082,  0.1160, -0.1654,\n",
      "        -0.0213, -0.1270, -0.0089, -0.0236, -0.0589,  0.1086,  0.1448, -0.0744,\n",
      "        -0.0636, -0.0634,  0.0920, -0.0227, -0.0138, -0.2378,  0.0915,  0.0410,\n",
      "        -0.0771, -0.2554, -0.0056, -0.0511,  0.0792, -0.1065,  0.0931,  0.0945,\n",
      "         0.1621, -0.0576,  0.0169,  0.0327,  0.0696,  0.0691, -0.0391,  0.1367,\n",
      "        -0.0409, -0.0418,  0.0063, -0.0193, -0.0210, -0.0267,  0.1050, -0.0354,\n",
      "         0.1055,  0.0792,  0.0318, -0.0377,  0.0276, -0.1005, -0.1378, -0.0189,\n",
      "         0.0850,  0.1300, -0.0152, -0.1383,  0.0798, -0.1142, -0.0370, -0.0458,\n",
      "        -0.1406,  0.0460, -0.0168,  0.1019,  0.0849,  0.0184,  0.0901, -0.1363,\n",
      "         0.2124,  0.0447, -0.1437,  0.0413, -0.3102,  0.0478,  0.0038, -0.0860,\n",
      "        -0.1465, -0.0472, -0.1059,  0.2646,  0.0621, -0.1125,  0.0021,  0.0192,\n",
      "        -0.0153, -0.1243, -0.0433, -0.1272, -0.0750,  0.0914, -0.1973,  0.0438],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([-4.2623e-02, -3.6569e-02,  2.1554e-01,  7.3948e-02,  4.5928e-02,\n",
      "         8.9498e-02,  5.3931e-02,  7.0713e-02, -1.2092e-01,  1.1116e-01,\n",
      "        -3.0323e-02, -1.2760e-01, -8.4349e-02,  8.3266e-02, -1.4023e-01,\n",
      "        -1.5514e-02, -1.0077e-01,  1.6821e-02,  1.4161e-01,  4.8444e-02,\n",
      "         2.6380e-01,  1.6819e-02,  2.7638e-02,  1.6771e-01, -4.9164e-02,\n",
      "        -5.8186e-02,  3.7733e-02, -7.5490e-02, -1.2715e-01,  6.3545e-03,\n",
      "         4.5001e-02, -1.1637e-01,  4.3808e-02, -6.4871e-02, -1.9854e-01,\n",
      "        -8.2011e-02,  4.6362e-02,  4.2026e-02,  3.5503e-02, -9.2668e-03,\n",
      "        -6.1567e-02, -7.3332e-02,  1.2077e-01,  9.6574e-03, -1.1398e-01,\n",
      "        -8.3217e-02,  9.7229e-02, -1.8468e-01,  1.0019e-01,  1.1664e-01,\n",
      "        -4.1808e-03, -1.5724e-01, -1.2025e-01, -4.9540e-02,  8.5170e-02,\n",
      "         7.6881e-02,  1.2248e-01, -4.9930e-02, -1.4443e-01,  1.7782e-01,\n",
      "         4.8178e-02,  5.4946e-02, -4.6922e-02,  2.5569e-02,  6.4497e-02,\n",
      "         6.3852e-03, -2.8890e-02, -2.0665e-02,  6.1656e-02, -8.2958e-03,\n",
      "        -1.1087e-01, -4.8137e-02,  1.0483e-01,  6.2579e-02, -1.6083e-04,\n",
      "        -1.1344e-01, -8.9015e-02, -1.7166e-02, -4.1521e-02, -1.5454e-01,\n",
      "        -1.4731e-01,  8.7373e-02, -2.9756e-02,  2.0193e-02, -1.1296e-01,\n",
      "        -3.1650e-02,  4.1894e-03, -1.8676e-01,  6.8287e-02, -2.2091e-02,\n",
      "        -1.2922e-01,  2.7839e-02, -1.4709e-01, -4.7553e-02, -3.7346e-03,\n",
      "         2.2710e-02,  2.5883e-02, -2.9356e-02,  1.6152e-01,  1.6246e-01,\n",
      "        -1.3543e-01,  8.9756e-02, -4.2627e-02, -8.3722e-02, -1.5255e-01,\n",
      "         2.2945e-01, -1.2727e-01, -2.3556e-01, -2.0420e-02,  1.3193e-01,\n",
      "         4.7064e-02,  1.3503e-01, -1.1775e-01,  4.4378e-02, -3.4163e-02,\n",
      "        -1.9214e-01,  1.5731e-01, -3.1321e-02,  6.0982e-02, -5.8376e-03,\n",
      "         1.5299e-01, -5.1013e-02, -1.3059e-01, -4.6080e-02, -4.5595e-02,\n",
      "         1.3429e-02, -6.2867e-02, -9.5215e-02, -1.7407e-02,  7.2585e-02,\n",
      "         3.3903e-02, -8.6348e-02, -1.3825e-01,  4.7451e-02, -5.0096e-02,\n",
      "        -3.1774e-02,  6.5457e-02,  1.4885e-01, -2.2025e-01, -4.0868e-02,\n",
      "         9.5003e-02, -2.9685e-02, -9.5620e-03, -1.2427e-01,  1.7506e-02,\n",
      "        -6.4477e-02,  7.1862e-02,  7.0507e-03, -5.8418e-02,  1.5294e-03,\n",
      "         3.2851e-02,  1.9095e-02, -8.0235e-02,  6.8884e-02,  6.0092e-02,\n",
      "         1.5271e-01,  1.9154e-01, -1.1346e-01,  6.4830e-02,  3.5148e-02,\n",
      "         1.3541e-01, -3.6145e-02,  7.7164e-02,  6.6006e-02, -9.8362e-02,\n",
      "        -5.8074e-02, -6.8333e-02, -6.2350e-02, -1.1440e-01, -7.7808e-02,\n",
      "        -5.0657e-02, -1.3394e-01,  7.3470e-03,  2.3200e-02,  5.6048e-02,\n",
      "        -1.1629e-01,  1.7295e-02, -3.8359e-02, -9.3535e-02,  5.3482e-02,\n",
      "         9.3346e-02, -5.7120e-03,  4.7397e-02, -1.3661e-01,  4.1543e-02,\n",
      "        -3.0960e-02, -2.6104e-02,  3.4355e-02, -8.6857e-02,  9.2401e-03,\n",
      "        -1.4995e-01,  1.1801e-01, -5.6550e-03, -9.9898e-02,  1.3610e-02,\n",
      "         1.8182e-02, -1.3491e-01, -8.2483e-02, -6.1044e-02, -5.1449e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-0.0404,  0.0334, -0.0873, -0.0599, -0.0711,  0.0188,  0.0374,  0.0498,\n",
      "         0.0180,  0.0348, -0.1043,  0.1819, -0.0947,  0.0222,  0.0732,  0.0492,\n",
      "         0.1486,  0.1599, -0.0004,  0.1500,  0.0163, -0.0051,  0.1059,  0.0772,\n",
      "         0.0216,  0.1549, -0.0047,  0.0367,  0.0610, -0.1326,  0.2364, -0.0735,\n",
      "        -0.1667,  0.0856,  0.0432, -0.1610, -0.1542,  0.0944, -0.0258,  0.0386,\n",
      "         0.0133,  0.0888,  0.0012, -0.1798,  0.0380, -0.0701,  0.0899, -0.0979,\n",
      "         0.1967, -0.0406, -0.0714, -0.1593,  0.0574,  0.0158, -0.1353, -0.0116,\n",
      "        -0.0659, -0.0594, -0.0836,  0.1687,  0.1432, -0.1235, -0.1411, -0.1314,\n",
      "        -0.0911,  0.2131, -0.1926, -0.0544, -0.1664,  0.0304,  0.1048, -0.0392,\n",
      "        -0.0422, -0.1233,  0.0076, -0.0738, -0.1557,  0.1152,  0.0373, -0.0577,\n",
      "         0.0090, -0.0523,  0.0164,  0.0149, -0.0101, -0.0796,  0.0258, -0.1029,\n",
      "         0.0345, -0.0131, -0.0666, -0.0779,  0.0692, -0.0341, -0.0940, -0.1161,\n",
      "        -0.0177,  0.1019, -0.1398,  0.0373, -0.1376, -0.0576,  0.0860, -0.0356,\n",
      "         0.1078,  0.0419,  0.2311, -0.0366,  0.1426, -0.1082, -0.0577, -0.0187,\n",
      "         0.0658, -0.1028, -0.0874, -0.0337,  0.1840,  0.0999,  0.2344,  0.0839,\n",
      "         0.1116,  0.0364,  0.0575,  0.0951,  0.0945,  0.0339, -0.0087,  0.0691,\n",
      "         0.0113,  0.0132, -0.0651,  0.0311, -0.0628,  0.0944, -0.0417,  0.1208,\n",
      "         0.0239,  0.1379,  0.1112,  0.1446, -0.1183, -0.0495, -0.0503, -0.1729,\n",
      "         0.0460, -0.1241, -0.0561,  0.0750, -0.1225, -0.0539,  0.2356,  0.0499,\n",
      "        -0.0831,  0.0315,  0.1233,  0.1682, -0.1815,  0.1077,  0.1187,  0.1555,\n",
      "         0.1233, -0.0816, -0.0952, -0.1361,  0.0105,  0.0832,  0.0652,  0.0702,\n",
      "         0.0459,  0.0453, -0.0204, -0.0968, -0.0660,  0.0714, -0.0433,  0.0744,\n",
      "        -0.0709,  0.0625,  0.0672, -0.1082, -0.0944,  0.1826,  0.0588,  0.0226,\n",
      "         0.1245, -0.0560,  0.1531,  0.0472,  0.0311, -0.0327, -0.1191,  0.0361,\n",
      "        -0.1240,  0.0982,  0.0693,  0.0030, -0.1002,  0.2047,  0.0471, -0.0009],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 46.64 seconds, 0.78 minutes\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.77 seconds, 0.75 minutes\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.42 seconds, 0.74 minutes\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 45.52 seconds, 0.76 minutes\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 46.37 seconds, 0.77 minutes\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.11 seconds, 0.74 minutes\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 45.28 seconds, 0.75 minutes\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.11 seconds, 0.74 minutes\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.59 seconds, 0.74 minutes\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.78 seconds, 0.75 minutes\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.95 seconds, 0.73 minutes\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.60 seconds, 0.73 minutes\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.62 seconds, 0.73 minutes\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.26 seconds, 0.70 minutes\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.88 seconds, 0.71 minutes\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.30 seconds, 0.72 minutes\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.85 seconds, 0.73 minutes\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.70 seconds, 0.73 minutes\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.21 seconds, 0.72 minutes\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.95 seconds, 0.72 minutes\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.11 seconds, 0.70 minutes\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.30 seconds, 0.69 minutes\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.90 seconds, 0.68 minutes\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.30 seconds, 0.72 minutes\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.17 seconds, 0.69 minutes\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.79 seconds, 0.70 minutes\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.28 seconds, 0.69 minutes\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.61 seconds, 0.71 minutes\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.79 seconds, 0.70 minutes\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.52 seconds, 0.69 minutes\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.50 seconds, 0.69 minutes\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 46.63 seconds, 0.78 minutes\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.40 seconds, 0.69 minutes\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.52 seconds, 0.69 minutes\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 48.58 seconds, 0.81 minutes\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.12 seconds, 0.69 minutes\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.02 seconds, 0.68 minutes\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 46.72 seconds, 0.78 minutes\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.52 seconds, 0.69 minutes\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.57 seconds, 0.71 minutes\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.69 seconds, 0.68 minutes\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.32 seconds, 0.71 minutes\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.43 seconds, 0.69 minutes\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.57 seconds, 0.69 minutes\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.83 seconds, 0.68 minutes\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.76 seconds, 0.71 minutes\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.03 seconds, 0.68 minutes\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.49 seconds, 0.69 minutes\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.85 seconds, 0.68 minutes\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.36 seconds, 0.71 minutes\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.24 seconds, 0.74 minutes\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 45.33 seconds, 0.76 minutes\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.57 seconds, 0.69 minutes\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.03 seconds, 0.68 minutes\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.02 seconds, 0.70 minutes\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.04 seconds, 0.73 minutes\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.58 seconds, 0.69 minutes\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.57 seconds, 0.69 minutes\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 39.30 seconds, 0.65 minutes\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.63 seconds, 0.69 minutes\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.87 seconds, 0.68 minutes\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.19 seconds, 0.69 minutes\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.12 seconds, 0.69 minutes\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.92 seconds, 0.70 minutes\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.29 seconds, 0.72 minutes\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.71 seconds, 0.70 minutes\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.74 seconds, 0.70 minutes\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.05 seconds, 0.70 minutes\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.83 seconds, 0.70 minutes\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.34 seconds, 0.71 minutes\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.16 seconds, 0.69 minutes\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.10 seconds, 0.69 minutes\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.37 seconds, 0.69 minutes\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 39.90 seconds, 0.66 minutes\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.71 seconds, 0.68 minutes\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.17 seconds, 0.69 minutes\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.84 seconds, 0.68 minutes\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.40 seconds, 0.71 minutes\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.23 seconds, 0.69 minutes\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.72 seconds, 0.70 minutes\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.34 seconds, 0.69 minutes\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.89 seconds, 0.70 minutes\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 47.99 seconds, 0.80 minutes\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.80 seconds, 0.71 minutes\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.96 seconds, 0.72 minutes\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.36 seconds, 0.69 minutes\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.96 seconds, 0.72 minutes\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.61 seconds, 0.73 minutes\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.11 seconds, 0.70 minutes\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.16 seconds, 0.70 minutes\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.69 seconds, 0.73 minutes\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.93 seconds, 0.72 minutes\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.24 seconds, 0.70 minutes\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.70 seconds, 0.73 minutes\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.18 seconds, 0.70 minutes\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 45.70 seconds, 0.76 minutes\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.43 seconds, 0.72 minutes\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.98 seconds, 0.70 minutes\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.07 seconds, 0.73 minutes\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.04 seconds, 0.73 minutes\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.62 seconds, 0.73 minutes\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.29 seconds, 0.72 minutes\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.61 seconds, 0.71 minutes\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.40 seconds, 0.72 minutes\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.97 seconds, 0.72 minutes\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.03 seconds, 0.72 minutes\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 45.31 seconds, 0.76 minutes\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.35 seconds, 0.72 minutes\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.09 seconds, 0.72 minutes\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.43 seconds, 0.72 minutes\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.23 seconds, 0.74 minutes\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.03 seconds, 0.72 minutes\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.66 seconds, 0.71 minutes\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.98 seconds, 0.73 minutes\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.07 seconds, 0.70 minutes\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.79 seconds, 0.73 minutes\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.70 seconds, 0.75 minutes\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.58 seconds, 0.71 minutes\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.52 seconds, 0.71 minutes\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.33 seconds, 0.71 minutes\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.24 seconds, 0.72 minutes\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.69 seconds, 0.71 minutes\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.68 seconds, 0.69 minutes\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.36 seconds, 0.71 minutes\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.77 seconds, 0.73 minutes\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.47 seconds, 0.71 minutes\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.48 seconds, 0.72 minutes\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.24 seconds, 0.74 minutes\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.48 seconds, 0.72 minutes\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.78 seconds, 0.70 minutes\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.46 seconds, 0.72 minutes\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 41.72 seconds, 0.70 minutes\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.38 seconds, 0.72 minutes\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 42.48 seconds, 0.71 minutes\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 46.46 seconds, 0.77 minutes\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 47.42 seconds, 0.79 minutes\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.40 seconds, 0.72 minutes\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 48.06 seconds, 0.80 minutes\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.30 seconds, 0.74 minutes\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.39 seconds, 0.72 minutes\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.89 seconds, 0.75 minutes\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.51 seconds, 0.73 minutes\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.39 seconds, 0.74 minutes\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.22 seconds, 0.74 minutes\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.78 seconds, 0.73 minutes\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 45.60 seconds, 0.76 minutes\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 45.05 seconds, 0.75 minutes\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 45.74 seconds, 0.76 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-184 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 45.64 seconds, 0.76 minutes\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.34 seconds, 0.72 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: context deadline exceeded (<Response [500]>)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (HTTPError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-187 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.19 seconds, 0.74 minutes\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 46.97 seconds, 0.78 minutes\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 47.34 seconds, 0.79 minutes\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.70 seconds, 0.75 minutes\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 43.59 seconds, 0.73 minutes\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.45 seconds, 0.74 minutes\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 44.04 seconds, 0.73 minutes\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  2.302601/  2.302585, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 45.34 seconds, 0.76 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9667c7130c4b9fa464208d4f244915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.09908</td></tr><tr><td>tr_epoch_loss</td><td>2.3026</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>2.30259</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-sweep-309</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xtm2ue6f' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xtm2ue6f</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251111_124815-xtm2ue6f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hvrtehxc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [512]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 5000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_threshold: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: one\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b9b7572c9c40f3aca624a9303cf743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112735027240382, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251111_151204-hvrtehxc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hvrtehxc' target=\"_blank\">flowing-sweep-316</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hvrtehxc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hvrtehxc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_threshold' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': '20251111_151214_690', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.75, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 0.5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.75, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [512], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'one', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'output_threshold': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 7a22c8a0ef5b9b252dbf98632e270efd\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=512, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.0625, v_reset=10000, sg_width=0.5, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=False, ANPI_MODE=True)\n",
      "      (4): SYNAPSE_FC(in_features=512, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient(single_step=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 506,880\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.0001\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "inFeed spike.shape torch.Size([10, 512]) self.weight_fb.shape torch.Size([10, 512])\n",
      "self.weight_fb[0] tensor([ 1.2009e-02,  1.3379e-01, -1.0650e-02,  5.2556e-02, -1.1912e-01,\n",
      "         4.0419e-02, -4.0199e-02, -5.0604e-02,  3.2680e-02, -7.8942e-02,\n",
      "        -1.0288e-01, -1.8775e-02, -5.7299e-03,  1.2332e-02, -6.9353e-02,\n",
      "         1.1499e-01, -4.4228e-02,  4.2593e-02,  4.9323e-02, -2.0675e-03,\n",
      "         9.2336e-02, -3.1971e-02, -1.5728e-02,  9.1276e-02, -2.0181e-02,\n",
      "        -7.1800e-02,  1.4578e-01, -4.2861e-02,  1.1373e-02, -7.3257e-02,\n",
      "        -1.1159e-01, -9.7846e-02,  5.1912e-02,  8.7845e-02,  4.0044e-02,\n",
      "         2.6324e-02, -9.8372e-02,  3.8522e-02,  1.0460e-01, -4.1150e-02,\n",
      "         5.8342e-02,  4.8482e-03,  5.2401e-03, -8.7172e-03,  2.0523e-02,\n",
      "        -3.6457e-02, -6.6373e-02,  5.9048e-03, -2.0717e-02, -3.2546e-02,\n",
      "        -5.4324e-02,  2.4378e-02,  1.0149e-02, -1.2236e-02,  6.2543e-02,\n",
      "        -8.3454e-02, -2.1650e-02, -3.9879e-02,  2.7655e-02, -3.3246e-02,\n",
      "         7.6898e-02, -5.0422e-02,  1.5484e-02, -2.6447e-02,  6.8359e-02,\n",
      "        -6.8262e-02,  3.4312e-02, -7.9518e-02, -2.3619e-02,  3.1812e-02,\n",
      "         6.2016e-03,  1.6009e-02,  2.2387e-02,  1.4105e-01,  1.4450e-03,\n",
      "         9.7970e-02, -7.1751e-02,  5.8704e-02, -2.8309e-02,  4.7077e-02,\n",
      "        -3.5820e-02, -4.3640e-02, -4.4777e-02, -3.1386e-02, -2.7226e-02,\n",
      "        -2.5884e-02,  1.0779e-02,  2.7401e-02,  3.1376e-02, -7.5319e-02,\n",
      "        -1.6829e-02,  1.7118e-02, -8.9122e-02, -4.0006e-02,  4.6343e-03,\n",
      "         1.2001e-02,  3.6892e-02,  1.4373e-02,  7.0655e-02, -4.2197e-02,\n",
      "        -1.0233e-01,  3.7360e-04,  8.5512e-02,  7.8637e-02,  1.4384e-03,\n",
      "        -8.0477e-02, -4.6482e-02,  2.3251e-02, -3.3886e-02, -2.4537e-03,\n",
      "        -4.8149e-02, -1.5486e-01,  4.3330e-02, -5.8045e-03, -1.3386e-02,\n",
      "         2.7755e-02, -1.9510e-02,  1.3393e-03,  3.8708e-02,  1.5263e-02,\n",
      "         4.6335e-02, -7.2374e-03, -6.3238e-03, -3.1016e-02, -3.1252e-02,\n",
      "        -7.4723e-02, -1.5088e-02, -4.1994e-02,  1.2212e-02,  6.0550e-02,\n",
      "        -1.7745e-03,  1.0415e-01,  6.7522e-02, -6.1409e-02, -4.1550e-02,\n",
      "         1.0644e-01,  1.5230e-01, -3.8367e-02,  7.8697e-02, -1.7323e-02,\n",
      "         2.6986e-02,  2.6370e-02,  6.5894e-02, -1.2553e-01, -3.9156e-02,\n",
      "         1.3065e-01, -5.8646e-03,  1.4600e-02, -4.5190e-02, -1.0434e-01,\n",
      "         5.6415e-02,  4.8810e-02, -3.8917e-02,  1.3367e-01,  7.2065e-02,\n",
      "        -2.6348e-02,  1.4814e-02, -7.9086e-02, -7.4679e-03, -3.7547e-02,\n",
      "        -4.9995e-02,  1.3292e-04, -1.2034e-02,  4.6384e-02,  5.0249e-02,\n",
      "         5.1038e-02, -3.7747e-02,  8.0393e-02, -6.6428e-02, -1.4425e-03,\n",
      "        -2.2637e-02, -3.0118e-02,  9.2677e-03, -9.3434e-02,  1.9207e-02,\n",
      "        -2.7770e-02, -6.7883e-02, -7.8605e-02, -9.7644e-02, -9.8327e-02,\n",
      "        -4.0612e-02,  4.7043e-02, -3.7591e-02,  1.8712e-02, -8.3181e-02,\n",
      "        -1.9715e-02,  3.6721e-02,  3.5419e-02, -4.6781e-02, -7.8367e-03,\n",
      "        -2.6748e-02, -8.6308e-02,  2.3989e-02, -1.2710e-02,  3.7118e-02,\n",
      "        -6.2088e-02, -2.2962e-04, -4.9640e-02,  2.4384e-02,  1.5691e-01,\n",
      "         1.5421e-02,  5.5528e-02,  4.8312e-02,  5.6640e-02, -2.2735e-02,\n",
      "         5.3113e-03, -5.2211e-02,  2.6325e-02,  6.9295e-02,  2.4738e-02,\n",
      "        -5.3518e-03,  5.2276e-02, -2.4634e-02, -5.3242e-03,  1.2084e-01,\n",
      "        -2.6133e-02,  3.3964e-02,  9.2582e-03, -1.2223e-01, -2.1360e-03,\n",
      "        -7.8244e-02, -1.5748e-02,  1.4439e-03,  1.2431e-01,  6.0634e-02,\n",
      "         8.5934e-02, -6.0989e-02, -2.9897e-02, -1.1970e-03, -1.0762e-01,\n",
      "         1.0423e-02,  1.6176e-02, -1.3812e-02, -5.2755e-02,  1.6920e-02,\n",
      "         6.1367e-02,  9.1813e-02,  2.1540e-02,  7.7856e-03, -4.0828e-02,\n",
      "        -9.7598e-02, -4.1089e-02,  9.0935e-02,  1.8519e-02, -3.4424e-02,\n",
      "         2.8530e-03, -6.6620e-02, -8.9594e-03, -6.7013e-03, -4.6130e-02,\n",
      "        -2.1535e-02,  5.8145e-03,  4.0000e-03, -5.7107e-02,  4.8855e-02,\n",
      "        -1.1148e-01, -1.1978e-01,  6.8131e-02,  1.5512e-03,  3.5912e-02,\n",
      "         3.3328e-02,  3.1726e-02, -8.8611e-02,  1.4725e-01, -9.5569e-02,\n",
      "        -1.0785e-02, -1.3891e-03,  1.3467e-02,  4.0348e-02,  9.6515e-02,\n",
      "         1.6649e-02,  3.0992e-02, -1.5092e-02, -5.3478e-02,  2.6478e-02,\n",
      "        -1.3042e-02, -9.5301e-02, -6.6575e-03, -1.5733e-03, -9.9895e-03,\n",
      "         3.4082e-02,  1.5740e-01, -9.9586e-03, -5.3744e-02,  8.7394e-02,\n",
      "         4.2685e-02,  5.2481e-02,  1.7623e-02,  1.0548e-03,  4.5100e-02,\n",
      "         7.4265e-02, -7.1658e-03, -8.7438e-02, -3.9754e-02,  5.4727e-02,\n",
      "         4.6412e-02,  4.2058e-02, -3.2855e-02, -1.1088e-01, -1.7722e-02,\n",
      "         4.9851e-03, -8.0476e-02,  8.2968e-02, -8.2024e-02,  1.6164e-02,\n",
      "         3.7377e-02, -9.2349e-02, -1.1127e-01,  6.9750e-02,  8.6820e-02,\n",
      "        -2.7057e-02, -2.3069e-02, -7.3103e-02, -1.6484e-01, -2.0014e-02,\n",
      "         6.3153e-03,  7.7782e-02, -8.4823e-02,  2.2121e-02,  1.0625e-01,\n",
      "        -1.4292e-01,  8.1527e-02, -7.1087e-02, -8.0429e-02, -4.0732e-03,\n",
      "         6.4006e-02, -1.4278e-01, -7.9276e-03,  5.2838e-02, -3.7510e-03,\n",
      "        -5.9070e-02, -1.1084e-01, -1.6297e-03,  5.6736e-03, -7.3166e-02,\n",
      "        -6.8036e-02,  1.5117e-01,  1.9150e-02, -9.3975e-02, -4.8127e-02,\n",
      "         4.4899e-02,  5.5049e-02,  6.3477e-02,  5.0466e-02,  1.4346e-01,\n",
      "        -1.4061e-02,  1.8790e-01,  3.4009e-02,  1.4160e-03, -2.5282e-02,\n",
      "        -1.6245e-02,  5.4068e-02, -7.5012e-02, -7.5148e-02, -1.8582e-02,\n",
      "        -2.3466e-02,  1.9578e-02, -6.2413e-02,  1.2314e-01,  1.3701e-02,\n",
      "        -5.7122e-03,  8.9041e-02,  3.7946e-02,  4.1243e-02,  4.7171e-02,\n",
      "         2.7039e-02, -5.9925e-03, -2.8245e-02, -7.2878e-02,  1.4521e-02,\n",
      "         9.9702e-02,  6.4296e-02,  7.4185e-02, -7.1993e-02,  1.4546e-02,\n",
      "         7.7495e-02, -9.2409e-03, -3.8808e-02,  7.1566e-02, -1.4977e-01,\n",
      "         4.2293e-02, -4.2540e-02, -5.6876e-03, -4.4148e-02, -8.0183e-02,\n",
      "         7.5278e-02, -2.9656e-03, -4.9337e-02,  2.6277e-02, -1.1994e-02,\n",
      "        -9.6900e-03, -8.8157e-03, -1.7625e-02, -8.9690e-02, -3.2884e-02,\n",
      "        -5.1021e-03, -1.0199e-01, -1.6831e-02,  1.1726e-01, -3.4447e-02,\n",
      "        -2.8511e-02, -1.9198e-02,  3.6576e-03,  3.2099e-02,  4.5579e-03,\n",
      "         8.7041e-02, -3.0138e-02,  1.8212e-02,  7.4119e-02, -1.3839e-02,\n",
      "         5.3415e-02,  2.2786e-02,  1.0557e-01, -5.6927e-02,  3.3285e-02,\n",
      "         7.3276e-02,  1.0244e-01, -1.4565e-02, -1.0259e-01,  1.2200e-01,\n",
      "         6.1812e-02,  4.8889e-02, -5.6486e-02,  5.1047e-02,  9.3909e-02,\n",
      "        -1.0201e-02,  6.4712e-02, -2.3649e-02,  3.8729e-02,  6.1245e-03,\n",
      "        -4.3430e-02,  6.4039e-03, -8.9212e-02,  1.5119e-01,  7.2071e-02,\n",
      "         1.5732e-02, -2.2774e-02,  5.2327e-02,  2.5401e-02,  2.9843e-02,\n",
      "        -1.1558e-01,  5.9937e-02, -5.8328e-02,  7.1370e-02,  4.9816e-02,\n",
      "         6.5657e-02,  3.2430e-02, -8.6861e-03,  8.5977e-02,  1.9082e-02,\n",
      "         2.7206e-02, -1.9106e-03, -6.5907e-02,  4.0442e-03,  1.7387e-02,\n",
      "         1.3066e-01, -8.5428e-02, -2.6442e-02,  5.6974e-02, -8.7909e-02,\n",
      "         3.4048e-02, -5.8666e-02,  1.8037e-02, -6.2223e-02, -1.8848e-02,\n",
      "         9.5296e-03, -5.1592e-03,  5.1242e-03,  9.5190e-02,  1.1389e-02,\n",
      "        -6.1644e-02,  2.7198e-02,  2.2262e-02, -4.7755e-02,  6.3539e-03,\n",
      "        -2.4203e-02,  1.3476e-02,  5.5816e-02,  3.3884e-02,  5.4144e-02,\n",
      "        -2.0123e-02, -2.5729e-02,  3.2092e-02, -3.4289e-02, -1.2439e-03,\n",
      "         1.8775e-01,  5.8437e-02,  1.8716e-02, -5.8857e-02, -6.8036e-02,\n",
      "        -5.9856e-04,  1.0747e-01, -7.1370e-02,  1.3296e-03, -3.0167e-02,\n",
      "        -5.6810e-02, -1.0447e-01, -8.7226e-03, -3.1270e-03,  1.2601e-02,\n",
      "         1.8155e-02, -9.4597e-02, -4.7340e-02,  2.7440e-02, -3.4883e-02,\n",
      "        -3.2968e-02, -6.2905e-02, -1.2657e-02,  3.2411e-02,  1.2026e-02,\n",
      "         2.2878e-02, -5.3231e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 6.6658e-02, -7.8302e-02, -3.9761e-02, -4.1793e-02,  4.5831e-02,\n",
      "         4.8306e-02, -6.7736e-03,  7.5574e-02, -7.4495e-02, -3.0042e-02,\n",
      "         5.2244e-03, -1.3071e-02, -5.5794e-03, -8.3971e-02, -6.9471e-03,\n",
      "        -2.4258e-02,  1.0854e-01, -6.1369e-02, -1.4674e-01,  1.1226e-01,\n",
      "        -6.0065e-02,  5.3451e-02,  1.1262e-01, -4.9005e-03,  1.5264e-01,\n",
      "         7.8240e-02,  3.1867e-02,  7.0535e-03, -8.8613e-02, -1.6180e-02,\n",
      "         7.1920e-03,  3.6067e-02, -1.8580e-02, -6.9305e-02,  5.7444e-02,\n",
      "        -9.3223e-02,  6.4325e-02, -1.2735e-01, -1.6280e-02, -5.1730e-02,\n",
      "        -1.6762e-02,  1.6986e-01,  2.8526e-02,  7.5887e-02,  4.1897e-03,\n",
      "         5.6685e-02,  4.6633e-02, -3.6862e-02, -3.9126e-02, -2.2331e-02,\n",
      "         9.3762e-02, -1.0613e-02,  1.1766e-01, -3.7826e-02,  6.4190e-02,\n",
      "         2.1247e-02, -9.1414e-03,  9.0567e-02, -1.1170e-01,  1.5015e-02,\n",
      "        -1.6912e-02,  1.8269e-02, -6.4949e-02, -5.4902e-02, -8.6944e-03,\n",
      "         1.3896e-01,  1.1010e-01,  1.0749e-02,  8.7195e-02, -6.8369e-03,\n",
      "        -3.5939e-02,  1.3870e-02,  5.9698e-02, -8.9737e-05,  8.3753e-02,\n",
      "        -4.8358e-03, -3.8847e-02, -1.0107e-01,  7.5683e-02, -1.1180e-01,\n",
      "         3.0140e-02, -4.3089e-02, -2.2418e-02, -3.6128e-02, -1.0527e-01,\n",
      "         2.2898e-02,  4.6009e-02, -7.4225e-03, -5.6874e-02,  8.5350e-02,\n",
      "         5.1923e-03,  2.5627e-02, -8.9285e-03, -5.8058e-02,  7.0525e-02,\n",
      "         3.8854e-02,  2.7697e-02,  1.4393e-01, -4.0282e-02,  2.0928e-02,\n",
      "        -2.4592e-02,  6.1504e-02,  8.4973e-02, -6.5030e-03, -1.1406e-02,\n",
      "        -1.5721e-01, -1.2213e-01, -3.2998e-02, -1.0606e-02,  1.5931e-01,\n",
      "         1.4261e-01,  2.5770e-02, -4.0473e-02, -6.6654e-02,  3.4934e-02,\n",
      "         9.9253e-02, -1.0173e-02, -1.4505e-02,  6.1864e-02,  4.7759e-02,\n",
      "        -1.6578e-02,  3.0713e-02,  1.4806e-02,  8.6155e-02, -1.2338e-02,\n",
      "         7.9021e-02, -7.8331e-02, -6.0098e-02,  7.8730e-02,  2.3303e-02,\n",
      "        -8.3858e-03,  4.4462e-02, -5.4935e-02,  4.2922e-02,  4.7366e-02,\n",
      "        -3.2290e-04,  1.8469e-02, -5.9237e-02,  6.0935e-02,  2.3421e-02,\n",
      "         7.0576e-02, -1.8194e-02,  5.7329e-03,  1.2694e-01, -1.6639e-02,\n",
      "         5.9829e-02, -7.5157e-02, -6.8489e-02, -1.1888e-01, -1.4575e-01,\n",
      "        -6.2740e-03,  8.6623e-02, -1.9370e-03, -1.2883e-01,  4.0742e-02,\n",
      "        -3.1368e-02, -6.8863e-03,  6.7565e-03, -5.5464e-02, -5.8365e-02,\n",
      "        -4.6925e-02, -1.8427e-03, -6.9821e-03, -5.4991e-02,  1.4936e-02,\n",
      "        -6.0094e-02,  2.1199e-02,  1.6101e-03, -6.6419e-02, -1.0129e-01,\n",
      "         3.2519e-04, -9.6969e-02,  2.2424e-02,  8.3956e-02, -1.0915e-01,\n",
      "        -5.2411e-02,  7.9012e-02,  7.7652e-02,  7.2692e-02,  5.3036e-02,\n",
      "         8.0605e-03,  1.2090e-01,  4.4321e-02, -1.3145e-02,  2.7608e-02,\n",
      "        -2.4626e-03, -8.6162e-02, -2.0906e-02, -8.0314e-02,  8.6478e-02,\n",
      "         3.2060e-02, -7.4949e-02, -4.5875e-02, -9.1144e-02,  8.5149e-02,\n",
      "         4.7841e-02, -5.8479e-02,  9.3823e-02, -8.9949e-02, -2.2137e-03,\n",
      "         5.3320e-02,  2.4241e-02,  7.6287e-02, -7.3501e-02,  5.9457e-02,\n",
      "         2.5991e-02, -4.9862e-02,  2.1058e-02,  3.7085e-02,  5.8227e-02,\n",
      "         1.6736e-02,  1.3518e-02, -3.6454e-02,  8.9511e-02, -6.0161e-02,\n",
      "         4.3647e-02,  2.5404e-02,  1.6810e-03, -3.8325e-02,  5.1655e-02,\n",
      "        -6.2435e-03, -7.4342e-02,  1.5280e-02, -3.8896e-02, -4.6945e-02,\n",
      "        -4.9156e-02,  5.0480e-02, -1.1144e-01,  4.6365e-02,  4.1312e-02,\n",
      "         4.3370e-02, -6.4439e-02,  1.4321e-01,  5.6491e-03,  4.6217e-02,\n",
      "        -7.8084e-02,  2.2043e-02,  2.4072e-02, -1.1090e-01, -5.7180e-02,\n",
      "         1.3553e-01,  2.0576e-03, -6.7463e-02, -3.7952e-02,  9.7044e-02,\n",
      "         3.9006e-02,  2.3112e-02,  3.6162e-02, -4.4879e-02, -5.0205e-02,\n",
      "        -6.6276e-02,  6.0393e-02, -1.6587e-02, -4.2223e-02,  4.9360e-02,\n",
      "        -5.2514e-02,  5.3070e-02,  3.0898e-02,  8.4096e-03,  4.2029e-02,\n",
      "         8.3128e-03,  7.7944e-02,  7.4944e-02,  3.7365e-02, -1.7412e-02,\n",
      "        -1.7034e-02, -5.1705e-02, -1.0178e-01,  8.1377e-03, -1.1124e-02,\n",
      "         6.0315e-02, -1.2464e-01, -8.2909e-02, -2.0721e-02,  1.5134e-01,\n",
      "        -7.6029e-03, -5.5703e-02,  1.3161e-01,  1.1009e-01,  8.7843e-02,\n",
      "        -1.1565e-02, -7.0188e-02, -1.7204e-01,  9.7961e-02,  1.4806e-01,\n",
      "        -4.5438e-02, -2.6664e-03, -4.6997e-02, -7.0638e-02, -7.9939e-02,\n",
      "        -7.0988e-02, -1.1400e-01, -7.8130e-03, -8.5862e-02, -3.9800e-02,\n",
      "         7.1482e-03, -1.3455e-01, -2.8474e-02, -8.3467e-02,  6.1789e-02,\n",
      "        -1.2440e-02, -1.4384e-01, -5.4934e-02,  1.7171e-02, -4.3710e-02,\n",
      "         5.2462e-03, -9.8457e-02,  6.4931e-02,  3.0336e-02, -8.2045e-03,\n",
      "        -2.1457e-02,  1.9863e-02, -3.9212e-02,  3.6250e-02, -2.9250e-02,\n",
      "         4.0146e-03,  9.8803e-02, -3.5044e-03, -1.3867e-01,  6.7823e-02,\n",
      "        -1.1386e-02,  4.5815e-02, -4.6995e-02, -6.0331e-02,  8.9048e-02,\n",
      "        -3.3910e-03,  5.5142e-02,  1.0962e-01,  7.8482e-02, -5.7451e-02,\n",
      "         6.7650e-02, -5.0193e-02, -1.0531e-01,  3.0873e-02,  4.0250e-02,\n",
      "         3.5226e-02,  3.5651e-02, -1.3163e-02, -1.5697e-02, -1.3301e-02,\n",
      "        -7.5622e-02,  4.6634e-02, -6.0863e-02,  1.1601e-02,  5.8555e-02,\n",
      "         1.9718e-02,  1.4490e-02,  4.6890e-02,  1.9770e-02,  1.8599e-02,\n",
      "         1.5324e-02,  9.0858e-02, -9.4841e-02,  4.4712e-02,  1.0196e-01,\n",
      "         7.1711e-02,  2.8857e-02, -7.6147e-02,  1.1056e-01,  3.8540e-02,\n",
      "        -7.5464e-02, -1.1109e-01,  1.1038e-02,  7.1191e-02,  3.8999e-02,\n",
      "         8.1577e-02,  1.4265e-01, -2.5305e-02,  7.0406e-02, -2.0950e-01,\n",
      "        -1.0905e-01, -7.9404e-02,  9.4908e-02, -6.2777e-02, -4.6448e-02,\n",
      "         6.7760e-02, -4.1111e-02, -3.0499e-02, -6.7737e-02, -1.6252e-02,\n",
      "         7.7219e-02, -9.5822e-02,  7.5935e-03, -2.3492e-02, -3.9966e-02,\n",
      "         2.2348e-02, -5.5910e-02, -2.2430e-02, -1.2789e-01,  1.1506e-02,\n",
      "        -3.6499e-02, -2.3789e-02,  8.8967e-02,  3.7748e-04,  1.4302e-01,\n",
      "        -3.3631e-02, -3.5510e-02, -1.5043e-01,  7.7718e-02,  1.4879e-01,\n",
      "         6.6394e-02, -1.8917e-02,  1.0423e-02, -4.4962e-03, -2.3098e-02,\n",
      "         8.4583e-02,  1.2187e-01,  2.5955e-02,  2.3483e-02, -1.2860e-01,\n",
      "         2.7167e-02,  3.6408e-02,  8.3306e-02,  1.1587e-01,  6.6651e-02,\n",
      "         5.9024e-02,  1.0206e-01, -6.6102e-02, -1.1416e-02,  6.7382e-02,\n",
      "        -1.8530e-01,  7.1940e-02, -3.7391e-02, -1.0281e-01,  5.0257e-02,\n",
      "         4.7398e-02,  2.7898e-02,  6.5546e-02, -3.5585e-02, -1.5329e-02,\n",
      "        -3.8707e-02, -5.4844e-02, -2.3227e-02,  3.0108e-02, -2.5781e-02,\n",
      "        -2.8408e-02,  3.9738e-03,  9.0303e-02,  8.2566e-03,  2.2979e-02,\n",
      "        -5.5796e-02, -3.8515e-02, -6.0057e-02,  7.1408e-02, -6.8506e-02,\n",
      "        -8.3587e-02, -1.1510e-01,  3.3540e-02, -1.6315e-02, -4.7617e-02,\n",
      "        -1.2741e-01, -2.6345e-02, -6.0932e-02, -2.5297e-02,  1.7280e-03,\n",
      "        -5.4365e-02, -5.7350e-02, -4.4366e-02, -1.8187e-02, -5.9762e-02,\n",
      "         1.8093e-02, -6.1407e-02,  1.3368e-01,  3.7309e-02, -2.3302e-02,\n",
      "        -3.6866e-02,  6.9024e-03,  7.7365e-03,  4.0508e-02, -2.5169e-02,\n",
      "        -8.2504e-02,  1.2014e-01, -6.4195e-02,  6.6726e-02,  1.5957e-02,\n",
      "         1.0247e-01,  9.6323e-02,  5.0310e-02, -7.1386e-02, -6.2054e-03,\n",
      "        -1.6760e-01,  3.7466e-03, -9.4249e-02,  7.7653e-02, -1.2555e-01,\n",
      "        -6.1608e-02, -2.9333e-02,  1.3478e-02, -1.4650e-02, -9.3798e-02,\n",
      "         6.4758e-02,  2.1284e-02,  1.5329e-01, -8.6474e-02, -5.4156e-03,\n",
      "        -2.4129e-02,  1.0983e-01, -2.6136e-02,  1.7877e-02,  7.2377e-02,\n",
      "         2.4865e-02,  5.1694e-02,  5.9210e-02,  1.3274e-01, -4.0805e-02,\n",
      "         2.4143e-02,  6.7355e-02,  6.0903e-02,  6.5552e-02,  1.7681e-01,\n",
      "         4.1771e-02,  1.2728e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 5.0966e-02, -1.4745e-01,  7.7494e-02,  1.4598e-02,  1.1066e-01,\n",
      "        -3.6061e-02, -3.4456e-02,  2.3449e-03,  3.6120e-02, -2.1529e-02,\n",
      "         1.0209e-01,  1.2287e-03, -5.0131e-02,  6.2569e-02, -2.0442e-02,\n",
      "         3.2035e-02,  6.1605e-02, -9.9639e-02,  1.5433e-02,  3.8132e-02,\n",
      "        -6.6866e-02, -6.3091e-02, -6.1747e-02,  6.8062e-02,  8.8035e-02,\n",
      "        -1.0674e-01,  5.1352e-02, -3.5963e-02, -4.7417e-03, -4.0600e-03,\n",
      "        -1.0709e-01, -8.8151e-02,  1.0923e-01, -5.1789e-02, -1.1943e-01,\n",
      "        -3.2427e-02,  8.7168e-02,  1.1600e-01, -3.1433e-02,  2.1007e-02,\n",
      "        -2.0211e-02,  5.1138e-02, -3.1195e-02, -1.7929e-02,  1.6682e-02,\n",
      "        -5.8549e-03, -3.0055e-02, -1.2022e-01,  4.2940e-02,  5.0219e-03,\n",
      "        -7.6352e-02,  1.2055e-02,  1.1379e-02,  7.7296e-02, -3.7195e-02,\n",
      "         6.2380e-02, -9.9886e-02,  1.3775e-02, -3.7782e-02, -8.0343e-03,\n",
      "         1.1148e-02, -1.7144e-02, -8.2952e-02,  6.2111e-02,  1.4023e-02,\n",
      "         9.3064e-02, -1.8222e-02,  8.8978e-02, -9.5613e-02,  5.1005e-02,\n",
      "         6.4407e-02, -1.5327e-02, -1.6592e-02, -4.5361e-02, -3.1602e-02,\n",
      "        -4.6708e-02, -4.0381e-02,  9.3572e-02,  1.4583e-02,  1.5900e-02,\n",
      "         5.2908e-02, -6.2023e-02,  9.5726e-02, -2.2317e-02, -1.0207e-02,\n",
      "        -8.4064e-02, -8.5376e-02,  1.4583e-02,  6.5636e-02,  8.2487e-02,\n",
      "         6.9251e-02, -3.3851e-03,  2.0579e-02, -6.4329e-03, -6.3405e-03,\n",
      "         2.8375e-02, -5.4557e-02,  4.9721e-02, -2.8327e-02,  7.1326e-02,\n",
      "        -2.7338e-02,  7.1745e-02,  2.0902e-02, -1.4693e-02, -6.4021e-03,\n",
      "        -3.6755e-02,  2.3320e-02, -1.8848e-02, -8.2152e-03, -7.3774e-02,\n",
      "        -6.4569e-02, -3.3738e-02,  2.3054e-02, -1.0855e-02,  3.3617e-02,\n",
      "         5.3611e-02, -6.7952e-02, -5.8561e-02, -4.5781e-02,  2.4040e-02,\n",
      "        -8.8937e-02,  3.5465e-02,  5.0535e-02,  2.5044e-02, -4.3513e-03,\n",
      "        -3.2971e-02, -1.3832e-01, -8.0301e-02,  1.5525e-01, -8.0106e-02,\n",
      "         2.0949e-02,  1.1226e-02,  5.7637e-02,  9.5634e-02, -4.6271e-02,\n",
      "         6.2753e-02, -4.8439e-02,  5.5866e-02, -5.6149e-02,  8.9882e-03,\n",
      "        -2.2475e-02,  2.6102e-03, -7.5365e-02, -3.5781e-02,  8.7820e-03,\n",
      "        -2.7019e-02,  5.6331e-02,  1.6614e-03, -3.3956e-02, -6.9785e-02,\n",
      "         1.1633e-01,  5.9738e-02, -8.4658e-02,  3.5563e-02,  1.0341e-01,\n",
      "         7.0607e-05, -4.0593e-02,  3.8467e-02,  1.0799e-01,  1.7658e-02,\n",
      "        -9.0117e-02, -9.2431e-02, -7.4624e-02,  3.1521e-02,  4.0765e-02,\n",
      "        -1.2515e-01,  3.0535e-02,  1.1851e-02, -4.0310e-02,  2.2916e-02,\n",
      "         1.2250e-01,  6.9152e-02, -6.2053e-03,  4.0321e-02,  1.6208e-02,\n",
      "        -6.8822e-02,  2.1849e-02, -3.6987e-02, -4.4603e-02, -1.5947e-01,\n",
      "        -1.6658e-02, -9.6214e-02, -3.7753e-02,  5.4041e-02, -1.7003e-02,\n",
      "         8.1025e-02,  2.4926e-02,  5.5767e-02, -7.9529e-02, -2.1234e-01,\n",
      "        -4.7282e-02, -5.5761e-02,  3.0091e-02,  1.4731e-01, -6.2581e-02,\n",
      "         2.2454e-02, -6.7485e-02,  1.5281e-01,  4.6557e-02,  8.2848e-02,\n",
      "        -9.2783e-03,  7.2040e-02, -9.9636e-02,  6.1564e-02, -5.9368e-02,\n",
      "        -1.9590e-02, -1.0435e-02, -4.1890e-02, -4.7181e-02, -1.2446e-02,\n",
      "        -4.0818e-02,  6.1132e-02, -8.5487e-03,  8.7448e-02,  2.1625e-02,\n",
      "        -1.7572e-02, -9.9109e-02,  3.0057e-02,  7.2901e-02, -1.2618e-02,\n",
      "         3.7349e-02, -2.1917e-02, -6.9758e-02, -1.2695e-03, -1.3122e-02,\n",
      "        -5.0221e-02,  2.3869e-02,  5.0954e-02,  7.0282e-04, -3.3970e-02,\n",
      "        -2.8963e-02, -8.4868e-02, -2.6569e-02, -6.5083e-02,  8.5820e-03,\n",
      "        -4.4336e-03,  5.8201e-03,  2.1587e-02,  7.3191e-03,  4.7043e-03,\n",
      "        -5.8309e-02,  2.1552e-02, -2.5648e-02, -2.2331e-02, -1.0112e-01,\n",
      "        -3.7041e-02, -4.1032e-02, -6.8042e-02,  1.7894e-02, -2.6997e-02,\n",
      "        -2.7584e-02,  1.7612e-02, -1.9444e-03,  5.9923e-02,  6.8182e-02,\n",
      "         2.6522e-02, -6.7600e-02,  3.6002e-02, -1.6933e-02,  9.7652e-03,\n",
      "        -1.0266e-01, -3.6495e-03,  1.1981e-01, -3.1746e-02, -2.1659e-02,\n",
      "        -4.1714e-02,  7.0952e-02, -8.4005e-02,  3.2536e-03, -2.2566e-02,\n",
      "        -3.9273e-02,  3.3117e-03, -8.4515e-02,  5.7761e-02,  9.1372e-02,\n",
      "         9.6171e-03, -1.2380e-01, -8.3872e-04, -1.1604e-02, -2.1467e-02,\n",
      "         3.9992e-02,  8.3243e-04, -5.9930e-03, -2.2868e-02,  2.3452e-02,\n",
      "         1.2934e-02,  1.4610e-01,  6.3666e-04, -4.7834e-02, -1.6290e-02,\n",
      "         6.7797e-02,  3.1905e-02, -6.1453e-02,  4.7708e-02,  4.9836e-02,\n",
      "        -3.2332e-02,  1.4693e-02, -8.0379e-02,  5.6533e-02,  6.9687e-02,\n",
      "         6.2967e-02, -3.5479e-02, -9.2222e-03, -6.3729e-03,  8.0024e-02,\n",
      "         1.0684e-02,  5.5488e-02, -5.7777e-03,  1.2793e-01,  2.4388e-02,\n",
      "         6.8428e-02, -2.1748e-03, -4.4633e-02,  1.3514e-02,  2.4887e-03,\n",
      "        -1.9060e-02, -1.2467e-01, -4.7357e-02, -4.9894e-02,  9.8269e-02,\n",
      "        -6.8453e-03,  3.6830e-02, -3.3399e-02, -4.3410e-02, -9.6036e-02,\n",
      "         8.1545e-02, -3.5613e-02,  6.0910e-02, -5.0575e-02,  6.5858e-03,\n",
      "         5.8657e-02,  2.9649e-02, -5.0301e-02, -1.8220e-02, -7.9198e-02,\n",
      "         4.7839e-02,  3.2613e-02, -9.3417e-02,  6.7337e-02, -8.7942e-03,\n",
      "        -1.6459e-02,  2.7349e-02, -4.9454e-02,  6.1516e-02,  6.7670e-02,\n",
      "         4.5408e-03,  3.2664e-02,  3.3849e-02, -8.3817e-03,  2.9799e-02,\n",
      "        -6.4481e-02,  6.9932e-02,  1.3802e-02, -7.4295e-02,  2.8266e-03,\n",
      "         1.3482e-01,  1.6569e-02, -4.2818e-02,  5.2147e-02,  4.8331e-02,\n",
      "        -2.2739e-02, -1.8746e-02,  2.8624e-02, -8.2209e-02, -4.9650e-02,\n",
      "        -2.9904e-02, -3.1530e-02, -4.7788e-02, -4.7805e-02,  4.2077e-02,\n",
      "        -5.1374e-03,  9.3389e-02,  7.7671e-02, -1.0206e-02, -5.3528e-02,\n",
      "        -6.0535e-03,  2.0553e-02,  2.7381e-02,  8.1292e-03, -6.6471e-02,\n",
      "        -1.9595e-02,  2.1768e-02,  4.5958e-02,  5.7396e-02,  1.7548e-02,\n",
      "        -6.3863e-03, -1.7971e-01,  2.8201e-02,  1.6888e-02, -6.0088e-02,\n",
      "        -4.4732e-02,  5.1204e-04,  5.4047e-02,  1.5042e-02,  8.6862e-02,\n",
      "        -5.6149e-02, -8.0252e-02, -1.7712e-02, -3.3251e-02,  6.7082e-02,\n",
      "         5.7277e-02,  7.4467e-02,  1.3210e-02,  8.0749e-02, -4.9230e-02,\n",
      "         4.0126e-02,  6.4328e-02,  3.2686e-02,  5.5669e-02, -4.5429e-02,\n",
      "        -6.0456e-02,  5.9471e-03, -7.2037e-03, -6.6578e-02,  6.4264e-02,\n",
      "        -3.4567e-02,  1.8057e-01,  9.6095e-02,  1.7282e-02, -5.5573e-03,\n",
      "        -1.5813e-02,  7.3891e-02, -9.6589e-03, -5.6928e-02,  3.5197e-02,\n",
      "        -3.6848e-02,  3.3619e-02, -7.9201e-02, -1.0853e-03, -6.1366e-02,\n",
      "        -4.6373e-02, -2.3210e-02,  2.4530e-02, -2.9117e-02, -2.6862e-02,\n",
      "         2.0443e-02, -1.0311e-02, -4.5818e-02,  3.2928e-02, -1.4177e-01,\n",
      "        -3.3394e-02, -8.0657e-02, -1.1610e-01,  2.7471e-03, -1.1582e-02,\n",
      "         1.8751e-03, -3.5150e-02,  9.0628e-02, -1.1234e-02, -6.3072e-03,\n",
      "        -2.9522e-03, -2.5991e-02,  7.4267e-02,  5.3881e-02, -4.0242e-03,\n",
      "         7.6560e-03,  8.1244e-02, -1.5535e-02, -7.0901e-02,  4.0996e-03,\n",
      "        -1.9212e-02,  1.5392e-02, -4.2169e-02,  1.7310e-02, -7.4863e-02,\n",
      "        -5.8399e-02, -4.7026e-02,  1.1410e-01, -1.0140e-01, -9.5707e-02,\n",
      "         2.0097e-02, -1.0625e-01,  6.2864e-02, -1.0046e-01,  4.0808e-02,\n",
      "        -5.9520e-02, -5.2804e-02,  1.8317e-02, -1.1327e-01, -1.7123e-02,\n",
      "        -2.9642e-03, -1.2108e-02,  4.3250e-02, -6.8001e-02,  2.8993e-02,\n",
      "         2.3379e-03,  6.4308e-03, -5.0257e-02, -2.6099e-02, -9.2139e-03,\n",
      "         1.4326e-01, -3.5042e-02, -5.5747e-03,  1.4443e-01,  6.4646e-02,\n",
      "        -3.6846e-02, -3.1642e-02,  1.8773e-04, -6.0860e-02,  7.3784e-02,\n",
      "         3.4365e-02, -5.6993e-02,  4.9817e-02, -4.8040e-02,  7.2079e-02,\n",
      "         6.0582e-02,  1.5344e-03, -6.8195e-02,  2.4479e-02, -6.7752e-02,\n",
      "        -7.2611e-02, -2.7682e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0768, -0.0110,  0.0261, -0.0717,  0.0138, -0.0634, -0.0912,  0.0113,\n",
      "        -0.0347, -0.0304, -0.0077, -0.0341, -0.0804, -0.0470, -0.0264,  0.0091,\n",
      "         0.0322,  0.0482, -0.0405, -0.0913,  0.0352, -0.0308,  0.0159,  0.0034,\n",
      "         0.0155, -0.0147,  0.0697,  0.0984,  0.0066,  0.0651, -0.1385, -0.0525,\n",
      "        -0.0866,  0.0596, -0.0648,  0.0693,  0.0717,  0.0327, -0.0749,  0.1113,\n",
      "         0.0407,  0.0465,  0.1108,  0.0816, -0.0240,  0.0117,  0.0365, -0.0328,\n",
      "         0.0209, -0.0589,  0.0395, -0.0040,  0.0484,  0.0579,  0.0430,  0.0961,\n",
      "         0.0019, -0.0478, -0.0156,  0.0328, -0.0624,  0.0715,  0.0612, -0.0883,\n",
      "         0.0393, -0.0688, -0.0231, -0.0230, -0.0219,  0.0156, -0.0243, -0.1010,\n",
      "        -0.0313,  0.0016, -0.0020, -0.0170, -0.0236, -0.0161, -0.0517, -0.0867,\n",
      "        -0.0712, -0.0125, -0.0954, -0.0109,  0.1592,  0.0375, -0.0574,  0.0412,\n",
      "        -0.0757,  0.1175,  0.0951, -0.0161, -0.0222, -0.1225,  0.0901,  0.0392,\n",
      "        -0.0461, -0.0242,  0.0155, -0.0975, -0.0425, -0.0112,  0.0040,  0.0077,\n",
      "         0.0669, -0.0678, -0.0185, -0.0830, -0.0124,  0.0362, -0.0285,  0.1085,\n",
      "        -0.0133,  0.0715, -0.0329, -0.0025,  0.0326, -0.0271,  0.0487, -0.0552,\n",
      "        -0.0141,  0.0521, -0.0023, -0.0375, -0.1438,  0.0137,  0.0634, -0.0483,\n",
      "        -0.0128,  0.0103,  0.0111,  0.0511,  0.1563,  0.0164,  0.0060, -0.1368,\n",
      "        -0.1142, -0.0285, -0.0205,  0.0208,  0.0782,  0.0446,  0.0960, -0.0340,\n",
      "        -0.0171,  0.0837,  0.1210,  0.0210, -0.0156, -0.0047,  0.0567,  0.1111,\n",
      "        -0.0234, -0.0498, -0.0705, -0.0082,  0.1107,  0.0074,  0.0705, -0.0538,\n",
      "         0.0613, -0.1379,  0.0155, -0.0276,  0.0236, -0.0070, -0.0942, -0.0741,\n",
      "         0.0344,  0.0320, -0.0537, -0.1111, -0.0324,  0.1613,  0.0198,  0.1086,\n",
      "        -0.0317,  0.0004, -0.0473,  0.0628,  0.0596, -0.0103, -0.0568,  0.0624,\n",
      "        -0.0776, -0.1148, -0.0166,  0.0027,  0.0078, -0.0937, -0.0514, -0.0138,\n",
      "        -0.1482, -0.0669, -0.0712,  0.0135,  0.1173, -0.0033, -0.0064, -0.0263,\n",
      "        -0.0567,  0.0106,  0.0777, -0.0619, -0.0526,  0.0932, -0.0841, -0.0340,\n",
      "        -0.1270,  0.0130,  0.0067, -0.0860,  0.1337, -0.0305, -0.0314, -0.0653,\n",
      "         0.1493, -0.0126, -0.0196, -0.0949, -0.0565,  0.0440, -0.0889,  0.0118,\n",
      "        -0.0558, -0.0214, -0.0157, -0.0387, -0.0158,  0.0084, -0.0396, -0.0521,\n",
      "        -0.0809,  0.0183,  0.0045,  0.0053, -0.0093, -0.0678, -0.1156,  0.0174,\n",
      "         0.1187,  0.0416,  0.0693, -0.0025,  0.0486,  0.0294, -0.0075, -0.0575,\n",
      "         0.1809,  0.0164,  0.0446, -0.0271, -0.0230,  0.0786, -0.0114, -0.0058,\n",
      "         0.0358, -0.0731, -0.0365, -0.0286,  0.1120, -0.0882,  0.0127,  0.0710,\n",
      "         0.0003,  0.0062, -0.0400,  0.0463,  0.0816,  0.0720,  0.0084,  0.0478,\n",
      "         0.0634,  0.0475,  0.0025, -0.0680, -0.0101,  0.0497,  0.0274,  0.0548,\n",
      "         0.0372, -0.0325,  0.1441,  0.0648,  0.0218,  0.0187,  0.0017,  0.0058,\n",
      "         0.0606,  0.0349, -0.0842, -0.0129,  0.1517, -0.0832, -0.0344,  0.0722,\n",
      "         0.0201, -0.0085,  0.0686, -0.0399, -0.1319,  0.0208, -0.0094, -0.0035,\n",
      "         0.0502,  0.0415,  0.0268,  0.0031, -0.0782, -0.0470,  0.0647, -0.0245,\n",
      "        -0.0220,  0.0053, -0.0115,  0.0109,  0.0431,  0.0079, -0.0562, -0.0070,\n",
      "         0.0463, -0.0588,  0.0339,  0.0052, -0.0210,  0.1090,  0.0647, -0.0540,\n",
      "         0.0085,  0.0879, -0.0313,  0.0073,  0.0437,  0.0494,  0.0060,  0.1026,\n",
      "         0.0076,  0.0393, -0.0335, -0.0069, -0.1043,  0.0803, -0.0891,  0.1589,\n",
      "        -0.0709, -0.0418, -0.0459, -0.0026,  0.1630, -0.0228,  0.0362,  0.0665,\n",
      "         0.0199,  0.0311, -0.0793,  0.0584, -0.0846, -0.0298,  0.0471,  0.1816,\n",
      "         0.1290, -0.0308, -0.0354,  0.0684,  0.0022,  0.1397,  0.1273, -0.0121,\n",
      "        -0.0255,  0.1549, -0.1043,  0.0030, -0.0070, -0.0533, -0.1327, -0.0505,\n",
      "        -0.0394, -0.0871, -0.1559, -0.1013, -0.0389,  0.0533, -0.0024,  0.0499,\n",
      "         0.0578, -0.0086, -0.0890, -0.0100,  0.0792, -0.0145, -0.0229, -0.0173,\n",
      "        -0.0718,  0.0246, -0.0108, -0.0746, -0.1079, -0.1119, -0.0225,  0.0620,\n",
      "        -0.0441,  0.0702,  0.1055, -0.0187,  0.0807,  0.0159,  0.0401,  0.0435,\n",
      "        -0.0720, -0.1575, -0.0476, -0.0490, -0.0268,  0.1036,  0.0390,  0.0015,\n",
      "        -0.1407, -0.0818, -0.0521, -0.0193,  0.0634,  0.0762, -0.0572,  0.0335,\n",
      "        -0.0147,  0.0902, -0.0812,  0.0083, -0.1243, -0.0758,  0.1391,  0.0418,\n",
      "         0.0337, -0.0012,  0.0702, -0.0611,  0.0674,  0.0109,  0.0365, -0.0833,\n",
      "        -0.0679, -0.0756,  0.0385, -0.0285,  0.0510, -0.0359,  0.0606,  0.0541,\n",
      "         0.0934, -0.0538, -0.0293,  0.0203, -0.0051,  0.1183, -0.0098,  0.0472,\n",
      "         0.0742, -0.0267, -0.0643, -0.0058,  0.0205,  0.0397, -0.0012,  0.0355,\n",
      "         0.0729,  0.0082,  0.0999,  0.0031,  0.0537,  0.0390,  0.0033,  0.0092,\n",
      "         0.0299, -0.0649,  0.0372,  0.0805,  0.0463, -0.0983, -0.0180, -0.0175,\n",
      "         0.0584, -0.0766,  0.0062, -0.0004,  0.0233, -0.0832,  0.0306,  0.0634,\n",
      "         0.0414, -0.0457,  0.0292, -0.0461,  0.0299,  0.0362,  0.0514,  0.0055,\n",
      "        -0.0551, -0.0026, -0.0381, -0.0229, -0.0396, -0.0021,  0.1161, -0.0633,\n",
      "         0.0352, -0.0886,  0.1244, -0.0195,  0.0971,  0.0900, -0.1717, -0.0553],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-2.8312e-02,  4.9911e-02,  9.7769e-03, -1.7147e-02,  4.0901e-02,\n",
      "        -1.2317e-01, -1.1881e-01,  8.5501e-02,  1.1018e-01,  6.2696e-02,\n",
      "         3.1070e-02, -1.0946e-01,  7.7663e-02,  6.7539e-02, -1.3375e-04,\n",
      "        -1.2912e-02,  5.7624e-02, -7.1261e-02,  9.6846e-04, -4.5915e-03,\n",
      "         6.0058e-02,  2.9872e-02,  4.2197e-02,  3.8850e-02,  5.4885e-02,\n",
      "         4.4528e-02, -8.8942e-02,  1.1722e-01, -4.4009e-02,  3.8589e-02,\n",
      "        -7.9293e-02, -1.1473e-02, -2.3653e-02, -4.3948e-02, -2.1827e-02,\n",
      "        -4.3308e-04,  8.2051e-02,  6.2999e-02,  3.0414e-02,  1.3454e-02,\n",
      "         5.9846e-03,  1.5785e-02, -6.2734e-02,  7.9752e-02, -1.4402e-01,\n",
      "        -5.4157e-02,  8.3404e-02, -5.4182e-02, -3.7938e-02,  1.9626e-03,\n",
      "         6.2376e-02, -9.8665e-02,  1.1238e-01,  8.4942e-02, -5.1376e-02,\n",
      "        -4.4197e-03,  1.0537e-02,  7.6728e-02,  7.0679e-02,  7.5002e-02,\n",
      "         2.3206e-02,  2.2686e-02,  3.7321e-02,  3.3898e-02, -2.2739e-02,\n",
      "        -1.1890e-01,  7.7856e-02,  1.0845e-01,  6.1648e-02, -2.4917e-02,\n",
      "        -5.6272e-02, -2.0143e-04, -6.7984e-02, -5.5723e-02,  1.5601e-03,\n",
      "         9.5723e-02, -1.2334e-01,  2.3138e-02,  1.5915e-03,  1.7391e-02,\n",
      "         1.0060e-03, -5.5752e-02, -7.3283e-03,  7.8786e-02, -8.5108e-02,\n",
      "         5.5049e-02,  1.5016e-01, -3.1859e-02,  4.4934e-03, -5.7109e-02,\n",
      "         8.0624e-03,  1.0309e-01, -3.0260e-03, -1.8075e-02,  1.0297e-01,\n",
      "         1.8190e-02,  8.1257e-02, -1.0586e-01,  4.6859e-02,  8.7545e-03,\n",
      "        -1.8347e-02,  7.8826e-04,  3.4076e-02,  3.4202e-02, -4.6036e-02,\n",
      "         7.8401e-02,  1.2534e-02, -2.9604e-02, -1.4013e-01, -1.2220e-01,\n",
      "        -3.9575e-02,  4.2375e-02,  6.8481e-02, -1.1031e-01,  1.7292e-03,\n",
      "         5.6505e-03, -1.3347e-01,  5.8967e-02,  1.0500e-01,  2.8959e-02,\n",
      "        -1.3579e-01, -3.6767e-02, -6.5603e-03,  5.9650e-02,  3.4714e-02,\n",
      "         3.4603e-02,  6.3472e-02,  8.8572e-02, -3.0379e-02,  1.2246e-02,\n",
      "         3.0892e-02, -1.9900e-02, -2.0532e-02, -9.3364e-02,  2.0879e-02,\n",
      "        -3.1082e-02,  7.4723e-02,  3.4827e-02,  9.9355e-03,  4.0432e-02,\n",
      "         9.0674e-02, -6.2378e-02, -1.7440e-02,  1.5880e-02, -1.3521e-02,\n",
      "         6.1648e-02, -2.5270e-02, -1.0506e-02,  1.8069e-02, -5.2453e-02,\n",
      "         1.3252e-02,  6.9504e-03, -5.8516e-02,  4.6623e-02,  1.4739e-02,\n",
      "         6.7765e-03,  3.7023e-03,  3.7319e-02,  1.9224e-02,  2.6738e-02,\n",
      "         8.2818e-02, -1.2007e-04,  7.7645e-02,  9.2141e-03,  4.3738e-03,\n",
      "        -1.0779e-01,  8.4956e-02,  3.7886e-02, -1.3384e-01, -1.1208e-01,\n",
      "        -5.7828e-02, -9.7238e-02,  1.0206e-02,  6.5645e-03, -2.8718e-02,\n",
      "         1.5325e-02,  6.6613e-02,  2.6445e-02, -2.4962e-02, -4.9788e-02,\n",
      "        -4.3545e-03, -4.5150e-02, -1.4951e-02,  6.1688e-02, -9.0608e-03,\n",
      "        -8.5805e-02, -1.0172e-01, -9.2241e-02, -1.5714e-03, -2.6098e-02,\n",
      "        -2.3720e-02, -4.2816e-03, -4.2465e-02,  4.0990e-03,  5.9952e-02,\n",
      "        -8.0171e-02,  3.4743e-02, -5.9418e-02, -5.0707e-04, -1.7003e-02,\n",
      "        -3.6289e-02,  9.0298e-02, -2.5486e-02,  2.2962e-02,  8.9927e-03,\n",
      "         3.8505e-02,  5.5345e-02, -2.0447e-02, -3.3111e-02,  3.7436e-02,\n",
      "         6.5773e-02, -4.5183e-02,  4.1996e-02, -8.7999e-02, -1.1769e-02,\n",
      "        -4.3234e-02, -6.6346e-02, -3.5659e-02, -5.7530e-03,  3.8261e-02,\n",
      "         6.5813e-02, -2.6030e-02, -7.3186e-03, -6.0748e-02, -5.1565e-02,\n",
      "        -2.2371e-02,  1.2256e-02,  7.5072e-02,  1.9970e-02,  2.4642e-02,\n",
      "        -7.0200e-02,  3.6686e-02,  2.4515e-02,  3.2946e-03,  6.7995e-03,\n",
      "         8.7247e-02, -6.1754e-02,  2.3224e-02,  4.8788e-02, -3.7919e-02,\n",
      "        -4.5916e-02, -6.3038e-03, -6.4867e-02,  9.7451e-03, -2.9809e-02,\n",
      "         1.9220e-02,  4.9873e-02, -8.4751e-02, -3.8756e-02,  2.4613e-03,\n",
      "         1.2979e-02, -1.9546e-02, -1.7456e-03,  6.0348e-02,  3.5478e-02,\n",
      "         8.5359e-02,  4.5793e-02, -2.9652e-02, -1.9533e-02,  2.8801e-02,\n",
      "         2.0128e-02, -1.6773e-02, -2.2567e-02,  8.6599e-02,  7.6258e-02,\n",
      "        -1.3919e-02, -5.2701e-03,  1.5254e-02, -5.6596e-03,  1.2512e-02,\n",
      "        -1.1107e-01, -3.9220e-02, -4.3274e-02, -1.4759e-02,  6.3456e-02,\n",
      "        -3.9313e-02,  6.6304e-02, -2.5031e-02, -8.0906e-02, -9.2574e-02,\n",
      "         7.7114e-03, -3.8525e-02,  2.6354e-02,  6.7656e-02, -3.6397e-02,\n",
      "        -6.6598e-02,  4.9100e-02, -4.5302e-02, -9.6687e-02,  3.2252e-03,\n",
      "        -1.6827e-02,  9.3235e-02, -2.9695e-02,  8.8593e-02,  1.0684e-01,\n",
      "         1.0159e-01,  7.8147e-02, -2.3984e-02,  7.4527e-02,  9.7435e-02,\n",
      "         9.9969e-02,  4.1802e-02,  5.5769e-02,  4.1883e-02,  3.7363e-02,\n",
      "        -1.2641e-02,  3.1162e-02, -5.7425e-04,  5.6984e-02,  2.1873e-03,\n",
      "         3.2089e-02, -7.0392e-02,  2.0635e-02,  9.4762e-03, -1.5822e-02,\n",
      "         5.4450e-02, -2.8916e-02,  1.6877e-02, -7.8206e-03, -1.1922e-01,\n",
      "         2.3058e-02,  6.5806e-02,  9.5983e-03,  4.4597e-02,  1.8453e-02,\n",
      "         4.3058e-02,  6.1493e-02, -6.8039e-02, -3.5424e-02, -3.8730e-02,\n",
      "        -4.6403e-02,  2.2619e-03,  1.3438e-02,  3.6322e-02, -9.0361e-02,\n",
      "         2.3885e-02, -6.8223e-02, -2.8933e-02,  1.0164e-01,  1.5505e-02,\n",
      "        -7.0034e-02,  7.1678e-02, -6.8170e-02,  4.8597e-02,  8.5489e-02,\n",
      "         3.4030e-02, -1.1827e-02,  4.7249e-02, -5.7491e-02,  6.4812e-02,\n",
      "        -3.8081e-02,  3.1269e-02,  4.8112e-02, -2.2889e-02, -1.2078e-01,\n",
      "         8.6875e-03,  2.7524e-03, -5.2020e-02, -1.3657e-02, -3.4252e-02,\n",
      "         1.2507e-01,  6.4650e-02, -4.3744e-02,  2.1554e-02,  7.2027e-02,\n",
      "         4.6084e-02,  1.0100e-01,  7.4042e-02, -5.4211e-02, -1.1455e-01,\n",
      "         5.7521e-02, -4.2710e-02, -7.8814e-02, -1.8124e-02,  4.4737e-02,\n",
      "        -5.1269e-02, -6.7855e-02, -8.3722e-02, -6.4286e-02,  3.4506e-02,\n",
      "         8.8117e-02,  4.1227e-02, -1.0366e-01, -5.4640e-02, -3.3339e-03,\n",
      "         1.3867e-01, -5.8631e-02,  1.0841e-02, -9.4331e-02,  1.0992e-01,\n",
      "        -1.8052e-02,  5.6607e-02, -3.0553e-03, -9.7665e-02,  3.6189e-03,\n",
      "         3.8424e-02, -2.0226e-02, -1.0399e-01,  7.1986e-02, -8.7396e-02,\n",
      "        -2.1321e-02, -3.3681e-02, -4.8806e-02, -9.9724e-03,  3.4821e-02,\n",
      "        -3.6701e-02, -1.0064e-01, -4.4952e-02, -2.9649e-02,  6.7568e-02,\n",
      "         1.0062e-01,  1.5413e-02, -5.2982e-03, -8.1491e-02,  6.9497e-02,\n",
      "         7.5970e-03,  2.6650e-02, -7.8061e-02,  8.9628e-02,  5.9069e-02,\n",
      "        -2.8076e-03,  2.2840e-02,  4.9031e-02, -3.0829e-02, -1.4460e-01,\n",
      "         2.0347e-02,  3.0446e-02,  4.5471e-02,  8.5173e-02, -1.1764e-02,\n",
      "        -1.9823e-02, -1.1526e-02, -1.4037e-02, -5.7210e-03,  3.2612e-02,\n",
      "         8.8098e-02,  2.5476e-02,  5.3235e-02,  9.3301e-02,  6.9620e-02,\n",
      "        -6.3628e-02,  6.8000e-02,  1.4908e-01, -5.6959e-02,  5.9116e-02,\n",
      "         2.2112e-02, -2.4973e-02, -2.7610e-02,  4.1903e-02, -2.0115e-02,\n",
      "         5.7806e-02,  1.3158e-03, -8.3065e-02,  4.6314e-02, -9.3857e-02,\n",
      "        -9.9200e-03,  4.4497e-02, -1.1722e-02, -6.1344e-02, -1.3309e-01,\n",
      "         4.0768e-02, -2.1628e-02, -5.0834e-02,  1.0866e-01,  1.6634e-02,\n",
      "         7.5386e-02,  1.1037e-01, -3.8678e-02,  5.1629e-02,  3.5886e-02,\n",
      "         3.2558e-02,  1.4227e-03,  5.5960e-02,  1.0197e-03, -5.6617e-02,\n",
      "         2.2816e-02, -1.3664e-01,  1.3298e-01, -3.5689e-02,  1.8169e-02,\n",
      "        -3.9363e-02, -4.9693e-02,  8.3050e-02, -1.3196e-02, -4.6567e-02,\n",
      "         3.9041e-02,  2.8396e-02, -2.6041e-02,  6.8008e-02, -1.0233e-01,\n",
      "        -1.5822e-02, -3.0579e-02, -4.8071e-02, -6.4514e-02,  1.8201e-02,\n",
      "        -4.3278e-02, -4.3680e-03, -8.4785e-02, -5.5908e-02, -6.7275e-02,\n",
      "         8.3114e-02,  1.3823e-02,  4.9019e-02,  4.0267e-02, -5.4514e-02,\n",
      "         4.9135e-02, -4.8312e-02, -2.4285e-02, -9.7027e-02,  2.4834e-02,\n",
      "         1.4886e-02,  6.9949e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 3.0289e-02,  3.1503e-02,  3.9986e-02,  1.3083e-01, -5.3132e-02,\n",
      "         2.9113e-02,  4.7187e-03,  5.0454e-02,  1.0700e-01, -2.2314e-02,\n",
      "         2.6524e-02, -1.1840e-02,  5.0855e-03,  7.3779e-04, -1.1865e-03,\n",
      "        -4.7954e-02,  1.0474e-02,  2.8582e-02, -7.9896e-02,  7.6038e-02,\n",
      "         4.5977e-02, -1.4148e-02,  3.9841e-02,  1.8766e-02,  8.0392e-02,\n",
      "         2.6746e-02,  2.9566e-02, -2.5976e-02,  1.6478e-02, -5.0035e-02,\n",
      "         2.4266e-02,  4.7684e-03, -4.6095e-02,  5.4383e-02, -5.5842e-02,\n",
      "        -6.3235e-02,  1.0002e-01, -7.9192e-03,  4.9059e-02, -2.9653e-02,\n",
      "         7.4298e-02,  3.2793e-02,  8.6242e-02,  1.3700e-03,  1.4234e-02,\n",
      "         7.6310e-02,  3.2565e-02, -5.5205e-02, -2.8722e-02, -3.9794e-02,\n",
      "         8.0323e-02, -1.0903e-01, -4.8134e-04,  4.3818e-02, -3.0959e-02,\n",
      "        -5.7084e-02,  4.3061e-02,  4.2138e-02,  7.2363e-02,  4.3792e-02,\n",
      "        -7.2850e-02,  5.2529e-03,  4.6195e-03, -6.2514e-02,  8.1972e-02,\n",
      "        -1.2628e-02,  1.1640e-01, -7.5081e-02,  2.6473e-02, -6.2586e-02,\n",
      "        -6.8327e-02,  5.4805e-03, -8.0045e-02, -1.0655e-02, -7.7074e-03,\n",
      "        -8.1215e-02, -1.6442e-02,  6.8840e-03, -6.9273e-03, -4.1731e-02,\n",
      "        -6.2782e-02,  6.2828e-02, -8.7719e-02,  1.7283e-02, -5.3315e-02,\n",
      "        -9.8364e-02, -9.7457e-02,  8.1505e-02,  2.6662e-02,  5.2712e-02,\n",
      "         5.1618e-02, -3.9540e-02, -1.0101e-01, -2.3273e-02,  1.6070e-02,\n",
      "        -3.2476e-02, -3.7883e-02, -1.9677e-02, -3.3466e-02,  1.7523e-02,\n",
      "        -9.1086e-02, -4.3556e-02,  7.8876e-02, -4.1143e-02, -3.5400e-02,\n",
      "        -1.7865e-02,  1.7630e-01,  1.3965e-01, -5.0848e-02, -3.6669e-02,\n",
      "         2.1116e-02, -1.0324e-01, -1.7145e-02,  6.3624e-02, -7.2753e-02,\n",
      "         8.1110e-04,  7.7122e-02,  6.0167e-02,  9.4302e-02,  3.3645e-02,\n",
      "         5.1997e-02,  9.3938e-03,  1.5380e-02,  3.0624e-02,  1.8364e-02,\n",
      "         9.4459e-02, -5.3204e-02,  5.3909e-02,  8.4368e-02, -2.6575e-02,\n",
      "         5.8741e-03,  1.7135e-01,  3.8734e-02,  1.1533e-01, -3.4991e-02,\n",
      "        -1.3902e-01, -5.0564e-02,  2.5342e-02,  1.9510e-03, -4.5458e-02,\n",
      "        -7.6664e-02,  1.0237e-01,  7.7267e-03,  5.8986e-02, -1.9288e-02,\n",
      "         5.3286e-02,  3.6359e-02,  8.0501e-02, -8.3045e-02,  3.3307e-02,\n",
      "         1.5659e-03,  9.6013e-03, -1.5590e-02, -5.1359e-02, -7.0246e-02,\n",
      "        -1.1975e-02,  2.6491e-02, -3.2005e-02,  6.8249e-02,  4.7669e-02,\n",
      "         4.7641e-02, -2.1512e-02, -6.3295e-02, -4.1788e-02, -1.5279e-02,\n",
      "        -9.7037e-02,  2.2685e-02,  2.0949e-02,  3.3309e-02,  9.4829e-03,\n",
      "         5.6710e-02, -7.6783e-03, -1.3969e-01, -4.1760e-02,  8.8335e-03,\n",
      "         4.3914e-02, -1.1144e-02,  2.1213e-02,  5.0143e-02, -1.7819e-02,\n",
      "        -3.6000e-02, -9.8346e-02,  1.8010e-02,  1.1031e-02, -4.7298e-02,\n",
      "        -2.5419e-02, -4.0803e-02,  3.5511e-02,  9.2070e-03,  6.9367e-03,\n",
      "        -4.2061e-02, -1.0377e-02,  8.0876e-02, -5.6107e-02,  5.7277e-02,\n",
      "         8.7439e-03,  1.8353e-02, -4.1559e-02,  3.4507e-02, -1.0548e-01,\n",
      "        -4.0571e-02, -2.1289e-02,  3.0586e-02,  5.1678e-03,  8.7577e-04,\n",
      "         1.3942e-01, -1.1645e-02,  7.2364e-02,  6.5043e-02,  2.4132e-02,\n",
      "         1.1002e-01,  6.1222e-03,  6.6061e-03, -5.2206e-02, -1.3325e-02,\n",
      "        -8.5573e-03, -2.0275e-03,  1.6365e-03,  2.6494e-02,  7.1705e-02,\n",
      "        -7.1865e-02,  8.4742e-02,  6.0429e-02, -5.9917e-04, -5.1137e-02,\n",
      "        -5.9481e-02, -7.6383e-02,  4.8239e-02, -3.4069e-02, -9.6994e-02,\n",
      "         1.8230e-02,  8.8950e-02,  8.6447e-02, -2.9383e-02, -9.0702e-02,\n",
      "        -3.7237e-02, -3.5979e-02, -4.2816e-02, -7.7253e-02,  7.3348e-03,\n",
      "         4.4436e-02, -1.5954e-01,  1.2394e-01,  1.1889e-02,  1.5041e-02,\n",
      "        -6.7389e-02, -4.5964e-02,  2.0859e-02, -3.0347e-02, -2.0750e-02,\n",
      "         3.9519e-02, -2.8886e-02, -8.1723e-02, -2.2986e-02, -2.3117e-03,\n",
      "         7.9396e-02, -4.6225e-02,  5.9592e-02, -6.6315e-02, -4.8456e-02,\n",
      "        -4.7836e-03, -6.7407e-02,  4.6288e-02,  1.5025e-01,  3.1964e-02,\n",
      "        -1.0685e-01, -3.1458e-02, -4.1457e-02,  7.1839e-02, -9.0231e-02,\n",
      "         3.3797e-02, -2.6273e-02, -6.0258e-02, -3.0063e-02, -9.9684e-02,\n",
      "         8.9154e-02,  4.6204e-02,  1.0030e-02, -2.1860e-02, -9.5296e-03,\n",
      "        -2.6632e-02, -2.0542e-02, -8.8112e-02, -3.1891e-02,  8.1285e-02,\n",
      "         3.4284e-02,  9.3343e-02, -7.2938e-02,  4.2222e-02,  8.5092e-02,\n",
      "        -6.9859e-02, -1.1665e-01, -1.7408e-02, -1.5403e-02,  5.4243e-02,\n",
      "         9.8341e-03, -2.8077e-02, -2.9991e-02,  3.4399e-02,  1.4826e-02,\n",
      "         1.0260e-02,  8.0673e-02,  5.1878e-03, -8.1736e-02,  8.6033e-02,\n",
      "         8.2636e-02,  5.0595e-02, -1.1922e-01,  9.3888e-03,  2.7255e-02,\n",
      "         2.7873e-02,  2.2796e-02,  1.8762e-02,  1.4380e-01, -1.4723e-01,\n",
      "        -1.4255e-02, -3.0604e-02, -3.7668e-03,  1.1167e-02, -8.0839e-02,\n",
      "         1.4414e-02, -2.5007e-02, -2.3666e-02, -2.7692e-02, -1.6474e-02,\n",
      "         5.1326e-02, -6.8901e-03,  2.6673e-02, -1.9049e-02, -4.9653e-02,\n",
      "         1.1313e-01,  8.5847e-02,  1.3205e-01, -4.7806e-02, -9.3220e-02,\n",
      "         4.1846e-02, -4.5715e-02,  2.4093e-02, -3.6066e-02,  5.0121e-02,\n",
      "         2.4745e-02, -9.0033e-02,  5.9747e-02, -5.9992e-02, -2.5795e-02,\n",
      "        -3.5649e-02,  2.3503e-02,  1.4340e-01, -5.7906e-02, -8.6132e-03,\n",
      "        -6.0701e-03,  3.0256e-03, -6.0207e-02,  1.3398e-02, -3.4405e-03,\n",
      "         3.6077e-02, -7.9061e-02, -4.5184e-02, -6.7206e-02,  8.3835e-02,\n",
      "        -1.4701e-02,  2.4760e-02,  1.7550e-02,  5.2360e-02, -1.1143e-01,\n",
      "        -6.0042e-02, -2.1617e-02, -2.3820e-02, -1.9716e-02, -1.1295e-01,\n",
      "        -1.7096e-02, -5.0607e-02,  9.7075e-02,  2.0780e-02, -4.8206e-02,\n",
      "         4.0675e-02, -5.4123e-02,  2.6274e-02, -1.1451e-01,  5.9652e-02,\n",
      "        -2.4965e-02, -2.3823e-02,  5.4150e-03, -2.5337e-03, -5.9982e-02,\n",
      "        -3.6474e-02, -1.8158e-02, -1.5301e-02,  1.1725e-02,  2.3499e-02,\n",
      "         7.4033e-02, -4.0130e-02, -5.1274e-02,  9.0815e-02,  5.4975e-02,\n",
      "        -3.4270e-02,  4.5382e-02, -7.2244e-02, -7.0036e-02, -9.7178e-03,\n",
      "        -3.3955e-02, -3.5253e-02,  8.1896e-02,  7.5562e-03, -7.9211e-02,\n",
      "        -1.0875e-01,  1.2409e-03,  7.7800e-02,  1.0634e-02, -8.2665e-02,\n",
      "         1.3230e-02, -3.4552e-02,  9.1453e-02, -6.4865e-02,  4.5128e-02,\n",
      "        -1.1324e-01, -5.8086e-02,  4.5286e-02, -3.5615e-02,  1.1491e-03,\n",
      "         4.5156e-02,  2.6197e-02, -9.7915e-02, -8.8574e-02,  6.3982e-02,\n",
      "        -7.3688e-02,  3.8706e-02,  8.2396e-02,  7.6938e-02, -2.0139e-02,\n",
      "        -6.2673e-02, -8.2048e-02,  5.6388e-02,  1.7644e-02,  4.3307e-02,\n",
      "         8.2072e-03, -4.8394e-02,  7.1145e-03, -1.4995e-01,  6.3767e-02,\n",
      "        -1.7300e-02, -4.0330e-04,  2.5645e-02,  6.1843e-02, -5.0088e-03,\n",
      "         3.9473e-03,  8.7710e-02,  3.0694e-02, -1.5863e-02,  1.2367e-01,\n",
      "         5.8815e-02,  6.1809e-02,  1.1823e-01,  3.4193e-02, -1.3734e-01,\n",
      "        -8.3475e-03, -1.3101e-02,  1.7372e-01,  3.1849e-02,  5.8699e-02,\n",
      "        -8.2168e-02,  2.9679e-02,  2.9754e-02, -1.9589e-02, -2.3867e-05,\n",
      "         2.9229e-03, -5.9795e-02,  1.0513e-01, -2.3250e-02,  1.5259e-02,\n",
      "        -9.9677e-04,  5.2436e-02,  4.5202e-02, -5.3536e-02, -3.1198e-02,\n",
      "         1.1600e-01,  8.2992e-02, -6.0462e-02, -6.9867e-02, -2.0561e-03,\n",
      "         6.2426e-02,  3.0686e-02,  7.3595e-03, -5.2512e-03, -8.7785e-02,\n",
      "         7.2232e-02, -5.5166e-02,  5.2830e-02, -3.4109e-02, -3.5072e-02,\n",
      "        -7.8913e-02,  3.6241e-02,  4.8680e-02, -2.4749e-02,  9.5748e-02,\n",
      "         1.1784e-01,  6.6303e-02, -3.3105e-02,  3.1397e-02,  4.8392e-02,\n",
      "        -9.6809e-02,  6.1331e-02,  3.0868e-02,  3.2937e-02,  1.4860e-02,\n",
      "        -8.8214e-02, -7.5167e-02, -2.6680e-02, -7.2619e-02, -3.8868e-02,\n",
      "         4.7005e-02, -1.5254e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-3.7034e-02, -4.5888e-02,  8.8781e-03,  2.7156e-02,  5.8858e-02,\n",
      "         1.2498e-03, -2.9473e-02, -2.4259e-02,  2.7695e-02,  4.8506e-02,\n",
      "        -1.3610e-02,  2.4264e-02, -1.0506e-02, -2.2343e-02, -1.2575e-02,\n",
      "        -2.7388e-02,  3.7047e-03, -9.8502e-02, -7.6187e-02, -1.3275e-02,\n",
      "         4.0868e-02,  3.1048e-02,  2.9744e-03, -3.4535e-02,  6.2692e-02,\n",
      "        -1.0555e-01, -1.8775e-03, -6.1323e-02,  1.1437e-02,  6.9841e-02,\n",
      "        -1.2952e-02,  7.9710e-02, -3.6756e-02,  1.2847e-02,  1.0407e-01,\n",
      "        -8.7324e-02, -1.0587e-01, -3.1902e-02, -8.2598e-03, -1.0516e-01,\n",
      "        -9.7262e-02,  1.1731e-02, -1.1542e-02, -1.0035e-01, -8.8628e-02,\n",
      "        -1.6604e-02, -6.6435e-04, -5.5660e-02, -5.8090e-03, -9.9288e-03,\n",
      "         2.7286e-02, -4.0562e-02, -1.3763e-02, -5.6210e-02, -8.2477e-03,\n",
      "         3.0968e-02, -2.2097e-02,  2.6884e-02, -4.4554e-03,  6.1624e-02,\n",
      "         5.7080e-02,  9.1388e-03, -2.2383e-02,  3.1594e-02,  9.0034e-02,\n",
      "         2.9283e-04, -2.4813e-03, -4.8279e-02,  2.9078e-02,  3.5868e-02,\n",
      "         4.1491e-02, -6.3660e-02, -8.4763e-02, -8.1597e-02, -5.1852e-02,\n",
      "         2.2601e-04,  1.0845e-01,  3.0973e-02, -1.5400e-01,  3.3164e-02,\n",
      "         7.9088e-02,  6.5250e-02,  5.1900e-02, -4.2283e-02, -1.1346e-01,\n",
      "        -9.0076e-03,  1.1980e-01, -1.1909e-02,  1.2310e-02,  1.8831e-02,\n",
      "        -4.9647e-02,  7.0969e-02, -2.3682e-02, -8.6618e-02,  5.2677e-02,\n",
      "         7.8079e-03, -1.0115e-01,  7.5915e-02, -4.8108e-02, -1.3128e-01,\n",
      "         6.4873e-02, -7.1029e-03, -1.4379e-01, -2.1432e-02, -5.3666e-02,\n",
      "         3.7874e-02, -8.1764e-02,  1.6618e-01,  7.1652e-02,  4.2189e-02,\n",
      "        -4.8112e-02,  5.0704e-02, -8.4332e-02,  2.3637e-02, -1.1713e-02,\n",
      "        -1.4738e-01, -5.6326e-02, -8.2328e-02, -6.9366e-03,  8.9393e-03,\n",
      "         9.0724e-02, -3.4346e-02, -1.7982e-02, -1.4817e-02, -9.2182e-02,\n",
      "         3.9414e-02, -1.3945e-02, -9.3391e-02,  1.0452e-01,  8.3443e-02,\n",
      "        -8.3101e-03,  5.8458e-02,  3.4724e-02, -9.1750e-02,  2.9846e-02,\n",
      "        -9.8895e-02, -2.4202e-02,  4.6580e-02,  4.4337e-02, -1.2447e-02,\n",
      "        -8.0480e-03, -5.6974e-03, -3.7265e-02,  7.7061e-02,  5.1464e-02,\n",
      "        -7.0224e-02, -4.4164e-02,  2.5564e-02,  1.2461e-02, -2.4537e-02,\n",
      "         2.2466e-02,  6.7765e-03, -2.1143e-02,  1.3173e-02, -4.8422e-02,\n",
      "        -2.4130e-02,  4.0795e-02, -6.9050e-02,  5.2960e-02,  2.9344e-02,\n",
      "         6.1323e-02,  2.6642e-02, -1.5501e-02,  1.1257e-02,  5.2199e-02,\n",
      "        -1.9131e-02, -7.1120e-02,  1.5206e-01, -5.5123e-02,  1.6600e-02,\n",
      "        -1.7471e-02,  5.4039e-02,  7.3465e-02, -1.4534e-02,  3.2988e-02,\n",
      "         1.0805e-01,  2.3235e-03,  2.6146e-02,  5.6207e-02,  2.4650e-02,\n",
      "         1.0190e-02, -4.5924e-03,  4.1432e-02, -4.8620e-02, -2.9034e-02,\n",
      "        -2.9012e-02,  1.4155e-02,  3.5942e-02, -9.4590e-03, -3.9627e-02,\n",
      "        -5.3268e-02,  1.3831e-01, -3.0257e-02, -5.7423e-03,  4.2466e-02,\n",
      "         1.2649e-01, -5.0767e-02, -1.1174e-02, -2.3112e-02,  3.8812e-02,\n",
      "        -6.3522e-02,  9.1453e-02,  2.6309e-02, -1.1686e-01, -3.9759e-02,\n",
      "         2.4578e-02, -4.7622e-03, -5.6869e-02,  9.6072e-02,  1.3556e-02,\n",
      "        -2.8459e-02, -4.5581e-02,  1.2914e-01, -1.1633e-02,  1.1193e-01,\n",
      "        -8.6753e-02, -8.5673e-03, -7.3127e-02, -3.6154e-02, -9.3040e-02,\n",
      "        -3.7462e-02,  1.2344e-01,  8.0146e-02, -1.7490e-02,  1.1924e-01,\n",
      "        -1.0738e-02,  6.7925e-02, -6.9445e-02, -2.5708e-02, -5.6665e-02,\n",
      "        -1.5419e-01,  1.2431e-01, -7.5615e-03, -1.0575e-01,  8.1955e-02,\n",
      "        -3.7937e-02,  8.6439e-02, -3.1533e-03,  1.4085e-01,  3.6980e-02,\n",
      "        -1.3440e-02, -5.1998e-02,  5.9634e-02, -4.4400e-02,  1.6468e-02,\n",
      "         3.7003e-02,  2.0843e-02,  4.8651e-02, -3.7829e-02,  1.0212e-01,\n",
      "        -1.8587e-02,  4.5990e-02, -4.5087e-03, -1.0517e-01, -7.8714e-02,\n",
      "        -2.2157e-02, -5.8386e-02,  7.0721e-02, -1.4240e-02, -1.0749e-01,\n",
      "        -6.8921e-02, -3.1443e-02, -3.2220e-02, -6.4972e-02,  1.1256e-02,\n",
      "         4.3494e-02,  1.8916e-02, -1.8547e-01, -2.1113e-02, -3.5792e-02,\n",
      "        -1.2145e-02,  4.6165e-02, -1.1010e-01,  3.3331e-04,  8.4547e-02,\n",
      "         5.4524e-02,  4.8118e-02, -9.5097e-02, -7.2445e-02, -6.6263e-05,\n",
      "         5.1787e-02,  4.9852e-02, -4.7932e-02, -1.2280e-02, -1.6250e-02,\n",
      "        -1.4342e-02, -1.1116e-01, -5.5778e-02, -7.7247e-03, -8.1662e-02,\n",
      "        -4.3206e-03,  6.6698e-02, -5.0373e-02, -1.2831e-01,  7.0735e-02,\n",
      "        -4.0484e-02, -2.6315e-02, -2.7391e-02, -8.0403e-02, -6.9732e-03,\n",
      "         5.4342e-02,  2.0656e-02,  1.5141e-01,  1.0275e-01,  1.5837e-03,\n",
      "        -1.4563e-01,  8.5911e-05,  4.7454e-03, -7.8300e-02,  4.8858e-02,\n",
      "        -2.1546e-02,  1.4427e-02,  4.6923e-02, -4.1582e-02,  3.4860e-02,\n",
      "         1.6094e-01, -2.8653e-02,  6.8671e-02,  3.9210e-02, -2.7989e-02,\n",
      "         1.2157e-01,  3.4874e-02,  1.0473e-01,  5.0698e-02, -6.6427e-02,\n",
      "        -8.5859e-02,  4.0868e-02, -8.1263e-02,  1.2227e-04, -4.1179e-02,\n",
      "         7.0834e-03,  8.5109e-02, -2.0567e-02,  6.0143e-03, -8.9583e-02,\n",
      "         6.3068e-02, -4.5089e-02,  2.6703e-02,  5.3511e-03,  9.8072e-03,\n",
      "         9.1949e-04,  4.8803e-02, -1.2944e-02, -1.6477e-02,  3.7466e-03,\n",
      "        -7.1968e-02, -6.9599e-02, -1.0072e-01, -7.0090e-02,  3.5817e-02,\n",
      "         6.2147e-02,  8.6350e-02,  8.2676e-02,  6.9734e-03, -1.6660e-01,\n",
      "         3.0636e-02, -7.5360e-02,  8.7070e-02,  4.6590e-02, -1.2240e-02,\n",
      "         4.7421e-02,  1.4499e-01, -3.2117e-02,  6.7256e-03, -9.1146e-03,\n",
      "         5.6627e-02,  3.4365e-02,  3.5674e-02,  1.1961e-03,  9.1195e-03,\n",
      "        -1.0258e-01, -2.6809e-02, -3.6439e-02, -5.3987e-02, -3.7285e-02,\n",
      "        -4.7299e-02,  2.0322e-02, -7.9408e-02, -7.7213e-02, -4.1219e-02,\n",
      "         1.1305e-01, -3.6860e-02,  3.4759e-02,  4.5197e-03, -1.8849e-02,\n",
      "        -1.1627e-02,  7.8283e-02, -5.6437e-02,  3.5024e-02,  6.2222e-02,\n",
      "        -8.2901e-02,  7.1049e-02,  9.9048e-03,  8.3881e-02,  3.7555e-03,\n",
      "         8.8532e-02,  9.2635e-02,  1.6246e-02, -3.0551e-02,  4.0173e-02,\n",
      "         3.9328e-02,  9.8969e-03,  7.2826e-04, -8.5527e-03,  1.9672e-02,\n",
      "         1.0268e-01, -4.0752e-03, -5.5843e-02,  1.5902e-02,  7.0855e-03,\n",
      "        -3.0325e-02,  2.9130e-02, -7.9757e-02,  2.0168e-02,  1.3599e-02,\n",
      "        -2.4822e-02, -8.0696e-03,  7.8805e-03,  3.1998e-04, -3.3752e-02,\n",
      "        -2.3653e-02,  7.4149e-02, -9.0394e-03, -6.5222e-03, -3.0573e-02,\n",
      "         1.1063e-01,  7.5828e-02,  4.1677e-02,  1.3911e-02, -7.0996e-03,\n",
      "         2.3597e-03,  2.6949e-03, -5.3042e-03,  7.1347e-02,  2.7978e-02,\n",
      "         9.5793e-04, -2.3873e-02, -7.2959e-02,  3.1148e-02, -6.5378e-02,\n",
      "         4.4773e-02, -4.6407e-02, -2.7808e-02,  6.0678e-02,  2.2824e-02,\n",
      "         1.2299e-02, -1.2252e-01, -9.4176e-02, -3.1335e-02,  6.1090e-02,\n",
      "        -8.9544e-02, -7.8463e-02, -1.0646e-01,  1.2856e-01,  5.3371e-02,\n",
      "        -3.5043e-02,  4.9204e-02, -2.7718e-02, -1.8169e-03, -3.2086e-02,\n",
      "         7.7823e-03,  6.8141e-03,  9.3693e-02,  1.6695e-02, -7.0995e-03,\n",
      "        -8.1406e-02, -1.0529e-02,  2.3930e-02, -2.4667e-02,  1.4599e-02,\n",
      "         2.2815e-02,  6.4431e-02, -8.6203e-02, -1.9157e-01,  3.7300e-02,\n",
      "        -2.8549e-02, -2.9900e-02,  2.0874e-02, -1.8929e-01,  6.7435e-02,\n",
      "        -4.1862e-02,  4.9628e-04,  7.5833e-03,  8.0471e-02, -1.7851e-02,\n",
      "        -4.5390e-02,  2.1833e-02, -1.6886e-02, -1.0043e-02, -7.4905e-02,\n",
      "        -9.9795e-04, -2.0626e-02,  8.3278e-02, -7.4464e-02,  3.2107e-02,\n",
      "         4.9412e-02, -5.9202e-02, -6.2015e-02,  1.0825e-02,  8.4142e-02,\n",
      "         6.0584e-02,  2.8453e-02, -6.4364e-02,  3.4312e-02, -3.1387e-02,\n",
      "        -1.0054e-02,  6.2364e-02,  9.9319e-02,  4.8268e-02,  3.6428e-02,\n",
      "         4.7602e-02, -2.9711e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 1.7651e-03,  3.1396e-03, -1.4551e-02, -3.5147e-03, -2.1400e-02,\n",
      "         1.1861e-01,  2.7863e-02, -5.9323e-02,  5.4408e-02, -3.9685e-02,\n",
      "        -2.9819e-02,  1.3290e-02, -3.3680e-02, -3.0514e-02, -9.5577e-02,\n",
      "        -2.7275e-02, -3.2411e-02,  1.0115e-01,  1.8278e-02,  5.4167e-02,\n",
      "        -6.5277e-02, -4.9623e-02,  3.8521e-02, -7.7113e-02,  2.3679e-02,\n",
      "        -2.1421e-02, -3.1328e-02, -3.8675e-02,  3.2301e-02,  8.0595e-02,\n",
      "        -1.7031e-01, -3.1086e-02,  7.4435e-02,  4.4086e-02, -5.2982e-02,\n",
      "        -3.7618e-02, -2.1757e-02,  2.6901e-02,  1.2416e-02, -6.1151e-02,\n",
      "        -5.6076e-03, -8.1322e-02,  1.2883e-01,  2.1244e-01,  6.3023e-03,\n",
      "        -6.4486e-02, -6.8903e-02, -5.2496e-02, -8.8419e-03, -4.8330e-03,\n",
      "        -9.8466e-02, -9.1724e-02, -4.6670e-03,  2.2265e-02,  7.4199e-03,\n",
      "        -6.7150e-03,  3.1992e-03, -1.9731e-02,  1.7806e-02, -7.7666e-03,\n",
      "         6.6665e-03, -4.9659e-03,  1.3266e-02, -3.1188e-02,  6.3222e-02,\n",
      "         2.4398e-02,  2.9437e-02, -7.7957e-04,  2.4054e-02,  1.6580e-01,\n",
      "        -7.9211e-02, -2.8934e-02,  3.4830e-02,  3.1386e-02,  1.2014e-03,\n",
      "         9.4098e-02, -5.5012e-03,  4.5756e-02,  3.2991e-02,  1.8693e-02,\n",
      "         4.4928e-02, -2.1605e-02,  4.0092e-02, -6.9511e-02, -8.6237e-02,\n",
      "        -1.2794e-01,  2.8559e-02, -3.4364e-02,  2.3834e-03,  9.2352e-03,\n",
      "        -2.0991e-03,  1.2794e-02,  1.0197e-02, -1.4751e-02, -5.3813e-03,\n",
      "         2.9286e-02,  8.8126e-02,  1.5448e-02, -1.4078e-02, -3.9143e-02,\n",
      "        -5.8560e-02,  5.4407e-02,  3.5490e-02, -7.9659e-02,  3.4453e-02,\n",
      "         2.5864e-02, -3.0899e-02, -1.5625e-02, -4.5447e-02,  4.7464e-02,\n",
      "        -3.1091e-02, -3.4445e-02, -5.4052e-02, -6.4918e-02,  4.4487e-02,\n",
      "         2.5045e-02, -1.1488e-02,  2.5262e-02, -1.2607e-02,  1.3235e-02,\n",
      "         2.8561e-02,  6.9778e-02,  3.5717e-02, -1.3796e-02, -1.6055e-01,\n",
      "        -6.3508e-02,  3.0388e-02,  3.6702e-02,  1.4510e-02,  8.2649e-02,\n",
      "        -9.6217e-03,  2.8959e-02,  3.8684e-02, -8.4300e-02, -1.5368e-01,\n",
      "         9.8709e-02, -7.2473e-02,  3.1997e-02,  1.1817e-01, -2.6140e-02,\n",
      "        -6.1742e-02, -1.6166e-02,  7.0216e-02, -1.2530e-01, -3.3601e-02,\n",
      "         1.8504e-02,  4.9253e-02,  1.5496e-01, -7.7431e-02, -1.4273e-02,\n",
      "        -1.3381e-02,  1.0467e-01, -7.3973e-02, -9.8395e-02, -2.9553e-02,\n",
      "         4.8231e-02,  6.4982e-02, -5.0469e-02,  3.5893e-02,  9.4489e-02,\n",
      "         6.2196e-02, -9.2381e-02, -8.7598e-02,  7.9401e-02, -6.6444e-02,\n",
      "        -1.0009e-02, -3.8275e-02, -2.5270e-02, -1.7952e-01, -9.5267e-03,\n",
      "        -1.3783e-01,  2.1312e-01, -1.1740e-02, -8.2986e-02,  3.5087e-02,\n",
      "        -1.9155e-02, -2.4328e-02, -4.0487e-02,  3.3686e-02, -1.7021e-02,\n",
      "        -5.0354e-02, -1.5596e-01, -1.7125e-03,  5.6674e-02,  6.6230e-03,\n",
      "         6.4058e-03, -3.7337e-03,  1.1259e-02, -2.4012e-02,  8.4532e-02,\n",
      "        -2.1994e-02,  3.6341e-03,  8.1102e-02, -5.8442e-02,  9.7022e-02,\n",
      "        -6.0901e-02,  5.0808e-02,  1.3352e-01,  1.6406e-02,  1.3148e-02,\n",
      "         2.8686e-02, -3.0704e-02, -4.3113e-02,  5.2098e-02, -5.5051e-02,\n",
      "        -1.1791e-01,  5.0002e-02,  2.3706e-03, -6.4074e-02,  5.0139e-02,\n",
      "        -3.7592e-02,  5.3099e-02,  3.9144e-02,  4.3691e-03,  1.4775e-02,\n",
      "        -7.3321e-02, -4.6698e-02,  1.2764e-01, -6.2895e-02, -2.6595e-02,\n",
      "         7.9530e-02,  3.6950e-02, -4.7796e-03,  3.2136e-02, -4.4875e-02,\n",
      "        -3.2131e-02,  8.3086e-02,  8.9513e-02, -6.2051e-03, -1.2118e-01,\n",
      "         2.6485e-02, -3.3139e-02,  4.4756e-02,  7.8008e-04,  7.1055e-02,\n",
      "         3.0050e-02,  8.2575e-03, -2.6538e-02, -3.9907e-02, -2.5800e-02,\n",
      "        -3.3800e-02,  1.8517e-02, -7.0688e-02, -1.3011e-01, -3.3101e-02,\n",
      "        -5.4424e-02,  3.0215e-02, -6.2839e-02,  2.4651e-02, -1.8812e-03,\n",
      "        -1.3442e-01,  1.2847e-02,  7.9453e-02,  8.0802e-02, -9.5993e-02,\n",
      "         3.4160e-02,  2.6102e-02, -8.6553e-03,  5.7268e-02,  8.5350e-02,\n",
      "         1.3918e-02,  1.1504e-02,  2.9779e-03,  1.0623e-02,  5.5536e-02,\n",
      "        -4.1146e-02, -9.3039e-02, -3.3455e-03,  1.5882e-02, -1.5050e-01,\n",
      "         7.5856e-03,  2.2823e-02, -3.8871e-02,  5.5844e-02,  5.4641e-03,\n",
      "        -2.4733e-02, -5.1179e-02, -1.8616e-02,  5.5658e-02, -6.9583e-02,\n",
      "        -6.0925e-02, -8.0161e-02, -1.0143e-01,  4.3837e-02,  1.3554e-01,\n",
      "         8.7156e-02,  2.5922e-02, -7.2726e-02, -1.8920e-02,  9.7482e-02,\n",
      "         2.0591e-02, -6.2224e-02,  5.4904e-02, -1.3960e-01, -7.6254e-02,\n",
      "         8.3799e-02, -3.9226e-02, -4.3723e-02, -3.3469e-02,  9.1810e-03,\n",
      "         4.9622e-02,  6.3080e-02, -2.8480e-02, -1.8700e-02,  6.6885e-02,\n",
      "        -6.8625e-03,  7.1043e-02,  7.1088e-02, -9.2783e-02,  9.1262e-02,\n",
      "         4.6247e-02, -2.9005e-02,  2.8690e-02,  1.9394e-02,  5.7164e-05,\n",
      "         2.2624e-02,  3.3163e-02,  1.7700e-02,  3.4232e-02, -2.9858e-02,\n",
      "        -7.4267e-02,  3.6014e-02, -4.4552e-02,  3.5258e-02, -1.0101e-01,\n",
      "        -6.7129e-03,  1.4119e-02,  2.7532e-02,  1.8333e-02,  1.0998e-01,\n",
      "        -4.3879e-04,  6.3078e-02,  1.9749e-02,  4.5188e-02,  1.7698e-02,\n",
      "        -1.6677e-02,  8.2497e-02, -7.5923e-02,  6.3407e-02,  6.3229e-02,\n",
      "         1.7209e-02,  8.9937e-02, -3.1758e-02,  2.4061e-02, -7.6937e-02,\n",
      "         2.9163e-03, -6.6448e-02, -1.3663e-02, -3.8498e-02, -6.1970e-02,\n",
      "        -5.3004e-02,  2.5560e-02,  1.7372e-01,  1.9347e-02,  7.7611e-02,\n",
      "         1.2019e-01, -1.5177e-01, -1.0369e-02, -3.0696e-02,  6.5096e-02,\n",
      "         1.3015e-02,  5.4550e-02, -5.5283e-02,  7.5891e-03, -2.0863e-02,\n",
      "        -2.2272e-02,  1.8210e-02, -6.6587e-03, -1.3865e-02,  5.7003e-02,\n",
      "        -1.9093e-02,  9.1872e-03,  9.9067e-02,  3.3590e-04,  4.0905e-02,\n",
      "         2.1044e-03, -6.7002e-03,  2.9374e-02, -1.1736e-02,  3.6019e-03,\n",
      "        -2.4367e-02, -3.7626e-02, -1.1231e-01,  1.7375e-02, -6.0035e-03,\n",
      "         5.7686e-02, -2.7193e-02,  1.9783e-02, -6.3263e-02,  2.2237e-02,\n",
      "         7.3779e-03, -2.8759e-03, -1.5603e-02,  5.7662e-02,  8.7457e-03,\n",
      "         1.0018e-02, -5.0072e-02, -3.7638e-02,  2.4585e-02, -9.2793e-02,\n",
      "        -1.1872e-01, -2.2116e-02, -1.0956e-01, -1.0836e-01,  8.6403e-02,\n",
      "         4.3467e-02,  2.0700e-02,  5.1945e-02,  3.0060e-02,  2.7744e-02,\n",
      "        -9.2273e-03,  7.5827e-02, -4.5446e-02,  8.2869e-02, -9.2931e-02,\n",
      "         1.2670e-02, -4.8179e-02, -1.5450e-01, -1.6038e-03, -2.9253e-02,\n",
      "        -2.7980e-02, -1.0475e-02,  2.7516e-02,  1.6998e-01,  2.4017e-02,\n",
      "         8.4535e-02, -2.9163e-04,  3.1187e-02,  5.4309e-02, -3.0479e-02,\n",
      "        -8.0611e-02, -6.6498e-02, -1.4551e-01,  2.1430e-03, -3.7552e-02,\n",
      "         7.6690e-02,  7.4113e-02, -1.0557e-01, -7.4909e-02,  6.7211e-02,\n",
      "        -7.8306e-02,  4.8829e-02, -4.6191e-02,  1.3408e-02,  2.7609e-02,\n",
      "        -7.1487e-03, -3.2693e-03,  6.9174e-02,  1.8630e-01,  8.2350e-02,\n",
      "        -7.1999e-02, -5.7636e-04, -1.0190e-01, -2.1849e-02, -2.4579e-02,\n",
      "         8.7751e-02, -3.5942e-02, -2.4704e-03, -1.1202e-01, -7.7516e-02,\n",
      "        -3.0877e-02,  5.2970e-02, -1.0476e-02,  9.5842e-03, -7.3720e-02,\n",
      "         4.0108e-02, -1.2442e-02,  3.8677e-02, -4.2649e-02,  3.2528e-02,\n",
      "         4.7383e-02, -1.2851e-03, -3.1630e-02,  9.8758e-02, -4.5205e-02,\n",
      "         9.8402e-02, -9.2297e-02, -1.9997e-02, -1.7744e-02, -2.2326e-02,\n",
      "         8.0307e-02, -1.7815e-02,  1.9394e-02, -5.2028e-02, -5.1993e-02,\n",
      "         5.9033e-03,  1.0825e-02, -1.7139e-02, -1.4043e-01,  3.3729e-02,\n",
      "         2.4079e-02,  3.1476e-02, -7.7750e-02,  4.4037e-03, -7.0054e-02,\n",
      "        -1.0412e-02,  8.7667e-03,  6.4475e-02, -6.7967e-02,  4.3379e-02,\n",
      "        -8.0798e-02,  1.3300e-01, -2.5715e-02,  4.1997e-02,  1.5607e-02,\n",
      "        -1.8457e-02, -1.4307e-02, -1.3592e-02, -8.4850e-04,  6.9601e-03,\n",
      "         1.7143e-02, -7.7591e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 7.0232e-02, -3.9177e-02, -5.6618e-02,  2.4083e-02, -1.1208e-01,\n",
      "        -8.1959e-02, -1.0443e-02,  3.7170e-02, -5.0199e-02,  3.4944e-02,\n",
      "        -1.1758e-01,  2.1176e-02, -1.3711e-02,  1.7198e-03,  5.5355e-03,\n",
      "        -2.0753e-02, -4.9122e-02, -1.6634e-02,  8.6761e-04,  5.3412e-02,\n",
      "        -5.2480e-02,  3.0198e-03, -2.5701e-02, -1.2355e-01,  5.1328e-02,\n",
      "        -6.1343e-02, -4.0117e-02,  6.6909e-02,  5.3694e-03,  2.3511e-02,\n",
      "        -8.8978e-03, -1.3987e-02,  5.8384e-02, -7.4027e-02,  7.0214e-03,\n",
      "        -2.3619e-02, -1.1647e-02,  5.0248e-03, -1.0472e-01,  5.0924e-02,\n",
      "         9.6729e-03,  4.4740e-02, -8.4304e-03,  3.1834e-02, -5.6028e-02,\n",
      "         1.2579e-02,  4.1046e-02, -7.6398e-02, -8.2215e-02, -7.4826e-02,\n",
      "        -5.8703e-03,  2.1899e-02, -3.4924e-02, -9.3841e-02,  9.2489e-02,\n",
      "        -3.9724e-02,  6.8778e-02,  6.2910e-02,  1.5035e-01, -8.5688e-02,\n",
      "         5.1889e-02, -9.3605e-02, -7.0402e-02,  4.7219e-02,  5.9798e-02,\n",
      "        -3.6312e-03, -1.3177e-02, -4.6579e-02,  2.6072e-02, -1.8031e-02,\n",
      "        -1.5455e-01,  1.6608e-01, -1.5167e-03, -2.2081e-02, -3.3239e-02,\n",
      "         7.1615e-03,  5.0772e-02,  6.4464e-03, -1.0717e-03,  1.1329e-01,\n",
      "        -4.0795e-03,  7.9883e-02, -4.3044e-02,  1.3580e-01, -1.0705e-02,\n",
      "         7.0666e-03, -7.1443e-03,  9.1426e-02, -1.3554e-03, -9.4658e-02,\n",
      "        -4.0040e-02,  5.9643e-02,  1.8720e-02, -6.1085e-03, -5.1143e-03,\n",
      "         5.2426e-03,  3.9795e-02,  5.7733e-02,  9.3336e-02,  4.6847e-03,\n",
      "         5.8702e-02,  2.5341e-02,  4.2893e-02,  7.5947e-02,  2.8520e-04,\n",
      "         7.1536e-03, -4.3884e-03, -4.4555e-02, -4.4503e-02,  5.6192e-02,\n",
      "        -5.1656e-02, -1.2393e-01,  4.3672e-02,  4.7996e-02, -1.0800e-02,\n",
      "         5.6755e-02, -8.3568e-02, -1.3538e-02, -5.6153e-02, -5.5316e-02,\n",
      "        -2.3975e-02, -1.1282e-01, -2.8566e-02,  7.2766e-02, -3.8624e-02,\n",
      "         7.6615e-02,  3.6164e-02,  1.0354e-01,  4.9160e-02,  1.9378e-02,\n",
      "        -2.2329e-02, -1.2350e-01,  1.2831e-01,  9.7161e-03,  8.3806e-02,\n",
      "        -5.0945e-02, -2.3909e-02, -2.4867e-02,  5.3618e-02,  4.3033e-02,\n",
      "        -8.6281e-03, -3.7764e-02, -1.2432e-01,  1.3901e-02, -8.2746e-02,\n",
      "         1.5292e-02, -1.0102e-01, -2.1163e-03,  2.4047e-02, -3.3842e-02,\n",
      "         1.7279e-01, -2.0493e-03, -1.4493e-02,  5.7667e-02, -2.8942e-02,\n",
      "        -3.2882e-03,  7.1961e-02,  1.5763e-02, -1.0857e-01,  3.1682e-02,\n",
      "        -1.5458e-02,  2.3903e-02, -7.8493e-02,  3.3385e-02, -1.1762e-02,\n",
      "         5.4726e-02, -1.0496e-01, -1.9116e-02,  4.4039e-02, -4.5159e-02,\n",
      "         1.1691e-01, -7.5459e-02, -3.4751e-02, -7.0932e-05, -5.4284e-03,\n",
      "        -3.1645e-02,  7.8052e-02, -1.3927e-02, -3.9138e-02, -6.9432e-02,\n",
      "        -5.6814e-02,  4.7092e-02, -9.7913e-02, -7.1706e-02, -7.4354e-02,\n",
      "         2.9061e-02,  1.2788e-01,  3.2878e-02,  6.8620e-02,  7.8050e-03,\n",
      "        -8.1034e-03,  1.2591e-01, -2.5306e-02,  2.3245e-02,  6.0525e-03,\n",
      "         5.1102e-02,  2.6583e-02, -2.1282e-03, -5.5411e-02,  4.6495e-02,\n",
      "        -2.4725e-02,  2.2852e-02, -1.2736e-02,  1.6637e-01, -5.4719e-02,\n",
      "         8.6107e-02, -5.4407e-02,  6.8237e-02, -6.1891e-02, -5.5849e-02,\n",
      "         7.3760e-03, -3.0345e-02, -3.1600e-02,  3.3583e-02,  2.8570e-02,\n",
      "         8.2200e-02,  2.6655e-02,  2.6249e-02, -1.2001e-02,  7.8356e-02,\n",
      "        -1.6183e-02, -1.4890e-02,  1.2511e-02,  3.7454e-02,  2.5717e-02,\n",
      "         2.4392e-03,  1.9375e-02,  6.4533e-02,  3.3817e-02, -6.6789e-02,\n",
      "        -8.1340e-02, -3.5166e-02, -2.8866e-02, -7.5490e-02,  3.9034e-02,\n",
      "        -4.9257e-02,  1.5981e-02,  1.2176e-02,  2.2973e-02,  1.0207e-02,\n",
      "        -1.0285e-03,  1.7862e-01,  6.4228e-02, -3.5339e-02,  8.1926e-02,\n",
      "         7.1711e-02, -1.0528e-02,  3.6034e-02, -2.3140e-02,  4.6343e-02,\n",
      "        -3.3368e-02, -4.6355e-02,  5.1168e-02,  1.8313e-02, -3.8195e-03,\n",
      "         1.0237e-01, -4.0303e-02,  3.3172e-02, -4.5773e-02, -1.4106e-02,\n",
      "        -3.1364e-02,  5.1665e-02, -3.1724e-02,  3.0433e-02, -3.9412e-02,\n",
      "        -3.3040e-02,  2.1146e-02, -1.1771e-01, -6.6739e-02, -3.3981e-02,\n",
      "        -4.4390e-03,  2.8506e-02,  1.9362e-02,  1.0839e-01, -1.5109e-02,\n",
      "         1.5135e-01, -2.9912e-02,  7.0132e-02, -6.2905e-02, -9.2045e-02,\n",
      "        -1.0811e-01, -6.4596e-02,  1.1569e-01,  4.2324e-02, -5.3588e-02,\n",
      "         2.2440e-02, -2.2649e-02, -7.6581e-02, -6.1811e-02,  6.7117e-02,\n",
      "        -8.8734e-02,  1.1926e-02, -1.0264e-02,  2.1893e-02,  6.1756e-02,\n",
      "         1.1959e-01, -9.6380e-02,  1.3470e-02, -7.0965e-02,  2.2478e-02,\n",
      "         5.0166e-02, -4.6788e-03,  9.3105e-02,  1.2183e-01, -1.0024e-01,\n",
      "         1.9777e-04,  8.1114e-02, -2.6921e-02,  1.0334e-01, -3.6504e-02,\n",
      "         1.0802e-02, -2.5081e-02, -6.5181e-02,  8.6339e-02,  3.7305e-02,\n",
      "        -1.2546e-01, -2.3171e-02, -5.1505e-02,  8.1840e-02,  4.9002e-02,\n",
      "         1.8363e-02,  2.9693e-02, -3.8902e-03, -4.1257e-02, -2.2935e-02,\n",
      "         8.3203e-02,  5.9329e-02,  7.7033e-03,  4.9673e-02, -3.4751e-02,\n",
      "        -3.4831e-03, -7.7208e-03,  1.0457e-01, -2.1170e-02,  6.3125e-02,\n",
      "        -1.2047e-02,  1.4499e-02, -5.0847e-02,  2.7684e-02,  8.1270e-02,\n",
      "        -2.4067e-02,  1.6061e-04,  4.5172e-02,  8.9830e-02,  5.0638e-03,\n",
      "        -2.7056e-02,  1.6215e-02, -1.2409e-01,  2.7129e-02, -3.0758e-02,\n",
      "        -4.1683e-02,  7.6068e-03,  1.4988e-02, -2.3955e-02, -8.0970e-02,\n",
      "        -1.0192e-01,  3.6965e-02, -2.6476e-02,  5.7144e-03,  7.6527e-02,\n",
      "         9.9065e-02,  4.3809e-02,  5.6087e-02, -6.8878e-02,  6.4834e-02,\n",
      "         2.2787e-02,  7.3976e-02,  9.9496e-03, -1.7695e-02,  8.8900e-02,\n",
      "        -9.6980e-02, -7.0818e-02,  4.9335e-02, -5.5873e-02, -6.7333e-03,\n",
      "        -1.0160e-02,  2.7102e-02, -2.2473e-02, -4.0724e-02, -4.3555e-02,\n",
      "        -4.3084e-02, -8.1544e-02, -2.8473e-02,  1.8932e-02,  2.8450e-02,\n",
      "        -8.1699e-02, -8.1030e-02,  4.9583e-02,  3.5871e-02, -1.5891e-02,\n",
      "        -8.8298e-03, -1.2130e-02, -8.1447e-02,  4.5123e-02,  6.6769e-02,\n",
      "         4.5007e-02, -1.0901e-02, -1.5257e-01,  1.2816e-02,  6.9188e-02,\n",
      "        -1.3537e-02,  1.0406e-01,  2.4015e-02,  4.3749e-02, -2.3074e-02,\n",
      "         6.2925e-02, -3.2508e-02, -5.4690e-02,  1.6847e-02, -5.7822e-02,\n",
      "         6.0195e-02, -1.6088e-02, -3.6611e-02,  4.3164e-03, -1.6129e-02,\n",
      "        -2.0027e-02,  2.7187e-02, -5.7546e-02,  1.6556e-02, -4.5320e-04,\n",
      "         2.4637e-02, -5.7647e-02, -4.5837e-02,  1.4810e-02,  1.4818e-02,\n",
      "        -9.2751e-03,  3.1316e-02,  4.6298e-02,  1.5679e-02,  2.5335e-02,\n",
      "         1.5162e-02, -6.5274e-02, -1.1448e-01,  3.5900e-02, -1.1034e-01,\n",
      "        -9.4011e-02,  3.3696e-02, -6.7059e-03,  1.4441e-02,  1.3973e-01,\n",
      "         7.2340e-02, -5.2067e-02, -1.5580e-02,  4.3312e-02, -6.7398e-02,\n",
      "         7.6808e-02, -4.1142e-02,  3.2319e-02,  1.2461e-01,  1.5610e-02,\n",
      "         7.3369e-02, -1.0851e-01, -4.5686e-02, -6.5544e-02,  7.0161e-02,\n",
      "        -4.9590e-03,  4.6399e-02,  4.5816e-02, -7.6833e-02,  5.7388e-02,\n",
      "         5.6216e-02,  1.7794e-02, -1.8920e-02, -4.4150e-02,  2.6347e-02,\n",
      "         8.7239e-02, -2.0536e-02, -1.2006e-02, -5.0354e-03,  3.5649e-02,\n",
      "        -8.1056e-02,  5.1311e-02,  1.9925e-02, -4.3425e-02,  2.6601e-02,\n",
      "        -7.5502e-02, -3.4638e-02, -7.5277e-02, -5.1211e-02, -4.9907e-02,\n",
      "         1.9271e-02,  2.3710e-02,  1.7192e-02, -7.7708e-02,  2.5729e-02,\n",
      "         5.5325e-02,  1.0182e-01, -9.2568e-02, -4.8824e-02,  2.3749e-02,\n",
      "         3.6623e-02, -1.6246e-02, -2.5600e-02, -8.7405e-02,  1.7550e-02,\n",
      "        -6.1699e-03, -4.0138e-02, -3.5954e-02, -6.4890e-02,  4.1684e-03,\n",
      "        -8.0014e-02, -7.6652e-02,  9.0478e-02,  2.2696e-03,  4.3178e-03,\n",
      "         1.3625e-01, -4.3848e-02,  3.4243e-02,  1.0695e-01,  2.2553e-02,\n",
      "        -1.3336e-02, -3.6943e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-6.3037e-02,  4.0266e-02, -4.1982e-02, -1.3677e-01, -2.8125e-02,\n",
      "        -3.5907e-02, -1.2441e-01,  3.9413e-02,  7.0257e-02,  2.5938e-02,\n",
      "         1.6168e-02, -4.9772e-03, -8.1783e-02,  5.8682e-02, -5.5678e-03,\n",
      "        -1.6609e-02, -7.0250e-02,  2.3007e-02, -1.1237e-01, -1.1362e-03,\n",
      "         2.6813e-02,  5.1206e-02, -1.0730e-01, -7.5571e-03,  6.9360e-02,\n",
      "        -3.0615e-02, -1.3997e-01, -1.6990e-02, -6.2863e-03,  3.9499e-02,\n",
      "         1.1053e-01,  7.3368e-02, -4.2755e-03, -6.1965e-02,  2.3791e-02,\n",
      "        -5.2535e-02, -2.8180e-02, -1.5272e-02, -2.4229e-02,  8.8722e-02,\n",
      "        -9.6024e-02,  4.9923e-02, -1.7209e-02,  1.6388e-02, -7.5840e-03,\n",
      "        -8.1901e-02,  6.1073e-02, -4.8348e-02, -1.0459e-02, -6.6470e-02,\n",
      "        -2.4781e-02,  5.3203e-02,  2.4020e-02,  4.9423e-02,  3.7947e-02,\n",
      "         2.0673e-01,  6.0895e-02, -4.0758e-02,  2.0071e-02,  1.0387e-01,\n",
      "         1.6163e-02, -1.5090e-02,  6.2854e-02,  2.6011e-02,  3.8561e-02,\n",
      "        -5.5162e-02, -7.5800e-02,  6.2836e-02,  1.3994e-02,  1.1939e-01,\n",
      "        -1.5908e-02,  4.2999e-02, -2.3383e-02,  1.5983e-02, -1.3671e-02,\n",
      "         9.9919e-02,  6.4056e-02, -7.5082e-02, -1.2190e-02, -1.4694e-02,\n",
      "         5.8069e-02,  5.5209e-02,  1.2079e-02,  5.1134e-02,  4.5578e-02,\n",
      "         5.2929e-02, -6.7412e-03,  9.7755e-02, -4.7786e-02, -1.6850e-02,\n",
      "        -5.9766e-02,  9.2122e-02, -2.9754e-02, -1.1698e-01,  2.3706e-02,\n",
      "        -2.3814e-02,  3.3031e-02, -1.1580e-01,  3.8596e-02, -3.3136e-03,\n",
      "        -1.2250e-02,  3.4611e-02, -8.4193e-02, -7.5750e-02, -3.5521e-02,\n",
      "        -5.1473e-03,  3.9007e-02,  6.4325e-03, -5.9280e-02, -1.3100e-02,\n",
      "        -4.1139e-02,  4.7848e-02,  8.4264e-03, -1.0753e-01, -4.3760e-02,\n",
      "         9.4994e-02, -1.7219e-02,  3.9596e-02, -4.1659e-02,  1.2531e-01,\n",
      "        -4.9070e-02,  1.2569e-02, -3.4510e-02,  5.6004e-02, -2.7773e-02,\n",
      "        -8.0413e-02,  7.7013e-02, -3.7365e-02, -7.8601e-02, -4.4590e-02,\n",
      "         1.6158e-02,  2.7064e-02,  1.0510e-01, -1.2408e-02,  1.6963e-02,\n",
      "        -9.1978e-03,  5.7486e-02, -4.6821e-02, -3.0573e-03, -1.0964e-02,\n",
      "        -8.9452e-02,  4.2682e-02, -1.1941e-02,  2.5132e-02, -3.7705e-02,\n",
      "         5.4186e-02, -7.1975e-02, -4.9173e-02, -6.7192e-02,  2.7494e-02,\n",
      "         2.4167e-03,  3.7371e-02,  4.2284e-02,  3.3118e-02,  5.1909e-02,\n",
      "        -6.6921e-02, -5.8869e-02, -6.1932e-02,  3.1455e-02, -2.2885e-02,\n",
      "        -9.3647e-02, -1.9637e-02,  5.1098e-02,  4.5610e-02, -4.1068e-02,\n",
      "         5.7816e-02, -8.5963e-04,  2.2186e-02, -1.8173e-02,  4.3025e-02,\n",
      "        -3.6500e-02,  4.6611e-02,  1.1417e-01, -6.0109e-02, -6.6532e-02,\n",
      "         9.2543e-02,  1.5739e-02, -7.0260e-03, -4.5298e-02, -4.6085e-02,\n",
      "        -1.7641e-02, -3.4245e-02, -2.9982e-02, -3.3564e-02, -2.3251e-02,\n",
      "        -9.0132e-02, -4.9113e-02, -1.5003e-02, -3.4544e-02, -1.2240e-02,\n",
      "        -6.6013e-02, -1.2225e-01,  2.1974e-02, -7.2869e-02,  7.3213e-02,\n",
      "         7.8171e-02, -1.1407e-02,  1.2900e-02,  1.3423e-02,  6.1885e-02,\n",
      "         8.2777e-02,  5.9639e-03, -2.9608e-02,  1.4335e-02, -3.0911e-02,\n",
      "        -2.6568e-02, -7.7970e-02,  5.7262e-02,  7.5148e-03, -8.3736e-02,\n",
      "         1.1164e-01, -3.6595e-02, -3.5647e-02, -2.2155e-02,  3.7071e-02,\n",
      "         5.2191e-03, -5.0187e-02, -1.0465e-02, -2.7389e-02,  2.4710e-02,\n",
      "         3.4442e-02, -3.3596e-02,  5.7857e-02,  4.2296e-02, -2.8121e-02,\n",
      "         3.7366e-02, -5.9914e-02,  1.6653e-02,  3.8050e-02,  5.3976e-02,\n",
      "         1.6561e-02,  5.0949e-02,  7.7352e-02,  8.3561e-02, -4.3670e-02,\n",
      "        -8.8957e-03,  2.0743e-03,  3.0768e-02, -3.4656e-02,  1.0132e-01,\n",
      "         2.0802e-02, -1.4734e-01, -1.1625e-02, -4.6762e-03,  1.0868e-01,\n",
      "         8.2071e-02, -1.4927e-02, -1.5449e-01, -7.1360e-02,  6.3504e-02,\n",
      "        -1.3678e-02, -3.2650e-02,  8.5200e-02, -4.5086e-02,  2.2611e-02,\n",
      "        -1.0392e-01, -6.0944e-02,  1.4738e-02,  3.9227e-02, -8.7592e-03,\n",
      "        -2.2234e-02, -5.5263e-03, -3.3027e-02,  3.9625e-03,  1.5417e-02,\n",
      "         1.2909e-02,  1.0592e-01, -5.5637e-02,  1.6255e-01, -8.2178e-02,\n",
      "         9.2043e-02,  1.9381e-03,  2.2714e-02,  3.5822e-02, -1.0901e-03,\n",
      "         1.2325e-02, -6.4859e-02, -2.5885e-02,  5.1314e-02, -4.6941e-04,\n",
      "        -2.8895e-03,  1.1293e-02, -1.7513e-02, -6.6949e-02,  6.9416e-02,\n",
      "         7.1142e-03, -1.4641e-03, -3.6779e-02,  1.1385e-01, -4.7641e-02,\n",
      "         1.4738e-02, -6.2718e-02,  8.7415e-02, -5.5629e-03,  2.7129e-02,\n",
      "         6.3722e-03,  3.4799e-02,  2.5760e-02, -7.6286e-02, -6.1321e-02,\n",
      "        -5.3081e-02, -1.3048e-03, -1.7442e-02, -1.6667e-01,  3.7299e-03,\n",
      "         1.3328e-02,  6.2362e-02,  1.6265e-02,  5.9280e-02, -9.6899e-02,\n",
      "        -9.9530e-03,  3.4732e-02,  5.4185e-03, -4.3835e-03,  3.4801e-02,\n",
      "         4.0341e-02, -1.1303e-02, -2.8805e-02,  2.6510e-02, -4.8988e-02,\n",
      "        -1.4906e-02, -8.7503e-02, -3.8591e-03,  3.9093e-02,  2.1345e-02,\n",
      "         4.3803e-02, -4.8825e-02, -3.8691e-02, -7.1864e-02, -5.9994e-02,\n",
      "         2.5898e-02, -4.4769e-02,  8.8324e-02, -7.2772e-02,  1.5155e-02,\n",
      "        -5.5817e-02,  5.3736e-02, -2.9101e-02,  1.5793e-03, -1.7930e-01,\n",
      "        -1.7445e-02, -6.8678e-02, -2.1378e-02, -4.4950e-02, -1.7106e-02,\n",
      "         1.5411e-01,  7.0336e-02,  3.1394e-02,  9.1400e-02, -6.3379e-02,\n",
      "         9.3097e-02,  6.2873e-02, -2.3895e-02,  4.8823e-02,  1.5050e-02,\n",
      "         1.5749e-01,  2.0483e-02,  2.5478e-02,  1.2565e-01,  6.4963e-02,\n",
      "        -3.3720e-02,  3.8453e-02, -6.7775e-02, -1.1753e-01,  6.8093e-02,\n",
      "         5.1249e-02, -1.5064e-01, -6.5369e-02,  4.8224e-02, -8.1458e-03,\n",
      "        -2.7762e-02, -2.5249e-02, -1.0149e-02, -1.9384e-02,  4.1005e-02,\n",
      "        -2.7609e-02, -9.2976e-02,  3.8276e-02,  7.2089e-02, -1.2936e-01,\n",
      "        -1.1778e-01, -6.5505e-02,  1.7166e-02,  1.5751e-02, -1.9162e-02,\n",
      "         5.5185e-03, -1.0558e-01, -2.3025e-02, -1.4394e-01,  1.1885e-01,\n",
      "         9.7875e-03, -9.7859e-02, -3.9622e-02, -4.5969e-02, -4.3369e-02,\n",
      "        -2.5617e-02, -5.2712e-02,  3.9468e-02,  1.0800e-01,  5.3185e-02,\n",
      "        -5.0451e-02,  5.3125e-02,  1.4214e-01,  1.0340e-01, -1.7702e-02,\n",
      "        -6.3901e-02,  3.0720e-02, -7.3908e-02,  9.5226e-02,  6.2002e-03,\n",
      "         5.0914e-02, -6.5561e-02,  5.4568e-02,  5.1027e-02, -4.2785e-02,\n",
      "        -7.9318e-02,  6.1157e-02,  6.2453e-02, -4.5603e-02, -2.7345e-02,\n",
      "        -5.6974e-02,  1.2981e-01,  1.0213e-01,  4.7302e-02, -2.4651e-02,\n",
      "        -3.3669e-02, -4.9926e-02,  7.3012e-02, -3.4709e-02,  1.2907e-01,\n",
      "         6.1702e-02,  3.1375e-02,  1.9113e-02, -9.1100e-02,  7.3931e-03,\n",
      "        -8.0293e-02, -3.6101e-02,  5.1210e-02, -2.9621e-02,  5.9973e-03,\n",
      "         9.6392e-02,  4.1492e-03,  2.3054e-02, -8.5028e-02,  1.3075e-03,\n",
      "        -8.0786e-02,  7.1889e-02, -3.7784e-02,  2.0823e-02, -5.1179e-02,\n",
      "         1.3547e-01,  4.0677e-02,  6.0206e-02, -4.4987e-03, -1.4705e-02,\n",
      "        -4.3924e-05,  2.2686e-02, -3.4385e-02, -3.4656e-02, -1.7687e-01,\n",
      "         4.2150e-02, -3.2622e-03, -4.4221e-02, -5.9327e-02, -1.2178e-01,\n",
      "        -9.8243e-02,  2.9285e-02,  1.0800e-01, -2.9136e-02, -1.2633e-02,\n",
      "        -8.9605e-02,  1.0191e-02,  2.9528e-02, -1.6184e-02, -2.1323e-02,\n",
      "         4.3191e-02, -5.9493e-02, -9.2964e-02, -2.2478e-02,  1.4769e-02,\n",
      "        -2.1768e-02,  4.5379e-02,  7.2459e-02,  6.8969e-02, -3.1864e-02,\n",
      "         9.2427e-03,  1.1675e-01, -1.2651e-02, -8.6167e-02, -7.0927e-02,\n",
      "        -3.2216e-02, -3.6091e-02,  1.1292e-02, -2.3667e-02,  1.0530e-01,\n",
      "        -2.7349e-02,  3.5006e-02, -8.4804e-02,  5.3443e-02, -3.9848e-02,\n",
      "        -1.8628e-02, -9.9607e-02, -1.0862e-01,  3.0266e-03,  6.8604e-02,\n",
      "        -2.6277e-02, -1.4869e-01, -4.6595e-02, -9.2243e-02,  5.9800e-02,\n",
      "         1.6846e-02, -5.4605e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  1.844741/ 18.090490, val:  47.92%, val_best:  47.92%, tr:  37.59%, tr_best:  37.59%, epoch time: 29.25 seconds, 0.49 minutes\n",
      "[module.layers.3] weight_fb parameter count: 5,120\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  1.397868/ 16.969030, val:  47.92%, val_best:  47.92%, tr:  48.42%, tr_best:  48.42%, epoch time: 29.75 seconds, 0.50 minutes\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  1.281240/ 17.853794, val:  50.42%, val_best:  50.42%, tr:  51.17%, tr_best:  51.17%, epoch time: 29.39 seconds, 0.49 minutes\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  1.189539/ 14.130339, val:  55.83%, val_best:  55.83%, tr:  55.77%, tr_best:  55.77%, epoch time: 30.25 seconds, 0.50 minutes\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  1.081268/ 15.168821, val:  59.58%, val_best:  59.58%, tr:  59.35%, tr_best:  59.35%, epoch time: 28.25 seconds, 0.47 minutes\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  1.044978/ 12.050562, val:  60.42%, val_best:  60.42%, tr:  62.51%, tr_best:  62.51%, epoch time: 30.04 seconds, 0.50 minutes\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  1.021193/ 14.555198, val:  57.50%, val_best:  60.42%, tr:  61.18%, tr_best:  62.51%, epoch time: 28.09 seconds, 0.47 minutes\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  0.986745/ 20.158487, val:  50.00%, val_best:  60.42%, tr:  64.04%, tr_best:  64.04%, epoch time: 29.24 seconds, 0.49 minutes\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  0.973813/ 11.274011, val:  62.08%, val_best:  62.08%, tr:  62.92%, tr_best:  64.04%, epoch time: 28.33 seconds, 0.47 minutes\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  0.899705/ 20.201815, val:  50.00%, val_best:  62.08%, tr:  68.64%, tr_best:  68.64%, epoch time: 28.98 seconds, 0.48 minutes\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  0.844973/ 15.897049, val:  57.92%, val_best:  62.08%, tr:  69.56%, tr_best:  69.56%, epoch time: 28.70 seconds, 0.48 minutes\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  0.898629/ 12.063983, val:  64.58%, val_best:  64.58%, tr:  65.88%, tr_best:  69.56%, epoch time: 28.93 seconds, 0.48 minutes\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  0.855853/ 18.289354, val:  58.33%, val_best:  64.58%, tr:  70.07%, tr_best:  70.07%, epoch time: 29.47 seconds, 0.49 minutes\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  0.857194/ 12.452986, val:  60.83%, val_best:  64.58%, tr:  68.95%, tr_best:  70.07%, epoch time: 28.41 seconds, 0.47 minutes\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  0.792251/ 15.695478, val:  56.67%, val_best:  64.58%, tr:  71.20%, tr_best:  71.20%, epoch time: 29.61 seconds, 0.49 minutes\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  0.823309/ 15.354878, val:  55.42%, val_best:  64.58%, tr:  70.89%, tr_best:  71.20%, epoch time: 28.77 seconds, 0.48 minutes\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  0.799975/ 12.862650, val:  60.83%, val_best:  64.58%, tr:  70.68%, tr_best:  71.20%, epoch time: 29.37 seconds, 0.49 minutes\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  0.781873/ 10.667490, val:  67.92%, val_best:  67.92%, tr:  70.89%, tr_best:  71.20%, epoch time: 29.25 seconds, 0.49 minutes\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  0.743590/ 11.141347, val:  70.83%, val_best:  70.83%, tr:  73.24%, tr_best:  73.24%, epoch time: 29.70 seconds, 0.50 minutes\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  0.728721/ 16.516714, val:  60.42%, val_best:  70.83%, tr:  72.52%, tr_best:  73.24%, epoch time: 29.21 seconds, 0.49 minutes\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  0.724153/ 12.535826, val:  60.00%, val_best:  70.83%, tr:  75.28%, tr_best:  75.28%, epoch time: 28.99 seconds, 0.48 minutes\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  0.732235/ 16.223589, val:  63.75%, val_best:  70.83%, tr:  72.52%, tr_best:  75.28%, epoch time: 28.93 seconds, 0.48 minutes\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  0.748391/ 15.640448, val:  68.33%, val_best:  70.83%, tr:  73.14%, tr_best:  75.28%, epoch time: 28.64 seconds, 0.48 minutes\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  0.731410/ 15.541622, val:  64.17%, val_best:  70.83%, tr:  73.14%, tr_best:  75.28%, epoch time: 29.61 seconds, 0.49 minutes\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  0.726410/ 15.223976, val:  60.83%, val_best:  70.83%, tr:  72.52%, tr_best:  75.28%, epoch time: 28.47 seconds, 0.47 minutes\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  0.669504/ 13.156348, val:  62.50%, val_best:  70.83%, tr:  75.69%, tr_best:  75.69%, epoch time: 29.94 seconds, 0.50 minutes\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  0.679672/ 16.919987, val:  61.67%, val_best:  70.83%, tr:  73.95%, tr_best:  75.69%, epoch time: 28.53 seconds, 0.48 minutes\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  0.695421/ 19.543388, val:  60.00%, val_best:  70.83%, tr:  73.75%, tr_best:  75.69%, epoch time: 29.77 seconds, 0.50 minutes\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  0.645839/ 19.850220, val:  57.50%, val_best:  70.83%, tr:  76.71%, tr_best:  76.71%, epoch time: 29.10 seconds, 0.48 minutes\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  0.670938/  9.806589, val:  71.25%, val_best:  71.25%, tr:  76.00%, tr_best:  76.71%, epoch time: 29.21 seconds, 0.49 minutes\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  0.620323/ 11.036677, val:  68.75%, val_best:  71.25%, tr:  78.35%, tr_best:  78.35%, epoch time: 28.95 seconds, 0.48 minutes\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  0.638584/ 18.118736, val:  61.67%, val_best:  71.25%, tr:  77.02%, tr_best:  78.35%, epoch time: 28.93 seconds, 0.48 minutes\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  0.600768/ 15.962651, val:  64.58%, val_best:  71.25%, tr:  78.65%, tr_best:  78.65%, epoch time: 29.90 seconds, 0.50 minutes\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  0.624489/ 14.138690, val:  63.33%, val_best:  71.25%, tr:  77.32%, tr_best:  78.65%, epoch time: 28.75 seconds, 0.48 minutes\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  0.610220/ 17.078398, val:  62.92%, val_best:  71.25%, tr:  79.57%, tr_best:  79.57%, epoch time: 30.70 seconds, 0.51 minutes\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  0.623274/ 16.266247, val:  64.17%, val_best:  71.25%, tr:  77.22%, tr_best:  79.57%, epoch time: 29.08 seconds, 0.48 minutes\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  0.587225/ 15.289602, val:  61.67%, val_best:  71.25%, tr:  78.35%, tr_best:  79.57%, epoch time: 28.76 seconds, 0.48 minutes\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  0.567605/ 17.371927, val:  62.50%, val_best:  71.25%, tr:  78.35%, tr_best:  79.57%, epoch time: 29.03 seconds, 0.48 minutes\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  0.607454/ 10.788736, val:  68.75%, val_best:  71.25%, tr:  77.83%, tr_best:  79.57%, epoch time: 29.89 seconds, 0.50 minutes\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  0.599861/ 14.092805, val:  62.50%, val_best:  71.25%, tr:  77.73%, tr_best:  79.57%, epoch time: 29.38 seconds, 0.49 minutes\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  0.594636/ 16.229288, val:  64.58%, val_best:  71.25%, tr:  79.57%, tr_best:  79.57%, epoch time: 30.00 seconds, 0.50 minutes\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  0.612552/ 17.753199, val:  62.08%, val_best:  71.25%, tr:  77.12%, tr_best:  79.57%, epoch time: 30.16 seconds, 0.50 minutes\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  0.602591/ 14.729235, val:  61.25%, val_best:  71.25%, tr:  77.94%, tr_best:  79.57%, epoch time: 28.90 seconds, 0.48 minutes\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  0.583711/ 15.355892, val:  64.58%, val_best:  71.25%, tr:  79.16%, tr_best:  79.57%, epoch time: 29.47 seconds, 0.49 minutes\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  0.537869/ 17.460083, val:  61.25%, val_best:  71.25%, tr:  79.26%, tr_best:  79.57%, epoch time: 28.90 seconds, 0.48 minutes\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  0.584593/ 11.697106, val:  71.25%, val_best:  71.25%, tr:  78.96%, tr_best:  79.57%, epoch time: 29.72 seconds, 0.50 minutes\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  0.553311/ 21.320612, val:  65.42%, val_best:  71.25%, tr:  80.90%, tr_best:  80.90%, epoch time: 30.33 seconds, 0.51 minutes\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  0.557201/ 11.864925, val:  68.75%, val_best:  71.25%, tr:  79.98%, tr_best:  80.90%, epoch time: 29.15 seconds, 0.49 minutes\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  0.512818/ 11.647858, val:  71.25%, val_best:  71.25%, tr:  82.23%, tr_best:  82.23%, epoch time: 30.14 seconds, 0.50 minutes\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  0.534068/ 16.821745, val:  57.92%, val_best:  71.25%, tr:  83.04%, tr_best:  83.04%, epoch time: 30.54 seconds, 0.51 minutes\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  0.569441/ 11.657430, val:  69.58%, val_best:  71.25%, tr:  80.18%, tr_best:  83.04%, epoch time: 29.62 seconds, 0.49 minutes\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  0.546659/ 13.223920, val:  63.33%, val_best:  71.25%, tr:  79.78%, tr_best:  83.04%, epoch time: 29.25 seconds, 0.49 minutes\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  0.563376/ 14.490541, val:  65.83%, val_best:  71.25%, tr:  78.65%, tr_best:  83.04%, epoch time: 30.28 seconds, 0.50 minutes\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  0.542328/ 11.606313, val:  65.83%, val_best:  71.25%, tr:  80.29%, tr_best:  83.04%, epoch time: 30.15 seconds, 0.50 minutes\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  0.537113/ 13.314172, val:  62.50%, val_best:  71.25%, tr:  79.57%, tr_best:  83.04%, epoch time: 28.88 seconds, 0.48 minutes\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  0.552372/ 17.240654, val:  63.33%, val_best:  71.25%, tr:  80.90%, tr_best:  83.04%, epoch time: 27.92 seconds, 0.47 minutes\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  0.523466/ 16.000084, val:  65.00%, val_best:  71.25%, tr:  80.80%, tr_best:  83.04%, epoch time: 29.33 seconds, 0.49 minutes\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  0.515194/ 13.291189, val:  66.25%, val_best:  71.25%, tr:  80.69%, tr_best:  83.04%, epoch time: 28.64 seconds, 0.48 minutes\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  0.512164/ 11.175978, val:  70.42%, val_best:  71.25%, tr:  81.00%, tr_best:  83.04%, epoch time: 29.63 seconds, 0.49 minutes\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  0.499865/ 17.421896, val:  64.17%, val_best:  71.25%, tr:  81.92%, tr_best:  83.04%, epoch time: 28.76 seconds, 0.48 minutes\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  0.506378/ 22.442610, val:  57.92%, val_best:  71.25%, tr:  81.72%, tr_best:  83.04%, epoch time: 30.16 seconds, 0.50 minutes\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  0.522349/ 16.959202, val:  65.00%, val_best:  71.25%, tr:  81.72%, tr_best:  83.04%, epoch time: 29.35 seconds, 0.49 minutes\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  0.536640/ 18.783567, val:  59.58%, val_best:  71.25%, tr:  79.26%, tr_best:  83.04%, epoch time: 29.11 seconds, 0.49 minutes\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  0.486878/ 18.422508, val:  62.50%, val_best:  71.25%, tr:  82.74%, tr_best:  83.04%, epoch time: 29.57 seconds, 0.49 minutes\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  0.488805/ 11.863545, val:  65.42%, val_best:  71.25%, tr:  82.84%, tr_best:  83.04%, epoch time: 29.25 seconds, 0.49 minutes\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  0.487553/ 11.202729, val:  69.58%, val_best:  71.25%, tr:  82.53%, tr_best:  83.04%, epoch time: 29.21 seconds, 0.49 minutes\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  0.501543/ 12.762075, val:  67.92%, val_best:  71.25%, tr:  82.64%, tr_best:  83.04%, epoch time: 29.49 seconds, 0.49 minutes\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  0.471601/ 14.349829, val:  65.83%, val_best:  71.25%, tr:  83.86%, tr_best:  83.86%, epoch time: 28.85 seconds, 0.48 minutes\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  0.475604/ 16.034626, val:  65.83%, val_best:  71.25%, tr:  83.55%, tr_best:  83.86%, epoch time: 28.48 seconds, 0.47 minutes\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  0.481018/ 14.803588, val:  65.00%, val_best:  71.25%, tr:  82.33%, tr_best:  83.86%, epoch time: 29.32 seconds, 0.49 minutes\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  0.498067/ 11.803315, val:  67.50%, val_best:  71.25%, tr:  82.43%, tr_best:  83.86%, epoch time: 28.61 seconds, 0.48 minutes\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  0.494479/ 13.639407, val:  67.92%, val_best:  71.25%, tr:  82.33%, tr_best:  83.86%, epoch time: 29.82 seconds, 0.50 minutes\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  0.470595/ 13.202357, val:  63.75%, val_best:  71.25%, tr:  82.53%, tr_best:  83.86%, epoch time: 28.97 seconds, 0.48 minutes\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  0.468747/ 18.316103, val:  62.08%, val_best:  71.25%, tr:  82.43%, tr_best:  83.86%, epoch time: 29.21 seconds, 0.49 minutes\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  0.461434/ 17.885147, val:  62.50%, val_best:  71.25%, tr:  84.78%, tr_best:  84.78%, epoch time: 29.31 seconds, 0.49 minutes\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  0.480913/ 11.953582, val:  66.25%, val_best:  71.25%, tr:  83.35%, tr_best:  84.78%, epoch time: 29.45 seconds, 0.49 minutes\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  0.502882/ 13.729205, val:  65.00%, val_best:  71.25%, tr:  82.02%, tr_best:  84.78%, epoch time: 29.46 seconds, 0.49 minutes\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  0.472636/ 22.884031, val:  58.33%, val_best:  71.25%, tr:  82.94%, tr_best:  84.78%, epoch time: 27.92 seconds, 0.47 minutes\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  0.458253/ 22.059994, val:  62.92%, val_best:  71.25%, tr:  83.76%, tr_best:  84.78%, epoch time: 29.61 seconds, 0.49 minutes\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  0.467108/ 14.846581, val:  64.17%, val_best:  71.25%, tr:  82.53%, tr_best:  84.78%, epoch time: 28.36 seconds, 0.47 minutes\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  0.458929/ 14.681391, val:  66.67%, val_best:  71.25%, tr:  83.45%, tr_best:  84.78%, epoch time: 29.75 seconds, 0.50 minutes\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  0.443567/ 21.293074, val:  61.67%, val_best:  71.25%, tr:  84.78%, tr_best:  84.78%, epoch time: 28.70 seconds, 0.48 minutes\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  0.459776/ 12.860131, val:  67.92%, val_best:  71.25%, tr:  83.55%, tr_best:  84.78%, epoch time: 29.94 seconds, 0.50 minutes\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  0.492905/ 15.536040, val:  63.33%, val_best:  71.25%, tr:  81.31%, tr_best:  84.78%, epoch time: 29.53 seconds, 0.49 minutes\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  0.441748/ 15.904539, val:  64.58%, val_best:  71.25%, tr:  83.76%, tr_best:  84.78%, epoch time: 28.95 seconds, 0.48 minutes\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  0.414200/ 21.789310, val:  58.33%, val_best:  71.25%, tr:  85.50%, tr_best:  85.50%, epoch time: 30.51 seconds, 0.51 minutes\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  0.440297/ 13.756534, val:  67.08%, val_best:  71.25%, tr:  84.88%, tr_best:  85.50%, epoch time: 28.71 seconds, 0.48 minutes\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  0.443114/ 15.498131, val:  67.08%, val_best:  71.25%, tr:  85.70%, tr_best:  85.70%, epoch time: 28.97 seconds, 0.48 minutes\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  0.451353/ 14.177991, val:  67.08%, val_best:  71.25%, tr:  83.66%, tr_best:  85.70%, epoch time: 29.29 seconds, 0.49 minutes\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  0.433814/ 13.687883, val:  68.75%, val_best:  71.25%, tr:  85.29%, tr_best:  85.70%, epoch time: 29.17 seconds, 0.49 minutes\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  0.448964/ 11.954502, val:  71.67%, val_best:  71.67%, tr:  83.45%, tr_best:  85.70%, epoch time: 28.72 seconds, 0.48 minutes\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  0.408622/ 12.067911, val:  73.33%, val_best:  73.33%, tr:  85.70%, tr_best:  85.70%, epoch time: 29.44 seconds, 0.49 minutes\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  0.438902/ 15.698264, val:  67.50%, val_best:  73.33%, tr:  84.27%, tr_best:  85.70%, epoch time: 29.25 seconds, 0.49 minutes\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  0.420798/ 12.019551, val:  71.67%, val_best:  73.33%, tr:  84.58%, tr_best:  85.70%, epoch time: 29.37 seconds, 0.49 minutes\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  0.417130/ 13.816421, val:  68.75%, val_best:  73.33%, tr:  86.11%, tr_best:  86.11%, epoch time: 29.06 seconds, 0.48 minutes\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  0.448891/ 17.640343, val:  65.83%, val_best:  73.33%, tr:  83.76%, tr_best:  86.11%, epoch time: 28.89 seconds, 0.48 minutes\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  0.435671/ 17.494400, val:  61.25%, val_best:  73.33%, tr:  84.17%, tr_best:  86.11%, epoch time: 29.23 seconds, 0.49 minutes\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  0.421188/ 12.894126, val:  71.67%, val_best:  73.33%, tr:  85.39%, tr_best:  86.11%, epoch time: 27.60 seconds, 0.46 minutes\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  0.398615/ 14.087665, val:  66.67%, val_best:  73.33%, tr:  87.64%, tr_best:  87.64%, epoch time: 30.02 seconds, 0.50 minutes\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  0.396540/ 14.443043, val:  68.33%, val_best:  73.33%, tr:  86.21%, tr_best:  87.64%, epoch time: 28.62 seconds, 0.48 minutes\n",
      "epoch-100 lr=['0.0001000'], tr/val_loss:  0.424898/ 13.429433, val:  68.75%, val_best:  73.33%, tr:  85.19%, tr_best:  87.64%, epoch time: 29.15 seconds, 0.49 minutes\n",
      "epoch-101 lr=['0.0001000'], tr/val_loss:  0.426482/ 17.007925, val:  65.83%, val_best:  73.33%, tr:  84.68%, tr_best:  87.64%, epoch time: 28.02 seconds, 0.47 minutes\n",
      "epoch-102 lr=['0.0001000'], tr/val_loss:  0.425158/ 15.278411, val:  65.83%, val_best:  73.33%, tr:  85.19%, tr_best:  87.64%, epoch time: 29.82 seconds, 0.50 minutes\n",
      "epoch-103 lr=['0.0001000'], tr/val_loss:  0.408687/ 12.514533, val:  73.33%, val_best:  73.33%, tr:  85.50%, tr_best:  87.64%, epoch time: 30.26 seconds, 0.50 minutes\n",
      "epoch-104 lr=['0.0001000'], tr/val_loss:  0.399748/ 14.828951, val:  68.75%, val_best:  73.33%, tr:  86.11%, tr_best:  87.64%, epoch time: 29.12 seconds, 0.49 minutes\n",
      "epoch-105 lr=['0.0001000'], tr/val_loss:  0.456827/ 13.659265, val:  70.42%, val_best:  73.33%, tr:  82.94%, tr_best:  87.64%, epoch time: 30.59 seconds, 0.51 minutes\n",
      "epoch-106 lr=['0.0001000'], tr/val_loss:  0.408936/ 12.975040, val:  72.50%, val_best:  73.33%, tr:  87.23%, tr_best:  87.64%, epoch time: 29.62 seconds, 0.49 minutes\n",
      "epoch-107 lr=['0.0001000'], tr/val_loss:  0.396940/ 11.815569, val:  72.92%, val_best:  73.33%, tr:  86.93%, tr_best:  87.64%, epoch time: 28.93 seconds, 0.48 minutes\n",
      "epoch-108 lr=['0.0001000'], tr/val_loss:  0.401723/ 18.093740, val:  69.17%, val_best:  73.33%, tr:  86.62%, tr_best:  87.64%, epoch time: 29.09 seconds, 0.48 minutes\n",
      "epoch-109 lr=['0.0001000'], tr/val_loss:  0.427577/ 11.767715, val:  74.58%, val_best:  74.58%, tr:  86.11%, tr_best:  87.64%, epoch time: 29.12 seconds, 0.49 minutes\n",
      "epoch-110 lr=['0.0001000'], tr/val_loss:  0.400789/ 12.436965, val:  69.58%, val_best:  74.58%, tr:  86.21%, tr_best:  87.64%, epoch time: 30.01 seconds, 0.50 minutes\n",
      "epoch-111 lr=['0.0001000'], tr/val_loss:  0.391251/ 16.883543, val:  69.17%, val_best:  74.58%, tr:  86.93%, tr_best:  87.64%, epoch time: 30.17 seconds, 0.50 minutes\n",
      "epoch-112 lr=['0.0001000'], tr/val_loss:  0.414799/ 13.997206, val:  68.75%, val_best:  74.58%, tr:  86.11%, tr_best:  87.64%, epoch time: 28.63 seconds, 0.48 minutes\n",
      "epoch-113 lr=['0.0001000'], tr/val_loss:  0.379692/ 13.124746, val:  70.83%, val_best:  74.58%, tr:  87.74%, tr_best:  87.74%, epoch time: 29.27 seconds, 0.49 minutes\n",
      "epoch-114 lr=['0.0001000'], tr/val_loss:  0.370920/ 12.834879, val:  69.17%, val_best:  74.58%, tr:  86.82%, tr_best:  87.74%, epoch time: 29.66 seconds, 0.49 minutes\n",
      "epoch-115 lr=['0.0001000'], tr/val_loss:  0.386004/ 14.639620, val:  67.92%, val_best:  74.58%, tr:  85.90%, tr_best:  87.74%, epoch time: 29.92 seconds, 0.50 minutes\n",
      "epoch-116 lr=['0.0001000'], tr/val_loss:  0.411962/ 19.478020, val:  66.25%, val_best:  74.58%, tr:  86.52%, tr_best:  87.74%, epoch time: 28.28 seconds, 0.47 minutes\n",
      "epoch-117 lr=['0.0001000'], tr/val_loss:  0.379303/ 16.294447, val:  67.92%, val_best:  74.58%, tr:  87.33%, tr_best:  87.74%, epoch time: 28.79 seconds, 0.48 minutes\n",
      "epoch-118 lr=['0.0001000'], tr/val_loss:  0.400351/ 17.985601, val:  66.67%, val_best:  74.58%, tr:  86.72%, tr_best:  87.74%, epoch time: 29.31 seconds, 0.49 minutes\n",
      "epoch-119 lr=['0.0001000'], tr/val_loss:  0.415726/ 12.508623, val:  69.58%, val_best:  74.58%, tr:  84.78%, tr_best:  87.74%, epoch time: 29.17 seconds, 0.49 minutes\n",
      "epoch-120 lr=['0.0001000'], tr/val_loss:  0.382556/ 13.708929, val:  64.58%, val_best:  74.58%, tr:  87.33%, tr_best:  87.74%, epoch time: 30.14 seconds, 0.50 minutes\n",
      "epoch-121 lr=['0.0001000'], tr/val_loss:  0.398406/ 17.545826, val:  64.17%, val_best:  74.58%, tr:  86.11%, tr_best:  87.74%, epoch time: 28.13 seconds, 0.47 minutes\n",
      "epoch-122 lr=['0.0001000'], tr/val_loss:  0.401607/ 14.059322, val:  67.08%, val_best:  74.58%, tr:  86.11%, tr_best:  87.74%, epoch time: 30.71 seconds, 0.51 minutes\n",
      "epoch-123 lr=['0.0001000'], tr/val_loss:  0.385941/ 14.704418, val:  69.58%, val_best:  74.58%, tr:  85.50%, tr_best:  87.74%, epoch time: 29.03 seconds, 0.48 minutes\n",
      "epoch-124 lr=['0.0001000'], tr/val_loss:  0.377667/ 14.606135, val:  67.50%, val_best:  74.58%, tr:  85.80%, tr_best:  87.74%, epoch time: 29.56 seconds, 0.49 minutes\n",
      "epoch-125 lr=['0.0001000'], tr/val_loss:  0.408069/ 15.719936, val:  71.25%, val_best:  74.58%, tr:  85.29%, tr_best:  87.74%, epoch time: 30.03 seconds, 0.50 minutes\n",
      "epoch-126 lr=['0.0001000'], tr/val_loss:  0.382690/ 18.570932, val:  66.25%, val_best:  74.58%, tr:  86.41%, tr_best:  87.74%, epoch time: 28.94 seconds, 0.48 minutes\n",
      "epoch-127 lr=['0.0001000'], tr/val_loss:  0.381586/ 13.774233, val:  67.92%, val_best:  74.58%, tr:  86.52%, tr_best:  87.74%, epoch time: 29.27 seconds, 0.49 minutes\n",
      "epoch-128 lr=['0.0001000'], tr/val_loss:  0.396868/ 13.385360, val:  69.58%, val_best:  74.58%, tr:  85.80%, tr_best:  87.74%, epoch time: 28.94 seconds, 0.48 minutes\n",
      "epoch-129 lr=['0.0001000'], tr/val_loss:  0.346763/ 15.038569, val:  69.58%, val_best:  74.58%, tr:  89.89%, tr_best:  89.89%, epoch time: 29.74 seconds, 0.50 minutes\n",
      "epoch-130 lr=['0.0001000'], tr/val_loss:  0.348672/ 19.727020, val:  64.58%, val_best:  74.58%, tr:  88.46%, tr_best:  89.89%, epoch time: 28.91 seconds, 0.48 minutes\n",
      "epoch-131 lr=['0.0001000'], tr/val_loss:  0.397717/ 21.098873, val:  65.42%, val_best:  74.58%, tr:  85.60%, tr_best:  89.89%, epoch time: 29.80 seconds, 0.50 minutes\n",
      "epoch-132 lr=['0.0001000'], tr/val_loss:  0.388285/ 13.574729, val:  71.67%, val_best:  74.58%, tr:  86.72%, tr_best:  89.89%, epoch time: 27.85 seconds, 0.46 minutes\n",
      "epoch-133 lr=['0.0001000'], tr/val_loss:  0.384460/ 16.069174, val:  68.75%, val_best:  74.58%, tr:  86.62%, tr_best:  89.89%, epoch time: 29.55 seconds, 0.49 minutes\n",
      "epoch-134 lr=['0.0001000'], tr/val_loss:  0.371665/ 10.889800, val:  75.83%, val_best:  75.83%, tr:  87.13%, tr_best:  89.89%, epoch time: 30.67 seconds, 0.51 minutes\n",
      "epoch-135 lr=['0.0001000'], tr/val_loss:  0.381441/ 17.100416, val:  66.25%, val_best:  75.83%, tr:  86.72%, tr_best:  89.89%, epoch time: 30.06 seconds, 0.50 minutes\n",
      "epoch-136 lr=['0.0001000'], tr/val_loss:  0.373609/ 19.497047, val:  62.08%, val_best:  75.83%, tr:  87.03%, tr_best:  89.89%, epoch time: 28.47 seconds, 0.47 minutes\n",
      "epoch-137 lr=['0.0001000'], tr/val_loss:  0.375835/ 12.359015, val:  71.67%, val_best:  75.83%, tr:  87.64%, tr_best:  89.89%, epoch time: 29.80 seconds, 0.50 minutes\n",
      "epoch-138 lr=['0.0001000'], tr/val_loss:  0.393595/ 14.684104, val:  68.33%, val_best:  75.83%, tr:  86.21%, tr_best:  89.89%, epoch time: 28.73 seconds, 0.48 minutes\n",
      "epoch-139 lr=['0.0001000'], tr/val_loss:  0.379713/ 12.554048, val:  70.42%, val_best:  75.83%, tr:  87.13%, tr_best:  89.89%, epoch time: 28.81 seconds, 0.48 minutes\n",
      "epoch-140 lr=['0.0001000'], tr/val_loss:  0.357404/ 14.314493, val:  63.75%, val_best:  75.83%, tr:  88.15%, tr_best:  89.89%, epoch time: 30.18 seconds, 0.50 minutes\n",
      "epoch-141 lr=['0.0001000'], tr/val_loss:  0.375536/ 18.694626, val:  65.00%, val_best:  75.83%, tr:  86.93%, tr_best:  89.89%, epoch time: 29.55 seconds, 0.49 minutes\n",
      "epoch-142 lr=['0.0001000'], tr/val_loss:  0.371417/ 12.185276, val:  73.75%, val_best:  75.83%, tr:  87.33%, tr_best:  89.89%, epoch time: 32.39 seconds, 0.54 minutes\n",
      "epoch-143 lr=['0.0001000'], tr/val_loss:  0.351593/ 12.793019, val:  73.75%, val_best:  75.83%, tr:  89.38%, tr_best:  89.89%, epoch time: 29.86 seconds, 0.50 minutes\n",
      "epoch-144 lr=['0.0001000'], tr/val_loss:  0.345476/ 14.666486, val:  67.92%, val_best:  75.83%, tr:  88.15%, tr_best:  89.89%, epoch time: 29.67 seconds, 0.49 minutes\n",
      "epoch-145 lr=['0.0001000'], tr/val_loss:  0.370200/ 23.488592, val:  60.83%, val_best:  75.83%, tr:  86.82%, tr_best:  89.89%, epoch time: 29.13 seconds, 0.49 minutes\n",
      "epoch-146 lr=['0.0001000'], tr/val_loss:  0.357747/ 14.869551, val:  66.67%, val_best:  75.83%, tr:  86.72%, tr_best:  89.89%, epoch time: 30.14 seconds, 0.50 minutes\n",
      "epoch-147 lr=['0.0001000'], tr/val_loss:  0.342558/ 16.818108, val:  68.33%, val_best:  75.83%, tr:  89.07%, tr_best:  89.89%, epoch time: 28.56 seconds, 0.48 minutes\n",
      "epoch-148 lr=['0.0001000'], tr/val_loss:  0.369283/ 18.577536, val:  65.42%, val_best:  75.83%, tr:  87.23%, tr_best:  89.89%, epoch time: 29.41 seconds, 0.49 minutes\n",
      "epoch-149 lr=['0.0001000'], tr/val_loss:  0.353212/ 16.112467, val:  66.67%, val_best:  75.83%, tr:  88.76%, tr_best:  89.89%, epoch time: 29.41 seconds, 0.49 minutes\n",
      "epoch-150 lr=['0.0001000'], tr/val_loss:  0.351754/ 17.362299, val:  69.17%, val_best:  75.83%, tr:  88.46%, tr_best:  89.89%, epoch time: 28.80 seconds, 0.48 minutes\n",
      "epoch-151 lr=['0.0001000'], tr/val_loss:  0.346238/ 14.503672, val:  66.67%, val_best:  75.83%, tr:  87.64%, tr_best:  89.89%, epoch time: 28.68 seconds, 0.48 minutes\n",
      "epoch-152 lr=['0.0001000'], tr/val_loss:  0.341852/ 16.256430, val:  65.42%, val_best:  75.83%, tr:  88.76%, tr_best:  89.89%, epoch time: 28.88 seconds, 0.48 minutes\n",
      "epoch-153 lr=['0.0001000'], tr/val_loss:  0.355909/ 15.672819, val:  66.25%, val_best:  75.83%, tr:  87.44%, tr_best:  89.89%, epoch time: 30.54 seconds, 0.51 minutes\n",
      "epoch-154 lr=['0.0001000'], tr/val_loss:  0.330883/ 20.524738, val:  64.17%, val_best:  75.83%, tr:  89.79%, tr_best:  89.89%, epoch time: 28.27 seconds, 0.47 minutes\n",
      "epoch-155 lr=['0.0001000'], tr/val_loss:  0.361551/ 15.113882, val:  71.25%, val_best:  75.83%, tr:  86.82%, tr_best:  89.89%, epoch time: 30.36 seconds, 0.51 minutes\n",
      "epoch-156 lr=['0.0001000'], tr/val_loss:  0.329271/ 17.779598, val:  65.83%, val_best:  75.83%, tr:  88.97%, tr_best:  89.89%, epoch time: 30.58 seconds, 0.51 minutes\n",
      "epoch-157 lr=['0.0001000'], tr/val_loss:  0.341686/ 15.013340, val:  71.25%, val_best:  75.83%, tr:  87.64%, tr_best:  89.89%, epoch time: 29.19 seconds, 0.49 minutes\n",
      "epoch-158 lr=['0.0001000'], tr/val_loss:  0.359925/ 15.953444, val:  68.75%, val_best:  75.83%, tr:  87.54%, tr_best:  89.89%, epoch time: 28.67 seconds, 0.48 minutes\n",
      "epoch-159 lr=['0.0001000'], tr/val_loss:  0.371769/ 12.606983, val:  72.50%, val_best:  75.83%, tr:  86.11%, tr_best:  89.89%, epoch time: 30.14 seconds, 0.50 minutes\n",
      "epoch-160 lr=['0.0001000'], tr/val_loss:  0.342949/ 14.580504, val:  69.58%, val_best:  75.83%, tr:  88.66%, tr_best:  89.89%, epoch time: 28.93 seconds, 0.48 minutes\n",
      "epoch-161 lr=['0.0001000'], tr/val_loss:  0.371713/ 15.833828, val:  68.75%, val_best:  75.83%, tr:  87.23%, tr_best:  89.89%, epoch time: 29.60 seconds, 0.49 minutes\n",
      "epoch-162 lr=['0.0001000'], tr/val_loss:  0.317543/ 17.705412, val:  65.83%, val_best:  75.83%, tr:  90.81%, tr_best:  90.81%, epoch time: 29.23 seconds, 0.49 minutes\n",
      "epoch-163 lr=['0.0001000'], tr/val_loss:  0.321204/ 14.728519, val:  71.25%, val_best:  75.83%, tr:  89.27%, tr_best:  90.81%, epoch time: 29.72 seconds, 0.50 minutes\n",
      "epoch-164 lr=['0.0001000'], tr/val_loss:  0.320496/ 13.704916, val:  72.92%, val_best:  75.83%, tr:  91.11%, tr_best:  91.11%, epoch time: 29.25 seconds, 0.49 minutes\n",
      "epoch-165 lr=['0.0001000'], tr/val_loss:  0.326898/ 15.411583, val:  67.50%, val_best:  75.83%, tr:  89.68%, tr_best:  91.11%, epoch time: 28.91 seconds, 0.48 minutes\n",
      "epoch-166 lr=['0.0001000'], tr/val_loss:  0.344225/ 19.763823, val:  65.00%, val_best:  75.83%, tr:  88.36%, tr_best:  91.11%, epoch time: 29.39 seconds, 0.49 minutes\n",
      "epoch-167 lr=['0.0001000'], tr/val_loss:  0.318050/ 18.998692, val:  70.42%, val_best:  75.83%, tr:  88.87%, tr_best:  91.11%, epoch time: 29.01 seconds, 0.48 minutes\n",
      "epoch-168 lr=['0.0001000'], tr/val_loss:  0.325489/ 14.826009, val:  70.00%, val_best:  75.83%, tr:  88.56%, tr_best:  91.11%, epoch time: 29.65 seconds, 0.49 minutes\n",
      "epoch-169 lr=['0.0001000'], tr/val_loss:  0.345408/ 19.697607, val:  66.25%, val_best:  75.83%, tr:  87.74%, tr_best:  91.11%, epoch time: 28.32 seconds, 0.47 minutes\n",
      "epoch-170 lr=['0.0001000'], tr/val_loss:  0.318890/ 16.038946, val:  66.67%, val_best:  75.83%, tr:  90.09%, tr_best:  91.11%, epoch time: 29.57 seconds, 0.49 minutes\n",
      "epoch-171 lr=['0.0001000'], tr/val_loss:  0.359803/ 21.828812, val:  65.83%, val_best:  75.83%, tr:  87.64%, tr_best:  91.11%, epoch time: 27.94 seconds, 0.47 minutes\n",
      "epoch-172 lr=['0.0001000'], tr/val_loss:  0.340743/ 13.541301, val:  72.92%, val_best:  75.83%, tr:  88.36%, tr_best:  91.11%, epoch time: 28.70 seconds, 0.48 minutes\n",
      "epoch-173 lr=['0.0001000'], tr/val_loss:  0.321635/ 14.456853, val:  73.33%, val_best:  75.83%, tr:  89.38%, tr_best:  91.11%, epoch time: 28.57 seconds, 0.48 minutes\n",
      "epoch-174 lr=['0.0001000'], tr/val_loss:  0.336102/ 17.142435, val:  67.08%, val_best:  75.83%, tr:  88.56%, tr_best:  91.11%, epoch time: 29.30 seconds, 0.49 minutes\n",
      "epoch-175 lr=['0.0001000'], tr/val_loss:  0.317878/ 13.081242, val:  73.75%, val_best:  75.83%, tr:  89.99%, tr_best:  91.11%, epoch time: 29.98 seconds, 0.50 minutes\n",
      "epoch-176 lr=['0.0001000'], tr/val_loss:  0.328513/ 12.844281, val:  72.08%, val_best:  75.83%, tr:  90.19%, tr_best:  91.11%, epoch time: 29.22 seconds, 0.49 minutes\n",
      "epoch-177 lr=['0.0001000'], tr/val_loss:  0.342087/ 14.550797, val:  72.92%, val_best:  75.83%, tr:  88.25%, tr_best:  91.11%, epoch time: 29.56 seconds, 0.49 minutes\n",
      "epoch-178 lr=['0.0001000'], tr/val_loss:  0.326887/ 13.509327, val:  72.92%, val_best:  75.83%, tr:  89.68%, tr_best:  91.11%, epoch time: 29.20 seconds, 0.49 minutes\n",
      "epoch-179 lr=['0.0001000'], tr/val_loss:  0.325639/ 16.151825, val:  69.17%, val_best:  75.83%, tr:  89.68%, tr_best:  91.11%, epoch time: 30.06 seconds, 0.50 minutes\n",
      "epoch-180 lr=['0.0001000'], tr/val_loss:  0.310158/ 14.357386, val:  71.67%, val_best:  75.83%, tr:  89.48%, tr_best:  91.11%, epoch time: 28.06 seconds, 0.47 minutes\n",
      "epoch-181 lr=['0.0001000'], tr/val_loss:  0.316943/ 12.918810, val:  72.08%, val_best:  75.83%, tr:  90.19%, tr_best:  91.11%, epoch time: 29.24 seconds, 0.49 minutes\n",
      "epoch-182 lr=['0.0001000'], tr/val_loss:  0.310759/ 13.699364, val:  68.33%, val_best:  75.83%, tr:  90.50%, tr_best:  91.11%, epoch time: 28.90 seconds, 0.48 minutes\n",
      "epoch-183 lr=['0.0001000'], tr/val_loss:  0.322042/ 14.170548, val:  67.50%, val_best:  75.83%, tr:  88.87%, tr_best:  91.11%, epoch time: 29.29 seconds, 0.49 minutes\n",
      "epoch-184 lr=['0.0001000'], tr/val_loss:  0.297331/ 11.700356, val:  73.75%, val_best:  75.83%, tr:  90.09%, tr_best:  91.11%, epoch time: 29.38 seconds, 0.49 minutes\n",
      "epoch-185 lr=['0.0001000'], tr/val_loss:  0.351643/ 13.753963, val:  72.50%, val_best:  75.83%, tr:  87.33%, tr_best:  91.11%, epoch time: 30.20 seconds, 0.50 minutes\n",
      "epoch-186 lr=['0.0001000'], tr/val_loss:  0.313352/ 12.517786, val:  71.67%, val_best:  75.83%, tr:  90.09%, tr_best:  91.11%, epoch time: 29.82 seconds, 0.50 minutes\n",
      "epoch-187 lr=['0.0001000'], tr/val_loss:  0.333740/ 16.414351, val:  67.08%, val_best:  75.83%, tr:  90.09%, tr_best:  91.11%, epoch time: 29.67 seconds, 0.49 minutes\n",
      "epoch-188 lr=['0.0001000'], tr/val_loss:  0.302040/ 13.377154, val:  72.50%, val_best:  75.83%, tr:  90.70%, tr_best:  91.11%, epoch time: 29.41 seconds, 0.49 minutes\n",
      "epoch-189 lr=['0.0001000'], tr/val_loss:  0.319564/ 14.000968, val:  68.75%, val_best:  75.83%, tr:  89.48%, tr_best:  91.11%, epoch time: 29.83 seconds, 0.50 minutes\n",
      "epoch-190 lr=['0.0001000'], tr/val_loss:  0.327798/ 12.227043, val:  71.67%, val_best:  75.83%, tr:  88.66%, tr_best:  91.11%, epoch time: 29.27 seconds, 0.49 minutes\n",
      "epoch-191 lr=['0.0001000'], tr/val_loss:  0.327937/ 16.675653, val:  65.42%, val_best:  75.83%, tr:  88.76%, tr_best:  91.11%, epoch time: 28.00 seconds, 0.47 minutes\n",
      "epoch-192 lr=['0.0001000'], tr/val_loss:  0.304920/ 13.299955, val:  72.50%, val_best:  75.83%, tr:  90.50%, tr_best:  91.11%, epoch time: 29.57 seconds, 0.49 minutes\n",
      "epoch-193 lr=['0.0001000'], tr/val_loss:  0.303395/ 12.412911, val:  73.33%, val_best:  75.83%, tr:  90.91%, tr_best:  91.11%, epoch time: 28.24 seconds, 0.47 minutes\n",
      "epoch-194 lr=['0.0001000'], tr/val_loss:  0.315561/ 19.080406, val:  70.42%, val_best:  75.83%, tr:  90.50%, tr_best:  91.11%, epoch time: 30.16 seconds, 0.50 minutes\n",
      "epoch-195 lr=['0.0001000'], tr/val_loss:  0.304460/ 12.904634, val:  75.83%, val_best:  75.83%, tr:  90.50%, tr_best:  91.11%, epoch time: 29.48 seconds, 0.49 minutes\n",
      "epoch-196 lr=['0.0001000'], tr/val_loss:  0.311957/ 17.219843, val:  72.92%, val_best:  75.83%, tr:  88.97%, tr_best:  91.11%, epoch time: 29.29 seconds, 0.49 minutes\n",
      "epoch-197 lr=['0.0001000'], tr/val_loss:  0.302241/ 16.750948, val:  65.42%, val_best:  75.83%, tr:  90.60%, tr_best:  91.11%, epoch time: 30.62 seconds, 0.51 minutes\n",
      "epoch-198 lr=['0.0001000'], tr/val_loss:  0.343340/ 18.916924, val:  68.75%, val_best:  75.83%, tr:  88.76%, tr_best:  91.11%, epoch time: 28.78 seconds, 0.48 minutes\n",
      "epoch-199 lr=['0.0001000'], tr/val_loss:  0.314412/ 15.984588, val:  67.50%, val_best:  75.83%, tr:  89.99%, tr_best:  91.11%, epoch time: 29.58 seconds, 0.49 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bfbf973ff44622a971ea15a802adb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñá‚ñÜ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñà‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñá‚ñÜ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñà‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñà‚ñá‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.8999</td></tr><tr><td>tr_epoch_loss</td><td>0.31441</td></tr><tr><td>val_acc_best</td><td>0.75833</td></tr><tr><td>val_acc_now</td><td>0.675</td></tr><tr><td>val_loss</td><td>15.98459</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">flowing-sweep-316</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hvrtehxc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hvrtehxc</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251111_151204-hvrtehxc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g32op7ip with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [512]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 5000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_threshold: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: one\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251111_165034-g32op7ip</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g32op7ip' target=\"_blank\">earnest-sweep-318</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g32op7ip' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g32op7ip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_threshold' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': '20251111_165042_349', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [512], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 5e-05, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'one', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'output_threshold': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 7a22c8a0ef5b9b252dbf98632e270efd\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=512, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=False, ANPI_MODE=True)\n",
      "      (4): SYNAPSE_FC(in_features=512, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient(single_step=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 506,880\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 5e-05\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "inFeed spike.shape torch.Size([10, 512]) self.weight_fb.shape torch.Size([10, 512])\n",
      "self.weight_fb[0] tensor([ 1.2009e-02,  1.3379e-01, -1.0650e-02,  5.2556e-02, -1.1912e-01,\n",
      "         4.0419e-02, -4.0199e-02, -5.0604e-02,  3.2680e-02, -7.8942e-02,\n",
      "        -1.0288e-01, -1.8775e-02, -5.7299e-03,  1.2332e-02, -6.9353e-02,\n",
      "         1.1499e-01, -4.4228e-02,  4.2593e-02,  4.9323e-02, -2.0675e-03,\n",
      "         9.2336e-02, -3.1971e-02, -1.5728e-02,  9.1276e-02, -2.0181e-02,\n",
      "        -7.1800e-02,  1.4578e-01, -4.2861e-02,  1.1373e-02, -7.3257e-02,\n",
      "        -1.1159e-01, -9.7846e-02,  5.1912e-02,  8.7845e-02,  4.0044e-02,\n",
      "         2.6324e-02, -9.8372e-02,  3.8522e-02,  1.0460e-01, -4.1150e-02,\n",
      "         5.8342e-02,  4.8482e-03,  5.2401e-03, -8.7172e-03,  2.0523e-02,\n",
      "        -3.6457e-02, -6.6373e-02,  5.9048e-03, -2.0717e-02, -3.2546e-02,\n",
      "        -5.4324e-02,  2.4378e-02,  1.0149e-02, -1.2236e-02,  6.2543e-02,\n",
      "        -8.3454e-02, -2.1650e-02, -3.9879e-02,  2.7655e-02, -3.3246e-02,\n",
      "         7.6898e-02, -5.0422e-02,  1.5484e-02, -2.6447e-02,  6.8359e-02,\n",
      "        -6.8262e-02,  3.4312e-02, -7.9518e-02, -2.3619e-02,  3.1812e-02,\n",
      "         6.2016e-03,  1.6009e-02,  2.2387e-02,  1.4105e-01,  1.4450e-03,\n",
      "         9.7970e-02, -7.1751e-02,  5.8704e-02, -2.8309e-02,  4.7077e-02,\n",
      "        -3.5820e-02, -4.3640e-02, -4.4777e-02, -3.1386e-02, -2.7226e-02,\n",
      "        -2.5884e-02,  1.0779e-02,  2.7401e-02,  3.1376e-02, -7.5319e-02,\n",
      "        -1.6829e-02,  1.7118e-02, -8.9122e-02, -4.0006e-02,  4.6343e-03,\n",
      "         1.2001e-02,  3.6892e-02,  1.4373e-02,  7.0655e-02, -4.2197e-02,\n",
      "        -1.0233e-01,  3.7360e-04,  8.5512e-02,  7.8637e-02,  1.4384e-03,\n",
      "        -8.0477e-02, -4.6482e-02,  2.3251e-02, -3.3886e-02, -2.4537e-03,\n",
      "        -4.8149e-02, -1.5486e-01,  4.3330e-02, -5.8045e-03, -1.3386e-02,\n",
      "         2.7755e-02, -1.9510e-02,  1.3393e-03,  3.8708e-02,  1.5263e-02,\n",
      "         4.6335e-02, -7.2374e-03, -6.3238e-03, -3.1016e-02, -3.1252e-02,\n",
      "        -7.4723e-02, -1.5088e-02, -4.1994e-02,  1.2212e-02,  6.0550e-02,\n",
      "        -1.7745e-03,  1.0415e-01,  6.7522e-02, -6.1409e-02, -4.1550e-02,\n",
      "         1.0644e-01,  1.5230e-01, -3.8367e-02,  7.8697e-02, -1.7323e-02,\n",
      "         2.6986e-02,  2.6370e-02,  6.5894e-02, -1.2553e-01, -3.9156e-02,\n",
      "         1.3065e-01, -5.8646e-03,  1.4600e-02, -4.5190e-02, -1.0434e-01,\n",
      "         5.6415e-02,  4.8810e-02, -3.8917e-02,  1.3367e-01,  7.2065e-02,\n",
      "        -2.6348e-02,  1.4814e-02, -7.9086e-02, -7.4679e-03, -3.7547e-02,\n",
      "        -4.9995e-02,  1.3292e-04, -1.2034e-02,  4.6384e-02,  5.0249e-02,\n",
      "         5.1038e-02, -3.7747e-02,  8.0393e-02, -6.6428e-02, -1.4425e-03,\n",
      "        -2.2637e-02, -3.0118e-02,  9.2677e-03, -9.3434e-02,  1.9207e-02,\n",
      "        -2.7770e-02, -6.7883e-02, -7.8605e-02, -9.7644e-02, -9.8327e-02,\n",
      "        -4.0612e-02,  4.7043e-02, -3.7591e-02,  1.8712e-02, -8.3181e-02,\n",
      "        -1.9715e-02,  3.6721e-02,  3.5419e-02, -4.6781e-02, -7.8367e-03,\n",
      "        -2.6748e-02, -8.6308e-02,  2.3989e-02, -1.2710e-02,  3.7118e-02,\n",
      "        -6.2088e-02, -2.2962e-04, -4.9640e-02,  2.4384e-02,  1.5691e-01,\n",
      "         1.5421e-02,  5.5528e-02,  4.8312e-02,  5.6640e-02, -2.2735e-02,\n",
      "         5.3113e-03, -5.2211e-02,  2.6325e-02,  6.9295e-02,  2.4738e-02,\n",
      "        -5.3518e-03,  5.2276e-02, -2.4634e-02, -5.3242e-03,  1.2084e-01,\n",
      "        -2.6133e-02,  3.3964e-02,  9.2582e-03, -1.2223e-01, -2.1360e-03,\n",
      "        -7.8244e-02, -1.5748e-02,  1.4439e-03,  1.2431e-01,  6.0634e-02,\n",
      "         8.5934e-02, -6.0989e-02, -2.9897e-02, -1.1970e-03, -1.0762e-01,\n",
      "         1.0423e-02,  1.6176e-02, -1.3812e-02, -5.2755e-02,  1.6920e-02,\n",
      "         6.1367e-02,  9.1813e-02,  2.1540e-02,  7.7856e-03, -4.0828e-02,\n",
      "        -9.7598e-02, -4.1089e-02,  9.0935e-02,  1.8519e-02, -3.4424e-02,\n",
      "         2.8530e-03, -6.6620e-02, -8.9594e-03, -6.7013e-03, -4.6130e-02,\n",
      "        -2.1535e-02,  5.8145e-03,  4.0000e-03, -5.7107e-02,  4.8855e-02,\n",
      "        -1.1148e-01, -1.1978e-01,  6.8131e-02,  1.5512e-03,  3.5912e-02,\n",
      "         3.3328e-02,  3.1726e-02, -8.8611e-02,  1.4725e-01, -9.5569e-02,\n",
      "        -1.0785e-02, -1.3891e-03,  1.3467e-02,  4.0348e-02,  9.6515e-02,\n",
      "         1.6649e-02,  3.0992e-02, -1.5092e-02, -5.3478e-02,  2.6478e-02,\n",
      "        -1.3042e-02, -9.5301e-02, -6.6575e-03, -1.5733e-03, -9.9895e-03,\n",
      "         3.4082e-02,  1.5740e-01, -9.9586e-03, -5.3744e-02,  8.7394e-02,\n",
      "         4.2685e-02,  5.2481e-02,  1.7623e-02,  1.0548e-03,  4.5100e-02,\n",
      "         7.4265e-02, -7.1658e-03, -8.7438e-02, -3.9754e-02,  5.4727e-02,\n",
      "         4.6412e-02,  4.2058e-02, -3.2855e-02, -1.1088e-01, -1.7722e-02,\n",
      "         4.9851e-03, -8.0476e-02,  8.2968e-02, -8.2024e-02,  1.6164e-02,\n",
      "         3.7377e-02, -9.2349e-02, -1.1127e-01,  6.9750e-02,  8.6820e-02,\n",
      "        -2.7057e-02, -2.3069e-02, -7.3103e-02, -1.6484e-01, -2.0014e-02,\n",
      "         6.3153e-03,  7.7782e-02, -8.4823e-02,  2.2121e-02,  1.0625e-01,\n",
      "        -1.4292e-01,  8.1527e-02, -7.1087e-02, -8.0429e-02, -4.0732e-03,\n",
      "         6.4006e-02, -1.4278e-01, -7.9276e-03,  5.2838e-02, -3.7510e-03,\n",
      "        -5.9070e-02, -1.1084e-01, -1.6297e-03,  5.6736e-03, -7.3166e-02,\n",
      "        -6.8036e-02,  1.5117e-01,  1.9150e-02, -9.3975e-02, -4.8127e-02,\n",
      "         4.4899e-02,  5.5049e-02,  6.3477e-02,  5.0466e-02,  1.4346e-01,\n",
      "        -1.4061e-02,  1.8790e-01,  3.4009e-02,  1.4160e-03, -2.5282e-02,\n",
      "        -1.6245e-02,  5.4068e-02, -7.5012e-02, -7.5148e-02, -1.8582e-02,\n",
      "        -2.3466e-02,  1.9578e-02, -6.2413e-02,  1.2314e-01,  1.3701e-02,\n",
      "        -5.7122e-03,  8.9041e-02,  3.7946e-02,  4.1243e-02,  4.7171e-02,\n",
      "         2.7039e-02, -5.9925e-03, -2.8245e-02, -7.2878e-02,  1.4521e-02,\n",
      "         9.9702e-02,  6.4296e-02,  7.4185e-02, -7.1993e-02,  1.4546e-02,\n",
      "         7.7495e-02, -9.2409e-03, -3.8808e-02,  7.1566e-02, -1.4977e-01,\n",
      "         4.2293e-02, -4.2540e-02, -5.6876e-03, -4.4148e-02, -8.0183e-02,\n",
      "         7.5278e-02, -2.9656e-03, -4.9337e-02,  2.6277e-02, -1.1994e-02,\n",
      "        -9.6900e-03, -8.8157e-03, -1.7625e-02, -8.9690e-02, -3.2884e-02,\n",
      "        -5.1021e-03, -1.0199e-01, -1.6831e-02,  1.1726e-01, -3.4447e-02,\n",
      "        -2.8511e-02, -1.9198e-02,  3.6576e-03,  3.2099e-02,  4.5579e-03,\n",
      "         8.7041e-02, -3.0138e-02,  1.8212e-02,  7.4119e-02, -1.3839e-02,\n",
      "         5.3415e-02,  2.2786e-02,  1.0557e-01, -5.6927e-02,  3.3285e-02,\n",
      "         7.3276e-02,  1.0244e-01, -1.4565e-02, -1.0259e-01,  1.2200e-01,\n",
      "         6.1812e-02,  4.8889e-02, -5.6486e-02,  5.1047e-02,  9.3909e-02,\n",
      "        -1.0201e-02,  6.4712e-02, -2.3649e-02,  3.8729e-02,  6.1245e-03,\n",
      "        -4.3430e-02,  6.4039e-03, -8.9212e-02,  1.5119e-01,  7.2071e-02,\n",
      "         1.5732e-02, -2.2774e-02,  5.2327e-02,  2.5401e-02,  2.9843e-02,\n",
      "        -1.1558e-01,  5.9937e-02, -5.8328e-02,  7.1370e-02,  4.9816e-02,\n",
      "         6.5657e-02,  3.2430e-02, -8.6861e-03,  8.5977e-02,  1.9082e-02,\n",
      "         2.7206e-02, -1.9106e-03, -6.5907e-02,  4.0442e-03,  1.7387e-02,\n",
      "         1.3066e-01, -8.5428e-02, -2.6442e-02,  5.6974e-02, -8.7909e-02,\n",
      "         3.4048e-02, -5.8666e-02,  1.8037e-02, -6.2223e-02, -1.8848e-02,\n",
      "         9.5296e-03, -5.1592e-03,  5.1242e-03,  9.5190e-02,  1.1389e-02,\n",
      "        -6.1644e-02,  2.7198e-02,  2.2262e-02, -4.7755e-02,  6.3539e-03,\n",
      "        -2.4203e-02,  1.3476e-02,  5.5816e-02,  3.3884e-02,  5.4144e-02,\n",
      "        -2.0123e-02, -2.5729e-02,  3.2092e-02, -3.4289e-02, -1.2439e-03,\n",
      "         1.8775e-01,  5.8437e-02,  1.8716e-02, -5.8857e-02, -6.8036e-02,\n",
      "        -5.9856e-04,  1.0747e-01, -7.1370e-02,  1.3296e-03, -3.0167e-02,\n",
      "        -5.6810e-02, -1.0447e-01, -8.7226e-03, -3.1270e-03,  1.2601e-02,\n",
      "         1.8155e-02, -9.4597e-02, -4.7340e-02,  2.7440e-02, -3.4883e-02,\n",
      "        -3.2968e-02, -6.2905e-02, -1.2657e-02,  3.2411e-02,  1.2026e-02,\n",
      "         2.2878e-02, -5.3231e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 6.6658e-02, -7.8302e-02, -3.9761e-02, -4.1793e-02,  4.5831e-02,\n",
      "         4.8306e-02, -6.7736e-03,  7.5574e-02, -7.4495e-02, -3.0042e-02,\n",
      "         5.2244e-03, -1.3071e-02, -5.5794e-03, -8.3971e-02, -6.9471e-03,\n",
      "        -2.4258e-02,  1.0854e-01, -6.1369e-02, -1.4674e-01,  1.1226e-01,\n",
      "        -6.0065e-02,  5.3451e-02,  1.1262e-01, -4.9005e-03,  1.5264e-01,\n",
      "         7.8240e-02,  3.1867e-02,  7.0535e-03, -8.8613e-02, -1.6180e-02,\n",
      "         7.1920e-03,  3.6067e-02, -1.8580e-02, -6.9305e-02,  5.7444e-02,\n",
      "        -9.3223e-02,  6.4325e-02, -1.2735e-01, -1.6280e-02, -5.1730e-02,\n",
      "        -1.6762e-02,  1.6986e-01,  2.8526e-02,  7.5887e-02,  4.1897e-03,\n",
      "         5.6685e-02,  4.6633e-02, -3.6862e-02, -3.9126e-02, -2.2331e-02,\n",
      "         9.3762e-02, -1.0613e-02,  1.1766e-01, -3.7826e-02,  6.4190e-02,\n",
      "         2.1247e-02, -9.1414e-03,  9.0567e-02, -1.1170e-01,  1.5015e-02,\n",
      "        -1.6912e-02,  1.8269e-02, -6.4949e-02, -5.4902e-02, -8.6944e-03,\n",
      "         1.3896e-01,  1.1010e-01,  1.0749e-02,  8.7195e-02, -6.8369e-03,\n",
      "        -3.5939e-02,  1.3870e-02,  5.9698e-02, -8.9737e-05,  8.3753e-02,\n",
      "        -4.8358e-03, -3.8847e-02, -1.0107e-01,  7.5683e-02, -1.1180e-01,\n",
      "         3.0140e-02, -4.3089e-02, -2.2418e-02, -3.6128e-02, -1.0527e-01,\n",
      "         2.2898e-02,  4.6009e-02, -7.4225e-03, -5.6874e-02,  8.5350e-02,\n",
      "         5.1923e-03,  2.5627e-02, -8.9285e-03, -5.8058e-02,  7.0525e-02,\n",
      "         3.8854e-02,  2.7697e-02,  1.4393e-01, -4.0282e-02,  2.0928e-02,\n",
      "        -2.4592e-02,  6.1504e-02,  8.4973e-02, -6.5030e-03, -1.1406e-02,\n",
      "        -1.5721e-01, -1.2213e-01, -3.2998e-02, -1.0606e-02,  1.5931e-01,\n",
      "         1.4261e-01,  2.5770e-02, -4.0473e-02, -6.6654e-02,  3.4934e-02,\n",
      "         9.9253e-02, -1.0173e-02, -1.4505e-02,  6.1864e-02,  4.7759e-02,\n",
      "        -1.6578e-02,  3.0713e-02,  1.4806e-02,  8.6155e-02, -1.2338e-02,\n",
      "         7.9021e-02, -7.8331e-02, -6.0098e-02,  7.8730e-02,  2.3303e-02,\n",
      "        -8.3858e-03,  4.4462e-02, -5.4935e-02,  4.2922e-02,  4.7366e-02,\n",
      "        -3.2290e-04,  1.8469e-02, -5.9237e-02,  6.0935e-02,  2.3421e-02,\n",
      "         7.0576e-02, -1.8194e-02,  5.7329e-03,  1.2694e-01, -1.6639e-02,\n",
      "         5.9829e-02, -7.5157e-02, -6.8489e-02, -1.1888e-01, -1.4575e-01,\n",
      "        -6.2740e-03,  8.6623e-02, -1.9370e-03, -1.2883e-01,  4.0742e-02,\n",
      "        -3.1368e-02, -6.8863e-03,  6.7565e-03, -5.5464e-02, -5.8365e-02,\n",
      "        -4.6925e-02, -1.8427e-03, -6.9821e-03, -5.4991e-02,  1.4936e-02,\n",
      "        -6.0094e-02,  2.1199e-02,  1.6101e-03, -6.6419e-02, -1.0129e-01,\n",
      "         3.2519e-04, -9.6969e-02,  2.2424e-02,  8.3956e-02, -1.0915e-01,\n",
      "        -5.2411e-02,  7.9012e-02,  7.7652e-02,  7.2692e-02,  5.3036e-02,\n",
      "         8.0605e-03,  1.2090e-01,  4.4321e-02, -1.3145e-02,  2.7608e-02,\n",
      "        -2.4626e-03, -8.6162e-02, -2.0906e-02, -8.0314e-02,  8.6478e-02,\n",
      "         3.2060e-02, -7.4949e-02, -4.5875e-02, -9.1144e-02,  8.5149e-02,\n",
      "         4.7841e-02, -5.8479e-02,  9.3823e-02, -8.9949e-02, -2.2137e-03,\n",
      "         5.3320e-02,  2.4241e-02,  7.6287e-02, -7.3501e-02,  5.9457e-02,\n",
      "         2.5991e-02, -4.9862e-02,  2.1058e-02,  3.7085e-02,  5.8227e-02,\n",
      "         1.6736e-02,  1.3518e-02, -3.6454e-02,  8.9511e-02, -6.0161e-02,\n",
      "         4.3647e-02,  2.5404e-02,  1.6810e-03, -3.8325e-02,  5.1655e-02,\n",
      "        -6.2435e-03, -7.4342e-02,  1.5280e-02, -3.8896e-02, -4.6945e-02,\n",
      "        -4.9156e-02,  5.0480e-02, -1.1144e-01,  4.6365e-02,  4.1312e-02,\n",
      "         4.3370e-02, -6.4439e-02,  1.4321e-01,  5.6491e-03,  4.6217e-02,\n",
      "        -7.8084e-02,  2.2043e-02,  2.4072e-02, -1.1090e-01, -5.7180e-02,\n",
      "         1.3553e-01,  2.0576e-03, -6.7463e-02, -3.7952e-02,  9.7044e-02,\n",
      "         3.9006e-02,  2.3112e-02,  3.6162e-02, -4.4879e-02, -5.0205e-02,\n",
      "        -6.6276e-02,  6.0393e-02, -1.6587e-02, -4.2223e-02,  4.9360e-02,\n",
      "        -5.2514e-02,  5.3070e-02,  3.0898e-02,  8.4096e-03,  4.2029e-02,\n",
      "         8.3128e-03,  7.7944e-02,  7.4944e-02,  3.7365e-02, -1.7412e-02,\n",
      "        -1.7034e-02, -5.1705e-02, -1.0178e-01,  8.1377e-03, -1.1124e-02,\n",
      "         6.0315e-02, -1.2464e-01, -8.2909e-02, -2.0721e-02,  1.5134e-01,\n",
      "        -7.6029e-03, -5.5703e-02,  1.3161e-01,  1.1009e-01,  8.7843e-02,\n",
      "        -1.1565e-02, -7.0188e-02, -1.7204e-01,  9.7961e-02,  1.4806e-01,\n",
      "        -4.5438e-02, -2.6664e-03, -4.6997e-02, -7.0638e-02, -7.9939e-02,\n",
      "        -7.0988e-02, -1.1400e-01, -7.8130e-03, -8.5862e-02, -3.9800e-02,\n",
      "         7.1482e-03, -1.3455e-01, -2.8474e-02, -8.3467e-02,  6.1789e-02,\n",
      "        -1.2440e-02, -1.4384e-01, -5.4934e-02,  1.7171e-02, -4.3710e-02,\n",
      "         5.2462e-03, -9.8457e-02,  6.4931e-02,  3.0336e-02, -8.2045e-03,\n",
      "        -2.1457e-02,  1.9863e-02, -3.9212e-02,  3.6250e-02, -2.9250e-02,\n",
      "         4.0146e-03,  9.8803e-02, -3.5044e-03, -1.3867e-01,  6.7823e-02,\n",
      "        -1.1386e-02,  4.5815e-02, -4.6995e-02, -6.0331e-02,  8.9048e-02,\n",
      "        -3.3910e-03,  5.5142e-02,  1.0962e-01,  7.8482e-02, -5.7451e-02,\n",
      "         6.7650e-02, -5.0193e-02, -1.0531e-01,  3.0873e-02,  4.0250e-02,\n",
      "         3.5226e-02,  3.5651e-02, -1.3163e-02, -1.5697e-02, -1.3301e-02,\n",
      "        -7.5622e-02,  4.6634e-02, -6.0863e-02,  1.1601e-02,  5.8555e-02,\n",
      "         1.9718e-02,  1.4490e-02,  4.6890e-02,  1.9770e-02,  1.8599e-02,\n",
      "         1.5324e-02,  9.0858e-02, -9.4841e-02,  4.4712e-02,  1.0196e-01,\n",
      "         7.1711e-02,  2.8857e-02, -7.6147e-02,  1.1056e-01,  3.8540e-02,\n",
      "        -7.5464e-02, -1.1109e-01,  1.1038e-02,  7.1191e-02,  3.8999e-02,\n",
      "         8.1577e-02,  1.4265e-01, -2.5305e-02,  7.0406e-02, -2.0950e-01,\n",
      "        -1.0905e-01, -7.9404e-02,  9.4908e-02, -6.2777e-02, -4.6448e-02,\n",
      "         6.7760e-02, -4.1111e-02, -3.0499e-02, -6.7737e-02, -1.6252e-02,\n",
      "         7.7219e-02, -9.5822e-02,  7.5935e-03, -2.3492e-02, -3.9966e-02,\n",
      "         2.2348e-02, -5.5910e-02, -2.2430e-02, -1.2789e-01,  1.1506e-02,\n",
      "        -3.6499e-02, -2.3789e-02,  8.8967e-02,  3.7748e-04,  1.4302e-01,\n",
      "        -3.3631e-02, -3.5510e-02, -1.5043e-01,  7.7718e-02,  1.4879e-01,\n",
      "         6.6394e-02, -1.8917e-02,  1.0423e-02, -4.4962e-03, -2.3098e-02,\n",
      "         8.4583e-02,  1.2187e-01,  2.5955e-02,  2.3483e-02, -1.2860e-01,\n",
      "         2.7167e-02,  3.6408e-02,  8.3306e-02,  1.1587e-01,  6.6651e-02,\n",
      "         5.9024e-02,  1.0206e-01, -6.6102e-02, -1.1416e-02,  6.7382e-02,\n",
      "        -1.8530e-01,  7.1940e-02, -3.7391e-02, -1.0281e-01,  5.0257e-02,\n",
      "         4.7398e-02,  2.7898e-02,  6.5546e-02, -3.5585e-02, -1.5329e-02,\n",
      "        -3.8707e-02, -5.4844e-02, -2.3227e-02,  3.0108e-02, -2.5781e-02,\n",
      "        -2.8408e-02,  3.9738e-03,  9.0303e-02,  8.2566e-03,  2.2979e-02,\n",
      "        -5.5796e-02, -3.8515e-02, -6.0057e-02,  7.1408e-02, -6.8506e-02,\n",
      "        -8.3587e-02, -1.1510e-01,  3.3540e-02, -1.6315e-02, -4.7617e-02,\n",
      "        -1.2741e-01, -2.6345e-02, -6.0932e-02, -2.5297e-02,  1.7280e-03,\n",
      "        -5.4365e-02, -5.7350e-02, -4.4366e-02, -1.8187e-02, -5.9762e-02,\n",
      "         1.8093e-02, -6.1407e-02,  1.3368e-01,  3.7309e-02, -2.3302e-02,\n",
      "        -3.6866e-02,  6.9024e-03,  7.7365e-03,  4.0508e-02, -2.5169e-02,\n",
      "        -8.2504e-02,  1.2014e-01, -6.4195e-02,  6.6726e-02,  1.5957e-02,\n",
      "         1.0247e-01,  9.6323e-02,  5.0310e-02, -7.1386e-02, -6.2054e-03,\n",
      "        -1.6760e-01,  3.7466e-03, -9.4249e-02,  7.7653e-02, -1.2555e-01,\n",
      "        -6.1608e-02, -2.9333e-02,  1.3478e-02, -1.4650e-02, -9.3798e-02,\n",
      "         6.4758e-02,  2.1284e-02,  1.5329e-01, -8.6474e-02, -5.4156e-03,\n",
      "        -2.4129e-02,  1.0983e-01, -2.6136e-02,  1.7877e-02,  7.2377e-02,\n",
      "         2.4865e-02,  5.1694e-02,  5.9210e-02,  1.3274e-01, -4.0805e-02,\n",
      "         2.4143e-02,  6.7355e-02,  6.0903e-02,  6.5552e-02,  1.7681e-01,\n",
      "         4.1771e-02,  1.2728e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 5.0966e-02, -1.4745e-01,  7.7494e-02,  1.4598e-02,  1.1066e-01,\n",
      "        -3.6061e-02, -3.4456e-02,  2.3449e-03,  3.6120e-02, -2.1529e-02,\n",
      "         1.0209e-01,  1.2287e-03, -5.0131e-02,  6.2569e-02, -2.0442e-02,\n",
      "         3.2035e-02,  6.1605e-02, -9.9639e-02,  1.5433e-02,  3.8132e-02,\n",
      "        -6.6866e-02, -6.3091e-02, -6.1747e-02,  6.8062e-02,  8.8035e-02,\n",
      "        -1.0674e-01,  5.1352e-02, -3.5963e-02, -4.7417e-03, -4.0600e-03,\n",
      "        -1.0709e-01, -8.8151e-02,  1.0923e-01, -5.1789e-02, -1.1943e-01,\n",
      "        -3.2427e-02,  8.7168e-02,  1.1600e-01, -3.1433e-02,  2.1007e-02,\n",
      "        -2.0211e-02,  5.1138e-02, -3.1195e-02, -1.7929e-02,  1.6682e-02,\n",
      "        -5.8549e-03, -3.0055e-02, -1.2022e-01,  4.2940e-02,  5.0219e-03,\n",
      "        -7.6352e-02,  1.2055e-02,  1.1379e-02,  7.7296e-02, -3.7195e-02,\n",
      "         6.2380e-02, -9.9886e-02,  1.3775e-02, -3.7782e-02, -8.0343e-03,\n",
      "         1.1148e-02, -1.7144e-02, -8.2952e-02,  6.2111e-02,  1.4023e-02,\n",
      "         9.3064e-02, -1.8222e-02,  8.8978e-02, -9.5613e-02,  5.1005e-02,\n",
      "         6.4407e-02, -1.5327e-02, -1.6592e-02, -4.5361e-02, -3.1602e-02,\n",
      "        -4.6708e-02, -4.0381e-02,  9.3572e-02,  1.4583e-02,  1.5900e-02,\n",
      "         5.2908e-02, -6.2023e-02,  9.5726e-02, -2.2317e-02, -1.0207e-02,\n",
      "        -8.4064e-02, -8.5376e-02,  1.4583e-02,  6.5636e-02,  8.2487e-02,\n",
      "         6.9251e-02, -3.3851e-03,  2.0579e-02, -6.4329e-03, -6.3405e-03,\n",
      "         2.8375e-02, -5.4557e-02,  4.9721e-02, -2.8327e-02,  7.1326e-02,\n",
      "        -2.7338e-02,  7.1745e-02,  2.0902e-02, -1.4693e-02, -6.4021e-03,\n",
      "        -3.6755e-02,  2.3320e-02, -1.8848e-02, -8.2152e-03, -7.3774e-02,\n",
      "        -6.4569e-02, -3.3738e-02,  2.3054e-02, -1.0855e-02,  3.3617e-02,\n",
      "         5.3611e-02, -6.7952e-02, -5.8561e-02, -4.5781e-02,  2.4040e-02,\n",
      "        -8.8937e-02,  3.5465e-02,  5.0535e-02,  2.5044e-02, -4.3513e-03,\n",
      "        -3.2971e-02, -1.3832e-01, -8.0301e-02,  1.5525e-01, -8.0106e-02,\n",
      "         2.0949e-02,  1.1226e-02,  5.7637e-02,  9.5634e-02, -4.6271e-02,\n",
      "         6.2753e-02, -4.8439e-02,  5.5866e-02, -5.6149e-02,  8.9882e-03,\n",
      "        -2.2475e-02,  2.6102e-03, -7.5365e-02, -3.5781e-02,  8.7820e-03,\n",
      "        -2.7019e-02,  5.6331e-02,  1.6614e-03, -3.3956e-02, -6.9785e-02,\n",
      "         1.1633e-01,  5.9738e-02, -8.4658e-02,  3.5563e-02,  1.0341e-01,\n",
      "         7.0607e-05, -4.0593e-02,  3.8467e-02,  1.0799e-01,  1.7658e-02,\n",
      "        -9.0117e-02, -9.2431e-02, -7.4624e-02,  3.1521e-02,  4.0765e-02,\n",
      "        -1.2515e-01,  3.0535e-02,  1.1851e-02, -4.0310e-02,  2.2916e-02,\n",
      "         1.2250e-01,  6.9152e-02, -6.2053e-03,  4.0321e-02,  1.6208e-02,\n",
      "        -6.8822e-02,  2.1849e-02, -3.6987e-02, -4.4603e-02, -1.5947e-01,\n",
      "        -1.6658e-02, -9.6214e-02, -3.7753e-02,  5.4041e-02, -1.7003e-02,\n",
      "         8.1025e-02,  2.4926e-02,  5.5767e-02, -7.9529e-02, -2.1234e-01,\n",
      "        -4.7282e-02, -5.5761e-02,  3.0091e-02,  1.4731e-01, -6.2581e-02,\n",
      "         2.2454e-02, -6.7485e-02,  1.5281e-01,  4.6557e-02,  8.2848e-02,\n",
      "        -9.2783e-03,  7.2040e-02, -9.9636e-02,  6.1564e-02, -5.9368e-02,\n",
      "        -1.9590e-02, -1.0435e-02, -4.1890e-02, -4.7181e-02, -1.2446e-02,\n",
      "        -4.0818e-02,  6.1132e-02, -8.5487e-03,  8.7448e-02,  2.1625e-02,\n",
      "        -1.7572e-02, -9.9109e-02,  3.0057e-02,  7.2901e-02, -1.2618e-02,\n",
      "         3.7349e-02, -2.1917e-02, -6.9758e-02, -1.2695e-03, -1.3122e-02,\n",
      "        -5.0221e-02,  2.3869e-02,  5.0954e-02,  7.0282e-04, -3.3970e-02,\n",
      "        -2.8963e-02, -8.4868e-02, -2.6569e-02, -6.5083e-02,  8.5820e-03,\n",
      "        -4.4336e-03,  5.8201e-03,  2.1587e-02,  7.3191e-03,  4.7043e-03,\n",
      "        -5.8309e-02,  2.1552e-02, -2.5648e-02, -2.2331e-02, -1.0112e-01,\n",
      "        -3.7041e-02, -4.1032e-02, -6.8042e-02,  1.7894e-02, -2.6997e-02,\n",
      "        -2.7584e-02,  1.7612e-02, -1.9444e-03,  5.9923e-02,  6.8182e-02,\n",
      "         2.6522e-02, -6.7600e-02,  3.6002e-02, -1.6933e-02,  9.7652e-03,\n",
      "        -1.0266e-01, -3.6495e-03,  1.1981e-01, -3.1746e-02, -2.1659e-02,\n",
      "        -4.1714e-02,  7.0952e-02, -8.4005e-02,  3.2536e-03, -2.2566e-02,\n",
      "        -3.9273e-02,  3.3117e-03, -8.4515e-02,  5.7761e-02,  9.1372e-02,\n",
      "         9.6171e-03, -1.2380e-01, -8.3872e-04, -1.1604e-02, -2.1467e-02,\n",
      "         3.9992e-02,  8.3243e-04, -5.9930e-03, -2.2868e-02,  2.3452e-02,\n",
      "         1.2934e-02,  1.4610e-01,  6.3666e-04, -4.7834e-02, -1.6290e-02,\n",
      "         6.7797e-02,  3.1905e-02, -6.1453e-02,  4.7708e-02,  4.9836e-02,\n",
      "        -3.2332e-02,  1.4693e-02, -8.0379e-02,  5.6533e-02,  6.9687e-02,\n",
      "         6.2967e-02, -3.5479e-02, -9.2222e-03, -6.3729e-03,  8.0024e-02,\n",
      "         1.0684e-02,  5.5488e-02, -5.7777e-03,  1.2793e-01,  2.4388e-02,\n",
      "         6.8428e-02, -2.1748e-03, -4.4633e-02,  1.3514e-02,  2.4887e-03,\n",
      "        -1.9060e-02, -1.2467e-01, -4.7357e-02, -4.9894e-02,  9.8269e-02,\n",
      "        -6.8453e-03,  3.6830e-02, -3.3399e-02, -4.3410e-02, -9.6036e-02,\n",
      "         8.1545e-02, -3.5613e-02,  6.0910e-02, -5.0575e-02,  6.5858e-03,\n",
      "         5.8657e-02,  2.9649e-02, -5.0301e-02, -1.8220e-02, -7.9198e-02,\n",
      "         4.7839e-02,  3.2613e-02, -9.3417e-02,  6.7337e-02, -8.7942e-03,\n",
      "        -1.6459e-02,  2.7349e-02, -4.9454e-02,  6.1516e-02,  6.7670e-02,\n",
      "         4.5408e-03,  3.2664e-02,  3.3849e-02, -8.3817e-03,  2.9799e-02,\n",
      "        -6.4481e-02,  6.9932e-02,  1.3802e-02, -7.4295e-02,  2.8266e-03,\n",
      "         1.3482e-01,  1.6569e-02, -4.2818e-02,  5.2147e-02,  4.8331e-02,\n",
      "        -2.2739e-02, -1.8746e-02,  2.8624e-02, -8.2209e-02, -4.9650e-02,\n",
      "        -2.9904e-02, -3.1530e-02, -4.7788e-02, -4.7805e-02,  4.2077e-02,\n",
      "        -5.1374e-03,  9.3389e-02,  7.7671e-02, -1.0206e-02, -5.3528e-02,\n",
      "        -6.0535e-03,  2.0553e-02,  2.7381e-02,  8.1292e-03, -6.6471e-02,\n",
      "        -1.9595e-02,  2.1768e-02,  4.5958e-02,  5.7396e-02,  1.7548e-02,\n",
      "        -6.3863e-03, -1.7971e-01,  2.8201e-02,  1.6888e-02, -6.0088e-02,\n",
      "        -4.4732e-02,  5.1204e-04,  5.4047e-02,  1.5042e-02,  8.6862e-02,\n",
      "        -5.6149e-02, -8.0252e-02, -1.7712e-02, -3.3251e-02,  6.7082e-02,\n",
      "         5.7277e-02,  7.4467e-02,  1.3210e-02,  8.0749e-02, -4.9230e-02,\n",
      "         4.0126e-02,  6.4328e-02,  3.2686e-02,  5.5669e-02, -4.5429e-02,\n",
      "        -6.0456e-02,  5.9471e-03, -7.2037e-03, -6.6578e-02,  6.4264e-02,\n",
      "        -3.4567e-02,  1.8057e-01,  9.6095e-02,  1.7282e-02, -5.5573e-03,\n",
      "        -1.5813e-02,  7.3891e-02, -9.6589e-03, -5.6928e-02,  3.5197e-02,\n",
      "        -3.6848e-02,  3.3619e-02, -7.9201e-02, -1.0853e-03, -6.1366e-02,\n",
      "        -4.6373e-02, -2.3210e-02,  2.4530e-02, -2.9117e-02, -2.6862e-02,\n",
      "         2.0443e-02, -1.0311e-02, -4.5818e-02,  3.2928e-02, -1.4177e-01,\n",
      "        -3.3394e-02, -8.0657e-02, -1.1610e-01,  2.7471e-03, -1.1582e-02,\n",
      "         1.8751e-03, -3.5150e-02,  9.0628e-02, -1.1234e-02, -6.3072e-03,\n",
      "        -2.9522e-03, -2.5991e-02,  7.4267e-02,  5.3881e-02, -4.0242e-03,\n",
      "         7.6560e-03,  8.1244e-02, -1.5535e-02, -7.0901e-02,  4.0996e-03,\n",
      "        -1.9212e-02,  1.5392e-02, -4.2169e-02,  1.7310e-02, -7.4863e-02,\n",
      "        -5.8399e-02, -4.7026e-02,  1.1410e-01, -1.0140e-01, -9.5707e-02,\n",
      "         2.0097e-02, -1.0625e-01,  6.2864e-02, -1.0046e-01,  4.0808e-02,\n",
      "        -5.9520e-02, -5.2804e-02,  1.8317e-02, -1.1327e-01, -1.7123e-02,\n",
      "        -2.9642e-03, -1.2108e-02,  4.3250e-02, -6.8001e-02,  2.8993e-02,\n",
      "         2.3379e-03,  6.4308e-03, -5.0257e-02, -2.6099e-02, -9.2139e-03,\n",
      "         1.4326e-01, -3.5042e-02, -5.5747e-03,  1.4443e-01,  6.4646e-02,\n",
      "        -3.6846e-02, -3.1642e-02,  1.8773e-04, -6.0860e-02,  7.3784e-02,\n",
      "         3.4365e-02, -5.6993e-02,  4.9817e-02, -4.8040e-02,  7.2079e-02,\n",
      "         6.0582e-02,  1.5344e-03, -6.8195e-02,  2.4479e-02, -6.7752e-02,\n",
      "        -7.2611e-02, -2.7682e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0768, -0.0110,  0.0261, -0.0717,  0.0138, -0.0634, -0.0912,  0.0113,\n",
      "        -0.0347, -0.0304, -0.0077, -0.0341, -0.0804, -0.0470, -0.0264,  0.0091,\n",
      "         0.0322,  0.0482, -0.0405, -0.0913,  0.0352, -0.0308,  0.0159,  0.0034,\n",
      "         0.0155, -0.0147,  0.0697,  0.0984,  0.0066,  0.0651, -0.1385, -0.0525,\n",
      "        -0.0866,  0.0596, -0.0648,  0.0693,  0.0717,  0.0327, -0.0749,  0.1113,\n",
      "         0.0407,  0.0465,  0.1108,  0.0816, -0.0240,  0.0117,  0.0365, -0.0328,\n",
      "         0.0209, -0.0589,  0.0395, -0.0040,  0.0484,  0.0579,  0.0430,  0.0961,\n",
      "         0.0019, -0.0478, -0.0156,  0.0328, -0.0624,  0.0715,  0.0612, -0.0883,\n",
      "         0.0393, -0.0688, -0.0231, -0.0230, -0.0219,  0.0156, -0.0243, -0.1010,\n",
      "        -0.0313,  0.0016, -0.0020, -0.0170, -0.0236, -0.0161, -0.0517, -0.0867,\n",
      "        -0.0712, -0.0125, -0.0954, -0.0109,  0.1592,  0.0375, -0.0574,  0.0412,\n",
      "        -0.0757,  0.1175,  0.0951, -0.0161, -0.0222, -0.1225,  0.0901,  0.0392,\n",
      "        -0.0461, -0.0242,  0.0155, -0.0975, -0.0425, -0.0112,  0.0040,  0.0077,\n",
      "         0.0669, -0.0678, -0.0185, -0.0830, -0.0124,  0.0362, -0.0285,  0.1085,\n",
      "        -0.0133,  0.0715, -0.0329, -0.0025,  0.0326, -0.0271,  0.0487, -0.0552,\n",
      "        -0.0141,  0.0521, -0.0023, -0.0375, -0.1438,  0.0137,  0.0634, -0.0483,\n",
      "        -0.0128,  0.0103,  0.0111,  0.0511,  0.1563,  0.0164,  0.0060, -0.1368,\n",
      "        -0.1142, -0.0285, -0.0205,  0.0208,  0.0782,  0.0446,  0.0960, -0.0340,\n",
      "        -0.0171,  0.0837,  0.1210,  0.0210, -0.0156, -0.0047,  0.0567,  0.1111,\n",
      "        -0.0234, -0.0498, -0.0705, -0.0082,  0.1107,  0.0074,  0.0705, -0.0538,\n",
      "         0.0613, -0.1379,  0.0155, -0.0276,  0.0236, -0.0070, -0.0942, -0.0741,\n",
      "         0.0344,  0.0320, -0.0537, -0.1111, -0.0324,  0.1613,  0.0198,  0.1086,\n",
      "        -0.0317,  0.0004, -0.0473,  0.0628,  0.0596, -0.0103, -0.0568,  0.0624,\n",
      "        -0.0776, -0.1148, -0.0166,  0.0027,  0.0078, -0.0937, -0.0514, -0.0138,\n",
      "        -0.1482, -0.0669, -0.0712,  0.0135,  0.1173, -0.0033, -0.0064, -0.0263,\n",
      "        -0.0567,  0.0106,  0.0777, -0.0619, -0.0526,  0.0932, -0.0841, -0.0340,\n",
      "        -0.1270,  0.0130,  0.0067, -0.0860,  0.1337, -0.0305, -0.0314, -0.0653,\n",
      "         0.1493, -0.0126, -0.0196, -0.0949, -0.0565,  0.0440, -0.0889,  0.0118,\n",
      "        -0.0558, -0.0214, -0.0157, -0.0387, -0.0158,  0.0084, -0.0396, -0.0521,\n",
      "        -0.0809,  0.0183,  0.0045,  0.0053, -0.0093, -0.0678, -0.1156,  0.0174,\n",
      "         0.1187,  0.0416,  0.0693, -0.0025,  0.0486,  0.0294, -0.0075, -0.0575,\n",
      "         0.1809,  0.0164,  0.0446, -0.0271, -0.0230,  0.0786, -0.0114, -0.0058,\n",
      "         0.0358, -0.0731, -0.0365, -0.0286,  0.1120, -0.0882,  0.0127,  0.0710,\n",
      "         0.0003,  0.0062, -0.0400,  0.0463,  0.0816,  0.0720,  0.0084,  0.0478,\n",
      "         0.0634,  0.0475,  0.0025, -0.0680, -0.0101,  0.0497,  0.0274,  0.0548,\n",
      "         0.0372, -0.0325,  0.1441,  0.0648,  0.0218,  0.0187,  0.0017,  0.0058,\n",
      "         0.0606,  0.0349, -0.0842, -0.0129,  0.1517, -0.0832, -0.0344,  0.0722,\n",
      "         0.0201, -0.0085,  0.0686, -0.0399, -0.1319,  0.0208, -0.0094, -0.0035,\n",
      "         0.0502,  0.0415,  0.0268,  0.0031, -0.0782, -0.0470,  0.0647, -0.0245,\n",
      "        -0.0220,  0.0053, -0.0115,  0.0109,  0.0431,  0.0079, -0.0562, -0.0070,\n",
      "         0.0463, -0.0588,  0.0339,  0.0052, -0.0210,  0.1090,  0.0647, -0.0540,\n",
      "         0.0085,  0.0879, -0.0313,  0.0073,  0.0437,  0.0494,  0.0060,  0.1026,\n",
      "         0.0076,  0.0393, -0.0335, -0.0069, -0.1043,  0.0803, -0.0891,  0.1589,\n",
      "        -0.0709, -0.0418, -0.0459, -0.0026,  0.1630, -0.0228,  0.0362,  0.0665,\n",
      "         0.0199,  0.0311, -0.0793,  0.0584, -0.0846, -0.0298,  0.0471,  0.1816,\n",
      "         0.1290, -0.0308, -0.0354,  0.0684,  0.0022,  0.1397,  0.1273, -0.0121,\n",
      "        -0.0255,  0.1549, -0.1043,  0.0030, -0.0070, -0.0533, -0.1327, -0.0505,\n",
      "        -0.0394, -0.0871, -0.1559, -0.1013, -0.0389,  0.0533, -0.0024,  0.0499,\n",
      "         0.0578, -0.0086, -0.0890, -0.0100,  0.0792, -0.0145, -0.0229, -0.0173,\n",
      "        -0.0718,  0.0246, -0.0108, -0.0746, -0.1079, -0.1119, -0.0225,  0.0620,\n",
      "        -0.0441,  0.0702,  0.1055, -0.0187,  0.0807,  0.0159,  0.0401,  0.0435,\n",
      "        -0.0720, -0.1575, -0.0476, -0.0490, -0.0268,  0.1036,  0.0390,  0.0015,\n",
      "        -0.1407, -0.0818, -0.0521, -0.0193,  0.0634,  0.0762, -0.0572,  0.0335,\n",
      "        -0.0147,  0.0902, -0.0812,  0.0083, -0.1243, -0.0758,  0.1391,  0.0418,\n",
      "         0.0337, -0.0012,  0.0702, -0.0611,  0.0674,  0.0109,  0.0365, -0.0833,\n",
      "        -0.0679, -0.0756,  0.0385, -0.0285,  0.0510, -0.0359,  0.0606,  0.0541,\n",
      "         0.0934, -0.0538, -0.0293,  0.0203, -0.0051,  0.1183, -0.0098,  0.0472,\n",
      "         0.0742, -0.0267, -0.0643, -0.0058,  0.0205,  0.0397, -0.0012,  0.0355,\n",
      "         0.0729,  0.0082,  0.0999,  0.0031,  0.0537,  0.0390,  0.0033,  0.0092,\n",
      "         0.0299, -0.0649,  0.0372,  0.0805,  0.0463, -0.0983, -0.0180, -0.0175,\n",
      "         0.0584, -0.0766,  0.0062, -0.0004,  0.0233, -0.0832,  0.0306,  0.0634,\n",
      "         0.0414, -0.0457,  0.0292, -0.0461,  0.0299,  0.0362,  0.0514,  0.0055,\n",
      "        -0.0551, -0.0026, -0.0381, -0.0229, -0.0396, -0.0021,  0.1161, -0.0633,\n",
      "         0.0352, -0.0886,  0.1244, -0.0195,  0.0971,  0.0900, -0.1717, -0.0553],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-2.8312e-02,  4.9911e-02,  9.7769e-03, -1.7147e-02,  4.0901e-02,\n",
      "        -1.2317e-01, -1.1881e-01,  8.5501e-02,  1.1018e-01,  6.2696e-02,\n",
      "         3.1070e-02, -1.0946e-01,  7.7663e-02,  6.7539e-02, -1.3375e-04,\n",
      "        -1.2912e-02,  5.7624e-02, -7.1261e-02,  9.6846e-04, -4.5915e-03,\n",
      "         6.0058e-02,  2.9872e-02,  4.2197e-02,  3.8850e-02,  5.4885e-02,\n",
      "         4.4528e-02, -8.8942e-02,  1.1722e-01, -4.4009e-02,  3.8589e-02,\n",
      "        -7.9293e-02, -1.1473e-02, -2.3653e-02, -4.3948e-02, -2.1827e-02,\n",
      "        -4.3308e-04,  8.2051e-02,  6.2999e-02,  3.0414e-02,  1.3454e-02,\n",
      "         5.9846e-03,  1.5785e-02, -6.2734e-02,  7.9752e-02, -1.4402e-01,\n",
      "        -5.4157e-02,  8.3404e-02, -5.4182e-02, -3.7938e-02,  1.9626e-03,\n",
      "         6.2376e-02, -9.8665e-02,  1.1238e-01,  8.4942e-02, -5.1376e-02,\n",
      "        -4.4197e-03,  1.0537e-02,  7.6728e-02,  7.0679e-02,  7.5002e-02,\n",
      "         2.3206e-02,  2.2686e-02,  3.7321e-02,  3.3898e-02, -2.2739e-02,\n",
      "        -1.1890e-01,  7.7856e-02,  1.0845e-01,  6.1648e-02, -2.4917e-02,\n",
      "        -5.6272e-02, -2.0143e-04, -6.7984e-02, -5.5723e-02,  1.5601e-03,\n",
      "         9.5723e-02, -1.2334e-01,  2.3138e-02,  1.5915e-03,  1.7391e-02,\n",
      "         1.0060e-03, -5.5752e-02, -7.3283e-03,  7.8786e-02, -8.5108e-02,\n",
      "         5.5049e-02,  1.5016e-01, -3.1859e-02,  4.4934e-03, -5.7109e-02,\n",
      "         8.0624e-03,  1.0309e-01, -3.0260e-03, -1.8075e-02,  1.0297e-01,\n",
      "         1.8190e-02,  8.1257e-02, -1.0586e-01,  4.6859e-02,  8.7545e-03,\n",
      "        -1.8347e-02,  7.8826e-04,  3.4076e-02,  3.4202e-02, -4.6036e-02,\n",
      "         7.8401e-02,  1.2534e-02, -2.9604e-02, -1.4013e-01, -1.2220e-01,\n",
      "        -3.9575e-02,  4.2375e-02,  6.8481e-02, -1.1031e-01,  1.7292e-03,\n",
      "         5.6505e-03, -1.3347e-01,  5.8967e-02,  1.0500e-01,  2.8959e-02,\n",
      "        -1.3579e-01, -3.6767e-02, -6.5603e-03,  5.9650e-02,  3.4714e-02,\n",
      "         3.4603e-02,  6.3472e-02,  8.8572e-02, -3.0379e-02,  1.2246e-02,\n",
      "         3.0892e-02, -1.9900e-02, -2.0532e-02, -9.3364e-02,  2.0879e-02,\n",
      "        -3.1082e-02,  7.4723e-02,  3.4827e-02,  9.9355e-03,  4.0432e-02,\n",
      "         9.0674e-02, -6.2378e-02, -1.7440e-02,  1.5880e-02, -1.3521e-02,\n",
      "         6.1648e-02, -2.5270e-02, -1.0506e-02,  1.8069e-02, -5.2453e-02,\n",
      "         1.3252e-02,  6.9504e-03, -5.8516e-02,  4.6623e-02,  1.4739e-02,\n",
      "         6.7765e-03,  3.7023e-03,  3.7319e-02,  1.9224e-02,  2.6738e-02,\n",
      "         8.2818e-02, -1.2007e-04,  7.7645e-02,  9.2141e-03,  4.3738e-03,\n",
      "        -1.0779e-01,  8.4956e-02,  3.7886e-02, -1.3384e-01, -1.1208e-01,\n",
      "        -5.7828e-02, -9.7238e-02,  1.0206e-02,  6.5645e-03, -2.8718e-02,\n",
      "         1.5325e-02,  6.6613e-02,  2.6445e-02, -2.4962e-02, -4.9788e-02,\n",
      "        -4.3545e-03, -4.5150e-02, -1.4951e-02,  6.1688e-02, -9.0608e-03,\n",
      "        -8.5805e-02, -1.0172e-01, -9.2241e-02, -1.5714e-03, -2.6098e-02,\n",
      "        -2.3720e-02, -4.2816e-03, -4.2465e-02,  4.0990e-03,  5.9952e-02,\n",
      "        -8.0171e-02,  3.4743e-02, -5.9418e-02, -5.0707e-04, -1.7003e-02,\n",
      "        -3.6289e-02,  9.0298e-02, -2.5486e-02,  2.2962e-02,  8.9927e-03,\n",
      "         3.8505e-02,  5.5345e-02, -2.0447e-02, -3.3111e-02,  3.7436e-02,\n",
      "         6.5773e-02, -4.5183e-02,  4.1996e-02, -8.7999e-02, -1.1769e-02,\n",
      "        -4.3234e-02, -6.6346e-02, -3.5659e-02, -5.7530e-03,  3.8261e-02,\n",
      "         6.5813e-02, -2.6030e-02, -7.3186e-03, -6.0748e-02, -5.1565e-02,\n",
      "        -2.2371e-02,  1.2256e-02,  7.5072e-02,  1.9970e-02,  2.4642e-02,\n",
      "        -7.0200e-02,  3.6686e-02,  2.4515e-02,  3.2946e-03,  6.7995e-03,\n",
      "         8.7247e-02, -6.1754e-02,  2.3224e-02,  4.8788e-02, -3.7919e-02,\n",
      "        -4.5916e-02, -6.3038e-03, -6.4867e-02,  9.7451e-03, -2.9809e-02,\n",
      "         1.9220e-02,  4.9873e-02, -8.4751e-02, -3.8756e-02,  2.4613e-03,\n",
      "         1.2979e-02, -1.9546e-02, -1.7456e-03,  6.0348e-02,  3.5478e-02,\n",
      "         8.5359e-02,  4.5793e-02, -2.9652e-02, -1.9533e-02,  2.8801e-02,\n",
      "         2.0128e-02, -1.6773e-02, -2.2567e-02,  8.6599e-02,  7.6258e-02,\n",
      "        -1.3919e-02, -5.2701e-03,  1.5254e-02, -5.6596e-03,  1.2512e-02,\n",
      "        -1.1107e-01, -3.9220e-02, -4.3274e-02, -1.4759e-02,  6.3456e-02,\n",
      "        -3.9313e-02,  6.6304e-02, -2.5031e-02, -8.0906e-02, -9.2574e-02,\n",
      "         7.7114e-03, -3.8525e-02,  2.6354e-02,  6.7656e-02, -3.6397e-02,\n",
      "        -6.6598e-02,  4.9100e-02, -4.5302e-02, -9.6687e-02,  3.2252e-03,\n",
      "        -1.6827e-02,  9.3235e-02, -2.9695e-02,  8.8593e-02,  1.0684e-01,\n",
      "         1.0159e-01,  7.8147e-02, -2.3984e-02,  7.4527e-02,  9.7435e-02,\n",
      "         9.9969e-02,  4.1802e-02,  5.5769e-02,  4.1883e-02,  3.7363e-02,\n",
      "        -1.2641e-02,  3.1162e-02, -5.7425e-04,  5.6984e-02,  2.1873e-03,\n",
      "         3.2089e-02, -7.0392e-02,  2.0635e-02,  9.4762e-03, -1.5822e-02,\n",
      "         5.4450e-02, -2.8916e-02,  1.6877e-02, -7.8206e-03, -1.1922e-01,\n",
      "         2.3058e-02,  6.5806e-02,  9.5983e-03,  4.4597e-02,  1.8453e-02,\n",
      "         4.3058e-02,  6.1493e-02, -6.8039e-02, -3.5424e-02, -3.8730e-02,\n",
      "        -4.6403e-02,  2.2619e-03,  1.3438e-02,  3.6322e-02, -9.0361e-02,\n",
      "         2.3885e-02, -6.8223e-02, -2.8933e-02,  1.0164e-01,  1.5505e-02,\n",
      "        -7.0034e-02,  7.1678e-02, -6.8170e-02,  4.8597e-02,  8.5489e-02,\n",
      "         3.4030e-02, -1.1827e-02,  4.7249e-02, -5.7491e-02,  6.4812e-02,\n",
      "        -3.8081e-02,  3.1269e-02,  4.8112e-02, -2.2889e-02, -1.2078e-01,\n",
      "         8.6875e-03,  2.7524e-03, -5.2020e-02, -1.3657e-02, -3.4252e-02,\n",
      "         1.2507e-01,  6.4650e-02, -4.3744e-02,  2.1554e-02,  7.2027e-02,\n",
      "         4.6084e-02,  1.0100e-01,  7.4042e-02, -5.4211e-02, -1.1455e-01,\n",
      "         5.7521e-02, -4.2710e-02, -7.8814e-02, -1.8124e-02,  4.4737e-02,\n",
      "        -5.1269e-02, -6.7855e-02, -8.3722e-02, -6.4286e-02,  3.4506e-02,\n",
      "         8.8117e-02,  4.1227e-02, -1.0366e-01, -5.4640e-02, -3.3339e-03,\n",
      "         1.3867e-01, -5.8631e-02,  1.0841e-02, -9.4331e-02,  1.0992e-01,\n",
      "        -1.8052e-02,  5.6607e-02, -3.0553e-03, -9.7665e-02,  3.6189e-03,\n",
      "         3.8424e-02, -2.0226e-02, -1.0399e-01,  7.1986e-02, -8.7396e-02,\n",
      "        -2.1321e-02, -3.3681e-02, -4.8806e-02, -9.9724e-03,  3.4821e-02,\n",
      "        -3.6701e-02, -1.0064e-01, -4.4952e-02, -2.9649e-02,  6.7568e-02,\n",
      "         1.0062e-01,  1.5413e-02, -5.2982e-03, -8.1491e-02,  6.9497e-02,\n",
      "         7.5970e-03,  2.6650e-02, -7.8061e-02,  8.9628e-02,  5.9069e-02,\n",
      "        -2.8076e-03,  2.2840e-02,  4.9031e-02, -3.0829e-02, -1.4460e-01,\n",
      "         2.0347e-02,  3.0446e-02,  4.5471e-02,  8.5173e-02, -1.1764e-02,\n",
      "        -1.9823e-02, -1.1526e-02, -1.4037e-02, -5.7210e-03,  3.2612e-02,\n",
      "         8.8098e-02,  2.5476e-02,  5.3235e-02,  9.3301e-02,  6.9620e-02,\n",
      "        -6.3628e-02,  6.8000e-02,  1.4908e-01, -5.6959e-02,  5.9116e-02,\n",
      "         2.2112e-02, -2.4973e-02, -2.7610e-02,  4.1903e-02, -2.0115e-02,\n",
      "         5.7806e-02,  1.3158e-03, -8.3065e-02,  4.6314e-02, -9.3857e-02,\n",
      "        -9.9200e-03,  4.4497e-02, -1.1722e-02, -6.1344e-02, -1.3309e-01,\n",
      "         4.0768e-02, -2.1628e-02, -5.0834e-02,  1.0866e-01,  1.6634e-02,\n",
      "         7.5386e-02,  1.1037e-01, -3.8678e-02,  5.1629e-02,  3.5886e-02,\n",
      "         3.2558e-02,  1.4227e-03,  5.5960e-02,  1.0197e-03, -5.6617e-02,\n",
      "         2.2816e-02, -1.3664e-01,  1.3298e-01, -3.5689e-02,  1.8169e-02,\n",
      "        -3.9363e-02, -4.9693e-02,  8.3050e-02, -1.3196e-02, -4.6567e-02,\n",
      "         3.9041e-02,  2.8396e-02, -2.6041e-02,  6.8008e-02, -1.0233e-01,\n",
      "        -1.5822e-02, -3.0579e-02, -4.8071e-02, -6.4514e-02,  1.8201e-02,\n",
      "        -4.3278e-02, -4.3680e-03, -8.4785e-02, -5.5908e-02, -6.7275e-02,\n",
      "         8.3114e-02,  1.3823e-02,  4.9019e-02,  4.0267e-02, -5.4514e-02,\n",
      "         4.9135e-02, -4.8312e-02, -2.4285e-02, -9.7027e-02,  2.4834e-02,\n",
      "         1.4886e-02,  6.9949e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 3.0289e-02,  3.1503e-02,  3.9986e-02,  1.3083e-01, -5.3132e-02,\n",
      "         2.9113e-02,  4.7187e-03,  5.0454e-02,  1.0700e-01, -2.2314e-02,\n",
      "         2.6524e-02, -1.1840e-02,  5.0855e-03,  7.3779e-04, -1.1865e-03,\n",
      "        -4.7954e-02,  1.0474e-02,  2.8582e-02, -7.9896e-02,  7.6038e-02,\n",
      "         4.5977e-02, -1.4148e-02,  3.9841e-02,  1.8766e-02,  8.0392e-02,\n",
      "         2.6746e-02,  2.9566e-02, -2.5976e-02,  1.6478e-02, -5.0035e-02,\n",
      "         2.4266e-02,  4.7684e-03, -4.6095e-02,  5.4383e-02, -5.5842e-02,\n",
      "        -6.3235e-02,  1.0002e-01, -7.9192e-03,  4.9059e-02, -2.9653e-02,\n",
      "         7.4298e-02,  3.2793e-02,  8.6242e-02,  1.3700e-03,  1.4234e-02,\n",
      "         7.6310e-02,  3.2565e-02, -5.5205e-02, -2.8722e-02, -3.9794e-02,\n",
      "         8.0323e-02, -1.0903e-01, -4.8134e-04,  4.3818e-02, -3.0959e-02,\n",
      "        -5.7084e-02,  4.3061e-02,  4.2138e-02,  7.2363e-02,  4.3792e-02,\n",
      "        -7.2850e-02,  5.2529e-03,  4.6195e-03, -6.2514e-02,  8.1972e-02,\n",
      "        -1.2628e-02,  1.1640e-01, -7.5081e-02,  2.6473e-02, -6.2586e-02,\n",
      "        -6.8327e-02,  5.4805e-03, -8.0045e-02, -1.0655e-02, -7.7074e-03,\n",
      "        -8.1215e-02, -1.6442e-02,  6.8840e-03, -6.9273e-03, -4.1731e-02,\n",
      "        -6.2782e-02,  6.2828e-02, -8.7719e-02,  1.7283e-02, -5.3315e-02,\n",
      "        -9.8364e-02, -9.7457e-02,  8.1505e-02,  2.6662e-02,  5.2712e-02,\n",
      "         5.1618e-02, -3.9540e-02, -1.0101e-01, -2.3273e-02,  1.6070e-02,\n",
      "        -3.2476e-02, -3.7883e-02, -1.9677e-02, -3.3466e-02,  1.7523e-02,\n",
      "        -9.1086e-02, -4.3556e-02,  7.8876e-02, -4.1143e-02, -3.5400e-02,\n",
      "        -1.7865e-02,  1.7630e-01,  1.3965e-01, -5.0848e-02, -3.6669e-02,\n",
      "         2.1116e-02, -1.0324e-01, -1.7145e-02,  6.3624e-02, -7.2753e-02,\n",
      "         8.1110e-04,  7.7122e-02,  6.0167e-02,  9.4302e-02,  3.3645e-02,\n",
      "         5.1997e-02,  9.3938e-03,  1.5380e-02,  3.0624e-02,  1.8364e-02,\n",
      "         9.4459e-02, -5.3204e-02,  5.3909e-02,  8.4368e-02, -2.6575e-02,\n",
      "         5.8741e-03,  1.7135e-01,  3.8734e-02,  1.1533e-01, -3.4991e-02,\n",
      "        -1.3902e-01, -5.0564e-02,  2.5342e-02,  1.9510e-03, -4.5458e-02,\n",
      "        -7.6664e-02,  1.0237e-01,  7.7267e-03,  5.8986e-02, -1.9288e-02,\n",
      "         5.3286e-02,  3.6359e-02,  8.0501e-02, -8.3045e-02,  3.3307e-02,\n",
      "         1.5659e-03,  9.6013e-03, -1.5590e-02, -5.1359e-02, -7.0246e-02,\n",
      "        -1.1975e-02,  2.6491e-02, -3.2005e-02,  6.8249e-02,  4.7669e-02,\n",
      "         4.7641e-02, -2.1512e-02, -6.3295e-02, -4.1788e-02, -1.5279e-02,\n",
      "        -9.7037e-02,  2.2685e-02,  2.0949e-02,  3.3309e-02,  9.4829e-03,\n",
      "         5.6710e-02, -7.6783e-03, -1.3969e-01, -4.1760e-02,  8.8335e-03,\n",
      "         4.3914e-02, -1.1144e-02,  2.1213e-02,  5.0143e-02, -1.7819e-02,\n",
      "        -3.6000e-02, -9.8346e-02,  1.8010e-02,  1.1031e-02, -4.7298e-02,\n",
      "        -2.5419e-02, -4.0803e-02,  3.5511e-02,  9.2070e-03,  6.9367e-03,\n",
      "        -4.2061e-02, -1.0377e-02,  8.0876e-02, -5.6107e-02,  5.7277e-02,\n",
      "         8.7439e-03,  1.8353e-02, -4.1559e-02,  3.4507e-02, -1.0548e-01,\n",
      "        -4.0571e-02, -2.1289e-02,  3.0586e-02,  5.1678e-03,  8.7577e-04,\n",
      "         1.3942e-01, -1.1645e-02,  7.2364e-02,  6.5043e-02,  2.4132e-02,\n",
      "         1.1002e-01,  6.1222e-03,  6.6061e-03, -5.2206e-02, -1.3325e-02,\n",
      "        -8.5573e-03, -2.0275e-03,  1.6365e-03,  2.6494e-02,  7.1705e-02,\n",
      "        -7.1865e-02,  8.4742e-02,  6.0429e-02, -5.9917e-04, -5.1137e-02,\n",
      "        -5.9481e-02, -7.6383e-02,  4.8239e-02, -3.4069e-02, -9.6994e-02,\n",
      "         1.8230e-02,  8.8950e-02,  8.6447e-02, -2.9383e-02, -9.0702e-02,\n",
      "        -3.7237e-02, -3.5979e-02, -4.2816e-02, -7.7253e-02,  7.3348e-03,\n",
      "         4.4436e-02, -1.5954e-01,  1.2394e-01,  1.1889e-02,  1.5041e-02,\n",
      "        -6.7389e-02, -4.5964e-02,  2.0859e-02, -3.0347e-02, -2.0750e-02,\n",
      "         3.9519e-02, -2.8886e-02, -8.1723e-02, -2.2986e-02, -2.3117e-03,\n",
      "         7.9396e-02, -4.6225e-02,  5.9592e-02, -6.6315e-02, -4.8456e-02,\n",
      "        -4.7836e-03, -6.7407e-02,  4.6288e-02,  1.5025e-01,  3.1964e-02,\n",
      "        -1.0685e-01, -3.1458e-02, -4.1457e-02,  7.1839e-02, -9.0231e-02,\n",
      "         3.3797e-02, -2.6273e-02, -6.0258e-02, -3.0063e-02, -9.9684e-02,\n",
      "         8.9154e-02,  4.6204e-02,  1.0030e-02, -2.1860e-02, -9.5296e-03,\n",
      "        -2.6632e-02, -2.0542e-02, -8.8112e-02, -3.1891e-02,  8.1285e-02,\n",
      "         3.4284e-02,  9.3343e-02, -7.2938e-02,  4.2222e-02,  8.5092e-02,\n",
      "        -6.9859e-02, -1.1665e-01, -1.7408e-02, -1.5403e-02,  5.4243e-02,\n",
      "         9.8341e-03, -2.8077e-02, -2.9991e-02,  3.4399e-02,  1.4826e-02,\n",
      "         1.0260e-02,  8.0673e-02,  5.1878e-03, -8.1736e-02,  8.6033e-02,\n",
      "         8.2636e-02,  5.0595e-02, -1.1922e-01,  9.3888e-03,  2.7255e-02,\n",
      "         2.7873e-02,  2.2796e-02,  1.8762e-02,  1.4380e-01, -1.4723e-01,\n",
      "        -1.4255e-02, -3.0604e-02, -3.7668e-03,  1.1167e-02, -8.0839e-02,\n",
      "         1.4414e-02, -2.5007e-02, -2.3666e-02, -2.7692e-02, -1.6474e-02,\n",
      "         5.1326e-02, -6.8901e-03,  2.6673e-02, -1.9049e-02, -4.9653e-02,\n",
      "         1.1313e-01,  8.5847e-02,  1.3205e-01, -4.7806e-02, -9.3220e-02,\n",
      "         4.1846e-02, -4.5715e-02,  2.4093e-02, -3.6066e-02,  5.0121e-02,\n",
      "         2.4745e-02, -9.0033e-02,  5.9747e-02, -5.9992e-02, -2.5795e-02,\n",
      "        -3.5649e-02,  2.3503e-02,  1.4340e-01, -5.7906e-02, -8.6132e-03,\n",
      "        -6.0701e-03,  3.0256e-03, -6.0207e-02,  1.3398e-02, -3.4405e-03,\n",
      "         3.6077e-02, -7.9061e-02, -4.5184e-02, -6.7206e-02,  8.3835e-02,\n",
      "        -1.4701e-02,  2.4760e-02,  1.7550e-02,  5.2360e-02, -1.1143e-01,\n",
      "        -6.0042e-02, -2.1617e-02, -2.3820e-02, -1.9716e-02, -1.1295e-01,\n",
      "        -1.7096e-02, -5.0607e-02,  9.7075e-02,  2.0780e-02, -4.8206e-02,\n",
      "         4.0675e-02, -5.4123e-02,  2.6274e-02, -1.1451e-01,  5.9652e-02,\n",
      "        -2.4965e-02, -2.3823e-02,  5.4150e-03, -2.5337e-03, -5.9982e-02,\n",
      "        -3.6474e-02, -1.8158e-02, -1.5301e-02,  1.1725e-02,  2.3499e-02,\n",
      "         7.4033e-02, -4.0130e-02, -5.1274e-02,  9.0815e-02,  5.4975e-02,\n",
      "        -3.4270e-02,  4.5382e-02, -7.2244e-02, -7.0036e-02, -9.7178e-03,\n",
      "        -3.3955e-02, -3.5253e-02,  8.1896e-02,  7.5562e-03, -7.9211e-02,\n",
      "        -1.0875e-01,  1.2409e-03,  7.7800e-02,  1.0634e-02, -8.2665e-02,\n",
      "         1.3230e-02, -3.4552e-02,  9.1453e-02, -6.4865e-02,  4.5128e-02,\n",
      "        -1.1324e-01, -5.8086e-02,  4.5286e-02, -3.5615e-02,  1.1491e-03,\n",
      "         4.5156e-02,  2.6197e-02, -9.7915e-02, -8.8574e-02,  6.3982e-02,\n",
      "        -7.3688e-02,  3.8706e-02,  8.2396e-02,  7.6938e-02, -2.0139e-02,\n",
      "        -6.2673e-02, -8.2048e-02,  5.6388e-02,  1.7644e-02,  4.3307e-02,\n",
      "         8.2072e-03, -4.8394e-02,  7.1145e-03, -1.4995e-01,  6.3767e-02,\n",
      "        -1.7300e-02, -4.0330e-04,  2.5645e-02,  6.1843e-02, -5.0088e-03,\n",
      "         3.9473e-03,  8.7710e-02,  3.0694e-02, -1.5863e-02,  1.2367e-01,\n",
      "         5.8815e-02,  6.1809e-02,  1.1823e-01,  3.4193e-02, -1.3734e-01,\n",
      "        -8.3475e-03, -1.3101e-02,  1.7372e-01,  3.1849e-02,  5.8699e-02,\n",
      "        -8.2168e-02,  2.9679e-02,  2.9754e-02, -1.9589e-02, -2.3867e-05,\n",
      "         2.9229e-03, -5.9795e-02,  1.0513e-01, -2.3250e-02,  1.5259e-02,\n",
      "        -9.9677e-04,  5.2436e-02,  4.5202e-02, -5.3536e-02, -3.1198e-02,\n",
      "         1.1600e-01,  8.2992e-02, -6.0462e-02, -6.9867e-02, -2.0561e-03,\n",
      "         6.2426e-02,  3.0686e-02,  7.3595e-03, -5.2512e-03, -8.7785e-02,\n",
      "         7.2232e-02, -5.5166e-02,  5.2830e-02, -3.4109e-02, -3.5072e-02,\n",
      "        -7.8913e-02,  3.6241e-02,  4.8680e-02, -2.4749e-02,  9.5748e-02,\n",
      "         1.1784e-01,  6.6303e-02, -3.3105e-02,  3.1397e-02,  4.8392e-02,\n",
      "        -9.6809e-02,  6.1331e-02,  3.0868e-02,  3.2937e-02,  1.4860e-02,\n",
      "        -8.8214e-02, -7.5167e-02, -2.6680e-02, -7.2619e-02, -3.8868e-02,\n",
      "         4.7005e-02, -1.5254e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-3.7034e-02, -4.5888e-02,  8.8781e-03,  2.7156e-02,  5.8858e-02,\n",
      "         1.2498e-03, -2.9473e-02, -2.4259e-02,  2.7695e-02,  4.8506e-02,\n",
      "        -1.3610e-02,  2.4264e-02, -1.0506e-02, -2.2343e-02, -1.2575e-02,\n",
      "        -2.7388e-02,  3.7047e-03, -9.8502e-02, -7.6187e-02, -1.3275e-02,\n",
      "         4.0868e-02,  3.1048e-02,  2.9744e-03, -3.4535e-02,  6.2692e-02,\n",
      "        -1.0555e-01, -1.8775e-03, -6.1323e-02,  1.1437e-02,  6.9841e-02,\n",
      "        -1.2952e-02,  7.9710e-02, -3.6756e-02,  1.2847e-02,  1.0407e-01,\n",
      "        -8.7324e-02, -1.0587e-01, -3.1902e-02, -8.2598e-03, -1.0516e-01,\n",
      "        -9.7262e-02,  1.1731e-02, -1.1542e-02, -1.0035e-01, -8.8628e-02,\n",
      "        -1.6604e-02, -6.6435e-04, -5.5660e-02, -5.8090e-03, -9.9288e-03,\n",
      "         2.7286e-02, -4.0562e-02, -1.3763e-02, -5.6210e-02, -8.2477e-03,\n",
      "         3.0968e-02, -2.2097e-02,  2.6884e-02, -4.4554e-03,  6.1624e-02,\n",
      "         5.7080e-02,  9.1388e-03, -2.2383e-02,  3.1594e-02,  9.0034e-02,\n",
      "         2.9283e-04, -2.4813e-03, -4.8279e-02,  2.9078e-02,  3.5868e-02,\n",
      "         4.1491e-02, -6.3660e-02, -8.4763e-02, -8.1597e-02, -5.1852e-02,\n",
      "         2.2601e-04,  1.0845e-01,  3.0973e-02, -1.5400e-01,  3.3164e-02,\n",
      "         7.9088e-02,  6.5250e-02,  5.1900e-02, -4.2283e-02, -1.1346e-01,\n",
      "        -9.0076e-03,  1.1980e-01, -1.1909e-02,  1.2310e-02,  1.8831e-02,\n",
      "        -4.9647e-02,  7.0969e-02, -2.3682e-02, -8.6618e-02,  5.2677e-02,\n",
      "         7.8079e-03, -1.0115e-01,  7.5915e-02, -4.8108e-02, -1.3128e-01,\n",
      "         6.4873e-02, -7.1029e-03, -1.4379e-01, -2.1432e-02, -5.3666e-02,\n",
      "         3.7874e-02, -8.1764e-02,  1.6618e-01,  7.1652e-02,  4.2189e-02,\n",
      "        -4.8112e-02,  5.0704e-02, -8.4332e-02,  2.3637e-02, -1.1713e-02,\n",
      "        -1.4738e-01, -5.6326e-02, -8.2328e-02, -6.9366e-03,  8.9393e-03,\n",
      "         9.0724e-02, -3.4346e-02, -1.7982e-02, -1.4817e-02, -9.2182e-02,\n",
      "         3.9414e-02, -1.3945e-02, -9.3391e-02,  1.0452e-01,  8.3443e-02,\n",
      "        -8.3101e-03,  5.8458e-02,  3.4724e-02, -9.1750e-02,  2.9846e-02,\n",
      "        -9.8895e-02, -2.4202e-02,  4.6580e-02,  4.4337e-02, -1.2447e-02,\n",
      "        -8.0480e-03, -5.6974e-03, -3.7265e-02,  7.7061e-02,  5.1464e-02,\n",
      "        -7.0224e-02, -4.4164e-02,  2.5564e-02,  1.2461e-02, -2.4537e-02,\n",
      "         2.2466e-02,  6.7765e-03, -2.1143e-02,  1.3173e-02, -4.8422e-02,\n",
      "        -2.4130e-02,  4.0795e-02, -6.9050e-02,  5.2960e-02,  2.9344e-02,\n",
      "         6.1323e-02,  2.6642e-02, -1.5501e-02,  1.1257e-02,  5.2199e-02,\n",
      "        -1.9131e-02, -7.1120e-02,  1.5206e-01, -5.5123e-02,  1.6600e-02,\n",
      "        -1.7471e-02,  5.4039e-02,  7.3465e-02, -1.4534e-02,  3.2988e-02,\n",
      "         1.0805e-01,  2.3235e-03,  2.6146e-02,  5.6207e-02,  2.4650e-02,\n",
      "         1.0190e-02, -4.5924e-03,  4.1432e-02, -4.8620e-02, -2.9034e-02,\n",
      "        -2.9012e-02,  1.4155e-02,  3.5942e-02, -9.4590e-03, -3.9627e-02,\n",
      "        -5.3268e-02,  1.3831e-01, -3.0257e-02, -5.7423e-03,  4.2466e-02,\n",
      "         1.2649e-01, -5.0767e-02, -1.1174e-02, -2.3112e-02,  3.8812e-02,\n",
      "        -6.3522e-02,  9.1453e-02,  2.6309e-02, -1.1686e-01, -3.9759e-02,\n",
      "         2.4578e-02, -4.7622e-03, -5.6869e-02,  9.6072e-02,  1.3556e-02,\n",
      "        -2.8459e-02, -4.5581e-02,  1.2914e-01, -1.1633e-02,  1.1193e-01,\n",
      "        -8.6753e-02, -8.5673e-03, -7.3127e-02, -3.6154e-02, -9.3040e-02,\n",
      "        -3.7462e-02,  1.2344e-01,  8.0146e-02, -1.7490e-02,  1.1924e-01,\n",
      "        -1.0738e-02,  6.7925e-02, -6.9445e-02, -2.5708e-02, -5.6665e-02,\n",
      "        -1.5419e-01,  1.2431e-01, -7.5615e-03, -1.0575e-01,  8.1955e-02,\n",
      "        -3.7937e-02,  8.6439e-02, -3.1533e-03,  1.4085e-01,  3.6980e-02,\n",
      "        -1.3440e-02, -5.1998e-02,  5.9634e-02, -4.4400e-02,  1.6468e-02,\n",
      "         3.7003e-02,  2.0843e-02,  4.8651e-02, -3.7829e-02,  1.0212e-01,\n",
      "        -1.8587e-02,  4.5990e-02, -4.5087e-03, -1.0517e-01, -7.8714e-02,\n",
      "        -2.2157e-02, -5.8386e-02,  7.0721e-02, -1.4240e-02, -1.0749e-01,\n",
      "        -6.8921e-02, -3.1443e-02, -3.2220e-02, -6.4972e-02,  1.1256e-02,\n",
      "         4.3494e-02,  1.8916e-02, -1.8547e-01, -2.1113e-02, -3.5792e-02,\n",
      "        -1.2145e-02,  4.6165e-02, -1.1010e-01,  3.3331e-04,  8.4547e-02,\n",
      "         5.4524e-02,  4.8118e-02, -9.5097e-02, -7.2445e-02, -6.6263e-05,\n",
      "         5.1787e-02,  4.9852e-02, -4.7932e-02, -1.2280e-02, -1.6250e-02,\n",
      "        -1.4342e-02, -1.1116e-01, -5.5778e-02, -7.7247e-03, -8.1662e-02,\n",
      "        -4.3206e-03,  6.6698e-02, -5.0373e-02, -1.2831e-01,  7.0735e-02,\n",
      "        -4.0484e-02, -2.6315e-02, -2.7391e-02, -8.0403e-02, -6.9732e-03,\n",
      "         5.4342e-02,  2.0656e-02,  1.5141e-01,  1.0275e-01,  1.5837e-03,\n",
      "        -1.4563e-01,  8.5911e-05,  4.7454e-03, -7.8300e-02,  4.8858e-02,\n",
      "        -2.1546e-02,  1.4427e-02,  4.6923e-02, -4.1582e-02,  3.4860e-02,\n",
      "         1.6094e-01, -2.8653e-02,  6.8671e-02,  3.9210e-02, -2.7989e-02,\n",
      "         1.2157e-01,  3.4874e-02,  1.0473e-01,  5.0698e-02, -6.6427e-02,\n",
      "        -8.5859e-02,  4.0868e-02, -8.1263e-02,  1.2227e-04, -4.1179e-02,\n",
      "         7.0834e-03,  8.5109e-02, -2.0567e-02,  6.0143e-03, -8.9583e-02,\n",
      "         6.3068e-02, -4.5089e-02,  2.6703e-02,  5.3511e-03,  9.8072e-03,\n",
      "         9.1949e-04,  4.8803e-02, -1.2944e-02, -1.6477e-02,  3.7466e-03,\n",
      "        -7.1968e-02, -6.9599e-02, -1.0072e-01, -7.0090e-02,  3.5817e-02,\n",
      "         6.2147e-02,  8.6350e-02,  8.2676e-02,  6.9734e-03, -1.6660e-01,\n",
      "         3.0636e-02, -7.5360e-02,  8.7070e-02,  4.6590e-02, -1.2240e-02,\n",
      "         4.7421e-02,  1.4499e-01, -3.2117e-02,  6.7256e-03, -9.1146e-03,\n",
      "         5.6627e-02,  3.4365e-02,  3.5674e-02,  1.1961e-03,  9.1195e-03,\n",
      "        -1.0258e-01, -2.6809e-02, -3.6439e-02, -5.3987e-02, -3.7285e-02,\n",
      "        -4.7299e-02,  2.0322e-02, -7.9408e-02, -7.7213e-02, -4.1219e-02,\n",
      "         1.1305e-01, -3.6860e-02,  3.4759e-02,  4.5197e-03, -1.8849e-02,\n",
      "        -1.1627e-02,  7.8283e-02, -5.6437e-02,  3.5024e-02,  6.2222e-02,\n",
      "        -8.2901e-02,  7.1049e-02,  9.9048e-03,  8.3881e-02,  3.7555e-03,\n",
      "         8.8532e-02,  9.2635e-02,  1.6246e-02, -3.0551e-02,  4.0173e-02,\n",
      "         3.9328e-02,  9.8969e-03,  7.2826e-04, -8.5527e-03,  1.9672e-02,\n",
      "         1.0268e-01, -4.0752e-03, -5.5843e-02,  1.5902e-02,  7.0855e-03,\n",
      "        -3.0325e-02,  2.9130e-02, -7.9757e-02,  2.0168e-02,  1.3599e-02,\n",
      "        -2.4822e-02, -8.0696e-03,  7.8805e-03,  3.1998e-04, -3.3752e-02,\n",
      "        -2.3653e-02,  7.4149e-02, -9.0394e-03, -6.5222e-03, -3.0573e-02,\n",
      "         1.1063e-01,  7.5828e-02,  4.1677e-02,  1.3911e-02, -7.0996e-03,\n",
      "         2.3597e-03,  2.6949e-03, -5.3042e-03,  7.1347e-02,  2.7978e-02,\n",
      "         9.5793e-04, -2.3873e-02, -7.2959e-02,  3.1148e-02, -6.5378e-02,\n",
      "         4.4773e-02, -4.6407e-02, -2.7808e-02,  6.0678e-02,  2.2824e-02,\n",
      "         1.2299e-02, -1.2252e-01, -9.4176e-02, -3.1335e-02,  6.1090e-02,\n",
      "        -8.9544e-02, -7.8463e-02, -1.0646e-01,  1.2856e-01,  5.3371e-02,\n",
      "        -3.5043e-02,  4.9204e-02, -2.7718e-02, -1.8169e-03, -3.2086e-02,\n",
      "         7.7823e-03,  6.8141e-03,  9.3693e-02,  1.6695e-02, -7.0995e-03,\n",
      "        -8.1406e-02, -1.0529e-02,  2.3930e-02, -2.4667e-02,  1.4599e-02,\n",
      "         2.2815e-02,  6.4431e-02, -8.6203e-02, -1.9157e-01,  3.7300e-02,\n",
      "        -2.8549e-02, -2.9900e-02,  2.0874e-02, -1.8929e-01,  6.7435e-02,\n",
      "        -4.1862e-02,  4.9628e-04,  7.5833e-03,  8.0471e-02, -1.7851e-02,\n",
      "        -4.5390e-02,  2.1833e-02, -1.6886e-02, -1.0043e-02, -7.4905e-02,\n",
      "        -9.9795e-04, -2.0626e-02,  8.3278e-02, -7.4464e-02,  3.2107e-02,\n",
      "         4.9412e-02, -5.9202e-02, -6.2015e-02,  1.0825e-02,  8.4142e-02,\n",
      "         6.0584e-02,  2.8453e-02, -6.4364e-02,  3.4312e-02, -3.1387e-02,\n",
      "        -1.0054e-02,  6.2364e-02,  9.9319e-02,  4.8268e-02,  3.6428e-02,\n",
      "         4.7602e-02, -2.9711e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 1.7651e-03,  3.1396e-03, -1.4551e-02, -3.5147e-03, -2.1400e-02,\n",
      "         1.1861e-01,  2.7863e-02, -5.9323e-02,  5.4408e-02, -3.9685e-02,\n",
      "        -2.9819e-02,  1.3290e-02, -3.3680e-02, -3.0514e-02, -9.5577e-02,\n",
      "        -2.7275e-02, -3.2411e-02,  1.0115e-01,  1.8278e-02,  5.4167e-02,\n",
      "        -6.5277e-02, -4.9623e-02,  3.8521e-02, -7.7113e-02,  2.3679e-02,\n",
      "        -2.1421e-02, -3.1328e-02, -3.8675e-02,  3.2301e-02,  8.0595e-02,\n",
      "        -1.7031e-01, -3.1086e-02,  7.4435e-02,  4.4086e-02, -5.2982e-02,\n",
      "        -3.7618e-02, -2.1757e-02,  2.6901e-02,  1.2416e-02, -6.1151e-02,\n",
      "        -5.6076e-03, -8.1322e-02,  1.2883e-01,  2.1244e-01,  6.3023e-03,\n",
      "        -6.4486e-02, -6.8903e-02, -5.2496e-02, -8.8419e-03, -4.8330e-03,\n",
      "        -9.8466e-02, -9.1724e-02, -4.6670e-03,  2.2265e-02,  7.4199e-03,\n",
      "        -6.7150e-03,  3.1992e-03, -1.9731e-02,  1.7806e-02, -7.7666e-03,\n",
      "         6.6665e-03, -4.9659e-03,  1.3266e-02, -3.1188e-02,  6.3222e-02,\n",
      "         2.4398e-02,  2.9437e-02, -7.7957e-04,  2.4054e-02,  1.6580e-01,\n",
      "        -7.9211e-02, -2.8934e-02,  3.4830e-02,  3.1386e-02,  1.2014e-03,\n",
      "         9.4098e-02, -5.5012e-03,  4.5756e-02,  3.2991e-02,  1.8693e-02,\n",
      "         4.4928e-02, -2.1605e-02,  4.0092e-02, -6.9511e-02, -8.6237e-02,\n",
      "        -1.2794e-01,  2.8559e-02, -3.4364e-02,  2.3834e-03,  9.2352e-03,\n",
      "        -2.0991e-03,  1.2794e-02,  1.0197e-02, -1.4751e-02, -5.3813e-03,\n",
      "         2.9286e-02,  8.8126e-02,  1.5448e-02, -1.4078e-02, -3.9143e-02,\n",
      "        -5.8560e-02,  5.4407e-02,  3.5490e-02, -7.9659e-02,  3.4453e-02,\n",
      "         2.5864e-02, -3.0899e-02, -1.5625e-02, -4.5447e-02,  4.7464e-02,\n",
      "        -3.1091e-02, -3.4445e-02, -5.4052e-02, -6.4918e-02,  4.4487e-02,\n",
      "         2.5045e-02, -1.1488e-02,  2.5262e-02, -1.2607e-02,  1.3235e-02,\n",
      "         2.8561e-02,  6.9778e-02,  3.5717e-02, -1.3796e-02, -1.6055e-01,\n",
      "        -6.3508e-02,  3.0388e-02,  3.6702e-02,  1.4510e-02,  8.2649e-02,\n",
      "        -9.6217e-03,  2.8959e-02,  3.8684e-02, -8.4300e-02, -1.5368e-01,\n",
      "         9.8709e-02, -7.2473e-02,  3.1997e-02,  1.1817e-01, -2.6140e-02,\n",
      "        -6.1742e-02, -1.6166e-02,  7.0216e-02, -1.2530e-01, -3.3601e-02,\n",
      "         1.8504e-02,  4.9253e-02,  1.5496e-01, -7.7431e-02, -1.4273e-02,\n",
      "        -1.3381e-02,  1.0467e-01, -7.3973e-02, -9.8395e-02, -2.9553e-02,\n",
      "         4.8231e-02,  6.4982e-02, -5.0469e-02,  3.5893e-02,  9.4489e-02,\n",
      "         6.2196e-02, -9.2381e-02, -8.7598e-02,  7.9401e-02, -6.6444e-02,\n",
      "        -1.0009e-02, -3.8275e-02, -2.5270e-02, -1.7952e-01, -9.5267e-03,\n",
      "        -1.3783e-01,  2.1312e-01, -1.1740e-02, -8.2986e-02,  3.5087e-02,\n",
      "        -1.9155e-02, -2.4328e-02, -4.0487e-02,  3.3686e-02, -1.7021e-02,\n",
      "        -5.0354e-02, -1.5596e-01, -1.7125e-03,  5.6674e-02,  6.6230e-03,\n",
      "         6.4058e-03, -3.7337e-03,  1.1259e-02, -2.4012e-02,  8.4532e-02,\n",
      "        -2.1994e-02,  3.6341e-03,  8.1102e-02, -5.8442e-02,  9.7022e-02,\n",
      "        -6.0901e-02,  5.0808e-02,  1.3352e-01,  1.6406e-02,  1.3148e-02,\n",
      "         2.8686e-02, -3.0704e-02, -4.3113e-02,  5.2098e-02, -5.5051e-02,\n",
      "        -1.1791e-01,  5.0002e-02,  2.3706e-03, -6.4074e-02,  5.0139e-02,\n",
      "        -3.7592e-02,  5.3099e-02,  3.9144e-02,  4.3691e-03,  1.4775e-02,\n",
      "        -7.3321e-02, -4.6698e-02,  1.2764e-01, -6.2895e-02, -2.6595e-02,\n",
      "         7.9530e-02,  3.6950e-02, -4.7796e-03,  3.2136e-02, -4.4875e-02,\n",
      "        -3.2131e-02,  8.3086e-02,  8.9513e-02, -6.2051e-03, -1.2118e-01,\n",
      "         2.6485e-02, -3.3139e-02,  4.4756e-02,  7.8008e-04,  7.1055e-02,\n",
      "         3.0050e-02,  8.2575e-03, -2.6538e-02, -3.9907e-02, -2.5800e-02,\n",
      "        -3.3800e-02,  1.8517e-02, -7.0688e-02, -1.3011e-01, -3.3101e-02,\n",
      "        -5.4424e-02,  3.0215e-02, -6.2839e-02,  2.4651e-02, -1.8812e-03,\n",
      "        -1.3442e-01,  1.2847e-02,  7.9453e-02,  8.0802e-02, -9.5993e-02,\n",
      "         3.4160e-02,  2.6102e-02, -8.6553e-03,  5.7268e-02,  8.5350e-02,\n",
      "         1.3918e-02,  1.1504e-02,  2.9779e-03,  1.0623e-02,  5.5536e-02,\n",
      "        -4.1146e-02, -9.3039e-02, -3.3455e-03,  1.5882e-02, -1.5050e-01,\n",
      "         7.5856e-03,  2.2823e-02, -3.8871e-02,  5.5844e-02,  5.4641e-03,\n",
      "        -2.4733e-02, -5.1179e-02, -1.8616e-02,  5.5658e-02, -6.9583e-02,\n",
      "        -6.0925e-02, -8.0161e-02, -1.0143e-01,  4.3837e-02,  1.3554e-01,\n",
      "         8.7156e-02,  2.5922e-02, -7.2726e-02, -1.8920e-02,  9.7482e-02,\n",
      "         2.0591e-02, -6.2224e-02,  5.4904e-02, -1.3960e-01, -7.6254e-02,\n",
      "         8.3799e-02, -3.9226e-02, -4.3723e-02, -3.3469e-02,  9.1810e-03,\n",
      "         4.9622e-02,  6.3080e-02, -2.8480e-02, -1.8700e-02,  6.6885e-02,\n",
      "        -6.8625e-03,  7.1043e-02,  7.1088e-02, -9.2783e-02,  9.1262e-02,\n",
      "         4.6247e-02, -2.9005e-02,  2.8690e-02,  1.9394e-02,  5.7164e-05,\n",
      "         2.2624e-02,  3.3163e-02,  1.7700e-02,  3.4232e-02, -2.9858e-02,\n",
      "        -7.4267e-02,  3.6014e-02, -4.4552e-02,  3.5258e-02, -1.0101e-01,\n",
      "        -6.7129e-03,  1.4119e-02,  2.7532e-02,  1.8333e-02,  1.0998e-01,\n",
      "        -4.3879e-04,  6.3078e-02,  1.9749e-02,  4.5188e-02,  1.7698e-02,\n",
      "        -1.6677e-02,  8.2497e-02, -7.5923e-02,  6.3407e-02,  6.3229e-02,\n",
      "         1.7209e-02,  8.9937e-02, -3.1758e-02,  2.4061e-02, -7.6937e-02,\n",
      "         2.9163e-03, -6.6448e-02, -1.3663e-02, -3.8498e-02, -6.1970e-02,\n",
      "        -5.3004e-02,  2.5560e-02,  1.7372e-01,  1.9347e-02,  7.7611e-02,\n",
      "         1.2019e-01, -1.5177e-01, -1.0369e-02, -3.0696e-02,  6.5096e-02,\n",
      "         1.3015e-02,  5.4550e-02, -5.5283e-02,  7.5891e-03, -2.0863e-02,\n",
      "        -2.2272e-02,  1.8210e-02, -6.6587e-03, -1.3865e-02,  5.7003e-02,\n",
      "        -1.9093e-02,  9.1872e-03,  9.9067e-02,  3.3590e-04,  4.0905e-02,\n",
      "         2.1044e-03, -6.7002e-03,  2.9374e-02, -1.1736e-02,  3.6019e-03,\n",
      "        -2.4367e-02, -3.7626e-02, -1.1231e-01,  1.7375e-02, -6.0035e-03,\n",
      "         5.7686e-02, -2.7193e-02,  1.9783e-02, -6.3263e-02,  2.2237e-02,\n",
      "         7.3779e-03, -2.8759e-03, -1.5603e-02,  5.7662e-02,  8.7457e-03,\n",
      "         1.0018e-02, -5.0072e-02, -3.7638e-02,  2.4585e-02, -9.2793e-02,\n",
      "        -1.1872e-01, -2.2116e-02, -1.0956e-01, -1.0836e-01,  8.6403e-02,\n",
      "         4.3467e-02,  2.0700e-02,  5.1945e-02,  3.0060e-02,  2.7744e-02,\n",
      "        -9.2273e-03,  7.5827e-02, -4.5446e-02,  8.2869e-02, -9.2931e-02,\n",
      "         1.2670e-02, -4.8179e-02, -1.5450e-01, -1.6038e-03, -2.9253e-02,\n",
      "        -2.7980e-02, -1.0475e-02,  2.7516e-02,  1.6998e-01,  2.4017e-02,\n",
      "         8.4535e-02, -2.9163e-04,  3.1187e-02,  5.4309e-02, -3.0479e-02,\n",
      "        -8.0611e-02, -6.6498e-02, -1.4551e-01,  2.1430e-03, -3.7552e-02,\n",
      "         7.6690e-02,  7.4113e-02, -1.0557e-01, -7.4909e-02,  6.7211e-02,\n",
      "        -7.8306e-02,  4.8829e-02, -4.6191e-02,  1.3408e-02,  2.7609e-02,\n",
      "        -7.1487e-03, -3.2693e-03,  6.9174e-02,  1.8630e-01,  8.2350e-02,\n",
      "        -7.1999e-02, -5.7636e-04, -1.0190e-01, -2.1849e-02, -2.4579e-02,\n",
      "         8.7751e-02, -3.5942e-02, -2.4704e-03, -1.1202e-01, -7.7516e-02,\n",
      "        -3.0877e-02,  5.2970e-02, -1.0476e-02,  9.5842e-03, -7.3720e-02,\n",
      "         4.0108e-02, -1.2442e-02,  3.8677e-02, -4.2649e-02,  3.2528e-02,\n",
      "         4.7383e-02, -1.2851e-03, -3.1630e-02,  9.8758e-02, -4.5205e-02,\n",
      "         9.8402e-02, -9.2297e-02, -1.9997e-02, -1.7744e-02, -2.2326e-02,\n",
      "         8.0307e-02, -1.7815e-02,  1.9394e-02, -5.2028e-02, -5.1993e-02,\n",
      "         5.9033e-03,  1.0825e-02, -1.7139e-02, -1.4043e-01,  3.3729e-02,\n",
      "         2.4079e-02,  3.1476e-02, -7.7750e-02,  4.4037e-03, -7.0054e-02,\n",
      "        -1.0412e-02,  8.7667e-03,  6.4475e-02, -6.7967e-02,  4.3379e-02,\n",
      "        -8.0798e-02,  1.3300e-01, -2.5715e-02,  4.1997e-02,  1.5607e-02,\n",
      "        -1.8457e-02, -1.4307e-02, -1.3592e-02, -8.4850e-04,  6.9601e-03,\n",
      "         1.7143e-02, -7.7591e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 7.0232e-02, -3.9177e-02, -5.6618e-02,  2.4083e-02, -1.1208e-01,\n",
      "        -8.1959e-02, -1.0443e-02,  3.7170e-02, -5.0199e-02,  3.4944e-02,\n",
      "        -1.1758e-01,  2.1176e-02, -1.3711e-02,  1.7198e-03,  5.5355e-03,\n",
      "        -2.0753e-02, -4.9122e-02, -1.6634e-02,  8.6761e-04,  5.3412e-02,\n",
      "        -5.2480e-02,  3.0198e-03, -2.5701e-02, -1.2355e-01,  5.1328e-02,\n",
      "        -6.1343e-02, -4.0117e-02,  6.6909e-02,  5.3694e-03,  2.3511e-02,\n",
      "        -8.8978e-03, -1.3987e-02,  5.8384e-02, -7.4027e-02,  7.0214e-03,\n",
      "        -2.3619e-02, -1.1647e-02,  5.0248e-03, -1.0472e-01,  5.0924e-02,\n",
      "         9.6729e-03,  4.4740e-02, -8.4304e-03,  3.1834e-02, -5.6028e-02,\n",
      "         1.2579e-02,  4.1046e-02, -7.6398e-02, -8.2215e-02, -7.4826e-02,\n",
      "        -5.8703e-03,  2.1899e-02, -3.4924e-02, -9.3841e-02,  9.2489e-02,\n",
      "        -3.9724e-02,  6.8778e-02,  6.2910e-02,  1.5035e-01, -8.5688e-02,\n",
      "         5.1889e-02, -9.3605e-02, -7.0402e-02,  4.7219e-02,  5.9798e-02,\n",
      "        -3.6312e-03, -1.3177e-02, -4.6579e-02,  2.6072e-02, -1.8031e-02,\n",
      "        -1.5455e-01,  1.6608e-01, -1.5167e-03, -2.2081e-02, -3.3239e-02,\n",
      "         7.1615e-03,  5.0772e-02,  6.4464e-03, -1.0717e-03,  1.1329e-01,\n",
      "        -4.0795e-03,  7.9883e-02, -4.3044e-02,  1.3580e-01, -1.0705e-02,\n",
      "         7.0666e-03, -7.1443e-03,  9.1426e-02, -1.3554e-03, -9.4658e-02,\n",
      "        -4.0040e-02,  5.9643e-02,  1.8720e-02, -6.1085e-03, -5.1143e-03,\n",
      "         5.2426e-03,  3.9795e-02,  5.7733e-02,  9.3336e-02,  4.6847e-03,\n",
      "         5.8702e-02,  2.5341e-02,  4.2893e-02,  7.5947e-02,  2.8520e-04,\n",
      "         7.1536e-03, -4.3884e-03, -4.4555e-02, -4.4503e-02,  5.6192e-02,\n",
      "        -5.1656e-02, -1.2393e-01,  4.3672e-02,  4.7996e-02, -1.0800e-02,\n",
      "         5.6755e-02, -8.3568e-02, -1.3538e-02, -5.6153e-02, -5.5316e-02,\n",
      "        -2.3975e-02, -1.1282e-01, -2.8566e-02,  7.2766e-02, -3.8624e-02,\n",
      "         7.6615e-02,  3.6164e-02,  1.0354e-01,  4.9160e-02,  1.9378e-02,\n",
      "        -2.2329e-02, -1.2350e-01,  1.2831e-01,  9.7161e-03,  8.3806e-02,\n",
      "        -5.0945e-02, -2.3909e-02, -2.4867e-02,  5.3618e-02,  4.3033e-02,\n",
      "        -8.6281e-03, -3.7764e-02, -1.2432e-01,  1.3901e-02, -8.2746e-02,\n",
      "         1.5292e-02, -1.0102e-01, -2.1163e-03,  2.4047e-02, -3.3842e-02,\n",
      "         1.7279e-01, -2.0493e-03, -1.4493e-02,  5.7667e-02, -2.8942e-02,\n",
      "        -3.2882e-03,  7.1961e-02,  1.5763e-02, -1.0857e-01,  3.1682e-02,\n",
      "        -1.5458e-02,  2.3903e-02, -7.8493e-02,  3.3385e-02, -1.1762e-02,\n",
      "         5.4726e-02, -1.0496e-01, -1.9116e-02,  4.4039e-02, -4.5159e-02,\n",
      "         1.1691e-01, -7.5459e-02, -3.4751e-02, -7.0932e-05, -5.4284e-03,\n",
      "        -3.1645e-02,  7.8052e-02, -1.3927e-02, -3.9138e-02, -6.9432e-02,\n",
      "        -5.6814e-02,  4.7092e-02, -9.7913e-02, -7.1706e-02, -7.4354e-02,\n",
      "         2.9061e-02,  1.2788e-01,  3.2878e-02,  6.8620e-02,  7.8050e-03,\n",
      "        -8.1034e-03,  1.2591e-01, -2.5306e-02,  2.3245e-02,  6.0525e-03,\n",
      "         5.1102e-02,  2.6583e-02, -2.1282e-03, -5.5411e-02,  4.6495e-02,\n",
      "        -2.4725e-02,  2.2852e-02, -1.2736e-02,  1.6637e-01, -5.4719e-02,\n",
      "         8.6107e-02, -5.4407e-02,  6.8237e-02, -6.1891e-02, -5.5849e-02,\n",
      "         7.3760e-03, -3.0345e-02, -3.1600e-02,  3.3583e-02,  2.8570e-02,\n",
      "         8.2200e-02,  2.6655e-02,  2.6249e-02, -1.2001e-02,  7.8356e-02,\n",
      "        -1.6183e-02, -1.4890e-02,  1.2511e-02,  3.7454e-02,  2.5717e-02,\n",
      "         2.4392e-03,  1.9375e-02,  6.4533e-02,  3.3817e-02, -6.6789e-02,\n",
      "        -8.1340e-02, -3.5166e-02, -2.8866e-02, -7.5490e-02,  3.9034e-02,\n",
      "        -4.9257e-02,  1.5981e-02,  1.2176e-02,  2.2973e-02,  1.0207e-02,\n",
      "        -1.0285e-03,  1.7862e-01,  6.4228e-02, -3.5339e-02,  8.1926e-02,\n",
      "         7.1711e-02, -1.0528e-02,  3.6034e-02, -2.3140e-02,  4.6343e-02,\n",
      "        -3.3368e-02, -4.6355e-02,  5.1168e-02,  1.8313e-02, -3.8195e-03,\n",
      "         1.0237e-01, -4.0303e-02,  3.3172e-02, -4.5773e-02, -1.4106e-02,\n",
      "        -3.1364e-02,  5.1665e-02, -3.1724e-02,  3.0433e-02, -3.9412e-02,\n",
      "        -3.3040e-02,  2.1146e-02, -1.1771e-01, -6.6739e-02, -3.3981e-02,\n",
      "        -4.4390e-03,  2.8506e-02,  1.9362e-02,  1.0839e-01, -1.5109e-02,\n",
      "         1.5135e-01, -2.9912e-02,  7.0132e-02, -6.2905e-02, -9.2045e-02,\n",
      "        -1.0811e-01, -6.4596e-02,  1.1569e-01,  4.2324e-02, -5.3588e-02,\n",
      "         2.2440e-02, -2.2649e-02, -7.6581e-02, -6.1811e-02,  6.7117e-02,\n",
      "        -8.8734e-02,  1.1926e-02, -1.0264e-02,  2.1893e-02,  6.1756e-02,\n",
      "         1.1959e-01, -9.6380e-02,  1.3470e-02, -7.0965e-02,  2.2478e-02,\n",
      "         5.0166e-02, -4.6788e-03,  9.3105e-02,  1.2183e-01, -1.0024e-01,\n",
      "         1.9777e-04,  8.1114e-02, -2.6921e-02,  1.0334e-01, -3.6504e-02,\n",
      "         1.0802e-02, -2.5081e-02, -6.5181e-02,  8.6339e-02,  3.7305e-02,\n",
      "        -1.2546e-01, -2.3171e-02, -5.1505e-02,  8.1840e-02,  4.9002e-02,\n",
      "         1.8363e-02,  2.9693e-02, -3.8902e-03, -4.1257e-02, -2.2935e-02,\n",
      "         8.3203e-02,  5.9329e-02,  7.7033e-03,  4.9673e-02, -3.4751e-02,\n",
      "        -3.4831e-03, -7.7208e-03,  1.0457e-01, -2.1170e-02,  6.3125e-02,\n",
      "        -1.2047e-02,  1.4499e-02, -5.0847e-02,  2.7684e-02,  8.1270e-02,\n",
      "        -2.4067e-02,  1.6061e-04,  4.5172e-02,  8.9830e-02,  5.0638e-03,\n",
      "        -2.7056e-02,  1.6215e-02, -1.2409e-01,  2.7129e-02, -3.0758e-02,\n",
      "        -4.1683e-02,  7.6068e-03,  1.4988e-02, -2.3955e-02, -8.0970e-02,\n",
      "        -1.0192e-01,  3.6965e-02, -2.6476e-02,  5.7144e-03,  7.6527e-02,\n",
      "         9.9065e-02,  4.3809e-02,  5.6087e-02, -6.8878e-02,  6.4834e-02,\n",
      "         2.2787e-02,  7.3976e-02,  9.9496e-03, -1.7695e-02,  8.8900e-02,\n",
      "        -9.6980e-02, -7.0818e-02,  4.9335e-02, -5.5873e-02, -6.7333e-03,\n",
      "        -1.0160e-02,  2.7102e-02, -2.2473e-02, -4.0724e-02, -4.3555e-02,\n",
      "        -4.3084e-02, -8.1544e-02, -2.8473e-02,  1.8932e-02,  2.8450e-02,\n",
      "        -8.1699e-02, -8.1030e-02,  4.9583e-02,  3.5871e-02, -1.5891e-02,\n",
      "        -8.8298e-03, -1.2130e-02, -8.1447e-02,  4.5123e-02,  6.6769e-02,\n",
      "         4.5007e-02, -1.0901e-02, -1.5257e-01,  1.2816e-02,  6.9188e-02,\n",
      "        -1.3537e-02,  1.0406e-01,  2.4015e-02,  4.3749e-02, -2.3074e-02,\n",
      "         6.2925e-02, -3.2508e-02, -5.4690e-02,  1.6847e-02, -5.7822e-02,\n",
      "         6.0195e-02, -1.6088e-02, -3.6611e-02,  4.3164e-03, -1.6129e-02,\n",
      "        -2.0027e-02,  2.7187e-02, -5.7546e-02,  1.6556e-02, -4.5320e-04,\n",
      "         2.4637e-02, -5.7647e-02, -4.5837e-02,  1.4810e-02,  1.4818e-02,\n",
      "        -9.2751e-03,  3.1316e-02,  4.6298e-02,  1.5679e-02,  2.5335e-02,\n",
      "         1.5162e-02, -6.5274e-02, -1.1448e-01,  3.5900e-02, -1.1034e-01,\n",
      "        -9.4011e-02,  3.3696e-02, -6.7059e-03,  1.4441e-02,  1.3973e-01,\n",
      "         7.2340e-02, -5.2067e-02, -1.5580e-02,  4.3312e-02, -6.7398e-02,\n",
      "         7.6808e-02, -4.1142e-02,  3.2319e-02,  1.2461e-01,  1.5610e-02,\n",
      "         7.3369e-02, -1.0851e-01, -4.5686e-02, -6.5544e-02,  7.0161e-02,\n",
      "        -4.9590e-03,  4.6399e-02,  4.5816e-02, -7.6833e-02,  5.7388e-02,\n",
      "         5.6216e-02,  1.7794e-02, -1.8920e-02, -4.4150e-02,  2.6347e-02,\n",
      "         8.7239e-02, -2.0536e-02, -1.2006e-02, -5.0354e-03,  3.5649e-02,\n",
      "        -8.1056e-02,  5.1311e-02,  1.9925e-02, -4.3425e-02,  2.6601e-02,\n",
      "        -7.5502e-02, -3.4638e-02, -7.5277e-02, -5.1211e-02, -4.9907e-02,\n",
      "         1.9271e-02,  2.3710e-02,  1.7192e-02, -7.7708e-02,  2.5729e-02,\n",
      "         5.5325e-02,  1.0182e-01, -9.2568e-02, -4.8824e-02,  2.3749e-02,\n",
      "         3.6623e-02, -1.6246e-02, -2.5600e-02, -8.7405e-02,  1.7550e-02,\n",
      "        -6.1699e-03, -4.0138e-02, -3.5954e-02, -6.4890e-02,  4.1684e-03,\n",
      "        -8.0014e-02, -7.6652e-02,  9.0478e-02,  2.2696e-03,  4.3178e-03,\n",
      "         1.3625e-01, -4.3848e-02,  3.4243e-02,  1.0695e-01,  2.2553e-02,\n",
      "        -1.3336e-02, -3.6943e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-6.3037e-02,  4.0266e-02, -4.1982e-02, -1.3677e-01, -2.8125e-02,\n",
      "        -3.5907e-02, -1.2441e-01,  3.9413e-02,  7.0257e-02,  2.5938e-02,\n",
      "         1.6168e-02, -4.9772e-03, -8.1783e-02,  5.8682e-02, -5.5678e-03,\n",
      "        -1.6609e-02, -7.0250e-02,  2.3007e-02, -1.1237e-01, -1.1362e-03,\n",
      "         2.6813e-02,  5.1206e-02, -1.0730e-01, -7.5571e-03,  6.9360e-02,\n",
      "        -3.0615e-02, -1.3997e-01, -1.6990e-02, -6.2863e-03,  3.9499e-02,\n",
      "         1.1053e-01,  7.3368e-02, -4.2755e-03, -6.1965e-02,  2.3791e-02,\n",
      "        -5.2535e-02, -2.8180e-02, -1.5272e-02, -2.4229e-02,  8.8722e-02,\n",
      "        -9.6024e-02,  4.9923e-02, -1.7209e-02,  1.6388e-02, -7.5840e-03,\n",
      "        -8.1901e-02,  6.1073e-02, -4.8348e-02, -1.0459e-02, -6.6470e-02,\n",
      "        -2.4781e-02,  5.3203e-02,  2.4020e-02,  4.9423e-02,  3.7947e-02,\n",
      "         2.0673e-01,  6.0895e-02, -4.0758e-02,  2.0071e-02,  1.0387e-01,\n",
      "         1.6163e-02, -1.5090e-02,  6.2854e-02,  2.6011e-02,  3.8561e-02,\n",
      "        -5.5162e-02, -7.5800e-02,  6.2836e-02,  1.3994e-02,  1.1939e-01,\n",
      "        -1.5908e-02,  4.2999e-02, -2.3383e-02,  1.5983e-02, -1.3671e-02,\n",
      "         9.9919e-02,  6.4056e-02, -7.5082e-02, -1.2190e-02, -1.4694e-02,\n",
      "         5.8069e-02,  5.5209e-02,  1.2079e-02,  5.1134e-02,  4.5578e-02,\n",
      "         5.2929e-02, -6.7412e-03,  9.7755e-02, -4.7786e-02, -1.6850e-02,\n",
      "        -5.9766e-02,  9.2122e-02, -2.9754e-02, -1.1698e-01,  2.3706e-02,\n",
      "        -2.3814e-02,  3.3031e-02, -1.1580e-01,  3.8596e-02, -3.3136e-03,\n",
      "        -1.2250e-02,  3.4611e-02, -8.4193e-02, -7.5750e-02, -3.5521e-02,\n",
      "        -5.1473e-03,  3.9007e-02,  6.4325e-03, -5.9280e-02, -1.3100e-02,\n",
      "        -4.1139e-02,  4.7848e-02,  8.4264e-03, -1.0753e-01, -4.3760e-02,\n",
      "         9.4994e-02, -1.7219e-02,  3.9596e-02, -4.1659e-02,  1.2531e-01,\n",
      "        -4.9070e-02,  1.2569e-02, -3.4510e-02,  5.6004e-02, -2.7773e-02,\n",
      "        -8.0413e-02,  7.7013e-02, -3.7365e-02, -7.8601e-02, -4.4590e-02,\n",
      "         1.6158e-02,  2.7064e-02,  1.0510e-01, -1.2408e-02,  1.6963e-02,\n",
      "        -9.1978e-03,  5.7486e-02, -4.6821e-02, -3.0573e-03, -1.0964e-02,\n",
      "        -8.9452e-02,  4.2682e-02, -1.1941e-02,  2.5132e-02, -3.7705e-02,\n",
      "         5.4186e-02, -7.1975e-02, -4.9173e-02, -6.7192e-02,  2.7494e-02,\n",
      "         2.4167e-03,  3.7371e-02,  4.2284e-02,  3.3118e-02,  5.1909e-02,\n",
      "        -6.6921e-02, -5.8869e-02, -6.1932e-02,  3.1455e-02, -2.2885e-02,\n",
      "        -9.3647e-02, -1.9637e-02,  5.1098e-02,  4.5610e-02, -4.1068e-02,\n",
      "         5.7816e-02, -8.5963e-04,  2.2186e-02, -1.8173e-02,  4.3025e-02,\n",
      "        -3.6500e-02,  4.6611e-02,  1.1417e-01, -6.0109e-02, -6.6532e-02,\n",
      "         9.2543e-02,  1.5739e-02, -7.0260e-03, -4.5298e-02, -4.6085e-02,\n",
      "        -1.7641e-02, -3.4245e-02, -2.9982e-02, -3.3564e-02, -2.3251e-02,\n",
      "        -9.0132e-02, -4.9113e-02, -1.5003e-02, -3.4544e-02, -1.2240e-02,\n",
      "        -6.6013e-02, -1.2225e-01,  2.1974e-02, -7.2869e-02,  7.3213e-02,\n",
      "         7.8171e-02, -1.1407e-02,  1.2900e-02,  1.3423e-02,  6.1885e-02,\n",
      "         8.2777e-02,  5.9639e-03, -2.9608e-02,  1.4335e-02, -3.0911e-02,\n",
      "        -2.6568e-02, -7.7970e-02,  5.7262e-02,  7.5148e-03, -8.3736e-02,\n",
      "         1.1164e-01, -3.6595e-02, -3.5647e-02, -2.2155e-02,  3.7071e-02,\n",
      "         5.2191e-03, -5.0187e-02, -1.0465e-02, -2.7389e-02,  2.4710e-02,\n",
      "         3.4442e-02, -3.3596e-02,  5.7857e-02,  4.2296e-02, -2.8121e-02,\n",
      "         3.7366e-02, -5.9914e-02,  1.6653e-02,  3.8050e-02,  5.3976e-02,\n",
      "         1.6561e-02,  5.0949e-02,  7.7352e-02,  8.3561e-02, -4.3670e-02,\n",
      "        -8.8957e-03,  2.0743e-03,  3.0768e-02, -3.4656e-02,  1.0132e-01,\n",
      "         2.0802e-02, -1.4734e-01, -1.1625e-02, -4.6762e-03,  1.0868e-01,\n",
      "         8.2071e-02, -1.4927e-02, -1.5449e-01, -7.1360e-02,  6.3504e-02,\n",
      "        -1.3678e-02, -3.2650e-02,  8.5200e-02, -4.5086e-02,  2.2611e-02,\n",
      "        -1.0392e-01, -6.0944e-02,  1.4738e-02,  3.9227e-02, -8.7592e-03,\n",
      "        -2.2234e-02, -5.5263e-03, -3.3027e-02,  3.9625e-03,  1.5417e-02,\n",
      "         1.2909e-02,  1.0592e-01, -5.5637e-02,  1.6255e-01, -8.2178e-02,\n",
      "         9.2043e-02,  1.9381e-03,  2.2714e-02,  3.5822e-02, -1.0901e-03,\n",
      "         1.2325e-02, -6.4859e-02, -2.5885e-02,  5.1314e-02, -4.6941e-04,\n",
      "        -2.8895e-03,  1.1293e-02, -1.7513e-02, -6.6949e-02,  6.9416e-02,\n",
      "         7.1142e-03, -1.4641e-03, -3.6779e-02,  1.1385e-01, -4.7641e-02,\n",
      "         1.4738e-02, -6.2718e-02,  8.7415e-02, -5.5629e-03,  2.7129e-02,\n",
      "         6.3722e-03,  3.4799e-02,  2.5760e-02, -7.6286e-02, -6.1321e-02,\n",
      "        -5.3081e-02, -1.3048e-03, -1.7442e-02, -1.6667e-01,  3.7299e-03,\n",
      "         1.3328e-02,  6.2362e-02,  1.6265e-02,  5.9280e-02, -9.6899e-02,\n",
      "        -9.9530e-03,  3.4732e-02,  5.4185e-03, -4.3835e-03,  3.4801e-02,\n",
      "         4.0341e-02, -1.1303e-02, -2.8805e-02,  2.6510e-02, -4.8988e-02,\n",
      "        -1.4906e-02, -8.7503e-02, -3.8591e-03,  3.9093e-02,  2.1345e-02,\n",
      "         4.3803e-02, -4.8825e-02, -3.8691e-02, -7.1864e-02, -5.9994e-02,\n",
      "         2.5898e-02, -4.4769e-02,  8.8324e-02, -7.2772e-02,  1.5155e-02,\n",
      "        -5.5817e-02,  5.3736e-02, -2.9101e-02,  1.5793e-03, -1.7930e-01,\n",
      "        -1.7445e-02, -6.8678e-02, -2.1378e-02, -4.4950e-02, -1.7106e-02,\n",
      "         1.5411e-01,  7.0336e-02,  3.1394e-02,  9.1400e-02, -6.3379e-02,\n",
      "         9.3097e-02,  6.2873e-02, -2.3895e-02,  4.8823e-02,  1.5050e-02,\n",
      "         1.5749e-01,  2.0483e-02,  2.5478e-02,  1.2565e-01,  6.4963e-02,\n",
      "        -3.3720e-02,  3.8453e-02, -6.7775e-02, -1.1753e-01,  6.8093e-02,\n",
      "         5.1249e-02, -1.5064e-01, -6.5369e-02,  4.8224e-02, -8.1458e-03,\n",
      "        -2.7762e-02, -2.5249e-02, -1.0149e-02, -1.9384e-02,  4.1005e-02,\n",
      "        -2.7609e-02, -9.2976e-02,  3.8276e-02,  7.2089e-02, -1.2936e-01,\n",
      "        -1.1778e-01, -6.5505e-02,  1.7166e-02,  1.5751e-02, -1.9162e-02,\n",
      "         5.5185e-03, -1.0558e-01, -2.3025e-02, -1.4394e-01,  1.1885e-01,\n",
      "         9.7875e-03, -9.7859e-02, -3.9622e-02, -4.5969e-02, -4.3369e-02,\n",
      "        -2.5617e-02, -5.2712e-02,  3.9468e-02,  1.0800e-01,  5.3185e-02,\n",
      "        -5.0451e-02,  5.3125e-02,  1.4214e-01,  1.0340e-01, -1.7702e-02,\n",
      "        -6.3901e-02,  3.0720e-02, -7.3908e-02,  9.5226e-02,  6.2002e-03,\n",
      "         5.0914e-02, -6.5561e-02,  5.4568e-02,  5.1027e-02, -4.2785e-02,\n",
      "        -7.9318e-02,  6.1157e-02,  6.2453e-02, -4.5603e-02, -2.7345e-02,\n",
      "        -5.6974e-02,  1.2981e-01,  1.0213e-01,  4.7302e-02, -2.4651e-02,\n",
      "        -3.3669e-02, -4.9926e-02,  7.3012e-02, -3.4709e-02,  1.2907e-01,\n",
      "         6.1702e-02,  3.1375e-02,  1.9113e-02, -9.1100e-02,  7.3931e-03,\n",
      "        -8.0293e-02, -3.6101e-02,  5.1210e-02, -2.9621e-02,  5.9973e-03,\n",
      "         9.6392e-02,  4.1492e-03,  2.3054e-02, -8.5028e-02,  1.3075e-03,\n",
      "        -8.0786e-02,  7.1889e-02, -3.7784e-02,  2.0823e-02, -5.1179e-02,\n",
      "         1.3547e-01,  4.0677e-02,  6.0206e-02, -4.4987e-03, -1.4705e-02,\n",
      "        -4.3924e-05,  2.2686e-02, -3.4385e-02, -3.4656e-02, -1.7687e-01,\n",
      "         4.2150e-02, -3.2622e-03, -4.4221e-02, -5.9327e-02, -1.2178e-01,\n",
      "        -9.8243e-02,  2.9285e-02,  1.0800e-01, -2.9136e-02, -1.2633e-02,\n",
      "        -8.9605e-02,  1.0191e-02,  2.9528e-02, -1.6184e-02, -2.1323e-02,\n",
      "         4.3191e-02, -5.9493e-02, -9.2964e-02, -2.2478e-02,  1.4769e-02,\n",
      "        -2.1768e-02,  4.5379e-02,  7.2459e-02,  6.8969e-02, -3.1864e-02,\n",
      "         9.2427e-03,  1.1675e-01, -1.2651e-02, -8.6167e-02, -7.0927e-02,\n",
      "        -3.2216e-02, -3.6091e-02,  1.1292e-02, -2.3667e-02,  1.0530e-01,\n",
      "        -2.7349e-02,  3.5006e-02, -8.4804e-02,  5.3443e-02, -3.9848e-02,\n",
      "        -1.8628e-02, -9.9607e-02, -1.0862e-01,  3.0266e-03,  6.8604e-02,\n",
      "        -2.6277e-02, -1.4869e-01, -4.6595e-02, -9.2243e-02,  5.9800e-02,\n",
      "         1.6846e-02, -5.4605e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0000500'], tr/val_loss:  2.371605/  5.118587, val:  10.83%, val_best:  10.83%, tr:  12.36%, tr_best:  12.36%, epoch time: 29.36 seconds, 0.49 minutes\n",
      "[module.layers.3] weight_fb parameter count: 5,120\n",
      "epoch-1   lr=['0.0000500'], tr/val_loss:  2.247068/  3.472830, val:  25.83%, val_best:  25.83%, tr:  19.82%, tr_best:  19.82%, epoch time: 28.81 seconds, 0.48 minutes\n",
      "epoch-2   lr=['0.0000500'], tr/val_loss:  2.170368/  2.995939, val:  31.25%, val_best:  31.25%, tr:  26.76%, tr_best:  26.76%, epoch time: 29.06 seconds, 0.48 minutes\n",
      "epoch-3   lr=['0.0000500'], tr/val_loss:  2.137637/  2.793401, val:  33.75%, val_best:  33.75%, tr:  28.80%, tr_best:  28.80%, epoch time: 29.42 seconds, 0.49 minutes\n",
      "epoch-4   lr=['0.0000500'], tr/val_loss:  2.113395/  2.649035, val:  33.75%, val_best:  33.75%, tr:  30.64%, tr_best:  30.64%, epoch time: 28.56 seconds, 0.48 minutes\n",
      "epoch-5   lr=['0.0000500'], tr/val_loss:  2.092077/  2.556839, val:  37.92%, val_best:  37.92%, tr:  34.01%, tr_best:  34.01%, epoch time: 29.68 seconds, 0.49 minutes\n",
      "epoch-6   lr=['0.0000500'], tr/val_loss:  2.078205/  2.454376, val:  37.50%, val_best:  37.92%, tr:  37.18%, tr_best:  37.18%, epoch time: 29.43 seconds, 0.49 minutes\n",
      "epoch-7   lr=['0.0000500'], tr/val_loss:  2.069847/  2.363671, val:  38.75%, val_best:  38.75%, tr:  36.47%, tr_best:  37.18%, epoch time: 30.01 seconds, 0.50 minutes\n",
      "epoch-8   lr=['0.0000500'], tr/val_loss:  2.056498/  2.352745, val:  40.00%, val_best:  40.00%, tr:  37.28%, tr_best:  37.28%, epoch time: 29.48 seconds, 0.49 minutes\n",
      "epoch-9   lr=['0.0000500'], tr/val_loss:  2.044362/  2.240498, val:  42.08%, val_best:  42.08%, tr:  40.04%, tr_best:  40.04%, epoch time: 29.31 seconds, 0.49 minutes\n",
      "epoch-10  lr=['0.0000500'], tr/val_loss:  2.036448/  2.244184, val:  43.75%, val_best:  43.75%, tr:  40.86%, tr_best:  40.86%, epoch time: 28.30 seconds, 0.47 minutes\n",
      "epoch-11  lr=['0.0000500'], tr/val_loss:  2.030485/  2.151830, val:  44.17%, val_best:  44.17%, tr:  41.27%, tr_best:  41.27%, epoch time: 29.45 seconds, 0.49 minutes\n",
      "epoch-12  lr=['0.0000500'], tr/val_loss:  2.025893/  2.070211, val:  44.17%, val_best:  44.17%, tr:  42.39%, tr_best:  42.39%, epoch time: 27.93 seconds, 0.47 minutes\n",
      "epoch-13  lr=['0.0000500'], tr/val_loss:  2.014351/  2.046974, val:  45.00%, val_best:  45.00%, tr:  42.70%, tr_best:  42.70%, epoch time: 30.06 seconds, 0.50 minutes\n",
      "epoch-14  lr=['0.0000500'], tr/val_loss:  2.009825/  2.011641, val:  46.25%, val_best:  46.25%, tr:  44.02%, tr_best:  44.02%, epoch time: 28.23 seconds, 0.47 minutes\n",
      "epoch-15  lr=['0.0000500'], tr/val_loss:  2.003589/  2.010746, val:  47.50%, val_best:  47.50%, tr:  44.23%, tr_best:  44.23%, epoch time: 28.79 seconds, 0.48 minutes\n",
      "epoch-16  lr=['0.0000500'], tr/val_loss:  1.997173/  1.952871, val:  47.92%, val_best:  47.92%, tr:  45.45%, tr_best:  45.45%, epoch time: 29.46 seconds, 0.49 minutes\n",
      "epoch-17  lr=['0.0000500'], tr/val_loss:  1.998143/  1.949483, val:  49.17%, val_best:  49.17%, tr:  44.74%, tr_best:  45.45%, epoch time: 28.54 seconds, 0.48 minutes\n",
      "epoch-18  lr=['0.0000500'], tr/val_loss:  1.993508/  1.940573, val:  48.75%, val_best:  49.17%, tr:  45.56%, tr_best:  45.56%, epoch time: 30.37 seconds, 0.51 minutes\n",
      "epoch-19  lr=['0.0000500'], tr/val_loss:  1.986683/  1.906897, val:  51.67%, val_best:  51.67%, tr:  45.56%, tr_best:  45.56%, epoch time: 28.22 seconds, 0.47 minutes\n",
      "epoch-20  lr=['0.0000500'], tr/val_loss:  1.981642/  1.910239, val:  52.08%, val_best:  52.08%, tr:  47.91%, tr_best:  47.91%, epoch time: 29.38 seconds, 0.49 minutes\n",
      "epoch-21  lr=['0.0000500'], tr/val_loss:  1.980783/  1.865987, val:  52.92%, val_best:  52.92%, tr:  47.80%, tr_best:  47.91%, epoch time: 28.01 seconds, 0.47 minutes\n",
      "epoch-22  lr=['0.0000500'], tr/val_loss:  1.977467/  1.831730, val:  52.92%, val_best:  52.92%, tr:  46.99%, tr_best:  47.91%, epoch time: 29.71 seconds, 0.50 minutes\n",
      "epoch-23  lr=['0.0000500'], tr/val_loss:  1.969095/  1.782180, val:  52.50%, val_best:  52.92%, tr:  49.74%, tr_best:  49.74%, epoch time: 30.15 seconds, 0.50 minutes\n",
      "epoch-24  lr=['0.0000500'], tr/val_loss:  1.974070/  1.734086, val:  52.08%, val_best:  52.92%, tr:  48.11%, tr_best:  49.74%, epoch time: 29.51 seconds, 0.49 minutes\n",
      "epoch-25  lr=['0.0000500'], tr/val_loss:  1.967487/  1.726013, val:  53.33%, val_best:  53.33%, tr:  48.21%, tr_best:  49.74%, epoch time: 29.22 seconds, 0.49 minutes\n",
      "epoch-26  lr=['0.0000500'], tr/val_loss:  1.960396/  1.733213, val:  54.17%, val_best:  54.17%, tr:  50.26%, tr_best:  50.26%, epoch time: 30.00 seconds, 0.50 minutes\n",
      "epoch-27  lr=['0.0000500'], tr/val_loss:  1.960100/  1.715336, val:  55.42%, val_best:  55.42%, tr:  50.36%, tr_best:  50.36%, epoch time: 29.54 seconds, 0.49 minutes\n",
      "epoch-28  lr=['0.0000500'], tr/val_loss:  1.955646/  1.736001, val:  55.00%, val_best:  55.42%, tr:  50.66%, tr_best:  50.66%, epoch time: 29.18 seconds, 0.49 minutes\n",
      "epoch-29  lr=['0.0000500'], tr/val_loss:  1.968061/  1.669347, val:  55.83%, val_best:  55.83%, tr:  48.42%, tr_best:  50.66%, epoch time: 29.06 seconds, 0.48 minutes\n",
      "epoch-30  lr=['0.0000500'], tr/val_loss:  1.950899/  1.644695, val:  56.25%, val_best:  56.25%, tr:  50.77%, tr_best:  50.77%, epoch time: 28.78 seconds, 0.48 minutes\n",
      "epoch-31  lr=['0.0000500'], tr/val_loss:  1.956361/  1.663942, val:  56.67%, val_best:  56.67%, tr:  50.46%, tr_best:  50.77%, epoch time: 29.55 seconds, 0.49 minutes\n",
      "epoch-32  lr=['0.0000500'], tr/val_loss:  1.938831/  1.674120, val:  55.83%, val_best:  56.67%, tr:  54.14%, tr_best:  54.14%, epoch time: 28.53 seconds, 0.48 minutes\n",
      "epoch-33  lr=['0.0000500'], tr/val_loss:  1.941357/  1.644099, val:  57.92%, val_best:  57.92%, tr:  51.28%, tr_best:  54.14%, epoch time: 31.03 seconds, 0.52 minutes\n",
      "epoch-34  lr=['0.0000500'], tr/val_loss:  1.934785/  1.635813, val:  60.42%, val_best:  60.42%, tr:  53.12%, tr_best:  54.14%, epoch time: 28.84 seconds, 0.48 minutes\n",
      "epoch-35  lr=['0.0000500'], tr/val_loss:  1.940050/  1.610890, val:  60.00%, val_best:  60.42%, tr:  53.42%, tr_best:  54.14%, epoch time: 29.70 seconds, 0.50 minutes\n",
      "epoch-36  lr=['0.0000500'], tr/val_loss:  1.936036/  1.608174, val:  58.75%, val_best:  60.42%, tr:  53.52%, tr_best:  54.14%, epoch time: 29.06 seconds, 0.48 minutes\n",
      "epoch-37  lr=['0.0000500'], tr/val_loss:  1.933932/  1.660430, val:  59.17%, val_best:  60.42%, tr:  50.26%, tr_best:  54.14%, epoch time: 29.30 seconds, 0.49 minutes\n",
      "epoch-38  lr=['0.0000500'], tr/val_loss:  1.927516/  1.571402, val:  57.92%, val_best:  60.42%, tr:  53.63%, tr_best:  54.14%, epoch time: 29.48 seconds, 0.49 minutes\n",
      "epoch-39  lr=['0.0000500'], tr/val_loss:  1.918447/  1.573113, val:  60.00%, val_best:  60.42%, tr:  55.46%, tr_best:  55.46%, epoch time: 29.42 seconds, 0.49 minutes\n",
      "epoch-40  lr=['0.0000500'], tr/val_loss:  1.934508/  1.560678, val:  60.42%, val_best:  60.42%, tr:  51.89%, tr_best:  55.46%, epoch time: 29.82 seconds, 0.50 minutes\n",
      "epoch-41  lr=['0.0000500'], tr/val_loss:  1.921081/  1.542846, val:  61.67%, val_best:  61.67%, tr:  55.67%, tr_best:  55.67%, epoch time: 28.73 seconds, 0.48 minutes\n",
      "epoch-42  lr=['0.0000500'], tr/val_loss:  1.935641/  1.515308, val:  60.83%, val_best:  61.67%, tr:  53.12%, tr_best:  55.67%, epoch time: 29.80 seconds, 0.50 minutes\n",
      "epoch-43  lr=['0.0000500'], tr/val_loss:  1.915539/  1.518520, val:  62.08%, val_best:  62.08%, tr:  54.95%, tr_best:  55.67%, epoch time: 28.48 seconds, 0.47 minutes\n",
      "epoch-44  lr=['0.0000500'], tr/val_loss:  1.919462/  1.484417, val:  62.50%, val_best:  62.50%, tr:  50.97%, tr_best:  55.67%, epoch time: 28.78 seconds, 0.48 minutes\n",
      "epoch-45  lr=['0.0000500'], tr/val_loss:  1.918766/  1.488877, val:  62.08%, val_best:  62.50%, tr:  55.26%, tr_best:  55.67%, epoch time: 29.13 seconds, 0.49 minutes\n",
      "epoch-46  lr=['0.0000500'], tr/val_loss:  1.905696/  1.504709, val:  62.92%, val_best:  62.92%, tr:  57.10%, tr_best:  57.10%, epoch time: 28.77 seconds, 0.48 minutes\n",
      "epoch-47  lr=['0.0000500'], tr/val_loss:  1.906702/  1.494247, val:  64.17%, val_best:  64.17%, tr:  55.77%, tr_best:  57.10%, epoch time: 29.27 seconds, 0.49 minutes\n",
      "epoch-48  lr=['0.0000500'], tr/val_loss:  1.905841/  1.497751, val:  62.92%, val_best:  64.17%, tr:  56.59%, tr_best:  57.10%, epoch time: 28.90 seconds, 0.48 minutes\n",
      "epoch-49  lr=['0.0000500'], tr/val_loss:  1.912788/  1.508057, val:  63.33%, val_best:  64.17%, tr:  54.95%, tr_best:  57.10%, epoch time: 29.00 seconds, 0.48 minutes\n",
      "epoch-50  lr=['0.0000500'], tr/val_loss:  1.905441/  1.532374, val:  62.92%, val_best:  64.17%, tr:  54.75%, tr_best:  57.10%, epoch time: 29.12 seconds, 0.49 minutes\n",
      "epoch-51  lr=['0.0000500'], tr/val_loss:  1.904467/  1.507125, val:  62.08%, val_best:  64.17%, tr:  55.67%, tr_best:  57.10%, epoch time: 29.07 seconds, 0.48 minutes\n",
      "epoch-52  lr=['0.0000500'], tr/val_loss:  1.903701/  1.574491, val:  62.08%, val_best:  64.17%, tr:  54.65%, tr_best:  57.10%, epoch time: 28.25 seconds, 0.47 minutes\n",
      "epoch-53  lr=['0.0000500'], tr/val_loss:  1.901541/  1.509487, val:  61.25%, val_best:  64.17%, tr:  58.63%, tr_best:  58.63%, epoch time: 29.83 seconds, 0.50 minutes\n",
      "epoch-54  lr=['0.0000500'], tr/val_loss:  1.900271/  1.493472, val:  62.92%, val_best:  64.17%, tr:  55.67%, tr_best:  58.63%, epoch time: 28.60 seconds, 0.48 minutes\n",
      "epoch-55  lr=['0.0000500'], tr/val_loss:  1.897403/  1.495532, val:  63.33%, val_best:  64.17%, tr:  58.73%, tr_best:  58.73%, epoch time: 28.87 seconds, 0.48 minutes\n",
      "epoch-56  lr=['0.0000500'], tr/val_loss:  1.899033/  1.474518, val:  63.33%, val_best:  64.17%, tr:  57.30%, tr_best:  58.73%, epoch time: 29.82 seconds, 0.50 minutes\n",
      "epoch-57  lr=['0.0000500'], tr/val_loss:  1.907136/  1.422888, val:  65.42%, val_best:  65.42%, tr:  56.49%, tr_best:  58.73%, epoch time: 30.55 seconds, 0.51 minutes\n",
      "epoch-58  lr=['0.0000500'], tr/val_loss:  1.885057/  1.470729, val:  65.00%, val_best:  65.42%, tr:  60.67%, tr_best:  60.67%, epoch time: 29.84 seconds, 0.50 minutes\n",
      "epoch-59  lr=['0.0000500'], tr/val_loss:  1.883832/  1.492103, val:  65.42%, val_best:  65.42%, tr:  58.63%, tr_best:  60.67%, epoch time: 28.69 seconds, 0.48 minutes\n",
      "epoch-60  lr=['0.0000500'], tr/val_loss:  1.892078/  1.440460, val:  65.00%, val_best:  65.42%, tr:  58.73%, tr_best:  60.67%, epoch time: 29.21 seconds, 0.49 minutes\n",
      "epoch-61  lr=['0.0000500'], tr/val_loss:  1.892362/  1.475619, val:  65.42%, val_best:  65.42%, tr:  56.79%, tr_best:  60.67%, epoch time: 28.74 seconds, 0.48 minutes\n",
      "epoch-62  lr=['0.0000500'], tr/val_loss:  1.881505/  1.485785, val:  64.58%, val_best:  65.42%, tr:  61.39%, tr_best:  61.39%, epoch time: 30.09 seconds, 0.50 minutes\n",
      "epoch-63  lr=['0.0000500'], tr/val_loss:  1.886653/  1.452250, val:  64.58%, val_best:  65.42%, tr:  59.14%, tr_best:  61.39%, epoch time: 29.09 seconds, 0.48 minutes\n",
      "epoch-64  lr=['0.0000500'], tr/val_loss:  1.883909/  1.475957, val:  63.75%, val_best:  65.42%, tr:  57.10%, tr_best:  61.39%, epoch time: 29.23 seconds, 0.49 minutes\n",
      "epoch-65  lr=['0.0000500'], tr/val_loss:  1.898046/  1.413967, val:  67.50%, val_best:  67.50%, tr:  57.41%, tr_best:  61.39%, epoch time: 29.04 seconds, 0.48 minutes\n",
      "epoch-66  lr=['0.0000500'], tr/val_loss:  1.881763/  1.398957, val:  69.58%, val_best:  69.58%, tr:  60.47%, tr_best:  61.39%, epoch time: 29.28 seconds, 0.49 minutes\n",
      "epoch-67  lr=['0.0000500'], tr/val_loss:  1.869444/  1.429384, val:  65.83%, val_best:  69.58%, tr:  62.21%, tr_best:  62.21%, epoch time: 29.66 seconds, 0.49 minutes\n",
      "epoch-68  lr=['0.0000500'], tr/val_loss:  1.885431/  1.422199, val:  65.83%, val_best:  69.58%, tr:  59.24%, tr_best:  62.21%, epoch time: 28.68 seconds, 0.48 minutes\n",
      "epoch-69  lr=['0.0000500'], tr/val_loss:  1.871158/  1.533293, val:  65.00%, val_best:  69.58%, tr:  59.55%, tr_best:  62.21%, epoch time: 29.61 seconds, 0.49 minutes\n",
      "epoch-70  lr=['0.0000500'], tr/val_loss:  1.874171/  1.423417, val:  65.83%, val_best:  69.58%, tr:  59.65%, tr_best:  62.21%, epoch time: 27.73 seconds, 0.46 minutes\n",
      "epoch-71  lr=['0.0000500'], tr/val_loss:  1.870494/  1.474966, val:  65.42%, val_best:  69.58%, tr:  62.31%, tr_best:  62.31%, epoch time: 28.97 seconds, 0.48 minutes\n",
      "epoch-72  lr=['0.0000500'], tr/val_loss:  1.876198/  1.412165, val:  67.08%, val_best:  69.58%, tr:  60.37%, tr_best:  62.31%, epoch time: 29.89 seconds, 0.50 minutes\n",
      "epoch-73  lr=['0.0000500'], tr/val_loss:  1.875849/  1.455370, val:  67.92%, val_best:  69.58%, tr:  61.29%, tr_best:  62.31%, epoch time: 29.29 seconds, 0.49 minutes\n",
      "epoch-74  lr=['0.0000500'], tr/val_loss:  1.870385/  1.401748, val:  66.25%, val_best:  69.58%, tr:  61.29%, tr_best:  62.31%, epoch time: 28.40 seconds, 0.47 minutes\n",
      "epoch-75  lr=['0.0000500'], tr/val_loss:  1.871071/  1.390697, val:  67.92%, val_best:  69.58%, tr:  62.21%, tr_best:  62.31%, epoch time: 29.26 seconds, 0.49 minutes\n",
      "epoch-76  lr=['0.0000500'], tr/val_loss:  1.859593/  1.423304, val:  65.83%, val_best:  69.58%, tr:  62.10%, tr_best:  62.31%, epoch time: 29.95 seconds, 0.50 minutes\n",
      "epoch-77  lr=['0.0000500'], tr/val_loss:  1.871832/  1.370679, val:  69.58%, val_best:  69.58%, tr:  59.86%, tr_best:  62.31%, epoch time: 30.05 seconds, 0.50 minutes\n",
      "epoch-78  lr=['0.0000500'], tr/val_loss:  1.867643/  1.445560, val:  67.92%, val_best:  69.58%, tr:  60.57%, tr_best:  62.31%, epoch time: 29.24 seconds, 0.49 minutes\n",
      "epoch-79  lr=['0.0000500'], tr/val_loss:  1.868236/  1.416979, val:  65.83%, val_best:  69.58%, tr:  59.55%, tr_best:  62.31%, epoch time: 28.23 seconds, 0.47 minutes\n",
      "epoch-80  lr=['0.0000500'], tr/val_loss:  1.870757/  1.404674, val:  66.67%, val_best:  69.58%, tr:  61.80%, tr_best:  62.31%, epoch time: 30.55 seconds, 0.51 minutes\n",
      "epoch-81  lr=['0.0000500'], tr/val_loss:  1.877039/  1.358864, val:  69.17%, val_best:  69.58%, tr:  60.16%, tr_best:  62.31%, epoch time: 28.21 seconds, 0.47 minutes\n",
      "epoch-82  lr=['0.0000500'], tr/val_loss:  1.873003/  1.398431, val:  69.58%, val_best:  69.58%, tr:  60.57%, tr_best:  62.31%, epoch time: 28.34 seconds, 0.47 minutes\n",
      "epoch-83  lr=['0.0000500'], tr/val_loss:  1.856541/  1.417382, val:  68.75%, val_best:  69.58%, tr:  64.04%, tr_best:  64.04%, epoch time: 29.77 seconds, 0.50 minutes\n",
      "epoch-84  lr=['0.0000500'], tr/val_loss:  1.862575/  1.393432, val:  67.50%, val_best:  69.58%, tr:  63.53%, tr_best:  64.04%, epoch time: 29.21 seconds, 0.49 minutes\n",
      "epoch-85  lr=['0.0000500'], tr/val_loss:  1.854061/  1.380958, val:  68.75%, val_best:  69.58%, tr:  63.23%, tr_best:  64.04%, epoch time: 27.78 seconds, 0.46 minutes\n",
      "epoch-86  lr=['0.0000500'], tr/val_loss:  1.854167/  1.396411, val:  67.92%, val_best:  69.58%, tr:  64.96%, tr_best:  64.96%, epoch time: 29.25 seconds, 0.49 minutes\n",
      "epoch-87  lr=['0.0000500'], tr/val_loss:  1.851388/  1.404276, val:  68.75%, val_best:  69.58%, tr:  61.39%, tr_best:  64.96%, epoch time: 28.87 seconds, 0.48 minutes\n",
      "epoch-88  lr=['0.0000500'], tr/val_loss:  1.855076/  1.383768, val:  68.33%, val_best:  69.58%, tr:  62.31%, tr_best:  64.96%, epoch time: 29.47 seconds, 0.49 minutes\n",
      "epoch-89  lr=['0.0000500'], tr/val_loss:  1.847095/  1.413501, val:  67.50%, val_best:  69.58%, tr:  63.23%, tr_best:  64.96%, epoch time: 30.56 seconds, 0.51 minutes\n",
      "epoch-90  lr=['0.0000500'], tr/val_loss:  1.852226/  1.374948, val:  67.92%, val_best:  69.58%, tr:  64.76%, tr_best:  64.96%, epoch time: 30.53 seconds, 0.51 minutes\n",
      "epoch-91  lr=['0.0000500'], tr/val_loss:  1.840378/  1.363022, val:  70.00%, val_best:  70.00%, tr:  67.31%, tr_best:  67.31%, epoch time: 29.62 seconds, 0.49 minutes\n",
      "epoch-92  lr=['0.0000500'], tr/val_loss:  1.850294/  1.369486, val:  68.75%, val_best:  70.00%, tr:  63.23%, tr_best:  67.31%, epoch time: 28.02 seconds, 0.47 minutes\n",
      "epoch-93  lr=['0.0000500'], tr/val_loss:  1.846684/  1.380700, val:  70.83%, val_best:  70.83%, tr:  64.66%, tr_best:  67.31%, epoch time: 30.26 seconds, 0.50 minutes\n",
      "epoch-94  lr=['0.0000500'], tr/val_loss:  1.841061/  1.445780, val:  69.58%, val_best:  70.83%, tr:  64.66%, tr_best:  67.31%, epoch time: 28.46 seconds, 0.47 minutes\n",
      "epoch-95  lr=['0.0000500'], tr/val_loss:  1.855423/  1.434332, val:  69.17%, val_best:  70.83%, tr:  62.41%, tr_best:  67.31%, epoch time: 28.74 seconds, 0.48 minutes\n",
      "epoch-96  lr=['0.0000500'], tr/val_loss:  1.843195/  1.378524, val:  71.67%, val_best:  71.67%, tr:  63.64%, tr_best:  67.31%, epoch time: 29.45 seconds, 0.49 minutes\n",
      "epoch-97  lr=['0.0000500'], tr/val_loss:  1.842221/  1.375418, val:  70.83%, val_best:  71.67%, tr:  65.27%, tr_best:  67.31%, epoch time: 29.70 seconds, 0.49 minutes\n",
      "epoch-98  lr=['0.0000500'], tr/val_loss:  1.843771/  1.416912, val:  70.00%, val_best:  71.67%, tr:  65.68%, tr_best:  67.31%, epoch time: 28.47 seconds, 0.47 minutes\n",
      "epoch-99  lr=['0.0000500'], tr/val_loss:  1.846561/  1.424029, val:  71.25%, val_best:  71.67%, tr:  65.58%, tr_best:  67.31%, epoch time: 28.70 seconds, 0.48 minutes\n",
      "epoch-100 lr=['0.0000500'], tr/val_loss:  1.843769/  1.357308, val:  71.25%, val_best:  71.67%, tr:  64.35%, tr_best:  67.31%, epoch time: 28.63 seconds, 0.48 minutes\n",
      "epoch-101 lr=['0.0000500'], tr/val_loss:  1.836411/  1.400334, val:  71.25%, val_best:  71.67%, tr:  64.66%, tr_best:  67.31%, epoch time: 29.22 seconds, 0.49 minutes\n",
      "epoch-102 lr=['0.0000500'], tr/val_loss:  1.837526/  1.368872, val:  71.25%, val_best:  71.67%, tr:  67.21%, tr_best:  67.31%, epoch time: 29.90 seconds, 0.50 minutes\n",
      "epoch-103 lr=['0.0000500'], tr/val_loss:  1.831364/  1.389813, val:  71.67%, val_best:  71.67%, tr:  68.03%, tr_best:  68.03%, epoch time: 29.06 seconds, 0.48 minutes\n",
      "epoch-104 lr=['0.0000500'], tr/val_loss:  1.835167/  1.383815, val:  72.08%, val_best:  72.08%, tr:  66.29%, tr_best:  68.03%, epoch time: 29.41 seconds, 0.49 minutes\n",
      "epoch-105 lr=['0.0000500'], tr/val_loss:  1.830202/  1.378637, val:  71.25%, val_best:  72.08%, tr:  65.99%, tr_best:  68.03%, epoch time: 27.74 seconds, 0.46 minutes\n",
      "epoch-106 lr=['0.0000500'], tr/val_loss:  1.823977/  1.357436, val:  70.83%, val_best:  72.08%, tr:  64.96%, tr_best:  68.03%, epoch time: 29.44 seconds, 0.49 minutes\n",
      "epoch-107 lr=['0.0000500'], tr/val_loss:  1.826380/  1.367184, val:  72.08%, val_best:  72.08%, tr:  66.19%, tr_best:  68.03%, epoch time: 28.55 seconds, 0.48 minutes\n",
      "epoch-108 lr=['0.0000500'], tr/val_loss:  1.833436/  1.359686, val:  71.25%, val_best:  72.08%, tr:  66.70%, tr_best:  68.03%, epoch time: 29.53 seconds, 0.49 minutes\n",
      "epoch-109 lr=['0.0000500'], tr/val_loss:  1.837104/  1.359312, val:  72.08%, val_best:  72.08%, tr:  67.52%, tr_best:  68.03%, epoch time: 28.25 seconds, 0.47 minutes\n",
      "epoch-110 lr=['0.0000500'], tr/val_loss:  1.820923/  1.345579, val:  72.50%, val_best:  72.50%, tr:  66.91%, tr_best:  68.03%, epoch time: 28.62 seconds, 0.48 minutes\n",
      "epoch-111 lr=['0.0000500'], tr/val_loss:  1.834962/  1.355777, val:  72.92%, val_best:  72.92%, tr:  65.58%, tr_best:  68.03%, epoch time: 29.64 seconds, 0.49 minutes\n",
      "epoch-112 lr=['0.0000500'], tr/val_loss:  1.827041/  1.376012, val:  72.50%, val_best:  72.92%, tr:  67.31%, tr_best:  68.03%, epoch time: 27.91 seconds, 0.47 minutes\n",
      "epoch-113 lr=['0.0000500'], tr/val_loss:  1.832185/  1.377283, val:  72.50%, val_best:  72.92%, tr:  66.50%, tr_best:  68.03%, epoch time: 29.54 seconds, 0.49 minutes\n",
      "epoch-114 lr=['0.0000500'], tr/val_loss:  1.822132/  1.419882, val:  70.83%, val_best:  72.92%, tr:  67.11%, tr_best:  68.03%, epoch time: 28.22 seconds, 0.47 minutes\n",
      "epoch-115 lr=['0.0000500'], tr/val_loss:  1.820647/  1.342180, val:  72.92%, val_best:  72.92%, tr:  66.50%, tr_best:  68.03%, epoch time: 28.60 seconds, 0.48 minutes\n",
      "epoch-116 lr=['0.0000500'], tr/val_loss:  1.822863/  1.364529, val:  72.92%, val_best:  72.92%, tr:  68.03%, tr_best:  68.03%, epoch time: 28.90 seconds, 0.48 minutes\n",
      "epoch-117 lr=['0.0000500'], tr/val_loss:  1.818277/  1.375595, val:  71.25%, val_best:  72.92%, tr:  65.88%, tr_best:  68.03%, epoch time: 29.26 seconds, 0.49 minutes\n",
      "epoch-118 lr=['0.0000500'], tr/val_loss:  1.817328/  1.381186, val:  73.33%, val_best:  73.33%, tr:  66.80%, tr_best:  68.03%, epoch time: 28.25 seconds, 0.47 minutes\n",
      "epoch-119 lr=['0.0000500'], tr/val_loss:  1.819296/  1.428177, val:  72.92%, val_best:  73.33%, tr:  67.31%, tr_best:  68.03%, epoch time: 28.44 seconds, 0.47 minutes\n",
      "epoch-120 lr=['0.0000500'], tr/val_loss:  1.823112/  1.393680, val:  73.33%, val_best:  73.33%, tr:  66.80%, tr_best:  68.03%, epoch time: 29.53 seconds, 0.49 minutes\n",
      "epoch-121 lr=['0.0000500'], tr/val_loss:  1.807027/  1.446186, val:  72.08%, val_best:  73.33%, tr:  68.54%, tr_best:  68.54%, epoch time: 28.19 seconds, 0.47 minutes\n",
      "epoch-122 lr=['0.0000500'], tr/val_loss:  1.816968/  1.434764, val:  71.67%, val_best:  73.33%, tr:  66.39%, tr_best:  68.54%, epoch time: 29.01 seconds, 0.48 minutes\n",
      "epoch-123 lr=['0.0000500'], tr/val_loss:  1.811001/  1.404904, val:  73.75%, val_best:  73.75%, tr:  67.11%, tr_best:  68.54%, epoch time: 29.80 seconds, 0.50 minutes\n",
      "epoch-124 lr=['0.0000500'], tr/val_loss:  1.813954/  1.412646, val:  72.92%, val_best:  73.75%, tr:  68.03%, tr_best:  68.54%, epoch time: 28.92 seconds, 0.48 minutes\n",
      "epoch-125 lr=['0.0000500'], tr/val_loss:  1.814724/  1.420939, val:  73.33%, val_best:  73.75%, tr:  68.23%, tr_best:  68.54%, epoch time: 28.35 seconds, 0.47 minutes\n",
      "epoch-126 lr=['0.0000500'], tr/val_loss:  1.812950/  1.438188, val:  70.83%, val_best:  73.75%, tr:  67.42%, tr_best:  68.54%, epoch time: 29.16 seconds, 0.49 minutes\n",
      "epoch-127 lr=['0.0000500'], tr/val_loss:  1.812136/  1.413963, val:  71.67%, val_best:  73.75%, tr:  69.05%, tr_best:  69.05%, epoch time: 28.44 seconds, 0.47 minutes\n",
      "epoch-128 lr=['0.0000500'], tr/val_loss:  1.809784/  1.382643, val:  72.50%, val_best:  73.75%, tr:  68.34%, tr_best:  69.05%, epoch time: 29.23 seconds, 0.49 minutes\n",
      "epoch-129 lr=['0.0000500'], tr/val_loss:  1.813952/  1.413214, val:  72.92%, val_best:  73.75%, tr:  68.74%, tr_best:  69.05%, epoch time: 28.46 seconds, 0.47 minutes\n",
      "epoch-130 lr=['0.0000500'], tr/val_loss:  1.809198/  1.427331, val:  73.33%, val_best:  73.75%, tr:  67.93%, tr_best:  69.05%, epoch time: 28.00 seconds, 0.47 minutes\n",
      "epoch-131 lr=['0.0000500'], tr/val_loss:  1.810972/  1.406950, val:  73.33%, val_best:  73.75%, tr:  68.13%, tr_best:  69.05%, epoch time: 29.61 seconds, 0.49 minutes\n",
      "epoch-132 lr=['0.0000500'], tr/val_loss:  1.812486/  1.390633, val:  73.33%, val_best:  73.75%, tr:  67.21%, tr_best:  69.05%, epoch time: 28.62 seconds, 0.48 minutes\n",
      "epoch-133 lr=['0.0000500'], tr/val_loss:  1.799842/  1.397908, val:  73.75%, val_best:  73.75%, tr:  70.79%, tr_best:  70.79%, epoch time: 29.03 seconds, 0.48 minutes\n",
      "epoch-134 lr=['0.0000500'], tr/val_loss:  1.816947/  1.423054, val:  73.33%, val_best:  73.75%, tr:  67.93%, tr_best:  70.79%, epoch time: 28.62 seconds, 0.48 minutes\n",
      "epoch-135 lr=['0.0000500'], tr/val_loss:  1.808485/  1.408095, val:  73.33%, val_best:  73.75%, tr:  70.48%, tr_best:  70.79%, epoch time: 29.39 seconds, 0.49 minutes\n",
      "epoch-136 lr=['0.0000500'], tr/val_loss:  1.808476/  1.411002, val:  72.08%, val_best:  73.75%, tr:  67.42%, tr_best:  70.79%, epoch time: 29.38 seconds, 0.49 minutes\n",
      "epoch-137 lr=['0.0000500'], tr/val_loss:  1.812422/  1.442820, val:  72.50%, val_best:  73.75%, tr:  68.74%, tr_best:  70.79%, epoch time: 29.34 seconds, 0.49 minutes\n",
      "epoch-138 lr=['0.0000500'], tr/val_loss:  1.803692/  1.382392, val:  74.58%, val_best:  74.58%, tr:  66.91%, tr_best:  70.79%, epoch time: 30.01 seconds, 0.50 minutes\n",
      "epoch-139 lr=['0.0000500'], tr/val_loss:  1.798532/  1.378698, val:  73.75%, val_best:  74.58%, tr:  68.34%, tr_best:  70.79%, epoch time: 28.43 seconds, 0.47 minutes\n",
      "epoch-140 lr=['0.0000500'], tr/val_loss:  1.792738/  1.379748, val:  74.58%, val_best:  74.58%, tr:  70.68%, tr_best:  70.79%, epoch time: 29.02 seconds, 0.48 minutes\n",
      "epoch-141 lr=['0.0000500'], tr/val_loss:  1.801614/  1.428416, val:  72.92%, val_best:  74.58%, tr:  68.54%, tr_best:  70.79%, epoch time: 27.82 seconds, 0.46 minutes\n",
      "epoch-142 lr=['0.0000500'], tr/val_loss:  1.805909/  1.391208, val:  75.00%, val_best:  75.00%, tr:  69.15%, tr_best:  70.79%, epoch time: 28.41 seconds, 0.47 minutes\n",
      "epoch-143 lr=['0.0000500'], tr/val_loss:  1.819891/  1.399065, val:  73.75%, val_best:  75.00%, tr:  66.70%, tr_best:  70.79%, epoch time: 28.82 seconds, 0.48 minutes\n",
      "epoch-144 lr=['0.0000500'], tr/val_loss:  1.800951/  1.404382, val:  74.17%, val_best:  75.00%, tr:  69.56%, tr_best:  70.79%, epoch time: 29.17 seconds, 0.49 minutes\n",
      "epoch-145 lr=['0.0000500'], tr/val_loss:  1.797719/  1.402497, val:  72.92%, val_best:  75.00%, tr:  68.13%, tr_best:  70.79%, epoch time: 29.82 seconds, 0.50 minutes\n",
      "epoch-146 lr=['0.0000500'], tr/val_loss:  1.802471/  1.394426, val:  73.33%, val_best:  75.00%, tr:  70.89%, tr_best:  70.89%, epoch time: 29.20 seconds, 0.49 minutes\n",
      "epoch-147 lr=['0.0000500'], tr/val_loss:  1.786233/  1.414410, val:  73.75%, val_best:  75.00%, tr:  69.87%, tr_best:  70.89%, epoch time: 29.23 seconds, 0.49 minutes\n",
      "epoch-148 lr=['0.0000500'], tr/val_loss:  1.801892/  1.474029, val:  72.08%, val_best:  75.00%, tr:  67.01%, tr_best:  70.89%, epoch time: 30.21 seconds, 0.50 minutes\n",
      "epoch-149 lr=['0.0000500'], tr/val_loss:  1.789385/  1.465871, val:  71.67%, val_best:  75.00%, tr:  70.89%, tr_best:  70.89%, epoch time: 29.04 seconds, 0.48 minutes\n",
      "epoch-150 lr=['0.0000500'], tr/val_loss:  1.792511/  1.410020, val:  73.33%, val_best:  75.00%, tr:  70.07%, tr_best:  70.89%, epoch time: 28.49 seconds, 0.47 minutes\n",
      "epoch-151 lr=['0.0000500'], tr/val_loss:  1.790918/  1.429141, val:  72.92%, val_best:  75.00%, tr:  69.25%, tr_best:  70.89%, epoch time: 30.25 seconds, 0.50 minutes\n",
      "epoch-152 lr=['0.0000500'], tr/val_loss:  1.788102/  1.397575, val:  73.33%, val_best:  75.00%, tr:  69.66%, tr_best:  70.89%, epoch time: 28.33 seconds, 0.47 minutes\n",
      "epoch-153 lr=['0.0000500'], tr/val_loss:  1.790723/  1.401249, val:  73.33%, val_best:  75.00%, tr:  70.38%, tr_best:  70.89%, epoch time: 29.35 seconds, 0.49 minutes\n",
      "epoch-154 lr=['0.0000500'], tr/val_loss:  1.788700/  1.456370, val:  72.50%, val_best:  75.00%, tr:  71.50%, tr_best:  71.50%, epoch time: 27.97 seconds, 0.47 minutes\n",
      "epoch-155 lr=['0.0000500'], tr/val_loss:  1.787967/  1.433963, val:  73.75%, val_best:  75.00%, tr:  70.38%, tr_best:  71.50%, epoch time: 29.32 seconds, 0.49 minutes\n",
      "epoch-156 lr=['0.0000500'], tr/val_loss:  1.782643/  1.467165, val:  73.33%, val_best:  75.00%, tr:  71.60%, tr_best:  71.60%, epoch time: 29.19 seconds, 0.49 minutes\n",
      "epoch-157 lr=['0.0000500'], tr/val_loss:  1.778578/  1.431158, val:  74.17%, val_best:  75.00%, tr:  71.09%, tr_best:  71.60%, epoch time: 28.64 seconds, 0.48 minutes\n",
      "epoch-158 lr=['0.0000500'], tr/val_loss:  1.779598/  1.467953, val:  73.33%, val_best:  75.00%, tr:  71.30%, tr_best:  71.60%, epoch time: 29.68 seconds, 0.49 minutes\n",
      "epoch-159 lr=['0.0000500'], tr/val_loss:  1.785990/  1.442132, val:  74.17%, val_best:  75.00%, tr:  70.07%, tr_best:  71.60%, epoch time: 28.77 seconds, 0.48 minutes\n",
      "epoch-160 lr=['0.0000500'], tr/val_loss:  1.791225/  1.443635, val:  73.33%, val_best:  75.00%, tr:  69.36%, tr_best:  71.60%, epoch time: 28.85 seconds, 0.48 minutes\n",
      "epoch-161 lr=['0.0000500'], tr/val_loss:  1.775002/  1.437266, val:  73.33%, val_best:  75.00%, tr:  72.42%, tr_best:  72.42%, epoch time: 29.36 seconds, 0.49 minutes\n",
      "epoch-162 lr=['0.0000500'], tr/val_loss:  1.770927/  1.414336, val:  74.17%, val_best:  75.00%, tr:  72.32%, tr_best:  72.42%, epoch time: 30.04 seconds, 0.50 minutes\n",
      "epoch-163 lr=['0.0000500'], tr/val_loss:  1.782039/  1.458905, val:  73.33%, val_best:  75.00%, tr:  72.01%, tr_best:  72.42%, epoch time: 29.07 seconds, 0.48 minutes\n",
      "epoch-164 lr=['0.0000500'], tr/val_loss:  1.775382/  1.455893, val:  73.75%, val_best:  75.00%, tr:  70.17%, tr_best:  72.42%, epoch time: 29.21 seconds, 0.49 minutes\n",
      "epoch-165 lr=['0.0000500'], tr/val_loss:  1.780321/  1.478144, val:  73.75%, val_best:  75.00%, tr:  70.79%, tr_best:  72.42%, epoch time: 28.69 seconds, 0.48 minutes\n",
      "epoch-166 lr=['0.0000500'], tr/val_loss:  1.777787/  1.488474, val:  72.50%, val_best:  75.00%, tr:  71.09%, tr_best:  72.42%, epoch time: 29.36 seconds, 0.49 minutes\n",
      "epoch-167 lr=['0.0000500'], tr/val_loss:  1.780423/  1.433419, val:  73.33%, val_best:  75.00%, tr:  73.44%, tr_best:  73.44%, epoch time: 30.69 seconds, 0.51 minutes\n",
      "epoch-168 lr=['0.0000500'], tr/val_loss:  1.780238/  1.449130, val:  75.00%, val_best:  75.00%, tr:  70.89%, tr_best:  73.44%, epoch time: 29.10 seconds, 0.48 minutes\n",
      "epoch-169 lr=['0.0000500'], tr/val_loss:  1.788609/  1.435284, val:  75.00%, val_best:  75.00%, tr:  71.60%, tr_best:  73.44%, epoch time: 28.75 seconds, 0.48 minutes\n",
      "epoch-170 lr=['0.0000500'], tr/val_loss:  1.782928/  1.430622, val:  73.33%, val_best:  75.00%, tr:  69.87%, tr_best:  73.44%, epoch time: 30.28 seconds, 0.50 minutes\n",
      "epoch-171 lr=['0.0000500'], tr/val_loss:  1.767044/  1.466095, val:  73.75%, val_best:  75.00%, tr:  72.11%, tr_best:  73.44%, epoch time: 29.89 seconds, 0.50 minutes\n",
      "epoch-172 lr=['0.0000500'], tr/val_loss:  1.778922/  1.426556, val:  74.58%, val_best:  75.00%, tr:  69.77%, tr_best:  73.44%, epoch time: 28.02 seconds, 0.47 minutes\n",
      "epoch-173 lr=['0.0000500'], tr/val_loss:  1.777239/  1.429693, val:  73.33%, val_best:  75.00%, tr:  73.34%, tr_best:  73.44%, epoch time: 29.44 seconds, 0.49 minutes\n",
      "epoch-174 lr=['0.0000500'], tr/val_loss:  1.774918/  1.428684, val:  73.75%, val_best:  75.00%, tr:  71.91%, tr_best:  73.44%, epoch time: 30.32 seconds, 0.51 minutes\n",
      "epoch-175 lr=['0.0000500'], tr/val_loss:  1.770599/  1.428546, val:  75.00%, val_best:  75.00%, tr:  73.75%, tr_best:  73.75%, epoch time: 30.62 seconds, 0.51 minutes\n",
      "epoch-176 lr=['0.0000500'], tr/val_loss:  1.775624/  1.464400, val:  74.58%, val_best:  75.00%, tr:  70.07%, tr_best:  73.75%, epoch time: 28.04 seconds, 0.47 minutes\n",
      "epoch-177 lr=['0.0000500'], tr/val_loss:  1.769375/  1.460691, val:  75.00%, val_best:  75.00%, tr:  70.99%, tr_best:  73.75%, epoch time: 30.78 seconds, 0.51 minutes\n",
      "epoch-178 lr=['0.0000500'], tr/val_loss:  1.773471/  1.483628, val:  74.58%, val_best:  75.00%, tr:  71.09%, tr_best:  73.75%, epoch time: 30.28 seconds, 0.50 minutes\n",
      "epoch-179 lr=['0.0000500'], tr/val_loss:  1.774373/  1.479553, val:  73.75%, val_best:  75.00%, tr:  72.32%, tr_best:  73.75%, epoch time: 29.27 seconds, 0.49 minutes\n",
      "epoch-180 lr=['0.0000500'], tr/val_loss:  1.765440/  1.409752, val:  75.00%, val_best:  75.00%, tr:  72.83%, tr_best:  73.75%, epoch time: 30.04 seconds, 0.50 minutes\n",
      "epoch-181 lr=['0.0000500'], tr/val_loss:  1.765401/  1.411963, val:  74.58%, val_best:  75.00%, tr:  74.36%, tr_best:  74.36%, epoch time: 29.58 seconds, 0.49 minutes\n",
      "epoch-182 lr=['0.0000500'], tr/val_loss:  1.772363/  1.478811, val:  75.00%, val_best:  75.00%, tr:  73.24%, tr_best:  74.36%, epoch time: 28.39 seconds, 0.47 minutes\n",
      "epoch-183 lr=['0.0000500'], tr/val_loss:  1.772202/  1.463376, val:  75.00%, val_best:  75.00%, tr:  72.22%, tr_best:  74.36%, epoch time: 28.42 seconds, 0.47 minutes\n",
      "epoch-184 lr=['0.0000500'], tr/val_loss:  1.770079/  1.446561, val:  74.58%, val_best:  75.00%, tr:  70.17%, tr_best:  74.36%, epoch time: 29.20 seconds, 0.49 minutes\n",
      "epoch-185 lr=['0.0000500'], tr/val_loss:  1.765245/  1.425458, val:  75.42%, val_best:  75.42%, tr:  72.83%, tr_best:  74.36%, epoch time: 28.39 seconds, 0.47 minutes\n",
      "epoch-186 lr=['0.0000500'], tr/val_loss:  1.776173/  1.423274, val:  73.75%, val_best:  75.42%, tr:  70.89%, tr_best:  74.36%, epoch time: 29.09 seconds, 0.48 minutes\n",
      "epoch-187 lr=['0.0000500'], tr/val_loss:  1.771635/  1.407817, val:  75.42%, val_best:  75.42%, tr:  73.44%, tr_best:  74.36%, epoch time: 27.92 seconds, 0.47 minutes\n",
      "epoch-188 lr=['0.0000500'], tr/val_loss:  1.771620/  1.383523, val:  73.33%, val_best:  75.42%, tr:  71.30%, tr_best:  74.36%, epoch time: 29.30 seconds, 0.49 minutes\n",
      "epoch-189 lr=['0.0000500'], tr/val_loss:  1.770168/  1.442499, val:  75.00%, val_best:  75.42%, tr:  71.91%, tr_best:  74.36%, epoch time: 29.29 seconds, 0.49 minutes\n",
      "epoch-190 lr=['0.0000500'], tr/val_loss:  1.772947/  1.423445, val:  74.58%, val_best:  75.42%, tr:  72.93%, tr_best:  74.36%, epoch time: 27.99 seconds, 0.47 minutes\n",
      "epoch-191 lr=['0.0000500'], tr/val_loss:  1.766555/  1.455246, val:  74.58%, val_best:  75.42%, tr:  72.63%, tr_best:  74.36%, epoch time: 27.26 seconds, 0.45 minutes\n",
      "epoch-192 lr=['0.0000500'], tr/val_loss:  1.769101/  1.454483, val:  73.33%, val_best:  75.42%, tr:  72.32%, tr_best:  74.36%, epoch time: 29.87 seconds, 0.50 minutes\n",
      "epoch-193 lr=['0.0000500'], tr/val_loss:  1.761891/  1.458311, val:  74.58%, val_best:  75.42%, tr:  72.73%, tr_best:  74.36%, epoch time: 27.14 seconds, 0.45 minutes\n",
      "epoch-194 lr=['0.0000500'], tr/val_loss:  1.765505/  1.419494, val:  74.58%, val_best:  75.42%, tr:  72.93%, tr_best:  74.36%, epoch time: 28.11 seconds, 0.47 minutes\n",
      "epoch-195 lr=['0.0000500'], tr/val_loss:  1.760760/  1.460125, val:  74.17%, val_best:  75.42%, tr:  71.20%, tr_best:  74.36%, epoch time: 27.23 seconds, 0.45 minutes\n",
      "epoch-196 lr=['0.0000500'], tr/val_loss:  1.771844/  1.421740, val:  75.42%, val_best:  75.42%, tr:  72.63%, tr_best:  74.36%, epoch time: 26.87 seconds, 0.45 minutes\n",
      "epoch-197 lr=['0.0000500'], tr/val_loss:  1.756838/  1.430548, val:  75.42%, val_best:  75.42%, tr:  74.46%, tr_best:  74.46%, epoch time: 28.93 seconds, 0.48 minutes\n",
      "epoch-198 lr=['0.0000500'], tr/val_loss:  1.768158/  1.450853, val:  74.58%, val_best:  75.42%, tr:  72.11%, tr_best:  74.46%, epoch time: 29.07 seconds, 0.48 minutes\n",
      "epoch-199 lr=['0.0000500'], tr/val_loss:  1.759472/  1.511926, val:  74.17%, val_best:  75.42%, tr:  72.01%, tr_best:  74.46%, epoch time: 28.94 seconds, 0.48 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2893e94e75b64948a70aad46c04c1b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.72012</td></tr><tr><td>tr_epoch_loss</td><td>1.75947</td></tr><tr><td>val_acc_best</td><td>0.75417</td></tr><tr><td>val_acc_now</td><td>0.74167</td></tr><tr><td>val_loss</td><td>1.51193</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earnest-sweep-318</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g32op7ip' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g32op7ip</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251111_165034-g32op7ip/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kjwweooo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [512]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 5000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_threshold: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: one\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251111_182821-kjwweooo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kjwweooo' target=\"_blank\">grateful-sweep-324</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/p2hpq51o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kjwweooo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kjwweooo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_threshold' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': '20251111_182828_486', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.75, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 0.25, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.75, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [512], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.005, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'one', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'output_threshold': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 7a22c8a0ef5b9b252dbf98632e270efd\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=512, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.0625, v_reset=10000, sg_width=0.25, surrogate=one, BPTT_on=False, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=True)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=False, ANPI_MODE=True)\n",
      "      (4): SYNAPSE_FC(in_features=512, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient(single_step=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 506,880\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.005\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "inFeed spike.shape torch.Size([10, 512]) self.weight_fb.shape torch.Size([10, 512])\n",
      "self.weight_fb[0] tensor([ 1.2009e-02,  1.3379e-01, -1.0650e-02,  5.2556e-02, -1.1912e-01,\n",
      "         4.0419e-02, -4.0199e-02, -5.0604e-02,  3.2680e-02, -7.8942e-02,\n",
      "        -1.0288e-01, -1.8775e-02, -5.7299e-03,  1.2332e-02, -6.9353e-02,\n",
      "         1.1499e-01, -4.4228e-02,  4.2593e-02,  4.9323e-02, -2.0675e-03,\n",
      "         9.2336e-02, -3.1971e-02, -1.5728e-02,  9.1276e-02, -2.0181e-02,\n",
      "        -7.1800e-02,  1.4578e-01, -4.2861e-02,  1.1373e-02, -7.3257e-02,\n",
      "        -1.1159e-01, -9.7846e-02,  5.1912e-02,  8.7845e-02,  4.0044e-02,\n",
      "         2.6324e-02, -9.8372e-02,  3.8522e-02,  1.0460e-01, -4.1150e-02,\n",
      "         5.8342e-02,  4.8482e-03,  5.2401e-03, -8.7172e-03,  2.0523e-02,\n",
      "        -3.6457e-02, -6.6373e-02,  5.9048e-03, -2.0717e-02, -3.2546e-02,\n",
      "        -5.4324e-02,  2.4378e-02,  1.0149e-02, -1.2236e-02,  6.2543e-02,\n",
      "        -8.3454e-02, -2.1650e-02, -3.9879e-02,  2.7655e-02, -3.3246e-02,\n",
      "         7.6898e-02, -5.0422e-02,  1.5484e-02, -2.6447e-02,  6.8359e-02,\n",
      "        -6.8262e-02,  3.4312e-02, -7.9518e-02, -2.3619e-02,  3.1812e-02,\n",
      "         6.2016e-03,  1.6009e-02,  2.2387e-02,  1.4105e-01,  1.4450e-03,\n",
      "         9.7970e-02, -7.1751e-02,  5.8704e-02, -2.8309e-02,  4.7077e-02,\n",
      "        -3.5820e-02, -4.3640e-02, -4.4777e-02, -3.1386e-02, -2.7226e-02,\n",
      "        -2.5884e-02,  1.0779e-02,  2.7401e-02,  3.1376e-02, -7.5319e-02,\n",
      "        -1.6829e-02,  1.7118e-02, -8.9122e-02, -4.0006e-02,  4.6343e-03,\n",
      "         1.2001e-02,  3.6892e-02,  1.4373e-02,  7.0655e-02, -4.2197e-02,\n",
      "        -1.0233e-01,  3.7360e-04,  8.5512e-02,  7.8637e-02,  1.4384e-03,\n",
      "        -8.0477e-02, -4.6482e-02,  2.3251e-02, -3.3886e-02, -2.4537e-03,\n",
      "        -4.8149e-02, -1.5486e-01,  4.3330e-02, -5.8045e-03, -1.3386e-02,\n",
      "         2.7755e-02, -1.9510e-02,  1.3393e-03,  3.8708e-02,  1.5263e-02,\n",
      "         4.6335e-02, -7.2374e-03, -6.3238e-03, -3.1016e-02, -3.1252e-02,\n",
      "        -7.4723e-02, -1.5088e-02, -4.1994e-02,  1.2212e-02,  6.0550e-02,\n",
      "        -1.7745e-03,  1.0415e-01,  6.7522e-02, -6.1409e-02, -4.1550e-02,\n",
      "         1.0644e-01,  1.5230e-01, -3.8367e-02,  7.8697e-02, -1.7323e-02,\n",
      "         2.6986e-02,  2.6370e-02,  6.5894e-02, -1.2553e-01, -3.9156e-02,\n",
      "         1.3065e-01, -5.8646e-03,  1.4600e-02, -4.5190e-02, -1.0434e-01,\n",
      "         5.6415e-02,  4.8810e-02, -3.8917e-02,  1.3367e-01,  7.2065e-02,\n",
      "        -2.6348e-02,  1.4814e-02, -7.9086e-02, -7.4679e-03, -3.7547e-02,\n",
      "        -4.9995e-02,  1.3292e-04, -1.2034e-02,  4.6384e-02,  5.0249e-02,\n",
      "         5.1038e-02, -3.7747e-02,  8.0393e-02, -6.6428e-02, -1.4425e-03,\n",
      "        -2.2637e-02, -3.0118e-02,  9.2677e-03, -9.3434e-02,  1.9207e-02,\n",
      "        -2.7770e-02, -6.7883e-02, -7.8605e-02, -9.7644e-02, -9.8327e-02,\n",
      "        -4.0612e-02,  4.7043e-02, -3.7591e-02,  1.8712e-02, -8.3181e-02,\n",
      "        -1.9715e-02,  3.6721e-02,  3.5419e-02, -4.6781e-02, -7.8367e-03,\n",
      "        -2.6748e-02, -8.6308e-02,  2.3989e-02, -1.2710e-02,  3.7118e-02,\n",
      "        -6.2088e-02, -2.2962e-04, -4.9640e-02,  2.4384e-02,  1.5691e-01,\n",
      "         1.5421e-02,  5.5528e-02,  4.8312e-02,  5.6640e-02, -2.2735e-02,\n",
      "         5.3113e-03, -5.2211e-02,  2.6325e-02,  6.9295e-02,  2.4738e-02,\n",
      "        -5.3518e-03,  5.2276e-02, -2.4634e-02, -5.3242e-03,  1.2084e-01,\n",
      "        -2.6133e-02,  3.3964e-02,  9.2582e-03, -1.2223e-01, -2.1360e-03,\n",
      "        -7.8244e-02, -1.5748e-02,  1.4439e-03,  1.2431e-01,  6.0634e-02,\n",
      "         8.5934e-02, -6.0989e-02, -2.9897e-02, -1.1970e-03, -1.0762e-01,\n",
      "         1.0423e-02,  1.6176e-02, -1.3812e-02, -5.2755e-02,  1.6920e-02,\n",
      "         6.1367e-02,  9.1813e-02,  2.1540e-02,  7.7856e-03, -4.0828e-02,\n",
      "        -9.7598e-02, -4.1089e-02,  9.0935e-02,  1.8519e-02, -3.4424e-02,\n",
      "         2.8530e-03, -6.6620e-02, -8.9594e-03, -6.7013e-03, -4.6130e-02,\n",
      "        -2.1535e-02,  5.8145e-03,  4.0000e-03, -5.7107e-02,  4.8855e-02,\n",
      "        -1.1148e-01, -1.1978e-01,  6.8131e-02,  1.5512e-03,  3.5912e-02,\n",
      "         3.3328e-02,  3.1726e-02, -8.8611e-02,  1.4725e-01, -9.5569e-02,\n",
      "        -1.0785e-02, -1.3891e-03,  1.3467e-02,  4.0348e-02,  9.6515e-02,\n",
      "         1.6649e-02,  3.0992e-02, -1.5092e-02, -5.3478e-02,  2.6478e-02,\n",
      "        -1.3042e-02, -9.5301e-02, -6.6575e-03, -1.5733e-03, -9.9895e-03,\n",
      "         3.4082e-02,  1.5740e-01, -9.9586e-03, -5.3744e-02,  8.7394e-02,\n",
      "         4.2685e-02,  5.2481e-02,  1.7623e-02,  1.0548e-03,  4.5100e-02,\n",
      "         7.4265e-02, -7.1658e-03, -8.7438e-02, -3.9754e-02,  5.4727e-02,\n",
      "         4.6412e-02,  4.2058e-02, -3.2855e-02, -1.1088e-01, -1.7722e-02,\n",
      "         4.9851e-03, -8.0476e-02,  8.2968e-02, -8.2024e-02,  1.6164e-02,\n",
      "         3.7377e-02, -9.2349e-02, -1.1127e-01,  6.9750e-02,  8.6820e-02,\n",
      "        -2.7057e-02, -2.3069e-02, -7.3103e-02, -1.6484e-01, -2.0014e-02,\n",
      "         6.3153e-03,  7.7782e-02, -8.4823e-02,  2.2121e-02,  1.0625e-01,\n",
      "        -1.4292e-01,  8.1527e-02, -7.1087e-02, -8.0429e-02, -4.0732e-03,\n",
      "         6.4006e-02, -1.4278e-01, -7.9276e-03,  5.2838e-02, -3.7510e-03,\n",
      "        -5.9070e-02, -1.1084e-01, -1.6297e-03,  5.6736e-03, -7.3166e-02,\n",
      "        -6.8036e-02,  1.5117e-01,  1.9150e-02, -9.3975e-02, -4.8127e-02,\n",
      "         4.4899e-02,  5.5049e-02,  6.3477e-02,  5.0466e-02,  1.4346e-01,\n",
      "        -1.4061e-02,  1.8790e-01,  3.4009e-02,  1.4160e-03, -2.5282e-02,\n",
      "        -1.6245e-02,  5.4068e-02, -7.5012e-02, -7.5148e-02, -1.8582e-02,\n",
      "        -2.3466e-02,  1.9578e-02, -6.2413e-02,  1.2314e-01,  1.3701e-02,\n",
      "        -5.7122e-03,  8.9041e-02,  3.7946e-02,  4.1243e-02,  4.7171e-02,\n",
      "         2.7039e-02, -5.9925e-03, -2.8245e-02, -7.2878e-02,  1.4521e-02,\n",
      "         9.9702e-02,  6.4296e-02,  7.4185e-02, -7.1993e-02,  1.4546e-02,\n",
      "         7.7495e-02, -9.2409e-03, -3.8808e-02,  7.1566e-02, -1.4977e-01,\n",
      "         4.2293e-02, -4.2540e-02, -5.6876e-03, -4.4148e-02, -8.0183e-02,\n",
      "         7.5278e-02, -2.9656e-03, -4.9337e-02,  2.6277e-02, -1.1994e-02,\n",
      "        -9.6900e-03, -8.8157e-03, -1.7625e-02, -8.9690e-02, -3.2884e-02,\n",
      "        -5.1021e-03, -1.0199e-01, -1.6831e-02,  1.1726e-01, -3.4447e-02,\n",
      "        -2.8511e-02, -1.9198e-02,  3.6576e-03,  3.2099e-02,  4.5579e-03,\n",
      "         8.7041e-02, -3.0138e-02,  1.8212e-02,  7.4119e-02, -1.3839e-02,\n",
      "         5.3415e-02,  2.2786e-02,  1.0557e-01, -5.6927e-02,  3.3285e-02,\n",
      "         7.3276e-02,  1.0244e-01, -1.4565e-02, -1.0259e-01,  1.2200e-01,\n",
      "         6.1812e-02,  4.8889e-02, -5.6486e-02,  5.1047e-02,  9.3909e-02,\n",
      "        -1.0201e-02,  6.4712e-02, -2.3649e-02,  3.8729e-02,  6.1245e-03,\n",
      "        -4.3430e-02,  6.4039e-03, -8.9212e-02,  1.5119e-01,  7.2071e-02,\n",
      "         1.5732e-02, -2.2774e-02,  5.2327e-02,  2.5401e-02,  2.9843e-02,\n",
      "        -1.1558e-01,  5.9937e-02, -5.8328e-02,  7.1370e-02,  4.9816e-02,\n",
      "         6.5657e-02,  3.2430e-02, -8.6861e-03,  8.5977e-02,  1.9082e-02,\n",
      "         2.7206e-02, -1.9106e-03, -6.5907e-02,  4.0442e-03,  1.7387e-02,\n",
      "         1.3066e-01, -8.5428e-02, -2.6442e-02,  5.6974e-02, -8.7909e-02,\n",
      "         3.4048e-02, -5.8666e-02,  1.8037e-02, -6.2223e-02, -1.8848e-02,\n",
      "         9.5296e-03, -5.1592e-03,  5.1242e-03,  9.5190e-02,  1.1389e-02,\n",
      "        -6.1644e-02,  2.7198e-02,  2.2262e-02, -4.7755e-02,  6.3539e-03,\n",
      "        -2.4203e-02,  1.3476e-02,  5.5816e-02,  3.3884e-02,  5.4144e-02,\n",
      "        -2.0123e-02, -2.5729e-02,  3.2092e-02, -3.4289e-02, -1.2439e-03,\n",
      "         1.8775e-01,  5.8437e-02,  1.8716e-02, -5.8857e-02, -6.8036e-02,\n",
      "        -5.9856e-04,  1.0747e-01, -7.1370e-02,  1.3296e-03, -3.0167e-02,\n",
      "        -5.6810e-02, -1.0447e-01, -8.7226e-03, -3.1270e-03,  1.2601e-02,\n",
      "         1.8155e-02, -9.4597e-02, -4.7340e-02,  2.7440e-02, -3.4883e-02,\n",
      "        -3.2968e-02, -6.2905e-02, -1.2657e-02,  3.2411e-02,  1.2026e-02,\n",
      "         2.2878e-02, -5.3231e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 6.6658e-02, -7.8302e-02, -3.9761e-02, -4.1793e-02,  4.5831e-02,\n",
      "         4.8306e-02, -6.7736e-03,  7.5574e-02, -7.4495e-02, -3.0042e-02,\n",
      "         5.2244e-03, -1.3071e-02, -5.5794e-03, -8.3971e-02, -6.9471e-03,\n",
      "        -2.4258e-02,  1.0854e-01, -6.1369e-02, -1.4674e-01,  1.1226e-01,\n",
      "        -6.0065e-02,  5.3451e-02,  1.1262e-01, -4.9005e-03,  1.5264e-01,\n",
      "         7.8240e-02,  3.1867e-02,  7.0535e-03, -8.8613e-02, -1.6180e-02,\n",
      "         7.1920e-03,  3.6067e-02, -1.8580e-02, -6.9305e-02,  5.7444e-02,\n",
      "        -9.3223e-02,  6.4325e-02, -1.2735e-01, -1.6280e-02, -5.1730e-02,\n",
      "        -1.6762e-02,  1.6986e-01,  2.8526e-02,  7.5887e-02,  4.1897e-03,\n",
      "         5.6685e-02,  4.6633e-02, -3.6862e-02, -3.9126e-02, -2.2331e-02,\n",
      "         9.3762e-02, -1.0613e-02,  1.1766e-01, -3.7826e-02,  6.4190e-02,\n",
      "         2.1247e-02, -9.1414e-03,  9.0567e-02, -1.1170e-01,  1.5015e-02,\n",
      "        -1.6912e-02,  1.8269e-02, -6.4949e-02, -5.4902e-02, -8.6944e-03,\n",
      "         1.3896e-01,  1.1010e-01,  1.0749e-02,  8.7195e-02, -6.8369e-03,\n",
      "        -3.5939e-02,  1.3870e-02,  5.9698e-02, -8.9737e-05,  8.3753e-02,\n",
      "        -4.8358e-03, -3.8847e-02, -1.0107e-01,  7.5683e-02, -1.1180e-01,\n",
      "         3.0140e-02, -4.3089e-02, -2.2418e-02, -3.6128e-02, -1.0527e-01,\n",
      "         2.2898e-02,  4.6009e-02, -7.4225e-03, -5.6874e-02,  8.5350e-02,\n",
      "         5.1923e-03,  2.5627e-02, -8.9285e-03, -5.8058e-02,  7.0525e-02,\n",
      "         3.8854e-02,  2.7697e-02,  1.4393e-01, -4.0282e-02,  2.0928e-02,\n",
      "        -2.4592e-02,  6.1504e-02,  8.4973e-02, -6.5030e-03, -1.1406e-02,\n",
      "        -1.5721e-01, -1.2213e-01, -3.2998e-02, -1.0606e-02,  1.5931e-01,\n",
      "         1.4261e-01,  2.5770e-02, -4.0473e-02, -6.6654e-02,  3.4934e-02,\n",
      "         9.9253e-02, -1.0173e-02, -1.4505e-02,  6.1864e-02,  4.7759e-02,\n",
      "        -1.6578e-02,  3.0713e-02,  1.4806e-02,  8.6155e-02, -1.2338e-02,\n",
      "         7.9021e-02, -7.8331e-02, -6.0098e-02,  7.8730e-02,  2.3303e-02,\n",
      "        -8.3858e-03,  4.4462e-02, -5.4935e-02,  4.2922e-02,  4.7366e-02,\n",
      "        -3.2290e-04,  1.8469e-02, -5.9237e-02,  6.0935e-02,  2.3421e-02,\n",
      "         7.0576e-02, -1.8194e-02,  5.7329e-03,  1.2694e-01, -1.6639e-02,\n",
      "         5.9829e-02, -7.5157e-02, -6.8489e-02, -1.1888e-01, -1.4575e-01,\n",
      "        -6.2740e-03,  8.6623e-02, -1.9370e-03, -1.2883e-01,  4.0742e-02,\n",
      "        -3.1368e-02, -6.8863e-03,  6.7565e-03, -5.5464e-02, -5.8365e-02,\n",
      "        -4.6925e-02, -1.8427e-03, -6.9821e-03, -5.4991e-02,  1.4936e-02,\n",
      "        -6.0094e-02,  2.1199e-02,  1.6101e-03, -6.6419e-02, -1.0129e-01,\n",
      "         3.2519e-04, -9.6969e-02,  2.2424e-02,  8.3956e-02, -1.0915e-01,\n",
      "        -5.2411e-02,  7.9012e-02,  7.7652e-02,  7.2692e-02,  5.3036e-02,\n",
      "         8.0605e-03,  1.2090e-01,  4.4321e-02, -1.3145e-02,  2.7608e-02,\n",
      "        -2.4626e-03, -8.6162e-02, -2.0906e-02, -8.0314e-02,  8.6478e-02,\n",
      "         3.2060e-02, -7.4949e-02, -4.5875e-02, -9.1144e-02,  8.5149e-02,\n",
      "         4.7841e-02, -5.8479e-02,  9.3823e-02, -8.9949e-02, -2.2137e-03,\n",
      "         5.3320e-02,  2.4241e-02,  7.6287e-02, -7.3501e-02,  5.9457e-02,\n",
      "         2.5991e-02, -4.9862e-02,  2.1058e-02,  3.7085e-02,  5.8227e-02,\n",
      "         1.6736e-02,  1.3518e-02, -3.6454e-02,  8.9511e-02, -6.0161e-02,\n",
      "         4.3647e-02,  2.5404e-02,  1.6810e-03, -3.8325e-02,  5.1655e-02,\n",
      "        -6.2435e-03, -7.4342e-02,  1.5280e-02, -3.8896e-02, -4.6945e-02,\n",
      "        -4.9156e-02,  5.0480e-02, -1.1144e-01,  4.6365e-02,  4.1312e-02,\n",
      "         4.3370e-02, -6.4439e-02,  1.4321e-01,  5.6491e-03,  4.6217e-02,\n",
      "        -7.8084e-02,  2.2043e-02,  2.4072e-02, -1.1090e-01, -5.7180e-02,\n",
      "         1.3553e-01,  2.0576e-03, -6.7463e-02, -3.7952e-02,  9.7044e-02,\n",
      "         3.9006e-02,  2.3112e-02,  3.6162e-02, -4.4879e-02, -5.0205e-02,\n",
      "        -6.6276e-02,  6.0393e-02, -1.6587e-02, -4.2223e-02,  4.9360e-02,\n",
      "        -5.2514e-02,  5.3070e-02,  3.0898e-02,  8.4096e-03,  4.2029e-02,\n",
      "         8.3128e-03,  7.7944e-02,  7.4944e-02,  3.7365e-02, -1.7412e-02,\n",
      "        -1.7034e-02, -5.1705e-02, -1.0178e-01,  8.1377e-03, -1.1124e-02,\n",
      "         6.0315e-02, -1.2464e-01, -8.2909e-02, -2.0721e-02,  1.5134e-01,\n",
      "        -7.6029e-03, -5.5703e-02,  1.3161e-01,  1.1009e-01,  8.7843e-02,\n",
      "        -1.1565e-02, -7.0188e-02, -1.7204e-01,  9.7961e-02,  1.4806e-01,\n",
      "        -4.5438e-02, -2.6664e-03, -4.6997e-02, -7.0638e-02, -7.9939e-02,\n",
      "        -7.0988e-02, -1.1400e-01, -7.8130e-03, -8.5862e-02, -3.9800e-02,\n",
      "         7.1482e-03, -1.3455e-01, -2.8474e-02, -8.3467e-02,  6.1789e-02,\n",
      "        -1.2440e-02, -1.4384e-01, -5.4934e-02,  1.7171e-02, -4.3710e-02,\n",
      "         5.2462e-03, -9.8457e-02,  6.4931e-02,  3.0336e-02, -8.2045e-03,\n",
      "        -2.1457e-02,  1.9863e-02, -3.9212e-02,  3.6250e-02, -2.9250e-02,\n",
      "         4.0146e-03,  9.8803e-02, -3.5044e-03, -1.3867e-01,  6.7823e-02,\n",
      "        -1.1386e-02,  4.5815e-02, -4.6995e-02, -6.0331e-02,  8.9048e-02,\n",
      "        -3.3910e-03,  5.5142e-02,  1.0962e-01,  7.8482e-02, -5.7451e-02,\n",
      "         6.7650e-02, -5.0193e-02, -1.0531e-01,  3.0873e-02,  4.0250e-02,\n",
      "         3.5226e-02,  3.5651e-02, -1.3163e-02, -1.5697e-02, -1.3301e-02,\n",
      "        -7.5622e-02,  4.6634e-02, -6.0863e-02,  1.1601e-02,  5.8555e-02,\n",
      "         1.9718e-02,  1.4490e-02,  4.6890e-02,  1.9770e-02,  1.8599e-02,\n",
      "         1.5324e-02,  9.0858e-02, -9.4841e-02,  4.4712e-02,  1.0196e-01,\n",
      "         7.1711e-02,  2.8857e-02, -7.6147e-02,  1.1056e-01,  3.8540e-02,\n",
      "        -7.5464e-02, -1.1109e-01,  1.1038e-02,  7.1191e-02,  3.8999e-02,\n",
      "         8.1577e-02,  1.4265e-01, -2.5305e-02,  7.0406e-02, -2.0950e-01,\n",
      "        -1.0905e-01, -7.9404e-02,  9.4908e-02, -6.2777e-02, -4.6448e-02,\n",
      "         6.7760e-02, -4.1111e-02, -3.0499e-02, -6.7737e-02, -1.6252e-02,\n",
      "         7.7219e-02, -9.5822e-02,  7.5935e-03, -2.3492e-02, -3.9966e-02,\n",
      "         2.2348e-02, -5.5910e-02, -2.2430e-02, -1.2789e-01,  1.1506e-02,\n",
      "        -3.6499e-02, -2.3789e-02,  8.8967e-02,  3.7748e-04,  1.4302e-01,\n",
      "        -3.3631e-02, -3.5510e-02, -1.5043e-01,  7.7718e-02,  1.4879e-01,\n",
      "         6.6394e-02, -1.8917e-02,  1.0423e-02, -4.4962e-03, -2.3098e-02,\n",
      "         8.4583e-02,  1.2187e-01,  2.5955e-02,  2.3483e-02, -1.2860e-01,\n",
      "         2.7167e-02,  3.6408e-02,  8.3306e-02,  1.1587e-01,  6.6651e-02,\n",
      "         5.9024e-02,  1.0206e-01, -6.6102e-02, -1.1416e-02,  6.7382e-02,\n",
      "        -1.8530e-01,  7.1940e-02, -3.7391e-02, -1.0281e-01,  5.0257e-02,\n",
      "         4.7398e-02,  2.7898e-02,  6.5546e-02, -3.5585e-02, -1.5329e-02,\n",
      "        -3.8707e-02, -5.4844e-02, -2.3227e-02,  3.0108e-02, -2.5781e-02,\n",
      "        -2.8408e-02,  3.9738e-03,  9.0303e-02,  8.2566e-03,  2.2979e-02,\n",
      "        -5.5796e-02, -3.8515e-02, -6.0057e-02,  7.1408e-02, -6.8506e-02,\n",
      "        -8.3587e-02, -1.1510e-01,  3.3540e-02, -1.6315e-02, -4.7617e-02,\n",
      "        -1.2741e-01, -2.6345e-02, -6.0932e-02, -2.5297e-02,  1.7280e-03,\n",
      "        -5.4365e-02, -5.7350e-02, -4.4366e-02, -1.8187e-02, -5.9762e-02,\n",
      "         1.8093e-02, -6.1407e-02,  1.3368e-01,  3.7309e-02, -2.3302e-02,\n",
      "        -3.6866e-02,  6.9024e-03,  7.7365e-03,  4.0508e-02, -2.5169e-02,\n",
      "        -8.2504e-02,  1.2014e-01, -6.4195e-02,  6.6726e-02,  1.5957e-02,\n",
      "         1.0247e-01,  9.6323e-02,  5.0310e-02, -7.1386e-02, -6.2054e-03,\n",
      "        -1.6760e-01,  3.7466e-03, -9.4249e-02,  7.7653e-02, -1.2555e-01,\n",
      "        -6.1608e-02, -2.9333e-02,  1.3478e-02, -1.4650e-02, -9.3798e-02,\n",
      "         6.4758e-02,  2.1284e-02,  1.5329e-01, -8.6474e-02, -5.4156e-03,\n",
      "        -2.4129e-02,  1.0983e-01, -2.6136e-02,  1.7877e-02,  7.2377e-02,\n",
      "         2.4865e-02,  5.1694e-02,  5.9210e-02,  1.3274e-01, -4.0805e-02,\n",
      "         2.4143e-02,  6.7355e-02,  6.0903e-02,  6.5552e-02,  1.7681e-01,\n",
      "         4.1771e-02,  1.2728e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 5.0966e-02, -1.4745e-01,  7.7494e-02,  1.4598e-02,  1.1066e-01,\n",
      "        -3.6061e-02, -3.4456e-02,  2.3449e-03,  3.6120e-02, -2.1529e-02,\n",
      "         1.0209e-01,  1.2287e-03, -5.0131e-02,  6.2569e-02, -2.0442e-02,\n",
      "         3.2035e-02,  6.1605e-02, -9.9639e-02,  1.5433e-02,  3.8132e-02,\n",
      "        -6.6866e-02, -6.3091e-02, -6.1747e-02,  6.8062e-02,  8.8035e-02,\n",
      "        -1.0674e-01,  5.1352e-02, -3.5963e-02, -4.7417e-03, -4.0600e-03,\n",
      "        -1.0709e-01, -8.8151e-02,  1.0923e-01, -5.1789e-02, -1.1943e-01,\n",
      "        -3.2427e-02,  8.7168e-02,  1.1600e-01, -3.1433e-02,  2.1007e-02,\n",
      "        -2.0211e-02,  5.1138e-02, -3.1195e-02, -1.7929e-02,  1.6682e-02,\n",
      "        -5.8549e-03, -3.0055e-02, -1.2022e-01,  4.2940e-02,  5.0219e-03,\n",
      "        -7.6352e-02,  1.2055e-02,  1.1379e-02,  7.7296e-02, -3.7195e-02,\n",
      "         6.2380e-02, -9.9886e-02,  1.3775e-02, -3.7782e-02, -8.0343e-03,\n",
      "         1.1148e-02, -1.7144e-02, -8.2952e-02,  6.2111e-02,  1.4023e-02,\n",
      "         9.3064e-02, -1.8222e-02,  8.8978e-02, -9.5613e-02,  5.1005e-02,\n",
      "         6.4407e-02, -1.5327e-02, -1.6592e-02, -4.5361e-02, -3.1602e-02,\n",
      "        -4.6708e-02, -4.0381e-02,  9.3572e-02,  1.4583e-02,  1.5900e-02,\n",
      "         5.2908e-02, -6.2023e-02,  9.5726e-02, -2.2317e-02, -1.0207e-02,\n",
      "        -8.4064e-02, -8.5376e-02,  1.4583e-02,  6.5636e-02,  8.2487e-02,\n",
      "         6.9251e-02, -3.3851e-03,  2.0579e-02, -6.4329e-03, -6.3405e-03,\n",
      "         2.8375e-02, -5.4557e-02,  4.9721e-02, -2.8327e-02,  7.1326e-02,\n",
      "        -2.7338e-02,  7.1745e-02,  2.0902e-02, -1.4693e-02, -6.4021e-03,\n",
      "        -3.6755e-02,  2.3320e-02, -1.8848e-02, -8.2152e-03, -7.3774e-02,\n",
      "        -6.4569e-02, -3.3738e-02,  2.3054e-02, -1.0855e-02,  3.3617e-02,\n",
      "         5.3611e-02, -6.7952e-02, -5.8561e-02, -4.5781e-02,  2.4040e-02,\n",
      "        -8.8937e-02,  3.5465e-02,  5.0535e-02,  2.5044e-02, -4.3513e-03,\n",
      "        -3.2971e-02, -1.3832e-01, -8.0301e-02,  1.5525e-01, -8.0106e-02,\n",
      "         2.0949e-02,  1.1226e-02,  5.7637e-02,  9.5634e-02, -4.6271e-02,\n",
      "         6.2753e-02, -4.8439e-02,  5.5866e-02, -5.6149e-02,  8.9882e-03,\n",
      "        -2.2475e-02,  2.6102e-03, -7.5365e-02, -3.5781e-02,  8.7820e-03,\n",
      "        -2.7019e-02,  5.6331e-02,  1.6614e-03, -3.3956e-02, -6.9785e-02,\n",
      "         1.1633e-01,  5.9738e-02, -8.4658e-02,  3.5563e-02,  1.0341e-01,\n",
      "         7.0607e-05, -4.0593e-02,  3.8467e-02,  1.0799e-01,  1.7658e-02,\n",
      "        -9.0117e-02, -9.2431e-02, -7.4624e-02,  3.1521e-02,  4.0765e-02,\n",
      "        -1.2515e-01,  3.0535e-02,  1.1851e-02, -4.0310e-02,  2.2916e-02,\n",
      "         1.2250e-01,  6.9152e-02, -6.2053e-03,  4.0321e-02,  1.6208e-02,\n",
      "        -6.8822e-02,  2.1849e-02, -3.6987e-02, -4.4603e-02, -1.5947e-01,\n",
      "        -1.6658e-02, -9.6214e-02, -3.7753e-02,  5.4041e-02, -1.7003e-02,\n",
      "         8.1025e-02,  2.4926e-02,  5.5767e-02, -7.9529e-02, -2.1234e-01,\n",
      "        -4.7282e-02, -5.5761e-02,  3.0091e-02,  1.4731e-01, -6.2581e-02,\n",
      "         2.2454e-02, -6.7485e-02,  1.5281e-01,  4.6557e-02,  8.2848e-02,\n",
      "        -9.2783e-03,  7.2040e-02, -9.9636e-02,  6.1564e-02, -5.9368e-02,\n",
      "        -1.9590e-02, -1.0435e-02, -4.1890e-02, -4.7181e-02, -1.2446e-02,\n",
      "        -4.0818e-02,  6.1132e-02, -8.5487e-03,  8.7448e-02,  2.1625e-02,\n",
      "        -1.7572e-02, -9.9109e-02,  3.0057e-02,  7.2901e-02, -1.2618e-02,\n",
      "         3.7349e-02, -2.1917e-02, -6.9758e-02, -1.2695e-03, -1.3122e-02,\n",
      "        -5.0221e-02,  2.3869e-02,  5.0954e-02,  7.0282e-04, -3.3970e-02,\n",
      "        -2.8963e-02, -8.4868e-02, -2.6569e-02, -6.5083e-02,  8.5820e-03,\n",
      "        -4.4336e-03,  5.8201e-03,  2.1587e-02,  7.3191e-03,  4.7043e-03,\n",
      "        -5.8309e-02,  2.1552e-02, -2.5648e-02, -2.2331e-02, -1.0112e-01,\n",
      "        -3.7041e-02, -4.1032e-02, -6.8042e-02,  1.7894e-02, -2.6997e-02,\n",
      "        -2.7584e-02,  1.7612e-02, -1.9444e-03,  5.9923e-02,  6.8182e-02,\n",
      "         2.6522e-02, -6.7600e-02,  3.6002e-02, -1.6933e-02,  9.7652e-03,\n",
      "        -1.0266e-01, -3.6495e-03,  1.1981e-01, -3.1746e-02, -2.1659e-02,\n",
      "        -4.1714e-02,  7.0952e-02, -8.4005e-02,  3.2536e-03, -2.2566e-02,\n",
      "        -3.9273e-02,  3.3117e-03, -8.4515e-02,  5.7761e-02,  9.1372e-02,\n",
      "         9.6171e-03, -1.2380e-01, -8.3872e-04, -1.1604e-02, -2.1467e-02,\n",
      "         3.9992e-02,  8.3243e-04, -5.9930e-03, -2.2868e-02,  2.3452e-02,\n",
      "         1.2934e-02,  1.4610e-01,  6.3666e-04, -4.7834e-02, -1.6290e-02,\n",
      "         6.7797e-02,  3.1905e-02, -6.1453e-02,  4.7708e-02,  4.9836e-02,\n",
      "        -3.2332e-02,  1.4693e-02, -8.0379e-02,  5.6533e-02,  6.9687e-02,\n",
      "         6.2967e-02, -3.5479e-02, -9.2222e-03, -6.3729e-03,  8.0024e-02,\n",
      "         1.0684e-02,  5.5488e-02, -5.7777e-03,  1.2793e-01,  2.4388e-02,\n",
      "         6.8428e-02, -2.1748e-03, -4.4633e-02,  1.3514e-02,  2.4887e-03,\n",
      "        -1.9060e-02, -1.2467e-01, -4.7357e-02, -4.9894e-02,  9.8269e-02,\n",
      "        -6.8453e-03,  3.6830e-02, -3.3399e-02, -4.3410e-02, -9.6036e-02,\n",
      "         8.1545e-02, -3.5613e-02,  6.0910e-02, -5.0575e-02,  6.5858e-03,\n",
      "         5.8657e-02,  2.9649e-02, -5.0301e-02, -1.8220e-02, -7.9198e-02,\n",
      "         4.7839e-02,  3.2613e-02, -9.3417e-02,  6.7337e-02, -8.7942e-03,\n",
      "        -1.6459e-02,  2.7349e-02, -4.9454e-02,  6.1516e-02,  6.7670e-02,\n",
      "         4.5408e-03,  3.2664e-02,  3.3849e-02, -8.3817e-03,  2.9799e-02,\n",
      "        -6.4481e-02,  6.9932e-02,  1.3802e-02, -7.4295e-02,  2.8266e-03,\n",
      "         1.3482e-01,  1.6569e-02, -4.2818e-02,  5.2147e-02,  4.8331e-02,\n",
      "        -2.2739e-02, -1.8746e-02,  2.8624e-02, -8.2209e-02, -4.9650e-02,\n",
      "        -2.9904e-02, -3.1530e-02, -4.7788e-02, -4.7805e-02,  4.2077e-02,\n",
      "        -5.1374e-03,  9.3389e-02,  7.7671e-02, -1.0206e-02, -5.3528e-02,\n",
      "        -6.0535e-03,  2.0553e-02,  2.7381e-02,  8.1292e-03, -6.6471e-02,\n",
      "        -1.9595e-02,  2.1768e-02,  4.5958e-02,  5.7396e-02,  1.7548e-02,\n",
      "        -6.3863e-03, -1.7971e-01,  2.8201e-02,  1.6888e-02, -6.0088e-02,\n",
      "        -4.4732e-02,  5.1204e-04,  5.4047e-02,  1.5042e-02,  8.6862e-02,\n",
      "        -5.6149e-02, -8.0252e-02, -1.7712e-02, -3.3251e-02,  6.7082e-02,\n",
      "         5.7277e-02,  7.4467e-02,  1.3210e-02,  8.0749e-02, -4.9230e-02,\n",
      "         4.0126e-02,  6.4328e-02,  3.2686e-02,  5.5669e-02, -4.5429e-02,\n",
      "        -6.0456e-02,  5.9471e-03, -7.2037e-03, -6.6578e-02,  6.4264e-02,\n",
      "        -3.4567e-02,  1.8057e-01,  9.6095e-02,  1.7282e-02, -5.5573e-03,\n",
      "        -1.5813e-02,  7.3891e-02, -9.6589e-03, -5.6928e-02,  3.5197e-02,\n",
      "        -3.6848e-02,  3.3619e-02, -7.9201e-02, -1.0853e-03, -6.1366e-02,\n",
      "        -4.6373e-02, -2.3210e-02,  2.4530e-02, -2.9117e-02, -2.6862e-02,\n",
      "         2.0443e-02, -1.0311e-02, -4.5818e-02,  3.2928e-02, -1.4177e-01,\n",
      "        -3.3394e-02, -8.0657e-02, -1.1610e-01,  2.7471e-03, -1.1582e-02,\n",
      "         1.8751e-03, -3.5150e-02,  9.0628e-02, -1.1234e-02, -6.3072e-03,\n",
      "        -2.9522e-03, -2.5991e-02,  7.4267e-02,  5.3881e-02, -4.0242e-03,\n",
      "         7.6560e-03,  8.1244e-02, -1.5535e-02, -7.0901e-02,  4.0996e-03,\n",
      "        -1.9212e-02,  1.5392e-02, -4.2169e-02,  1.7310e-02, -7.4863e-02,\n",
      "        -5.8399e-02, -4.7026e-02,  1.1410e-01, -1.0140e-01, -9.5707e-02,\n",
      "         2.0097e-02, -1.0625e-01,  6.2864e-02, -1.0046e-01,  4.0808e-02,\n",
      "        -5.9520e-02, -5.2804e-02,  1.8317e-02, -1.1327e-01, -1.7123e-02,\n",
      "        -2.9642e-03, -1.2108e-02,  4.3250e-02, -6.8001e-02,  2.8993e-02,\n",
      "         2.3379e-03,  6.4308e-03, -5.0257e-02, -2.6099e-02, -9.2139e-03,\n",
      "         1.4326e-01, -3.5042e-02, -5.5747e-03,  1.4443e-01,  6.4646e-02,\n",
      "        -3.6846e-02, -3.1642e-02,  1.8773e-04, -6.0860e-02,  7.3784e-02,\n",
      "         3.4365e-02, -5.6993e-02,  4.9817e-02, -4.8040e-02,  7.2079e-02,\n",
      "         6.0582e-02,  1.5344e-03, -6.8195e-02,  2.4479e-02, -6.7752e-02,\n",
      "        -7.2611e-02, -2.7682e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0768, -0.0110,  0.0261, -0.0717,  0.0138, -0.0634, -0.0912,  0.0113,\n",
      "        -0.0347, -0.0304, -0.0077, -0.0341, -0.0804, -0.0470, -0.0264,  0.0091,\n",
      "         0.0322,  0.0482, -0.0405, -0.0913,  0.0352, -0.0308,  0.0159,  0.0034,\n",
      "         0.0155, -0.0147,  0.0697,  0.0984,  0.0066,  0.0651, -0.1385, -0.0525,\n",
      "        -0.0866,  0.0596, -0.0648,  0.0693,  0.0717,  0.0327, -0.0749,  0.1113,\n",
      "         0.0407,  0.0465,  0.1108,  0.0816, -0.0240,  0.0117,  0.0365, -0.0328,\n",
      "         0.0209, -0.0589,  0.0395, -0.0040,  0.0484,  0.0579,  0.0430,  0.0961,\n",
      "         0.0019, -0.0478, -0.0156,  0.0328, -0.0624,  0.0715,  0.0612, -0.0883,\n",
      "         0.0393, -0.0688, -0.0231, -0.0230, -0.0219,  0.0156, -0.0243, -0.1010,\n",
      "        -0.0313,  0.0016, -0.0020, -0.0170, -0.0236, -0.0161, -0.0517, -0.0867,\n",
      "        -0.0712, -0.0125, -0.0954, -0.0109,  0.1592,  0.0375, -0.0574,  0.0412,\n",
      "        -0.0757,  0.1175,  0.0951, -0.0161, -0.0222, -0.1225,  0.0901,  0.0392,\n",
      "        -0.0461, -0.0242,  0.0155, -0.0975, -0.0425, -0.0112,  0.0040,  0.0077,\n",
      "         0.0669, -0.0678, -0.0185, -0.0830, -0.0124,  0.0362, -0.0285,  0.1085,\n",
      "        -0.0133,  0.0715, -0.0329, -0.0025,  0.0326, -0.0271,  0.0487, -0.0552,\n",
      "        -0.0141,  0.0521, -0.0023, -0.0375, -0.1438,  0.0137,  0.0634, -0.0483,\n",
      "        -0.0128,  0.0103,  0.0111,  0.0511,  0.1563,  0.0164,  0.0060, -0.1368,\n",
      "        -0.1142, -0.0285, -0.0205,  0.0208,  0.0782,  0.0446,  0.0960, -0.0340,\n",
      "        -0.0171,  0.0837,  0.1210,  0.0210, -0.0156, -0.0047,  0.0567,  0.1111,\n",
      "        -0.0234, -0.0498, -0.0705, -0.0082,  0.1107,  0.0074,  0.0705, -0.0538,\n",
      "         0.0613, -0.1379,  0.0155, -0.0276,  0.0236, -0.0070, -0.0942, -0.0741,\n",
      "         0.0344,  0.0320, -0.0537, -0.1111, -0.0324,  0.1613,  0.0198,  0.1086,\n",
      "        -0.0317,  0.0004, -0.0473,  0.0628,  0.0596, -0.0103, -0.0568,  0.0624,\n",
      "        -0.0776, -0.1148, -0.0166,  0.0027,  0.0078, -0.0937, -0.0514, -0.0138,\n",
      "        -0.1482, -0.0669, -0.0712,  0.0135,  0.1173, -0.0033, -0.0064, -0.0263,\n",
      "        -0.0567,  0.0106,  0.0777, -0.0619, -0.0526,  0.0932, -0.0841, -0.0340,\n",
      "        -0.1270,  0.0130,  0.0067, -0.0860,  0.1337, -0.0305, -0.0314, -0.0653,\n",
      "         0.1493, -0.0126, -0.0196, -0.0949, -0.0565,  0.0440, -0.0889,  0.0118,\n",
      "        -0.0558, -0.0214, -0.0157, -0.0387, -0.0158,  0.0084, -0.0396, -0.0521,\n",
      "        -0.0809,  0.0183,  0.0045,  0.0053, -0.0093, -0.0678, -0.1156,  0.0174,\n",
      "         0.1187,  0.0416,  0.0693, -0.0025,  0.0486,  0.0294, -0.0075, -0.0575,\n",
      "         0.1809,  0.0164,  0.0446, -0.0271, -0.0230,  0.0786, -0.0114, -0.0058,\n",
      "         0.0358, -0.0731, -0.0365, -0.0286,  0.1120, -0.0882,  0.0127,  0.0710,\n",
      "         0.0003,  0.0062, -0.0400,  0.0463,  0.0816,  0.0720,  0.0084,  0.0478,\n",
      "         0.0634,  0.0475,  0.0025, -0.0680, -0.0101,  0.0497,  0.0274,  0.0548,\n",
      "         0.0372, -0.0325,  0.1441,  0.0648,  0.0218,  0.0187,  0.0017,  0.0058,\n",
      "         0.0606,  0.0349, -0.0842, -0.0129,  0.1517, -0.0832, -0.0344,  0.0722,\n",
      "         0.0201, -0.0085,  0.0686, -0.0399, -0.1319,  0.0208, -0.0094, -0.0035,\n",
      "         0.0502,  0.0415,  0.0268,  0.0031, -0.0782, -0.0470,  0.0647, -0.0245,\n",
      "        -0.0220,  0.0053, -0.0115,  0.0109,  0.0431,  0.0079, -0.0562, -0.0070,\n",
      "         0.0463, -0.0588,  0.0339,  0.0052, -0.0210,  0.1090,  0.0647, -0.0540,\n",
      "         0.0085,  0.0879, -0.0313,  0.0073,  0.0437,  0.0494,  0.0060,  0.1026,\n",
      "         0.0076,  0.0393, -0.0335, -0.0069, -0.1043,  0.0803, -0.0891,  0.1589,\n",
      "        -0.0709, -0.0418, -0.0459, -0.0026,  0.1630, -0.0228,  0.0362,  0.0665,\n",
      "         0.0199,  0.0311, -0.0793,  0.0584, -0.0846, -0.0298,  0.0471,  0.1816,\n",
      "         0.1290, -0.0308, -0.0354,  0.0684,  0.0022,  0.1397,  0.1273, -0.0121,\n",
      "        -0.0255,  0.1549, -0.1043,  0.0030, -0.0070, -0.0533, -0.1327, -0.0505,\n",
      "        -0.0394, -0.0871, -0.1559, -0.1013, -0.0389,  0.0533, -0.0024,  0.0499,\n",
      "         0.0578, -0.0086, -0.0890, -0.0100,  0.0792, -0.0145, -0.0229, -0.0173,\n",
      "        -0.0718,  0.0246, -0.0108, -0.0746, -0.1079, -0.1119, -0.0225,  0.0620,\n",
      "        -0.0441,  0.0702,  0.1055, -0.0187,  0.0807,  0.0159,  0.0401,  0.0435,\n",
      "        -0.0720, -0.1575, -0.0476, -0.0490, -0.0268,  0.1036,  0.0390,  0.0015,\n",
      "        -0.1407, -0.0818, -0.0521, -0.0193,  0.0634,  0.0762, -0.0572,  0.0335,\n",
      "        -0.0147,  0.0902, -0.0812,  0.0083, -0.1243, -0.0758,  0.1391,  0.0418,\n",
      "         0.0337, -0.0012,  0.0702, -0.0611,  0.0674,  0.0109,  0.0365, -0.0833,\n",
      "        -0.0679, -0.0756,  0.0385, -0.0285,  0.0510, -0.0359,  0.0606,  0.0541,\n",
      "         0.0934, -0.0538, -0.0293,  0.0203, -0.0051,  0.1183, -0.0098,  0.0472,\n",
      "         0.0742, -0.0267, -0.0643, -0.0058,  0.0205,  0.0397, -0.0012,  0.0355,\n",
      "         0.0729,  0.0082,  0.0999,  0.0031,  0.0537,  0.0390,  0.0033,  0.0092,\n",
      "         0.0299, -0.0649,  0.0372,  0.0805,  0.0463, -0.0983, -0.0180, -0.0175,\n",
      "         0.0584, -0.0766,  0.0062, -0.0004,  0.0233, -0.0832,  0.0306,  0.0634,\n",
      "         0.0414, -0.0457,  0.0292, -0.0461,  0.0299,  0.0362,  0.0514,  0.0055,\n",
      "        -0.0551, -0.0026, -0.0381, -0.0229, -0.0396, -0.0021,  0.1161, -0.0633,\n",
      "         0.0352, -0.0886,  0.1244, -0.0195,  0.0971,  0.0900, -0.1717, -0.0553],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-2.8312e-02,  4.9911e-02,  9.7769e-03, -1.7147e-02,  4.0901e-02,\n",
      "        -1.2317e-01, -1.1881e-01,  8.5501e-02,  1.1018e-01,  6.2696e-02,\n",
      "         3.1070e-02, -1.0946e-01,  7.7663e-02,  6.7539e-02, -1.3375e-04,\n",
      "        -1.2912e-02,  5.7624e-02, -7.1261e-02,  9.6846e-04, -4.5915e-03,\n",
      "         6.0058e-02,  2.9872e-02,  4.2197e-02,  3.8850e-02,  5.4885e-02,\n",
      "         4.4528e-02, -8.8942e-02,  1.1722e-01, -4.4009e-02,  3.8589e-02,\n",
      "        -7.9293e-02, -1.1473e-02, -2.3653e-02, -4.3948e-02, -2.1827e-02,\n",
      "        -4.3308e-04,  8.2051e-02,  6.2999e-02,  3.0414e-02,  1.3454e-02,\n",
      "         5.9846e-03,  1.5785e-02, -6.2734e-02,  7.9752e-02, -1.4402e-01,\n",
      "        -5.4157e-02,  8.3404e-02, -5.4182e-02, -3.7938e-02,  1.9626e-03,\n",
      "         6.2376e-02, -9.8665e-02,  1.1238e-01,  8.4942e-02, -5.1376e-02,\n",
      "        -4.4197e-03,  1.0537e-02,  7.6728e-02,  7.0679e-02,  7.5002e-02,\n",
      "         2.3206e-02,  2.2686e-02,  3.7321e-02,  3.3898e-02, -2.2739e-02,\n",
      "        -1.1890e-01,  7.7856e-02,  1.0845e-01,  6.1648e-02, -2.4917e-02,\n",
      "        -5.6272e-02, -2.0143e-04, -6.7984e-02, -5.5723e-02,  1.5601e-03,\n",
      "         9.5723e-02, -1.2334e-01,  2.3138e-02,  1.5915e-03,  1.7391e-02,\n",
      "         1.0060e-03, -5.5752e-02, -7.3283e-03,  7.8786e-02, -8.5108e-02,\n",
      "         5.5049e-02,  1.5016e-01, -3.1859e-02,  4.4934e-03, -5.7109e-02,\n",
      "         8.0624e-03,  1.0309e-01, -3.0260e-03, -1.8075e-02,  1.0297e-01,\n",
      "         1.8190e-02,  8.1257e-02, -1.0586e-01,  4.6859e-02,  8.7545e-03,\n",
      "        -1.8347e-02,  7.8826e-04,  3.4076e-02,  3.4202e-02, -4.6036e-02,\n",
      "         7.8401e-02,  1.2534e-02, -2.9604e-02, -1.4013e-01, -1.2220e-01,\n",
      "        -3.9575e-02,  4.2375e-02,  6.8481e-02, -1.1031e-01,  1.7292e-03,\n",
      "         5.6505e-03, -1.3347e-01,  5.8967e-02,  1.0500e-01,  2.8959e-02,\n",
      "        -1.3579e-01, -3.6767e-02, -6.5603e-03,  5.9650e-02,  3.4714e-02,\n",
      "         3.4603e-02,  6.3472e-02,  8.8572e-02, -3.0379e-02,  1.2246e-02,\n",
      "         3.0892e-02, -1.9900e-02, -2.0532e-02, -9.3364e-02,  2.0879e-02,\n",
      "        -3.1082e-02,  7.4723e-02,  3.4827e-02,  9.9355e-03,  4.0432e-02,\n",
      "         9.0674e-02, -6.2378e-02, -1.7440e-02,  1.5880e-02, -1.3521e-02,\n",
      "         6.1648e-02, -2.5270e-02, -1.0506e-02,  1.8069e-02, -5.2453e-02,\n",
      "         1.3252e-02,  6.9504e-03, -5.8516e-02,  4.6623e-02,  1.4739e-02,\n",
      "         6.7765e-03,  3.7023e-03,  3.7319e-02,  1.9224e-02,  2.6738e-02,\n",
      "         8.2818e-02, -1.2007e-04,  7.7645e-02,  9.2141e-03,  4.3738e-03,\n",
      "        -1.0779e-01,  8.4956e-02,  3.7886e-02, -1.3384e-01, -1.1208e-01,\n",
      "        -5.7828e-02, -9.7238e-02,  1.0206e-02,  6.5645e-03, -2.8718e-02,\n",
      "         1.5325e-02,  6.6613e-02,  2.6445e-02, -2.4962e-02, -4.9788e-02,\n",
      "        -4.3545e-03, -4.5150e-02, -1.4951e-02,  6.1688e-02, -9.0608e-03,\n",
      "        -8.5805e-02, -1.0172e-01, -9.2241e-02, -1.5714e-03, -2.6098e-02,\n",
      "        -2.3720e-02, -4.2816e-03, -4.2465e-02,  4.0990e-03,  5.9952e-02,\n",
      "        -8.0171e-02,  3.4743e-02, -5.9418e-02, -5.0707e-04, -1.7003e-02,\n",
      "        -3.6289e-02,  9.0298e-02, -2.5486e-02,  2.2962e-02,  8.9927e-03,\n",
      "         3.8505e-02,  5.5345e-02, -2.0447e-02, -3.3111e-02,  3.7436e-02,\n",
      "         6.5773e-02, -4.5183e-02,  4.1996e-02, -8.7999e-02, -1.1769e-02,\n",
      "        -4.3234e-02, -6.6346e-02, -3.5659e-02, -5.7530e-03,  3.8261e-02,\n",
      "         6.5813e-02, -2.6030e-02, -7.3186e-03, -6.0748e-02, -5.1565e-02,\n",
      "        -2.2371e-02,  1.2256e-02,  7.5072e-02,  1.9970e-02,  2.4642e-02,\n",
      "        -7.0200e-02,  3.6686e-02,  2.4515e-02,  3.2946e-03,  6.7995e-03,\n",
      "         8.7247e-02, -6.1754e-02,  2.3224e-02,  4.8788e-02, -3.7919e-02,\n",
      "        -4.5916e-02, -6.3038e-03, -6.4867e-02,  9.7451e-03, -2.9809e-02,\n",
      "         1.9220e-02,  4.9873e-02, -8.4751e-02, -3.8756e-02,  2.4613e-03,\n",
      "         1.2979e-02, -1.9546e-02, -1.7456e-03,  6.0348e-02,  3.5478e-02,\n",
      "         8.5359e-02,  4.5793e-02, -2.9652e-02, -1.9533e-02,  2.8801e-02,\n",
      "         2.0128e-02, -1.6773e-02, -2.2567e-02,  8.6599e-02,  7.6258e-02,\n",
      "        -1.3919e-02, -5.2701e-03,  1.5254e-02, -5.6596e-03,  1.2512e-02,\n",
      "        -1.1107e-01, -3.9220e-02, -4.3274e-02, -1.4759e-02,  6.3456e-02,\n",
      "        -3.9313e-02,  6.6304e-02, -2.5031e-02, -8.0906e-02, -9.2574e-02,\n",
      "         7.7114e-03, -3.8525e-02,  2.6354e-02,  6.7656e-02, -3.6397e-02,\n",
      "        -6.6598e-02,  4.9100e-02, -4.5302e-02, -9.6687e-02,  3.2252e-03,\n",
      "        -1.6827e-02,  9.3235e-02, -2.9695e-02,  8.8593e-02,  1.0684e-01,\n",
      "         1.0159e-01,  7.8147e-02, -2.3984e-02,  7.4527e-02,  9.7435e-02,\n",
      "         9.9969e-02,  4.1802e-02,  5.5769e-02,  4.1883e-02,  3.7363e-02,\n",
      "        -1.2641e-02,  3.1162e-02, -5.7425e-04,  5.6984e-02,  2.1873e-03,\n",
      "         3.2089e-02, -7.0392e-02,  2.0635e-02,  9.4762e-03, -1.5822e-02,\n",
      "         5.4450e-02, -2.8916e-02,  1.6877e-02, -7.8206e-03, -1.1922e-01,\n",
      "         2.3058e-02,  6.5806e-02,  9.5983e-03,  4.4597e-02,  1.8453e-02,\n",
      "         4.3058e-02,  6.1493e-02, -6.8039e-02, -3.5424e-02, -3.8730e-02,\n",
      "        -4.6403e-02,  2.2619e-03,  1.3438e-02,  3.6322e-02, -9.0361e-02,\n",
      "         2.3885e-02, -6.8223e-02, -2.8933e-02,  1.0164e-01,  1.5505e-02,\n",
      "        -7.0034e-02,  7.1678e-02, -6.8170e-02,  4.8597e-02,  8.5489e-02,\n",
      "         3.4030e-02, -1.1827e-02,  4.7249e-02, -5.7491e-02,  6.4812e-02,\n",
      "        -3.8081e-02,  3.1269e-02,  4.8112e-02, -2.2889e-02, -1.2078e-01,\n",
      "         8.6875e-03,  2.7524e-03, -5.2020e-02, -1.3657e-02, -3.4252e-02,\n",
      "         1.2507e-01,  6.4650e-02, -4.3744e-02,  2.1554e-02,  7.2027e-02,\n",
      "         4.6084e-02,  1.0100e-01,  7.4042e-02, -5.4211e-02, -1.1455e-01,\n",
      "         5.7521e-02, -4.2710e-02, -7.8814e-02, -1.8124e-02,  4.4737e-02,\n",
      "        -5.1269e-02, -6.7855e-02, -8.3722e-02, -6.4286e-02,  3.4506e-02,\n",
      "         8.8117e-02,  4.1227e-02, -1.0366e-01, -5.4640e-02, -3.3339e-03,\n",
      "         1.3867e-01, -5.8631e-02,  1.0841e-02, -9.4331e-02,  1.0992e-01,\n",
      "        -1.8052e-02,  5.6607e-02, -3.0553e-03, -9.7665e-02,  3.6189e-03,\n",
      "         3.8424e-02, -2.0226e-02, -1.0399e-01,  7.1986e-02, -8.7396e-02,\n",
      "        -2.1321e-02, -3.3681e-02, -4.8806e-02, -9.9724e-03,  3.4821e-02,\n",
      "        -3.6701e-02, -1.0064e-01, -4.4952e-02, -2.9649e-02,  6.7568e-02,\n",
      "         1.0062e-01,  1.5413e-02, -5.2982e-03, -8.1491e-02,  6.9497e-02,\n",
      "         7.5970e-03,  2.6650e-02, -7.8061e-02,  8.9628e-02,  5.9069e-02,\n",
      "        -2.8076e-03,  2.2840e-02,  4.9031e-02, -3.0829e-02, -1.4460e-01,\n",
      "         2.0347e-02,  3.0446e-02,  4.5471e-02,  8.5173e-02, -1.1764e-02,\n",
      "        -1.9823e-02, -1.1526e-02, -1.4037e-02, -5.7210e-03,  3.2612e-02,\n",
      "         8.8098e-02,  2.5476e-02,  5.3235e-02,  9.3301e-02,  6.9620e-02,\n",
      "        -6.3628e-02,  6.8000e-02,  1.4908e-01, -5.6959e-02,  5.9116e-02,\n",
      "         2.2112e-02, -2.4973e-02, -2.7610e-02,  4.1903e-02, -2.0115e-02,\n",
      "         5.7806e-02,  1.3158e-03, -8.3065e-02,  4.6314e-02, -9.3857e-02,\n",
      "        -9.9200e-03,  4.4497e-02, -1.1722e-02, -6.1344e-02, -1.3309e-01,\n",
      "         4.0768e-02, -2.1628e-02, -5.0834e-02,  1.0866e-01,  1.6634e-02,\n",
      "         7.5386e-02,  1.1037e-01, -3.8678e-02,  5.1629e-02,  3.5886e-02,\n",
      "         3.2558e-02,  1.4227e-03,  5.5960e-02,  1.0197e-03, -5.6617e-02,\n",
      "         2.2816e-02, -1.3664e-01,  1.3298e-01, -3.5689e-02,  1.8169e-02,\n",
      "        -3.9363e-02, -4.9693e-02,  8.3050e-02, -1.3196e-02, -4.6567e-02,\n",
      "         3.9041e-02,  2.8396e-02, -2.6041e-02,  6.8008e-02, -1.0233e-01,\n",
      "        -1.5822e-02, -3.0579e-02, -4.8071e-02, -6.4514e-02,  1.8201e-02,\n",
      "        -4.3278e-02, -4.3680e-03, -8.4785e-02, -5.5908e-02, -6.7275e-02,\n",
      "         8.3114e-02,  1.3823e-02,  4.9019e-02,  4.0267e-02, -5.4514e-02,\n",
      "         4.9135e-02, -4.8312e-02, -2.4285e-02, -9.7027e-02,  2.4834e-02,\n",
      "         1.4886e-02,  6.9949e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 3.0289e-02,  3.1503e-02,  3.9986e-02,  1.3083e-01, -5.3132e-02,\n",
      "         2.9113e-02,  4.7187e-03,  5.0454e-02,  1.0700e-01, -2.2314e-02,\n",
      "         2.6524e-02, -1.1840e-02,  5.0855e-03,  7.3779e-04, -1.1865e-03,\n",
      "        -4.7954e-02,  1.0474e-02,  2.8582e-02, -7.9896e-02,  7.6038e-02,\n",
      "         4.5977e-02, -1.4148e-02,  3.9841e-02,  1.8766e-02,  8.0392e-02,\n",
      "         2.6746e-02,  2.9566e-02, -2.5976e-02,  1.6478e-02, -5.0035e-02,\n",
      "         2.4266e-02,  4.7684e-03, -4.6095e-02,  5.4383e-02, -5.5842e-02,\n",
      "        -6.3235e-02,  1.0002e-01, -7.9192e-03,  4.9059e-02, -2.9653e-02,\n",
      "         7.4298e-02,  3.2793e-02,  8.6242e-02,  1.3700e-03,  1.4234e-02,\n",
      "         7.6310e-02,  3.2565e-02, -5.5205e-02, -2.8722e-02, -3.9794e-02,\n",
      "         8.0323e-02, -1.0903e-01, -4.8134e-04,  4.3818e-02, -3.0959e-02,\n",
      "        -5.7084e-02,  4.3061e-02,  4.2138e-02,  7.2363e-02,  4.3792e-02,\n",
      "        -7.2850e-02,  5.2529e-03,  4.6195e-03, -6.2514e-02,  8.1972e-02,\n",
      "        -1.2628e-02,  1.1640e-01, -7.5081e-02,  2.6473e-02, -6.2586e-02,\n",
      "        -6.8327e-02,  5.4805e-03, -8.0045e-02, -1.0655e-02, -7.7074e-03,\n",
      "        -8.1215e-02, -1.6442e-02,  6.8840e-03, -6.9273e-03, -4.1731e-02,\n",
      "        -6.2782e-02,  6.2828e-02, -8.7719e-02,  1.7283e-02, -5.3315e-02,\n",
      "        -9.8364e-02, -9.7457e-02,  8.1505e-02,  2.6662e-02,  5.2712e-02,\n",
      "         5.1618e-02, -3.9540e-02, -1.0101e-01, -2.3273e-02,  1.6070e-02,\n",
      "        -3.2476e-02, -3.7883e-02, -1.9677e-02, -3.3466e-02,  1.7523e-02,\n",
      "        -9.1086e-02, -4.3556e-02,  7.8876e-02, -4.1143e-02, -3.5400e-02,\n",
      "        -1.7865e-02,  1.7630e-01,  1.3965e-01, -5.0848e-02, -3.6669e-02,\n",
      "         2.1116e-02, -1.0324e-01, -1.7145e-02,  6.3624e-02, -7.2753e-02,\n",
      "         8.1110e-04,  7.7122e-02,  6.0167e-02,  9.4302e-02,  3.3645e-02,\n",
      "         5.1997e-02,  9.3938e-03,  1.5380e-02,  3.0624e-02,  1.8364e-02,\n",
      "         9.4459e-02, -5.3204e-02,  5.3909e-02,  8.4368e-02, -2.6575e-02,\n",
      "         5.8741e-03,  1.7135e-01,  3.8734e-02,  1.1533e-01, -3.4991e-02,\n",
      "        -1.3902e-01, -5.0564e-02,  2.5342e-02,  1.9510e-03, -4.5458e-02,\n",
      "        -7.6664e-02,  1.0237e-01,  7.7267e-03,  5.8986e-02, -1.9288e-02,\n",
      "         5.3286e-02,  3.6359e-02,  8.0501e-02, -8.3045e-02,  3.3307e-02,\n",
      "         1.5659e-03,  9.6013e-03, -1.5590e-02, -5.1359e-02, -7.0246e-02,\n",
      "        -1.1975e-02,  2.6491e-02, -3.2005e-02,  6.8249e-02,  4.7669e-02,\n",
      "         4.7641e-02, -2.1512e-02, -6.3295e-02, -4.1788e-02, -1.5279e-02,\n",
      "        -9.7037e-02,  2.2685e-02,  2.0949e-02,  3.3309e-02,  9.4829e-03,\n",
      "         5.6710e-02, -7.6783e-03, -1.3969e-01, -4.1760e-02,  8.8335e-03,\n",
      "         4.3914e-02, -1.1144e-02,  2.1213e-02,  5.0143e-02, -1.7819e-02,\n",
      "        -3.6000e-02, -9.8346e-02,  1.8010e-02,  1.1031e-02, -4.7298e-02,\n",
      "        -2.5419e-02, -4.0803e-02,  3.5511e-02,  9.2070e-03,  6.9367e-03,\n",
      "        -4.2061e-02, -1.0377e-02,  8.0876e-02, -5.6107e-02,  5.7277e-02,\n",
      "         8.7439e-03,  1.8353e-02, -4.1559e-02,  3.4507e-02, -1.0548e-01,\n",
      "        -4.0571e-02, -2.1289e-02,  3.0586e-02,  5.1678e-03,  8.7577e-04,\n",
      "         1.3942e-01, -1.1645e-02,  7.2364e-02,  6.5043e-02,  2.4132e-02,\n",
      "         1.1002e-01,  6.1222e-03,  6.6061e-03, -5.2206e-02, -1.3325e-02,\n",
      "        -8.5573e-03, -2.0275e-03,  1.6365e-03,  2.6494e-02,  7.1705e-02,\n",
      "        -7.1865e-02,  8.4742e-02,  6.0429e-02, -5.9917e-04, -5.1137e-02,\n",
      "        -5.9481e-02, -7.6383e-02,  4.8239e-02, -3.4069e-02, -9.6994e-02,\n",
      "         1.8230e-02,  8.8950e-02,  8.6447e-02, -2.9383e-02, -9.0702e-02,\n",
      "        -3.7237e-02, -3.5979e-02, -4.2816e-02, -7.7253e-02,  7.3348e-03,\n",
      "         4.4436e-02, -1.5954e-01,  1.2394e-01,  1.1889e-02,  1.5041e-02,\n",
      "        -6.7389e-02, -4.5964e-02,  2.0859e-02, -3.0347e-02, -2.0750e-02,\n",
      "         3.9519e-02, -2.8886e-02, -8.1723e-02, -2.2986e-02, -2.3117e-03,\n",
      "         7.9396e-02, -4.6225e-02,  5.9592e-02, -6.6315e-02, -4.8456e-02,\n",
      "        -4.7836e-03, -6.7407e-02,  4.6288e-02,  1.5025e-01,  3.1964e-02,\n",
      "        -1.0685e-01, -3.1458e-02, -4.1457e-02,  7.1839e-02, -9.0231e-02,\n",
      "         3.3797e-02, -2.6273e-02, -6.0258e-02, -3.0063e-02, -9.9684e-02,\n",
      "         8.9154e-02,  4.6204e-02,  1.0030e-02, -2.1860e-02, -9.5296e-03,\n",
      "        -2.6632e-02, -2.0542e-02, -8.8112e-02, -3.1891e-02,  8.1285e-02,\n",
      "         3.4284e-02,  9.3343e-02, -7.2938e-02,  4.2222e-02,  8.5092e-02,\n",
      "        -6.9859e-02, -1.1665e-01, -1.7408e-02, -1.5403e-02,  5.4243e-02,\n",
      "         9.8341e-03, -2.8077e-02, -2.9991e-02,  3.4399e-02,  1.4826e-02,\n",
      "         1.0260e-02,  8.0673e-02,  5.1878e-03, -8.1736e-02,  8.6033e-02,\n",
      "         8.2636e-02,  5.0595e-02, -1.1922e-01,  9.3888e-03,  2.7255e-02,\n",
      "         2.7873e-02,  2.2796e-02,  1.8762e-02,  1.4380e-01, -1.4723e-01,\n",
      "        -1.4255e-02, -3.0604e-02, -3.7668e-03,  1.1167e-02, -8.0839e-02,\n",
      "         1.4414e-02, -2.5007e-02, -2.3666e-02, -2.7692e-02, -1.6474e-02,\n",
      "         5.1326e-02, -6.8901e-03,  2.6673e-02, -1.9049e-02, -4.9653e-02,\n",
      "         1.1313e-01,  8.5847e-02,  1.3205e-01, -4.7806e-02, -9.3220e-02,\n",
      "         4.1846e-02, -4.5715e-02,  2.4093e-02, -3.6066e-02,  5.0121e-02,\n",
      "         2.4745e-02, -9.0033e-02,  5.9747e-02, -5.9992e-02, -2.5795e-02,\n",
      "        -3.5649e-02,  2.3503e-02,  1.4340e-01, -5.7906e-02, -8.6132e-03,\n",
      "        -6.0701e-03,  3.0256e-03, -6.0207e-02,  1.3398e-02, -3.4405e-03,\n",
      "         3.6077e-02, -7.9061e-02, -4.5184e-02, -6.7206e-02,  8.3835e-02,\n",
      "        -1.4701e-02,  2.4760e-02,  1.7550e-02,  5.2360e-02, -1.1143e-01,\n",
      "        -6.0042e-02, -2.1617e-02, -2.3820e-02, -1.9716e-02, -1.1295e-01,\n",
      "        -1.7096e-02, -5.0607e-02,  9.7075e-02,  2.0780e-02, -4.8206e-02,\n",
      "         4.0675e-02, -5.4123e-02,  2.6274e-02, -1.1451e-01,  5.9652e-02,\n",
      "        -2.4965e-02, -2.3823e-02,  5.4150e-03, -2.5337e-03, -5.9982e-02,\n",
      "        -3.6474e-02, -1.8158e-02, -1.5301e-02,  1.1725e-02,  2.3499e-02,\n",
      "         7.4033e-02, -4.0130e-02, -5.1274e-02,  9.0815e-02,  5.4975e-02,\n",
      "        -3.4270e-02,  4.5382e-02, -7.2244e-02, -7.0036e-02, -9.7178e-03,\n",
      "        -3.3955e-02, -3.5253e-02,  8.1896e-02,  7.5562e-03, -7.9211e-02,\n",
      "        -1.0875e-01,  1.2409e-03,  7.7800e-02,  1.0634e-02, -8.2665e-02,\n",
      "         1.3230e-02, -3.4552e-02,  9.1453e-02, -6.4865e-02,  4.5128e-02,\n",
      "        -1.1324e-01, -5.8086e-02,  4.5286e-02, -3.5615e-02,  1.1491e-03,\n",
      "         4.5156e-02,  2.6197e-02, -9.7915e-02, -8.8574e-02,  6.3982e-02,\n",
      "        -7.3688e-02,  3.8706e-02,  8.2396e-02,  7.6938e-02, -2.0139e-02,\n",
      "        -6.2673e-02, -8.2048e-02,  5.6388e-02,  1.7644e-02,  4.3307e-02,\n",
      "         8.2072e-03, -4.8394e-02,  7.1145e-03, -1.4995e-01,  6.3767e-02,\n",
      "        -1.7300e-02, -4.0330e-04,  2.5645e-02,  6.1843e-02, -5.0088e-03,\n",
      "         3.9473e-03,  8.7710e-02,  3.0694e-02, -1.5863e-02,  1.2367e-01,\n",
      "         5.8815e-02,  6.1809e-02,  1.1823e-01,  3.4193e-02, -1.3734e-01,\n",
      "        -8.3475e-03, -1.3101e-02,  1.7372e-01,  3.1849e-02,  5.8699e-02,\n",
      "        -8.2168e-02,  2.9679e-02,  2.9754e-02, -1.9589e-02, -2.3867e-05,\n",
      "         2.9229e-03, -5.9795e-02,  1.0513e-01, -2.3250e-02,  1.5259e-02,\n",
      "        -9.9677e-04,  5.2436e-02,  4.5202e-02, -5.3536e-02, -3.1198e-02,\n",
      "         1.1600e-01,  8.2992e-02, -6.0462e-02, -6.9867e-02, -2.0561e-03,\n",
      "         6.2426e-02,  3.0686e-02,  7.3595e-03, -5.2512e-03, -8.7785e-02,\n",
      "         7.2232e-02, -5.5166e-02,  5.2830e-02, -3.4109e-02, -3.5072e-02,\n",
      "        -7.8913e-02,  3.6241e-02,  4.8680e-02, -2.4749e-02,  9.5748e-02,\n",
      "         1.1784e-01,  6.6303e-02, -3.3105e-02,  3.1397e-02,  4.8392e-02,\n",
      "        -9.6809e-02,  6.1331e-02,  3.0868e-02,  3.2937e-02,  1.4860e-02,\n",
      "        -8.8214e-02, -7.5167e-02, -2.6680e-02, -7.2619e-02, -3.8868e-02,\n",
      "         4.7005e-02, -1.5254e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-3.7034e-02, -4.5888e-02,  8.8781e-03,  2.7156e-02,  5.8858e-02,\n",
      "         1.2498e-03, -2.9473e-02, -2.4259e-02,  2.7695e-02,  4.8506e-02,\n",
      "        -1.3610e-02,  2.4264e-02, -1.0506e-02, -2.2343e-02, -1.2575e-02,\n",
      "        -2.7388e-02,  3.7047e-03, -9.8502e-02, -7.6187e-02, -1.3275e-02,\n",
      "         4.0868e-02,  3.1048e-02,  2.9744e-03, -3.4535e-02,  6.2692e-02,\n",
      "        -1.0555e-01, -1.8775e-03, -6.1323e-02,  1.1437e-02,  6.9841e-02,\n",
      "        -1.2952e-02,  7.9710e-02, -3.6756e-02,  1.2847e-02,  1.0407e-01,\n",
      "        -8.7324e-02, -1.0587e-01, -3.1902e-02, -8.2598e-03, -1.0516e-01,\n",
      "        -9.7262e-02,  1.1731e-02, -1.1542e-02, -1.0035e-01, -8.8628e-02,\n",
      "        -1.6604e-02, -6.6435e-04, -5.5660e-02, -5.8090e-03, -9.9288e-03,\n",
      "         2.7286e-02, -4.0562e-02, -1.3763e-02, -5.6210e-02, -8.2477e-03,\n",
      "         3.0968e-02, -2.2097e-02,  2.6884e-02, -4.4554e-03,  6.1624e-02,\n",
      "         5.7080e-02,  9.1388e-03, -2.2383e-02,  3.1594e-02,  9.0034e-02,\n",
      "         2.9283e-04, -2.4813e-03, -4.8279e-02,  2.9078e-02,  3.5868e-02,\n",
      "         4.1491e-02, -6.3660e-02, -8.4763e-02, -8.1597e-02, -5.1852e-02,\n",
      "         2.2601e-04,  1.0845e-01,  3.0973e-02, -1.5400e-01,  3.3164e-02,\n",
      "         7.9088e-02,  6.5250e-02,  5.1900e-02, -4.2283e-02, -1.1346e-01,\n",
      "        -9.0076e-03,  1.1980e-01, -1.1909e-02,  1.2310e-02,  1.8831e-02,\n",
      "        -4.9647e-02,  7.0969e-02, -2.3682e-02, -8.6618e-02,  5.2677e-02,\n",
      "         7.8079e-03, -1.0115e-01,  7.5915e-02, -4.8108e-02, -1.3128e-01,\n",
      "         6.4873e-02, -7.1029e-03, -1.4379e-01, -2.1432e-02, -5.3666e-02,\n",
      "         3.7874e-02, -8.1764e-02,  1.6618e-01,  7.1652e-02,  4.2189e-02,\n",
      "        -4.8112e-02,  5.0704e-02, -8.4332e-02,  2.3637e-02, -1.1713e-02,\n",
      "        -1.4738e-01, -5.6326e-02, -8.2328e-02, -6.9366e-03,  8.9393e-03,\n",
      "         9.0724e-02, -3.4346e-02, -1.7982e-02, -1.4817e-02, -9.2182e-02,\n",
      "         3.9414e-02, -1.3945e-02, -9.3391e-02,  1.0452e-01,  8.3443e-02,\n",
      "        -8.3101e-03,  5.8458e-02,  3.4724e-02, -9.1750e-02,  2.9846e-02,\n",
      "        -9.8895e-02, -2.4202e-02,  4.6580e-02,  4.4337e-02, -1.2447e-02,\n",
      "        -8.0480e-03, -5.6974e-03, -3.7265e-02,  7.7061e-02,  5.1464e-02,\n",
      "        -7.0224e-02, -4.4164e-02,  2.5564e-02,  1.2461e-02, -2.4537e-02,\n",
      "         2.2466e-02,  6.7765e-03, -2.1143e-02,  1.3173e-02, -4.8422e-02,\n",
      "        -2.4130e-02,  4.0795e-02, -6.9050e-02,  5.2960e-02,  2.9344e-02,\n",
      "         6.1323e-02,  2.6642e-02, -1.5501e-02,  1.1257e-02,  5.2199e-02,\n",
      "        -1.9131e-02, -7.1120e-02,  1.5206e-01, -5.5123e-02,  1.6600e-02,\n",
      "        -1.7471e-02,  5.4039e-02,  7.3465e-02, -1.4534e-02,  3.2988e-02,\n",
      "         1.0805e-01,  2.3235e-03,  2.6146e-02,  5.6207e-02,  2.4650e-02,\n",
      "         1.0190e-02, -4.5924e-03,  4.1432e-02, -4.8620e-02, -2.9034e-02,\n",
      "        -2.9012e-02,  1.4155e-02,  3.5942e-02, -9.4590e-03, -3.9627e-02,\n",
      "        -5.3268e-02,  1.3831e-01, -3.0257e-02, -5.7423e-03,  4.2466e-02,\n",
      "         1.2649e-01, -5.0767e-02, -1.1174e-02, -2.3112e-02,  3.8812e-02,\n",
      "        -6.3522e-02,  9.1453e-02,  2.6309e-02, -1.1686e-01, -3.9759e-02,\n",
      "         2.4578e-02, -4.7622e-03, -5.6869e-02,  9.6072e-02,  1.3556e-02,\n",
      "        -2.8459e-02, -4.5581e-02,  1.2914e-01, -1.1633e-02,  1.1193e-01,\n",
      "        -8.6753e-02, -8.5673e-03, -7.3127e-02, -3.6154e-02, -9.3040e-02,\n",
      "        -3.7462e-02,  1.2344e-01,  8.0146e-02, -1.7490e-02,  1.1924e-01,\n",
      "        -1.0738e-02,  6.7925e-02, -6.9445e-02, -2.5708e-02, -5.6665e-02,\n",
      "        -1.5419e-01,  1.2431e-01, -7.5615e-03, -1.0575e-01,  8.1955e-02,\n",
      "        -3.7937e-02,  8.6439e-02, -3.1533e-03,  1.4085e-01,  3.6980e-02,\n",
      "        -1.3440e-02, -5.1998e-02,  5.9634e-02, -4.4400e-02,  1.6468e-02,\n",
      "         3.7003e-02,  2.0843e-02,  4.8651e-02, -3.7829e-02,  1.0212e-01,\n",
      "        -1.8587e-02,  4.5990e-02, -4.5087e-03, -1.0517e-01, -7.8714e-02,\n",
      "        -2.2157e-02, -5.8386e-02,  7.0721e-02, -1.4240e-02, -1.0749e-01,\n",
      "        -6.8921e-02, -3.1443e-02, -3.2220e-02, -6.4972e-02,  1.1256e-02,\n",
      "         4.3494e-02,  1.8916e-02, -1.8547e-01, -2.1113e-02, -3.5792e-02,\n",
      "        -1.2145e-02,  4.6165e-02, -1.1010e-01,  3.3331e-04,  8.4547e-02,\n",
      "         5.4524e-02,  4.8118e-02, -9.5097e-02, -7.2445e-02, -6.6263e-05,\n",
      "         5.1787e-02,  4.9852e-02, -4.7932e-02, -1.2280e-02, -1.6250e-02,\n",
      "        -1.4342e-02, -1.1116e-01, -5.5778e-02, -7.7247e-03, -8.1662e-02,\n",
      "        -4.3206e-03,  6.6698e-02, -5.0373e-02, -1.2831e-01,  7.0735e-02,\n",
      "        -4.0484e-02, -2.6315e-02, -2.7391e-02, -8.0403e-02, -6.9732e-03,\n",
      "         5.4342e-02,  2.0656e-02,  1.5141e-01,  1.0275e-01,  1.5837e-03,\n",
      "        -1.4563e-01,  8.5911e-05,  4.7454e-03, -7.8300e-02,  4.8858e-02,\n",
      "        -2.1546e-02,  1.4427e-02,  4.6923e-02, -4.1582e-02,  3.4860e-02,\n",
      "         1.6094e-01, -2.8653e-02,  6.8671e-02,  3.9210e-02, -2.7989e-02,\n",
      "         1.2157e-01,  3.4874e-02,  1.0473e-01,  5.0698e-02, -6.6427e-02,\n",
      "        -8.5859e-02,  4.0868e-02, -8.1263e-02,  1.2227e-04, -4.1179e-02,\n",
      "         7.0834e-03,  8.5109e-02, -2.0567e-02,  6.0143e-03, -8.9583e-02,\n",
      "         6.3068e-02, -4.5089e-02,  2.6703e-02,  5.3511e-03,  9.8072e-03,\n",
      "         9.1949e-04,  4.8803e-02, -1.2944e-02, -1.6477e-02,  3.7466e-03,\n",
      "        -7.1968e-02, -6.9599e-02, -1.0072e-01, -7.0090e-02,  3.5817e-02,\n",
      "         6.2147e-02,  8.6350e-02,  8.2676e-02,  6.9734e-03, -1.6660e-01,\n",
      "         3.0636e-02, -7.5360e-02,  8.7070e-02,  4.6590e-02, -1.2240e-02,\n",
      "         4.7421e-02,  1.4499e-01, -3.2117e-02,  6.7256e-03, -9.1146e-03,\n",
      "         5.6627e-02,  3.4365e-02,  3.5674e-02,  1.1961e-03,  9.1195e-03,\n",
      "        -1.0258e-01, -2.6809e-02, -3.6439e-02, -5.3987e-02, -3.7285e-02,\n",
      "        -4.7299e-02,  2.0322e-02, -7.9408e-02, -7.7213e-02, -4.1219e-02,\n",
      "         1.1305e-01, -3.6860e-02,  3.4759e-02,  4.5197e-03, -1.8849e-02,\n",
      "        -1.1627e-02,  7.8283e-02, -5.6437e-02,  3.5024e-02,  6.2222e-02,\n",
      "        -8.2901e-02,  7.1049e-02,  9.9048e-03,  8.3881e-02,  3.7555e-03,\n",
      "         8.8532e-02,  9.2635e-02,  1.6246e-02, -3.0551e-02,  4.0173e-02,\n",
      "         3.9328e-02,  9.8969e-03,  7.2826e-04, -8.5527e-03,  1.9672e-02,\n",
      "         1.0268e-01, -4.0752e-03, -5.5843e-02,  1.5902e-02,  7.0855e-03,\n",
      "        -3.0325e-02,  2.9130e-02, -7.9757e-02,  2.0168e-02,  1.3599e-02,\n",
      "        -2.4822e-02, -8.0696e-03,  7.8805e-03,  3.1998e-04, -3.3752e-02,\n",
      "        -2.3653e-02,  7.4149e-02, -9.0394e-03, -6.5222e-03, -3.0573e-02,\n",
      "         1.1063e-01,  7.5828e-02,  4.1677e-02,  1.3911e-02, -7.0996e-03,\n",
      "         2.3597e-03,  2.6949e-03, -5.3042e-03,  7.1347e-02,  2.7978e-02,\n",
      "         9.5793e-04, -2.3873e-02, -7.2959e-02,  3.1148e-02, -6.5378e-02,\n",
      "         4.4773e-02, -4.6407e-02, -2.7808e-02,  6.0678e-02,  2.2824e-02,\n",
      "         1.2299e-02, -1.2252e-01, -9.4176e-02, -3.1335e-02,  6.1090e-02,\n",
      "        -8.9544e-02, -7.8463e-02, -1.0646e-01,  1.2856e-01,  5.3371e-02,\n",
      "        -3.5043e-02,  4.9204e-02, -2.7718e-02, -1.8169e-03, -3.2086e-02,\n",
      "         7.7823e-03,  6.8141e-03,  9.3693e-02,  1.6695e-02, -7.0995e-03,\n",
      "        -8.1406e-02, -1.0529e-02,  2.3930e-02, -2.4667e-02,  1.4599e-02,\n",
      "         2.2815e-02,  6.4431e-02, -8.6203e-02, -1.9157e-01,  3.7300e-02,\n",
      "        -2.8549e-02, -2.9900e-02,  2.0874e-02, -1.8929e-01,  6.7435e-02,\n",
      "        -4.1862e-02,  4.9628e-04,  7.5833e-03,  8.0471e-02, -1.7851e-02,\n",
      "        -4.5390e-02,  2.1833e-02, -1.6886e-02, -1.0043e-02, -7.4905e-02,\n",
      "        -9.9795e-04, -2.0626e-02,  8.3278e-02, -7.4464e-02,  3.2107e-02,\n",
      "         4.9412e-02, -5.9202e-02, -6.2015e-02,  1.0825e-02,  8.4142e-02,\n",
      "         6.0584e-02,  2.8453e-02, -6.4364e-02,  3.4312e-02, -3.1387e-02,\n",
      "        -1.0054e-02,  6.2364e-02,  9.9319e-02,  4.8268e-02,  3.6428e-02,\n",
      "         4.7602e-02, -2.9711e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 1.7651e-03,  3.1396e-03, -1.4551e-02, -3.5147e-03, -2.1400e-02,\n",
      "         1.1861e-01,  2.7863e-02, -5.9323e-02,  5.4408e-02, -3.9685e-02,\n",
      "        -2.9819e-02,  1.3290e-02, -3.3680e-02, -3.0514e-02, -9.5577e-02,\n",
      "        -2.7275e-02, -3.2411e-02,  1.0115e-01,  1.8278e-02,  5.4167e-02,\n",
      "        -6.5277e-02, -4.9623e-02,  3.8521e-02, -7.7113e-02,  2.3679e-02,\n",
      "        -2.1421e-02, -3.1328e-02, -3.8675e-02,  3.2301e-02,  8.0595e-02,\n",
      "        -1.7031e-01, -3.1086e-02,  7.4435e-02,  4.4086e-02, -5.2982e-02,\n",
      "        -3.7618e-02, -2.1757e-02,  2.6901e-02,  1.2416e-02, -6.1151e-02,\n",
      "        -5.6076e-03, -8.1322e-02,  1.2883e-01,  2.1244e-01,  6.3023e-03,\n",
      "        -6.4486e-02, -6.8903e-02, -5.2496e-02, -8.8419e-03, -4.8330e-03,\n",
      "        -9.8466e-02, -9.1724e-02, -4.6670e-03,  2.2265e-02,  7.4199e-03,\n",
      "        -6.7150e-03,  3.1992e-03, -1.9731e-02,  1.7806e-02, -7.7666e-03,\n",
      "         6.6665e-03, -4.9659e-03,  1.3266e-02, -3.1188e-02,  6.3222e-02,\n",
      "         2.4398e-02,  2.9437e-02, -7.7957e-04,  2.4054e-02,  1.6580e-01,\n",
      "        -7.9211e-02, -2.8934e-02,  3.4830e-02,  3.1386e-02,  1.2014e-03,\n",
      "         9.4098e-02, -5.5012e-03,  4.5756e-02,  3.2991e-02,  1.8693e-02,\n",
      "         4.4928e-02, -2.1605e-02,  4.0092e-02, -6.9511e-02, -8.6237e-02,\n",
      "        -1.2794e-01,  2.8559e-02, -3.4364e-02,  2.3834e-03,  9.2352e-03,\n",
      "        -2.0991e-03,  1.2794e-02,  1.0197e-02, -1.4751e-02, -5.3813e-03,\n",
      "         2.9286e-02,  8.8126e-02,  1.5448e-02, -1.4078e-02, -3.9143e-02,\n",
      "        -5.8560e-02,  5.4407e-02,  3.5490e-02, -7.9659e-02,  3.4453e-02,\n",
      "         2.5864e-02, -3.0899e-02, -1.5625e-02, -4.5447e-02,  4.7464e-02,\n",
      "        -3.1091e-02, -3.4445e-02, -5.4052e-02, -6.4918e-02,  4.4487e-02,\n",
      "         2.5045e-02, -1.1488e-02,  2.5262e-02, -1.2607e-02,  1.3235e-02,\n",
      "         2.8561e-02,  6.9778e-02,  3.5717e-02, -1.3796e-02, -1.6055e-01,\n",
      "        -6.3508e-02,  3.0388e-02,  3.6702e-02,  1.4510e-02,  8.2649e-02,\n",
      "        -9.6217e-03,  2.8959e-02,  3.8684e-02, -8.4300e-02, -1.5368e-01,\n",
      "         9.8709e-02, -7.2473e-02,  3.1997e-02,  1.1817e-01, -2.6140e-02,\n",
      "        -6.1742e-02, -1.6166e-02,  7.0216e-02, -1.2530e-01, -3.3601e-02,\n",
      "         1.8504e-02,  4.9253e-02,  1.5496e-01, -7.7431e-02, -1.4273e-02,\n",
      "        -1.3381e-02,  1.0467e-01, -7.3973e-02, -9.8395e-02, -2.9553e-02,\n",
      "         4.8231e-02,  6.4982e-02, -5.0469e-02,  3.5893e-02,  9.4489e-02,\n",
      "         6.2196e-02, -9.2381e-02, -8.7598e-02,  7.9401e-02, -6.6444e-02,\n",
      "        -1.0009e-02, -3.8275e-02, -2.5270e-02, -1.7952e-01, -9.5267e-03,\n",
      "        -1.3783e-01,  2.1312e-01, -1.1740e-02, -8.2986e-02,  3.5087e-02,\n",
      "        -1.9155e-02, -2.4328e-02, -4.0487e-02,  3.3686e-02, -1.7021e-02,\n",
      "        -5.0354e-02, -1.5596e-01, -1.7125e-03,  5.6674e-02,  6.6230e-03,\n",
      "         6.4058e-03, -3.7337e-03,  1.1259e-02, -2.4012e-02,  8.4532e-02,\n",
      "        -2.1994e-02,  3.6341e-03,  8.1102e-02, -5.8442e-02,  9.7022e-02,\n",
      "        -6.0901e-02,  5.0808e-02,  1.3352e-01,  1.6406e-02,  1.3148e-02,\n",
      "         2.8686e-02, -3.0704e-02, -4.3113e-02,  5.2098e-02, -5.5051e-02,\n",
      "        -1.1791e-01,  5.0002e-02,  2.3706e-03, -6.4074e-02,  5.0139e-02,\n",
      "        -3.7592e-02,  5.3099e-02,  3.9144e-02,  4.3691e-03,  1.4775e-02,\n",
      "        -7.3321e-02, -4.6698e-02,  1.2764e-01, -6.2895e-02, -2.6595e-02,\n",
      "         7.9530e-02,  3.6950e-02, -4.7796e-03,  3.2136e-02, -4.4875e-02,\n",
      "        -3.2131e-02,  8.3086e-02,  8.9513e-02, -6.2051e-03, -1.2118e-01,\n",
      "         2.6485e-02, -3.3139e-02,  4.4756e-02,  7.8008e-04,  7.1055e-02,\n",
      "         3.0050e-02,  8.2575e-03, -2.6538e-02, -3.9907e-02, -2.5800e-02,\n",
      "        -3.3800e-02,  1.8517e-02, -7.0688e-02, -1.3011e-01, -3.3101e-02,\n",
      "        -5.4424e-02,  3.0215e-02, -6.2839e-02,  2.4651e-02, -1.8812e-03,\n",
      "        -1.3442e-01,  1.2847e-02,  7.9453e-02,  8.0802e-02, -9.5993e-02,\n",
      "         3.4160e-02,  2.6102e-02, -8.6553e-03,  5.7268e-02,  8.5350e-02,\n",
      "         1.3918e-02,  1.1504e-02,  2.9779e-03,  1.0623e-02,  5.5536e-02,\n",
      "        -4.1146e-02, -9.3039e-02, -3.3455e-03,  1.5882e-02, -1.5050e-01,\n",
      "         7.5856e-03,  2.2823e-02, -3.8871e-02,  5.5844e-02,  5.4641e-03,\n",
      "        -2.4733e-02, -5.1179e-02, -1.8616e-02,  5.5658e-02, -6.9583e-02,\n",
      "        -6.0925e-02, -8.0161e-02, -1.0143e-01,  4.3837e-02,  1.3554e-01,\n",
      "         8.7156e-02,  2.5922e-02, -7.2726e-02, -1.8920e-02,  9.7482e-02,\n",
      "         2.0591e-02, -6.2224e-02,  5.4904e-02, -1.3960e-01, -7.6254e-02,\n",
      "         8.3799e-02, -3.9226e-02, -4.3723e-02, -3.3469e-02,  9.1810e-03,\n",
      "         4.9622e-02,  6.3080e-02, -2.8480e-02, -1.8700e-02,  6.6885e-02,\n",
      "        -6.8625e-03,  7.1043e-02,  7.1088e-02, -9.2783e-02,  9.1262e-02,\n",
      "         4.6247e-02, -2.9005e-02,  2.8690e-02,  1.9394e-02,  5.7164e-05,\n",
      "         2.2624e-02,  3.3163e-02,  1.7700e-02,  3.4232e-02, -2.9858e-02,\n",
      "        -7.4267e-02,  3.6014e-02, -4.4552e-02,  3.5258e-02, -1.0101e-01,\n",
      "        -6.7129e-03,  1.4119e-02,  2.7532e-02,  1.8333e-02,  1.0998e-01,\n",
      "        -4.3879e-04,  6.3078e-02,  1.9749e-02,  4.5188e-02,  1.7698e-02,\n",
      "        -1.6677e-02,  8.2497e-02, -7.5923e-02,  6.3407e-02,  6.3229e-02,\n",
      "         1.7209e-02,  8.9937e-02, -3.1758e-02,  2.4061e-02, -7.6937e-02,\n",
      "         2.9163e-03, -6.6448e-02, -1.3663e-02, -3.8498e-02, -6.1970e-02,\n",
      "        -5.3004e-02,  2.5560e-02,  1.7372e-01,  1.9347e-02,  7.7611e-02,\n",
      "         1.2019e-01, -1.5177e-01, -1.0369e-02, -3.0696e-02,  6.5096e-02,\n",
      "         1.3015e-02,  5.4550e-02, -5.5283e-02,  7.5891e-03, -2.0863e-02,\n",
      "        -2.2272e-02,  1.8210e-02, -6.6587e-03, -1.3865e-02,  5.7003e-02,\n",
      "        -1.9093e-02,  9.1872e-03,  9.9067e-02,  3.3590e-04,  4.0905e-02,\n",
      "         2.1044e-03, -6.7002e-03,  2.9374e-02, -1.1736e-02,  3.6019e-03,\n",
      "        -2.4367e-02, -3.7626e-02, -1.1231e-01,  1.7375e-02, -6.0035e-03,\n",
      "         5.7686e-02, -2.7193e-02,  1.9783e-02, -6.3263e-02,  2.2237e-02,\n",
      "         7.3779e-03, -2.8759e-03, -1.5603e-02,  5.7662e-02,  8.7457e-03,\n",
      "         1.0018e-02, -5.0072e-02, -3.7638e-02,  2.4585e-02, -9.2793e-02,\n",
      "        -1.1872e-01, -2.2116e-02, -1.0956e-01, -1.0836e-01,  8.6403e-02,\n",
      "         4.3467e-02,  2.0700e-02,  5.1945e-02,  3.0060e-02,  2.7744e-02,\n",
      "        -9.2273e-03,  7.5827e-02, -4.5446e-02,  8.2869e-02, -9.2931e-02,\n",
      "         1.2670e-02, -4.8179e-02, -1.5450e-01, -1.6038e-03, -2.9253e-02,\n",
      "        -2.7980e-02, -1.0475e-02,  2.7516e-02,  1.6998e-01,  2.4017e-02,\n",
      "         8.4535e-02, -2.9163e-04,  3.1187e-02,  5.4309e-02, -3.0479e-02,\n",
      "        -8.0611e-02, -6.6498e-02, -1.4551e-01,  2.1430e-03, -3.7552e-02,\n",
      "         7.6690e-02,  7.4113e-02, -1.0557e-01, -7.4909e-02,  6.7211e-02,\n",
      "        -7.8306e-02,  4.8829e-02, -4.6191e-02,  1.3408e-02,  2.7609e-02,\n",
      "        -7.1487e-03, -3.2693e-03,  6.9174e-02,  1.8630e-01,  8.2350e-02,\n",
      "        -7.1999e-02, -5.7636e-04, -1.0190e-01, -2.1849e-02, -2.4579e-02,\n",
      "         8.7751e-02, -3.5942e-02, -2.4704e-03, -1.1202e-01, -7.7516e-02,\n",
      "        -3.0877e-02,  5.2970e-02, -1.0476e-02,  9.5842e-03, -7.3720e-02,\n",
      "         4.0108e-02, -1.2442e-02,  3.8677e-02, -4.2649e-02,  3.2528e-02,\n",
      "         4.7383e-02, -1.2851e-03, -3.1630e-02,  9.8758e-02, -4.5205e-02,\n",
      "         9.8402e-02, -9.2297e-02, -1.9997e-02, -1.7744e-02, -2.2326e-02,\n",
      "         8.0307e-02, -1.7815e-02,  1.9394e-02, -5.2028e-02, -5.1993e-02,\n",
      "         5.9033e-03,  1.0825e-02, -1.7139e-02, -1.4043e-01,  3.3729e-02,\n",
      "         2.4079e-02,  3.1476e-02, -7.7750e-02,  4.4037e-03, -7.0054e-02,\n",
      "        -1.0412e-02,  8.7667e-03,  6.4475e-02, -6.7967e-02,  4.3379e-02,\n",
      "        -8.0798e-02,  1.3300e-01, -2.5715e-02,  4.1997e-02,  1.5607e-02,\n",
      "        -1.8457e-02, -1.4307e-02, -1.3592e-02, -8.4850e-04,  6.9601e-03,\n",
      "         1.7143e-02, -7.7591e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([ 7.0232e-02, -3.9177e-02, -5.6618e-02,  2.4083e-02, -1.1208e-01,\n",
      "        -8.1959e-02, -1.0443e-02,  3.7170e-02, -5.0199e-02,  3.4944e-02,\n",
      "        -1.1758e-01,  2.1176e-02, -1.3711e-02,  1.7198e-03,  5.5355e-03,\n",
      "        -2.0753e-02, -4.9122e-02, -1.6634e-02,  8.6761e-04,  5.3412e-02,\n",
      "        -5.2480e-02,  3.0198e-03, -2.5701e-02, -1.2355e-01,  5.1328e-02,\n",
      "        -6.1343e-02, -4.0117e-02,  6.6909e-02,  5.3694e-03,  2.3511e-02,\n",
      "        -8.8978e-03, -1.3987e-02,  5.8384e-02, -7.4027e-02,  7.0214e-03,\n",
      "        -2.3619e-02, -1.1647e-02,  5.0248e-03, -1.0472e-01,  5.0924e-02,\n",
      "         9.6729e-03,  4.4740e-02, -8.4304e-03,  3.1834e-02, -5.6028e-02,\n",
      "         1.2579e-02,  4.1046e-02, -7.6398e-02, -8.2215e-02, -7.4826e-02,\n",
      "        -5.8703e-03,  2.1899e-02, -3.4924e-02, -9.3841e-02,  9.2489e-02,\n",
      "        -3.9724e-02,  6.8778e-02,  6.2910e-02,  1.5035e-01, -8.5688e-02,\n",
      "         5.1889e-02, -9.3605e-02, -7.0402e-02,  4.7219e-02,  5.9798e-02,\n",
      "        -3.6312e-03, -1.3177e-02, -4.6579e-02,  2.6072e-02, -1.8031e-02,\n",
      "        -1.5455e-01,  1.6608e-01, -1.5167e-03, -2.2081e-02, -3.3239e-02,\n",
      "         7.1615e-03,  5.0772e-02,  6.4464e-03, -1.0717e-03,  1.1329e-01,\n",
      "        -4.0795e-03,  7.9883e-02, -4.3044e-02,  1.3580e-01, -1.0705e-02,\n",
      "         7.0666e-03, -7.1443e-03,  9.1426e-02, -1.3554e-03, -9.4658e-02,\n",
      "        -4.0040e-02,  5.9643e-02,  1.8720e-02, -6.1085e-03, -5.1143e-03,\n",
      "         5.2426e-03,  3.9795e-02,  5.7733e-02,  9.3336e-02,  4.6847e-03,\n",
      "         5.8702e-02,  2.5341e-02,  4.2893e-02,  7.5947e-02,  2.8520e-04,\n",
      "         7.1536e-03, -4.3884e-03, -4.4555e-02, -4.4503e-02,  5.6192e-02,\n",
      "        -5.1656e-02, -1.2393e-01,  4.3672e-02,  4.7996e-02, -1.0800e-02,\n",
      "         5.6755e-02, -8.3568e-02, -1.3538e-02, -5.6153e-02, -5.5316e-02,\n",
      "        -2.3975e-02, -1.1282e-01, -2.8566e-02,  7.2766e-02, -3.8624e-02,\n",
      "         7.6615e-02,  3.6164e-02,  1.0354e-01,  4.9160e-02,  1.9378e-02,\n",
      "        -2.2329e-02, -1.2350e-01,  1.2831e-01,  9.7161e-03,  8.3806e-02,\n",
      "        -5.0945e-02, -2.3909e-02, -2.4867e-02,  5.3618e-02,  4.3033e-02,\n",
      "        -8.6281e-03, -3.7764e-02, -1.2432e-01,  1.3901e-02, -8.2746e-02,\n",
      "         1.5292e-02, -1.0102e-01, -2.1163e-03,  2.4047e-02, -3.3842e-02,\n",
      "         1.7279e-01, -2.0493e-03, -1.4493e-02,  5.7667e-02, -2.8942e-02,\n",
      "        -3.2882e-03,  7.1961e-02,  1.5763e-02, -1.0857e-01,  3.1682e-02,\n",
      "        -1.5458e-02,  2.3903e-02, -7.8493e-02,  3.3385e-02, -1.1762e-02,\n",
      "         5.4726e-02, -1.0496e-01, -1.9116e-02,  4.4039e-02, -4.5159e-02,\n",
      "         1.1691e-01, -7.5459e-02, -3.4751e-02, -7.0932e-05, -5.4284e-03,\n",
      "        -3.1645e-02,  7.8052e-02, -1.3927e-02, -3.9138e-02, -6.9432e-02,\n",
      "        -5.6814e-02,  4.7092e-02, -9.7913e-02, -7.1706e-02, -7.4354e-02,\n",
      "         2.9061e-02,  1.2788e-01,  3.2878e-02,  6.8620e-02,  7.8050e-03,\n",
      "        -8.1034e-03,  1.2591e-01, -2.5306e-02,  2.3245e-02,  6.0525e-03,\n",
      "         5.1102e-02,  2.6583e-02, -2.1282e-03, -5.5411e-02,  4.6495e-02,\n",
      "        -2.4725e-02,  2.2852e-02, -1.2736e-02,  1.6637e-01, -5.4719e-02,\n",
      "         8.6107e-02, -5.4407e-02,  6.8237e-02, -6.1891e-02, -5.5849e-02,\n",
      "         7.3760e-03, -3.0345e-02, -3.1600e-02,  3.3583e-02,  2.8570e-02,\n",
      "         8.2200e-02,  2.6655e-02,  2.6249e-02, -1.2001e-02,  7.8356e-02,\n",
      "        -1.6183e-02, -1.4890e-02,  1.2511e-02,  3.7454e-02,  2.5717e-02,\n",
      "         2.4392e-03,  1.9375e-02,  6.4533e-02,  3.3817e-02, -6.6789e-02,\n",
      "        -8.1340e-02, -3.5166e-02, -2.8866e-02, -7.5490e-02,  3.9034e-02,\n",
      "        -4.9257e-02,  1.5981e-02,  1.2176e-02,  2.2973e-02,  1.0207e-02,\n",
      "        -1.0285e-03,  1.7862e-01,  6.4228e-02, -3.5339e-02,  8.1926e-02,\n",
      "         7.1711e-02, -1.0528e-02,  3.6034e-02, -2.3140e-02,  4.6343e-02,\n",
      "        -3.3368e-02, -4.6355e-02,  5.1168e-02,  1.8313e-02, -3.8195e-03,\n",
      "         1.0237e-01, -4.0303e-02,  3.3172e-02, -4.5773e-02, -1.4106e-02,\n",
      "        -3.1364e-02,  5.1665e-02, -3.1724e-02,  3.0433e-02, -3.9412e-02,\n",
      "        -3.3040e-02,  2.1146e-02, -1.1771e-01, -6.6739e-02, -3.3981e-02,\n",
      "        -4.4390e-03,  2.8506e-02,  1.9362e-02,  1.0839e-01, -1.5109e-02,\n",
      "         1.5135e-01, -2.9912e-02,  7.0132e-02, -6.2905e-02, -9.2045e-02,\n",
      "        -1.0811e-01, -6.4596e-02,  1.1569e-01,  4.2324e-02, -5.3588e-02,\n",
      "         2.2440e-02, -2.2649e-02, -7.6581e-02, -6.1811e-02,  6.7117e-02,\n",
      "        -8.8734e-02,  1.1926e-02, -1.0264e-02,  2.1893e-02,  6.1756e-02,\n",
      "         1.1959e-01, -9.6380e-02,  1.3470e-02, -7.0965e-02,  2.2478e-02,\n",
      "         5.0166e-02, -4.6788e-03,  9.3105e-02,  1.2183e-01, -1.0024e-01,\n",
      "         1.9777e-04,  8.1114e-02, -2.6921e-02,  1.0334e-01, -3.6504e-02,\n",
      "         1.0802e-02, -2.5081e-02, -6.5181e-02,  8.6339e-02,  3.7305e-02,\n",
      "        -1.2546e-01, -2.3171e-02, -5.1505e-02,  8.1840e-02,  4.9002e-02,\n",
      "         1.8363e-02,  2.9693e-02, -3.8902e-03, -4.1257e-02, -2.2935e-02,\n",
      "         8.3203e-02,  5.9329e-02,  7.7033e-03,  4.9673e-02, -3.4751e-02,\n",
      "        -3.4831e-03, -7.7208e-03,  1.0457e-01, -2.1170e-02,  6.3125e-02,\n",
      "        -1.2047e-02,  1.4499e-02, -5.0847e-02,  2.7684e-02,  8.1270e-02,\n",
      "        -2.4067e-02,  1.6061e-04,  4.5172e-02,  8.9830e-02,  5.0638e-03,\n",
      "        -2.7056e-02,  1.6215e-02, -1.2409e-01,  2.7129e-02, -3.0758e-02,\n",
      "        -4.1683e-02,  7.6068e-03,  1.4988e-02, -2.3955e-02, -8.0970e-02,\n",
      "        -1.0192e-01,  3.6965e-02, -2.6476e-02,  5.7144e-03,  7.6527e-02,\n",
      "         9.9065e-02,  4.3809e-02,  5.6087e-02, -6.8878e-02,  6.4834e-02,\n",
      "         2.2787e-02,  7.3976e-02,  9.9496e-03, -1.7695e-02,  8.8900e-02,\n",
      "        -9.6980e-02, -7.0818e-02,  4.9335e-02, -5.5873e-02, -6.7333e-03,\n",
      "        -1.0160e-02,  2.7102e-02, -2.2473e-02, -4.0724e-02, -4.3555e-02,\n",
      "        -4.3084e-02, -8.1544e-02, -2.8473e-02,  1.8932e-02,  2.8450e-02,\n",
      "        -8.1699e-02, -8.1030e-02,  4.9583e-02,  3.5871e-02, -1.5891e-02,\n",
      "        -8.8298e-03, -1.2130e-02, -8.1447e-02,  4.5123e-02,  6.6769e-02,\n",
      "         4.5007e-02, -1.0901e-02, -1.5257e-01,  1.2816e-02,  6.9188e-02,\n",
      "        -1.3537e-02,  1.0406e-01,  2.4015e-02,  4.3749e-02, -2.3074e-02,\n",
      "         6.2925e-02, -3.2508e-02, -5.4690e-02,  1.6847e-02, -5.7822e-02,\n",
      "         6.0195e-02, -1.6088e-02, -3.6611e-02,  4.3164e-03, -1.6129e-02,\n",
      "        -2.0027e-02,  2.7187e-02, -5.7546e-02,  1.6556e-02, -4.5320e-04,\n",
      "         2.4637e-02, -5.7647e-02, -4.5837e-02,  1.4810e-02,  1.4818e-02,\n",
      "        -9.2751e-03,  3.1316e-02,  4.6298e-02,  1.5679e-02,  2.5335e-02,\n",
      "         1.5162e-02, -6.5274e-02, -1.1448e-01,  3.5900e-02, -1.1034e-01,\n",
      "        -9.4011e-02,  3.3696e-02, -6.7059e-03,  1.4441e-02,  1.3973e-01,\n",
      "         7.2340e-02, -5.2067e-02, -1.5580e-02,  4.3312e-02, -6.7398e-02,\n",
      "         7.6808e-02, -4.1142e-02,  3.2319e-02,  1.2461e-01,  1.5610e-02,\n",
      "         7.3369e-02, -1.0851e-01, -4.5686e-02, -6.5544e-02,  7.0161e-02,\n",
      "        -4.9590e-03,  4.6399e-02,  4.5816e-02, -7.6833e-02,  5.7388e-02,\n",
      "         5.6216e-02,  1.7794e-02, -1.8920e-02, -4.4150e-02,  2.6347e-02,\n",
      "         8.7239e-02, -2.0536e-02, -1.2006e-02, -5.0354e-03,  3.5649e-02,\n",
      "        -8.1056e-02,  5.1311e-02,  1.9925e-02, -4.3425e-02,  2.6601e-02,\n",
      "        -7.5502e-02, -3.4638e-02, -7.5277e-02, -5.1211e-02, -4.9907e-02,\n",
      "         1.9271e-02,  2.3710e-02,  1.7192e-02, -7.7708e-02,  2.5729e-02,\n",
      "         5.5325e-02,  1.0182e-01, -9.2568e-02, -4.8824e-02,  2.3749e-02,\n",
      "         3.6623e-02, -1.6246e-02, -2.5600e-02, -8.7405e-02,  1.7550e-02,\n",
      "        -6.1699e-03, -4.0138e-02, -3.5954e-02, -6.4890e-02,  4.1684e-03,\n",
      "        -8.0014e-02, -7.6652e-02,  9.0478e-02,  2.2696e-03,  4.3178e-03,\n",
      "         1.3625e-01, -4.3848e-02,  3.4243e-02,  1.0695e-01,  2.2553e-02,\n",
      "        -1.3336e-02, -3.6943e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([-6.3037e-02,  4.0266e-02, -4.1982e-02, -1.3677e-01, -2.8125e-02,\n",
      "        -3.5907e-02, -1.2441e-01,  3.9413e-02,  7.0257e-02,  2.5938e-02,\n",
      "         1.6168e-02, -4.9772e-03, -8.1783e-02,  5.8682e-02, -5.5678e-03,\n",
      "        -1.6609e-02, -7.0250e-02,  2.3007e-02, -1.1237e-01, -1.1362e-03,\n",
      "         2.6813e-02,  5.1206e-02, -1.0730e-01, -7.5571e-03,  6.9360e-02,\n",
      "        -3.0615e-02, -1.3997e-01, -1.6990e-02, -6.2863e-03,  3.9499e-02,\n",
      "         1.1053e-01,  7.3368e-02, -4.2755e-03, -6.1965e-02,  2.3791e-02,\n",
      "        -5.2535e-02, -2.8180e-02, -1.5272e-02, -2.4229e-02,  8.8722e-02,\n",
      "        -9.6024e-02,  4.9923e-02, -1.7209e-02,  1.6388e-02, -7.5840e-03,\n",
      "        -8.1901e-02,  6.1073e-02, -4.8348e-02, -1.0459e-02, -6.6470e-02,\n",
      "        -2.4781e-02,  5.3203e-02,  2.4020e-02,  4.9423e-02,  3.7947e-02,\n",
      "         2.0673e-01,  6.0895e-02, -4.0758e-02,  2.0071e-02,  1.0387e-01,\n",
      "         1.6163e-02, -1.5090e-02,  6.2854e-02,  2.6011e-02,  3.8561e-02,\n",
      "        -5.5162e-02, -7.5800e-02,  6.2836e-02,  1.3994e-02,  1.1939e-01,\n",
      "        -1.5908e-02,  4.2999e-02, -2.3383e-02,  1.5983e-02, -1.3671e-02,\n",
      "         9.9919e-02,  6.4056e-02, -7.5082e-02, -1.2190e-02, -1.4694e-02,\n",
      "         5.8069e-02,  5.5209e-02,  1.2079e-02,  5.1134e-02,  4.5578e-02,\n",
      "         5.2929e-02, -6.7412e-03,  9.7755e-02, -4.7786e-02, -1.6850e-02,\n",
      "        -5.9766e-02,  9.2122e-02, -2.9754e-02, -1.1698e-01,  2.3706e-02,\n",
      "        -2.3814e-02,  3.3031e-02, -1.1580e-01,  3.8596e-02, -3.3136e-03,\n",
      "        -1.2250e-02,  3.4611e-02, -8.4193e-02, -7.5750e-02, -3.5521e-02,\n",
      "        -5.1473e-03,  3.9007e-02,  6.4325e-03, -5.9280e-02, -1.3100e-02,\n",
      "        -4.1139e-02,  4.7848e-02,  8.4264e-03, -1.0753e-01, -4.3760e-02,\n",
      "         9.4994e-02, -1.7219e-02,  3.9596e-02, -4.1659e-02,  1.2531e-01,\n",
      "        -4.9070e-02,  1.2569e-02, -3.4510e-02,  5.6004e-02, -2.7773e-02,\n",
      "        -8.0413e-02,  7.7013e-02, -3.7365e-02, -7.8601e-02, -4.4590e-02,\n",
      "         1.6158e-02,  2.7064e-02,  1.0510e-01, -1.2408e-02,  1.6963e-02,\n",
      "        -9.1978e-03,  5.7486e-02, -4.6821e-02, -3.0573e-03, -1.0964e-02,\n",
      "        -8.9452e-02,  4.2682e-02, -1.1941e-02,  2.5132e-02, -3.7705e-02,\n",
      "         5.4186e-02, -7.1975e-02, -4.9173e-02, -6.7192e-02,  2.7494e-02,\n",
      "         2.4167e-03,  3.7371e-02,  4.2284e-02,  3.3118e-02,  5.1909e-02,\n",
      "        -6.6921e-02, -5.8869e-02, -6.1932e-02,  3.1455e-02, -2.2885e-02,\n",
      "        -9.3647e-02, -1.9637e-02,  5.1098e-02,  4.5610e-02, -4.1068e-02,\n",
      "         5.7816e-02, -8.5963e-04,  2.2186e-02, -1.8173e-02,  4.3025e-02,\n",
      "        -3.6500e-02,  4.6611e-02,  1.1417e-01, -6.0109e-02, -6.6532e-02,\n",
      "         9.2543e-02,  1.5739e-02, -7.0260e-03, -4.5298e-02, -4.6085e-02,\n",
      "        -1.7641e-02, -3.4245e-02, -2.9982e-02, -3.3564e-02, -2.3251e-02,\n",
      "        -9.0132e-02, -4.9113e-02, -1.5003e-02, -3.4544e-02, -1.2240e-02,\n",
      "        -6.6013e-02, -1.2225e-01,  2.1974e-02, -7.2869e-02,  7.3213e-02,\n",
      "         7.8171e-02, -1.1407e-02,  1.2900e-02,  1.3423e-02,  6.1885e-02,\n",
      "         8.2777e-02,  5.9639e-03, -2.9608e-02,  1.4335e-02, -3.0911e-02,\n",
      "        -2.6568e-02, -7.7970e-02,  5.7262e-02,  7.5148e-03, -8.3736e-02,\n",
      "         1.1164e-01, -3.6595e-02, -3.5647e-02, -2.2155e-02,  3.7071e-02,\n",
      "         5.2191e-03, -5.0187e-02, -1.0465e-02, -2.7389e-02,  2.4710e-02,\n",
      "         3.4442e-02, -3.3596e-02,  5.7857e-02,  4.2296e-02, -2.8121e-02,\n",
      "         3.7366e-02, -5.9914e-02,  1.6653e-02,  3.8050e-02,  5.3976e-02,\n",
      "         1.6561e-02,  5.0949e-02,  7.7352e-02,  8.3561e-02, -4.3670e-02,\n",
      "        -8.8957e-03,  2.0743e-03,  3.0768e-02, -3.4656e-02,  1.0132e-01,\n",
      "         2.0802e-02, -1.4734e-01, -1.1625e-02, -4.6762e-03,  1.0868e-01,\n",
      "         8.2071e-02, -1.4927e-02, -1.5449e-01, -7.1360e-02,  6.3504e-02,\n",
      "        -1.3678e-02, -3.2650e-02,  8.5200e-02, -4.5086e-02,  2.2611e-02,\n",
      "        -1.0392e-01, -6.0944e-02,  1.4738e-02,  3.9227e-02, -8.7592e-03,\n",
      "        -2.2234e-02, -5.5263e-03, -3.3027e-02,  3.9625e-03,  1.5417e-02,\n",
      "         1.2909e-02,  1.0592e-01, -5.5637e-02,  1.6255e-01, -8.2178e-02,\n",
      "         9.2043e-02,  1.9381e-03,  2.2714e-02,  3.5822e-02, -1.0901e-03,\n",
      "         1.2325e-02, -6.4859e-02, -2.5885e-02,  5.1314e-02, -4.6941e-04,\n",
      "        -2.8895e-03,  1.1293e-02, -1.7513e-02, -6.6949e-02,  6.9416e-02,\n",
      "         7.1142e-03, -1.4641e-03, -3.6779e-02,  1.1385e-01, -4.7641e-02,\n",
      "         1.4738e-02, -6.2718e-02,  8.7415e-02, -5.5629e-03,  2.7129e-02,\n",
      "         6.3722e-03,  3.4799e-02,  2.5760e-02, -7.6286e-02, -6.1321e-02,\n",
      "        -5.3081e-02, -1.3048e-03, -1.7442e-02, -1.6667e-01,  3.7299e-03,\n",
      "         1.3328e-02,  6.2362e-02,  1.6265e-02,  5.9280e-02, -9.6899e-02,\n",
      "        -9.9530e-03,  3.4732e-02,  5.4185e-03, -4.3835e-03,  3.4801e-02,\n",
      "         4.0341e-02, -1.1303e-02, -2.8805e-02,  2.6510e-02, -4.8988e-02,\n",
      "        -1.4906e-02, -8.7503e-02, -3.8591e-03,  3.9093e-02,  2.1345e-02,\n",
      "         4.3803e-02, -4.8825e-02, -3.8691e-02, -7.1864e-02, -5.9994e-02,\n",
      "         2.5898e-02, -4.4769e-02,  8.8324e-02, -7.2772e-02,  1.5155e-02,\n",
      "        -5.5817e-02,  5.3736e-02, -2.9101e-02,  1.5793e-03, -1.7930e-01,\n",
      "        -1.7445e-02, -6.8678e-02, -2.1378e-02, -4.4950e-02, -1.7106e-02,\n",
      "         1.5411e-01,  7.0336e-02,  3.1394e-02,  9.1400e-02, -6.3379e-02,\n",
      "         9.3097e-02,  6.2873e-02, -2.3895e-02,  4.8823e-02,  1.5050e-02,\n",
      "         1.5749e-01,  2.0483e-02,  2.5478e-02,  1.2565e-01,  6.4963e-02,\n",
      "        -3.3720e-02,  3.8453e-02, -6.7775e-02, -1.1753e-01,  6.8093e-02,\n",
      "         5.1249e-02, -1.5064e-01, -6.5369e-02,  4.8224e-02, -8.1458e-03,\n",
      "        -2.7762e-02, -2.5249e-02, -1.0149e-02, -1.9384e-02,  4.1005e-02,\n",
      "        -2.7609e-02, -9.2976e-02,  3.8276e-02,  7.2089e-02, -1.2936e-01,\n",
      "        -1.1778e-01, -6.5505e-02,  1.7166e-02,  1.5751e-02, -1.9162e-02,\n",
      "         5.5185e-03, -1.0558e-01, -2.3025e-02, -1.4394e-01,  1.1885e-01,\n",
      "         9.7875e-03, -9.7859e-02, -3.9622e-02, -4.5969e-02, -4.3369e-02,\n",
      "        -2.5617e-02, -5.2712e-02,  3.9468e-02,  1.0800e-01,  5.3185e-02,\n",
      "        -5.0451e-02,  5.3125e-02,  1.4214e-01,  1.0340e-01, -1.7702e-02,\n",
      "        -6.3901e-02,  3.0720e-02, -7.3908e-02,  9.5226e-02,  6.2002e-03,\n",
      "         5.0914e-02, -6.5561e-02,  5.4568e-02,  5.1027e-02, -4.2785e-02,\n",
      "        -7.9318e-02,  6.1157e-02,  6.2453e-02, -4.5603e-02, -2.7345e-02,\n",
      "        -5.6974e-02,  1.2981e-01,  1.0213e-01,  4.7302e-02, -2.4651e-02,\n",
      "        -3.3669e-02, -4.9926e-02,  7.3012e-02, -3.4709e-02,  1.2907e-01,\n",
      "         6.1702e-02,  3.1375e-02,  1.9113e-02, -9.1100e-02,  7.3931e-03,\n",
      "        -8.0293e-02, -3.6101e-02,  5.1210e-02, -2.9621e-02,  5.9973e-03,\n",
      "         9.6392e-02,  4.1492e-03,  2.3054e-02, -8.5028e-02,  1.3075e-03,\n",
      "        -8.0786e-02,  7.1889e-02, -3.7784e-02,  2.0823e-02, -5.1179e-02,\n",
      "         1.3547e-01,  4.0677e-02,  6.0206e-02, -4.4987e-03, -1.4705e-02,\n",
      "        -4.3924e-05,  2.2686e-02, -3.4385e-02, -3.4656e-02, -1.7687e-01,\n",
      "         4.2150e-02, -3.2622e-03, -4.4221e-02, -5.9327e-02, -1.2178e-01,\n",
      "        -9.8243e-02,  2.9285e-02,  1.0800e-01, -2.9136e-02, -1.2633e-02,\n",
      "        -8.9605e-02,  1.0191e-02,  2.9528e-02, -1.6184e-02, -2.1323e-02,\n",
      "         4.3191e-02, -5.9493e-02, -9.2964e-02, -2.2478e-02,  1.4769e-02,\n",
      "        -2.1768e-02,  4.5379e-02,  7.2459e-02,  6.8969e-02, -3.1864e-02,\n",
      "         9.2427e-03,  1.1675e-01, -1.2651e-02, -8.6167e-02, -7.0927e-02,\n",
      "        -3.2216e-02, -3.6091e-02,  1.1292e-02, -2.3667e-02,  1.0530e-01,\n",
      "        -2.7349e-02,  3.5006e-02, -8.4804e-02,  5.3443e-02, -3.9848e-02,\n",
      "        -1.8628e-02, -9.9607e-02, -1.0862e-01,  3.0266e-03,  6.8604e-02,\n",
      "        -2.6277e-02, -1.4869e-01, -4.6595e-02, -9.2243e-02,  5.9800e-02,\n",
      "         1.6846e-02, -5.4605e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0050000'], tr/val_loss: 30.250719/1019.005859, val:  39.17%, val_best:  39.17%, tr:  35.04%, tr_best:  35.04%, epoch time: 28.62 seconds, 0.48 minutes\n",
      "[module.layers.3] weight_fb parameter count: 5,120\n",
      "epoch-1   lr=['0.0050000'], tr/val_loss: 28.469086/987.139771, val:  34.17%, val_best:  39.17%, tr:  45.56%, tr_best:  45.56%, epoch time: 27.55 seconds, 0.46 minutes\n",
      "epoch-2   lr=['0.0050000'], tr/val_loss: 27.573299/582.853271, val:  51.25%, val_best:  51.25%, tr:  48.72%, tr_best:  48.72%, epoch time: 29.52 seconds, 0.49 minutes\n",
      "epoch-3   lr=['0.0050000'], tr/val_loss: 26.145008/1053.512695, val:  47.08%, val_best:  51.25%, tr:  54.14%, tr_best:  54.14%, epoch time: 28.29 seconds, 0.47 minutes\n",
      "epoch-4   lr=['0.0050000'], tr/val_loss: 23.536968/792.762512, val:  53.33%, val_best:  53.33%, tr:  57.20%, tr_best:  57.20%, epoch time: 27.16 seconds, 0.45 minutes\n",
      "epoch-5   lr=['0.0050000'], tr/val_loss: 24.674610/1132.913452, val:  47.50%, val_best:  53.33%, tr:  57.41%, tr_best:  57.41%, epoch time: 27.79 seconds, 0.46 minutes\n",
      "epoch-6   lr=['0.0050000'], tr/val_loss: 23.415289/1061.425293, val:  50.83%, val_best:  53.33%, tr:  59.55%, tr_best:  59.55%, epoch time: 27.56 seconds, 0.46 minutes\n",
      "epoch-7   lr=['0.0050000'], tr/val_loss: 21.415024/754.281555, val:  55.42%, val_best:  55.42%, tr:  60.78%, tr_best:  60.78%, epoch time: 27.83 seconds, 0.46 minutes\n",
      "epoch-8   lr=['0.0050000'], tr/val_loss: 22.448072/655.080872, val:  59.17%, val_best:  59.17%, tr:  61.39%, tr_best:  61.39%, epoch time: 27.82 seconds, 0.46 minutes\n",
      "epoch-9   lr=['0.0050000'], tr/val_loss: 21.917269/1034.686035, val:  50.83%, val_best:  59.17%, tr:  63.43%, tr_best:  63.43%, epoch time: 28.27 seconds, 0.47 minutes\n",
      "epoch-10  lr=['0.0050000'], tr/val_loss: 20.655952/964.647156, val:  47.50%, val_best:  59.17%, tr:  64.25%, tr_best:  64.25%, epoch time: 27.43 seconds, 0.46 minutes\n",
      "epoch-11  lr=['0.0050000'], tr/val_loss: 21.375870/726.494202, val:  55.83%, val_best:  59.17%, tr:  64.76%, tr_best:  64.76%, epoch time: 27.72 seconds, 0.46 minutes\n",
      "epoch-12  lr=['0.0050000'], tr/val_loss: 20.578169/796.046204, val:  56.25%, val_best:  59.17%, tr:  66.70%, tr_best:  66.70%, epoch time: 28.35 seconds, 0.47 minutes\n",
      "epoch-13  lr=['0.0050000'], tr/val_loss: 19.663202/847.329468, val:  48.33%, val_best:  59.17%, tr:  65.47%, tr_best:  66.70%, epoch time: 27.90 seconds, 0.47 minutes\n",
      "epoch-14  lr=['0.0050000'], tr/val_loss: 18.659695/1116.020386, val:  58.33%, val_best:  59.17%, tr:  66.39%, tr_best:  66.70%, epoch time: 28.07 seconds, 0.47 minutes\n",
      "epoch-15  lr=['0.0050000'], tr/val_loss: 21.138973/629.682373, val:  50.42%, val_best:  59.17%, tr:  66.19%, tr_best:  66.70%, epoch time: 28.17 seconds, 0.47 minutes\n",
      "epoch-16  lr=['0.0050000'], tr/val_loss: 19.642435/819.273743, val:  57.50%, val_best:  59.17%, tr:  66.50%, tr_best:  66.70%, epoch time: 27.20 seconds, 0.45 minutes\n",
      "epoch-17  lr=['0.0050000'], tr/val_loss: 18.679659/533.028442, val:  61.25%, val_best:  61.25%, tr:  68.74%, tr_best:  68.74%, epoch time: 28.72 seconds, 0.48 minutes\n",
      "epoch-18  lr=['0.0050000'], tr/val_loss: 17.258209/825.738586, val:  55.00%, val_best:  61.25%, tr:  70.17%, tr_best:  70.17%, epoch time: 28.02 seconds, 0.47 minutes\n",
      "epoch-19  lr=['0.0050000'], tr/val_loss: 18.331699/774.804932, val:  57.08%, val_best:  61.25%, tr:  68.54%, tr_best:  70.17%, epoch time: 27.85 seconds, 0.46 minutes\n",
      "epoch-20  lr=['0.0050000'], tr/val_loss: 16.809727/1263.480103, val:  54.17%, val_best:  61.25%, tr:  70.89%, tr_best:  70.89%, epoch time: 27.50 seconds, 0.46 minutes\n",
      "epoch-21  lr=['0.0050000'], tr/val_loss: 18.444525/874.332764, val:  58.75%, val_best:  61.25%, tr:  68.64%, tr_best:  70.89%, epoch time: 27.61 seconds, 0.46 minutes\n",
      "epoch-22  lr=['0.0050000'], tr/val_loss: 17.599144/976.995239, val:  55.42%, val_best:  61.25%, tr:  70.17%, tr_best:  70.89%, epoch time: 27.11 seconds, 0.45 minutes\n",
      "epoch-23  lr=['0.0050000'], tr/val_loss: 16.573133/561.988403, val:  61.25%, val_best:  61.25%, tr:  67.72%, tr_best:  70.89%, epoch time: 27.50 seconds, 0.46 minutes\n",
      "epoch-24  lr=['0.0050000'], tr/val_loss: 17.410528/559.050476, val:  67.08%, val_best:  67.08%, tr:  68.74%, tr_best:  70.89%, epoch time: 27.64 seconds, 0.46 minutes\n",
      "epoch-25  lr=['0.0050000'], tr/val_loss: 18.228165/815.946899, val:  57.50%, val_best:  67.08%, tr:  68.85%, tr_best:  70.89%, epoch time: 27.89 seconds, 0.46 minutes\n",
      "epoch-26  lr=['0.0050000'], tr/val_loss: 15.863974/915.650818, val:  63.33%, val_best:  67.08%, tr:  71.30%, tr_best:  71.30%, epoch time: 27.98 seconds, 0.47 minutes\n",
      "epoch-27  lr=['0.0050000'], tr/val_loss: 15.744777/938.216064, val:  55.00%, val_best:  67.08%, tr:  71.60%, tr_best:  71.60%, epoch time: 27.96 seconds, 0.47 minutes\n",
      "epoch-28  lr=['0.0050000'], tr/val_loss: 15.255530/731.473022, val:  58.33%, val_best:  67.08%, tr:  73.75%, tr_best:  73.75%, epoch time: 27.17 seconds, 0.45 minutes\n",
      "epoch-29  lr=['0.0050000'], tr/val_loss: 15.354049/672.541321, val:  64.17%, val_best:  67.08%, tr:  72.73%, tr_best:  73.75%, epoch time: 27.39 seconds, 0.46 minutes\n",
      "epoch-30  lr=['0.0050000'], tr/val_loss: 14.523935/697.484314, val:  63.33%, val_best:  67.08%, tr:  73.85%, tr_best:  73.85%, epoch time: 27.65 seconds, 0.46 minutes\n",
      "epoch-31  lr=['0.0050000'], tr/val_loss: 15.518348/760.394958, val:  62.08%, val_best:  67.08%, tr:  72.11%, tr_best:  73.85%, epoch time: 26.57 seconds, 0.44 minutes\n",
      "epoch-32  lr=['0.0050000'], tr/val_loss: 14.738527/1087.805420, val:  54.17%, val_best:  67.08%, tr:  72.93%, tr_best:  73.85%, epoch time: 27.84 seconds, 0.46 minutes\n",
      "epoch-33  lr=['0.0050000'], tr/val_loss: 15.049126/649.988953, val:  57.50%, val_best:  67.08%, tr:  74.26%, tr_best:  74.26%, epoch time: 26.53 seconds, 0.44 minutes\n",
      "epoch-34  lr=['0.0050000'], tr/val_loss: 14.730718/754.972717, val:  58.75%, val_best:  67.08%, tr:  74.16%, tr_best:  74.26%, epoch time: 27.98 seconds, 0.47 minutes\n",
      "epoch-35  lr=['0.0050000'], tr/val_loss: 15.129428/845.319031, val:  60.83%, val_best:  67.08%, tr:  74.97%, tr_best:  74.97%, epoch time: 27.75 seconds, 0.46 minutes\n",
      "epoch-36  lr=['0.0050000'], tr/val_loss: 13.240026/681.871338, val:  62.08%, val_best:  67.08%, tr:  76.00%, tr_best:  76.00%, epoch time: 27.50 seconds, 0.46 minutes\n",
      "epoch-37  lr=['0.0050000'], tr/val_loss: 12.046452/875.770447, val:  63.33%, val_best:  67.08%, tr:  77.83%, tr_best:  77.83%, epoch time: 27.50 seconds, 0.46 minutes\n",
      "epoch-38  lr=['0.0050000'], tr/val_loss: 13.990516/600.643250, val:  63.75%, val_best:  67.08%, tr:  74.67%, tr_best:  77.83%, epoch time: 27.51 seconds, 0.46 minutes\n",
      "epoch-39  lr=['0.0050000'], tr/val_loss: 13.930386/584.599915, val:  63.75%, val_best:  67.08%, tr:  74.16%, tr_best:  77.83%, epoch time: 28.06 seconds, 0.47 minutes\n",
      "epoch-40  lr=['0.0050000'], tr/val_loss: 14.278496/1101.681274, val:  60.83%, val_best:  67.08%, tr:  76.10%, tr_best:  77.83%, epoch time: 27.55 seconds, 0.46 minutes\n",
      "epoch-41  lr=['0.0050000'], tr/val_loss: 15.740520/761.059875, val:  62.50%, val_best:  67.08%, tr:  73.65%, tr_best:  77.83%, epoch time: 27.62 seconds, 0.46 minutes\n",
      "epoch-42  lr=['0.0050000'], tr/val_loss: 15.375414/543.408752, val:  64.58%, val_best:  67.08%, tr:  74.87%, tr_best:  77.83%, epoch time: 30.37 seconds, 0.51 minutes\n",
      "epoch-43  lr=['0.0050000'], tr/val_loss: 13.928120/494.622589, val:  65.83%, val_best:  67.08%, tr:  76.10%, tr_best:  77.83%, epoch time: 27.20 seconds, 0.45 minutes\n",
      "epoch-44  lr=['0.0050000'], tr/val_loss: 13.143402/654.546570, val:  67.50%, val_best:  67.50%, tr:  75.79%, tr_best:  77.83%, epoch time: 28.86 seconds, 0.48 minutes\n",
      "epoch-45  lr=['0.0050000'], tr/val_loss: 15.060611/628.990662, val:  63.33%, val_best:  67.50%, tr:  75.38%, tr_best:  77.83%, epoch time: 28.07 seconds, 0.47 minutes\n",
      "epoch-46  lr=['0.0050000'], tr/val_loss: 13.361470/721.809387, val:  67.50%, val_best:  67.50%, tr:  76.40%, tr_best:  77.83%, epoch time: 28.24 seconds, 0.47 minutes\n",
      "epoch-47  lr=['0.0050000'], tr/val_loss: 14.010287/853.044128, val:  59.17%, val_best:  67.50%, tr:  77.02%, tr_best:  77.83%, epoch time: 27.19 seconds, 0.45 minutes\n",
      "epoch-48  lr=['0.0050000'], tr/val_loss: 13.676884/466.033386, val:  70.42%, val_best:  70.42%, tr:  75.08%, tr_best:  77.83%, epoch time: 29.29 seconds, 0.49 minutes\n",
      "epoch-49  lr=['0.0050000'], tr/val_loss: 12.266401/683.793274, val:  63.33%, val_best:  70.42%, tr:  77.32%, tr_best:  77.83%, epoch time: 26.83 seconds, 0.45 minutes\n",
      "epoch-50  lr=['0.0050000'], tr/val_loss: 14.261554/726.525879, val:  64.17%, val_best:  70.42%, tr:  73.54%, tr_best:  77.83%, epoch time: 28.62 seconds, 0.48 minutes\n",
      "epoch-51  lr=['0.0050000'], tr/val_loss: 13.823754/726.993958, val:  65.83%, val_best:  70.42%, tr:  75.69%, tr_best:  77.83%, epoch time: 27.16 seconds, 0.45 minutes\n",
      "epoch-52  lr=['0.0050000'], tr/val_loss: 13.345764/647.258423, val:  62.50%, val_best:  70.42%, tr:  76.40%, tr_best:  77.83%, epoch time: 27.82 seconds, 0.46 minutes\n",
      "epoch-53  lr=['0.0050000'], tr/val_loss: 12.389524/701.483459, val:  62.50%, val_best:  70.42%, tr:  77.83%, tr_best:  77.83%, epoch time: 28.06 seconds, 0.47 minutes\n",
      "epoch-54  lr=['0.0050000'], tr/val_loss: 12.020923/758.848328, val:  65.00%, val_best:  70.42%, tr:  76.61%, tr_best:  77.83%, epoch time: 28.25 seconds, 0.47 minutes\n",
      "epoch-55  lr=['0.0050000'], tr/val_loss: 13.760791/815.578491, val:  57.50%, val_best:  70.42%, tr:  76.20%, tr_best:  77.83%, epoch time: 27.45 seconds, 0.46 minutes\n",
      "epoch-56  lr=['0.0050000'], tr/val_loss: 12.500860/1193.155640, val:  57.50%, val_best:  70.42%, tr:  77.73%, tr_best:  77.83%, epoch time: 28.07 seconds, 0.47 minutes\n",
      "epoch-57  lr=['0.0050000'], tr/val_loss: 12.482247/810.851318, val:  67.92%, val_best:  70.42%, tr:  79.57%, tr_best:  79.57%, epoch time: 29.33 seconds, 0.49 minutes\n",
      "epoch-58  lr=['0.0050000'], tr/val_loss: 12.600729/853.648193, val:  60.00%, val_best:  70.42%, tr:  78.55%, tr_best:  79.57%, epoch time: 28.51 seconds, 0.48 minutes\n",
      "epoch-59  lr=['0.0050000'], tr/val_loss: 12.508814/1088.187622, val:  57.50%, val_best:  70.42%, tr:  78.04%, tr_best:  79.57%, epoch time: 27.85 seconds, 0.46 minutes\n",
      "epoch-60  lr=['0.0050000'], tr/val_loss: 12.859813/913.990662, val:  61.25%, val_best:  70.42%, tr:  77.94%, tr_best:  79.57%, epoch time: 27.96 seconds, 0.47 minutes\n",
      "epoch-61  lr=['0.0050000'], tr/val_loss: 13.405236/726.680847, val:  65.83%, val_best:  70.42%, tr:  76.61%, tr_best:  79.57%, epoch time: 27.89 seconds, 0.46 minutes\n",
      "epoch-62  lr=['0.0050000'], tr/val_loss: 12.277648/1073.589600, val:  60.42%, val_best:  70.42%, tr:  78.55%, tr_best:  79.57%, epoch time: 27.72 seconds, 0.46 minutes\n",
      "epoch-63  lr=['0.0050000'], tr/val_loss: 11.581816/684.529907, val:  64.58%, val_best:  70.42%, tr:  79.57%, tr_best:  79.57%, epoch time: 28.67 seconds, 0.48 minutes\n",
      "epoch-64  lr=['0.0050000'], tr/val_loss: 11.463552/853.138428, val:  67.92%, val_best:  70.42%, tr:  80.08%, tr_best:  80.08%, epoch time: 28.38 seconds, 0.47 minutes\n",
      "epoch-65  lr=['0.0050000'], tr/val_loss: 11.457173/752.069946, val:  65.83%, val_best:  70.42%, tr:  79.37%, tr_best:  80.08%, epoch time: 28.43 seconds, 0.47 minutes\n",
      "epoch-66  lr=['0.0050000'], tr/val_loss: 10.884335/563.960327, val:  65.00%, val_best:  70.42%, tr:  79.37%, tr_best:  80.08%, epoch time: 27.63 seconds, 0.46 minutes\n",
      "epoch-67  lr=['0.0050000'], tr/val_loss: 10.509014/799.911743, val:  64.17%, val_best:  70.42%, tr:  80.29%, tr_best:  80.29%, epoch time: 27.87 seconds, 0.46 minutes\n",
      "epoch-68  lr=['0.0050000'], tr/val_loss: 10.932777/808.203125, val:  65.42%, val_best:  70.42%, tr:  80.49%, tr_best:  80.49%, epoch time: 27.31 seconds, 0.46 minutes\n",
      "epoch-69  lr=['0.0050000'], tr/val_loss: 10.796235/614.604248, val:  67.08%, val_best:  70.42%, tr:  82.33%, tr_best:  82.33%, epoch time: 27.69 seconds, 0.46 minutes\n",
      "epoch-70  lr=['0.0050000'], tr/val_loss: 11.154952/682.088684, val:  63.33%, val_best:  70.42%, tr:  80.80%, tr_best:  82.33%, epoch time: 27.42 seconds, 0.46 minutes\n",
      "epoch-71  lr=['0.0050000'], tr/val_loss: 12.092105/968.783325, val:  63.33%, val_best:  70.42%, tr:  77.43%, tr_best:  82.33%, epoch time: 28.04 seconds, 0.47 minutes\n",
      "epoch-72  lr=['0.0050000'], tr/val_loss: 11.508022/731.384460, val:  68.33%, val_best:  70.42%, tr:  79.26%, tr_best:  82.33%, epoch time: 28.11 seconds, 0.47 minutes\n",
      "epoch-73  lr=['0.0050000'], tr/val_loss: 10.482441/758.858826, val:  65.83%, val_best:  70.42%, tr:  79.88%, tr_best:  82.33%, epoch time: 27.69 seconds, 0.46 minutes\n",
      "epoch-74  lr=['0.0050000'], tr/val_loss: 11.363926/1389.829956, val:  59.58%, val_best:  70.42%, tr:  80.18%, tr_best:  82.33%, epoch time: 29.28 seconds, 0.49 minutes\n",
      "epoch-75  lr=['0.0050000'], tr/val_loss: 10.449583/559.308289, val:  67.50%, val_best:  70.42%, tr:  80.29%, tr_best:  82.33%, epoch time: 28.85 seconds, 0.48 minutes\n",
      "epoch-76  lr=['0.0050000'], tr/val_loss: 11.997765/878.595520, val:  67.08%, val_best:  70.42%, tr:  78.86%, tr_best:  82.33%, epoch time: 27.77 seconds, 0.46 minutes\n",
      "epoch-77  lr=['0.0050000'], tr/val_loss: 10.322760/1110.043701, val:  60.83%, val_best:  70.42%, tr:  82.23%, tr_best:  82.33%, epoch time: 27.25 seconds, 0.45 minutes\n",
      "epoch-78  lr=['0.0050000'], tr/val_loss: 10.694581/883.905640, val:  62.08%, val_best:  70.42%, tr:  80.49%, tr_best:  82.33%, epoch time: 29.05 seconds, 0.48 minutes\n",
      "epoch-79  lr=['0.0050000'], tr/val_loss: 11.761108/845.190063, val:  66.25%, val_best:  70.42%, tr:  79.16%, tr_best:  82.33%, epoch time: 27.67 seconds, 0.46 minutes\n",
      "epoch-80  lr=['0.0050000'], tr/val_loss: 10.511318/753.635864, val:  67.08%, val_best:  70.42%, tr:  80.69%, tr_best:  82.33%, epoch time: 27.60 seconds, 0.46 minutes\n",
      "epoch-81  lr=['0.0050000'], tr/val_loss:  9.589116/1116.169434, val:  57.50%, val_best:  70.42%, tr:  82.64%, tr_best:  82.64%, epoch time: 27.18 seconds, 0.45 minutes\n",
      "epoch-82  lr=['0.0050000'], tr/val_loss: 11.671824/910.778015, val:  64.58%, val_best:  70.42%, tr:  79.88%, tr_best:  82.64%, epoch time: 27.53 seconds, 0.46 minutes\n",
      "epoch-83  lr=['0.0050000'], tr/val_loss: 11.597087/799.868896, val:  61.67%, val_best:  70.42%, tr:  80.49%, tr_best:  82.64%, epoch time: 26.66 seconds, 0.44 minutes\n",
      "epoch-84  lr=['0.0050000'], tr/val_loss:  9.791612/667.462036, val:  64.17%, val_best:  70.42%, tr:  82.02%, tr_best:  82.64%, epoch time: 27.81 seconds, 0.46 minutes\n",
      "epoch-85  lr=['0.0050000'], tr/val_loss: 10.268604/889.352234, val:  60.42%, val_best:  70.42%, tr:  80.49%, tr_best:  82.64%, epoch time: 27.18 seconds, 0.45 minutes\n",
      "epoch-86  lr=['0.0050000'], tr/val_loss: 10.417368/872.495422, val:  64.17%, val_best:  70.42%, tr:  82.23%, tr_best:  82.64%, epoch time: 28.63 seconds, 0.48 minutes\n",
      "epoch-87  lr=['0.0050000'], tr/val_loss: 10.925367/570.387085, val:  72.08%, val_best:  72.08%, tr:  80.08%, tr_best:  82.64%, epoch time: 27.46 seconds, 0.46 minutes\n",
      "epoch-88  lr=['0.0050000'], tr/val_loss:  9.455591/769.644287, val:  65.00%, val_best:  72.08%, tr:  82.84%, tr_best:  82.84%, epoch time: 27.38 seconds, 0.46 minutes\n",
      "epoch-89  lr=['0.0050000'], tr/val_loss:  9.896793/698.816528, val:  65.00%, val_best:  72.08%, tr:  81.31%, tr_best:  82.84%, epoch time: 29.62 seconds, 0.49 minutes\n",
      "epoch-90  lr=['0.0050000'], tr/val_loss:  9.721884/715.715576, val:  70.42%, val_best:  72.08%, tr:  80.59%, tr_best:  82.84%, epoch time: 28.88 seconds, 0.48 minutes\n",
      "epoch-91  lr=['0.0050000'], tr/val_loss:  9.848625/936.391663, val:  62.92%, val_best:  72.08%, tr:  82.33%, tr_best:  82.84%, epoch time: 27.97 seconds, 0.47 minutes\n",
      "epoch-92  lr=['0.0050000'], tr/val_loss: 10.231650/735.214111, val:  69.58%, val_best:  72.08%, tr:  83.66%, tr_best:  83.66%, epoch time: 29.41 seconds, 0.49 minutes\n",
      "epoch-93  lr=['0.0050000'], tr/val_loss:  9.390401/592.105957, val:  70.00%, val_best:  72.08%, tr:  81.82%, tr_best:  83.66%, epoch time: 29.89 seconds, 0.50 minutes\n",
      "epoch-94  lr=['0.0050000'], tr/val_loss:  9.398078/950.159851, val:  63.33%, val_best:  72.08%, tr:  83.04%, tr_best:  83.66%, epoch time: 28.15 seconds, 0.47 minutes\n",
      "epoch-95  lr=['0.0050000'], tr/val_loss: 10.139464/821.911682, val:  63.75%, val_best:  72.08%, tr:  82.94%, tr_best:  83.66%, epoch time: 27.28 seconds, 0.45 minutes\n",
      "epoch-96  lr=['0.0050000'], tr/val_loss: 10.904239/1023.349365, val:  60.42%, val_best:  72.08%, tr:  81.10%, tr_best:  83.66%, epoch time: 27.47 seconds, 0.46 minutes\n",
      "epoch-97  lr=['0.0050000'], tr/val_loss:  9.709602/831.742065, val:  65.00%, val_best:  72.08%, tr:  82.84%, tr_best:  83.66%, epoch time: 28.03 seconds, 0.47 minutes\n",
      "epoch-98  lr=['0.0050000'], tr/val_loss:  8.430292/833.054016, val:  65.42%, val_best:  72.08%, tr:  84.58%, tr_best:  84.58%, epoch time: 27.66 seconds, 0.46 minutes\n",
      "epoch-99  lr=['0.0050000'], tr/val_loss:  8.569901/924.196594, val:  63.75%, val_best:  72.08%, tr:  84.78%, tr_best:  84.78%, epoch time: 27.97 seconds, 0.47 minutes\n",
      "epoch-100 lr=['0.0050000'], tr/val_loss: 10.118011/659.541199, val:  66.25%, val_best:  72.08%, tr:  81.92%, tr_best:  84.78%, epoch time: 27.22 seconds, 0.45 minutes\n",
      "epoch-101 lr=['0.0050000'], tr/val_loss:  9.531557/903.107910, val:  60.83%, val_best:  72.08%, tr:  82.23%, tr_best:  84.78%, epoch time: 27.26 seconds, 0.45 minutes\n",
      "epoch-102 lr=['0.0050000'], tr/val_loss:  8.030763/766.273010, val:  62.50%, val_best:  72.08%, tr:  83.66%, tr_best:  84.78%, epoch time: 27.77 seconds, 0.46 minutes\n",
      "epoch-103 lr=['0.0050000'], tr/val_loss:  9.550897/653.011414, val:  65.83%, val_best:  72.08%, tr:  83.76%, tr_best:  84.78%, epoch time: 27.99 seconds, 0.47 minutes\n",
      "epoch-104 lr=['0.0050000'], tr/val_loss:  9.038427/994.547913, val:  66.67%, val_best:  72.08%, tr:  83.45%, tr_best:  84.78%, epoch time: 27.79 seconds, 0.46 minutes\n",
      "epoch-105 lr=['0.0050000'], tr/val_loss: 10.853684/611.930603, val:  70.83%, val_best:  72.08%, tr:  81.61%, tr_best:  84.78%, epoch time: 27.42 seconds, 0.46 minutes\n",
      "epoch-106 lr=['0.0050000'], tr/val_loss:  9.351088/606.664612, val:  71.25%, val_best:  72.08%, tr:  83.76%, tr_best:  84.78%, epoch time: 26.95 seconds, 0.45 minutes\n",
      "epoch-107 lr=['0.0050000'], tr/val_loss:  8.284723/784.361511, val:  67.08%, val_best:  72.08%, tr:  82.64%, tr_best:  84.78%, epoch time: 29.30 seconds, 0.49 minutes\n",
      "epoch-108 lr=['0.0050000'], tr/val_loss:  8.643683/743.423157, val:  67.92%, val_best:  72.08%, tr:  83.04%, tr_best:  84.78%, epoch time: 28.78 seconds, 0.48 minutes\n",
      "epoch-109 lr=['0.0050000'], tr/val_loss:  8.712956/566.750061, val:  71.25%, val_best:  72.08%, tr:  84.07%, tr_best:  84.78%, epoch time: 27.11 seconds, 0.45 minutes\n",
      "epoch-110 lr=['0.0050000'], tr/val_loss:  8.413452/644.158997, val:  68.33%, val_best:  72.08%, tr:  83.86%, tr_best:  84.78%, epoch time: 26.99 seconds, 0.45 minutes\n",
      "epoch-111 lr=['0.0050000'], tr/val_loss:  9.733833/823.035583, val:  62.92%, val_best:  72.08%, tr:  83.25%, tr_best:  84.78%, epoch time: 27.64 seconds, 0.46 minutes\n",
      "epoch-112 lr=['0.0050000'], tr/val_loss:  9.541900/813.101135, val:  66.67%, val_best:  72.08%, tr:  83.76%, tr_best:  84.78%, epoch time: 26.18 seconds, 0.44 minutes\n",
      "epoch-113 lr=['0.0050000'], tr/val_loss:  8.466671/835.851440, val:  66.25%, val_best:  72.08%, tr:  84.68%, tr_best:  84.78%, epoch time: 27.01 seconds, 0.45 minutes\n",
      "epoch-114 lr=['0.0050000'], tr/val_loss:  7.644419/655.669861, val:  68.33%, val_best:  72.08%, tr:  85.60%, tr_best:  85.60%, epoch time: 27.37 seconds, 0.46 minutes\n",
      "epoch-115 lr=['0.0050000'], tr/val_loss:  8.960991/746.759338, val:  73.75%, val_best:  73.75%, tr:  83.96%, tr_best:  85.60%, epoch time: 26.80 seconds, 0.45 minutes\n",
      "epoch-116 lr=['0.0050000'], tr/val_loss:  8.070921/902.655762, val:  67.08%, val_best:  73.75%, tr:  84.27%, tr_best:  85.60%, epoch time: 28.74 seconds, 0.48 minutes\n",
      "epoch-117 lr=['0.0050000'], tr/val_loss:  9.468028/643.503357, val:  64.58%, val_best:  73.75%, tr:  84.88%, tr_best:  85.60%, epoch time: 27.77 seconds, 0.46 minutes\n",
      "epoch-118 lr=['0.0050000'], tr/val_loss:  9.736407/664.979675, val:  68.75%, val_best:  73.75%, tr:  82.23%, tr_best:  85.60%, epoch time: 27.35 seconds, 0.46 minutes\n",
      "epoch-119 lr=['0.0050000'], tr/val_loss:  7.722714/640.048584, val:  73.33%, val_best:  73.75%, tr:  84.47%, tr_best:  85.60%, epoch time: 27.69 seconds, 0.46 minutes\n",
      "epoch-120 lr=['0.0050000'], tr/val_loss:  9.570345/649.319763, val:  71.67%, val_best:  73.75%, tr:  82.43%, tr_best:  85.60%, epoch time: 27.27 seconds, 0.45 minutes\n",
      "epoch-121 lr=['0.0050000'], tr/val_loss:  8.627695/1217.298706, val:  60.00%, val_best:  73.75%, tr:  83.15%, tr_best:  85.60%, epoch time: 27.97 seconds, 0.47 minutes\n",
      "epoch-122 lr=['0.0050000'], tr/val_loss:  9.783976/738.814880, val:  65.42%, val_best:  73.75%, tr:  82.23%, tr_best:  85.60%, epoch time: 28.92 seconds, 0.48 minutes\n",
      "epoch-123 lr=['0.0050000'], tr/val_loss:  8.735087/909.525574, val:  62.92%, val_best:  73.75%, tr:  83.86%, tr_best:  85.60%, epoch time: 28.18 seconds, 0.47 minutes\n",
      "epoch-124 lr=['0.0050000'], tr/val_loss:  8.924010/679.912415, val:  69.58%, val_best:  73.75%, tr:  84.17%, tr_best:  85.60%, epoch time: 27.41 seconds, 0.46 minutes\n",
      "epoch-125 lr=['0.0050000'], tr/val_loss:  9.458371/953.455444, val:  65.42%, val_best:  73.75%, tr:  82.84%, tr_best:  85.60%, epoch time: 27.56 seconds, 0.46 minutes\n",
      "epoch-126 lr=['0.0050000'], tr/val_loss:  9.381473/828.187805, val:  62.92%, val_best:  73.75%, tr:  83.96%, tr_best:  85.60%, epoch time: 27.73 seconds, 0.46 minutes\n",
      "epoch-127 lr=['0.0050000'], tr/val_loss:  8.744454/814.944397, val:  67.50%, val_best:  73.75%, tr:  83.25%, tr_best:  85.60%, epoch time: 27.48 seconds, 0.46 minutes\n",
      "epoch-128 lr=['0.0050000'], tr/val_loss:  8.537362/725.483459, val:  64.58%, val_best:  73.75%, tr:  84.88%, tr_best:  85.60%, epoch time: 27.23 seconds, 0.45 minutes\n",
      "epoch-129 lr=['0.0050000'], tr/val_loss:  7.545954/673.795166, val:  70.00%, val_best:  73.75%, tr:  86.21%, tr_best:  86.21%, epoch time: 27.54 seconds, 0.46 minutes\n",
      "epoch-130 lr=['0.0050000'], tr/val_loss:  8.413420/717.980530, val:  67.92%, val_best:  73.75%, tr:  84.88%, tr_best:  86.21%, epoch time: 27.47 seconds, 0.46 minutes\n",
      "epoch-131 lr=['0.0050000'], tr/val_loss:  8.207678/717.103943, val:  66.67%, val_best:  73.75%, tr:  85.19%, tr_best:  86.21%, epoch time: 27.44 seconds, 0.46 minutes\n",
      "epoch-132 lr=['0.0050000'], tr/val_loss:  7.317835/629.953674, val:  73.33%, val_best:  73.75%, tr:  86.52%, tr_best:  86.52%, epoch time: 28.01 seconds, 0.47 minutes\n",
      "epoch-133 lr=['0.0050000'], tr/val_loss:  9.247271/627.526978, val:  68.75%, val_best:  73.75%, tr:  83.15%, tr_best:  86.52%, epoch time: 27.08 seconds, 0.45 minutes\n",
      "epoch-134 lr=['0.0050000'], tr/val_loss:  8.515000/555.627930, val:  74.58%, val_best:  74.58%, tr:  84.07%, tr_best:  86.52%, epoch time: 27.37 seconds, 0.46 minutes\n",
      "epoch-135 lr=['0.0050000'], tr/val_loss:  7.653247/659.774902, val:  72.50%, val_best:  74.58%, tr:  84.98%, tr_best:  86.52%, epoch time: 27.52 seconds, 0.46 minutes\n",
      "epoch-136 lr=['0.0050000'], tr/val_loss:  8.343542/1043.562622, val:  67.08%, val_best:  74.58%, tr:  84.98%, tr_best:  86.52%, epoch time: 27.79 seconds, 0.46 minutes\n",
      "epoch-137 lr=['0.0050000'], tr/val_loss:  8.075374/626.693176, val:  69.17%, val_best:  74.58%, tr:  85.19%, tr_best:  86.52%, epoch time: 27.89 seconds, 0.46 minutes\n",
      "epoch-138 lr=['0.0050000'], tr/val_loss:  9.031534/736.579468, val:  71.25%, val_best:  74.58%, tr:  84.47%, tr_best:  86.52%, epoch time: 26.70 seconds, 0.44 minutes\n",
      "epoch-139 lr=['0.0050000'], tr/val_loss:  9.271818/537.264526, val:  73.33%, val_best:  74.58%, tr:  83.76%, tr_best:  86.52%, epoch time: 29.58 seconds, 0.49 minutes\n",
      "epoch-140 lr=['0.0050000'], tr/val_loss:  7.311612/791.419739, val:  64.17%, val_best:  74.58%, tr:  86.31%, tr_best:  86.52%, epoch time: 28.55 seconds, 0.48 minutes\n",
      "epoch-141 lr=['0.0050000'], tr/val_loss:  8.130500/755.151428, val:  67.50%, val_best:  74.58%, tr:  84.78%, tr_best:  86.52%, epoch time: 28.02 seconds, 0.47 minutes\n",
      "epoch-142 lr=['0.0050000'], tr/val_loss:  8.198160/803.908630, val:  69.58%, val_best:  74.58%, tr:  84.98%, tr_best:  86.52%, epoch time: 28.56 seconds, 0.48 minutes\n",
      "epoch-143 lr=['0.0050000'], tr/val_loss:  7.840147/916.047668, val:  67.92%, val_best:  74.58%, tr:  84.68%, tr_best:  86.52%, epoch time: 28.00 seconds, 0.47 minutes\n",
      "epoch-144 lr=['0.0050000'], tr/val_loss:  8.239654/928.370667, val:  65.42%, val_best:  74.58%, tr:  84.68%, tr_best:  86.52%, epoch time: 28.22 seconds, 0.47 minutes\n",
      "epoch-145 lr=['0.0050000'], tr/val_loss:  7.947697/1126.194702, val:  62.50%, val_best:  74.58%, tr:  86.41%, tr_best:  86.52%, epoch time: 27.30 seconds, 0.45 minutes\n",
      "epoch-146 lr=['0.0050000'], tr/val_loss:  6.988978/747.286255, val:  65.83%, val_best:  74.58%, tr:  86.21%, tr_best:  86.52%, epoch time: 27.22 seconds, 0.45 minutes\n",
      "epoch-147 lr=['0.0050000'], tr/val_loss:  7.059734/754.827393, val:  63.75%, val_best:  74.58%, tr:  87.23%, tr_best:  87.23%, epoch time: 27.55 seconds, 0.46 minutes\n",
      "epoch-148 lr=['0.0050000'], tr/val_loss:  8.536581/790.864319, val:  65.00%, val_best:  74.58%, tr:  84.78%, tr_best:  87.23%, epoch time: 28.37 seconds, 0.47 minutes\n",
      "epoch-149 lr=['0.0050000'], tr/val_loss:  7.436575/665.439087, val:  67.92%, val_best:  74.58%, tr:  84.68%, tr_best:  87.23%, epoch time: 27.87 seconds, 0.46 minutes\n",
      "epoch-150 lr=['0.0050000'], tr/val_loss:  7.056611/823.299500, val:  63.33%, val_best:  74.58%, tr:  86.21%, tr_best:  87.23%, epoch time: 27.84 seconds, 0.46 minutes\n",
      "epoch-151 lr=['0.0050000'], tr/val_loss:  8.242493/685.861877, val:  67.50%, val_best:  74.58%, tr:  83.76%, tr_best:  87.23%, epoch time: 26.85 seconds, 0.45 minutes\n",
      "epoch-152 lr=['0.0050000'], tr/val_loss:  6.852666/817.002258, val:  65.42%, val_best:  74.58%, tr:  86.82%, tr_best:  87.23%, epoch time: 27.34 seconds, 0.46 minutes\n",
      "epoch-153 lr=['0.0050000'], tr/val_loss:  9.022425/547.064819, val:  73.75%, val_best:  74.58%, tr:  85.60%, tr_best:  87.23%, epoch time: 26.59 seconds, 0.44 minutes\n",
      "epoch-154 lr=['0.0050000'], tr/val_loss:  7.141123/744.263367, val:  66.67%, val_best:  74.58%, tr:  87.95%, tr_best:  87.95%, epoch time: 27.44 seconds, 0.46 minutes\n",
      "epoch-155 lr=['0.0050000'], tr/val_loss:  9.102920/721.634705, val:  67.92%, val_best:  74.58%, tr:  84.27%, tr_best:  87.95%, epoch time: 26.96 seconds, 0.45 minutes\n",
      "epoch-156 lr=['0.0050000'], tr/val_loss:  6.864213/939.384888, val:  62.08%, val_best:  74.58%, tr:  86.93%, tr_best:  87.95%, epoch time: 27.42 seconds, 0.46 minutes\n",
      "epoch-157 lr=['0.0050000'], tr/val_loss:  6.739665/924.710144, val:  65.42%, val_best:  74.58%, tr:  86.41%, tr_best:  87.95%, epoch time: 27.91 seconds, 0.47 minutes\n",
      "epoch-158 lr=['0.0050000'], tr/val_loss:  7.659405/676.232849, val:  70.83%, val_best:  74.58%, tr:  85.39%, tr_best:  87.95%, epoch time: 27.36 seconds, 0.46 minutes\n",
      "epoch-159 lr=['0.0050000'], tr/val_loss:  8.741140/613.458496, val:  72.08%, val_best:  74.58%, tr:  85.09%, tr_best:  87.95%, epoch time: 26.91 seconds, 0.45 minutes\n",
      "epoch-160 lr=['0.0050000'], tr/val_loss:  8.862317/718.452332, val:  69.17%, val_best:  74.58%, tr:  84.68%, tr_best:  87.95%, epoch time: 27.90 seconds, 0.47 minutes\n",
      "epoch-161 lr=['0.0050000'], tr/val_loss:  7.680568/675.162781, val:  68.33%, val_best:  74.58%, tr:  86.11%, tr_best:  87.95%, epoch time: 27.90 seconds, 0.47 minutes\n",
      "epoch-162 lr=['0.0050000'], tr/val_loss:  7.046078/654.070435, val:  70.83%, val_best:  74.58%, tr:  85.80%, tr_best:  87.95%, epoch time: 28.66 seconds, 0.48 minutes\n",
      "epoch-163 lr=['0.0050000'], tr/val_loss:  7.074471/838.651184, val:  66.67%, val_best:  74.58%, tr:  87.13%, tr_best:  87.95%, epoch time: 28.14 seconds, 0.47 minutes\n",
      "epoch-164 lr=['0.0050000'], tr/val_loss:  7.000254/790.773804, val:  67.92%, val_best:  74.58%, tr:  87.95%, tr_best:  87.95%, epoch time: 27.33 seconds, 0.46 minutes\n",
      "epoch-165 lr=['0.0050000'], tr/val_loss:  7.335478/1222.894043, val:  62.92%, val_best:  74.58%, tr:  85.80%, tr_best:  87.95%, epoch time: 28.03 seconds, 0.47 minutes\n",
      "epoch-166 lr=['0.0050000'], tr/val_loss:  7.581716/901.584106, val:  62.92%, val_best:  74.58%, tr:  86.82%, tr_best:  87.95%, epoch time: 28.74 seconds, 0.48 minutes\n",
      "epoch-167 lr=['0.0050000'], tr/val_loss:  6.695479/1085.284058, val:  65.42%, val_best:  74.58%, tr:  86.93%, tr_best:  87.95%, epoch time: 27.25 seconds, 0.45 minutes\n",
      "epoch-168 lr=['0.0050000'], tr/val_loss:  7.985382/635.562622, val:  74.17%, val_best:  74.58%, tr:  86.01%, tr_best:  87.95%, epoch time: 28.24 seconds, 0.47 minutes\n",
      "epoch-169 lr=['0.0050000'], tr/val_loss:  7.655497/957.115479, val:  65.00%, val_best:  74.58%, tr:  85.70%, tr_best:  87.95%, epoch time: 28.30 seconds, 0.47 minutes\n",
      "epoch-170 lr=['0.0050000'], tr/val_loss:  6.394882/769.544617, val:  68.75%, val_best:  74.58%, tr:  85.60%, tr_best:  87.95%, epoch time: 27.65 seconds, 0.46 minutes\n",
      "epoch-171 lr=['0.0050000'], tr/val_loss:  7.684480/1193.030396, val:  56.67%, val_best:  74.58%, tr:  86.62%, tr_best:  87.95%, epoch time: 27.86 seconds, 0.46 minutes\n",
      "epoch-172 lr=['0.0050000'], tr/val_loss:  7.050223/698.379150, val:  71.25%, val_best:  74.58%, tr:  86.62%, tr_best:  87.95%, epoch time: 27.73 seconds, 0.46 minutes\n",
      "epoch-173 lr=['0.0050000'], tr/val_loss:  7.009327/766.563416, val:  66.67%, val_best:  74.58%, tr:  86.52%, tr_best:  87.95%, epoch time: 28.62 seconds, 0.48 minutes\n",
      "epoch-174 lr=['0.0050000'], tr/val_loss:  7.266869/669.477173, val:  70.83%, val_best:  74.58%, tr:  86.72%, tr_best:  87.95%, epoch time: 28.33 seconds, 0.47 minutes\n",
      "epoch-175 lr=['0.0050000'], tr/val_loss:  6.297657/1054.045654, val:  69.17%, val_best:  74.58%, tr:  87.74%, tr_best:  87.95%, epoch time: 27.98 seconds, 0.47 minutes\n",
      "epoch-176 lr=['0.0050000'], tr/val_loss:  7.027654/570.857666, val:  74.58%, val_best:  74.58%, tr:  85.29%, tr_best:  87.95%, epoch time: 28.26 seconds, 0.47 minutes\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [False]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.125,0.25,0.5, 0.75]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.0625,0.5,1.0,0.25,0.125]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [0.25, 0.5, 1.0, 2.0, 4.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        # \"synapse_trace_const2\": {\"values\": [0.5]}, #lif_layer_v_decay\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200], [512]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['one']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [5, 10, 15, 20, 25]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [5_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [True, False]},\n",
    "\n",
    "        # \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        # \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        # \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "        \"output_threshold\": {\"values\": [0.0001, 0.001, 0.01, 0.1]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.lif_layer_v_decay,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'p2hpq51o'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
