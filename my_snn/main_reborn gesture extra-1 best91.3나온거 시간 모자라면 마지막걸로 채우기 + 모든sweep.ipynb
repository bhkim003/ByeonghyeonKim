{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28035/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        smallest_now_T = 99999\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                if epoch == 0 and now_T < smallest_now_T:\n",
    "                    smallest_now_T = now_T\n",
    "                    print(f'smallest_now_T updated: {smallest_now_T}')\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                if now_T < now_time_steps:\n",
    "                    # Î∂ÄÏ°±Ìïú timestep Í∞úÏàò\n",
    "                    diff = now_time_steps - now_T\n",
    "\n",
    "                    # ÎßàÏßÄÎßâ timestep Î≥µÏÇ¨ (shape: [B, 1, C, H, W])\n",
    "                    last_frame = inputs[:, -1:, :, :, :]\n",
    "\n",
    "                    # diffÎßåÌÅº repeatÌïòÏó¨ Ìå®Îî© Íµ¨ÏÑ±\n",
    "                    pad_frames = last_frame.repeat(1, diff, 1, 1, 1)\n",
    "\n",
    "                    # ÏõêÎ≥∏ + Ìå®Îî© Í≤∞Ìï©\n",
    "                    inputs = torch.cat([inputs, pad_frames], dim=1)\n",
    "                else:\n",
    "                    # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                    start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                    # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                    inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            smallest_now_T_val = 99999\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            if epoch == 0 and now_T < smallest_now_T_val:\n",
    "                                smallest_now_T_val = now_T\n",
    "                                print(f'smallest_now_T_val updated: {smallest_now_T_val}')\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "\n",
    "                            if now_T < now_time_steps:\n",
    "                                # Î∂ÄÏ°±Ìïú timestep Í∞úÏàò\n",
    "                                diff = now_time_steps - now_T\n",
    "\n",
    "                                # ÎßàÏßÄÎßâ timestep Î≥µÏÇ¨ (shape: [B, 1, C, H, W])\n",
    "                                last_frame = inputs_val[:, -1:, :, :, :]\n",
    "\n",
    "                                # diffÎßåÌÅº repeatÌïòÏó¨ Ìå®Îî© Íµ¨ÏÑ±\n",
    "                                pad_frames = last_frame.repeat(1, diff, 1, 1, 1)\n",
    "\n",
    "                                # ÏõêÎ≥∏ + Ìå®Îî© Í≤∞Ìï©\n",
    "                                inputs_val = torch.cat([inputs_val, pad_frames], dim=1)\n",
    "                            else:\n",
    "                                pass\n",
    "                            \n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "                \n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.8\n",
    "                elif epoch > 150:\n",
    "                    assert val_acc_best > 0.88\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"1\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.25,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-9,-9],[-9,-9],[-8,-8]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5ugzn66v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 12000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251126_211809-5ugzn66v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ugzn66v' target=\"_blank\">northern-sweep-278</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ugzn66v' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ugzn66v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251126_211816_424', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 12000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 67f09733060e9328908e01cda0ab3532\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 592\n",
      "fc layer 1 self.abs_max_out: 171.0\n",
      "lif layer 1 self.abs_max_v: 171.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 241.0\n",
      "lif layer 1 self.abs_max_v: 268.5\n",
      "fc layer 1 self.abs_max_out: 254.0\n",
      "lif layer 1 self.abs_max_v: 309.5\n",
      "fc layer 2 self.abs_max_out: 177.0\n",
      "lif layer 2 self.abs_max_v: 177.0\n",
      "smallest_now_T updated: 534\n",
      "fc layer 1 self.abs_max_out: 340.0\n",
      "lif layer 1 self.abs_max_v: 340.0\n",
      "fc layer 1 self.abs_max_out: 388.0\n",
      "lif layer 1 self.abs_max_v: 509.0\n",
      "fc layer 2 self.abs_max_out: 385.0\n",
      "lif layer 2 self.abs_max_v: 377.5\n",
      "fc layer 3 self.abs_max_out: 113.0\n",
      "lif layer 1 self.abs_max_v: 544.5\n",
      "lif layer 2 self.abs_max_v: 436.5\n",
      "lif layer 1 self.abs_max_v: 602.5\n",
      "fc layer 3 self.abs_max_out: 143.0\n",
      "smallest_now_T updated: 407\n",
      "fc layer 1 self.abs_max_out: 480.0\n",
      "fc layer 2 self.abs_max_out: 453.0\n",
      "lif layer 2 self.abs_max_v: 553.0\n",
      "fc layer 1 self.abs_max_out: 732.0\n",
      "lif layer 1 self.abs_max_v: 732.0\n",
      "fc layer 2 self.abs_max_out: 523.0\n",
      "lif layer 2 self.abs_max_v: 663.5\n",
      "fc layer 3 self.abs_max_out: 156.0\n",
      "lif layer 2 self.abs_max_v: 723.0\n",
      "fc layer 3 self.abs_max_out: 161.0\n",
      "fc layer 2 self.abs_max_out: 586.0\n",
      "fc layer 3 self.abs_max_out: 212.0\n",
      "lif layer 1 self.abs_max_v: 743.0\n",
      "fc layer 2 self.abs_max_out: 595.0\n",
      "lif layer 2 self.abs_max_v: 838.5\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "lif layer 1 self.abs_max_v: 745.5\n",
      "fc layer 2 self.abs_max_out: 665.0\n",
      "lif layer 2 self.abs_max_v: 952.5\n",
      "fc layer 1 self.abs_max_out: 991.0\n",
      "lif layer 1 self.abs_max_v: 991.0\n",
      "lif layer 2 self.abs_max_v: 958.5\n",
      "lif layer 2 self.abs_max_v: 976.5\n",
      "fc layer 1 self.abs_max_out: 1093.0\n",
      "lif layer 1 self.abs_max_v: 1093.0\n",
      "fc layer 3 self.abs_max_out: 266.0\n",
      "fc layer 1 self.abs_max_out: 1259.0\n",
      "lif layer 1 self.abs_max_v: 1259.0\n",
      "fc layer 2 self.abs_max_out: 730.0\n",
      "fc layer 2 self.abs_max_out: 785.0\n",
      "fc layer 3 self.abs_max_out: 296.0\n",
      "fc layer 2 self.abs_max_out: 854.0\n",
      "lif layer 2 self.abs_max_v: 1027.5\n",
      "fc layer 2 self.abs_max_out: 878.0\n",
      "lif layer 2 self.abs_max_v: 1030.5\n",
      "fc layer 3 self.abs_max_out: 314.0\n",
      "lif layer 2 self.abs_max_v: 1126.5\n",
      "smallest_now_T updated: 345\n",
      "fc layer 2 self.abs_max_out: 954.0\n",
      "fc layer 3 self.abs_max_out: 328.0\n",
      "fc layer 2 self.abs_max_out: 980.0\n",
      "fc layer 3 self.abs_max_out: 366.0\n",
      "lif layer 2 self.abs_max_v: 1133.0\n",
      "lif layer 2 self.abs_max_v: 1137.0\n",
      "fc layer 1 self.abs_max_out: 1311.0\n",
      "lif layer 1 self.abs_max_v: 1311.0\n",
      "lif layer 2 self.abs_max_v: 1276.5\n",
      "lif layer 2 self.abs_max_v: 1402.5\n",
      "fc layer 2 self.abs_max_out: 1005.0\n",
      "lif layer 2 self.abs_max_v: 1523.5\n",
      "fc layer 2 self.abs_max_out: 1074.0\n",
      "fc layer 3 self.abs_max_out: 416.0\n",
      "smallest_now_T updated: 317\n",
      "fc layer 2 self.abs_max_out: 1085.0\n",
      "fc layer 2 self.abs_max_out: 1125.0\n",
      "fc layer 2 self.abs_max_out: 1174.0\n",
      "fc layer 2 self.abs_max_out: 1182.0\n",
      "fc layer 2 self.abs_max_out: 1331.0\n",
      "fc layer 3 self.abs_max_out: 462.0\n",
      "fc layer 1 self.abs_max_out: 1417.0\n",
      "lif layer 1 self.abs_max_v: 1417.0\n",
      "smallest_now_T updated: 286\n",
      "lif layer 2 self.abs_max_v: 1797.0\n",
      "fc layer 2 self.abs_max_out: 1353.0\n",
      "fc layer 1 self.abs_max_out: 1512.0\n",
      "lif layer 1 self.abs_max_v: 1512.0\n",
      "fc layer 2 self.abs_max_out: 1773.0\n",
      "fc layer 2 self.abs_max_out: 1832.0\n",
      "lif layer 2 self.abs_max_v: 1832.0\n",
      "fc layer 2 self.abs_max_out: 1866.0\n",
      "lif layer 2 self.abs_max_v: 1866.0\n",
      "fc layer 2 self.abs_max_out: 2076.0\n",
      "lif layer 2 self.abs_max_v: 2076.0\n",
      "fc layer 3 self.abs_max_out: 563.0\n",
      "lif layer 1 self.abs_max_v: 1629.0\n",
      "fc layer 1 self.abs_max_out: 1562.0\n",
      "fc layer 1 self.abs_max_out: 1789.0\n",
      "lif layer 1 self.abs_max_v: 1789.0\n",
      "fc layer 3 self.abs_max_out: 565.0\n",
      "fc layer 1 self.abs_max_out: 1906.0\n",
      "lif layer 1 self.abs_max_v: 1906.0\n",
      "fc layer 1 self.abs_max_out: 1937.0\n",
      "lif layer 1 self.abs_max_v: 1937.0\n",
      "fc layer 1 self.abs_max_out: 1943.0\n",
      "lif layer 1 self.abs_max_v: 1943.0\n",
      "smallest_now_T updated: 247\n",
      "smallest_now_T updated: 192\n",
      "fc layer 1 self.abs_max_out: 2169.0\n",
      "lif layer 1 self.abs_max_v: 2169.0\n",
      "fc layer 2 self.abs_max_out: 2108.0\n",
      "lif layer 2 self.abs_max_v: 2108.0\n",
      "fc layer 2 self.abs_max_out: 2116.0\n",
      "lif layer 2 self.abs_max_v: 2116.0\n",
      "fc layer 2 self.abs_max_out: 2119.0\n",
      "lif layer 2 self.abs_max_v: 2119.0\n",
      "fc layer 2 self.abs_max_out: 2166.0\n",
      "lif layer 2 self.abs_max_v: 2166.0\n",
      "fc layer 3 self.abs_max_out: 581.0\n",
      "fc layer 3 self.abs_max_out: 583.0\n",
      "fc layer 3 self.abs_max_out: 597.0\n",
      "fc layer 2 self.abs_max_out: 2194.0\n",
      "lif layer 2 self.abs_max_v: 2194.0\n",
      "fc layer 2 self.abs_max_out: 2244.0\n",
      "lif layer 2 self.abs_max_v: 2244.0\n",
      "fc layer 2 self.abs_max_out: 2272.0\n",
      "lif layer 2 self.abs_max_v: 2272.5\n",
      "fc layer 2 self.abs_max_out: 2292.0\n",
      "lif layer 2 self.abs_max_v: 2292.0\n",
      "fc layer 2 self.abs_max_out: 2393.0\n",
      "lif layer 2 self.abs_max_v: 2393.0\n",
      "fc layer 3 self.abs_max_out: 649.0\n",
      "fc layer 2 self.abs_max_out: 2491.0\n",
      "lif layer 2 self.abs_max_v: 2491.0\n",
      "lif layer 2 self.abs_max_v: 2588.5\n",
      "lif layer 2 self.abs_max_v: 2750.5\n",
      "fc layer 2 self.abs_max_out: 2721.0\n",
      "fc layer 2 self.abs_max_out: 2747.0\n",
      "lif layer 2 self.abs_max_v: 2762.5\n",
      "fc layer 3 self.abs_max_out: 715.0\n",
      "fc layer 3 self.abs_max_out: 755.0\n",
      "fc layer 3 self.abs_max_out: 773.0\n",
      "fc layer 3 self.abs_max_out: 778.0\n",
      "lif layer 2 self.abs_max_v: 2843.5\n",
      "lif layer 2 self.abs_max_v: 3031.0\n",
      "lif layer 2 self.abs_max_v: 3237.5\n",
      "fc layer 1 self.abs_max_out: 2293.0\n",
      "lif layer 1 self.abs_max_v: 2293.0\n",
      "fc layer 2 self.abs_max_out: 2816.0\n",
      "fc layer 1 self.abs_max_out: 2356.0\n",
      "lif layer 1 self.abs_max_v: 2356.0\n",
      "lif layer 2 self.abs_max_v: 3286.0\n",
      "fc layer 1 self.abs_max_out: 2582.0\n",
      "lif layer 1 self.abs_max_v: 2582.0\n",
      "lif layer 2 self.abs_max_v: 3511.0\n",
      "lif layer 2 self.abs_max_v: 3615.5\n",
      "fc layer 1 self.abs_max_out: 3217.0\n",
      "lif layer 1 self.abs_max_v: 3217.0\n",
      "fc layer 3 self.abs_max_out: 810.0\n",
      "fc layer 2 self.abs_max_out: 2824.0\n",
      "fc layer 2 self.abs_max_out: 2861.0\n",
      "lif layer 2 self.abs_max_v: 3652.5\n",
      "lif layer 2 self.abs_max_v: 4030.5\n",
      "fc layer 2 self.abs_max_out: 2897.0\n",
      "fc layer 2 self.abs_max_out: 2899.0\n",
      "fc layer 3 self.abs_max_out: 847.0\n",
      "smallest_now_T_val updated: 552\n",
      "smallest_now_T_val updated: 456\n",
      "smallest_now_T_val updated: 448\n",
      "fc layer 2 self.abs_max_out: 3001.0\n",
      "smallest_now_T_val updated: 440\n",
      "smallest_now_T_val updated: 368\n",
      "fc layer 2 self.abs_max_out: 3051.0\n",
      "smallest_now_T_val updated: 137\n",
      "fc layer 2 self.abs_max_out: 3087.0\n",
      "fc layer 2 self.abs_max_out: 3102.0\n",
      "fc layer 2 self.abs_max_out: 3139.0\n",
      "fc layer 1 self.abs_max_out: 3241.0\n",
      "lif layer 1 self.abs_max_v: 3241.0\n",
      "fc layer 2 self.abs_max_out: 3432.0\n",
      "lif layer 2 self.abs_max_v: 4120.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.914412/  2.067783, val:  25.00%, val_best:  25.00%, tr:  72.73%, tr_best:  72.73%, epoch time: 30.43 seconds, 0.51 minutes\n",
      "layer   1  Sparsity: 97.2584%\n",
      "layer   2  Sparsity: 82.8555%\n",
      "layer   3  Sparsity: 81.8213%\n",
      "total_backward_count 4895 real_backward_count 2044  41.757%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 4258.0\n",
      "fc layer 1 self.abs_max_out: 3549.0\n",
      "lif layer 1 self.abs_max_v: 3549.0\n",
      "fc layer 3 self.abs_max_out: 886.0\n",
      "fc layer 3 self.abs_max_out: 897.0\n",
      "fc layer 3 self.abs_max_out: 974.0\n",
      "lif layer 2 self.abs_max_v: 4269.5\n",
      "lif layer 2 self.abs_max_v: 4289.0\n",
      "lif layer 2 self.abs_max_v: 4330.0\n",
      "fc layer 2 self.abs_max_out: 3533.0\n",
      "fc layer 2 self.abs_max_out: 3597.0\n",
      "lif layer 2 self.abs_max_v: 4442.5\n",
      "fc layer 2 self.abs_max_out: 3695.0\n",
      "fc layer 2 self.abs_max_out: 3745.0\n",
      "fc layer 1 self.abs_max_out: 3808.0\n",
      "lif layer 1 self.abs_max_v: 3808.0\n",
      "lif layer 2 self.abs_max_v: 4568.5\n",
      "fc layer 2 self.abs_max_out: 4017.0\n",
      "lif layer 2 self.abs_max_v: 4865.5\n",
      "fc layer 2 self.abs_max_out: 4091.0\n",
      "fc layer 2 self.abs_max_out: 4149.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.797533/  2.012653, val:  37.08%, val_best:  37.08%, tr:  82.43%, tr_best:  82.43%, epoch time: 27.90 seconds, 0.46 minutes\n",
      "layer   1  Sparsity: 97.2779%\n",
      "layer   2  Sparsity: 79.8579%\n",
      "layer   3  Sparsity: 78.9778%\n",
      "total_backward_count 9790 real_backward_count 3699  37.783%\n",
      "fc layer 3 self.abs_max_out: 1030.0\n",
      "fc layer 2 self.abs_max_out: 4290.0\n",
      "fc layer 1 self.abs_max_out: 4103.0\n",
      "lif layer 1 self.abs_max_v: 4103.0\n",
      "lif layer 2 self.abs_max_v: 5256.0\n",
      "lif layer 2 self.abs_max_v: 5369.0\n",
      "fc layer 3 self.abs_max_out: 1042.0\n",
      "fc layer 3 self.abs_max_out: 1100.0\n",
      "fc layer 3 self.abs_max_out: 1105.0\n",
      "lif layer 2 self.abs_max_v: 5863.5\n",
      "lif layer 2 self.abs_max_v: 5983.0\n",
      "lif layer 1 self.abs_max_v: 4188.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.720548/  1.999478, val:  32.50%, val_best:  37.08%, tr:  82.02%, tr_best:  82.43%, epoch time: 28.47 seconds, 0.47 minutes\n",
      "layer   1  Sparsity: 97.2890%\n",
      "layer   2  Sparsity: 79.0066%\n",
      "layer   3  Sparsity: 77.7126%\n",
      "total_backward_count 14685 real_backward_count 5283  35.975%\n",
      "fc layer 2 self.abs_max_out: 4454.0\n",
      "lif layer 2 self.abs_max_v: 6298.0\n",
      "fc layer 1 self.abs_max_out: 4437.0\n",
      "lif layer 1 self.abs_max_v: 4437.0\n",
      "lif layer 2 self.abs_max_v: 6725.0\n",
      "fc layer 3 self.abs_max_out: 1226.0\n",
      "fc layer 2 self.abs_max_out: 4475.0\n",
      "lif layer 1 self.abs_max_v: 4721.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.708057/  1.909879, val:  42.08%, val_best:  42.08%, tr:  82.74%, tr_best:  82.74%, epoch time: 27.87 seconds, 0.46 minutes\n",
      "layer   1  Sparsity: 97.2609%\n",
      "layer   2  Sparsity: 78.9535%\n",
      "layer   3  Sparsity: 76.7066%\n",
      "total_backward_count 19580 real_backward_count 6865  35.061%\n",
      "lif layer 2 self.abs_max_v: 6766.5\n",
      "fc layer 1 self.abs_max_out: 4596.0\n",
      "lif layer 1 self.abs_max_v: 4772.5\n",
      "lif layer 1 self.abs_max_v: 5194.5\n",
      "fc layer 1 self.abs_max_out: 4676.0\n",
      "fc layer 2 self.abs_max_out: 4508.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.649351/  1.927475, val:  34.17%, val_best:  42.08%, tr:  85.39%, tr_best:  85.39%, epoch time: 27.72 seconds, 0.46 minutes\n",
      "layer   1  Sparsity: 97.2698%\n",
      "layer   2  Sparsity: 79.1344%\n",
      "layer   3  Sparsity: 75.7021%\n",
      "total_backward_count 24475 real_backward_count 8365  34.178%\n",
      "fc layer 1 self.abs_max_out: 4769.0\n",
      "fc layer 2 self.abs_max_out: 4721.0\n",
      "fc layer 2 self.abs_max_out: 4804.0\n",
      "fc layer 3 self.abs_max_out: 1363.0\n",
      "fc layer 2 self.abs_max_out: 4873.0\n",
      "fc layer 2 self.abs_max_out: 4985.0\n",
      "fc layer 1 self.abs_max_out: 4795.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.631661/  1.889024, val:  31.67%, val_best:  42.08%, tr:  85.50%, tr_best:  85.50%, epoch time: 27.58 seconds, 0.46 minutes\n",
      "layer   1  Sparsity: 97.2727%\n",
      "layer   2  Sparsity: 78.4455%\n",
      "layer   3  Sparsity: 75.1925%\n",
      "total_backward_count 29370 real_backward_count 9857  33.561%\n",
      "fc layer 2 self.abs_max_out: 5035.0\n",
      "lif layer 2 self.abs_max_v: 7149.5\n",
      "lif layer 2 self.abs_max_v: 7355.0\n",
      "lif layer 1 self.abs_max_v: 5324.0\n",
      "fc layer 2 self.abs_max_out: 5090.0\n",
      "fc layer 2 self.abs_max_out: 5330.0\n",
      "fc layer 1 self.abs_max_out: 4849.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.586608/  1.900257, val:  38.75%, val_best:  42.08%, tr:  85.60%, tr_best:  85.60%, epoch time: 28.62 seconds, 0.48 minutes\n",
      "layer   1  Sparsity: 97.2865%\n",
      "layer   2  Sparsity: 78.3716%\n",
      "layer   3  Sparsity: 74.7774%\n",
      "total_backward_count 34265 real_backward_count 11370  33.183%\n",
      "fc layer 1 self.abs_max_out: 5225.0\n",
      "fc layer 2 self.abs_max_out: 5344.0\n",
      "fc layer 1 self.abs_max_out: 5261.0\n",
      "lif layer 1 self.abs_max_v: 5497.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.570158/  1.863559, val:  28.33%, val_best:  42.08%, tr:  85.39%, tr_best:  85.60%, epoch time: 30.09 seconds, 0.50 minutes\n",
      "layer   1  Sparsity: 97.2813%\n",
      "layer   2  Sparsity: 77.0857%\n",
      "layer   3  Sparsity: 74.2939%\n",
      "total_backward_count 39160 real_backward_count 12811  32.715%\n",
      "fc layer 3 self.abs_max_out: 1386.0\n",
      "fc layer 2 self.abs_max_out: 5456.0\n",
      "fc layer 2 self.abs_max_out: 5457.0\n",
      "lif layer 2 self.abs_max_v: 7595.5\n",
      "fc layer 2 self.abs_max_out: 5511.0\n",
      "lif layer 1 self.abs_max_v: 5540.0\n",
      "fc layer 2 self.abs_max_out: 5637.0\n",
      "fc layer 2 self.abs_max_out: 5941.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.600610/  1.847607, val:  35.83%, val_best:  42.08%, tr:  86.01%, tr_best:  86.01%, epoch time: 29.30 seconds, 0.49 minutes\n",
      "layer   1  Sparsity: 97.2717%\n",
      "layer   2  Sparsity: 77.1340%\n",
      "layer   3  Sparsity: 74.1447%\n",
      "total_backward_count 44055 real_backward_count 14382  32.646%\n",
      "fc layer 1 self.abs_max_out: 5366.0\n",
      "fc layer 1 self.abs_max_out: 5674.0\n",
      "lif layer 1 self.abs_max_v: 5674.0\n",
      "lif layer 1 self.abs_max_v: 5941.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.578559/  1.944802, val:  32.08%, val_best:  42.08%, tr:  88.15%, tr_best:  88.15%, epoch time: 28.77 seconds, 0.48 minutes\n",
      "layer   1  Sparsity: 97.2632%\n",
      "layer   2  Sparsity: 77.2110%\n",
      "layer   3  Sparsity: 74.2281%\n",
      "total_backward_count 48950 real_backward_count 15777  32.231%\n",
      "fc layer 1 self.abs_max_out: 5687.0\n",
      "lif layer 1 self.abs_max_v: 6226.5\n",
      "fc layer 2 self.abs_max_out: 5966.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.552080/  1.859078, val:  36.67%, val_best:  42.08%, tr:  86.93%, tr_best:  88.15%, epoch time: 28.34 seconds, 0.47 minutes\n",
      "layer   1  Sparsity: 97.2424%\n",
      "layer   2  Sparsity: 77.6082%\n",
      "layer   3  Sparsity: 73.7488%\n",
      "total_backward_count 53845 real_backward_count 17203  31.949%\n",
      "fc layer 2 self.abs_max_out: 6066.0\n",
      "lif layer 2 self.abs_max_v: 7748.5\n",
      "lif layer 2 self.abs_max_v: 7788.5\n",
      "lif layer 1 self.abs_max_v: 6547.5\n",
      "lif layer 1 self.abs_max_v: 6613.0\n",
      "fc layer 1 self.abs_max_out: 5930.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.578980/  1.875887, val:  43.75%, val_best:  43.75%, tr:  85.80%, tr_best:  88.15%, epoch time: 29.14 seconds, 0.49 minutes\n",
      "layer   1  Sparsity: 97.2821%\n",
      "layer   2  Sparsity: 77.3039%\n",
      "layer   3  Sparsity: 73.4308%\n",
      "total_backward_count 58740 real_backward_count 18654  31.757%\n",
      "lif layer 2 self.abs_max_v: 7885.0\n",
      "lif layer 2 self.abs_max_v: 7914.0\n",
      "lif layer 2 self.abs_max_v: 7929.5\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.556061/  1.892639, val:  33.75%, val_best:  43.75%, tr:  88.15%, tr_best:  88.15%, epoch time: 29.34 seconds, 0.49 minutes\n",
      "layer   1  Sparsity: 97.2585%\n",
      "layer   2  Sparsity: 77.5033%\n",
      "layer   3  Sparsity: 73.7798%\n",
      "total_backward_count 63635 real_backward_count 20041  31.494%\n",
      "lif layer 2 self.abs_max_v: 8346.0\n",
      "lif layer 2 self.abs_max_v: 9186.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.564565/  1.908789, val:  31.67%, val_best:  43.75%, tr:  87.74%, tr_best:  88.15%, epoch time: 29.03 seconds, 0.48 minutes\n",
      "layer   1  Sparsity: 97.2652%\n",
      "layer   2  Sparsity: 77.1517%\n",
      "layer   3  Sparsity: 73.7028%\n",
      "total_backward_count 68530 real_backward_count 21402  31.230%\n",
      "fc layer 2 self.abs_max_out: 6205.0\n",
      "fc layer 1 self.abs_max_out: 6094.0\n",
      "fc layer 1 self.abs_max_out: 6192.0\n",
      "fc layer 3 self.abs_max_out: 1500.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.540424/  1.844774, val:  42.92%, val_best:  43.75%, tr:  87.44%, tr_best:  88.15%, epoch time: 28.51 seconds, 0.48 minutes\n",
      "layer   1  Sparsity: 97.2767%\n",
      "layer   2  Sparsity: 77.3752%\n",
      "layer   3  Sparsity: 74.6180%\n",
      "total_backward_count 73425 real_backward_count 22753  30.988%\n",
      "fc layer 1 self.abs_max_out: 6219.0\n",
      "lif layer 1 self.abs_max_v: 6751.5\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.525718/  1.877110, val:  30.42%, val_best:  43.75%, tr:  88.46%, tr_best:  88.46%, epoch time: 28.83 seconds, 0.48 minutes\n",
      "layer   1  Sparsity: 97.2674%\n",
      "layer   2  Sparsity: 77.6357%\n",
      "layer   3  Sparsity: 73.9573%\n",
      "total_backward_count 78320 real_backward_count 24127  30.806%\n",
      "lif layer 1 self.abs_max_v: 7338.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.506202/  1.787766, val:  50.83%, val_best:  50.83%, tr:  88.97%, tr_best:  88.97%, epoch time: 28.46 seconds, 0.47 minutes\n",
      "layer   1  Sparsity: 97.2816%\n",
      "layer   2  Sparsity: 77.6001%\n",
      "layer   3  Sparsity: 73.0598%\n",
      "total_backward_count 83215 real_backward_count 25477  30.616%\n",
      "fc layer 1 self.abs_max_out: 6487.0\n",
      "fc layer 2 self.abs_max_out: 6213.0\n",
      "fc layer 3 self.abs_max_out: 1507.0\n",
      "fc layer 3 self.abs_max_out: 1562.0\n",
      "fc layer 1 self.abs_max_out: 6576.0\n",
      "fc layer 1 self.abs_max_out: 6592.0\n",
      "lif layer 1 self.abs_max_v: 7546.5\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.497224/  1.835686, val:  35.83%, val_best:  50.83%, tr:  90.09%, tr_best:  90.09%, epoch time: 28.65 seconds, 0.48 minutes\n",
      "layer   1  Sparsity: 97.2780%\n",
      "layer   2  Sparsity: 77.1178%\n",
      "layer   3  Sparsity: 72.5654%\n",
      "total_backward_count 88110 real_backward_count 26855  30.479%\n",
      "fc layer 1 self.abs_max_out: 8145.0\n",
      "lif layer 1 self.abs_max_v: 8145.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.480861/  1.763111, val:  43.75%, val_best:  50.83%, tr:  87.74%, tr_best:  90.09%, epoch time: 28.86 seconds, 0.48 minutes\n",
      "layer   1  Sparsity: 97.2704%\n",
      "layer   2  Sparsity: 76.8363%\n",
      "layer   3  Sparsity: 72.4744%\n",
      "total_backward_count 93005 real_backward_count 28238  30.362%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.445414/  1.797953, val:  39.17%, val_best:  50.83%, tr:  88.66%, tr_best:  90.09%, epoch time: 27.10 seconds, 0.45 minutes\n",
      "layer   1  Sparsity: 97.2518%\n",
      "layer   2  Sparsity: 76.7423%\n",
      "layer   3  Sparsity: 72.6442%\n",
      "total_backward_count 97900 real_backward_count 29556  30.190%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.472954/  1.823873, val:  37.50%, val_best:  50.83%, tr:  87.54%, tr_best:  90.09%, epoch time: 28.27 seconds, 0.47 minutes\n",
      "layer   1  Sparsity: 97.2790%\n",
      "layer   2  Sparsity: 76.5642%\n",
      "layer   3  Sparsity: 72.9363%\n",
      "total_backward_count 102795 real_backward_count 30953  30.111%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.496437/  1.807820, val:  43.75%, val_best:  50.83%, tr:  86.01%, tr_best:  90.09%, epoch time: 28.31 seconds, 0.47 minutes\n",
      "layer   1  Sparsity: 97.2803%\n",
      "layer   2  Sparsity: 76.7401%\n",
      "layer   3  Sparsity: 72.7101%\n",
      "total_backward_count 107690 real_backward_count 32369  30.058%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.476868/  1.808303, val:  39.17%, val_best:  50.83%, tr:  88.36%, tr_best:  90.09%, epoch time: 28.53 seconds, 0.48 minutes\n",
      "layer   1  Sparsity: 97.2840%\n",
      "layer   2  Sparsity: 76.7650%\n",
      "layer   3  Sparsity: 73.1033%\n",
      "total_backward_count 112585 real_backward_count 33699  29.932%\n",
      "lif layer 2 self.abs_max_v: 9346.5\n",
      "lif layer 2 self.abs_max_v: 9449.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.464295/  1.785506, val:  46.25%, val_best:  50.83%, tr:  89.99%, tr_best:  90.09%, epoch time: 28.71 seconds, 0.48 minutes\n",
      "layer   1  Sparsity: 97.2615%\n",
      "layer   2  Sparsity: 76.7825%\n",
      "layer   3  Sparsity: 72.9704%\n",
      "total_backward_count 117480 real_backward_count 35012  29.803%\n",
      "fc layer 3 self.abs_max_out: 1631.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.483559/  1.778255, val:  46.25%, val_best:  50.83%, tr:  89.17%, tr_best:  90.09%, epoch time: 28.83 seconds, 0.48 minutes\n",
      "layer   1  Sparsity: 97.2954%\n",
      "layer   2  Sparsity: 76.7119%\n",
      "layer   3  Sparsity: 73.1136%\n",
      "total_backward_count 122375 real_backward_count 36385  29.732%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.489657/  1.754427, val:  47.50%, val_best:  50.83%, tr:  88.05%, tr_best:  90.09%, epoch time: 28.71 seconds, 0.48 minutes\n",
      "layer   1  Sparsity: 97.2776%\n",
      "layer   2  Sparsity: 76.1313%\n",
      "layer   3  Sparsity: 72.9760%\n",
      "total_backward_count 127270 real_backward_count 37797  29.698%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.462578/  1.760381, val:  42.08%, val_best:  50.83%, tr:  89.17%, tr_best:  90.09%, epoch time: 28.78 seconds, 0.48 minutes\n",
      "layer   1  Sparsity: 97.2754%\n",
      "layer   2  Sparsity: 75.9840%\n",
      "layer   3  Sparsity: 72.8310%\n",
      "total_backward_count 132165 real_backward_count 39134  29.610%\n",
      "lif layer 1 self.abs_max_v: 8602.5\n",
      "lif layer 2 self.abs_max_v: 9501.5\n",
      "lif layer 2 self.abs_max_v: 9509.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.450284/  1.700552, val:  49.17%, val_best:  50.83%, tr:  88.97%, tr_best:  90.09%, epoch time: 28.79 seconds, 0.48 minutes\n",
      "layer   1  Sparsity: 97.2777%\n",
      "layer   2  Sparsity: 76.3448%\n",
      "layer   3  Sparsity: 72.7839%\n",
      "total_backward_count 137060 real_backward_count 40531  29.572%\n",
      "lif layer 2 self.abs_max_v: 9662.0\n",
      "lif layer 2 self.abs_max_v: 9997.5\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.432674/  1.785513, val:  45.83%, val_best:  50.83%, tr:  89.68%, tr_best:  90.09%, epoch time: 29.19 seconds, 0.49 minutes\n",
      "layer   1  Sparsity: 97.2756%\n",
      "layer   2  Sparsity: 76.1246%\n",
      "layer   3  Sparsity: 72.3957%\n",
      "total_backward_count 141955 real_backward_count 41847  29.479%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.445700/  1.805794, val:  24.17%, val_best:  50.83%, tr:  90.60%, tr_best:  90.60%, epoch time: 29.84 seconds, 0.50 minutes\n",
      "layer   1  Sparsity: 97.2737%\n",
      "layer   2  Sparsity: 76.0667%\n",
      "layer   3  Sparsity: 72.7307%\n",
      "total_backward_count 146850 real_backward_count 43180  29.404%\n",
      "lif layer 2 self.abs_max_v: 10135.5\n",
      "fc layer 3 self.abs_max_out: 1684.0\n",
      "lif layer 2 self.abs_max_v: 10253.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.451640/  1.731980, val:  44.58%, val_best:  50.83%, tr:  89.17%, tr_best:  90.60%, epoch time: 36.69 seconds, 0.61 minutes\n",
      "layer   1  Sparsity: 97.2819%\n",
      "layer   2  Sparsity: 76.0610%\n",
      "layer   3  Sparsity: 73.2791%\n",
      "total_backward_count 151745 real_backward_count 44520  29.339%\n",
      "fc layer 2 self.abs_max_out: 6670.0\n",
      "fc layer 1 self.abs_max_out: 8720.0\n",
      "lif layer 1 self.abs_max_v: 8720.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.439921/  1.727426, val:  45.83%, val_best:  50.83%, tr:  89.68%, tr_best:  90.60%, epoch time: 36.58 seconds, 0.61 minutes\n",
      "layer   1  Sparsity: 97.2860%\n",
      "layer   2  Sparsity: 75.7377%\n",
      "layer   3  Sparsity: 72.9563%\n",
      "total_backward_count 156640 real_backward_count 45883  29.292%\n",
      "fc layer 3 self.abs_max_out: 1733.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.454152/  1.812309, val:  32.92%, val_best:  50.83%, tr:  89.07%, tr_best:  90.60%, epoch time: 36.95 seconds, 0.62 minutes\n",
      "layer   1  Sparsity: 97.2555%\n",
      "layer   2  Sparsity: 75.9221%\n",
      "layer   3  Sparsity: 72.8460%\n",
      "total_backward_count 161535 real_backward_count 47219  29.231%\n",
      "lif layer 2 self.abs_max_v: 10523.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.445615/  1.747508, val:  41.67%, val_best:  50.83%, tr:  90.60%, tr_best:  90.60%, epoch time: 36.52 seconds, 0.61 minutes\n",
      "layer   1  Sparsity: 97.2583%\n",
      "layer   2  Sparsity: 76.0455%\n",
      "layer   3  Sparsity: 72.4986%\n",
      "total_backward_count 166430 real_backward_count 48596  29.199%\n",
      "lif layer 2 self.abs_max_v: 10636.0\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.408123/  1.749080, val:  42.08%, val_best:  50.83%, tr:  90.60%, tr_best:  90.60%, epoch time: 36.72 seconds, 0.61 minutes\n",
      "layer   1  Sparsity: 97.2441%\n",
      "layer   2  Sparsity: 75.7313%\n",
      "layer   3  Sparsity: 73.0068%\n",
      "total_backward_count 171325 real_backward_count 49876  29.112%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.417465/  1.736402, val:  50.83%, val_best:  50.83%, tr:  89.68%, tr_best:  90.60%, epoch time: 36.80 seconds, 0.61 minutes\n",
      "layer   1  Sparsity: 97.2819%\n",
      "layer   2  Sparsity: 76.4518%\n",
      "layer   3  Sparsity: 72.8423%\n",
      "total_backward_count 176220 real_backward_count 51212  29.061%\n",
      "lif layer 2 self.abs_max_v: 10830.5\n",
      "lif layer 2 self.abs_max_v: 10846.5\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.437385/  1.735076, val:  52.92%, val_best:  52.92%, tr:  91.11%, tr_best:  91.11%, epoch time: 36.37 seconds, 0.61 minutes\n",
      "layer   1  Sparsity: 97.2877%\n",
      "layer   2  Sparsity: 76.2861%\n",
      "layer   3  Sparsity: 73.1195%\n",
      "total_backward_count 181115 real_backward_count 52517  28.996%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.414092/  1.845253, val:  22.50%, val_best:  52.92%, tr:  90.60%, tr_best:  91.11%, epoch time: 36.82 seconds, 0.61 minutes\n",
      "layer   1  Sparsity: 97.2592%\n",
      "layer   2  Sparsity: 76.0134%\n",
      "layer   3  Sparsity: 72.6689%\n",
      "total_backward_count 186010 real_backward_count 53792  28.919%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.446198/  1.781906, val:  34.58%, val_best:  52.92%, tr:  91.01%, tr_best:  91.11%, epoch time: 36.72 seconds, 0.61 minutes\n",
      "layer   1  Sparsity: 97.2878%\n",
      "layer   2  Sparsity: 75.8596%\n",
      "layer   3  Sparsity: 72.5649%\n",
      "total_backward_count 190905 real_backward_count 55082  28.853%\n",
      "lif layer 2 self.abs_max_v: 10901.5\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.402691/  1.749782, val:  41.25%, val_best:  52.92%, tr:  90.81%, tr_best:  91.11%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 97.2870%\n",
      "layer   2  Sparsity: 75.9842%\n",
      "layer   3  Sparsity: 72.6094%\n",
      "total_backward_count 195800 real_backward_count 56448  28.829%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.415984/  1.717079, val:  46.67%, val_best:  52.92%, tr:  88.36%, tr_best:  91.11%, epoch time: 40.70 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 97.2453%\n",
      "layer   2  Sparsity: 75.7381%\n",
      "layer   3  Sparsity: 72.8871%\n",
      "total_backward_count 200695 real_backward_count 57752  28.776%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.421991/  1.679661, val:  48.33%, val_best:  52.92%, tr:  89.38%, tr_best:  91.11%, epoch time: 38.99 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2823%\n",
      "layer   2  Sparsity: 75.5435%\n",
      "layer   3  Sparsity: 72.5747%\n",
      "total_backward_count 205590 real_backward_count 59086  28.740%\n",
      "lif layer 1 self.abs_max_v: 8767.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.371872/  1.725478, val:  40.83%, val_best:  52.92%, tr:  89.89%, tr_best:  91.11%, epoch time: 39.70 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 97.2372%\n",
      "layer   2  Sparsity: 75.4855%\n",
      "layer   3  Sparsity: 73.0667%\n",
      "total_backward_count 210485 real_backward_count 60387  28.689%\n",
      "lif layer 1 self.abs_max_v: 9029.5\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.415109/  1.695554, val:  58.33%, val_best:  58.33%, tr:  89.79%, tr_best:  91.11%, epoch time: 38.71 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2707%\n",
      "layer   2  Sparsity: 75.6895%\n",
      "layer   3  Sparsity: 73.2736%\n",
      "total_backward_count 215380 real_backward_count 61712  28.653%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.445921/  1.758098, val:  41.67%, val_best:  58.33%, tr:  90.40%, tr_best:  91.11%, epoch time: 39.61 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 97.2678%\n",
      "layer   2  Sparsity: 75.7669%\n",
      "layer   3  Sparsity: 73.1845%\n",
      "total_backward_count 220275 real_backward_count 63028  28.613%\n",
      "lif layer 2 self.abs_max_v: 10918.5\n",
      "fc layer 2 self.abs_max_out: 6717.0\n",
      "lif layer 2 self.abs_max_v: 11194.5\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.402954/  1.751599, val:  36.67%, val_best:  58.33%, tr:  90.30%, tr_best:  91.11%, epoch time: 38.24 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2608%\n",
      "layer   2  Sparsity: 75.6680%\n",
      "layer   3  Sparsity: 72.6949%\n",
      "total_backward_count 225170 real_backward_count 64319  28.565%\n",
      "fc layer 2 self.abs_max_out: 6722.0\n",
      "fc layer 2 self.abs_max_out: 6829.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.434450/  1.763504, val:  37.92%, val_best:  58.33%, tr:  90.40%, tr_best:  91.11%, epoch time: 39.61 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 97.2842%\n",
      "layer   2  Sparsity: 75.5991%\n",
      "layer   3  Sparsity: 73.4717%\n",
      "total_backward_count 230065 real_backward_count 65629  28.526%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.442328/  1.752538, val:  39.58%, val_best:  58.33%, tr:  90.70%, tr_best:  91.11%, epoch time: 38.53 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2953%\n",
      "layer   2  Sparsity: 75.7941%\n",
      "layer   3  Sparsity: 73.4739%\n",
      "total_backward_count 234960 real_backward_count 66933  28.487%\n",
      "lif layer 1 self.abs_max_v: 9453.0\n",
      "lif layer 2 self.abs_max_v: 11267.5\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.429650/  1.773039, val:  38.33%, val_best:  58.33%, tr:  90.19%, tr_best:  91.11%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 97.2740%\n",
      "layer   2  Sparsity: 75.9394%\n",
      "layer   3  Sparsity: 73.5422%\n",
      "total_backward_count 239855 real_backward_count 68180  28.426%\n",
      "lif layer 1 self.abs_max_v: 9827.5\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.417989/  1.681132, val:  51.67%, val_best:  58.33%, tr:  91.22%, tr_best:  91.22%, epoch time: 38.47 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2536%\n",
      "layer   2  Sparsity: 75.3655%\n",
      "layer   3  Sparsity: 73.5127%\n",
      "total_backward_count 244750 real_backward_count 69485  28.390%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.434409/  1.789621, val:  36.25%, val_best:  58.33%, tr:  89.58%, tr_best:  91.22%, epoch time: 39.54 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 97.2465%\n",
      "layer   2  Sparsity: 75.2433%\n",
      "layer   3  Sparsity: 73.2936%\n",
      "total_backward_count 249645 real_backward_count 70781  28.353%\n",
      "fc layer 1 self.abs_max_out: 9184.0\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.422210/  1.731251, val:  44.58%, val_best:  58.33%, tr:  90.40%, tr_best:  91.22%, epoch time: 38.10 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2439%\n",
      "layer   2  Sparsity: 75.3705%\n",
      "layer   3  Sparsity: 73.7513%\n",
      "total_backward_count 254540 real_backward_count 72052  28.307%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.419061/  1.698416, val:  46.25%, val_best:  58.33%, tr:  89.07%, tr_best:  91.22%, epoch time: 38.72 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2804%\n",
      "layer   2  Sparsity: 75.4818%\n",
      "layer   3  Sparsity: 73.6679%\n",
      "total_backward_count 259435 real_backward_count 73390  28.288%\n",
      "fc layer 2 self.abs_max_out: 7236.0\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.417692/  1.670169, val:  49.58%, val_best:  58.33%, tr:  89.99%, tr_best:  91.22%, epoch time: 38.41 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2658%\n",
      "layer   2  Sparsity: 75.5129%\n",
      "layer   3  Sparsity: 73.9016%\n",
      "total_backward_count 264330 real_backward_count 74712  28.265%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.405957/  1.693930, val:  49.58%, val_best:  58.33%, tr:  88.97%, tr_best:  91.22%, epoch time: 39.48 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 97.2636%\n",
      "layer   2  Sparsity: 75.3443%\n",
      "layer   3  Sparsity: 73.8017%\n",
      "total_backward_count 269225 real_backward_count 76006  28.231%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.404358/  1.742944, val:  48.75%, val_best:  58.33%, tr:  89.68%, tr_best:  91.22%, epoch time: 38.10 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2820%\n",
      "layer   2  Sparsity: 75.2802%\n",
      "layer   3  Sparsity: 73.2325%\n",
      "total_backward_count 274120 real_backward_count 77300  28.199%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.401444/  1.726313, val:  36.67%, val_best:  58.33%, tr:  91.01%, tr_best:  91.22%, epoch time: 39.53 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 97.2633%\n",
      "layer   2  Sparsity: 74.7480%\n",
      "layer   3  Sparsity: 73.1307%\n",
      "total_backward_count 279015 real_backward_count 78543  28.150%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.398919/  1.666746, val:  55.83%, val_best:  58.33%, tr:  92.13%, tr_best:  92.13%, epoch time: 38.07 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.3038%\n",
      "layer   2  Sparsity: 75.0642%\n",
      "layer   3  Sparsity: 73.5626%\n",
      "total_backward_count 283910 real_backward_count 79844  28.123%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.385674/  1.662555, val:  50.83%, val_best:  58.33%, tr:  90.70%, tr_best:  92.13%, epoch time: 39.33 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 97.2744%\n",
      "layer   2  Sparsity: 75.1697%\n",
      "layer   3  Sparsity: 73.4153%\n",
      "total_backward_count 288805 real_backward_count 81069  28.070%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.393466/  1.809770, val:  39.17%, val_best:  58.33%, tr:  90.91%, tr_best:  92.13%, epoch time: 37.77 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2862%\n",
      "layer   2  Sparsity: 75.2571%\n",
      "layer   3  Sparsity: 73.9457%\n",
      "total_backward_count 293700 real_backward_count 82330  28.032%\n",
      "lif layer 1 self.abs_max_v: 9945.0\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.407584/  1.746717, val:  45.42%, val_best:  58.33%, tr:  90.70%, tr_best:  92.13%, epoch time: 38.91 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2702%\n",
      "layer   2  Sparsity: 74.8812%\n",
      "layer   3  Sparsity: 73.8947%\n",
      "total_backward_count 298595 real_backward_count 83601  27.998%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.390044/  1.657466, val:  45.42%, val_best:  58.33%, tr:  91.52%, tr_best:  92.13%, epoch time: 37.93 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2470%\n",
      "layer   2  Sparsity: 74.9537%\n",
      "layer   3  Sparsity: 73.5235%\n",
      "total_backward_count 303490 real_backward_count 84866  27.963%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.403783/  1.692469, val:  51.25%, val_best:  58.33%, tr:  89.89%, tr_best:  92.13%, epoch time: 39.14 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2735%\n",
      "layer   2  Sparsity: 75.4920%\n",
      "layer   3  Sparsity: 74.1747%\n",
      "total_backward_count 308385 real_backward_count 86153  27.937%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.401066/  1.804910, val:  27.08%, val_best:  58.33%, tr:  91.73%, tr_best:  92.13%, epoch time: 38.34 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2791%\n",
      "layer   2  Sparsity: 75.4676%\n",
      "layer   3  Sparsity: 73.6419%\n",
      "total_backward_count 313280 real_backward_count 87398  27.898%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.404335/  1.669756, val:  50.83%, val_best:  58.33%, tr:  91.52%, tr_best:  92.13%, epoch time: 38.65 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2761%\n",
      "layer   2  Sparsity: 75.1104%\n",
      "layer   3  Sparsity: 74.0032%\n",
      "total_backward_count 318175 real_backward_count 88656  27.864%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.379356/  1.704746, val:  45.42%, val_best:  58.33%, tr:  91.22%, tr_best:  92.13%, epoch time: 38.56 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2885%\n",
      "layer   2  Sparsity: 75.1893%\n",
      "layer   3  Sparsity: 74.2503%\n",
      "total_backward_count 323070 real_backward_count 89867  27.817%\n",
      "fc layer 2 self.abs_max_out: 7311.0\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.405619/  1.715021, val:  44.17%, val_best:  58.33%, tr:  91.93%, tr_best:  92.13%, epoch time: 38.80 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2883%\n",
      "layer   2  Sparsity: 74.8805%\n",
      "layer   3  Sparsity: 74.2808%\n",
      "total_backward_count 327965 real_backward_count 91082  27.772%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.439551/  1.702234, val:  57.92%, val_best:  58.33%, tr:  91.32%, tr_best:  92.13%, epoch time: 38.29 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2676%\n",
      "layer   2  Sparsity: 74.7916%\n",
      "layer   3  Sparsity: 74.6400%\n",
      "total_backward_count 332860 real_backward_count 92362  27.748%\n",
      "lif layer 2 self.abs_max_v: 11565.5\n",
      "fc layer 2 self.abs_max_out: 7453.0\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.457488/  1.677269, val:  57.08%, val_best:  58.33%, tr:  89.38%, tr_best:  92.13%, epoch time: 38.58 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2715%\n",
      "layer   2  Sparsity: 74.9564%\n",
      "layer   3  Sparsity: 75.4625%\n",
      "total_backward_count 337755 real_backward_count 93653  27.728%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.468328/  1.796795, val:  42.92%, val_best:  58.33%, tr:  90.81%, tr_best:  92.13%, epoch time: 37.93 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2620%\n",
      "layer   2  Sparsity: 74.9380%\n",
      "layer   3  Sparsity: 75.1827%\n",
      "total_backward_count 342650 real_backward_count 94923  27.703%\n",
      "lif layer 2 self.abs_max_v: 11572.0\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.461581/  1.680200, val:  60.42%, val_best:  60.42%, tr:  91.42%, tr_best:  92.13%, epoch time: 38.73 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2650%\n",
      "layer   2  Sparsity: 75.1678%\n",
      "layer   3  Sparsity: 75.5585%\n",
      "total_backward_count 347545 real_backward_count 96212  27.683%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.444805/  1.793498, val:  41.25%, val_best:  60.42%, tr:  90.91%, tr_best:  92.13%, epoch time: 38.35 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2654%\n",
      "layer   2  Sparsity: 75.1071%\n",
      "layer   3  Sparsity: 75.5294%\n",
      "total_backward_count 352440 real_backward_count 97496  27.663%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.449841/  1.671497, val:  52.50%, val_best:  60.42%, tr:  90.40%, tr_best:  92.13%, epoch time: 39.40 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 97.2344%\n",
      "layer   2  Sparsity: 75.0136%\n",
      "layer   3  Sparsity: 75.1617%\n",
      "total_backward_count 357335 real_backward_count 98767  27.640%\n",
      "lif layer 1 self.abs_max_v: 10442.0\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.436182/  1.753432, val:  45.42%, val_best:  60.42%, tr:  91.83%, tr_best:  92.13%, epoch time: 38.61 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2946%\n",
      "layer   2  Sparsity: 75.1301%\n",
      "layer   3  Sparsity: 75.0021%\n",
      "total_backward_count 362230 real_backward_count 99966  27.597%\n",
      "lif layer 2 self.abs_max_v: 11780.0\n",
      "fc layer 2 self.abs_max_out: 7786.0\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.451777/  1.761118, val:  46.25%, val_best:  60.42%, tr:  90.91%, tr_best:  92.13%, epoch time: 39.34 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 97.2409%\n",
      "layer   2  Sparsity: 74.6265%\n",
      "layer   3  Sparsity: 75.0901%\n",
      "total_backward_count 367125 real_backward_count 101257  27.581%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.435613/  1.722244, val:  50.42%, val_best:  60.42%, tr:  90.09%, tr_best:  92.13%, epoch time: 38.43 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2866%\n",
      "layer   2  Sparsity: 75.0649%\n",
      "layer   3  Sparsity: 75.4514%\n",
      "total_backward_count 372020 real_backward_count 102490  27.550%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.464275/  1.674590, val:  61.25%, val_best:  61.25%, tr:  91.01%, tr_best:  92.13%, epoch time: 39.25 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2755%\n",
      "layer   2  Sparsity: 75.1333%\n",
      "layer   3  Sparsity: 75.3842%\n",
      "total_backward_count 376915 real_backward_count 103742  27.524%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.454632/  1.803214, val:  33.75%, val_best:  61.25%, tr:  90.19%, tr_best:  92.13%, epoch time: 38.93 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2831%\n",
      "layer   2  Sparsity: 74.8784%\n",
      "layer   3  Sparsity: 74.9274%\n",
      "total_backward_count 381810 real_backward_count 105023  27.507%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.465529/  1.805664, val:  27.50%, val_best:  61.25%, tr:  91.62%, tr_best:  92.13%, epoch time: 38.51 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2683%\n",
      "layer   2  Sparsity: 74.9179%\n",
      "layer   3  Sparsity: 75.2842%\n",
      "total_backward_count 386705 real_backward_count 106292  27.487%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.468962/  1.714303, val:  60.42%, val_best:  61.25%, tr:  90.91%, tr_best:  92.13%, epoch time: 38.74 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2750%\n",
      "layer   2  Sparsity: 74.9083%\n",
      "layer   3  Sparsity: 75.3801%\n",
      "total_backward_count 391600 real_backward_count 107551  27.465%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.417342/  1.676311, val:  59.17%, val_best:  61.25%, tr:  91.01%, tr_best:  92.13%, epoch time: 38.89 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2632%\n",
      "layer   2  Sparsity: 74.6182%\n",
      "layer   3  Sparsity: 74.7720%\n",
      "total_backward_count 396495 real_backward_count 108766  27.432%\n",
      "fc layer 2 self.abs_max_out: 7845.0\n",
      "fc layer 2 self.abs_max_out: 7918.0\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.472142/  1.730818, val:  43.33%, val_best:  61.25%, tr:  90.19%, tr_best:  92.13%, epoch time: 39.04 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2907%\n",
      "layer   2  Sparsity: 74.9214%\n",
      "layer   3  Sparsity: 75.6342%\n",
      "total_backward_count 401390 real_backward_count 110051  27.417%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.464837/  1.737297, val:  50.83%, val_best:  61.25%, tr:  91.11%, tr_best:  92.13%, epoch time: 38.67 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2738%\n",
      "layer   2  Sparsity: 74.5549%\n",
      "layer   3  Sparsity: 76.1639%\n",
      "total_backward_count 406285 real_backward_count 111327  27.401%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.438175/  1.734000, val:  49.17%, val_best:  61.25%, tr:  91.73%, tr_best:  92.13%, epoch time: 38.82 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2480%\n",
      "layer   2  Sparsity: 74.5708%\n",
      "layer   3  Sparsity: 76.1485%\n",
      "total_backward_count 411180 real_backward_count 112573  27.378%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.456164/  1.723423, val:  44.17%, val_best:  61.25%, tr:  91.62%, tr_best:  92.13%, epoch time: 38.65 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2599%\n",
      "layer   2  Sparsity: 74.8384%\n",
      "layer   3  Sparsity: 75.8476%\n",
      "total_backward_count 416075 real_backward_count 113804  27.352%\n",
      "lif layer 1 self.abs_max_v: 10572.0\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.450013/  1.816397, val:  26.67%, val_best:  61.25%, tr:  91.93%, tr_best:  92.13%, epoch time: 38.78 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2764%\n",
      "layer   2  Sparsity: 74.8934%\n",
      "layer   3  Sparsity: 75.6248%\n",
      "total_backward_count 420970 real_backward_count 115071  27.335%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.424001/  1.695121, val:  54.58%, val_best:  61.25%, tr:  91.73%, tr_best:  92.13%, epoch time: 38.84 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2758%\n",
      "layer   2  Sparsity: 74.6780%\n",
      "layer   3  Sparsity: 75.1722%\n",
      "total_backward_count 425865 real_backward_count 116315  27.313%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.436558/  1.673573, val:  62.08%, val_best:  62.08%, tr:  91.01%, tr_best:  92.13%, epoch time: 38.54 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2535%\n",
      "layer   2  Sparsity: 74.7934%\n",
      "layer   3  Sparsity: 75.4318%\n",
      "total_backward_count 430760 real_backward_count 117556  27.290%\n",
      "fc layer 2 self.abs_max_out: 8104.0\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.445638/  1.719740, val:  48.75%, val_best:  62.08%, tr:  91.42%, tr_best:  92.13%, epoch time: 38.58 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2728%\n",
      "layer   2  Sparsity: 75.1574%\n",
      "layer   3  Sparsity: 75.0418%\n",
      "total_backward_count 435655 real_backward_count 118814  27.272%\n",
      "lif layer 1 self.abs_max_v: 10774.0\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.463203/  1.689170, val:  52.08%, val_best:  62.08%, tr:  91.11%, tr_best:  92.13%, epoch time: 38.92 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2941%\n",
      "layer   2  Sparsity: 75.0159%\n",
      "layer   3  Sparsity: 75.8953%\n",
      "total_backward_count 440550 real_backward_count 120095  27.260%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.446667/  1.705923, val:  50.42%, val_best:  62.08%, tr:  92.65%, tr_best:  92.65%, epoch time: 38.41 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2779%\n",
      "layer   2  Sparsity: 75.0946%\n",
      "layer   3  Sparsity: 76.2956%\n",
      "total_backward_count 445445 real_backward_count 121324  27.237%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.424890/  1.639109, val:  59.58%, val_best:  62.08%, tr:  91.32%, tr_best:  92.65%, epoch time: 38.55 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2526%\n",
      "layer   2  Sparsity: 74.8349%\n",
      "layer   3  Sparsity: 76.3671%\n",
      "total_backward_count 450340 real_backward_count 122529  27.208%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.434733/  1.698776, val:  51.25%, val_best:  62.08%, tr:  89.99%, tr_best:  92.65%, epoch time: 38.17 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2761%\n",
      "layer   2  Sparsity: 74.8194%\n",
      "layer   3  Sparsity: 76.3580%\n",
      "total_backward_count 455235 real_backward_count 123809  27.197%\n",
      "fc layer 1 self.abs_max_out: 9427.0\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.441249/  1.679265, val:  55.00%, val_best:  62.08%, tr:  89.89%, tr_best:  92.65%, epoch time: 39.11 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2647%\n",
      "layer   2  Sparsity: 75.0547%\n",
      "layer   3  Sparsity: 76.0686%\n",
      "total_backward_count 460130 real_backward_count 125014  27.169%\n",
      "fc layer 2 self.abs_max_out: 8393.0\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.455093/  1.692536, val:  47.92%, val_best:  62.08%, tr:  92.13%, tr_best:  92.65%, epoch time: 38.29 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2964%\n",
      "layer   2  Sparsity: 74.8814%\n",
      "layer   3  Sparsity: 76.8288%\n",
      "total_backward_count 465025 real_backward_count 126265  27.152%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.461402/  1.707693, val:  63.33%, val_best:  63.33%, tr:  91.93%, tr_best:  92.65%, epoch time: 38.49 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2661%\n",
      "layer   2  Sparsity: 74.8852%\n",
      "layer   3  Sparsity: 77.1860%\n",
      "total_backward_count 469920 real_backward_count 127531  27.139%\n",
      "fc layer 1 self.abs_max_out: 9481.0\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.449153/  1.743479, val:  38.33%, val_best:  63.33%, tr:  91.52%, tr_best:  92.65%, epoch time: 38.86 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2723%\n",
      "layer   2  Sparsity: 75.1448%\n",
      "layer   3  Sparsity: 76.5705%\n",
      "total_backward_count 474815 real_backward_count 128723  27.110%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.438961/  1.691928, val:  62.08%, val_best:  63.33%, tr:  92.13%, tr_best:  92.65%, epoch time: 38.69 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2833%\n",
      "layer   2  Sparsity: 74.9710%\n",
      "layer   3  Sparsity: 76.8899%\n",
      "total_backward_count 479710 real_backward_count 129915  27.082%\n",
      "lif layer 1 self.abs_max_v: 11155.0\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.436136/  1.780748, val:  37.92%, val_best:  63.33%, tr:  91.93%, tr_best:  92.65%, epoch time: 38.40 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2946%\n",
      "layer   2  Sparsity: 75.3778%\n",
      "layer   3  Sparsity: 76.2301%\n",
      "total_backward_count 484605 real_backward_count 131146  27.062%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.446306/  1.640944, val:  60.42%, val_best:  63.33%, tr:  92.03%, tr_best:  92.65%, epoch time: 38.37 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2919%\n",
      "layer   2  Sparsity: 75.2306%\n",
      "layer   3  Sparsity: 76.1224%\n",
      "total_backward_count 489500 real_backward_count 132385  27.045%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.445866/  1.717732, val:  46.25%, val_best:  63.33%, tr:  91.42%, tr_best:  92.65%, epoch time: 38.35 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2738%\n",
      "layer   2  Sparsity: 75.1824%\n",
      "layer   3  Sparsity: 76.4998%\n",
      "total_backward_count 494395 real_backward_count 133581  27.019%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.473201/  1.674096, val:  46.67%, val_best:  63.33%, tr:  91.22%, tr_best:  92.65%, epoch time: 38.27 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2588%\n",
      "layer   2  Sparsity: 74.6900%\n",
      "layer   3  Sparsity: 76.5310%\n",
      "total_backward_count 499290 real_backward_count 134845  27.007%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.440846/  1.724707, val:  36.25%, val_best:  63.33%, tr:  91.83%, tr_best:  92.65%, epoch time: 38.30 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2583%\n",
      "layer   2  Sparsity: 74.8502%\n",
      "layer   3  Sparsity: 76.0062%\n",
      "total_backward_count 504185 real_backward_count 136106  26.995%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.439640/  1.679802, val:  53.33%, val_best:  63.33%, tr:  92.24%, tr_best:  92.65%, epoch time: 37.83 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2454%\n",
      "layer   2  Sparsity: 74.9622%\n",
      "layer   3  Sparsity: 75.7345%\n",
      "total_backward_count 509080 real_backward_count 137333  26.977%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.463706/  1.729884, val:  49.58%, val_best:  63.33%, tr:  91.22%, tr_best:  92.65%, epoch time: 38.49 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2468%\n",
      "layer   2  Sparsity: 74.8176%\n",
      "layer   3  Sparsity: 75.6515%\n",
      "total_backward_count 513975 real_backward_count 138565  26.959%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.451335/  1.701035, val:  61.67%, val_best:  63.33%, tr:  91.93%, tr_best:  92.65%, epoch time: 39.25 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2872%\n",
      "layer   2  Sparsity: 75.1936%\n",
      "layer   3  Sparsity: 76.3009%\n",
      "total_backward_count 518870 real_backward_count 139828  26.949%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.450156/  1.746012, val:  39.17%, val_best:  63.33%, tr:  91.32%, tr_best:  92.65%, epoch time: 38.83 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2772%\n",
      "layer   2  Sparsity: 75.1343%\n",
      "layer   3  Sparsity: 76.4911%\n",
      "total_backward_count 523765 real_backward_count 141027  26.926%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.456883/  1.679557, val:  55.00%, val_best:  63.33%, tr:  93.56%, tr_best:  93.56%, epoch time: 37.88 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2886%\n",
      "layer   2  Sparsity: 75.0386%\n",
      "layer   3  Sparsity: 76.1575%\n",
      "total_backward_count 528660 real_backward_count 142233  26.904%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.452476/  1.673185, val:  60.42%, val_best:  63.33%, tr:  92.24%, tr_best:  93.56%, epoch time: 38.39 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2939%\n",
      "layer   2  Sparsity: 75.0720%\n",
      "layer   3  Sparsity: 75.9595%\n",
      "total_backward_count 533555 real_backward_count 143460  26.888%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.423704/  1.721125, val:  46.25%, val_best:  63.33%, tr:  91.52%, tr_best:  93.56%, epoch time: 38.00 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2667%\n",
      "layer   2  Sparsity: 75.1878%\n",
      "layer   3  Sparsity: 75.2984%\n",
      "total_backward_count 538450 real_backward_count 144664  26.867%\n",
      "fc layer 2 self.abs_max_out: 8621.0\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.442569/  1.664003, val:  62.08%, val_best:  63.33%, tr:  92.34%, tr_best:  93.56%, epoch time: 38.58 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2700%\n",
      "layer   2  Sparsity: 74.8621%\n",
      "layer   3  Sparsity: 75.6390%\n",
      "total_backward_count 543345 real_backward_count 145870  26.847%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.434689/  1.724264, val:  40.42%, val_best:  63.33%, tr:  92.54%, tr_best:  93.56%, epoch time: 37.95 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2587%\n",
      "layer   2  Sparsity: 74.8410%\n",
      "layer   3  Sparsity: 75.5669%\n",
      "total_backward_count 548240 real_backward_count 147037  26.820%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.426510/  1.731271, val:  53.75%, val_best:  63.33%, tr:  92.34%, tr_best:  93.56%, epoch time: 38.88 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2758%\n",
      "layer   2  Sparsity: 75.2501%\n",
      "layer   3  Sparsity: 75.5488%\n",
      "total_backward_count 553135 real_backward_count 148305  26.812%\n",
      "fc layer 2 self.abs_max_out: 8664.0\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.408938/  1.669949, val:  50.42%, val_best:  63.33%, tr:  92.85%, tr_best:  93.56%, epoch time: 38.27 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2594%\n",
      "layer   2  Sparsity: 75.0131%\n",
      "layer   3  Sparsity: 75.5059%\n",
      "total_backward_count 558030 real_backward_count 149504  26.791%\n",
      "fc layer 2 self.abs_max_out: 8704.0\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.436210/  1.702533, val:  40.00%, val_best:  63.33%, tr:  90.70%, tr_best:  93.56%, epoch time: 38.23 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2618%\n",
      "layer   2  Sparsity: 75.1945%\n",
      "layer   3  Sparsity: 75.4281%\n",
      "total_backward_count 562925 real_backward_count 150718  26.774%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.434388/  1.764240, val:  32.08%, val_best:  63.33%, tr:  91.83%, tr_best:  93.56%, epoch time: 37.95 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2673%\n",
      "layer   2  Sparsity: 75.0256%\n",
      "layer   3  Sparsity: 74.8083%\n",
      "total_backward_count 567820 real_backward_count 151983  26.766%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.415046/  1.666903, val:  52.92%, val_best:  63.33%, tr:  90.60%, tr_best:  93.56%, epoch time: 38.37 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2783%\n",
      "layer   2  Sparsity: 75.0397%\n",
      "layer   3  Sparsity: 74.3582%\n",
      "total_backward_count 572715 real_backward_count 153230  26.755%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.420931/  1.667645, val:  51.67%, val_best:  63.33%, tr:  91.73%, tr_best:  93.56%, epoch time: 38.07 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2761%\n",
      "layer   2  Sparsity: 75.2826%\n",
      "layer   3  Sparsity: 73.9032%\n",
      "total_backward_count 577610 real_backward_count 154469  26.743%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.404252/  1.699943, val:  40.42%, val_best:  63.33%, tr:  91.22%, tr_best:  93.56%, epoch time: 39.30 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2945%\n",
      "layer   2  Sparsity: 75.3872%\n",
      "layer   3  Sparsity: 73.9053%\n",
      "total_backward_count 582505 real_backward_count 155671  26.724%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.399125/  1.634244, val:  62.50%, val_best:  63.33%, tr:  92.95%, tr_best:  93.56%, epoch time: 38.01 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2814%\n",
      "layer   2  Sparsity: 75.1646%\n",
      "layer   3  Sparsity: 74.4979%\n",
      "total_backward_count 587400 real_backward_count 156821  26.697%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.397777/  1.693730, val:  48.33%, val_best:  63.33%, tr:  91.83%, tr_best:  93.56%, epoch time: 38.92 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2798%\n",
      "layer   2  Sparsity: 74.9500%\n",
      "layer   3  Sparsity: 74.1833%\n",
      "total_backward_count 592295 real_backward_count 158008  26.677%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.410900/  1.701606, val:  58.75%, val_best:  63.33%, tr:  91.73%, tr_best:  93.56%, epoch time: 38.20 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2816%\n",
      "layer   2  Sparsity: 75.2916%\n",
      "layer   3  Sparsity: 74.1031%\n",
      "total_backward_count 597190 real_backward_count 159239  26.665%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.413324/  1.726326, val:  37.08%, val_best:  63.33%, tr:  92.85%, tr_best:  93.56%, epoch time: 38.75 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2526%\n",
      "layer   2  Sparsity: 74.7334%\n",
      "layer   3  Sparsity: 73.7172%\n",
      "total_backward_count 602085 real_backward_count 160482  26.654%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.426874/  1.654807, val:  62.08%, val_best:  63.33%, tr:  91.52%, tr_best:  93.56%, epoch time: 38.10 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2769%\n",
      "layer   2  Sparsity: 75.1927%\n",
      "layer   3  Sparsity: 74.4298%\n",
      "total_backward_count 606980 real_backward_count 161739  26.647%\n",
      "fc layer 2 self.abs_max_out: 9317.0\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.412339/  1.649165, val:  60.42%, val_best:  63.33%, tr:  93.16%, tr_best:  93.56%, epoch time: 38.51 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2587%\n",
      "layer   2  Sparsity: 75.1748%\n",
      "layer   3  Sparsity: 74.2071%\n",
      "total_backward_count 611875 real_backward_count 162912  26.625%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.417901/  1.700155, val:  42.08%, val_best:  63.33%, tr:  92.24%, tr_best:  93.56%, epoch time: 38.19 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2662%\n",
      "layer   2  Sparsity: 75.1080%\n",
      "layer   3  Sparsity: 74.0695%\n",
      "total_backward_count 616770 real_backward_count 164132  26.612%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.427620/  1.733191, val:  42.50%, val_best:  63.33%, tr:  92.34%, tr_best:  93.56%, epoch time: 38.73 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2451%\n",
      "layer   2  Sparsity: 75.0446%\n",
      "layer   3  Sparsity: 74.5376%\n",
      "total_backward_count 621665 real_backward_count 165384  26.603%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.411439/  1.726586, val:  39.17%, val_best:  63.33%, tr:  92.54%, tr_best:  93.56%, epoch time: 38.33 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2495%\n",
      "layer   2  Sparsity: 75.0240%\n",
      "layer   3  Sparsity: 74.1267%\n",
      "total_backward_count 626560 real_backward_count 166578  26.586%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.393809/  1.652931, val:  52.08%, val_best:  63.33%, tr:  91.62%, tr_best:  93.56%, epoch time: 38.86 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2648%\n",
      "layer   2  Sparsity: 75.1692%\n",
      "layer   3  Sparsity: 73.2884%\n",
      "total_backward_count 631455 real_backward_count 167844  26.581%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.381857/  1.691190, val:  55.00%, val_best:  63.33%, tr:  90.81%, tr_best:  93.56%, epoch time: 38.14 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2901%\n",
      "layer   2  Sparsity: 75.1627%\n",
      "layer   3  Sparsity: 73.5687%\n",
      "total_backward_count 636350 real_backward_count 169108  26.575%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.404238/  1.668420, val:  39.17%, val_best:  63.33%, tr:  91.32%, tr_best:  93.56%, epoch time: 38.74 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2820%\n",
      "layer   2  Sparsity: 74.8756%\n",
      "layer   3  Sparsity: 74.3571%\n",
      "total_backward_count 641245 real_backward_count 170329  26.562%\n",
      "fc layer 2 self.abs_max_out: 9693.0\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.367115/  1.693822, val:  35.42%, val_best:  63.33%, tr:  92.24%, tr_best:  93.56%, epoch time: 38.25 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2921%\n",
      "layer   2  Sparsity: 74.8933%\n",
      "layer   3  Sparsity: 74.2386%\n",
      "total_backward_count 646140 real_backward_count 171561  26.552%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.384964/  1.640612, val:  54.17%, val_best:  63.33%, tr:  94.28%, tr_best:  94.28%, epoch time: 38.33 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2594%\n",
      "layer   2  Sparsity: 75.1950%\n",
      "layer   3  Sparsity: 74.6935%\n",
      "total_backward_count 651035 real_backward_count 172736  26.533%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.382557/  1.699157, val:  56.67%, val_best:  63.33%, tr:  92.03%, tr_best:  94.28%, epoch time: 38.37 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2475%\n",
      "layer   2  Sparsity: 74.8855%\n",
      "layer   3  Sparsity: 74.3231%\n",
      "total_backward_count 655930 real_backward_count 173930  26.517%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.410861/  1.715556, val:  50.83%, val_best:  63.33%, tr:  92.65%, tr_best:  94.28%, epoch time: 38.45 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2784%\n",
      "layer   2  Sparsity: 75.0219%\n",
      "layer   3  Sparsity: 74.5255%\n",
      "total_backward_count 660825 real_backward_count 175170  26.508%\n",
      "lif layer 1 self.abs_max_v: 11172.0\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.396394/  1.632514, val:  61.25%, val_best:  63.33%, tr:  92.24%, tr_best:  94.28%, epoch time: 38.32 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2726%\n",
      "layer   2  Sparsity: 74.7945%\n",
      "layer   3  Sparsity: 74.4249%\n",
      "total_backward_count 665720 real_backward_count 176420  26.501%\n",
      "lif layer 1 self.abs_max_v: 11627.0\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.371487/  1.663953, val:  49.17%, val_best:  63.33%, tr:  93.05%, tr_best:  94.28%, epoch time: 38.04 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2745%\n",
      "layer   2  Sparsity: 74.8193%\n",
      "layer   3  Sparsity: 73.9846%\n",
      "total_backward_count 670615 real_backward_count 177623  26.487%\n",
      "lif layer 1 self.abs_max_v: 11692.5\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.365510/  1.685424, val:  51.25%, val_best:  63.33%, tr:  92.95%, tr_best:  94.28%, epoch time: 38.28 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2552%\n",
      "layer   2  Sparsity: 74.9512%\n",
      "layer   3  Sparsity: 74.1300%\n",
      "total_backward_count 675510 real_backward_count 178817  26.471%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.393963/  1.659578, val:  45.00%, val_best:  63.33%, tr:  91.11%, tr_best:  94.28%, epoch time: 37.85 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2662%\n",
      "layer   2  Sparsity: 75.0583%\n",
      "layer   3  Sparsity: 73.5243%\n",
      "total_backward_count 680405 real_backward_count 180123  26.473%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.380338/  1.675137, val:  47.08%, val_best:  63.33%, tr:  92.85%, tr_best:  94.28%, epoch time: 38.64 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2665%\n",
      "layer   2  Sparsity: 74.9450%\n",
      "layer   3  Sparsity: 73.5315%\n",
      "total_backward_count 685300 real_backward_count 181325  26.459%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.353685/  1.637782, val:  52.50%, val_best:  63.33%, tr:  92.65%, tr_best:  94.28%, epoch time: 38.16 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2643%\n",
      "layer   2  Sparsity: 74.9259%\n",
      "layer   3  Sparsity: 72.7782%\n",
      "total_backward_count 690195 real_backward_count 182524  26.445%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.337842/  1.655755, val:  55.83%, val_best:  63.33%, tr:  92.85%, tr_best:  94.28%, epoch time: 38.44 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2774%\n",
      "layer   2  Sparsity: 75.1414%\n",
      "layer   3  Sparsity: 73.2450%\n",
      "total_backward_count 695090 real_backward_count 183750  26.435%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.365954/  1.663002, val:  49.58%, val_best:  63.33%, tr:  91.73%, tr_best:  94.28%, epoch time: 38.15 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2824%\n",
      "layer   2  Sparsity: 75.0502%\n",
      "layer   3  Sparsity: 73.6042%\n",
      "total_backward_count 699985 real_backward_count 184972  26.425%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.349837/  1.605971, val:  59.17%, val_best:  63.33%, tr:  93.16%, tr_best:  94.28%, epoch time: 38.55 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2701%\n",
      "layer   2  Sparsity: 74.9705%\n",
      "layer   3  Sparsity: 73.9276%\n",
      "total_backward_count 704880 real_backward_count 186141  26.407%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.378458/  1.620172, val:  57.08%, val_best:  63.33%, tr:  91.83%, tr_best:  94.28%, epoch time: 38.46 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2712%\n",
      "layer   2  Sparsity: 75.0290%\n",
      "layer   3  Sparsity: 74.0891%\n",
      "total_backward_count 709775 real_backward_count 187371  26.399%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.352954/  1.659649, val:  56.25%, val_best:  63.33%, tr:  92.03%, tr_best:  94.28%, epoch time: 38.89 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2774%\n",
      "layer   2  Sparsity: 74.7223%\n",
      "layer   3  Sparsity: 73.4430%\n",
      "total_backward_count 714670 real_backward_count 188575  26.386%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.372675/  1.686712, val:  42.92%, val_best:  63.33%, tr:  91.01%, tr_best:  94.28%, epoch time: 38.32 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.3059%\n",
      "layer   2  Sparsity: 74.9678%\n",
      "layer   3  Sparsity: 73.0891%\n",
      "total_backward_count 719565 real_backward_count 189794  26.376%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.338657/  1.681315, val:  48.75%, val_best:  63.33%, tr:  93.16%, tr_best:  94.28%, epoch time: 38.21 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2786%\n",
      "layer   2  Sparsity: 75.0262%\n",
      "layer   3  Sparsity: 73.3588%\n",
      "total_backward_count 724460 real_backward_count 190976  26.361%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.325294/  1.569671, val:  55.83%, val_best:  63.33%, tr:  93.67%, tr_best:  94.28%, epoch time: 38.55 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2488%\n",
      "layer   2  Sparsity: 74.8493%\n",
      "layer   3  Sparsity: 72.7289%\n",
      "total_backward_count 729355 real_backward_count 192201  26.352%\n",
      "fc layer 2 self.abs_max_out: 9948.0\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.379823/  1.635220, val:  55.83%, val_best:  63.33%, tr:  92.85%, tr_best:  94.28%, epoch time: 39.18 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2900%\n",
      "layer   2  Sparsity: 75.0256%\n",
      "layer   3  Sparsity: 73.3957%\n",
      "total_backward_count 734250 real_backward_count 193422  26.343%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.355552/  1.667268, val:  50.83%, val_best:  63.33%, tr:  92.54%, tr_best:  94.28%, epoch time: 38.45 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2621%\n",
      "layer   2  Sparsity: 74.9878%\n",
      "layer   3  Sparsity: 73.6189%\n",
      "total_backward_count 739145 real_backward_count 194626  26.331%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.351535/  1.622377, val:  53.33%, val_best:  63.33%, tr:  93.26%, tr_best:  94.28%, epoch time: 38.74 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2833%\n",
      "layer   2  Sparsity: 75.0760%\n",
      "layer   3  Sparsity: 74.0460%\n",
      "total_backward_count 744040 real_backward_count 195878  26.326%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.351191/  1.638966, val:  59.58%, val_best:  63.33%, tr:  91.93%, tr_best:  94.28%, epoch time: 38.60 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2804%\n",
      "layer   2  Sparsity: 75.1121%\n",
      "layer   3  Sparsity: 73.0837%\n",
      "total_backward_count 748935 real_backward_count 197095  26.317%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.350476/  1.553489, val:  60.42%, val_best:  63.33%, tr:  92.54%, tr_best:  94.28%, epoch time: 38.27 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2829%\n",
      "layer   2  Sparsity: 74.8584%\n",
      "layer   3  Sparsity: 73.1928%\n",
      "total_backward_count 753830 real_backward_count 198324  26.309%\n",
      "fc layer 2 self.abs_max_out: 9957.0\n",
      "fc layer 2 self.abs_max_out: 10237.0\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.374536/  1.598990, val:  65.83%, val_best:  65.83%, tr:  91.52%, tr_best:  94.28%, epoch time: 38.40 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2741%\n",
      "layer   2  Sparsity: 74.7990%\n",
      "layer   3  Sparsity: 73.5771%\n",
      "total_backward_count 758725 real_backward_count 199577  26.304%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.349212/  1.644637, val:  52.92%, val_best:  65.83%, tr:  93.05%, tr_best:  94.28%, epoch time: 38.02 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2552%\n",
      "layer   2  Sparsity: 74.5353%\n",
      "layer   3  Sparsity: 73.8134%\n",
      "total_backward_count 763620 real_backward_count 200749  26.289%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.381318/  1.680463, val:  52.50%, val_best:  65.83%, tr:  92.24%, tr_best:  94.28%, epoch time: 38.53 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2711%\n",
      "layer   2  Sparsity: 74.3507%\n",
      "layer   3  Sparsity: 73.8203%\n",
      "total_backward_count 768515 real_backward_count 201945  26.277%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.439815/  1.692301, val:  55.83%, val_best:  65.83%, tr:  91.93%, tr_best:  94.28%, epoch time: 38.25 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2955%\n",
      "layer   2  Sparsity: 74.5312%\n",
      "layer   3  Sparsity: 74.5336%\n",
      "total_backward_count 773410 real_backward_count 203145  26.266%\n",
      "fc layer 2 self.abs_max_out: 10299.0\n",
      "lif layer 1 self.abs_max_v: 11700.0\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.420396/  1.666292, val:  59.17%, val_best:  65.83%, tr:  91.32%, tr_best:  94.28%, epoch time: 38.33 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2794%\n",
      "layer   2  Sparsity: 74.5526%\n",
      "layer   3  Sparsity: 75.0789%\n",
      "total_backward_count 778305 real_backward_count 204428  26.266%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.412753/  1.607409, val:  69.17%, val_best:  69.17%, tr:  92.75%, tr_best:  94.28%, epoch time: 38.13 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2611%\n",
      "layer   2  Sparsity: 74.5852%\n",
      "layer   3  Sparsity: 74.7805%\n",
      "total_backward_count 783200 real_backward_count 205625  26.254%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.396444/  1.658955, val:  52.50%, val_best:  69.17%, tr:  93.16%, tr_best:  94.28%, epoch time: 38.53 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2687%\n",
      "layer   2  Sparsity: 74.5733%\n",
      "layer   3  Sparsity: 74.8873%\n",
      "total_backward_count 788095 real_backward_count 206818  26.243%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.401905/  1.650438, val:  52.08%, val_best:  69.17%, tr:  92.44%, tr_best:  94.28%, epoch time: 38.27 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2655%\n",
      "layer   2  Sparsity: 74.5111%\n",
      "layer   3  Sparsity: 75.5618%\n",
      "total_backward_count 792990 real_backward_count 208051  26.236%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.396336/  1.646290, val:  56.67%, val_best:  69.17%, tr:  92.03%, tr_best:  94.28%, epoch time: 38.89 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2652%\n",
      "layer   2  Sparsity: 74.7253%\n",
      "layer   3  Sparsity: 76.1051%\n",
      "total_backward_count 797885 real_backward_count 209288  26.230%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.385187/  1.705630, val:  57.08%, val_best:  69.17%, tr:  93.77%, tr_best:  94.28%, epoch time: 38.27 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2604%\n",
      "layer   2  Sparsity: 74.5180%\n",
      "layer   3  Sparsity: 75.7730%\n",
      "total_backward_count 802780 real_backward_count 210414  26.211%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.447426/  1.706966, val:  36.67%, val_best:  69.17%, tr:  91.01%, tr_best:  94.28%, epoch time: 38.49 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2750%\n",
      "layer   2  Sparsity: 74.5590%\n",
      "layer   3  Sparsity: 76.2244%\n",
      "total_backward_count 807675 real_backward_count 211617  26.201%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.425792/  1.681265, val:  55.00%, val_best:  69.17%, tr:  91.52%, tr_best:  94.28%, epoch time: 38.46 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2530%\n",
      "layer   2  Sparsity: 74.7967%\n",
      "layer   3  Sparsity: 75.5293%\n",
      "total_backward_count 812570 real_backward_count 212868  26.197%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.421508/  1.665250, val:  52.92%, val_best:  69.17%, tr:  93.05%, tr_best:  94.28%, epoch time: 38.48 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2871%\n",
      "layer   2  Sparsity: 74.7743%\n",
      "layer   3  Sparsity: 76.3491%\n",
      "total_backward_count 817465 real_backward_count 214053  26.185%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.417026/  1.676103, val:  50.42%, val_best:  69.17%, tr:  92.85%, tr_best:  94.28%, epoch time: 37.81 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2461%\n",
      "layer   2  Sparsity: 74.3227%\n",
      "layer   3  Sparsity: 74.8887%\n",
      "total_backward_count 822360 real_backward_count 215263  26.176%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.430030/  1.699113, val:  47.50%, val_best:  69.17%, tr:  91.01%, tr_best:  94.28%, epoch time: 38.66 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2860%\n",
      "layer   2  Sparsity: 74.6479%\n",
      "layer   3  Sparsity: 75.8400%\n",
      "total_backward_count 827255 real_backward_count 216515  26.173%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.428487/  1.661016, val:  56.25%, val_best:  69.17%, tr:  91.73%, tr_best:  94.28%, epoch time: 38.37 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2435%\n",
      "layer   2  Sparsity: 74.6392%\n",
      "layer   3  Sparsity: 75.6366%\n",
      "total_backward_count 832150 real_backward_count 217789  26.172%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.403943/  1.717166, val:  42.92%, val_best:  69.17%, tr:  92.24%, tr_best:  94.28%, epoch time: 38.38 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2760%\n",
      "layer   2  Sparsity: 74.4140%\n",
      "layer   3  Sparsity: 75.1337%\n",
      "total_backward_count 837045 real_backward_count 218971  26.160%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.386848/  1.681498, val:  47.08%, val_best:  69.17%, tr:  93.36%, tr_best:  94.28%, epoch time: 38.05 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2693%\n",
      "layer   2  Sparsity: 74.3769%\n",
      "layer   3  Sparsity: 74.2870%\n",
      "total_backward_count 841940 real_backward_count 220152  26.148%\n",
      "lif layer 1 self.abs_max_v: 12700.5\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.401520/  1.679111, val:  58.75%, val_best:  69.17%, tr:  92.03%, tr_best:  94.28%, epoch time: 38.53 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2635%\n",
      "layer   2  Sparsity: 74.4961%\n",
      "layer   3  Sparsity: 74.5267%\n",
      "total_backward_count 846835 real_backward_count 221353  26.139%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.396496/  1.654112, val:  42.50%, val_best:  69.17%, tr:  92.24%, tr_best:  94.28%, epoch time: 38.38 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2814%\n",
      "layer   2  Sparsity: 74.6815%\n",
      "layer   3  Sparsity: 74.5816%\n",
      "total_backward_count 851730 real_backward_count 222494  26.123%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.372906/  1.651275, val:  53.75%, val_best:  69.17%, tr:  92.85%, tr_best:  94.28%, epoch time: 38.76 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2492%\n",
      "layer   2  Sparsity: 74.7249%\n",
      "layer   3  Sparsity: 74.5715%\n",
      "total_backward_count 856625 real_backward_count 223707  26.115%\n",
      "fc layer 1 self.abs_max_out: 9823.0\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.374483/  1.712042, val:  51.67%, val_best:  69.17%, tr:  91.93%, tr_best:  94.28%, epoch time: 38.08 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2613%\n",
      "layer   2  Sparsity: 74.6704%\n",
      "layer   3  Sparsity: 74.3048%\n",
      "total_backward_count 861520 real_backward_count 224938  26.109%\n",
      "fc layer 2 self.abs_max_out: 10327.0\n",
      "fc layer 2 self.abs_max_out: 10831.0\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.387993/  1.662065, val:  55.83%, val_best:  69.17%, tr:  93.56%, tr_best:  94.28%, epoch time: 38.20 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2866%\n",
      "layer   2  Sparsity: 74.4676%\n",
      "layer   3  Sparsity: 74.5364%\n",
      "total_backward_count 866415 real_backward_count 226128  26.099%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.399307/  1.730876, val:  51.67%, val_best:  69.17%, tr:  93.16%, tr_best:  94.28%, epoch time: 38.44 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2476%\n",
      "layer   2  Sparsity: 74.9069%\n",
      "layer   3  Sparsity: 74.9059%\n",
      "total_backward_count 871310 real_backward_count 227361  26.094%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.398510/  1.792719, val:  36.25%, val_best:  69.17%, tr:  92.85%, tr_best:  94.28%, epoch time: 38.04 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2637%\n",
      "layer   2  Sparsity: 74.8770%\n",
      "layer   3  Sparsity: 75.0245%\n",
      "total_backward_count 876205 real_backward_count 228573  26.087%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.392539/  1.627815, val:  64.58%, val_best:  69.17%, tr:  92.24%, tr_best:  94.28%, epoch time: 38.20 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2883%\n",
      "layer   2  Sparsity: 74.8212%\n",
      "layer   3  Sparsity: 74.8929%\n",
      "total_backward_count 881100 real_backward_count 229756  26.076%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.373120/  1.595075, val:  60.42%, val_best:  69.17%, tr:  92.13%, tr_best:  94.28%, epoch time: 38.84 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2621%\n",
      "layer   2  Sparsity: 74.9675%\n",
      "layer   3  Sparsity: 74.3021%\n",
      "total_backward_count 885995 real_backward_count 230931  26.065%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.388541/  1.646333, val:  60.00%, val_best:  69.17%, tr:  92.65%, tr_best:  94.28%, epoch time: 38.12 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2732%\n",
      "layer   2  Sparsity: 75.0639%\n",
      "layer   3  Sparsity: 74.9305%\n",
      "total_backward_count 890890 real_backward_count 232132  26.056%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.390940/  1.676622, val:  51.25%, val_best:  69.17%, tr:  93.16%, tr_best:  94.28%, epoch time: 38.36 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2527%\n",
      "layer   2  Sparsity: 74.8034%\n",
      "layer   3  Sparsity: 74.9699%\n",
      "total_backward_count 895785 real_backward_count 233316  26.046%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.384907/  1.787986, val:  32.08%, val_best:  69.17%, tr:  93.46%, tr_best:  94.28%, epoch time: 38.20 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2705%\n",
      "layer   2  Sparsity: 74.3806%\n",
      "layer   3  Sparsity: 75.3556%\n",
      "total_backward_count 900680 real_backward_count 234489  26.035%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.383613/  1.668476, val:  50.00%, val_best:  69.17%, tr:  92.44%, tr_best:  94.28%, epoch time: 38.67 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2625%\n",
      "layer   2  Sparsity: 74.4322%\n",
      "layer   3  Sparsity: 75.0975%\n",
      "total_backward_count 905575 real_backward_count 235689  26.026%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.401609/  1.654331, val:  59.58%, val_best:  69.17%, tr:  91.73%, tr_best:  94.28%, epoch time: 38.09 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2878%\n",
      "layer   2  Sparsity: 74.5000%\n",
      "layer   3  Sparsity: 74.6741%\n",
      "total_backward_count 910470 real_backward_count 236889  26.018%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.392396/  1.693613, val:  37.92%, val_best:  69.17%, tr:  92.75%, tr_best:  94.28%, epoch time: 38.34 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2576%\n",
      "layer   2  Sparsity: 74.7454%\n",
      "layer   3  Sparsity: 74.8093%\n",
      "total_backward_count 915365 real_backward_count 238068  26.008%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.380163/  1.638377, val:  47.08%, val_best:  69.17%, tr:  92.85%, tr_best:  94.28%, epoch time: 38.26 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2645%\n",
      "layer   2  Sparsity: 74.7943%\n",
      "layer   3  Sparsity: 74.3837%\n",
      "total_backward_count 920260 real_backward_count 239286  26.002%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.373064/  1.659211, val:  60.42%, val_best:  69.17%, tr:  91.52%, tr_best:  94.28%, epoch time: 39.09 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2506%\n",
      "layer   2  Sparsity: 74.7783%\n",
      "layer   3  Sparsity: 74.6541%\n",
      "total_backward_count 925155 real_backward_count 240530  25.999%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.377925/  1.661288, val:  51.67%, val_best:  69.17%, tr:  92.95%, tr_best:  94.28%, epoch time: 38.17 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2522%\n",
      "layer   2  Sparsity: 74.7498%\n",
      "layer   3  Sparsity: 75.3209%\n",
      "total_backward_count 930050 real_backward_count 241716  25.990%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.397526/  1.639362, val:  59.58%, val_best:  69.17%, tr:  91.62%, tr_best:  94.28%, epoch time: 38.50 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2772%\n",
      "layer   2  Sparsity: 74.6655%\n",
      "layer   3  Sparsity: 74.8181%\n",
      "total_backward_count 934945 real_backward_count 242878  25.978%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.393342/  1.686767, val:  44.58%, val_best:  69.17%, tr:  92.44%, tr_best:  94.28%, epoch time: 38.24 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2941%\n",
      "layer   2  Sparsity: 74.5104%\n",
      "layer   3  Sparsity: 74.0751%\n",
      "total_backward_count 939840 real_backward_count 244067  25.969%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.381462/  1.651505, val:  46.67%, val_best:  69.17%, tr:  92.85%, tr_best:  94.28%, epoch time: 37.97 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2561%\n",
      "layer   2  Sparsity: 74.5270%\n",
      "layer   3  Sparsity: 73.9325%\n",
      "total_backward_count 944735 real_backward_count 245250  25.960%\n",
      "fc layer 1 self.abs_max_out: 9897.0\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.356875/  1.702317, val:  35.83%, val_best:  69.17%, tr:  94.48%, tr_best:  94.48%, epoch time: 38.31 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2497%\n",
      "layer   2  Sparsity: 74.7251%\n",
      "layer   3  Sparsity: 74.6244%\n",
      "total_backward_count 949630 real_backward_count 246392  25.946%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.376338/  1.603695, val:  60.42%, val_best:  69.17%, tr:  91.62%, tr_best:  94.48%, epoch time: 38.46 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2647%\n",
      "layer   2  Sparsity: 74.7957%\n",
      "layer   3  Sparsity: 74.2198%\n",
      "total_backward_count 954525 real_backward_count 247589  25.938%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.369871/  1.637114, val:  55.83%, val_best:  69.17%, tr:  92.34%, tr_best:  94.48%, epoch time: 38.36 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2850%\n",
      "layer   2  Sparsity: 74.8533%\n",
      "layer   3  Sparsity: 73.7272%\n",
      "total_backward_count 959420 real_backward_count 248819  25.934%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.354819/  1.625910, val:  55.83%, val_best:  69.17%, tr:  93.46%, tr_best:  94.48%, epoch time: 38.92 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 97.2836%\n",
      "layer   2  Sparsity: 74.7426%\n",
      "layer   3  Sparsity: 73.2457%\n",
      "total_backward_count 964315 real_backward_count 249976  25.923%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.378741/  1.575903, val:  59.17%, val_best:  69.17%, tr:  92.85%, tr_best:  94.48%, epoch time: 38.37 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2673%\n",
      "layer   2  Sparsity: 74.6103%\n",
      "layer   3  Sparsity: 74.2124%\n",
      "total_backward_count 969210 real_backward_count 251129  25.911%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.360732/  1.613033, val:  60.42%, val_best:  69.17%, tr:  91.42%, tr_best:  94.48%, epoch time: 38.10 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 97.2449%\n",
      "layer   2  Sparsity: 74.8958%\n",
      "layer   3  Sparsity: 73.4856%\n",
      "total_backward_count 974105 real_backward_count 252335  25.904%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.350223/  1.591502, val:  62.92%, val_best:  69.17%, tr:  93.67%, tr_best:  94.48%, epoch time: 38.08 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 97.2762%\n",
      "layer   2  Sparsity: 74.6100%\n",
      "layer   3  Sparsity: 74.2084%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2ca92de0b34627b8e7459a3b7fecda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÇ‚ñÑ‚ñà‚ñá‚ñà‚ñÖ‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñá‚ñÖ‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÅ‚ñÖ‚ñÇ‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÇ‚ñÑ‚ñà‚ñá‚ñà‚ñÖ‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñá‚ñÖ‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÅ‚ñÖ‚ñÇ‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.93667</td></tr><tr><td>tr_epoch_loss</td><td>1.35022</td></tr><tr><td>val_acc_best</td><td>0.69167</td></tr><tr><td>val_acc_now</td><td>0.62917</td></tr><tr><td>val_loss</td><td>1.5915</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">northern-sweep-278</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ugzn66v' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ugzn66v</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251126_211809-5ugzn66v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f52qboo0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 12000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251126_232220-f52qboo0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f52qboo0' target=\"_blank\">lyric-sweep-286</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f52qboo0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f52qboo0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251126_232228_656', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 12000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c247f45ff938aa370993ba27bace6d15\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 592\n",
      "fc layer 1 self.abs_max_out: 340.0\n",
      "lif layer 1 self.abs_max_v: 340.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1293.0\n",
      "lif layer 2 self.abs_max_v: 1293.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 898.0\n",
      "fc layer 1 self.abs_max_out: 488.0\n",
      "lif layer 1 self.abs_max_v: 543.0\n",
      "fc layer 2 self.abs_max_out: 1298.0\n",
      "lif layer 2 self.abs_max_v: 1603.5\n",
      "fc layer 2 self.abs_max_out: 1421.0\n",
      "lif layer 2 self.abs_max_v: 1972.0\n",
      "lif layer 1 self.abs_max_v: 561.0\n",
      "fc layer 2 self.abs_max_out: 1581.0\n",
      "lif layer 2 self.abs_max_v: 2223.5\n",
      "fc layer 2 self.abs_max_out: 1660.0\n",
      "lif layer 1 self.abs_max_v: 626.5\n",
      "smallest_now_T updated: 534\n",
      "fc layer 2 self.abs_max_out: 1718.0\n",
      "fc layer 3 self.abs_max_out: 1021.0\n",
      "lif layer 1 self.abs_max_v: 644.0\n",
      "fc layer 1 self.abs_max_out: 530.0\n",
      "lif layer 1 self.abs_max_v: 852.0\n",
      "fc layer 2 self.abs_max_out: 1876.0\n",
      "lif layer 2 self.abs_max_v: 2593.5\n",
      "fc layer 1 self.abs_max_out: 612.0\n",
      "lif layer 1 self.abs_max_v: 945.0\n",
      "lif layer 2 self.abs_max_v: 3155.0\n",
      "fc layer 1 self.abs_max_out: 680.0\n",
      "lif layer 1 self.abs_max_v: 1115.0\n",
      "fc layer 2 self.abs_max_out: 1951.0\n",
      "fc layer 1 self.abs_max_out: 730.0\n",
      "fc layer 2 self.abs_max_out: 2099.0\n",
      "smallest_now_T updated: 407\n",
      "fc layer 1 self.abs_max_out: 747.0\n",
      "lif layer 1 self.abs_max_v: 1150.0\n",
      "fc layer 1 self.abs_max_out: 859.0\n",
      "fc layer 2 self.abs_max_out: 2103.0\n",
      "fc layer 1 self.abs_max_out: 946.0\n",
      "lif layer 1 self.abs_max_v: 1491.5\n",
      "fc layer 1 self.abs_max_out: 963.0\n",
      "fc layer 1 self.abs_max_out: 1003.0\n",
      "lif layer 2 self.abs_max_v: 3298.5\n",
      "fc layer 2 self.abs_max_out: 2144.0\n",
      "fc layer 3 self.abs_max_out: 1066.0\n",
      "fc layer 2 self.abs_max_out: 2288.0\n",
      "lif layer 2 self.abs_max_v: 3351.5\n",
      "fc layer 2 self.abs_max_out: 2343.0\n",
      "lif layer 2 self.abs_max_v: 3471.5\n",
      "smallest_now_T updated: 345\n",
      "lif layer 2 self.abs_max_v: 3859.0\n",
      "fc layer 1 self.abs_max_out: 1015.0\n",
      "fc layer 1 self.abs_max_out: 1102.0\n",
      "fc layer 1 self.abs_max_out: 1124.0\n",
      "fc layer 1 self.abs_max_out: 1230.0\n",
      "lif layer 1 self.abs_max_v: 1864.5\n",
      "fc layer 2 self.abs_max_out: 2488.0\n",
      "fc layer 2 self.abs_max_out: 2671.0\n",
      "smallest_now_T updated: 317\n",
      "fc layer 2 self.abs_max_out: 2975.0\n",
      "lif layer 2 self.abs_max_v: 3873.0\n",
      "fc layer 1 self.abs_max_out: 1233.0\n",
      "fc layer 3 self.abs_max_out: 1123.0\n",
      "lif layer 1 self.abs_max_v: 1883.0\n",
      "smallest_now_T updated: 286\n",
      "lif layer 2 self.abs_max_v: 4068.0\n",
      "fc layer 1 self.abs_max_out: 1257.0\n",
      "fc layer 1 self.abs_max_out: 1477.0\n",
      "fc layer 1 self.abs_max_out: 1607.0\n",
      "fc layer 1 self.abs_max_out: 1617.0\n",
      "fc layer 1 self.abs_max_out: 1755.0\n",
      "fc layer 1 self.abs_max_out: 1783.0\n",
      "fc layer 1 self.abs_max_out: 1934.0\n",
      "lif layer 1 self.abs_max_v: 1934.0\n",
      "fc layer 1 self.abs_max_out: 2283.0\n",
      "lif layer 1 self.abs_max_v: 2283.0\n",
      "lif layer 2 self.abs_max_v: 4146.0\n",
      "smallest_now_T updated: 247\n",
      "smallest_now_T updated: 192\n",
      "fc layer 3 self.abs_max_out: 1127.0\n",
      "lif layer 2 self.abs_max_v: 4156.0\n",
      "lif layer 2 self.abs_max_v: 4236.0\n",
      "lif layer 2 self.abs_max_v: 4432.5\n",
      "lif layer 2 self.abs_max_v: 4643.0\n",
      "fc layer 3 self.abs_max_out: 1140.0\n",
      "lif layer 1 self.abs_max_v: 2309.0\n",
      "lif layer 2 self.abs_max_v: 4799.0\n",
      "lif layer 2 self.abs_max_v: 4904.5\n",
      "fc layer 1 self.abs_max_out: 2284.0\n",
      "fc layer 2 self.abs_max_out: 3005.0\n",
      "lif layer 2 self.abs_max_v: 5002.0\n",
      "lif layer 2 self.abs_max_v: 5051.0\n",
      "fc layer 2 self.abs_max_out: 3036.0\n",
      "fc layer 2 self.abs_max_out: 3046.0\n",
      "fc layer 2 self.abs_max_out: 3090.0\n",
      "lif layer 1 self.abs_max_v: 2332.0\n",
      "fc layer 2 self.abs_max_out: 3237.0\n",
      "lif layer 1 self.abs_max_v: 2927.5\n",
      "lif layer 2 self.abs_max_v: 5177.0\n",
      "smallest_now_T_val updated: 552\n",
      "smallest_now_T_val updated: 456\n",
      "smallest_now_T_val updated: 448\n",
      "smallest_now_T_val updated: 440\n",
      "smallest_now_T_val updated: 368\n",
      "smallest_now_T_val updated: 137\n",
      "fc layer 1 self.abs_max_out: 2520.0\n",
      "lif layer 1 self.abs_max_v: 2943.5\n",
      "lif layer 1 self.abs_max_v: 3517.5\n",
      "lif layer 1 self.abs_max_v: 4093.0\n",
      "fc layer 1 self.abs_max_out: 2687.0\n",
      "fc layer 3 self.abs_max_out: 1187.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.006094/  2.111056, val:  34.58%, val_best:  34.58%, tr:  75.38%, tr_best:  75.38%, epoch time: 72.89 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2922%\n",
      "layer   2  Sparsity: 75.0620%\n",
      "layer   3  Sparsity: 66.3995%\n",
      "total_backward_count 9790 real_backward_count 4450  45.455%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 3315.0\n",
      "fc layer 2 self.abs_max_out: 3356.0\n",
      "fc layer 1 self.abs_max_out: 2688.0\n",
      "fc layer 1 self.abs_max_out: 2825.0\n",
      "fc layer 2 self.abs_max_out: 3415.0\n",
      "lif layer 2 self.abs_max_v: 5352.5\n",
      "fc layer 2 self.abs_max_out: 3437.0\n",
      "fc layer 2 self.abs_max_out: 3472.0\n",
      "fc layer 1 self.abs_max_out: 2900.0\n",
      "lif layer 1 self.abs_max_v: 4257.5\n",
      "lif layer 1 self.abs_max_v: 4275.5\n",
      "fc layer 1 self.abs_max_out: 3062.0\n",
      "lif layer 2 self.abs_max_v: 5643.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.984881/  2.103863, val:  40.83%, val_best:  40.83%, tr:  90.30%, tr_best:  90.30%, epoch time: 74.35 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 97.2864%\n",
      "layer   2  Sparsity: 73.5098%\n",
      "layer   3  Sparsity: 65.3467%\n",
      "total_backward_count 19580 real_backward_count 7552  38.570%\n",
      "fc layer 1 self.abs_max_out: 3065.0\n",
      "fc layer 2 self.abs_max_out: 3479.0\n",
      "lif layer 2 self.abs_max_v: 5743.0\n",
      "fc layer 2 self.abs_max_out: 3503.0\n",
      "fc layer 2 self.abs_max_out: 3704.0\n",
      "fc layer 1 self.abs_max_out: 3232.0\n",
      "lif layer 1 self.abs_max_v: 4335.5\n",
      "lif layer 1 self.abs_max_v: 4355.0\n",
      "fc layer 1 self.abs_max_out: 3436.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.997236/  2.121855, val:  34.58%, val_best:  40.83%, tr:  93.56%, tr_best:  93.56%, epoch time: 72.71 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2847%\n",
      "layer   2  Sparsity: 72.7324%\n",
      "layer   3  Sparsity: 63.9005%\n",
      "total_backward_count 29370 real_backward_count 10276  34.988%\n",
      "fc layer 1 self.abs_max_out: 3567.0\n",
      "lif layer 1 self.abs_max_v: 4376.5\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  2.008084/  2.127962, val:  38.33%, val_best:  40.83%, tr:  96.63%, tr_best:  96.63%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2839%\n",
      "layer   2  Sparsity: 72.0412%\n",
      "layer   3  Sparsity: 61.8214%\n",
      "total_backward_count 39160 real_backward_count 12613  32.209%\n",
      "fc layer 2 self.abs_max_out: 3706.0\n",
      "fc layer 2 self.abs_max_out: 3795.0\n",
      "fc layer 2 self.abs_max_out: 3871.0\n",
      "lif layer 1 self.abs_max_v: 4419.5\n",
      "lif layer 1 self.abs_max_v: 4828.0\n",
      "fc layer 1 self.abs_max_out: 3625.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  2.014438/  2.124561, val:  36.25%, val_best:  40.83%, tr:  96.73%, tr_best:  96.73%, epoch time: 72.78 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2805%\n",
      "layer   2  Sparsity: 72.0219%\n",
      "layer   3  Sparsity: 61.0286%\n",
      "total_backward_count 48950 real_backward_count 14830  30.296%\n",
      "lif layer 2 self.abs_max_v: 5927.5\n",
      "lif layer 1 self.abs_max_v: 4960.0\n",
      "fc layer 1 self.abs_max_out: 3729.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  2.013003/  2.141918, val:  37.50%, val_best:  40.83%, tr:  97.14%, tr_best:  97.14%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.3120%\n",
      "layer   2  Sparsity: 71.8489%\n",
      "layer   3  Sparsity: 60.3139%\n",
      "total_backward_count 58740 real_backward_count 16957  28.868%\n",
      "lif layer 2 self.abs_max_v: 5983.0\n",
      "fc layer 2 self.abs_max_out: 3888.0\n",
      "fc layer 1 self.abs_max_out: 3826.0\n",
      "fc layer 2 self.abs_max_out: 3966.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  2.025309/  2.143424, val:  45.42%, val_best:  45.42%, tr:  97.65%, tr_best:  97.65%, epoch time: 72.35 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2798%\n",
      "layer   2  Sparsity: 71.7669%\n",
      "layer   3  Sparsity: 59.9983%\n",
      "total_backward_count 68530 real_backward_count 18958  27.664%\n",
      "lif layer 2 self.abs_max_v: 5984.5\n",
      "lif layer 2 self.abs_max_v: 6202.5\n",
      "lif layer 1 self.abs_max_v: 5094.0\n",
      "fc layer 1 self.abs_max_out: 3917.0\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  2.032171/  2.124588, val:  41.25%, val_best:  45.42%, tr:  98.06%, tr_best:  98.06%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2779%\n",
      "layer   2  Sparsity: 72.0386%\n",
      "layer   3  Sparsity: 60.4339%\n",
      "total_backward_count 78320 real_backward_count 20969  26.773%\n",
      "fc layer 2 self.abs_max_out: 4147.0\n",
      "lif layer 1 self.abs_max_v: 5867.5\n",
      "fc layer 1 self.abs_max_out: 4063.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  2.034192/  2.131117, val:  41.25%, val_best:  45.42%, tr:  98.06%, tr_best:  98.06%, epoch time: 72.60 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2834%\n",
      "layer   2  Sparsity: 71.7394%\n",
      "layer   3  Sparsity: 60.6288%\n",
      "total_backward_count 88110 real_backward_count 22985  26.087%\n",
      "fc layer 1 self.abs_max_out: 4208.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  2.038992/  2.141406, val:  34.58%, val_best:  45.42%, tr:  97.96%, tr_best:  98.06%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2656%\n",
      "layer   2  Sparsity: 71.7332%\n",
      "layer   3  Sparsity: 60.4760%\n",
      "total_backward_count 97900 real_backward_count 24882  25.416%\n",
      "fc layer 1 self.abs_max_out: 4320.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  2.032412/  2.143642, val:  41.67%, val_best:  45.42%, tr:  98.47%, tr_best:  98.47%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 97.2800%\n",
      "layer   2  Sparsity: 71.7699%\n",
      "layer   3  Sparsity: 60.5808%\n",
      "total_backward_count 107690 real_backward_count 26791  24.878%\n",
      "fc layer 1 self.abs_max_out: 4351.0\n",
      "lif layer 1 self.abs_max_v: 6164.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  2.049443/  2.135482, val:  47.50%, val_best:  47.50%, tr:  98.67%, tr_best:  98.67%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2886%\n",
      "layer   2  Sparsity: 71.8984%\n",
      "layer   3  Sparsity: 61.0321%\n",
      "total_backward_count 117480 real_backward_count 28696  24.426%\n",
      "fc layer 2 self.abs_max_out: 4213.0\n",
      "lif layer 2 self.abs_max_v: 6211.0\n",
      "lif layer 1 self.abs_max_v: 6238.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  2.053168/  2.143357, val:  35.83%, val_best:  47.50%, tr:  98.16%, tr_best:  98.67%, epoch time: 72.66 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2843%\n",
      "layer   2  Sparsity: 71.8430%\n",
      "layer   3  Sparsity: 61.7220%\n",
      "total_backward_count 127270 real_backward_count 30495  23.961%\n",
      "fc layer 1 self.abs_max_out: 4385.0\n",
      "lif layer 2 self.abs_max_v: 6345.5\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  2.049039/  2.148155, val:  27.08%, val_best:  47.50%, tr:  98.26%, tr_best:  98.67%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2719%\n",
      "layer   2  Sparsity: 71.6639%\n",
      "layer   3  Sparsity: 61.6527%\n",
      "total_backward_count 137060 real_backward_count 32374  23.620%\n",
      "lif layer 2 self.abs_max_v: 6432.5\n",
      "fc layer 1 self.abs_max_out: 4535.0\n",
      "lif layer 2 self.abs_max_v: 6449.5\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  2.048196/  2.146332, val:  46.25%, val_best:  47.50%, tr:  98.77%, tr_best:  98.77%, epoch time: 73.30 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.3003%\n",
      "layer   2  Sparsity: 71.4102%\n",
      "layer   3  Sparsity: 61.2376%\n",
      "total_backward_count 146850 real_backward_count 34170  23.269%\n",
      "fc layer 2 self.abs_max_out: 4230.0\n",
      "lif layer 2 self.abs_max_v: 6614.5\n",
      "lif layer 2 self.abs_max_v: 6723.0\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  2.058692/  2.155371, val:  35.83%, val_best:  47.50%, tr:  98.77%, tr_best:  98.77%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2860%\n",
      "layer   2  Sparsity: 71.3979%\n",
      "layer   3  Sparsity: 62.3717%\n",
      "total_backward_count 156640 real_backward_count 36036  23.006%\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  2.045559/  2.145317, val:  45.00%, val_best:  47.50%, tr:  98.16%, tr_best:  98.77%, epoch time: 72.23 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2693%\n",
      "layer   2  Sparsity: 71.5802%\n",
      "layer   3  Sparsity: 62.5130%\n",
      "total_backward_count 166430 real_backward_count 37917  22.783%\n",
      "fc layer 2 self.abs_max_out: 4271.0\n",
      "lif layer 1 self.abs_max_v: 6240.0\n",
      "lif layer 2 self.abs_max_v: 6819.5\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  2.062254/  2.152880, val:  39.17%, val_best:  47.50%, tr:  98.57%, tr_best:  98.77%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2764%\n",
      "layer   2  Sparsity: 71.3642%\n",
      "layer   3  Sparsity: 63.2670%\n",
      "total_backward_count 176220 real_backward_count 39741  22.552%\n",
      "lif layer 1 self.abs_max_v: 6451.0\n",
      "fc layer 1 self.abs_max_out: 4556.0\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  2.062425/  2.139622, val:  45.83%, val_best:  47.50%, tr:  98.88%, tr_best:  98.88%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2994%\n",
      "layer   2  Sparsity: 71.4609%\n",
      "layer   3  Sparsity: 63.0836%\n",
      "total_backward_count 186010 real_backward_count 41560  22.343%\n",
      "fc layer 1 self.abs_max_out: 4571.0\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  2.055751/  2.148798, val:  32.92%, val_best:  47.50%, tr:  98.88%, tr_best:  98.88%, epoch time: 73.81 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 97.2643%\n",
      "layer   2  Sparsity: 71.6316%\n",
      "layer   3  Sparsity: 62.8956%\n",
      "total_backward_count 195800 real_backward_count 43300  22.114%\n",
      "fc layer 1 self.abs_max_out: 4956.0\n",
      "fc layer 2 self.abs_max_out: 4279.0\n",
      "lif layer 1 self.abs_max_v: 6597.5\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  2.062424/  2.149586, val:  37.08%, val_best:  47.50%, tr:  99.18%, tr_best:  99.18%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2812%\n",
      "layer   2  Sparsity: 71.4297%\n",
      "layer   3  Sparsity: 63.5158%\n",
      "total_backward_count 205590 real_backward_count 45099  21.936%\n",
      "fc layer 2 self.abs_max_out: 4291.0\n",
      "fc layer 2 self.abs_max_out: 4300.0\n",
      "lif layer 1 self.abs_max_v: 6602.0\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  2.066681/  2.146899, val:  40.00%, val_best:  47.50%, tr:  98.88%, tr_best:  99.18%, epoch time: 72.10 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2766%\n",
      "layer   2  Sparsity: 71.1687%\n",
      "layer   3  Sparsity: 63.0575%\n",
      "total_backward_count 215380 real_backward_count 46884  21.768%\n",
      "lif layer 2 self.abs_max_v: 6956.0\n",
      "fc layer 2 self.abs_max_out: 4336.0\n",
      "lif layer 1 self.abs_max_v: 6653.0\n",
      "fc layer 2 self.abs_max_out: 4357.0\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  2.061891/  2.141106, val:  56.67%, val_best:  56.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2703%\n",
      "layer   2  Sparsity: 71.1952%\n",
      "layer   3  Sparsity: 62.9146%\n",
      "total_backward_count 225170 real_backward_count 48670  21.615%\n",
      "lif layer 1 self.abs_max_v: 6849.5\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  2.060294/  2.135216, val:  48.75%, val_best:  56.67%, tr:  98.67%, tr_best:  99.18%, epoch time: 73.14 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2961%\n",
      "layer   2  Sparsity: 71.4145%\n",
      "layer   3  Sparsity: 63.3600%\n",
      "total_backward_count 234960 real_backward_count 50444  21.469%\n",
      "fc layer 2 self.abs_max_out: 4426.0\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  2.061502/  2.154431, val:  46.67%, val_best:  56.67%, tr:  98.57%, tr_best:  99.18%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2967%\n",
      "layer   2  Sparsity: 71.3503%\n",
      "layer   3  Sparsity: 63.2434%\n",
      "total_backward_count 244750 real_backward_count 52257  21.351%\n",
      "fc layer 2 self.abs_max_out: 4449.0\n",
      "lif layer 1 self.abs_max_v: 7252.5\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  2.073133/  2.142739, val:  55.83%, val_best:  56.67%, tr:  98.98%, tr_best:  99.18%, epoch time: 72.53 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2786%\n",
      "layer   2  Sparsity: 71.1162%\n",
      "layer   3  Sparsity: 63.6496%\n",
      "total_backward_count 254540 real_backward_count 54113  21.259%\n",
      "fc layer 2 self.abs_max_out: 4454.0\n",
      "lif layer 2 self.abs_max_v: 6962.5\n",
      "fc layer 2 self.abs_max_out: 4528.0\n",
      "lif layer 2 self.abs_max_v: 7120.0\n",
      "lif layer 1 self.abs_max_v: 7281.5\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  2.059668/  2.138574, val:  52.08%, val_best:  56.67%, tr:  98.98%, tr_best:  99.18%, epoch time: 74.02 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 97.2821%\n",
      "layer   2  Sparsity: 71.2839%\n",
      "layer   3  Sparsity: 64.4363%\n",
      "total_backward_count 264330 real_backward_count 55899  21.147%\n",
      "lif layer 2 self.abs_max_v: 7306.0\n",
      "lif layer 1 self.abs_max_v: 7379.5\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  2.063829/  2.135580, val:  53.33%, val_best:  56.67%, tr:  98.77%, tr_best:  99.18%, epoch time: 72.90 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2835%\n",
      "layer   2  Sparsity: 71.2372%\n",
      "layer   3  Sparsity: 64.1738%\n",
      "total_backward_count 274120 real_backward_count 57719  21.056%\n",
      "fc layer 1 self.abs_max_out: 5370.0\n",
      "lif layer 1 self.abs_max_v: 7632.5\n",
      "fc layer 2 self.abs_max_out: 4564.0\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  2.062808/  2.151008, val:  52.08%, val_best:  56.67%, tr:  98.98%, tr_best:  99.18%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2801%\n",
      "layer   2  Sparsity: 71.0361%\n",
      "layer   3  Sparsity: 64.3174%\n",
      "total_backward_count 283910 real_backward_count 59460  20.943%\n",
      "lif layer 2 self.abs_max_v: 7498.0\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  2.075758/  2.147241, val:  37.08%, val_best:  56.67%, tr:  99.18%, tr_best:  99.18%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2777%\n",
      "layer   2  Sparsity: 70.9884%\n",
      "layer   3  Sparsity: 64.3504%\n",
      "total_backward_count 293700 real_backward_count 61171  20.828%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  2.067715/  2.129809, val:  63.75%, val_best:  63.75%, tr:  99.49%, tr_best:  99.49%, epoch time: 72.52 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2753%\n",
      "layer   2  Sparsity: 70.9162%\n",
      "layer   3  Sparsity: 64.2358%\n",
      "total_backward_count 303490 real_backward_count 62871  20.716%\n",
      "fc layer 2 self.abs_max_out: 4610.0\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  2.066973/  2.138658, val:  35.42%, val_best:  63.75%, tr:  99.28%, tr_best:  99.49%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2641%\n",
      "layer   2  Sparsity: 70.8240%\n",
      "layer   3  Sparsity: 64.9898%\n",
      "total_backward_count 313280 real_backward_count 64553  20.606%\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  2.068108/  2.151200, val:  41.25%, val_best:  63.75%, tr:  98.98%, tr_best:  99.49%, epoch time: 72.60 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2793%\n",
      "layer   2  Sparsity: 71.3763%\n",
      "layer   3  Sparsity: 65.4007%\n",
      "total_backward_count 323070 real_backward_count 66315  20.527%\n",
      "lif layer 1 self.abs_max_v: 7796.0\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  2.071903/  2.137952, val:  50.83%, val_best:  63.75%, tr:  98.98%, tr_best:  99.49%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2709%\n",
      "layer   2  Sparsity: 71.1632%\n",
      "layer   3  Sparsity: 64.8686%\n",
      "total_backward_count 332860 real_backward_count 68016  20.434%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  2.064106/  2.130798, val:  52.50%, val_best:  63.75%, tr:  98.77%, tr_best:  99.49%, epoch time: 72.28 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2774%\n",
      "layer   2  Sparsity: 71.1047%\n",
      "layer   3  Sparsity: 64.3281%\n",
      "total_backward_count 342650 real_backward_count 69725  20.349%\n",
      "fc layer 1 self.abs_max_out: 5456.0\n",
      "fc layer 2 self.abs_max_out: 4715.0\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  2.059452/  2.129639, val:  58.33%, val_best:  63.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 72.16 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2827%\n",
      "layer   2  Sparsity: 71.2883%\n",
      "layer   3  Sparsity: 65.3762%\n",
      "total_backward_count 352440 real_backward_count 71422  20.265%\n",
      "fc layer 2 self.abs_max_out: 4769.0\n",
      "fc layer 2 self.abs_max_out: 4780.0\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  2.058256/  2.132697, val:  66.25%, val_best:  66.25%, tr:  99.18%, tr_best:  99.59%, epoch time: 72.51 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2930%\n",
      "layer   2  Sparsity: 71.2536%\n",
      "layer   3  Sparsity: 66.0442%\n",
      "total_backward_count 362230 real_backward_count 73089  20.178%\n",
      "lif layer 1 self.abs_max_v: 7831.0\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  2.058414/  2.148203, val:  31.67%, val_best:  66.25%, tr:  99.18%, tr_best:  99.59%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2938%\n",
      "layer   2  Sparsity: 71.2749%\n",
      "layer   3  Sparsity: 66.2778%\n",
      "total_backward_count 372020 real_backward_count 74675  20.073%\n",
      "lif layer 1 self.abs_max_v: 8089.0\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  2.071490/  2.139369, val:  49.58%, val_best:  66.25%, tr:  99.08%, tr_best:  99.59%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2903%\n",
      "layer   2  Sparsity: 71.0573%\n",
      "layer   3  Sparsity: 65.7359%\n",
      "total_backward_count 381810 real_backward_count 76321  19.989%\n",
      "fc layer 2 self.abs_max_out: 4826.0\n",
      "lif layer 1 self.abs_max_v: 8133.0\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  2.065408/  2.124740, val:  47.92%, val_best:  66.25%, tr:  99.28%, tr_best:  99.59%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2856%\n",
      "layer   2  Sparsity: 71.0292%\n",
      "layer   3  Sparsity: 64.6058%\n",
      "total_backward_count 391600 real_backward_count 77959  19.908%\n",
      "fc layer 2 self.abs_max_out: 5104.0\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  2.059281/  2.144305, val:  51.25%, val_best:  66.25%, tr:  99.69%, tr_best:  99.69%, epoch time: 72.42 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2761%\n",
      "layer   2  Sparsity: 70.7401%\n",
      "layer   3  Sparsity: 65.6256%\n",
      "total_backward_count 401390 real_backward_count 79555  19.820%\n",
      "lif layer 2 self.abs_max_v: 7506.5\n",
      "fc layer 1 self.abs_max_out: 5542.0\n",
      "lif layer 2 self.abs_max_v: 7510.5\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  2.065560/  2.137940, val:  62.50%, val_best:  66.25%, tr:  99.08%, tr_best:  99.69%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2864%\n",
      "layer   2  Sparsity: 70.6876%\n",
      "layer   3  Sparsity: 65.6441%\n",
      "total_backward_count 411180 real_backward_count 81240  19.758%\n",
      "fc layer 2 self.abs_max_out: 5133.0\n",
      "lif layer 1 self.abs_max_v: 8155.0\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  2.063833/  2.139709, val:  55.83%, val_best:  66.25%, tr:  99.18%, tr_best:  99.69%, epoch time: 72.34 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2780%\n",
      "layer   2  Sparsity: 71.0853%\n",
      "layer   3  Sparsity: 65.2883%\n",
      "total_backward_count 420970 real_backward_count 82881  19.688%\n",
      "fc layer 2 self.abs_max_out: 5136.0\n",
      "lif layer 1 self.abs_max_v: 8220.5\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  2.058066/  2.133286, val:  63.75%, val_best:  66.25%, tr:  99.39%, tr_best:  99.69%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2711%\n",
      "layer   2  Sparsity: 70.8609%\n",
      "layer   3  Sparsity: 65.6663%\n",
      "total_backward_count 430760 real_backward_count 84569  19.633%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  2.063406/  2.135085, val:  45.83%, val_best:  66.25%, tr:  98.88%, tr_best:  99.69%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2828%\n",
      "layer   2  Sparsity: 70.8475%\n",
      "layer   3  Sparsity: 66.4659%\n",
      "total_backward_count 440550 real_backward_count 86252  19.578%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  2.060276/  2.130295, val:  51.25%, val_best:  66.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 72.78 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2789%\n",
      "layer   2  Sparsity: 70.9416%\n",
      "layer   3  Sparsity: 67.1114%\n",
      "total_backward_count 450340 real_backward_count 87908  19.520%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  2.066632/  2.119096, val:  60.00%, val_best:  66.25%, tr:  98.98%, tr_best:  99.69%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2839%\n",
      "layer   2  Sparsity: 71.0179%\n",
      "layer   3  Sparsity: 67.1743%\n",
      "total_backward_count 460130 real_backward_count 89589  19.470%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  2.054638/  2.124363, val:  43.33%, val_best:  66.25%, tr:  98.88%, tr_best:  99.69%, epoch time: 72.64 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2825%\n",
      "layer   2  Sparsity: 70.8586%\n",
      "layer   3  Sparsity: 66.5232%\n",
      "total_backward_count 469920 real_backward_count 91241  19.416%\n",
      "lif layer 1 self.abs_max_v: 8268.0\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  2.065425/  2.131984, val:  54.17%, val_best:  66.25%, tr:  98.57%, tr_best:  99.69%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.3058%\n",
      "layer   2  Sparsity: 71.0359%\n",
      "layer   3  Sparsity: 66.8879%\n",
      "total_backward_count 479710 real_backward_count 92867  19.359%\n",
      "fc layer 1 self.abs_max_out: 5695.0\n",
      "lif layer 1 self.abs_max_v: 8280.5\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  2.062869/  2.120823, val:  56.25%, val_best:  66.25%, tr:  98.98%, tr_best:  99.69%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2793%\n",
      "layer   2  Sparsity: 70.9836%\n",
      "layer   3  Sparsity: 67.1324%\n",
      "total_backward_count 489500 real_backward_count 94492  19.304%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  2.063728/  2.131376, val:  50.00%, val_best:  66.25%, tr:  99.18%, tr_best:  99.69%, epoch time: 71.90 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2964%\n",
      "layer   2  Sparsity: 71.0317%\n",
      "layer   3  Sparsity: 67.0636%\n",
      "total_backward_count 499290 real_backward_count 96141  19.256%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  2.061828/  2.127751, val:  65.83%, val_best:  66.25%, tr:  99.39%, tr_best:  99.69%, epoch time: 72.48 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2703%\n",
      "layer   2  Sparsity: 70.9080%\n",
      "layer   3  Sparsity: 67.2109%\n",
      "total_backward_count 509080 real_backward_count 97684  19.188%\n",
      "lif layer 2 self.abs_max_v: 7526.0\n",
      "fc layer 1 self.abs_max_out: 5744.0\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  2.064226/  2.138600, val:  50.00%, val_best:  66.25%, tr:  99.18%, tr_best:  99.69%, epoch time: 72.24 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2804%\n",
      "layer   2  Sparsity: 71.0560%\n",
      "layer   3  Sparsity: 67.0637%\n",
      "total_backward_count 518870 real_backward_count 99336  19.145%\n",
      "fc layer 2 self.abs_max_out: 5137.0\n",
      "fc layer 1 self.abs_max_out: 5872.0\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  2.066058/  2.127424, val:  52.08%, val_best:  66.25%, tr:  99.28%, tr_best:  99.69%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2957%\n",
      "layer   2  Sparsity: 71.0870%\n",
      "layer   3  Sparsity: 66.8341%\n",
      "total_backward_count 528660 real_backward_count 100868  19.080%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  2.063737/  2.137752, val:  55.42%, val_best:  66.25%, tr:  98.88%, tr_best:  99.69%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2665%\n",
      "layer   2  Sparsity: 70.6574%\n",
      "layer   3  Sparsity: 66.4115%\n",
      "total_backward_count 538450 real_backward_count 102479  19.032%\n",
      "fc layer 2 self.abs_max_out: 5164.0\n",
      "fc layer 2 self.abs_max_out: 5320.0\n",
      "lif layer 1 self.abs_max_v: 8437.0\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  2.062539/  2.132834, val:  58.33%, val_best:  66.25%, tr:  99.08%, tr_best:  99.69%, epoch time: 72.22 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2682%\n",
      "layer   2  Sparsity: 70.7639%\n",
      "layer   3  Sparsity: 66.6781%\n",
      "total_backward_count 548240 real_backward_count 104058  18.980%\n",
      "lif layer 1 self.abs_max_v: 8527.5\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  2.062494/  2.133819, val:  52.50%, val_best:  66.25%, tr:  99.18%, tr_best:  99.69%, epoch time: 71.75 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2815%\n",
      "layer   2  Sparsity: 70.8683%\n",
      "layer   3  Sparsity: 66.6867%\n",
      "total_backward_count 558030 real_backward_count 105708  18.943%\n",
      "lif layer 1 self.abs_max_v: 8709.0\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  2.054813/  2.112748, val:  72.50%, val_best:  72.50%, tr:  99.80%, tr_best:  99.80%, epoch time: 71.61 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 97.2979%\n",
      "layer   2  Sparsity: 70.8222%\n",
      "layer   3  Sparsity: 66.7329%\n",
      "total_backward_count 567820 real_backward_count 107252  18.888%\n",
      "fc layer 2 self.abs_max_out: 5537.0\n",
      "lif layer 1 self.abs_max_v: 9021.5\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  2.047070/  2.115482, val:  53.75%, val_best:  72.50%, tr:  99.49%, tr_best:  99.80%, epoch time: 72.49 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2840%\n",
      "layer   2  Sparsity: 70.9012%\n",
      "layer   3  Sparsity: 66.8269%\n",
      "total_backward_count 577610 real_backward_count 108785  18.834%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  2.048837/  2.120996, val:  50.83%, val_best:  72.50%, tr:  99.39%, tr_best:  99.80%, epoch time: 71.85 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2728%\n",
      "layer   2  Sparsity: 70.7553%\n",
      "layer   3  Sparsity: 66.7473%\n",
      "total_backward_count 587400 real_backward_count 110345  18.785%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  2.049073/  2.115216, val:  57.08%, val_best:  72.50%, tr:  99.18%, tr_best:  99.80%, epoch time: 71.92 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2865%\n",
      "layer   2  Sparsity: 70.6812%\n",
      "layer   3  Sparsity: 67.1378%\n",
      "total_backward_count 597190 real_backward_count 111952  18.746%\n",
      "fc layer 1 self.abs_max_out: 5882.0\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  2.051559/  2.116414, val:  51.67%, val_best:  72.50%, tr:  99.59%, tr_best:  99.80%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2807%\n",
      "layer   2  Sparsity: 70.6817%\n",
      "layer   3  Sparsity: 67.3338%\n",
      "total_backward_count 606980 real_backward_count 113528  18.704%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  2.049992/  2.118405, val:  61.25%, val_best:  72.50%, tr:  99.08%, tr_best:  99.80%, epoch time: 73.01 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2908%\n",
      "layer   2  Sparsity: 70.6742%\n",
      "layer   3  Sparsity: 66.8569%\n",
      "total_backward_count 616770 real_backward_count 115049  18.653%\n",
      "fc layer 2 self.abs_max_out: 5540.0\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  2.056257/  2.130790, val:  48.33%, val_best:  72.50%, tr:  98.98%, tr_best:  99.80%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2767%\n",
      "layer   2  Sparsity: 70.6150%\n",
      "layer   3  Sparsity: 66.6572%\n",
      "total_backward_count 626560 real_backward_count 116568  18.604%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  2.060034/  2.116617, val:  68.75%, val_best:  72.50%, tr:  99.08%, tr_best:  99.80%, epoch time: 72.83 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2865%\n",
      "layer   2  Sparsity: 70.7432%\n",
      "layer   3  Sparsity: 66.2703%\n",
      "total_backward_count 636350 real_backward_count 118172  18.570%\n",
      "lif layer 1 self.abs_max_v: 9117.0\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  2.050328/  2.126681, val:  62.92%, val_best:  72.50%, tr:  99.28%, tr_best:  99.80%, epoch time: 72.48 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2859%\n",
      "layer   2  Sparsity: 70.9681%\n",
      "layer   3  Sparsity: 66.2018%\n",
      "total_backward_count 646140 real_backward_count 119745  18.532%\n",
      "fc layer 1 self.abs_max_out: 6147.0\n",
      "lif layer 1 self.abs_max_v: 9616.0\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  2.049377/  2.117981, val:  60.00%, val_best:  72.50%, tr:  99.18%, tr_best:  99.80%, epoch time: 72.12 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2796%\n",
      "layer   2  Sparsity: 70.8555%\n",
      "layer   3  Sparsity: 66.0422%\n",
      "total_backward_count 655930 real_backward_count 121298  18.493%\n",
      "lif layer 2 self.abs_max_v: 7636.5\n",
      "fc layer 2 self.abs_max_out: 5669.0\n",
      "fc layer 2 self.abs_max_out: 5778.0\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  2.048350/  2.109529, val:  62.50%, val_best:  72.50%, tr:  99.59%, tr_best:  99.80%, epoch time: 72.20 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2778%\n",
      "layer   2  Sparsity: 70.8443%\n",
      "layer   3  Sparsity: 66.8828%\n",
      "total_backward_count 665720 real_backward_count 122763  18.441%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  2.049308/  2.107869, val:  72.08%, val_best:  72.50%, tr:  98.98%, tr_best:  99.80%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2787%\n",
      "layer   2  Sparsity: 70.8566%\n",
      "layer   3  Sparsity: 67.3113%\n",
      "total_backward_count 675510 real_backward_count 124352  18.409%\n",
      "fc layer 1 self.abs_max_out: 6204.0\n",
      "lif layer 1 self.abs_max_v: 9738.5\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  2.051650/  2.122610, val:  51.25%, val_best:  72.50%, tr:  99.59%, tr_best:  99.80%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2673%\n",
      "layer   2  Sparsity: 70.6555%\n",
      "layer   3  Sparsity: 66.9097%\n",
      "total_backward_count 685300 real_backward_count 125907  18.373%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  2.052409/  2.114963, val:  75.42%, val_best:  75.42%, tr:  98.98%, tr_best:  99.80%, epoch time: 72.40 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2938%\n",
      "layer   2  Sparsity: 70.5523%\n",
      "layer   3  Sparsity: 66.6922%\n",
      "total_backward_count 695090 real_backward_count 127427  18.332%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  2.048012/  2.123645, val:  50.83%, val_best:  75.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2710%\n",
      "layer   2  Sparsity: 70.7230%\n",
      "layer   3  Sparsity: 66.7531%\n",
      "total_backward_count 704880 real_backward_count 128877  18.284%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  2.055362/  2.112114, val:  70.42%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 72.22 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.3025%\n",
      "layer   2  Sparsity: 70.7897%\n",
      "layer   3  Sparsity: 66.6392%\n",
      "total_backward_count 714670 real_backward_count 130424  18.250%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  2.049948/  2.113347, val:  62.92%, val_best:  75.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.3059%\n",
      "layer   2  Sparsity: 70.6709%\n",
      "layer   3  Sparsity: 66.4368%\n",
      "total_backward_count 724460 real_backward_count 131913  18.208%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  2.049792/  2.126084, val:  58.75%, val_best:  75.42%, tr:  99.18%, tr_best:  99.80%, epoch time: 73.01 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2819%\n",
      "layer   2  Sparsity: 70.4324%\n",
      "layer   3  Sparsity: 67.0484%\n",
      "total_backward_count 734250 real_backward_count 133455  18.176%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  2.052813/  2.119985, val:  65.83%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 73.01 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2761%\n",
      "layer   2  Sparsity: 70.4363%\n",
      "layer   3  Sparsity: 66.4145%\n",
      "total_backward_count 744040 real_backward_count 134984  18.142%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  2.060750/  2.129531, val:  66.25%, val_best:  75.42%, tr:  98.98%, tr_best:  99.80%, epoch time: 72.27 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2648%\n",
      "layer   2  Sparsity: 70.5590%\n",
      "layer   3  Sparsity: 67.1107%\n",
      "total_backward_count 753830 real_backward_count 136578  18.118%\n",
      "lif layer 2 self.abs_max_v: 7646.5\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  2.048903/  2.118188, val:  56.25%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.19 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2900%\n",
      "layer   2  Sparsity: 70.8868%\n",
      "layer   3  Sparsity: 66.7263%\n",
      "total_backward_count 763620 real_backward_count 138108  18.086%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  2.050714/  2.114720, val:  61.67%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 72.33 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2666%\n",
      "layer   2  Sparsity: 70.8016%\n",
      "layer   3  Sparsity: 65.9872%\n",
      "total_backward_count 773410 real_backward_count 139607  18.051%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  2.046616/  2.116687, val:  68.75%, val_best:  75.42%, tr:  99.08%, tr_best:  99.80%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2795%\n",
      "layer   2  Sparsity: 70.8009%\n",
      "layer   3  Sparsity: 65.7179%\n",
      "total_backward_count 783200 real_backward_count 141097  18.015%\n",
      "fc layer 2 self.abs_max_out: 5895.0\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  2.055142/  2.123714, val:  58.75%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 73.59 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 97.2835%\n",
      "layer   2  Sparsity: 70.8725%\n",
      "layer   3  Sparsity: 66.2859%\n",
      "total_backward_count 792990 real_backward_count 142644  17.988%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  2.055464/  2.124200, val:  47.50%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2694%\n",
      "layer   2  Sparsity: 70.5767%\n",
      "layer   3  Sparsity: 66.5589%\n",
      "total_backward_count 802780 real_backward_count 144109  17.951%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  2.054884/  2.115354, val:  63.33%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 97.2659%\n",
      "layer   2  Sparsity: 70.7427%\n",
      "layer   3  Sparsity: 67.3635%\n",
      "total_backward_count 812570 real_backward_count 145619  17.921%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  2.054239/  2.129137, val:  60.83%, val_best:  75.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2673%\n",
      "layer   2  Sparsity: 70.6943%\n",
      "layer   3  Sparsity: 67.1539%\n",
      "total_backward_count 822360 real_backward_count 147092  17.887%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  2.051206/  2.113070, val:  60.00%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 72.56 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2795%\n",
      "layer   2  Sparsity: 70.5687%\n",
      "layer   3  Sparsity: 66.8615%\n",
      "total_backward_count 832150 real_backward_count 148584  17.855%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  2.047133/  2.119719, val:  59.58%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2762%\n",
      "layer   2  Sparsity: 70.4721%\n",
      "layer   3  Sparsity: 66.8569%\n",
      "total_backward_count 841940 real_backward_count 150069  17.824%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  2.056211/  2.126898, val:  63.75%, val_best:  75.42%, tr:  99.08%, tr_best:  99.80%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2997%\n",
      "layer   2  Sparsity: 70.5026%\n",
      "layer   3  Sparsity: 66.1299%\n",
      "total_backward_count 851730 real_backward_count 151546  17.793%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  2.050537/  2.118498, val:  58.75%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2813%\n",
      "layer   2  Sparsity: 70.5087%\n",
      "layer   3  Sparsity: 66.6178%\n",
      "total_backward_count 861520 real_backward_count 153026  17.762%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  2.045024/  2.111536, val:  62.08%, val_best:  75.42%, tr:  99.08%, tr_best:  99.80%, epoch time: 72.62 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2726%\n",
      "layer   2  Sparsity: 70.4269%\n",
      "layer   3  Sparsity: 66.0325%\n",
      "total_backward_count 871310 real_backward_count 154522  17.734%\n",
      "fc layer 2 self.abs_max_out: 5932.0\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  2.046587/  2.113918, val:  66.67%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2737%\n",
      "layer   2  Sparsity: 70.3974%\n",
      "layer   3  Sparsity: 66.1029%\n",
      "total_backward_count 881100 real_backward_count 156035  17.709%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  2.047041/  2.117438, val:  62.50%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.07 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2928%\n",
      "layer   2  Sparsity: 70.3829%\n",
      "layer   3  Sparsity: 66.9139%\n",
      "total_backward_count 890890 real_backward_count 157517  17.681%\n",
      "lif layer 2 self.abs_max_v: 7744.0\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  2.047880/  2.113234, val:  70.42%, val_best:  75.42%, tr:  98.77%, tr_best:  99.80%, epoch time: 72.31 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2730%\n",
      "layer   2  Sparsity: 70.3466%\n",
      "layer   3  Sparsity: 66.8335%\n",
      "total_backward_count 900680 real_backward_count 159048  17.659%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  2.051524/  2.123440, val:  64.17%, val_best:  75.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2921%\n",
      "layer   2  Sparsity: 70.5123%\n",
      "layer   3  Sparsity: 67.5249%\n",
      "total_backward_count 910470 real_backward_count 160561  17.635%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  2.056265/  2.122671, val:  62.08%, val_best:  75.42%, tr:  99.08%, tr_best:  99.80%, epoch time: 72.30 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2805%\n",
      "layer   2  Sparsity: 70.5061%\n",
      "layer   3  Sparsity: 67.0874%\n",
      "total_backward_count 920260 real_backward_count 162053  17.609%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  2.057518/  2.120057, val:  60.83%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 72.66 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2739%\n",
      "layer   2  Sparsity: 70.5873%\n",
      "layer   3  Sparsity: 67.3659%\n",
      "total_backward_count 930050 real_backward_count 163516  17.581%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  2.056662/  2.117210, val:  68.33%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 72.36 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2810%\n",
      "layer   2  Sparsity: 70.4870%\n",
      "layer   3  Sparsity: 67.0756%\n",
      "total_backward_count 939840 real_backward_count 164958  17.552%\n",
      "fc layer 2 self.abs_max_out: 5969.0\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  2.046301/  2.118185, val:  56.67%, val_best:  75.42%, tr:  99.18%, tr_best:  99.80%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2599%\n",
      "layer   2  Sparsity: 70.3890%\n",
      "layer   3  Sparsity: 66.9517%\n",
      "total_backward_count 949630 real_backward_count 166405  17.523%\n",
      "fc layer 2 self.abs_max_out: 6029.0\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  2.049800/  2.117100, val:  66.25%, val_best:  75.42%, tr:  98.88%, tr_best:  99.80%, epoch time: 72.49 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2932%\n",
      "layer   2  Sparsity: 70.5097%\n",
      "layer   3  Sparsity: 67.2313%\n",
      "total_backward_count 959420 real_backward_count 167872  17.497%\n",
      "fc layer 2 self.abs_max_out: 6083.0\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  2.052052/  2.120104, val:  56.25%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 72.46 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2880%\n",
      "layer   2  Sparsity: 70.3456%\n",
      "layer   3  Sparsity: 67.2536%\n",
      "total_backward_count 969210 real_backward_count 169359  17.474%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  2.048724/  2.112126, val:  74.58%, val_best:  75.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 97.2927%\n",
      "layer   2  Sparsity: 70.2530%\n",
      "layer   3  Sparsity: 66.8235%\n",
      "total_backward_count 979000 real_backward_count 170796  17.446%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  2.052371/  2.115614, val:  68.75%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2814%\n",
      "layer   2  Sparsity: 70.5098%\n",
      "layer   3  Sparsity: 67.5139%\n",
      "total_backward_count 988790 real_backward_count 172288  17.424%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  2.053756/  2.112486, val:  71.25%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2785%\n",
      "layer   2  Sparsity: 70.3189%\n",
      "layer   3  Sparsity: 67.9578%\n",
      "total_backward_count 998580 real_backward_count 173749  17.400%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  2.045677/  2.114699, val:  65.83%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 72.62 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2774%\n",
      "layer   2  Sparsity: 70.5482%\n",
      "layer   3  Sparsity: 67.8139%\n",
      "total_backward_count 1008370 real_backward_count 175228  17.377%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  2.050424/  2.116107, val:  58.75%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 73.01 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2701%\n",
      "layer   2  Sparsity: 70.4954%\n",
      "layer   3  Sparsity: 67.4425%\n",
      "total_backward_count 1018160 real_backward_count 176693  17.354%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  2.044630/  2.104191, val:  78.75%, val_best:  78.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2645%\n",
      "layer   2  Sparsity: 70.2125%\n",
      "layer   3  Sparsity: 66.6090%\n",
      "total_backward_count 1027950 real_backward_count 178144  17.330%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  2.042835/  2.109235, val:  65.83%, val_best:  78.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2906%\n",
      "layer   2  Sparsity: 70.4809%\n",
      "layer   3  Sparsity: 66.3444%\n",
      "total_backward_count 1037740 real_backward_count 179590  17.306%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  2.046984/  2.121415, val:  64.58%, val_best:  78.75%, tr:  98.98%, tr_best:  99.80%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2945%\n",
      "layer   2  Sparsity: 70.4625%\n",
      "layer   3  Sparsity: 67.1735%\n",
      "total_backward_count 1047530 real_backward_count 181066  17.285%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  2.050931/  2.111217, val:  65.42%, val_best:  78.75%, tr:  99.28%, tr_best:  99.80%, epoch time: 72.72 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2439%\n",
      "layer   2  Sparsity: 70.1813%\n",
      "layer   3  Sparsity: 67.5244%\n",
      "total_backward_count 1057320 real_backward_count 182485  17.259%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  2.051088/  2.109986, val:  67.08%, val_best:  78.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 72.62 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2793%\n",
      "layer   2  Sparsity: 70.3808%\n",
      "layer   3  Sparsity: 67.1791%\n",
      "total_backward_count 1067110 real_backward_count 183916  17.235%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  2.051983/  2.114718, val:  69.58%, val_best:  78.75%, tr:  99.18%, tr_best:  99.80%, epoch time: 72.82 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2813%\n",
      "layer   2  Sparsity: 70.3224%\n",
      "layer   3  Sparsity: 67.5097%\n",
      "total_backward_count 1076900 real_backward_count 185345  17.211%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  2.046953/  2.109441, val:  71.67%, val_best:  78.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 72.20 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2822%\n",
      "layer   2  Sparsity: 70.3204%\n",
      "layer   3  Sparsity: 66.9733%\n",
      "total_backward_count 1086690 real_backward_count 186771  17.187%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  2.048521/  2.116792, val:  67.92%, val_best:  78.75%, tr:  99.18%, tr_best:  99.80%, epoch time: 72.46 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2798%\n",
      "layer   2  Sparsity: 70.4339%\n",
      "layer   3  Sparsity: 67.4499%\n",
      "total_backward_count 1096480 real_backward_count 188201  17.164%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  2.048456/  2.116110, val:  81.25%, val_best:  81.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 72.35 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2927%\n",
      "layer   2  Sparsity: 70.5320%\n",
      "layer   3  Sparsity: 67.2303%\n",
      "total_backward_count 1106270 real_backward_count 189588  17.138%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  2.042856/  2.104850, val:  57.50%, val_best:  81.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2964%\n",
      "layer   2  Sparsity: 70.3712%\n",
      "layer   3  Sparsity: 66.9180%\n",
      "total_backward_count 1116060 real_backward_count 191042  17.118%\n",
      "fc layer 1 self.abs_max_out: 6394.0\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  2.044838/  2.115236, val:  72.08%, val_best:  81.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 72.78 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2744%\n",
      "layer   2  Sparsity: 70.4504%\n",
      "layer   3  Sparsity: 67.1770%\n",
      "total_backward_count 1125850 real_backward_count 192516  17.100%\n",
      "fc layer 1 self.abs_max_out: 6407.0\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  2.048857/  2.111703, val:  74.58%, val_best:  81.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 72.15 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2783%\n",
      "layer   2  Sparsity: 70.2544%\n",
      "layer   3  Sparsity: 67.0995%\n",
      "total_backward_count 1135640 real_backward_count 193943  17.078%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  2.043465/  2.104800, val:  60.83%, val_best:  81.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 73.94 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 97.2936%\n",
      "layer   2  Sparsity: 70.3930%\n",
      "layer   3  Sparsity: 67.2106%\n",
      "total_backward_count 1145430 real_backward_count 195463  17.065%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  2.043024/  2.109599, val:  73.33%, val_best:  81.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 72.68 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2814%\n",
      "layer   2  Sparsity: 70.2448%\n",
      "layer   3  Sparsity: 67.2176%\n",
      "total_backward_count 1155220 real_backward_count 196909  17.045%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  2.046797/  2.113293, val:  61.25%, val_best:  81.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 70.51 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 97.2985%\n",
      "layer   2  Sparsity: 70.3200%\n",
      "layer   3  Sparsity: 67.5368%\n",
      "total_backward_count 1165010 real_backward_count 198355  17.026%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  2.049968/  2.107856, val:  77.92%, val_best:  81.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 97.2573%\n",
      "layer   2  Sparsity: 70.2931%\n",
      "layer   3  Sparsity: 66.7841%\n",
      "total_backward_count 1174800 real_backward_count 199769  17.005%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  2.050594/  2.111991, val:  65.42%, val_best:  81.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 72.89 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 97.2857%\n",
      "layer   2  Sparsity: 70.3382%\n",
      "layer   3  Sparsity: 66.7398%\n",
      "total_backward_count 1184590 real_backward_count 201141  16.980%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  2.044959/  2.117077, val:  65.42%, val_best:  81.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.18 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 97.2984%\n",
      "layer   2  Sparsity: 70.4506%\n",
      "layer   3  Sparsity: 66.4011%\n",
      "total_backward_count 1194380 real_backward_count 202543  16.958%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  2.049581/  2.117009, val:  67.50%, val_best:  81.25%, tr:  99.59%, tr_best:  99.80%, epoch time: 70.87 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 97.2702%\n",
      "layer   2  Sparsity: 70.2401%\n",
      "layer   3  Sparsity: 66.7555%\n",
      "total_backward_count 1204170 real_backward_count 204015  16.942%\n",
      "fc layer 1 self.abs_max_out: 6427.0\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  2.047316/  2.118415, val:  74.58%, val_best:  81.25%, tr:  98.98%, tr_best:  99.80%, epoch time: 71.36 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 97.2750%\n",
      "layer   2  Sparsity: 70.2633%\n",
      "layer   3  Sparsity: 66.9241%\n",
      "total_backward_count 1213960 real_backward_count 205419  16.921%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  2.053608/  2.111146, val:  76.25%, val_best:  81.25%, tr:  99.08%, tr_best:  99.80%, epoch time: 70.71 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 97.2705%\n",
      "layer   2  Sparsity: 70.4200%\n",
      "layer   3  Sparsity: 66.7625%\n",
      "total_backward_count 1223750 real_backward_count 206777  16.897%\n",
      "fc layer 1 self.abs_max_out: 6458.0\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  2.048498/  2.113001, val:  68.33%, val_best:  81.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 70.52 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 97.2775%\n",
      "layer   2  Sparsity: 70.1602%\n",
      "layer   3  Sparsity: 67.3264%\n",
      "total_backward_count 1233540 real_backward_count 208145  16.874%\n",
      "fc layer 2 self.abs_max_out: 6096.0\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  2.042403/  2.114581, val:  42.08%, val_best:  81.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 70.74 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 97.2974%\n",
      "layer   2  Sparsity: 70.1778%\n",
      "layer   3  Sparsity: 66.9902%\n",
      "total_backward_count 1243330 real_backward_count 209542  16.853%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  2.044299/  2.112863, val:  75.00%, val_best:  81.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 70.98 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 97.2829%\n",
      "layer   2  Sparsity: 70.1964%\n",
      "layer   3  Sparsity: 66.8278%\n",
      "total_backward_count 1253120 real_backward_count 210931  16.832%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  2.044336/  2.104033, val:  69.58%, val_best:  81.25%, tr:  98.77%, tr_best:  99.80%, epoch time: 70.91 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 97.2872%\n",
      "layer   2  Sparsity: 70.3497%\n",
      "layer   3  Sparsity: 66.6860%\n",
      "total_backward_count 1262910 real_backward_count 212335  16.813%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  2.042229/  2.104537, val:  74.17%, val_best:  81.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2892%\n",
      "layer   2  Sparsity: 70.4469%\n",
      "layer   3  Sparsity: 66.4694%\n",
      "total_backward_count 1272700 real_backward_count 213687  16.790%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  2.041897/  2.104842, val:  53.75%, val_best:  81.25%, tr:  98.98%, tr_best:  99.80%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2592%\n",
      "layer   2  Sparsity: 70.3502%\n",
      "layer   3  Sparsity: 66.5929%\n",
      "total_backward_count 1282490 real_backward_count 215050  16.768%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  2.041595/  2.104903, val:  70.00%, val_best:  81.25%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2728%\n",
      "layer   2  Sparsity: 70.3494%\n",
      "layer   3  Sparsity: 66.3326%\n",
      "total_backward_count 1292280 real_backward_count 216458  16.750%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  2.038841/  2.111673, val:  65.00%, val_best:  81.25%, tr:  98.98%, tr_best:  99.80%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2968%\n",
      "layer   2  Sparsity: 70.3667%\n",
      "layer   3  Sparsity: 66.0244%\n",
      "total_backward_count 1302070 real_backward_count 217882  16.734%\n",
      "fc layer 1 self.abs_max_out: 6476.0\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  2.038843/  2.112052, val:  63.33%, val_best:  81.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2728%\n",
      "layer   2  Sparsity: 70.1608%\n",
      "layer   3  Sparsity: 66.3225%\n",
      "total_backward_count 1311860 real_backward_count 219304  16.717%\n",
      "fc layer 1 self.abs_max_out: 6540.0\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  2.029304/  2.103259, val:  72.92%, val_best:  81.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2743%\n",
      "layer   2  Sparsity: 70.1685%\n",
      "layer   3  Sparsity: 66.4845%\n",
      "total_backward_count 1321650 real_backward_count 220717  16.700%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  2.034637/  2.092366, val:  62.92%, val_best:  81.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.3011%\n",
      "layer   2  Sparsity: 70.2425%\n",
      "layer   3  Sparsity: 66.7255%\n",
      "total_backward_count 1331440 real_backward_count 222185  16.688%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  2.022503/  2.087103, val:  72.92%, val_best:  81.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2892%\n",
      "layer   2  Sparsity: 70.1490%\n",
      "layer   3  Sparsity: 66.2385%\n",
      "total_backward_count 1341230 real_backward_count 223515  16.665%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  2.026394/  2.087930, val:  74.17%, val_best:  81.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2647%\n",
      "layer   2  Sparsity: 70.2277%\n",
      "layer   3  Sparsity: 66.0683%\n",
      "total_backward_count 1351020 real_backward_count 224894  16.646%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  2.026177/  2.091345, val:  77.92%, val_best:  81.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2635%\n",
      "layer   2  Sparsity: 70.3125%\n",
      "layer   3  Sparsity: 66.5333%\n",
      "total_backward_count 1360810 real_backward_count 226273  16.628%\n",
      "fc layer 1 self.abs_max_out: 6600.0\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  2.030084/  2.101266, val:  63.33%, val_best:  81.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2780%\n",
      "layer   2  Sparsity: 70.5345%\n",
      "layer   3  Sparsity: 67.4354%\n",
      "total_backward_count 1370600 real_backward_count 227747  16.617%\n",
      "fc layer 2 self.abs_max_out: 6135.0\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  2.035971/  2.098766, val:  52.92%, val_best:  81.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2881%\n",
      "layer   2  Sparsity: 70.4975%\n",
      "layer   3  Sparsity: 66.9265%\n",
      "total_backward_count 1380390 real_backward_count 229185  16.603%\n",
      "lif layer 1 self.abs_max_v: 9750.0\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  2.029244/  2.102369, val:  66.25%, val_best:  81.25%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2638%\n",
      "layer   2  Sparsity: 70.4445%\n",
      "layer   3  Sparsity: 66.7856%\n",
      "total_backward_count 1390180 real_backward_count 230473  16.579%\n",
      "lif layer 2 self.abs_max_v: 7814.0\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  2.030943/  2.094472, val:  56.67%, val_best:  81.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2896%\n",
      "layer   2  Sparsity: 70.3961%\n",
      "layer   3  Sparsity: 66.6801%\n",
      "total_backward_count 1399970 real_backward_count 231820  16.559%\n",
      "lif layer 1 self.abs_max_v: 9763.5\n",
      "lif layer 1 self.abs_max_v: 9799.5\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  2.033600/  2.102900, val:  57.50%, val_best:  81.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2640%\n",
      "layer   2  Sparsity: 70.3866%\n",
      "layer   3  Sparsity: 66.8624%\n",
      "total_backward_count 1409760 real_backward_count 233155  16.539%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  2.034724/  2.095201, val:  82.08%, val_best:  82.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2808%\n",
      "layer   2  Sparsity: 70.4535%\n",
      "layer   3  Sparsity: 65.7639%\n",
      "total_backward_count 1419550 real_backward_count 234557  16.523%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  2.029898/  2.101132, val:  71.25%, val_best:  82.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2859%\n",
      "layer   2  Sparsity: 70.2822%\n",
      "layer   3  Sparsity: 65.5538%\n",
      "total_backward_count 1429340 real_backward_count 235918  16.505%\n",
      "fc layer 2 self.abs_max_out: 6221.0\n",
      "lif layer 2 self.abs_max_v: 7816.5\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  2.038063/  2.101416, val:  70.83%, val_best:  82.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2818%\n",
      "layer   2  Sparsity: 70.2838%\n",
      "layer   3  Sparsity: 66.0632%\n",
      "total_backward_count 1439130 real_backward_count 237327  16.491%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  2.035167/  2.102327, val:  45.83%, val_best:  82.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2786%\n",
      "layer   2  Sparsity: 70.3012%\n",
      "layer   3  Sparsity: 66.6950%\n",
      "total_backward_count 1448920 real_backward_count 238658  16.471%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  2.028003/  2.089799, val:  59.58%, val_best:  82.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.3014%\n",
      "layer   2  Sparsity: 70.4103%\n",
      "layer   3  Sparsity: 66.1037%\n",
      "total_backward_count 1458710 real_backward_count 240055  16.457%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  2.030398/  2.089089, val:  72.92%, val_best:  82.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 97.2806%\n",
      "layer   2  Sparsity: 70.3511%\n",
      "layer   3  Sparsity: 66.2601%\n",
      "total_backward_count 1468500 real_backward_count 241432  16.441%\n",
      "fc layer 1 self.abs_max_out: 6626.0\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  2.027201/  2.087241, val:  72.08%, val_best:  82.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2540%\n",
      "layer   2  Sparsity: 70.2183%\n",
      "layer   3  Sparsity: 66.8898%\n",
      "total_backward_count 1478290 real_backward_count 242786  16.423%\n",
      "fc layer 1 self.abs_max_out: 6728.0\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  2.029480/  2.096191, val:  70.83%, val_best:  82.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.3031%\n",
      "layer   2  Sparsity: 70.2267%\n",
      "layer   3  Sparsity: 67.1901%\n",
      "total_backward_count 1488080 real_backward_count 244103  16.404%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  2.034824/  2.103497, val:  53.75%, val_best:  82.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2831%\n",
      "layer   2  Sparsity: 70.2745%\n",
      "layer   3  Sparsity: 67.0005%\n",
      "total_backward_count 1497870 real_backward_count 245489  16.389%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  2.034165/  2.088377, val:  78.33%, val_best:  82.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2825%\n",
      "layer   2  Sparsity: 70.1949%\n",
      "layer   3  Sparsity: 66.4268%\n",
      "total_backward_count 1507660 real_backward_count 246899  16.376%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  2.029281/  2.094772, val:  75.00%, val_best:  82.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2814%\n",
      "layer   2  Sparsity: 70.3422%\n",
      "layer   3  Sparsity: 66.5060%\n",
      "total_backward_count 1517450 real_backward_count 248319  16.364%\n",
      "lif layer 2 self.abs_max_v: 8059.5\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  2.026210/  2.087949, val:  60.83%, val_best:  82.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2869%\n",
      "layer   2  Sparsity: 70.4019%\n",
      "layer   3  Sparsity: 66.3341%\n",
      "total_backward_count 1527240 real_backward_count 249753  16.353%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  2.026296/  2.100773, val:  60.42%, val_best:  82.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2890%\n",
      "layer   2  Sparsity: 70.3427%\n",
      "layer   3  Sparsity: 65.2532%\n",
      "total_backward_count 1537030 real_backward_count 251158  16.340%\n",
      "lif layer 1 self.abs_max_v: 10017.0\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  2.028808/  2.086526, val:  75.83%, val_best:  82.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2684%\n",
      "layer   2  Sparsity: 70.3085%\n",
      "layer   3  Sparsity: 65.6831%\n",
      "total_backward_count 1546820 real_backward_count 252534  16.326%\n",
      "fc layer 1 self.abs_max_out: 6841.0\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  2.019485/  2.086643, val:  76.67%, val_best:  82.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2754%\n",
      "layer   2  Sparsity: 70.3407%\n",
      "layer   3  Sparsity: 65.8673%\n",
      "total_backward_count 1556610 real_backward_count 253887  16.310%\n",
      "fc layer 1 self.abs_max_out: 6889.0\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  2.019137/  2.084347, val:  61.67%, val_best:  82.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 97.2855%\n",
      "layer   2  Sparsity: 70.3657%\n",
      "layer   3  Sparsity: 65.5943%\n",
      "total_backward_count 1566400 real_backward_count 255305  16.299%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  2.027049/  2.095341, val:  61.25%, val_best:  82.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2680%\n",
      "layer   2  Sparsity: 70.3505%\n",
      "layer   3  Sparsity: 65.7356%\n",
      "total_backward_count 1576190 real_backward_count 256683  16.285%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  2.020595/  2.086189, val:  62.50%, val_best:  82.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2783%\n",
      "layer   2  Sparsity: 70.4379%\n",
      "layer   3  Sparsity: 66.1123%\n",
      "total_backward_count 1585980 real_backward_count 257992  16.267%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  2.013987/  2.085580, val:  68.75%, val_best:  82.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2728%\n",
      "layer   2  Sparsity: 70.2791%\n",
      "layer   3  Sparsity: 66.0030%\n",
      "total_backward_count 1595770 real_backward_count 259297  16.249%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  2.015262/  2.094968, val:  64.58%, val_best:  82.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2785%\n",
      "layer   2  Sparsity: 70.1703%\n",
      "layer   3  Sparsity: 66.0416%\n",
      "total_backward_count 1605560 real_backward_count 260663  16.235%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  2.032423/  2.099368, val:  72.08%, val_best:  82.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2955%\n",
      "layer   2  Sparsity: 70.2969%\n",
      "layer   3  Sparsity: 67.1016%\n",
      "total_backward_count 1615350 real_backward_count 262049  16.222%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  2.027344/  2.099935, val:  68.75%, val_best:  82.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2861%\n",
      "layer   2  Sparsity: 70.4427%\n",
      "layer   3  Sparsity: 66.3680%\n",
      "total_backward_count 1625140 real_backward_count 263417  16.209%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  2.026408/  2.097204, val:  58.75%, val_best:  82.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2712%\n",
      "layer   2  Sparsity: 70.4069%\n",
      "layer   3  Sparsity: 66.0877%\n",
      "total_backward_count 1634930 real_backward_count 264762  16.194%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  2.022312/  2.090053, val:  50.42%, val_best:  82.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2819%\n",
      "layer   2  Sparsity: 70.4463%\n",
      "layer   3  Sparsity: 65.0717%\n",
      "total_backward_count 1644720 real_backward_count 266148  16.182%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  2.021552/  2.089189, val:  68.33%, val_best:  82.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2754%\n",
      "layer   2  Sparsity: 70.3893%\n",
      "layer   3  Sparsity: 65.9590%\n",
      "total_backward_count 1654510 real_backward_count 267519  16.169%\n",
      "lif layer 1 self.abs_max_v: 10361.0\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  2.019801/  2.085694, val:  66.25%, val_best:  82.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 97.2677%\n",
      "layer   2  Sparsity: 70.3065%\n",
      "layer   3  Sparsity: 65.7646%\n",
      "total_backward_count 1664300 real_backward_count 268894  16.157%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  2.016530/  2.083358, val:  66.67%, val_best:  82.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2886%\n",
      "layer   2  Sparsity: 70.3307%\n",
      "layer   3  Sparsity: 66.0247%\n",
      "total_backward_count 1674090 real_backward_count 270296  16.146%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  2.017333/  2.092190, val:  56.25%, val_best:  82.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 97.2768%\n",
      "layer   2  Sparsity: 70.2011%\n",
      "layer   3  Sparsity: 66.3724%\n",
      "total_backward_count 1683880 real_backward_count 271710  16.136%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  2.020592/  2.083282, val:  75.83%, val_best:  82.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2711%\n",
      "layer   2  Sparsity: 70.3613%\n",
      "layer   3  Sparsity: 66.7428%\n",
      "total_backward_count 1693670 real_backward_count 273138  16.127%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  2.015005/  2.084129, val:  62.92%, val_best:  82.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.3063%\n",
      "layer   2  Sparsity: 70.2629%\n",
      "layer   3  Sparsity: 65.7709%\n",
      "total_backward_count 1703460 real_backward_count 274483  16.113%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  2.018866/  2.074506, val:  72.08%, val_best:  82.08%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 97.2630%\n",
      "layer   2  Sparsity: 70.0401%\n",
      "layer   3  Sparsity: 65.7712%\n",
      "total_backward_count 1713250 real_backward_count 275887  16.103%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  2.016843/  2.080755, val:  72.92%, val_best:  82.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 97.2727%\n",
      "layer   2  Sparsity: 70.2005%\n",
      "layer   3  Sparsity: 66.3151%\n",
      "total_backward_count 1723040 real_backward_count 277199  16.088%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  2.017099/  2.081321, val:  77.08%, val_best:  82.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2784%\n",
      "layer   2  Sparsity: 70.0025%\n",
      "layer   3  Sparsity: 66.4035%\n",
      "total_backward_count 1732830 real_backward_count 278509  16.072%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  2.017775/  2.087784, val:  76.25%, val_best:  82.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2649%\n",
      "layer   2  Sparsity: 70.1083%\n",
      "layer   3  Sparsity: 66.6123%\n",
      "total_backward_count 1742620 real_backward_count 279810  16.057%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  2.024611/  2.096165, val:  61.67%, val_best:  82.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2651%\n",
      "layer   2  Sparsity: 69.9368%\n",
      "layer   3  Sparsity: 66.7542%\n",
      "total_backward_count 1752410 real_backward_count 281152  16.044%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  2.025762/  2.091863, val:  71.67%, val_best:  82.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 97.2844%\n",
      "layer   2  Sparsity: 70.1693%\n",
      "layer   3  Sparsity: 66.7031%\n",
      "total_backward_count 1762200 real_backward_count 282487  16.030%\n",
      "fc layer 2 self.abs_max_out: 6268.0\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  2.025405/  2.089486, val:  67.08%, val_best:  82.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2520%\n",
      "layer   2  Sparsity: 70.2100%\n",
      "layer   3  Sparsity: 66.4861%\n",
      "total_backward_count 1771990 real_backward_count 283822  16.017%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  2.021013/  2.083806, val:  77.08%, val_best:  82.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 97.2766%\n",
      "layer   2  Sparsity: 70.2619%\n",
      "layer   3  Sparsity: 66.4327%\n",
      "total_backward_count 1781780 real_backward_count 285137  16.003%\n",
      "lif layer 1 self.abs_max_v: 10622.5\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  2.021732/  2.091443, val:  72.08%, val_best:  82.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2811%\n",
      "layer   2  Sparsity: 70.0565%\n",
      "layer   3  Sparsity: 66.7021%\n",
      "total_backward_count 1791570 real_backward_count 286478  15.990%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  2.019968/  2.085744, val:  73.33%, val_best:  82.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 97.2935%\n",
      "layer   2  Sparsity: 70.3060%\n",
      "layer   3  Sparsity: 66.5721%\n",
      "total_backward_count 1801360 real_backward_count 287856  15.980%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  2.017340/  2.092378, val:  61.67%, val_best:  82.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2971%\n",
      "layer   2  Sparsity: 70.4366%\n",
      "layer   3  Sparsity: 66.6684%\n",
      "total_backward_count 1811150 real_backward_count 289228  15.969%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  2.024802/  2.082875, val:  71.25%, val_best:  82.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2954%\n",
      "layer   2  Sparsity: 70.3155%\n",
      "layer   3  Sparsity: 66.0288%\n",
      "total_backward_count 1820940 real_backward_count 290566  15.957%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  2.018227/  2.087251, val:  67.08%, val_best:  82.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2865%\n",
      "layer   2  Sparsity: 70.2763%\n",
      "layer   3  Sparsity: 65.3270%\n",
      "total_backward_count 1830730 real_backward_count 291963  15.948%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  2.018964/  2.091901, val:  55.83%, val_best:  82.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2940%\n",
      "layer   2  Sparsity: 70.2851%\n",
      "layer   3  Sparsity: 65.6274%\n",
      "total_backward_count 1840520 real_backward_count 293316  15.937%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  2.014119/  2.090713, val:  78.75%, val_best:  82.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2799%\n",
      "layer   2  Sparsity: 70.3606%\n",
      "layer   3  Sparsity: 66.1657%\n",
      "total_backward_count 1850310 real_backward_count 294651  15.924%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  2.016711/  2.075439, val:  75.42%, val_best:  82.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2761%\n",
      "layer   2  Sparsity: 70.2985%\n",
      "layer   3  Sparsity: 66.6289%\n",
      "total_backward_count 1860100 real_backward_count 295961  15.911%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  2.016493/  2.084044, val:  75.83%, val_best:  82.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2752%\n",
      "layer   2  Sparsity: 70.2975%\n",
      "layer   3  Sparsity: 66.5210%\n",
      "total_backward_count 1869890 real_backward_count 297228  15.895%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  2.019601/  2.084310, val:  40.00%, val_best:  82.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2826%\n",
      "layer   2  Sparsity: 70.3314%\n",
      "layer   3  Sparsity: 66.3139%\n",
      "total_backward_count 1879680 real_backward_count 298521  15.881%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  2.020655/  2.083179, val:  72.92%, val_best:  82.08%, tr:  99.18%, tr_best:  99.90%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2839%\n",
      "layer   2  Sparsity: 70.4538%\n",
      "layer   3  Sparsity: 66.8876%\n",
      "total_backward_count 1889470 real_backward_count 299899  15.872%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  2.016740/  2.086426, val:  65.00%, val_best:  82.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2816%\n",
      "layer   2  Sparsity: 70.3290%\n",
      "layer   3  Sparsity: 66.6202%\n",
      "total_backward_count 1899260 real_backward_count 301182  15.858%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  2.013822/  2.085024, val:  56.25%, val_best:  82.08%, tr:  99.28%, tr_best:  99.90%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2828%\n",
      "layer   2  Sparsity: 70.3213%\n",
      "layer   3  Sparsity: 65.9608%\n",
      "total_backward_count 1909050 real_backward_count 302502  15.846%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  2.019750/  2.076606, val:  55.00%, val_best:  82.08%, tr:  99.28%, tr_best:  99.90%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2854%\n",
      "layer   2  Sparsity: 70.3135%\n",
      "layer   3  Sparsity: 66.2851%\n",
      "total_backward_count 1918840 real_backward_count 303910  15.838%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  2.019967/  2.076817, val:  71.25%, val_best:  82.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2859%\n",
      "layer   2  Sparsity: 70.2288%\n",
      "layer   3  Sparsity: 66.1035%\n",
      "total_backward_count 1928630 real_backward_count 305267  15.828%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  2.019724/  2.077456, val:  75.00%, val_best:  82.08%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2955%\n",
      "layer   2  Sparsity: 70.3002%\n",
      "layer   3  Sparsity: 65.7282%\n",
      "total_backward_count 1938420 real_backward_count 306657  15.820%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  2.017920/  2.103548, val:  71.67%, val_best:  82.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2802%\n",
      "layer   2  Sparsity: 70.1751%\n",
      "layer   3  Sparsity: 66.8364%\n",
      "total_backward_count 1948210 real_backward_count 307961  15.807%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  2.027278/  2.100668, val:  60.42%, val_best:  82.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2968%\n",
      "layer   2  Sparsity: 70.2302%\n",
      "layer   3  Sparsity: 66.9676%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64203a52657d4bac8db34b5f4f2bcad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÉ‚ñà‚ñà‚ñÜ‚ñÉ‚ñÖ‚ñà‚ñá‚ñÑ‚ñÜ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÉ‚ñà‚ñà‚ñÜ‚ñÉ‚ñÖ‚ñà‚ñá‚ñÑ‚ñÜ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñÉ‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99387</td></tr><tr><td>tr_epoch_loss</td><td>2.02728</td></tr><tr><td>val_acc_best</td><td>0.82083</td></tr><tr><td>val_acc_now</td><td>0.60417</td></tr><tr><td>val_loss</td><td>2.10067</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lyric-sweep-286</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f52qboo0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f52qboo0</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251126_232220-f52qboo0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v04h9w20 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 12000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251127_032901-v04h9w20</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v04h9w20' target=\"_blank\">gentle-sweep-295</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v04h9w20' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v04h9w20</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251127_032910_329', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 20, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 12000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 67f09733060e9328908e01cda0ab3532\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=20, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=20, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 592\n",
      "fc layer 1 self.abs_max_out: 841.0\n",
      "lif layer 1 self.abs_max_v: 841.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 867.0\n",
      "lif layer 1 self.abs_max_v: 1013.0\n",
      "lif layer 1 self.abs_max_v: 1030.5\n",
      "fc layer 2 self.abs_max_out: 128.0\n",
      "lif layer 2 self.abs_max_v: 128.0\n",
      "fc layer 1 self.abs_max_out: 928.0\n",
      "lif layer 1 self.abs_max_v: 1340.5\n",
      "lif layer 2 self.abs_max_v: 192.0\n",
      "fc layer 1 self.abs_max_out: 948.0\n",
      "fc layer 2 self.abs_max_out: 363.0\n",
      "lif layer 2 self.abs_max_v: 420.5\n",
      "smallest_now_T updated: 534\n",
      "fc layer 1 self.abs_max_out: 1265.0\n",
      "fc layer 1 self.abs_max_out: 1290.0\n",
      "lif layer 1 self.abs_max_v: 1742.5\n",
      "fc layer 2 self.abs_max_out: 924.0\n",
      "lif layer 2 self.abs_max_v: 982.5\n",
      "fc layer 1 self.abs_max_out: 1653.0\n",
      "lif layer 1 self.abs_max_v: 1835.0\n",
      "lif layer 2 self.abs_max_v: 992.0\n",
      "lif layer 1 self.abs_max_v: 2059.5\n",
      "lif layer 2 self.abs_max_v: 1225.0\n",
      "fc layer 3 self.abs_max_out: 70.0\n",
      "smallest_now_T updated: 407\n",
      "lif layer 1 self.abs_max_v: 2065.0\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "lif layer 1 self.abs_max_v: 2351.5\n",
      "fc layer 2 self.abs_max_out: 1038.0\n",
      "lif layer 2 self.abs_max_v: 1440.5\n",
      "fc layer 1 self.abs_max_out: 1806.0\n",
      "fc layer 2 self.abs_max_out: 1106.0\n",
      "fc layer 1 self.abs_max_out: 1894.0\n",
      "lif layer 1 self.abs_max_v: 2688.0\n",
      "fc layer 3 self.abs_max_out: 121.0\n",
      "lif layer 2 self.abs_max_v: 1504.5\n",
      "lif layer 1 self.abs_max_v: 2848.5\n",
      "fc layer 2 self.abs_max_out: 1127.0\n",
      "fc layer 2 self.abs_max_out: 1237.0\n",
      "fc layer 3 self.abs_max_out: 155.0\n",
      "lif layer 1 self.abs_max_v: 2856.5\n",
      "lif layer 1 self.abs_max_v: 2874.5\n",
      "fc layer 3 self.abs_max_out: 181.0\n",
      "fc layer 1 self.abs_max_out: 2662.0\n",
      "fc layer 2 self.abs_max_out: 1333.0\n",
      "lif layer 2 self.abs_max_v: 1557.0\n",
      "smallest_now_T updated: 345\n",
      "fc layer 1 self.abs_max_out: 2702.0\n",
      "lif layer 2 self.abs_max_v: 1680.0\n",
      "lif layer 2 self.abs_max_v: 1806.0\n",
      "fc layer 2 self.abs_max_out: 1359.0\n",
      "lif layer 2 self.abs_max_v: 1958.0\n",
      "fc layer 3 self.abs_max_out: 195.0\n",
      "fc layer 2 self.abs_max_out: 1385.0\n",
      "smallest_now_T updated: 317\n",
      "fc layer 1 self.abs_max_out: 3098.0\n",
      "lif layer 1 self.abs_max_v: 3098.0\n",
      "fc layer 3 self.abs_max_out: 251.0\n",
      "fc layer 2 self.abs_max_out: 1387.0\n",
      "lif layer 2 self.abs_max_v: 1976.5\n",
      "fc layer 2 self.abs_max_out: 1520.0\n",
      "lif layer 2 self.abs_max_v: 2362.5\n",
      "fc layer 3 self.abs_max_out: 256.0\n",
      "lif layer 2 self.abs_max_v: 2375.5\n",
      "fc layer 3 self.abs_max_out: 344.0\n",
      "fc layer 2 self.abs_max_out: 1733.0\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "lif layer 1 self.abs_max_v: 3282.0\n",
      "fc layer 1 self.abs_max_out: 3101.0\n",
      "fc layer 1 self.abs_max_out: 3131.0\n",
      "lif layer 1 self.abs_max_v: 3606.0\n",
      "smallest_now_T updated: 286\n",
      "lif layer 2 self.abs_max_v: 2674.0\n",
      "fc layer 2 self.abs_max_out: 1909.0\n",
      "lif layer 2 self.abs_max_v: 3041.0\n",
      "fc layer 1 self.abs_max_out: 3645.0\n",
      "lif layer 1 self.abs_max_v: 3645.0\n",
      "fc layer 1 self.abs_max_out: 3736.0\n",
      "lif layer 1 self.abs_max_v: 3736.0\n",
      "lif layer 2 self.abs_max_v: 3219.5\n",
      "fc layer 1 self.abs_max_out: 3913.0\n",
      "lif layer 1 self.abs_max_v: 3913.0\n",
      "fc layer 1 self.abs_max_out: 3943.0\n",
      "lif layer 1 self.abs_max_v: 3943.0\n",
      "fc layer 1 self.abs_max_out: 4352.0\n",
      "lif layer 1 self.abs_max_v: 4352.0\n",
      "smallest_now_T updated: 247\n",
      "smallest_now_T updated: 192\n",
      "fc layer 2 self.abs_max_out: 1951.0\n",
      "fc layer 3 self.abs_max_out: 419.0\n",
      "lif layer 2 self.abs_max_v: 3296.0\n",
      "fc layer 3 self.abs_max_out: 540.0\n",
      "fc layer 2 self.abs_max_out: 1995.0\n",
      "fc layer 2 self.abs_max_out: 2001.0\n",
      "fc layer 2 self.abs_max_out: 2030.0\n",
      "fc layer 2 self.abs_max_out: 2224.0\n",
      "fc layer 3 self.abs_max_out: 552.0\n",
      "fc layer 2 self.abs_max_out: 2230.0\n",
      "fc layer 3 self.abs_max_out: 558.0\n",
      "fc layer 2 self.abs_max_out: 2488.0\n",
      "fc layer 2 self.abs_max_out: 2586.0\n",
      "lif layer 2 self.abs_max_v: 3616.0\n",
      "fc layer 1 self.abs_max_out: 4587.0\n",
      "lif layer 1 self.abs_max_v: 4587.0\n",
      "fc layer 3 self.abs_max_out: 566.0\n",
      "fc layer 1 self.abs_max_out: 4962.0\n",
      "lif layer 1 self.abs_max_v: 4962.0\n",
      "fc layer 2 self.abs_max_out: 2710.0\n",
      "fc layer 2 self.abs_max_out: 2715.0\n",
      "lif layer 2 self.abs_max_v: 3770.5\n",
      "fc layer 3 self.abs_max_out: 568.0\n",
      "fc layer 2 self.abs_max_out: 2733.0\n",
      "fc layer 1 self.abs_max_out: 4977.0\n",
      "lif layer 1 self.abs_max_v: 4977.0\n",
      "fc layer 1 self.abs_max_out: 5162.0\n",
      "lif layer 1 self.abs_max_v: 5162.0\n",
      "fc layer 2 self.abs_max_out: 3069.0\n",
      "fc layer 1 self.abs_max_out: 5454.0\n",
      "lif layer 1 self.abs_max_v: 5454.0\n",
      "lif layer 2 self.abs_max_v: 3946.0\n",
      "smallest_now_T_val updated: 552\n",
      "smallest_now_T_val updated: 456\n",
      "smallest_now_T_val updated: 448\n",
      "smallest_now_T_val updated: 440\n",
      "smallest_now_T_val updated: 368\n",
      "smallest_now_T_val updated: 137\n",
      "fc layer 1 self.abs_max_out: 5691.0\n",
      "lif layer 1 self.abs_max_v: 5691.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.260003/  2.242584, val:  30.42%, val_best:  30.42%, tr:  29.62%, tr_best:  29.62%, epoch time: 40.85 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9581%\n",
      "layer   2  Sparsity: 86.4158%\n",
      "layer   3  Sparsity: 95.5143%\n",
      "total_backward_count 4895 real_backward_count 3754  76.691%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 6178.0\n",
      "lif layer 1 self.abs_max_v: 6178.0\n",
      "fc layer 1 self.abs_max_out: 6205.0\n",
      "lif layer 1 self.abs_max_v: 6205.0\n",
      "lif layer 2 self.abs_max_v: 3956.5\n",
      "lif layer 2 self.abs_max_v: 4300.5\n",
      "lif layer 2 self.abs_max_v: 4546.5\n",
      "fc layer 3 self.abs_max_out: 574.0\n",
      "fc layer 3 self.abs_max_out: 581.0\n",
      "fc layer 1 self.abs_max_out: 6572.0\n",
      "lif layer 1 self.abs_max_v: 6572.0\n",
      "fc layer 1 self.abs_max_out: 6700.0\n",
      "lif layer 1 self.abs_max_v: 6700.0\n",
      "fc layer 3 self.abs_max_out: 653.0\n",
      "fc layer 1 self.abs_max_out: 7283.0\n",
      "lif layer 1 self.abs_max_v: 7283.0\n",
      "fc layer 2 self.abs_max_out: 3179.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.216737/  2.224379, val:  42.50%, val_best:  42.50%, tr:  52.20%, tr_best:  52.20%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9085%\n",
      "layer   2  Sparsity: 84.9634%\n",
      "layer   3  Sparsity: 93.9518%\n",
      "total_backward_count 9790 real_backward_count 6772  69.173%\n",
      "fc layer 1 self.abs_max_out: 7531.0\n",
      "lif layer 1 self.abs_max_v: 7531.0\n",
      "fc layer 1 self.abs_max_out: 7965.0\n",
      "lif layer 1 self.abs_max_v: 7965.0\n",
      "fc layer 1 self.abs_max_out: 7981.0\n",
      "lif layer 1 self.abs_max_v: 7981.0\n",
      "fc layer 1 self.abs_max_out: 8018.0\n",
      "lif layer 1 self.abs_max_v: 8018.0\n",
      "fc layer 2 self.abs_max_out: 3246.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  2.214314/  2.229486, val:  44.58%, val_best:  44.58%, tr:  62.10%, tr_best:  62.10%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9757%\n",
      "layer   2  Sparsity: 84.7805%\n",
      "layer   3  Sparsity: 93.5530%\n",
      "total_backward_count 14685 real_backward_count 9332  63.548%\n",
      "fc layer 1 self.abs_max_out: 8485.0\n",
      "lif layer 1 self.abs_max_v: 8485.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  2.220975/  2.224546, val:  40.00%, val_best:  44.58%, tr:  66.60%, tr_best:  66.60%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9203%\n",
      "layer   2  Sparsity: 85.1575%\n",
      "layer   3  Sparsity: 93.3717%\n",
      "total_backward_count 19580 real_backward_count 11705  59.780%\n",
      "fc layer 1 self.abs_max_out: 8686.0\n",
      "lif layer 1 self.abs_max_v: 8686.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  2.223197/  2.240155, val:  44.58%, val_best:  44.58%, tr:  68.44%, tr_best:  68.44%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9186%\n",
      "layer   2  Sparsity: 85.4093%\n",
      "layer   3  Sparsity: 93.4797%\n",
      "total_backward_count 24475 real_backward_count 13977  57.107%\n",
      "fc layer 1 self.abs_max_out: 8966.0\n",
      "lif layer 1 self.abs_max_v: 8966.0\n",
      "fc layer 2 self.abs_max_out: 3350.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  2.227453/  2.233131, val:  46.25%, val_best:  46.25%, tr:  70.89%, tr_best:  70.89%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8628%\n",
      "layer   2  Sparsity: 85.1851%\n",
      "layer   3  Sparsity: 93.2810%\n",
      "total_backward_count 29370 real_backward_count 16179  55.087%\n",
      "fc layer 1 self.abs_max_out: 9111.0\n",
      "lif layer 1 self.abs_max_v: 9111.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  2.227220/  2.239271, val:  44.58%, val_best:  46.25%, tr:  68.64%, tr_best:  70.89%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.0034%\n",
      "layer   2  Sparsity: 85.6833%\n",
      "layer   3  Sparsity: 93.2625%\n",
      "total_backward_count 34265 real_backward_count 18387  53.661%\n",
      "fc layer 1 self.abs_max_out: 9315.0\n",
      "lif layer 1 self.abs_max_v: 9315.0\n",
      "fc layer 1 self.abs_max_out: 9334.0\n",
      "lif layer 1 self.abs_max_v: 9334.0\n",
      "fc layer 1 self.abs_max_out: 9613.0\n",
      "lif layer 1 self.abs_max_v: 9613.0\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  2.231934/  2.241669, val:  43.33%, val_best:  46.25%, tr:  72.22%, tr_best:  72.22%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9666%\n",
      "layer   2  Sparsity: 85.9292%\n",
      "layer   3  Sparsity: 93.0847%\n",
      "total_backward_count 39160 real_backward_count 20399  52.091%\n",
      "fc layer 1 self.abs_max_out: 9876.0\n",
      "lif layer 1 self.abs_max_v: 9876.0\n",
      "fc layer 1 self.abs_max_out: 10827.0\n",
      "lif layer 1 self.abs_max_v: 10827.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  2.234554/  2.241024, val:  41.67%, val_best:  46.25%, tr:  72.01%, tr_best:  72.22%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9650%\n",
      "layer   2  Sparsity: 85.2076%\n",
      "layer   3  Sparsity: 92.8785%\n",
      "total_backward_count 44055 real_backward_count 22542  51.168%\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  2.228586/  2.241143, val:  45.83%, val_best:  46.25%, tr:  73.85%, tr_best:  73.85%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9296%\n",
      "layer   2  Sparsity: 84.9011%\n",
      "layer   3  Sparsity: 92.4790%\n",
      "total_backward_count 48950 real_backward_count 24539  50.131%\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  2.223295/  2.236180, val:  45.00%, val_best:  46.25%, tr:  75.28%, tr_best:  75.28%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9336%\n",
      "layer   2  Sparsity: 85.1178%\n",
      "layer   3  Sparsity: 92.1591%\n",
      "total_backward_count 53845 real_backward_count 26502  49.219%\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  2.231287/  2.240797, val:  45.83%, val_best:  46.25%, tr:  72.93%, tr_best:  75.28%, epoch time: 40.07 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9596%\n",
      "layer   2  Sparsity: 85.3004%\n",
      "layer   3  Sparsity: 92.1349%\n",
      "total_backward_count 58740 real_backward_count 28512  48.539%\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  2.228194/  2.241686, val:  41.67%, val_best:  46.25%, tr:  76.20%, tr_best:  76.20%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8906%\n",
      "layer   2  Sparsity: 84.7130%\n",
      "layer   3  Sparsity: 91.8159%\n",
      "total_backward_count 63635 real_backward_count 30398  47.769%\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  2.230008/  2.245416, val:  38.75%, val_best:  46.25%, tr:  77.32%, tr_best:  77.32%, epoch time: 40.60 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9368%\n",
      "layer   2  Sparsity: 84.5913%\n",
      "layer   3  Sparsity: 91.7547%\n",
      "total_backward_count 68530 real_backward_count 32291  47.120%\n",
      "fc layer 2 self.abs_max_out: 3508.0\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  2.230161/  2.240583, val:  43.33%, val_best:  46.25%, tr:  78.96%, tr_best:  78.96%, epoch time: 39.82 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9640%\n",
      "layer   2  Sparsity: 84.4852%\n",
      "layer   3  Sparsity: 91.0957%\n",
      "total_backward_count 73425 real_backward_count 34057  46.383%\n",
      "fc layer 2 self.abs_max_out: 3667.0\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  2.225940/  2.242427, val:  35.83%, val_best:  46.25%, tr:  79.47%, tr_best:  79.47%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9520%\n",
      "layer   2  Sparsity: 84.5475%\n",
      "layer   3  Sparsity: 90.7187%\n",
      "total_backward_count 78320 real_backward_count 35808  45.720%\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  2.226454/  2.241202, val:  46.67%, val_best:  46.67%, tr:  79.16%, tr_best:  79.47%, epoch time: 40.50 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9318%\n",
      "layer   2  Sparsity: 84.9258%\n",
      "layer   3  Sparsity: 91.0478%\n",
      "total_backward_count 83215 real_backward_count 37551  45.125%\n",
      "fc layer 2 self.abs_max_out: 3732.0\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  2.222383/  2.235122, val:  49.58%, val_best:  49.58%, tr:  81.10%, tr_best:  81.10%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9219%\n",
      "layer   2  Sparsity: 84.3030%\n",
      "layer   3  Sparsity: 90.4153%\n",
      "total_backward_count 88110 real_backward_count 39251  44.548%\n",
      "fc layer 2 self.abs_max_out: 3751.0\n",
      "fc layer 1 self.abs_max_out: 10879.0\n",
      "lif layer 1 self.abs_max_v: 10879.0\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  2.220372/  2.234718, val:  48.75%, val_best:  49.58%, tr:  78.75%, tr_best:  81.10%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9421%\n",
      "layer   2  Sparsity: 84.3049%\n",
      "layer   3  Sparsity: 90.1604%\n",
      "total_backward_count 93005 real_backward_count 41066  44.155%\n",
      "fc layer 1 self.abs_max_out: 11204.0\n",
      "lif layer 1 self.abs_max_v: 11204.0\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  2.223680/  2.234405, val:  44.17%, val_best:  49.58%, tr:  81.61%, tr_best:  81.61%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.8576%\n",
      "layer   2  Sparsity: 84.2315%\n",
      "layer   3  Sparsity: 90.3440%\n",
      "total_backward_count 97900 real_backward_count 42716  43.632%\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  2.225090/  2.237519, val:  47.50%, val_best:  49.58%, tr:  81.21%, tr_best:  81.61%, epoch time: 40.13 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9626%\n",
      "layer   2  Sparsity: 84.4359%\n",
      "layer   3  Sparsity: 90.1681%\n",
      "total_backward_count 102795 real_backward_count 44428  43.220%\n",
      "fc layer 1 self.abs_max_out: 11427.0\n",
      "lif layer 1 self.abs_max_v: 11427.0\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  2.225069/  2.242548, val:  50.00%, val_best:  50.00%, tr:  80.29%, tr_best:  81.61%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9332%\n",
      "layer   2  Sparsity: 84.1802%\n",
      "layer   3  Sparsity: 89.8215%\n",
      "total_backward_count 107690 real_backward_count 46122  42.828%\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  2.227017/  2.241043, val:  55.83%, val_best:  55.83%, tr:  82.43%, tr_best:  82.43%, epoch time: 39.94 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9683%\n",
      "layer   2  Sparsity: 84.2752%\n",
      "layer   3  Sparsity: 89.6151%\n",
      "total_backward_count 112585 real_backward_count 47778  42.437%\n",
      "fc layer 1 self.abs_max_out: 11622.0\n",
      "lif layer 1 self.abs_max_v: 11622.0\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  2.226411/  2.241582, val:  44.17%, val_best:  55.83%, tr:  84.17%, tr_best:  84.17%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9387%\n",
      "layer   2  Sparsity: 84.4824%\n",
      "layer   3  Sparsity: 89.9158%\n",
      "total_backward_count 117480 real_backward_count 49413  42.061%\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  2.228183/  2.243827, val:  40.42%, val_best:  55.83%, tr:  83.96%, tr_best:  84.17%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9034%\n",
      "layer   2  Sparsity: 84.3774%\n",
      "layer   3  Sparsity: 89.8313%\n",
      "total_backward_count 122375 real_backward_count 51028  41.698%\n",
      "fc layer 1 self.abs_max_out: 11808.0\n",
      "lif layer 1 self.abs_max_v: 11808.0\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  2.230200/  2.244912, val:  45.83%, val_best:  55.83%, tr:  84.58%, tr_best:  84.58%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9485%\n",
      "layer   2  Sparsity: 84.5917%\n",
      "layer   3  Sparsity: 89.8103%\n",
      "total_backward_count 127270 real_backward_count 52616  41.342%\n",
      "fc layer 1 self.abs_max_out: 12045.0\n",
      "lif layer 1 self.abs_max_v: 12045.0\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  2.230184/  2.239781, val:  45.42%, val_best:  55.83%, tr:  83.25%, tr_best:  84.58%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9026%\n",
      "layer   2  Sparsity: 84.5079%\n",
      "layer   3  Sparsity: 89.5253%\n",
      "total_backward_count 132165 real_backward_count 54144  40.967%\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  2.228720/  2.241943, val:  49.17%, val_best:  55.83%, tr:  85.29%, tr_best:  85.29%, epoch time: 40.16 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9310%\n",
      "layer   2  Sparsity: 84.2161%\n",
      "layer   3  Sparsity: 89.6641%\n",
      "total_backward_count 137060 real_backward_count 55712  40.648%\n",
      "fc layer 1 self.abs_max_out: 12892.0\n",
      "lif layer 1 self.abs_max_v: 12892.0\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  2.228724/  2.245506, val:  40.00%, val_best:  55.83%, tr:  84.47%, tr_best:  85.29%, epoch time: 40.13 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9152%\n",
      "layer   2  Sparsity: 83.9184%\n",
      "layer   3  Sparsity: 89.4613%\n",
      "total_backward_count 141955 real_backward_count 57265  40.340%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  2.229210/  2.237989, val:  43.75%, val_best:  55.83%, tr:  85.80%, tr_best:  85.80%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.0167%\n",
      "layer   2  Sparsity: 83.5705%\n",
      "layer   3  Sparsity: 89.3536%\n",
      "total_backward_count 146850 real_backward_count 58776  40.025%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  2.227574/  2.239045, val:  50.83%, val_best:  55.83%, tr:  86.72%, tr_best:  86.72%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9286%\n",
      "layer   2  Sparsity: 83.2781%\n",
      "layer   3  Sparsity: 88.9776%\n",
      "total_backward_count 151745 real_backward_count 60297  39.736%\n",
      "fc layer 2 self.abs_max_out: 3771.0\n",
      "fc layer 2 self.abs_max_out: 4177.0\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  2.230737/  2.241479, val:  44.17%, val_best:  55.83%, tr:  84.07%, tr_best:  86.72%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9855%\n",
      "layer   2  Sparsity: 84.0246%\n",
      "layer   3  Sparsity: 89.2249%\n",
      "total_backward_count 156640 real_backward_count 61893  39.513%\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  2.229789/  2.248778, val:  50.42%, val_best:  55.83%, tr:  84.37%, tr_best:  86.72%, epoch time: 39.73 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.8710%\n",
      "layer   2  Sparsity: 84.1459%\n",
      "layer   3  Sparsity: 89.3783%\n",
      "total_backward_count 161535 real_backward_count 63370  39.230%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  2.233997/  2.242088, val:  49.58%, val_best:  55.83%, tr:  85.39%, tr_best:  86.72%, epoch time: 39.65 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9209%\n",
      "layer   2  Sparsity: 84.1240%\n",
      "layer   3  Sparsity: 89.1855%\n",
      "total_backward_count 166430 real_backward_count 64844  38.962%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  2.231335/  2.239796, val:  42.08%, val_best:  55.83%, tr:  85.90%, tr_best:  86.72%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8517%\n",
      "layer   2  Sparsity: 83.9593%\n",
      "layer   3  Sparsity: 89.1683%\n",
      "total_backward_count 171325 real_backward_count 66348  38.726%\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  2.233730/  2.248331, val:  39.58%, val_best:  55.83%, tr:  85.50%, tr_best:  86.72%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9368%\n",
      "layer   2  Sparsity: 84.3865%\n",
      "layer   3  Sparsity: 89.3100%\n",
      "total_backward_count 176220 real_backward_count 67884  38.522%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  2.233127/  2.244530, val:  50.83%, val_best:  55.83%, tr:  86.01%, tr_best:  86.72%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9833%\n",
      "layer   2  Sparsity: 84.1616%\n",
      "layer   3  Sparsity: 89.1045%\n",
      "total_backward_count 181115 real_backward_count 69347  38.289%\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  2.229822/  2.244465, val:  46.25%, val_best:  55.83%, tr:  86.93%, tr_best:  86.93%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9156%\n",
      "layer   2  Sparsity: 83.6320%\n",
      "layer   3  Sparsity: 88.6201%\n",
      "total_backward_count 186010 real_backward_count 70801  38.063%\n",
      "fc layer 1 self.abs_max_out: 13291.0\n",
      "lif layer 1 self.abs_max_v: 13291.0\n",
      "lif layer 2 self.abs_max_v: 4633.0\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  2.229412/  2.238801, val:  51.25%, val_best:  55.83%, tr:  86.62%, tr_best:  86.93%, epoch time: 39.87 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9533%\n",
      "layer   2  Sparsity: 83.1386%\n",
      "layer   3  Sparsity: 88.2263%\n",
      "total_backward_count 190905 real_backward_count 72330  37.888%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  2.228849/  2.244102, val:  44.17%, val_best:  55.83%, tr:  86.52%, tr_best:  86.93%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9817%\n",
      "layer   2  Sparsity: 83.2273%\n",
      "layer   3  Sparsity: 88.6095%\n",
      "total_backward_count 195800 real_backward_count 73763  37.673%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  2.230812/  2.245306, val:  49.58%, val_best:  55.83%, tr:  86.21%, tr_best:  86.93%, epoch time: 40.13 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9116%\n",
      "layer   2  Sparsity: 83.1763%\n",
      "layer   3  Sparsity: 88.3733%\n",
      "total_backward_count 200695 real_backward_count 75233  37.486%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  2.230319/  2.241211, val:  47.50%, val_best:  55.83%, tr:  87.23%, tr_best:  87.23%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9551%\n",
      "layer   2  Sparsity: 83.1614%\n",
      "layer   3  Sparsity: 88.2506%\n",
      "total_backward_count 205590 real_backward_count 76656  37.286%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  2.225781/  2.245739, val:  52.50%, val_best:  55.83%, tr:  86.93%, tr_best:  87.23%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8997%\n",
      "layer   2  Sparsity: 83.0417%\n",
      "layer   3  Sparsity: 88.1536%\n",
      "total_backward_count 210485 real_backward_count 78083  37.097%\n",
      "fc layer 1 self.abs_max_out: 13317.0\n",
      "lif layer 1 self.abs_max_v: 13317.0\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  2.229084/  2.244364, val:  49.17%, val_best:  55.83%, tr:  87.54%, tr_best:  87.54%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9281%\n",
      "layer   2  Sparsity: 82.8386%\n",
      "layer   3  Sparsity: 88.1271%\n",
      "total_backward_count 215380 real_backward_count 79467  36.896%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  2.228655/  2.242707, val:  54.17%, val_best:  55.83%, tr:  87.23%, tr_best:  87.54%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9572%\n",
      "layer   2  Sparsity: 83.0984%\n",
      "layer   3  Sparsity: 88.3358%\n",
      "total_backward_count 220275 real_backward_count 80995  36.770%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  2.230138/  2.246230, val:  45.42%, val_best:  55.83%, tr:  86.01%, tr_best:  87.54%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9019%\n",
      "layer   2  Sparsity: 83.1358%\n",
      "layer   3  Sparsity: 88.7077%\n",
      "total_backward_count 225170 real_backward_count 82533  36.654%\n",
      "fc layer 1 self.abs_max_out: 13386.0\n",
      "lif layer 1 self.abs_max_v: 13386.0\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  2.228102/  2.244191, val:  51.67%, val_best:  55.83%, tr:  86.21%, tr_best:  87.54%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8906%\n",
      "layer   2  Sparsity: 83.3432%\n",
      "layer   3  Sparsity: 88.6322%\n",
      "total_backward_count 230065 real_backward_count 83981  36.503%\n",
      "fc layer 2 self.abs_max_out: 4269.0\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  2.226977/  2.250182, val:  38.33%, val_best:  55.83%, tr:  86.31%, tr_best:  87.54%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9767%\n",
      "layer   2  Sparsity: 83.0143%\n",
      "layer   3  Sparsity: 88.3829%\n",
      "total_backward_count 234960 real_backward_count 85417  36.354%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  2.228364/  2.247241, val:  43.33%, val_best:  55.83%, tr:  86.82%, tr_best:  87.54%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.0144%\n",
      "layer   2  Sparsity: 82.7377%\n",
      "layer   3  Sparsity: 88.2392%\n",
      "total_backward_count 239855 real_backward_count 86831  36.201%\n",
      "fc layer 1 self.abs_max_out: 13837.0\n",
      "lif layer 1 self.abs_max_v: 13837.0\n",
      "lif layer 2 self.abs_max_v: 4697.0\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  2.227986/  2.244921, val:  48.75%, val_best:  55.83%, tr:  86.82%, tr_best:  87.54%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9238%\n",
      "layer   2  Sparsity: 82.6252%\n",
      "layer   3  Sparsity: 87.8349%\n",
      "total_backward_count 244750 real_backward_count 88279  36.069%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  2.229647/  2.241433, val:  42.50%, val_best:  55.83%, tr:  86.41%, tr_best:  87.54%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9059%\n",
      "layer   2  Sparsity: 82.7084%\n",
      "layer   3  Sparsity: 88.1474%\n",
      "total_backward_count 249645 real_backward_count 89757  35.954%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  2.227711/  2.246228, val:  51.25%, val_best:  55.83%, tr:  86.52%, tr_best:  87.54%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8901%\n",
      "layer   2  Sparsity: 82.7815%\n",
      "layer   3  Sparsity: 87.8949%\n",
      "total_backward_count 254540 real_backward_count 91186  35.824%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  2.225661/  2.243232, val:  50.42%, val_best:  55.83%, tr:  86.82%, tr_best:  87.54%, epoch time: 41.29 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 87.9386%\n",
      "layer   2  Sparsity: 82.8931%\n",
      "layer   3  Sparsity: 87.6420%\n",
      "total_backward_count 259435 real_backward_count 92643  35.710%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  2.227652/  2.245459, val:  52.50%, val_best:  55.83%, tr:  87.54%, tr_best:  87.54%, epoch time: 40.77 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9222%\n",
      "layer   2  Sparsity: 82.8756%\n",
      "layer   3  Sparsity: 88.2427%\n",
      "total_backward_count 264330 real_backward_count 94093  35.597%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  2.231241/  2.247135, val:  60.00%, val_best:  60.00%, tr:  87.23%, tr_best:  87.54%, epoch time: 39.91 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9210%\n",
      "layer   2  Sparsity: 82.7975%\n",
      "layer   3  Sparsity: 88.1560%\n",
      "total_backward_count 269225 real_backward_count 95512  35.477%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  2.229172/  2.247726, val:  46.25%, val_best:  60.00%, tr:  87.84%, tr_best:  87.84%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9620%\n",
      "layer   2  Sparsity: 82.9153%\n",
      "layer   3  Sparsity: 88.2967%\n",
      "total_backward_count 274120 real_backward_count 96914  35.355%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  2.225195/  2.241523, val:  45.00%, val_best:  60.00%, tr:  89.07%, tr_best:  89.07%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9193%\n",
      "layer   2  Sparsity: 82.1861%\n",
      "layer   3  Sparsity: 87.8977%\n",
      "total_backward_count 279015 real_backward_count 98318  35.238%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  2.229043/  2.248324, val:  51.67%, val_best:  60.00%, tr:  86.93%, tr_best:  89.07%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.0307%\n",
      "layer   2  Sparsity: 82.1263%\n",
      "layer   3  Sparsity: 88.2252%\n",
      "total_backward_count 283910 real_backward_count 99697  35.116%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  2.226176/  2.244039, val:  50.83%, val_best:  60.00%, tr:  87.64%, tr_best:  89.07%, epoch time: 40.58 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9263%\n",
      "layer   2  Sparsity: 81.9737%\n",
      "layer   3  Sparsity: 88.4970%\n",
      "total_backward_count 288805 real_backward_count 101080  34.999%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  2.228078/  2.247988, val:  40.00%, val_best:  60.00%, tr:  86.52%, tr_best:  89.07%, epoch time: 39.98 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9537%\n",
      "layer   2  Sparsity: 82.4316%\n",
      "layer   3  Sparsity: 88.7939%\n",
      "total_backward_count 293700 real_backward_count 102467  34.888%\n",
      "fc layer 1 self.abs_max_out: 13850.0\n",
      "lif layer 1 self.abs_max_v: 13850.0\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  2.228976/  2.245720, val:  42.92%, val_best:  60.00%, tr:  87.23%, tr_best:  89.07%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9363%\n",
      "layer   2  Sparsity: 82.4442%\n",
      "layer   3  Sparsity: 88.5189%\n",
      "total_backward_count 298595 real_backward_count 103879  34.789%\n",
      "fc layer 1 self.abs_max_out: 13881.0\n",
      "lif layer 1 self.abs_max_v: 13881.0\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  2.226726/  2.246534, val:  41.67%, val_best:  60.00%, tr:  86.62%, tr_best:  89.07%, epoch time: 39.48 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.8619%\n",
      "layer   2  Sparsity: 82.4968%\n",
      "layer   3  Sparsity: 88.2553%\n",
      "total_backward_count 303490 real_backward_count 105305  34.698%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  2.223879/  2.244817, val:  44.58%, val_best:  60.00%, tr:  86.11%, tr_best:  89.07%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9377%\n",
      "layer   2  Sparsity: 82.1701%\n",
      "layer   3  Sparsity: 87.8838%\n",
      "total_backward_count 308385 real_backward_count 106691  34.597%\n",
      "lif layer 1 self.abs_max_v: 13926.0\n",
      "lif layer 1 self.abs_max_v: 14205.5\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  2.229997/  2.246927, val:  39.58%, val_best:  60.00%, tr:  87.13%, tr_best:  89.07%, epoch time: 39.87 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9734%\n",
      "layer   2  Sparsity: 82.2112%\n",
      "layer   3  Sparsity: 87.9183%\n",
      "total_backward_count 313280 real_backward_count 108091  34.503%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  2.227985/  2.245985, val:  50.83%, val_best:  60.00%, tr:  86.11%, tr_best:  89.07%, epoch time: 40.38 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9343%\n",
      "layer   2  Sparsity: 81.9852%\n",
      "layer   3  Sparsity: 87.8420%\n",
      "total_backward_count 318175 real_backward_count 109470  34.406%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  2.227043/  2.244693, val:  44.17%, val_best:  60.00%, tr:  87.84%, tr_best:  89.07%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9728%\n",
      "layer   2  Sparsity: 82.3564%\n",
      "layer   3  Sparsity: 87.8999%\n",
      "total_backward_count 323070 real_backward_count 110837  34.307%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  2.225939/  2.244915, val:  45.83%, val_best:  60.00%, tr:  86.72%, tr_best:  89.07%, epoch time: 40.10 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9281%\n",
      "layer   2  Sparsity: 82.6654%\n",
      "layer   3  Sparsity: 88.0816%\n",
      "total_backward_count 327965 real_backward_count 112165  34.200%\n",
      "fc layer 2 self.abs_max_out: 4300.0\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  2.228204/  2.248085, val:  50.83%, val_best:  60.00%, tr:  88.76%, tr_best:  89.07%, epoch time: 40.13 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9098%\n",
      "layer   2  Sparsity: 82.4623%\n",
      "layer   3  Sparsity: 87.7846%\n",
      "total_backward_count 332860 real_backward_count 113507  34.101%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  2.229042/  2.241397, val:  54.58%, val_best:  60.00%, tr:  87.95%, tr_best:  89.07%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9043%\n",
      "layer   2  Sparsity: 82.6102%\n",
      "layer   3  Sparsity: 88.3127%\n",
      "total_backward_count 337755 real_backward_count 114864  34.008%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  2.224916/  2.244713, val:  46.25%, val_best:  60.00%, tr:  87.03%, tr_best:  89.07%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8865%\n",
      "layer   2  Sparsity: 82.6996%\n",
      "layer   3  Sparsity: 87.8995%\n",
      "total_backward_count 342650 real_backward_count 116311  33.945%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  2.227436/  2.244441, val:  54.17%, val_best:  60.00%, tr:  87.95%, tr_best:  89.07%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9599%\n",
      "layer   2  Sparsity: 82.5787%\n",
      "layer   3  Sparsity: 87.8390%\n",
      "total_backward_count 347545 real_backward_count 117751  33.881%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  2.225003/  2.243038, val:  51.25%, val_best:  60.00%, tr:  87.84%, tr_best:  89.07%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8802%\n",
      "layer   2  Sparsity: 82.4011%\n",
      "layer   3  Sparsity: 87.7988%\n",
      "total_backward_count 352440 real_backward_count 119111  33.796%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  2.226803/  2.246600, val:  47.08%, val_best:  60.00%, tr:  86.93%, tr_best:  89.07%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8902%\n",
      "layer   2  Sparsity: 82.1714%\n",
      "layer   3  Sparsity: 87.9351%\n",
      "total_backward_count 357335 real_backward_count 120513  33.725%\n",
      "fc layer 1 self.abs_max_out: 14180.0\n",
      "fc layer 1 self.abs_max_out: 14466.0\n",
      "lif layer 1 self.abs_max_v: 14466.0\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  2.224733/  2.241344, val:  48.75%, val_best:  60.00%, tr:  88.36%, tr_best:  89.07%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.0145%\n",
      "layer   2  Sparsity: 81.8197%\n",
      "layer   3  Sparsity: 87.3971%\n",
      "total_backward_count 362230 real_backward_count 121848  33.638%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  2.222080/  2.242818, val:  50.00%, val_best:  60.00%, tr:  88.36%, tr_best:  89.07%, epoch time: 40.24 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9415%\n",
      "layer   2  Sparsity: 82.0275%\n",
      "layer   3  Sparsity: 87.4368%\n",
      "total_backward_count 367125 real_backward_count 123248  33.571%\n",
      "fc layer 2 self.abs_max_out: 4432.0\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  2.225865/  2.239942, val:  57.50%, val_best:  60.00%, tr:  89.07%, tr_best:  89.07%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9445%\n",
      "layer   2  Sparsity: 82.0692%\n",
      "layer   3  Sparsity: 87.3153%\n",
      "total_backward_count 372020 real_backward_count 124577  33.487%\n",
      "fc layer 1 self.abs_max_out: 14841.0\n",
      "lif layer 1 self.abs_max_v: 14841.0\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  2.223467/  2.243897, val:  45.00%, val_best:  60.00%, tr:  87.84%, tr_best:  89.07%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8953%\n",
      "layer   2  Sparsity: 82.1739%\n",
      "layer   3  Sparsity: 87.2001%\n",
      "total_backward_count 376915 real_backward_count 125980  33.424%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  2.225201/  2.246655, val:  43.33%, val_best:  60.00%, tr:  89.48%, tr_best:  89.48%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9543%\n",
      "layer   2  Sparsity: 82.4359%\n",
      "layer   3  Sparsity: 87.2122%\n",
      "total_backward_count 381810 real_backward_count 127317  33.346%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  2.226004/  2.239176, val:  43.33%, val_best:  60.00%, tr:  89.48%, tr_best:  89.48%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9803%\n",
      "layer   2  Sparsity: 82.6873%\n",
      "layer   3  Sparsity: 87.8728%\n",
      "total_backward_count 386705 real_backward_count 128656  33.270%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  2.228693/  2.245159, val:  56.25%, val_best:  60.00%, tr:  87.13%, tr_best:  89.48%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9604%\n",
      "layer   2  Sparsity: 82.6749%\n",
      "layer   3  Sparsity: 87.7542%\n",
      "total_backward_count 391600 real_backward_count 130053  33.211%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  2.231984/  2.252803, val:  52.08%, val_best:  60.00%, tr:  88.36%, tr_best:  89.48%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9409%\n",
      "layer   2  Sparsity: 82.6587%\n",
      "layer   3  Sparsity: 87.7495%\n",
      "total_backward_count 396495 real_backward_count 131425  33.147%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  2.228820/  2.241886, val:  42.92%, val_best:  60.00%, tr:  87.03%, tr_best:  89.48%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9943%\n",
      "layer   2  Sparsity: 82.0660%\n",
      "layer   3  Sparsity: 87.1470%\n",
      "total_backward_count 401390 real_backward_count 132747  33.072%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  2.227449/  2.243097, val:  49.58%, val_best:  60.00%, tr:  88.87%, tr_best:  89.48%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9319%\n",
      "layer   2  Sparsity: 82.5654%\n",
      "layer   3  Sparsity: 87.3907%\n",
      "total_backward_count 406285 real_backward_count 134076  33.000%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  2.229800/  2.247281, val:  45.00%, val_best:  60.00%, tr:  88.76%, tr_best:  89.48%, epoch time: 40.75 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.8921%\n",
      "layer   2  Sparsity: 82.6784%\n",
      "layer   3  Sparsity: 87.6095%\n",
      "total_backward_count 411180 real_backward_count 135448  32.941%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  2.229285/  2.245936, val:  50.83%, val_best:  60.00%, tr:  88.36%, tr_best:  89.48%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9146%\n",
      "layer   2  Sparsity: 82.5843%\n",
      "layer   3  Sparsity: 87.7590%\n",
      "total_backward_count 416075 real_backward_count 136849  32.890%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  2.226872/  2.245630, val:  43.75%, val_best:  60.00%, tr:  87.54%, tr_best:  89.48%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.0049%\n",
      "layer   2  Sparsity: 82.4413%\n",
      "layer   3  Sparsity: 87.3952%\n",
      "total_backward_count 420970 real_backward_count 138206  32.830%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  2.224857/  2.248678, val:  44.17%, val_best:  60.00%, tr:  89.48%, tr_best:  89.48%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.0316%\n",
      "layer   2  Sparsity: 82.3553%\n",
      "layer   3  Sparsity: 87.5532%\n",
      "total_backward_count 425865 real_backward_count 139551  32.769%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  2.228037/  2.239099, val:  49.17%, val_best:  60.00%, tr:  88.05%, tr_best:  89.48%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.7924%\n",
      "layer   2  Sparsity: 82.3716%\n",
      "layer   3  Sparsity: 87.6679%\n",
      "total_backward_count 430760 real_backward_count 140965  32.725%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  2.222295/  2.240499, val:  52.92%, val_best:  60.00%, tr:  88.76%, tr_best:  89.48%, epoch time: 40.10 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.0244%\n",
      "layer   2  Sparsity: 82.2312%\n",
      "layer   3  Sparsity: 87.1788%\n",
      "total_backward_count 435655 real_backward_count 142288  32.661%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  2.225074/  2.244724, val:  43.75%, val_best:  60.00%, tr:  87.95%, tr_best:  89.48%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.0618%\n",
      "layer   2  Sparsity: 82.1818%\n",
      "layer   3  Sparsity: 87.0893%\n",
      "total_backward_count 440550 real_backward_count 143653  32.608%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  2.223660/  2.248986, val:  44.17%, val_best:  60.00%, tr:  88.87%, tr_best:  89.48%, epoch time: 39.98 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9529%\n",
      "layer   2  Sparsity: 82.2289%\n",
      "layer   3  Sparsity: 87.0088%\n",
      "total_backward_count 445445 real_backward_count 145017  32.556%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  2.226521/  2.245102, val:  51.67%, val_best:  60.00%, tr:  88.87%, tr_best:  89.48%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8732%\n",
      "layer   2  Sparsity: 82.3690%\n",
      "layer   3  Sparsity: 87.5936%\n",
      "total_backward_count 450340 real_backward_count 146328  32.493%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  2.227682/  2.244464, val:  45.00%, val_best:  60.00%, tr:  89.58%, tr_best:  89.58%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9598%\n",
      "layer   2  Sparsity: 82.3157%\n",
      "layer   3  Sparsity: 87.5218%\n",
      "total_backward_count 455235 real_backward_count 147646  32.433%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  2.226235/  2.243929, val:  50.00%, val_best:  60.00%, tr:  87.54%, tr_best:  89.58%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9335%\n",
      "layer   2  Sparsity: 82.4011%\n",
      "layer   3  Sparsity: 87.4911%\n",
      "total_backward_count 460130 real_backward_count 148978  32.377%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  2.230240/  2.245035, val:  54.17%, val_best:  60.00%, tr:  88.05%, tr_best:  89.58%, epoch time: 39.94 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.0510%\n",
      "layer   2  Sparsity: 82.9092%\n",
      "layer   3  Sparsity: 87.6823%\n",
      "total_backward_count 465025 real_backward_count 150322  32.326%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  2.225629/  2.244527, val:  50.83%, val_best:  60.00%, tr:  88.87%, tr_best:  89.58%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9051%\n",
      "layer   2  Sparsity: 82.7978%\n",
      "layer   3  Sparsity: 87.4454%\n",
      "total_backward_count 469920 real_backward_count 151660  32.274%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  2.226289/  2.246079, val:  47.50%, val_best:  60.00%, tr:  89.17%, tr_best:  89.58%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9703%\n",
      "layer   2  Sparsity: 82.8072%\n",
      "layer   3  Sparsity: 87.4715%\n",
      "total_backward_count 474815 real_backward_count 152922  32.207%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  2.222289/  2.240488, val:  51.25%, val_best:  60.00%, tr:  89.58%, tr_best:  89.58%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9600%\n",
      "layer   2  Sparsity: 82.3174%\n",
      "layer   3  Sparsity: 87.4023%\n",
      "total_backward_count 479710 real_backward_count 154215  32.148%\n",
      "fc layer 1 self.abs_max_out: 14970.0\n",
      "lif layer 1 self.abs_max_v: 14970.0\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  2.222278/  2.247415, val:  45.42%, val_best:  60.00%, tr:  88.76%, tr_best:  89.58%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.0317%\n",
      "layer   2  Sparsity: 82.4152%\n",
      "layer   3  Sparsity: 87.3111%\n",
      "total_backward_count 484605 real_backward_count 155529  32.094%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  2.223191/  2.243628, val:  51.25%, val_best:  60.00%, tr:  89.68%, tr_best:  89.68%, epoch time: 39.78 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9584%\n",
      "layer   2  Sparsity: 82.4071%\n",
      "layer   3  Sparsity: 87.4719%\n",
      "total_backward_count 489500 real_backward_count 156823  32.037%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  2.223100/  2.242733, val:  53.75%, val_best:  60.00%, tr:  89.68%, tr_best:  89.68%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9244%\n",
      "layer   2  Sparsity: 82.7784%\n",
      "layer   3  Sparsity: 87.9584%\n",
      "total_backward_count 494395 real_backward_count 158121  31.983%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  2.225598/  2.248288, val:  55.00%, val_best:  60.00%, tr:  89.48%, tr_best:  89.68%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.8765%\n",
      "layer   2  Sparsity: 82.3988%\n",
      "layer   3  Sparsity: 87.6330%\n",
      "total_backward_count 499290 real_backward_count 159466  31.939%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  2.227512/  2.242411, val:  45.83%, val_best:  60.00%, tr:  89.17%, tr_best:  89.68%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9356%\n",
      "layer   2  Sparsity: 82.2362%\n",
      "layer   3  Sparsity: 87.6243%\n",
      "total_backward_count 504185 real_backward_count 160776  31.888%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  2.229272/  2.247460, val:  52.92%, val_best:  60.00%, tr:  89.27%, tr_best:  89.68%, epoch time: 39.84 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9008%\n",
      "layer   2  Sparsity: 82.4471%\n",
      "layer   3  Sparsity: 87.9212%\n",
      "total_backward_count 509080 real_backward_count 162077  31.837%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  2.232219/  2.247278, val:  51.67%, val_best:  60.00%, tr:  88.97%, tr_best:  89.68%, epoch time: 41.02 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.8987%\n",
      "layer   2  Sparsity: 82.4135%\n",
      "layer   3  Sparsity: 87.8988%\n",
      "total_backward_count 513975 real_backward_count 163430  31.797%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  2.230531/  2.250157, val:  48.75%, val_best:  60.00%, tr:  87.95%, tr_best:  89.68%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.0135%\n",
      "layer   2  Sparsity: 82.4965%\n",
      "layer   3  Sparsity: 87.8469%\n",
      "total_backward_count 518870 real_backward_count 164799  31.761%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  2.230443/  2.245268, val:  49.58%, val_best:  60.00%, tr:  88.66%, tr_best:  89.68%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.0319%\n",
      "layer   2  Sparsity: 82.6615%\n",
      "layer   3  Sparsity: 87.9965%\n",
      "total_backward_count 523765 real_backward_count 166057  31.704%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  2.227397/  2.243521, val:  48.33%, val_best:  60.00%, tr:  90.70%, tr_best:  90.70%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9980%\n",
      "layer   2  Sparsity: 82.3526%\n",
      "layer   3  Sparsity: 87.8568%\n",
      "total_backward_count 528660 real_backward_count 167359  31.657%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  2.230897/  2.250064, val:  56.25%, val_best:  60.00%, tr:  89.27%, tr_best:  90.70%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9356%\n",
      "layer   2  Sparsity: 82.3710%\n",
      "layer   3  Sparsity: 87.9272%\n",
      "total_backward_count 533555 real_backward_count 168645  31.608%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  2.229366/  2.244965, val:  47.92%, val_best:  60.00%, tr:  89.48%, tr_best:  90.70%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9845%\n",
      "layer   2  Sparsity: 82.4095%\n",
      "layer   3  Sparsity: 88.0100%\n",
      "total_backward_count 538450 real_backward_count 169906  31.555%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  2.229954/  2.244604, val:  49.17%, val_best:  60.00%, tr:  89.48%, tr_best:  90.70%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.0023%\n",
      "layer   2  Sparsity: 82.5075%\n",
      "layer   3  Sparsity: 88.2002%\n",
      "total_backward_count 543345 real_backward_count 171202  31.509%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  2.227704/  2.241102, val:  58.75%, val_best:  60.00%, tr:  89.17%, tr_best:  90.70%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9489%\n",
      "layer   2  Sparsity: 81.7562%\n",
      "layer   3  Sparsity: 87.7177%\n",
      "total_backward_count 548240 real_backward_count 172460  31.457%\n",
      "fc layer 1 self.abs_max_out: 15006.0\n",
      "lif layer 1 self.abs_max_v: 15006.0\n",
      "fc layer 1 self.abs_max_out: 15260.0\n",
      "lif layer 1 self.abs_max_v: 15260.0\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  2.223441/  2.240923, val:  50.42%, val_best:  60.00%, tr:  89.07%, tr_best:  90.70%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9121%\n",
      "layer   2  Sparsity: 81.7267%\n",
      "layer   3  Sparsity: 87.3546%\n",
      "total_backward_count 553135 real_backward_count 173736  31.409%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  2.222412/  2.239925, val:  45.83%, val_best:  60.00%, tr:  90.60%, tr_best:  90.70%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9276%\n",
      "layer   2  Sparsity: 81.8824%\n",
      "layer   3  Sparsity: 87.2075%\n",
      "total_backward_count 558030 real_backward_count 174994  31.359%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  2.222885/  2.244322, val:  45.83%, val_best:  60.00%, tr:  89.38%, tr_best:  90.70%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9000%\n",
      "layer   2  Sparsity: 82.4339%\n",
      "layer   3  Sparsity: 87.3431%\n",
      "total_backward_count 562925 real_backward_count 176287  31.316%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  2.228529/  2.251921, val:  46.25%, val_best:  60.00%, tr:  88.76%, tr_best:  90.70%, epoch time: 39.90 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9855%\n",
      "layer   2  Sparsity: 82.5855%\n",
      "layer   3  Sparsity: 87.4136%\n",
      "total_backward_count 567820 real_backward_count 177589  31.276%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  2.228379/  2.244457, val:  45.00%, val_best:  60.00%, tr:  88.66%, tr_best:  90.70%, epoch time: 41.01 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9008%\n",
      "layer   2  Sparsity: 82.2281%\n",
      "layer   3  Sparsity: 87.4692%\n",
      "total_backward_count 572715 real_backward_count 178923  31.241%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  2.226980/  2.242450, val:  53.75%, val_best:  60.00%, tr:  89.58%, tr_best:  90.70%, epoch time: 40.81 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9229%\n",
      "layer   2  Sparsity: 82.1315%\n",
      "layer   3  Sparsity: 87.4628%\n",
      "total_backward_count 577610 real_backward_count 180276  31.211%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  2.226133/  2.242724, val:  56.25%, val_best:  60.00%, tr:  88.25%, tr_best:  90.70%, epoch time: 41.62 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 87.9719%\n",
      "layer   2  Sparsity: 81.8763%\n",
      "layer   3  Sparsity: 87.2934%\n",
      "total_backward_count 582505 real_backward_count 181618  31.179%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  2.223887/  2.241796, val:  55.42%, val_best:  60.00%, tr:  89.58%, tr_best:  90.70%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9128%\n",
      "layer   2  Sparsity: 81.8710%\n",
      "layer   3  Sparsity: 87.2546%\n",
      "total_backward_count 587400 real_backward_count 182910  31.139%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  2.227519/  2.246724, val:  50.83%, val_best:  60.00%, tr:  88.56%, tr_best:  90.70%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9448%\n",
      "layer   2  Sparsity: 82.2275%\n",
      "layer   3  Sparsity: 87.5030%\n",
      "total_backward_count 592295 real_backward_count 184206  31.100%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  2.227953/  2.240563, val:  54.58%, val_best:  60.00%, tr:  89.07%, tr_best:  90.70%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9705%\n",
      "layer   2  Sparsity: 82.1762%\n",
      "layer   3  Sparsity: 87.3792%\n",
      "total_backward_count 597190 real_backward_count 185493  31.061%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  2.227073/  2.246942, val:  43.33%, val_best:  60.00%, tr:  89.99%, tr_best:  90.70%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9108%\n",
      "layer   2  Sparsity: 82.1026%\n",
      "layer   3  Sparsity: 87.4263%\n",
      "total_backward_count 602085 real_backward_count 186831  31.031%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  2.227556/  2.242978, val:  52.08%, val_best:  60.00%, tr:  89.48%, tr_best:  90.70%, epoch time: 41.02 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9165%\n",
      "layer   2  Sparsity: 81.9509%\n",
      "layer   3  Sparsity: 87.2433%\n",
      "total_backward_count 606980 real_backward_count 188148  30.997%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  2.222537/  2.239942, val:  55.83%, val_best:  60.00%, tr:  89.38%, tr_best:  90.70%, epoch time: 41.40 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 87.8456%\n",
      "layer   2  Sparsity: 81.6581%\n",
      "layer   3  Sparsity: 87.0912%\n",
      "total_backward_count 611875 real_backward_count 189451  30.962%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  2.223644/  2.244465, val:  52.92%, val_best:  60.00%, tr:  89.89%, tr_best:  90.70%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9206%\n",
      "layer   2  Sparsity: 81.8817%\n",
      "layer   3  Sparsity: 88.0043%\n",
      "total_backward_count 616770 real_backward_count 190731  30.924%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  2.227495/  2.240977, val:  47.50%, val_best:  60.00%, tr:  88.46%, tr_best:  90.70%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9410%\n",
      "layer   2  Sparsity: 82.0568%\n",
      "layer   3  Sparsity: 87.8266%\n",
      "total_backward_count 621665 real_backward_count 192054  30.893%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  2.221425/  2.241069, val:  50.83%, val_best:  60.00%, tr:  90.19%, tr_best:  90.70%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9364%\n",
      "layer   2  Sparsity: 82.0088%\n",
      "layer   3  Sparsity: 87.4365%\n",
      "total_backward_count 626560 real_backward_count 193341  30.858%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  2.222893/  2.242215, val:  47.08%, val_best:  60.00%, tr:  89.68%, tr_best:  90.70%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9258%\n",
      "layer   2  Sparsity: 81.9201%\n",
      "layer   3  Sparsity: 86.8414%\n",
      "total_backward_count 631455 real_backward_count 194668  30.828%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  2.219731/  2.241232, val:  46.25%, val_best:  60.00%, tr:  90.81%, tr_best:  90.81%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9747%\n",
      "layer   2  Sparsity: 82.1147%\n",
      "layer   3  Sparsity: 87.0445%\n",
      "total_backward_count 636350 real_backward_count 195940  30.791%\n",
      "fc layer 1 self.abs_max_out: 15418.0\n",
      "lif layer 1 self.abs_max_v: 15418.0\n",
      "fc layer 1 self.abs_max_out: 15643.0\n",
      "lif layer 1 self.abs_max_v: 15643.0\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  2.225268/  2.237199, val:  45.83%, val_best:  60.00%, tr:  88.87%, tr_best:  90.81%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9428%\n",
      "layer   2  Sparsity: 82.4121%\n",
      "layer   3  Sparsity: 87.3463%\n",
      "total_backward_count 641245 real_backward_count 197213  30.755%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  2.220585/  2.240861, val:  47.92%, val_best:  60.00%, tr:  89.07%, tr_best:  90.81%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9647%\n",
      "layer   2  Sparsity: 82.1295%\n",
      "layer   3  Sparsity: 87.0473%\n",
      "total_backward_count 646140 real_backward_count 198520  30.724%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  2.223595/  2.240905, val:  56.67%, val_best:  60.00%, tr:  89.27%, tr_best:  90.81%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9622%\n",
      "layer   2  Sparsity: 82.1121%\n",
      "layer   3  Sparsity: 87.2390%\n",
      "total_backward_count 651035 real_backward_count 199774  30.686%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  2.222012/  2.236145, val:  50.42%, val_best:  60.00%, tr:  89.38%, tr_best:  90.81%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8220%\n",
      "layer   2  Sparsity: 81.8838%\n",
      "layer   3  Sparsity: 87.1212%\n",
      "total_backward_count 655930 real_backward_count 201043  30.650%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  2.225499/  2.242824, val:  41.25%, val_best:  60.00%, tr:  89.07%, tr_best:  90.81%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9585%\n",
      "layer   2  Sparsity: 81.9640%\n",
      "layer   3  Sparsity: 87.3928%\n",
      "total_backward_count 660825 real_backward_count 202361  30.622%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  2.226120/  2.239568, val:  54.58%, val_best:  60.00%, tr:  90.40%, tr_best:  90.81%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9774%\n",
      "layer   2  Sparsity: 82.2530%\n",
      "layer   3  Sparsity: 87.3554%\n",
      "total_backward_count 665720 real_backward_count 203673  30.594%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  2.224979/  2.239891, val:  55.00%, val_best:  60.00%, tr:  88.46%, tr_best:  90.81%, epoch time: 39.84 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9731%\n",
      "layer   2  Sparsity: 82.7487%\n",
      "layer   3  Sparsity: 87.8592%\n",
      "total_backward_count 670615 real_backward_count 204947  30.561%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  2.220758/  2.242464, val:  56.25%, val_best:  60.00%, tr:  90.81%, tr_best:  90.81%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8990%\n",
      "layer   2  Sparsity: 82.1754%\n",
      "layer   3  Sparsity: 87.2040%\n",
      "total_backward_count 675510 real_backward_count 206168  30.520%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  2.225041/  2.243360, val:  54.17%, val_best:  60.00%, tr:  89.27%, tr_best:  90.81%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9680%\n",
      "layer   2  Sparsity: 82.2815%\n",
      "layer   3  Sparsity: 87.7353%\n",
      "total_backward_count 680405 real_backward_count 207471  30.492%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  2.227746/  2.242226, val:  62.08%, val_best:  62.08%, tr:  90.60%, tr_best:  90.81%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.8975%\n",
      "layer   2  Sparsity: 82.4122%\n",
      "layer   3  Sparsity: 87.8061%\n",
      "total_backward_count 685300 real_backward_count 208718  30.456%\n",
      "lif layer 1 self.abs_max_v: 15890.5\n",
      "lif layer 1 self.abs_max_v: 15895.5\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  2.224945/  2.244150, val:  47.92%, val_best:  62.08%, tr:  90.19%, tr_best:  90.81%, epoch time: 39.83 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9753%\n",
      "layer   2  Sparsity: 82.5148%\n",
      "layer   3  Sparsity: 87.5935%\n",
      "total_backward_count 690195 real_backward_count 210005  30.427%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  2.225917/  2.241325, val:  53.33%, val_best:  62.08%, tr:  88.97%, tr_best:  90.81%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9263%\n",
      "layer   2  Sparsity: 82.3167%\n",
      "layer   3  Sparsity: 87.8106%\n",
      "total_backward_count 695090 real_backward_count 211291  30.398%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  2.228134/  2.244216, val:  44.17%, val_best:  62.08%, tr:  89.07%, tr_best:  90.81%, epoch time: 40.83 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9411%\n",
      "layer   2  Sparsity: 82.6166%\n",
      "layer   3  Sparsity: 87.7758%\n",
      "total_backward_count 699985 real_backward_count 212586  30.370%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  2.225192/  2.241744, val:  57.08%, val_best:  62.08%, tr:  88.97%, tr_best:  90.81%, epoch time: 41.18 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 87.9100%\n",
      "layer   2  Sparsity: 82.3053%\n",
      "layer   3  Sparsity: 87.6409%\n",
      "total_backward_count 704880 real_backward_count 213913  30.347%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  2.226375/  2.242780, val:  53.75%, val_best:  62.08%, tr:  90.91%, tr_best:  90.91%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9388%\n",
      "layer   2  Sparsity: 82.1085%\n",
      "layer   3  Sparsity: 87.2966%\n",
      "total_backward_count 709775 real_backward_count 215153  30.313%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  2.222117/  2.238566, val:  51.25%, val_best:  62.08%, tr:  91.11%, tr_best:  91.11%, epoch time: 40.07 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9666%\n",
      "layer   2  Sparsity: 81.9556%\n",
      "layer   3  Sparsity: 87.2173%\n",
      "total_backward_count 714670 real_backward_count 216349  30.273%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  2.225287/  2.241308, val:  51.25%, val_best:  62.08%, tr:  89.07%, tr_best:  91.11%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9949%\n",
      "layer   2  Sparsity: 82.2016%\n",
      "layer   3  Sparsity: 87.4356%\n",
      "total_backward_count 719565 real_backward_count 217635  30.245%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  2.222873/  2.248527, val:  35.42%, val_best:  62.08%, tr:  89.99%, tr_best:  91.11%, epoch time: 41.44 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 87.9517%\n",
      "layer   2  Sparsity: 82.1007%\n",
      "layer   3  Sparsity: 87.1688%\n",
      "total_backward_count 724460 real_backward_count 218815  30.204%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  2.225954/  2.244729, val:  53.75%, val_best:  62.08%, tr:  90.60%, tr_best:  91.11%, epoch time: 41.48 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 87.8912%\n",
      "layer   2  Sparsity: 82.0090%\n",
      "layer   3  Sparsity: 87.1011%\n",
      "total_backward_count 729355 real_backward_count 220089  30.176%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  2.227697/  2.240930, val:  49.17%, val_best:  62.08%, tr:  88.97%, tr_best:  91.11%, epoch time: 41.51 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 87.9711%\n",
      "layer   2  Sparsity: 81.7231%\n",
      "layer   3  Sparsity: 87.3058%\n",
      "total_backward_count 734250 real_backward_count 221369  30.149%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  2.228248/  2.245492, val:  49.17%, val_best:  62.08%, tr:  89.17%, tr_best:  91.11%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8930%\n",
      "layer   2  Sparsity: 81.6101%\n",
      "layer   3  Sparsity: 87.3118%\n",
      "total_backward_count 739145 real_backward_count 222623  30.119%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  2.230213/  2.249143, val:  54.17%, val_best:  62.08%, tr:  89.07%, tr_best:  91.11%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9668%\n",
      "layer   2  Sparsity: 81.6895%\n",
      "layer   3  Sparsity: 87.5476%\n",
      "total_backward_count 744040 real_backward_count 223917  30.095%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  2.228882/  2.250663, val:  50.83%, val_best:  62.08%, tr:  89.99%, tr_best:  91.11%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9484%\n",
      "layer   2  Sparsity: 81.8610%\n",
      "layer   3  Sparsity: 87.8551%\n",
      "total_backward_count 748935 real_backward_count 225166  30.065%\n",
      "fc layer 1 self.abs_max_out: 16477.0\n",
      "lif layer 1 self.abs_max_v: 16477.0\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  2.227672/  2.243356, val:  51.25%, val_best:  62.08%, tr:  90.70%, tr_best:  91.11%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.0494%\n",
      "layer   2  Sparsity: 82.0178%\n",
      "layer   3  Sparsity: 87.5477%\n",
      "total_backward_count 753830 real_backward_count 226389  30.032%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  2.226503/  2.246476, val:  53.75%, val_best:  62.08%, tr:  89.79%, tr_best:  91.11%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9939%\n",
      "layer   2  Sparsity: 81.9427%\n",
      "layer   3  Sparsity: 87.5016%\n",
      "total_backward_count 758725 real_backward_count 227705  30.012%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  2.224579/  2.242398, val:  57.50%, val_best:  62.08%, tr:  89.27%, tr_best:  91.11%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9275%\n",
      "layer   2  Sparsity: 82.0275%\n",
      "layer   3  Sparsity: 87.2989%\n",
      "total_backward_count 763620 real_backward_count 228961  29.984%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  2.225855/  2.246798, val:  52.92%, val_best:  62.08%, tr:  87.84%, tr_best:  91.11%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9355%\n",
      "layer   2  Sparsity: 81.8137%\n",
      "layer   3  Sparsity: 87.2748%\n",
      "total_backward_count 768515 real_backward_count 230255  29.961%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  2.224273/  2.245397, val:  52.92%, val_best:  62.08%, tr:  90.91%, tr_best:  91.11%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9915%\n",
      "layer   2  Sparsity: 81.3958%\n",
      "layer   3  Sparsity: 87.0407%\n",
      "total_backward_count 773410 real_backward_count 231522  29.935%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  2.224255/  2.245303, val:  43.33%, val_best:  62.08%, tr:  89.89%, tr_best:  91.11%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9182%\n",
      "layer   2  Sparsity: 81.6413%\n",
      "layer   3  Sparsity: 87.1483%\n",
      "total_backward_count 778305 real_backward_count 232816  29.913%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  2.226400/  2.240123, val:  49.17%, val_best:  62.08%, tr:  89.07%, tr_best:  91.11%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.8669%\n",
      "layer   2  Sparsity: 81.9018%\n",
      "layer   3  Sparsity: 87.1193%\n",
      "total_backward_count 783200 real_backward_count 234109  29.891%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  2.223710/  2.236230, val:  55.83%, val_best:  62.08%, tr:  90.81%, tr_best:  91.11%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9596%\n",
      "layer   2  Sparsity: 81.9561%\n",
      "layer   3  Sparsity: 87.0430%\n",
      "total_backward_count 788095 real_backward_count 235367  29.865%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  2.220866/  2.238190, val:  46.25%, val_best:  62.08%, tr:  91.32%, tr_best:  91.32%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9545%\n",
      "layer   2  Sparsity: 81.7504%\n",
      "layer   3  Sparsity: 87.0480%\n",
      "total_backward_count 792990 real_backward_count 236636  29.841%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  2.223801/  2.245295, val:  46.67%, val_best:  62.08%, tr:  88.97%, tr_best:  91.32%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9359%\n",
      "layer   2  Sparsity: 81.8239%\n",
      "layer   3  Sparsity: 87.0291%\n",
      "total_backward_count 797885 real_backward_count 237902  29.817%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  2.224465/  2.247698, val:  47.50%, val_best:  62.08%, tr:  90.40%, tr_best:  91.32%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9343%\n",
      "layer   2  Sparsity: 81.6858%\n",
      "layer   3  Sparsity: 87.1524%\n",
      "total_backward_count 802780 real_backward_count 239095  29.783%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  2.226293/  2.243271, val:  44.58%, val_best:  62.08%, tr:  90.19%, tr_best:  91.32%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8875%\n",
      "layer   2  Sparsity: 81.4461%\n",
      "layer   3  Sparsity: 87.1073%\n",
      "total_backward_count 807675 real_backward_count 240372  29.761%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  2.227116/  2.242817, val:  58.33%, val_best:  62.08%, tr:  91.42%, tr_best:  91.42%, epoch time: 39.85 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9456%\n",
      "layer   2  Sparsity: 81.8473%\n",
      "layer   3  Sparsity: 87.5004%\n",
      "total_backward_count 812570 real_backward_count 241662  29.740%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  2.221057/  2.244450, val:  52.92%, val_best:  62.08%, tr:  91.11%, tr_best:  91.42%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.0488%\n",
      "layer   2  Sparsity: 81.7710%\n",
      "layer   3  Sparsity: 87.1611%\n",
      "total_backward_count 817465 real_backward_count 242883  29.712%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  2.223550/  2.244891, val:  50.83%, val_best:  62.08%, tr:  91.01%, tr_best:  91.42%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8785%\n",
      "layer   2  Sparsity: 81.5435%\n",
      "layer   3  Sparsity: 86.8773%\n",
      "total_backward_count 822360 real_backward_count 244150  29.689%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  2.224277/  2.238203, val:  51.67%, val_best:  62.08%, tr:  88.97%, tr_best:  91.42%, epoch time: 39.81 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9324%\n",
      "layer   2  Sparsity: 81.4679%\n",
      "layer   3  Sparsity: 86.6832%\n",
      "total_backward_count 827255 real_backward_count 245381  29.662%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  2.220305/  2.240933, val:  52.50%, val_best:  62.08%, tr:  90.40%, tr_best:  91.42%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9238%\n",
      "layer   2  Sparsity: 81.6363%\n",
      "layer   3  Sparsity: 86.8753%\n",
      "total_backward_count 832150 real_backward_count 246608  29.635%\n",
      "fc layer 1 self.abs_max_out: 16704.0\n",
      "lif layer 1 self.abs_max_v: 16704.0\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  2.223126/  2.241940, val:  49.17%, val_best:  62.08%, tr:  91.73%, tr_best:  91.73%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9959%\n",
      "layer   2  Sparsity: 81.4212%\n",
      "layer   3  Sparsity: 86.9666%\n",
      "total_backward_count 837045 real_backward_count 247824  29.607%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  2.218487/  2.240352, val:  51.67%, val_best:  62.08%, tr:  90.40%, tr_best:  91.73%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9647%\n",
      "layer   2  Sparsity: 81.5514%\n",
      "layer   3  Sparsity: 86.9480%\n",
      "total_backward_count 841940 real_backward_count 249079  29.584%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  2.223083/  2.243378, val:  52.50%, val_best:  62.08%, tr:  89.68%, tr_best:  91.73%, epoch time: 39.94 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9620%\n",
      "layer   2  Sparsity: 81.4683%\n",
      "layer   3  Sparsity: 86.6000%\n",
      "total_backward_count 846835 real_backward_count 250334  29.561%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  2.224845/  2.246722, val:  47.08%, val_best:  62.08%, tr:  89.07%, tr_best:  91.73%, epoch time: 40.96 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9668%\n",
      "layer   2  Sparsity: 81.8990%\n",
      "layer   3  Sparsity: 86.7364%\n",
      "total_backward_count 851730 real_backward_count 251517  29.530%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  2.228241/  2.245253, val:  54.58%, val_best:  62.08%, tr:  90.70%, tr_best:  91.73%, epoch time: 40.72 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9030%\n",
      "layer   2  Sparsity: 81.9692%\n",
      "layer   3  Sparsity: 87.2972%\n",
      "total_backward_count 856625 real_backward_count 252748  29.505%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  2.228051/  2.246630, val:  52.92%, val_best:  62.08%, tr:  89.89%, tr_best:  91.73%, epoch time: 40.70 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9077%\n",
      "layer   2  Sparsity: 81.8275%\n",
      "layer   3  Sparsity: 87.1674%\n",
      "total_backward_count 861520 real_backward_count 254017  29.485%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  2.225759/  2.241728, val:  57.92%, val_best:  62.08%, tr:  91.22%, tr_best:  91.73%, epoch time: 39.90 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.0019%\n",
      "layer   2  Sparsity: 81.8612%\n",
      "layer   3  Sparsity: 86.8809%\n",
      "total_backward_count 866415 real_backward_count 255232  29.458%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  2.226567/  2.243791, val:  51.67%, val_best:  62.08%, tr:  90.91%, tr_best:  91.73%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8740%\n",
      "layer   2  Sparsity: 81.7495%\n",
      "layer   3  Sparsity: 87.1762%\n",
      "total_backward_count 871310 real_backward_count 256476  29.436%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  2.226629/  2.244299, val:  50.42%, val_best:  62.08%, tr:  89.48%, tr_best:  91.73%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.0110%\n",
      "layer   2  Sparsity: 81.9188%\n",
      "layer   3  Sparsity: 87.7559%\n",
      "total_backward_count 876205 real_backward_count 257733  29.415%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  2.226405/  2.244011, val:  54.17%, val_best:  62.08%, tr:  90.91%, tr_best:  91.73%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9763%\n",
      "layer   2  Sparsity: 81.5522%\n",
      "layer   3  Sparsity: 87.1972%\n",
      "total_backward_count 881100 real_backward_count 258974  29.392%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  2.224854/  2.244490, val:  55.42%, val_best:  62.08%, tr:  89.99%, tr_best:  91.73%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8448%\n",
      "layer   2  Sparsity: 81.6063%\n",
      "layer   3  Sparsity: 87.0921%\n",
      "total_backward_count 885995 real_backward_count 260195  29.368%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  2.228069/  2.247566, val:  56.25%, val_best:  62.08%, tr:  91.32%, tr_best:  91.73%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9008%\n",
      "layer   2  Sparsity: 81.7155%\n",
      "layer   3  Sparsity: 87.2851%\n",
      "total_backward_count 890890 real_backward_count 261397  29.341%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  2.224649/  2.243046, val:  53.33%, val_best:  62.08%, tr:  90.70%, tr_best:  91.73%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8899%\n",
      "layer   2  Sparsity: 81.7144%\n",
      "layer   3  Sparsity: 87.0084%\n",
      "total_backward_count 895785 real_backward_count 262618  29.317%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  2.226426/  2.243430, val:  37.50%, val_best:  62.08%, tr:  90.70%, tr_best:  91.73%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8910%\n",
      "layer   2  Sparsity: 81.9385%\n",
      "layer   3  Sparsity: 87.0287%\n",
      "total_backward_count 900680 real_backward_count 263823  29.292%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  2.227463/  2.244503, val:  53.75%, val_best:  62.08%, tr:  89.48%, tr_best:  91.73%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.8574%\n",
      "layer   2  Sparsity: 81.9879%\n",
      "layer   3  Sparsity: 87.3016%\n",
      "total_backward_count 905575 real_backward_count 265100  29.274%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  2.224653/  2.238250, val:  55.83%, val_best:  62.08%, tr:  90.50%, tr_best:  91.73%, epoch time: 39.79 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9283%\n",
      "layer   2  Sparsity: 81.7868%\n",
      "layer   3  Sparsity: 87.1740%\n",
      "total_backward_count 910470 real_backward_count 266342  29.253%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  2.224106/  2.240178, val:  54.17%, val_best:  62.08%, tr:  89.58%, tr_best:  91.73%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9702%\n",
      "layer   2  Sparsity: 81.7006%\n",
      "layer   3  Sparsity: 86.9805%\n",
      "total_backward_count 915365 real_backward_count 267598  29.234%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  2.223895/  2.242664, val:  52.08%, val_best:  62.08%, tr:  90.09%, tr_best:  91.73%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.0054%\n",
      "layer   2  Sparsity: 81.5742%\n",
      "layer   3  Sparsity: 87.2103%\n",
      "total_backward_count 920260 real_backward_count 268861  29.216%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  2.224452/  2.242423, val:  53.33%, val_best:  62.08%, tr:  90.40%, tr_best:  91.73%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.9196%\n",
      "layer   2  Sparsity: 81.6057%\n",
      "layer   3  Sparsity: 87.2189%\n",
      "total_backward_count 925155 real_backward_count 270077  29.193%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  2.222346/  2.238519, val:  54.58%, val_best:  62.08%, tr:  91.73%, tr_best:  91.73%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9124%\n",
      "layer   2  Sparsity: 81.9647%\n",
      "layer   3  Sparsity: 87.0499%\n",
      "total_backward_count 930050 real_backward_count 271265  29.167%\n",
      "fc layer 1 self.abs_max_out: 16820.0\n",
      "lif layer 1 self.abs_max_v: 16820.0\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  2.223395/  2.240766, val:  58.75%, val_best:  62.08%, tr:  89.68%, tr_best:  91.73%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9435%\n",
      "layer   2  Sparsity: 82.1236%\n",
      "layer   3  Sparsity: 87.3609%\n",
      "total_backward_count 934945 real_backward_count 272519  29.148%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  2.223572/  2.240936, val:  47.08%, val_best:  62.08%, tr:  91.42%, tr_best:  91.73%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9654%\n",
      "layer   2  Sparsity: 81.9199%\n",
      "layer   3  Sparsity: 86.7822%\n",
      "total_backward_count 939840 real_backward_count 273739  29.126%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  2.220697/  2.241205, val:  48.75%, val_best:  62.08%, tr:  91.11%, tr_best:  91.73%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.9373%\n",
      "layer   2  Sparsity: 81.8628%\n",
      "layer   3  Sparsity: 86.6417%\n",
      "total_backward_count 944735 real_backward_count 275015  29.110%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  2.221589/  2.240196, val:  42.50%, val_best:  62.08%, tr:  91.11%, tr_best:  91.73%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.8542%\n",
      "layer   2  Sparsity: 81.9025%\n",
      "layer   3  Sparsity: 86.8171%\n",
      "total_backward_count 949630 real_backward_count 276244  29.090%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  2.223928/  2.240140, val:  45.83%, val_best:  62.08%, tr:  90.60%, tr_best:  91.73%, epoch time: 39.87 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 87.8880%\n",
      "layer   2  Sparsity: 81.9250%\n",
      "layer   3  Sparsity: 86.8571%\n",
      "total_backward_count 954525 real_backward_count 277469  29.069%\n",
      "fc layer 1 self.abs_max_out: 16932.0\n",
      "lif layer 1 self.abs_max_v: 16932.0\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  2.224073/  2.239357, val:  50.83%, val_best:  62.08%, tr:  90.19%, tr_best:  91.73%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9456%\n",
      "layer   2  Sparsity: 82.0201%\n",
      "layer   3  Sparsity: 87.1756%\n",
      "total_backward_count 959420 real_backward_count 278752  29.054%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  2.222587/  2.239848, val:  57.08%, val_best:  62.08%, tr:  91.01%, tr_best:  91.73%, epoch time: 40.39 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.8992%\n",
      "layer   2  Sparsity: 81.8043%\n",
      "layer   3  Sparsity: 86.9733%\n",
      "total_backward_count 964315 real_backward_count 279965  29.033%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  2.219890/  2.235733, val:  60.00%, val_best:  62.08%, tr:  90.40%, tr_best:  91.73%, epoch time: 41.38 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 87.8500%\n",
      "layer   2  Sparsity: 81.6494%\n",
      "layer   3  Sparsity: 87.2151%\n",
      "total_backward_count 969210 real_backward_count 281239  29.017%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  2.224134/  2.242410, val:  46.67%, val_best:  62.08%, tr:  89.89%, tr_best:  91.73%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 87.8990%\n",
      "layer   2  Sparsity: 81.5308%\n",
      "layer   3  Sparsity: 87.2275%\n",
      "total_backward_count 974105 real_backward_count 282500  29.001%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  2.224333/  2.238878, val:  60.42%, val_best:  62.08%, tr:  90.81%, tr_best:  91.73%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 87.9593%\n",
      "layer   2  Sparsity: 81.6923%\n",
      "layer   3  Sparsity: 87.5062%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c0b305dca94813b62fa6ae3944d0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÇ‚ñÜ‚ñÉ‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÇ‚ñÜ‚ñÉ‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.90807</td></tr><tr><td>tr_epoch_loss</td><td>2.22433</td></tr><tr><td>val_acc_best</td><td>0.62083</td></tr><tr><td>val_acc_now</td><td>0.60417</td></tr><tr><td>val_loss</td><td>2.23888</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-sweep-295</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v04h9w20' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v04h9w20</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251127_032901-v04h9w20/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a9fiz5b7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 75000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251127_054415-a9fiz5b7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a9fiz5b7' target=\"_blank\">feasible-sweep-302</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a9fiz5b7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a9fiz5b7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251127_054424_419', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 75000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d42a941a5ec61eeb71eeb0784ba56a39\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 91\n",
      "fc layer 1 self.abs_max_out: 1206.0\n",
      "lif layer 1 self.abs_max_v: 1206.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1772.0\n",
      "lif layer 2 self.abs_max_v: 1772.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 751.0\n",
      "lif layer 1 self.abs_max_v: 1401.5\n",
      "fc layer 2 self.abs_max_out: 2164.0\n",
      "lif layer 2 self.abs_max_v: 2884.5\n",
      "lif layer 1 self.abs_max_v: 1415.0\n",
      "lif layer 2 self.abs_max_v: 3094.5\n",
      "lif layer 1 self.abs_max_v: 1440.5\n",
      "lif layer 2 self.abs_max_v: 3192.5\n",
      "lif layer 1 self.abs_max_v: 1441.5\n",
      "lif layer 2 self.abs_max_v: 3376.0\n",
      "lif layer 1 self.abs_max_v: 1775.0\n",
      "fc layer 2 self.abs_max_out: 2262.0\n",
      "lif layer 2 self.abs_max_v: 3796.0\n",
      "smallest_now_T updated: 82\n",
      "fc layer 2 self.abs_max_out: 2850.0\n",
      "fc layer 1 self.abs_max_out: 1345.0\n",
      "fc layer 1 self.abs_max_out: 1404.0\n",
      "lif layer 1 self.abs_max_v: 2103.0\n",
      "fc layer 3 self.abs_max_out: 802.0\n",
      "lif layer 1 self.abs_max_v: 2133.5\n",
      "lif layer 1 self.abs_max_v: 2325.0\n",
      "fc layer 3 self.abs_max_out: 1053.0\n",
      "fc layer 1 self.abs_max_out: 1471.0\n",
      "fc layer 1 self.abs_max_out: 1521.0\n",
      "smallest_now_T updated: 61\n",
      "fc layer 1 self.abs_max_out: 1579.0\n",
      "lif layer 2 self.abs_max_v: 3946.5\n",
      "fc layer 1 self.abs_max_out: 1771.0\n",
      "fc layer 3 self.abs_max_out: 1113.0\n",
      "lif layer 1 self.abs_max_v: 2406.5\n",
      "lif layer 1 self.abs_max_v: 2665.5\n",
      "lif layer 1 self.abs_max_v: 2962.0\n",
      "lif layer 2 self.abs_max_v: 4064.5\n",
      "fc layer 1 self.abs_max_out: 1960.0\n",
      "lif layer 2 self.abs_max_v: 4173.5\n",
      "lif layer 2 self.abs_max_v: 4520.0\n",
      "lif layer 1 self.abs_max_v: 3024.0\n",
      "lif layer 1 self.abs_max_v: 3234.0\n",
      "fc layer 1 self.abs_max_out: 2017.0\n",
      "lif layer 1 self.abs_max_v: 3634.0\n",
      "fc layer 1 self.abs_max_out: 2067.0\n",
      "lif layer 2 self.abs_max_v: 4860.0\n",
      "smallest_now_T updated: 51\n",
      "fc layer 1 self.abs_max_out: 2168.0\n",
      "fc layer 1 self.abs_max_out: 2216.0\n",
      "fc layer 1 self.abs_max_out: 2331.0\n",
      "lif layer 1 self.abs_max_v: 3920.5\n",
      "fc layer 1 self.abs_max_out: 2431.0\n",
      "fc layer 2 self.abs_max_out: 2967.0\n",
      "smallest_now_T updated: 50\n",
      "fc layer 1 self.abs_max_out: 2546.0\n",
      "lif layer 1 self.abs_max_v: 4158.0\n",
      "lif layer 1 self.abs_max_v: 4217.0\n",
      "fc layer 1 self.abs_max_out: 2774.0\n",
      "fc layer 1 self.abs_max_out: 2796.0\n",
      "lif layer 1 self.abs_max_v: 4456.0\n",
      "lif layer 1 self.abs_max_v: 4463.5\n",
      "lif layer 1 self.abs_max_v: 4698.0\n",
      "fc layer 1 self.abs_max_out: 2820.0\n",
      "fc layer 1 self.abs_max_out: 3093.0\n",
      "lif layer 1 self.abs_max_v: 4814.0\n",
      "fc layer 3 self.abs_max_out: 1117.0\n",
      "fc layer 3 self.abs_max_out: 1174.0\n",
      "fc layer 2 self.abs_max_out: 3208.0\n",
      "fc layer 2 self.abs_max_out: 3286.0\n",
      "fc layer 1 self.abs_max_out: 3208.0\n",
      "fc layer 3 self.abs_max_out: 1219.0\n",
      "fc layer 3 self.abs_max_out: 1236.0\n",
      "fc layer 3 self.abs_max_out: 1313.0\n",
      "fc layer 3 self.abs_max_out: 1342.0\n",
      "lif layer 1 self.abs_max_v: 4972.5\n",
      "lif layer 1 self.abs_max_v: 4989.5\n",
      "fc layer 1 self.abs_max_out: 3452.0\n",
      "lif layer 1 self.abs_max_v: 5718.5\n",
      "lif layer 1 self.abs_max_v: 6004.5\n",
      "lif layer 1 self.abs_max_v: 6269.0\n",
      "fc layer 1 self.abs_max_out: 3638.0\n",
      "lif layer 2 self.abs_max_v: 4868.5\n",
      "fc layer 1 self.abs_max_out: 3802.0\n",
      "fc layer 1 self.abs_max_out: 3804.0\n",
      "lif layer 1 self.abs_max_v: 6577.0\n",
      "lif layer 1 self.abs_max_v: 6978.5\n",
      "lif layer 2 self.abs_max_v: 5119.5\n",
      "fc layer 1 self.abs_max_out: 4252.0\n",
      "lif layer 1 self.abs_max_v: 7006.5\n",
      "lif layer 1 self.abs_max_v: 7402.5\n",
      "lif layer 1 self.abs_max_v: 7594.5\n",
      "fc layer 1 self.abs_max_out: 4754.0\n",
      "lif layer 1 self.abs_max_v: 8551.5\n",
      "fc layer 3 self.abs_max_out: 1473.0\n",
      "smallest_now_T_val updated: 84\n",
      "smallest_now_T_val updated: 69\n",
      "smallest_now_T_val updated: 68\n",
      "smallest_now_T_val updated: 67\n",
      "smallest_now_T_val updated: 55\n",
      "smallest_now_T_val updated: 50\n",
      "fc layer 1 self.abs_max_out: 4986.0\n",
      "lif layer 1 self.abs_max_v: 8671.0\n",
      "lif layer 1 self.abs_max_v: 8969.5\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  1.911021/  1.956878, val:  52.92%, val_best:  52.92%, tr:  82.74%, tr_best:  82.74%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9409%\n",
      "layer   2  Sparsity: 72.1628%\n",
      "layer   3  Sparsity: 68.4539%\n",
      "total_backward_count 9790 real_backward_count 3254  33.238%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 5273.5\n",
      "fc layer 3 self.abs_max_out: 1501.0\n",
      "fc layer 1 self.abs_max_out: 5051.0\n",
      "lif layer 1 self.abs_max_v: 9117.0\n",
      "lif layer 2 self.abs_max_v: 5418.0\n",
      "fc layer 1 self.abs_max_out: 5328.0\n",
      "lif layer 1 self.abs_max_v: 9281.0\n",
      "lif layer 1 self.abs_max_v: 9310.5\n",
      "lif layer 1 self.abs_max_v: 9610.5\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.817611/  1.942748, val:  57.08%, val_best:  57.08%, tr:  93.56%, tr_best:  93.56%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9400%\n",
      "layer   2  Sparsity: 74.7794%\n",
      "layer   3  Sparsity: 69.2358%\n",
      "total_backward_count 19580 real_backward_count 5282  26.977%\n",
      "fc layer 3 self.abs_max_out: 1529.0\n",
      "lif layer 2 self.abs_max_v: 5443.0\n",
      "lif layer 2 self.abs_max_v: 5468.0\n",
      "lif layer 2 self.abs_max_v: 5506.5\n",
      "fc layer 1 self.abs_max_out: 5364.0\n",
      "lif layer 1 self.abs_max_v: 9701.0\n",
      "lif layer 2 self.abs_max_v: 5515.0\n",
      "lif layer 2 self.abs_max_v: 5517.5\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.814231/  1.925036, val:  58.75%, val_best:  58.75%, tr:  97.45%, tr_best:  97.45%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9487%\n",
      "layer   2  Sparsity: 74.7127%\n",
      "layer   3  Sparsity: 70.2377%\n",
      "total_backward_count 29370 real_backward_count 6850  23.323%\n",
      "lif layer 2 self.abs_max_v: 5540.5\n",
      "lif layer 2 self.abs_max_v: 5543.5\n",
      "lif layer 2 self.abs_max_v: 5748.5\n",
      "fc layer 2 self.abs_max_out: 3298.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  1.816884/  1.950216, val:  51.67%, val_best:  58.75%, tr:  98.16%, tr_best:  98.16%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9524%\n",
      "layer   2  Sparsity: 74.8560%\n",
      "layer   3  Sparsity: 71.2543%\n",
      "total_backward_count 39160 real_backward_count 8248  21.062%\n",
      "lif layer 2 self.abs_max_v: 5883.0\n",
      "lif layer 2 self.abs_max_v: 5927.0\n",
      "fc layer 1 self.abs_max_out: 5525.0\n",
      "lif layer 1 self.abs_max_v: 9830.5\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  1.815674/  1.918832, val:  67.50%, val_best:  67.50%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9535%\n",
      "layer   2  Sparsity: 74.8715%\n",
      "layer   3  Sparsity: 71.3896%\n",
      "total_backward_count 48950 real_backward_count 9467  19.340%\n",
      "fc layer 1 self.abs_max_out: 5628.0\n",
      "fc layer 2 self.abs_max_out: 3310.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  1.811500/  1.921295, val:  72.50%, val_best:  72.50%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9400%\n",
      "layer   2  Sparsity: 74.4946%\n",
      "layer   3  Sparsity: 71.7498%\n",
      "total_backward_count 58740 real_backward_count 10559  17.976%\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  1.812070/  1.912256, val:  71.67%, val_best:  72.50%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9433%\n",
      "layer   2  Sparsity: 74.8845%\n",
      "layer   3  Sparsity: 72.3501%\n",
      "total_backward_count 68530 real_backward_count 11537  16.835%\n",
      "fc layer 2 self.abs_max_out: 3379.0\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  1.802055/  1.900314, val:  77.50%, val_best:  77.50%, tr:  99.80%, tr_best:  99.80%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9561%\n",
      "layer   2  Sparsity: 74.5132%\n",
      "layer   3  Sparsity: 72.1361%\n",
      "total_backward_count 78320 real_backward_count 12368  15.792%\n",
      "lif layer 1 self.abs_max_v: 9896.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  1.809433/  1.899735, val:  78.75%, val_best:  78.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9598%\n",
      "layer   2  Sparsity: 74.0814%\n",
      "layer   3  Sparsity: 72.5201%\n",
      "total_backward_count 88110 real_backward_count 13202  14.984%\n",
      "lif layer 1 self.abs_max_v: 9993.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  1.796030/  1.893046, val:  78.75%, val_best:  78.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9429%\n",
      "layer   2  Sparsity: 74.3039%\n",
      "layer   3  Sparsity: 72.2753%\n",
      "total_backward_count 97900 real_backward_count 13955  14.254%\n",
      "lif layer 2 self.abs_max_v: 6032.5\n",
      "fc layer 1 self.abs_max_out: 5708.0\n",
      "fc layer 2 self.abs_max_out: 3773.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  1.793546/  1.885127, val:  70.42%, val_best:  78.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.28 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 88.9449%\n",
      "layer   2  Sparsity: 74.1820%\n",
      "layer   3  Sparsity: 72.2743%\n",
      "total_backward_count 107690 real_backward_count 14632  13.587%\n",
      "fc layer 1 self.abs_max_out: 5723.0\n",
      "lif layer 1 self.abs_max_v: 10204.5\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  1.792291/  1.877701, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9493%\n",
      "layer   2  Sparsity: 74.2035%\n",
      "layer   3  Sparsity: 71.9904%\n",
      "total_backward_count 117480 real_backward_count 15310  13.032%\n",
      "fc layer 1 self.abs_max_out: 5796.0\n",
      "fc layer 1 self.abs_max_out: 5954.0\n",
      "lif layer 1 self.abs_max_v: 10221.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  1.782131/  1.883133, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9562%\n",
      "layer   2  Sparsity: 74.8013%\n",
      "layer   3  Sparsity: 72.5043%\n",
      "total_backward_count 127270 real_backward_count 15899  12.492%\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  1.778726/  1.887472, val:  75.83%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9558%\n",
      "layer   2  Sparsity: 74.4881%\n",
      "layer   3  Sparsity: 72.6003%\n",
      "total_backward_count 137060 real_backward_count 16413  11.975%\n",
      "lif layer 2 self.abs_max_v: 6186.0\n",
      "fc layer 1 self.abs_max_out: 5985.0\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  1.778618/  1.870069, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9212%\n",
      "layer   2  Sparsity: 73.7120%\n",
      "layer   3  Sparsity: 72.5730%\n",
      "total_backward_count 146850 real_backward_count 16909  11.514%\n",
      "fc layer 1 self.abs_max_out: 6111.0\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  1.776729/  1.882776, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9420%\n",
      "layer   2  Sparsity: 74.0475%\n",
      "layer   3  Sparsity: 72.1251%\n",
      "total_backward_count 156640 real_backward_count 17403  11.110%\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  1.775771/  1.865121, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9435%\n",
      "layer   2  Sparsity: 74.1237%\n",
      "layer   3  Sparsity: 72.1726%\n",
      "total_backward_count 166430 real_backward_count 17818  10.706%\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  1.770635/  1.869938, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9499%\n",
      "layer   2  Sparsity: 74.0655%\n",
      "layer   3  Sparsity: 72.3149%\n",
      "total_backward_count 176220 real_backward_count 18232  10.346%\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  1.775526/  1.853713, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9522%\n",
      "layer   2  Sparsity: 74.2320%\n",
      "layer   3  Sparsity: 72.5439%\n",
      "total_backward_count 186010 real_backward_count 18652  10.027%\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  1.754306/  1.841758, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9337%\n",
      "layer   2  Sparsity: 74.2598%\n",
      "layer   3  Sparsity: 72.5470%\n",
      "total_backward_count 195800 real_backward_count 19016   9.712%\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  1.749925/  1.853343, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9488%\n",
      "layer   2  Sparsity: 74.0981%\n",
      "layer   3  Sparsity: 73.0340%\n",
      "total_backward_count 205590 real_backward_count 19358   9.416%\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  1.747248/  1.849384, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9500%\n",
      "layer   2  Sparsity: 74.1578%\n",
      "layer   3  Sparsity: 73.2615%\n",
      "total_backward_count 215380 real_backward_count 19708   9.150%\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  1.747341/  1.843232, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9662%\n",
      "layer   2  Sparsity: 74.2183%\n",
      "layer   3  Sparsity: 73.3734%\n",
      "total_backward_count 225170 real_backward_count 20000   8.882%\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  1.749861/  1.846841, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9549%\n",
      "layer   2  Sparsity: 74.2550%\n",
      "layer   3  Sparsity: 73.0794%\n",
      "total_backward_count 234960 real_backward_count 20277   8.630%\n",
      "lif layer 1 self.abs_max_v: 10296.5\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  1.740499/  1.836225, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9407%\n",
      "layer   2  Sparsity: 74.0212%\n",
      "layer   3  Sparsity: 73.5041%\n",
      "total_backward_count 244750 real_backward_count 20568   8.404%\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  1.736446/  1.833581, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9514%\n",
      "layer   2  Sparsity: 73.9547%\n",
      "layer   3  Sparsity: 73.2605%\n",
      "total_backward_count 254540 real_backward_count 20863   8.196%\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  1.732289/  1.831183, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9382%\n",
      "layer   2  Sparsity: 73.5871%\n",
      "layer   3  Sparsity: 73.4449%\n",
      "total_backward_count 264330 real_backward_count 21108   7.985%\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  1.732606/  1.834669, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.34 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 88.9484%\n",
      "layer   2  Sparsity: 73.4484%\n",
      "layer   3  Sparsity: 73.4783%\n",
      "total_backward_count 274120 real_backward_count 21360   7.792%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  1.731009/  1.834564, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9423%\n",
      "layer   2  Sparsity: 73.9817%\n",
      "layer   3  Sparsity: 73.3843%\n",
      "total_backward_count 283910 real_backward_count 21621   7.615%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  1.726893/  1.829934, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9227%\n",
      "layer   2  Sparsity: 73.9853%\n",
      "layer   3  Sparsity: 73.5568%\n",
      "total_backward_count 293700 real_backward_count 21862   7.444%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  1.729296/  1.834094, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9475%\n",
      "layer   2  Sparsity: 73.4406%\n",
      "layer   3  Sparsity: 73.8377%\n",
      "total_backward_count 303490 real_backward_count 22140   7.295%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  1.726445/  1.823035, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9457%\n",
      "layer   2  Sparsity: 73.4403%\n",
      "layer   3  Sparsity: 74.0231%\n",
      "total_backward_count 313280 real_backward_count 22368   7.140%\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  1.717842/  1.825603, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9378%\n",
      "layer   2  Sparsity: 73.8132%\n",
      "layer   3  Sparsity: 74.1926%\n",
      "total_backward_count 323070 real_backward_count 22584   6.990%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  1.716705/  1.822719, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9527%\n",
      "layer   2  Sparsity: 74.0660%\n",
      "layer   3  Sparsity: 74.3331%\n",
      "total_backward_count 332860 real_backward_count 22801   6.850%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  1.715003/  1.825168, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9465%\n",
      "layer   2  Sparsity: 74.3169%\n",
      "layer   3  Sparsity: 74.5333%\n",
      "total_backward_count 342650 real_backward_count 22964   6.702%\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  1.721683/  1.831952, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9597%\n",
      "layer   2  Sparsity: 74.4259%\n",
      "layer   3  Sparsity: 74.6686%\n",
      "total_backward_count 352440 real_backward_count 23140   6.566%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  1.709657/  1.823390, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9459%\n",
      "layer   2  Sparsity: 74.7091%\n",
      "layer   3  Sparsity: 75.0992%\n",
      "total_backward_count 362230 real_backward_count 23310   6.435%\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  1.708401/  1.819091, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9494%\n",
      "layer   2  Sparsity: 74.5755%\n",
      "layer   3  Sparsity: 75.1080%\n",
      "total_backward_count 372020 real_backward_count 23492   6.315%\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  1.707479/  1.812878, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9462%\n",
      "layer   2  Sparsity: 74.3830%\n",
      "layer   3  Sparsity: 75.0253%\n",
      "total_backward_count 381810 real_backward_count 23652   6.195%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  1.708703/  1.815983, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9579%\n",
      "layer   2  Sparsity: 74.6868%\n",
      "layer   3  Sparsity: 75.2283%\n",
      "total_backward_count 391600 real_backward_count 23792   6.076%\n",
      "lif layer 1 self.abs_max_v: 10315.5\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  1.703286/  1.811359, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9456%\n",
      "layer   2  Sparsity: 74.7387%\n",
      "layer   3  Sparsity: 75.1496%\n",
      "total_backward_count 401390 real_backward_count 23947   5.966%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  1.702454/  1.815820, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9666%\n",
      "layer   2  Sparsity: 74.4932%\n",
      "layer   3  Sparsity: 74.9867%\n",
      "total_backward_count 411180 real_backward_count 24102   5.862%\n",
      "lif layer 1 self.abs_max_v: 10483.5\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  1.701365/  1.814546, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9385%\n",
      "layer   2  Sparsity: 74.4216%\n",
      "layer   3  Sparsity: 74.6790%\n",
      "total_backward_count 420970 real_backward_count 24229   5.756%\n",
      "lif layer 1 self.abs_max_v: 10533.0\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  1.700744/  1.815066, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9534%\n",
      "layer   2  Sparsity: 74.5507%\n",
      "layer   3  Sparsity: 74.9934%\n",
      "total_backward_count 430760 real_backward_count 24382   5.660%\n",
      "lif layer 1 self.abs_max_v: 10562.0\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  1.699205/  1.808429, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9437%\n",
      "layer   2  Sparsity: 74.5129%\n",
      "layer   3  Sparsity: 74.9205%\n",
      "total_backward_count 440550 real_backward_count 24534   5.569%\n",
      "fc layer 1 self.abs_max_out: 6157.0\n",
      "lif layer 1 self.abs_max_v: 10737.5\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  1.698422/  1.811432, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9311%\n",
      "layer   2  Sparsity: 74.5852%\n",
      "layer   3  Sparsity: 74.9290%\n",
      "total_backward_count 450340 real_backward_count 24668   5.478%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  1.698792/  1.813106, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9454%\n",
      "layer   2  Sparsity: 74.4972%\n",
      "layer   3  Sparsity: 75.2929%\n",
      "total_backward_count 460130 real_backward_count 24780   5.385%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  1.699056/  1.811674, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9468%\n",
      "layer   2  Sparsity: 74.5485%\n",
      "layer   3  Sparsity: 75.4678%\n",
      "total_backward_count 469920 real_backward_count 24888   5.296%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  1.694530/  1.811838, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9447%\n",
      "layer   2  Sparsity: 74.5605%\n",
      "layer   3  Sparsity: 75.5608%\n",
      "total_backward_count 479710 real_backward_count 25009   5.213%\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  1.698467/  1.815769, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.9642%\n",
      "layer   2  Sparsity: 74.5678%\n",
      "layer   3  Sparsity: 75.5287%\n",
      "total_backward_count 489500 real_backward_count 25131   5.134%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  1.693897/  1.811916, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9453%\n",
      "layer   2  Sparsity: 74.7261%\n",
      "layer   3  Sparsity: 75.4667%\n",
      "total_backward_count 499290 real_backward_count 25247   5.057%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  1.695178/  1.810272, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9571%\n",
      "layer   2  Sparsity: 74.5556%\n",
      "layer   3  Sparsity: 75.4379%\n",
      "total_backward_count 509080 real_backward_count 25363   4.982%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  1.693351/  1.813950, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9570%\n",
      "layer   2  Sparsity: 74.3574%\n",
      "layer   3  Sparsity: 75.5536%\n",
      "total_backward_count 518870 real_backward_count 25472   4.909%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  1.690081/  1.810742, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.9299%\n",
      "layer   2  Sparsity: 74.5225%\n",
      "layer   3  Sparsity: 75.3338%\n",
      "total_backward_count 528660 real_backward_count 25579   4.838%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  1.694398/  1.818814, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9611%\n",
      "layer   2  Sparsity: 74.6386%\n",
      "layer   3  Sparsity: 75.0572%\n",
      "total_backward_count 538450 real_backward_count 25699   4.773%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.686253/  1.802760, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9513%\n",
      "layer   2  Sparsity: 74.7467%\n",
      "layer   3  Sparsity: 75.0261%\n",
      "total_backward_count 548240 real_backward_count 25800   4.706%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.686528/  1.801997, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9323%\n",
      "layer   2  Sparsity: 74.7059%\n",
      "layer   3  Sparsity: 75.4050%\n",
      "total_backward_count 558030 real_backward_count 25894   4.640%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.688866/  1.807081, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9444%\n",
      "layer   2  Sparsity: 74.6792%\n",
      "layer   3  Sparsity: 75.5795%\n",
      "total_backward_count 567820 real_backward_count 25989   4.577%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.687461/  1.803709, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9408%\n",
      "layer   2  Sparsity: 74.5757%\n",
      "layer   3  Sparsity: 75.3348%\n",
      "total_backward_count 577610 real_backward_count 26084   4.516%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.684039/  1.803206, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9727%\n",
      "layer   2  Sparsity: 74.5527%\n",
      "layer   3  Sparsity: 75.2389%\n",
      "total_backward_count 587400 real_backward_count 26195   4.459%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.685385/  1.802712, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9472%\n",
      "layer   2  Sparsity: 74.5347%\n",
      "layer   3  Sparsity: 75.0760%\n",
      "total_backward_count 597190 real_backward_count 26293   4.403%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.681761/  1.804303, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9314%\n",
      "layer   2  Sparsity: 74.5941%\n",
      "layer   3  Sparsity: 75.1685%\n",
      "total_backward_count 606980 real_backward_count 26397   4.349%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.679191/  1.792439, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9584%\n",
      "layer   2  Sparsity: 74.8379%\n",
      "layer   3  Sparsity: 75.0742%\n",
      "total_backward_count 616770 real_backward_count 26484   4.294%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.676395/  1.790667, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9417%\n",
      "layer   2  Sparsity: 74.9720%\n",
      "layer   3  Sparsity: 75.3525%\n",
      "total_backward_count 626560 real_backward_count 26550   4.237%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.673619/  1.791659, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9432%\n",
      "layer   2  Sparsity: 75.0696%\n",
      "layer   3  Sparsity: 75.4246%\n",
      "total_backward_count 636350 real_backward_count 26634   4.185%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.674685/  1.793183, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9429%\n",
      "layer   2  Sparsity: 74.8529%\n",
      "layer   3  Sparsity: 75.3151%\n",
      "total_backward_count 646140 real_backward_count 26699   4.132%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.674907/  1.795568, val:  79.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9329%\n",
      "layer   2  Sparsity: 74.7742%\n",
      "layer   3  Sparsity: 75.2984%\n",
      "total_backward_count 655930 real_backward_count 26781   4.083%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.674501/  1.797381, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9400%\n",
      "layer   2  Sparsity: 74.7350%\n",
      "layer   3  Sparsity: 75.5454%\n",
      "total_backward_count 665720 real_backward_count 26859   4.035%\n",
      "fc layer 1 self.abs_max_out: 6185.0\n",
      "lif layer 1 self.abs_max_v: 10781.5\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.675608/  1.797797, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9486%\n",
      "layer   2  Sparsity: 74.7582%\n",
      "layer   3  Sparsity: 75.4372%\n",
      "total_backward_count 675510 real_backward_count 26930   3.987%\n",
      "fc layer 1 self.abs_max_out: 6194.0\n",
      "lif layer 1 self.abs_max_v: 10853.0\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.670933/  1.791211, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9439%\n",
      "layer   2  Sparsity: 74.8772%\n",
      "layer   3  Sparsity: 75.6646%\n",
      "total_backward_count 685300 real_backward_count 27000   3.940%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.669580/  1.794747, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9554%\n",
      "layer   2  Sparsity: 74.9674%\n",
      "layer   3  Sparsity: 75.8756%\n",
      "total_backward_count 695090 real_backward_count 27064   3.894%\n",
      "fc layer 1 self.abs_max_out: 6199.0\n",
      "lif layer 1 self.abs_max_v: 10856.5\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.676070/  1.797527, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9529%\n",
      "layer   2  Sparsity: 74.7643%\n",
      "layer   3  Sparsity: 75.9167%\n",
      "total_backward_count 704880 real_backward_count 27145   3.851%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.674332/  1.798039, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9447%\n",
      "layer   2  Sparsity: 74.9390%\n",
      "layer   3  Sparsity: 76.1754%\n",
      "total_backward_count 714670 real_backward_count 27227   3.810%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.674275/  1.796524, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9533%\n",
      "layer   2  Sparsity: 74.8223%\n",
      "layer   3  Sparsity: 76.1334%\n",
      "total_backward_count 724460 real_backward_count 27285   3.766%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.674486/  1.800766, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9508%\n",
      "layer   2  Sparsity: 74.9284%\n",
      "layer   3  Sparsity: 76.0923%\n",
      "total_backward_count 734250 real_backward_count 27360   3.726%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.672699/  1.794635, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9450%\n",
      "layer   2  Sparsity: 74.8500%\n",
      "layer   3  Sparsity: 75.9600%\n",
      "total_backward_count 744040 real_backward_count 27430   3.687%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.675012/  1.797642, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9455%\n",
      "layer   2  Sparsity: 74.7025%\n",
      "layer   3  Sparsity: 76.0849%\n",
      "total_backward_count 753830 real_backward_count 27508   3.649%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.673806/  1.796107, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9421%\n",
      "layer   2  Sparsity: 74.5264%\n",
      "layer   3  Sparsity: 75.9236%\n",
      "total_backward_count 763620 real_backward_count 27581   3.612%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.674332/  1.793330, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9545%\n",
      "layer   2  Sparsity: 74.5121%\n",
      "layer   3  Sparsity: 75.8114%\n",
      "total_backward_count 773410 real_backward_count 27636   3.573%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.672989/  1.788456, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9378%\n",
      "layer   2  Sparsity: 74.3551%\n",
      "layer   3  Sparsity: 75.4709%\n",
      "total_backward_count 783200 real_backward_count 27698   3.537%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.671494/  1.789684, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9349%\n",
      "layer   2  Sparsity: 74.3675%\n",
      "layer   3  Sparsity: 75.4851%\n",
      "total_backward_count 792990 real_backward_count 27763   3.501%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.665111/  1.784323, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9508%\n",
      "layer   2  Sparsity: 74.4306%\n",
      "layer   3  Sparsity: 75.5991%\n",
      "total_backward_count 802780 real_backward_count 27820   3.465%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.665168/  1.786140, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9437%\n",
      "layer   2  Sparsity: 74.5068%\n",
      "layer   3  Sparsity: 75.6345%\n",
      "total_backward_count 812570 real_backward_count 27883   3.431%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.665196/  1.785458, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9570%\n",
      "layer   2  Sparsity: 74.6283%\n",
      "layer   3  Sparsity: 75.8840%\n",
      "total_backward_count 822360 real_backward_count 27924   3.396%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.664933/  1.788346, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9468%\n",
      "layer   2  Sparsity: 74.6141%\n",
      "layer   3  Sparsity: 75.9438%\n",
      "total_backward_count 832150 real_backward_count 27965   3.361%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.663866/  1.783270, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9426%\n",
      "layer   2  Sparsity: 74.5244%\n",
      "layer   3  Sparsity: 75.9235%\n",
      "total_backward_count 841940 real_backward_count 28010   3.327%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.663081/  1.779451, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9425%\n",
      "layer   2  Sparsity: 74.5068%\n",
      "layer   3  Sparsity: 75.9527%\n",
      "total_backward_count 851730 real_backward_count 28068   3.295%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.660881/  1.783171, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9540%\n",
      "layer   2  Sparsity: 74.5344%\n",
      "layer   3  Sparsity: 75.7315%\n",
      "total_backward_count 861520 real_backward_count 28117   3.264%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.660475/  1.781359, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9503%\n",
      "layer   2  Sparsity: 74.6575%\n",
      "layer   3  Sparsity: 75.6964%\n",
      "total_backward_count 871310 real_backward_count 28169   3.233%\n",
      "lif layer 1 self.abs_max_v: 10990.5\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.660529/  1.777679, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9622%\n",
      "layer   2  Sparsity: 74.6575%\n",
      "layer   3  Sparsity: 75.5564%\n",
      "total_backward_count 881100 real_backward_count 28233   3.204%\n",
      "fc layer 1 self.abs_max_out: 6247.0\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.660308/  1.781416, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9559%\n",
      "layer   2  Sparsity: 74.5393%\n",
      "layer   3  Sparsity: 75.5084%\n",
      "total_backward_count 890890 real_backward_count 28289   3.175%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.662447/  1.780246, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9521%\n",
      "layer   2  Sparsity: 74.5155%\n",
      "layer   3  Sparsity: 75.5450%\n",
      "total_backward_count 900680 real_backward_count 28336   3.146%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.662610/  1.783850, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9478%\n",
      "layer   2  Sparsity: 74.6156%\n",
      "layer   3  Sparsity: 75.6456%\n",
      "total_backward_count 910470 real_backward_count 28390   3.118%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.662697/  1.782206, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9482%\n",
      "layer   2  Sparsity: 74.5604%\n",
      "layer   3  Sparsity: 75.7132%\n",
      "total_backward_count 920260 real_backward_count 28452   3.092%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.662723/  1.782868, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9607%\n",
      "layer   2  Sparsity: 74.5918%\n",
      "layer   3  Sparsity: 75.5891%\n",
      "total_backward_count 930050 real_backward_count 28513   3.066%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.662695/  1.775247, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9482%\n",
      "layer   2  Sparsity: 74.6161%\n",
      "layer   3  Sparsity: 75.5803%\n",
      "total_backward_count 939840 real_backward_count 28563   3.039%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.663771/  1.775028, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9615%\n",
      "layer   2  Sparsity: 74.7056%\n",
      "layer   3  Sparsity: 75.6431%\n",
      "total_backward_count 949630 real_backward_count 28626   3.014%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.657950/  1.775524, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9430%\n",
      "layer   2  Sparsity: 74.8796%\n",
      "layer   3  Sparsity: 75.4901%\n",
      "total_backward_count 959420 real_backward_count 28689   2.990%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.655571/  1.779743, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9413%\n",
      "layer   2  Sparsity: 74.6768%\n",
      "layer   3  Sparsity: 75.5587%\n",
      "total_backward_count 969210 real_backward_count 28750   2.966%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.657673/  1.775004, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9513%\n",
      "layer   2  Sparsity: 74.6584%\n",
      "layer   3  Sparsity: 75.5641%\n",
      "total_backward_count 979000 real_backward_count 28800   2.942%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.655605/  1.785109, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9403%\n",
      "layer   2  Sparsity: 74.7438%\n",
      "layer   3  Sparsity: 75.6653%\n",
      "total_backward_count 988790 real_backward_count 28850   2.918%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.659955/  1.783882, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9417%\n",
      "layer   2  Sparsity: 74.6697%\n",
      "layer   3  Sparsity: 75.8153%\n",
      "total_backward_count 998580 real_backward_count 28904   2.895%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.656006/  1.778028, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9516%\n",
      "layer   2  Sparsity: 74.7397%\n",
      "layer   3  Sparsity: 75.6487%\n",
      "total_backward_count 1008370 real_backward_count 28951   2.871%\n",
      "lif layer 1 self.abs_max_v: 11021.0\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.656022/  1.779733, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9533%\n",
      "layer   2  Sparsity: 74.7162%\n",
      "layer   3  Sparsity: 75.4616%\n",
      "total_backward_count 1018160 real_backward_count 29008   2.849%\n",
      "fc layer 1 self.abs_max_out: 6294.0\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.660374/  1.781855, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9470%\n",
      "layer   2  Sparsity: 74.7612%\n",
      "layer   3  Sparsity: 75.5715%\n",
      "total_backward_count 1027950 real_backward_count 29051   2.826%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.653162/  1.779243, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9466%\n",
      "layer   2  Sparsity: 74.6922%\n",
      "layer   3  Sparsity: 75.5754%\n",
      "total_backward_count 1037740 real_backward_count 29102   2.804%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.654217/  1.778411, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9320%\n",
      "layer   2  Sparsity: 74.6956%\n",
      "layer   3  Sparsity: 75.4188%\n",
      "total_backward_count 1047530 real_backward_count 29148   2.783%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.655307/  1.780194, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9468%\n",
      "layer   2  Sparsity: 74.5681%\n",
      "layer   3  Sparsity: 75.4581%\n",
      "total_backward_count 1057320 real_backward_count 29201   2.762%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.655533/  1.784199, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9626%\n",
      "layer   2  Sparsity: 74.4942%\n",
      "layer   3  Sparsity: 75.6983%\n",
      "total_backward_count 1067110 real_backward_count 29247   2.741%\n",
      "fc layer 1 self.abs_max_out: 6317.0\n",
      "lif layer 1 self.abs_max_v: 11073.0\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.658362/  1.780788, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.9482%\n",
      "layer   2  Sparsity: 74.5062%\n",
      "layer   3  Sparsity: 75.8847%\n",
      "total_backward_count 1076900 real_backward_count 29310   2.722%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.656784/  1.784493, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9612%\n",
      "layer   2  Sparsity: 74.3957%\n",
      "layer   3  Sparsity: 75.7445%\n",
      "total_backward_count 1086690 real_backward_count 29370   2.703%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.658093/  1.786077, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9475%\n",
      "layer   2  Sparsity: 74.4119%\n",
      "layer   3  Sparsity: 75.6915%\n",
      "total_backward_count 1096480 real_backward_count 29408   2.682%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.659787/  1.785821, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9376%\n",
      "layer   2  Sparsity: 74.4614%\n",
      "layer   3  Sparsity: 75.7461%\n",
      "total_backward_count 1106270 real_backward_count 29467   2.664%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.658642/  1.786049, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9589%\n",
      "layer   2  Sparsity: 74.4946%\n",
      "layer   3  Sparsity: 75.6884%\n",
      "total_backward_count 1116060 real_backward_count 29524   2.645%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.657018/  1.780951, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9477%\n",
      "layer   2  Sparsity: 74.6106%\n",
      "layer   3  Sparsity: 75.7062%\n",
      "total_backward_count 1125850 real_backward_count 29581   2.627%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.653089/  1.774273, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9355%\n",
      "layer   2  Sparsity: 74.7385%\n",
      "layer   3  Sparsity: 75.7680%\n",
      "total_backward_count 1135640 real_backward_count 29628   2.609%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.649574/  1.777855, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9435%\n",
      "layer   2  Sparsity: 74.8482%\n",
      "layer   3  Sparsity: 75.8032%\n",
      "total_backward_count 1145430 real_backward_count 29672   2.590%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.651027/  1.776685, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9373%\n",
      "layer   2  Sparsity: 74.7218%\n",
      "layer   3  Sparsity: 75.7467%\n",
      "total_backward_count 1155220 real_backward_count 29713   2.572%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.652847/  1.780216, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9479%\n",
      "layer   2  Sparsity: 74.7284%\n",
      "layer   3  Sparsity: 75.6890%\n",
      "total_backward_count 1165010 real_backward_count 29773   2.556%\n",
      "lif layer 1 self.abs_max_v: 11121.5\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.650420/  1.778729, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9574%\n",
      "layer   2  Sparsity: 74.8542%\n",
      "layer   3  Sparsity: 75.6665%\n",
      "total_backward_count 1174800 real_backward_count 29812   2.538%\n",
      "fc layer 1 self.abs_max_out: 6335.0\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.652078/  1.781160, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9485%\n",
      "layer   2  Sparsity: 74.8715%\n",
      "layer   3  Sparsity: 75.6905%\n",
      "total_backward_count 1184590 real_backward_count 29862   2.521%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.651878/  1.779643, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9620%\n",
      "layer   2  Sparsity: 74.7026%\n",
      "layer   3  Sparsity: 75.7209%\n",
      "total_backward_count 1194380 real_backward_count 29906   2.504%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.651437/  1.784746, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9571%\n",
      "layer   2  Sparsity: 74.6747%\n",
      "layer   3  Sparsity: 75.7425%\n",
      "total_backward_count 1204170 real_backward_count 29956   2.488%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.649672/  1.780774, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9513%\n",
      "layer   2  Sparsity: 74.8078%\n",
      "layer   3  Sparsity: 75.8257%\n",
      "total_backward_count 1213960 real_backward_count 29992   2.471%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.649433/  1.779109, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9647%\n",
      "layer   2  Sparsity: 74.9622%\n",
      "layer   3  Sparsity: 75.6699%\n",
      "total_backward_count 1223750 real_backward_count 30039   2.455%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.645945/  1.771717, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9518%\n",
      "layer   2  Sparsity: 74.9845%\n",
      "layer   3  Sparsity: 75.5954%\n",
      "total_backward_count 1233540 real_backward_count 30092   2.439%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.646520/  1.773595, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9490%\n",
      "layer   2  Sparsity: 74.8967%\n",
      "layer   3  Sparsity: 75.7089%\n",
      "total_backward_count 1243330 real_backward_count 30136   2.424%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.646513/  1.774501, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9436%\n",
      "layer   2  Sparsity: 74.7969%\n",
      "layer   3  Sparsity: 75.8196%\n",
      "total_backward_count 1253120 real_backward_count 30186   2.409%\n",
      "lif layer 1 self.abs_max_v: 11122.5\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.643482/  1.777928, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9499%\n",
      "layer   2  Sparsity: 74.8781%\n",
      "layer   3  Sparsity: 75.8509%\n",
      "total_backward_count 1262910 real_backward_count 30225   2.393%\n",
      "fc layer 1 self.abs_max_out: 6336.0\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.644953/  1.773827, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9468%\n",
      "layer   2  Sparsity: 74.9323%\n",
      "layer   3  Sparsity: 75.8567%\n",
      "total_backward_count 1272700 real_backward_count 30276   2.379%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.640884/  1.769488, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9437%\n",
      "layer   2  Sparsity: 74.9283%\n",
      "layer   3  Sparsity: 75.7707%\n",
      "total_backward_count 1282490 real_backward_count 30323   2.364%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.638237/  1.772637, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9588%\n",
      "layer   2  Sparsity: 74.9087%\n",
      "layer   3  Sparsity: 75.7064%\n",
      "total_backward_count 1292280 real_backward_count 30366   2.350%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.637581/  1.769771, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9291%\n",
      "layer   2  Sparsity: 74.8243%\n",
      "layer   3  Sparsity: 75.6721%\n",
      "total_backward_count 1302070 real_backward_count 30414   2.336%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.639376/  1.770652, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9270%\n",
      "layer   2  Sparsity: 74.8149%\n",
      "layer   3  Sparsity: 75.7116%\n",
      "total_backward_count 1311860 real_backward_count 30459   2.322%\n",
      "fc layer 1 self.abs_max_out: 6350.0\n",
      "lif layer 1 self.abs_max_v: 11160.0\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.638743/  1.772526, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9509%\n",
      "layer   2  Sparsity: 74.7701%\n",
      "layer   3  Sparsity: 75.6568%\n",
      "total_backward_count 1321650 real_backward_count 30498   2.308%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.640525/  1.778021, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9523%\n",
      "layer   2  Sparsity: 74.8437%\n",
      "layer   3  Sparsity: 75.9200%\n",
      "total_backward_count 1331440 real_backward_count 30552   2.295%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.641103/  1.772949, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9443%\n",
      "layer   2  Sparsity: 74.7449%\n",
      "layer   3  Sparsity: 75.9032%\n",
      "total_backward_count 1341230 real_backward_count 30593   2.281%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.640756/  1.772212, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9398%\n",
      "layer   2  Sparsity: 74.6614%\n",
      "layer   3  Sparsity: 75.8468%\n",
      "total_backward_count 1351020 real_backward_count 30638   2.268%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.640416/  1.775435, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9423%\n",
      "layer   2  Sparsity: 74.6830%\n",
      "layer   3  Sparsity: 75.9657%\n",
      "total_backward_count 1360810 real_backward_count 30676   2.254%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.640707/  1.776666, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9494%\n",
      "layer   2  Sparsity: 74.7539%\n",
      "layer   3  Sparsity: 76.0509%\n",
      "total_backward_count 1370600 real_backward_count 30714   2.241%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.639227/  1.774854, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9485%\n",
      "layer   2  Sparsity: 74.7336%\n",
      "layer   3  Sparsity: 75.9668%\n",
      "total_backward_count 1380390 real_backward_count 30749   2.228%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.639108/  1.774918, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9442%\n",
      "layer   2  Sparsity: 74.7367%\n",
      "layer   3  Sparsity: 75.9326%\n",
      "total_backward_count 1390180 real_backward_count 30780   2.214%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.639383/  1.773758, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9486%\n",
      "layer   2  Sparsity: 74.7137%\n",
      "layer   3  Sparsity: 75.8744%\n",
      "total_backward_count 1399970 real_backward_count 30814   2.201%\n",
      "lif layer 1 self.abs_max_v: 11200.0\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.637113/  1.773990, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9615%\n",
      "layer   2  Sparsity: 74.7992%\n",
      "layer   3  Sparsity: 75.7920%\n",
      "total_backward_count 1409760 real_backward_count 30850   2.188%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.638330/  1.772979, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9480%\n",
      "layer   2  Sparsity: 74.8941%\n",
      "layer   3  Sparsity: 75.7759%\n",
      "total_backward_count 1419550 real_backward_count 30892   2.176%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.637407/  1.768634, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9520%\n",
      "layer   2  Sparsity: 74.8100%\n",
      "layer   3  Sparsity: 75.7244%\n",
      "total_backward_count 1429340 real_backward_count 30930   2.164%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.634569/  1.766661, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9475%\n",
      "layer   2  Sparsity: 74.7580%\n",
      "layer   3  Sparsity: 75.6561%\n",
      "total_backward_count 1439130 real_backward_count 30978   2.153%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.634694/  1.763502, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9416%\n",
      "layer   2  Sparsity: 74.7514%\n",
      "layer   3  Sparsity: 75.7115%\n",
      "total_backward_count 1448920 real_backward_count 31021   2.141%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.634393/  1.764113, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9514%\n",
      "layer   2  Sparsity: 74.7707%\n",
      "layer   3  Sparsity: 75.7374%\n",
      "total_backward_count 1458710 real_backward_count 31057   2.129%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.633542/  1.764193, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9463%\n",
      "layer   2  Sparsity: 74.7513%\n",
      "layer   3  Sparsity: 75.7561%\n",
      "total_backward_count 1468500 real_backward_count 31096   2.118%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.636812/  1.765638, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9498%\n",
      "layer   2  Sparsity: 74.6742%\n",
      "layer   3  Sparsity: 75.8127%\n",
      "total_backward_count 1478290 real_backward_count 31135   2.106%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.635565/  1.764953, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9372%\n",
      "layer   2  Sparsity: 74.6368%\n",
      "layer   3  Sparsity: 75.7609%\n",
      "total_backward_count 1488080 real_backward_count 31174   2.095%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.637174/  1.766294, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9472%\n",
      "layer   2  Sparsity: 74.6185%\n",
      "layer   3  Sparsity: 75.9536%\n",
      "total_backward_count 1497870 real_backward_count 31212   2.084%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.634810/  1.766020, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9577%\n",
      "layer   2  Sparsity: 74.8850%\n",
      "layer   3  Sparsity: 76.0411%\n",
      "total_backward_count 1507660 real_backward_count 31253   2.073%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.634370/  1.765770, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9576%\n",
      "layer   2  Sparsity: 74.9486%\n",
      "layer   3  Sparsity: 76.1139%\n",
      "total_backward_count 1517450 real_backward_count 31290   2.062%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.632953/  1.765704, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9377%\n",
      "layer   2  Sparsity: 74.9534%\n",
      "layer   3  Sparsity: 76.2398%\n",
      "total_backward_count 1527240 real_backward_count 31325   2.051%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.634819/  1.765894, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9346%\n",
      "layer   2  Sparsity: 74.9276%\n",
      "layer   3  Sparsity: 76.1592%\n",
      "total_backward_count 1537030 real_backward_count 31364   2.041%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.633390/  1.766299, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9463%\n",
      "layer   2  Sparsity: 74.9652%\n",
      "layer   3  Sparsity: 76.1438%\n",
      "total_backward_count 1546820 real_backward_count 31395   2.030%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.634062/  1.763017, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9339%\n",
      "layer   2  Sparsity: 74.9934%\n",
      "layer   3  Sparsity: 76.1322%\n",
      "total_backward_count 1556610 real_backward_count 31430   2.019%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.632818/  1.764587, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9433%\n",
      "layer   2  Sparsity: 74.9752%\n",
      "layer   3  Sparsity: 75.9724%\n",
      "total_backward_count 1566400 real_backward_count 31472   2.009%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.634755/  1.766296, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9566%\n",
      "layer   2  Sparsity: 74.9678%\n",
      "layer   3  Sparsity: 76.0694%\n",
      "total_backward_count 1576190 real_backward_count 31509   1.999%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.633173/  1.767132, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9552%\n",
      "layer   2  Sparsity: 74.9667%\n",
      "layer   3  Sparsity: 76.2290%\n",
      "total_backward_count 1585980 real_backward_count 31547   1.989%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.631790/  1.764920, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9515%\n",
      "layer   2  Sparsity: 74.9468%\n",
      "layer   3  Sparsity: 76.2780%\n",
      "total_backward_count 1595770 real_backward_count 31581   1.979%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.631654/  1.765279, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9413%\n",
      "layer   2  Sparsity: 75.0051%\n",
      "layer   3  Sparsity: 76.2596%\n",
      "total_backward_count 1605560 real_backward_count 31614   1.969%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.630430/  1.763271, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9443%\n",
      "layer   2  Sparsity: 75.0905%\n",
      "layer   3  Sparsity: 76.2186%\n",
      "total_backward_count 1615350 real_backward_count 31647   1.959%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.630894/  1.764217, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9466%\n",
      "layer   2  Sparsity: 75.0933%\n",
      "layer   3  Sparsity: 76.2281%\n",
      "total_backward_count 1625140 real_backward_count 31677   1.949%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.630142/  1.764028, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9496%\n",
      "layer   2  Sparsity: 75.0884%\n",
      "layer   3  Sparsity: 76.0623%\n",
      "total_backward_count 1634930 real_backward_count 31716   1.940%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.631377/  1.765534, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9549%\n",
      "layer   2  Sparsity: 75.0925%\n",
      "layer   3  Sparsity: 76.1079%\n",
      "total_backward_count 1644720 real_backward_count 31753   1.931%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.631429/  1.765124, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9413%\n",
      "layer   2  Sparsity: 75.0217%\n",
      "layer   3  Sparsity: 76.2004%\n",
      "total_backward_count 1654510 real_backward_count 31784   1.921%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.631498/  1.765124, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9450%\n",
      "layer   2  Sparsity: 75.0023%\n",
      "layer   3  Sparsity: 76.2361%\n",
      "total_backward_count 1664300 real_backward_count 31813   1.911%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.632222/  1.765464, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9497%\n",
      "layer   2  Sparsity: 75.0021%\n",
      "layer   3  Sparsity: 76.2437%\n",
      "total_backward_count 1674090 real_backward_count 31846   1.902%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.632670/  1.766271, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9544%\n",
      "layer   2  Sparsity: 74.9922%\n",
      "layer   3  Sparsity: 76.1654%\n",
      "total_backward_count 1683880 real_backward_count 31879   1.893%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.632798/  1.768478, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9577%\n",
      "layer   2  Sparsity: 74.9094%\n",
      "layer   3  Sparsity: 76.2176%\n",
      "total_backward_count 1693670 real_backward_count 31912   1.884%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.634652/  1.768253, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9419%\n",
      "layer   2  Sparsity: 74.8040%\n",
      "layer   3  Sparsity: 76.2271%\n",
      "total_backward_count 1703460 real_backward_count 31961   1.876%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.635038/  1.769629, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9444%\n",
      "layer   2  Sparsity: 74.8291%\n",
      "layer   3  Sparsity: 76.3445%\n",
      "total_backward_count 1713250 real_backward_count 31997   1.868%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.634206/  1.770129, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9545%\n",
      "layer   2  Sparsity: 74.7992%\n",
      "layer   3  Sparsity: 76.3117%\n",
      "total_backward_count 1723040 real_backward_count 32031   1.859%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.635696/  1.768905, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9280%\n",
      "layer   2  Sparsity: 74.8421%\n",
      "layer   3  Sparsity: 76.3006%\n",
      "total_backward_count 1732830 real_backward_count 32063   1.850%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.635583/  1.772298, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9516%\n",
      "layer   2  Sparsity: 74.7987%\n",
      "layer   3  Sparsity: 76.2079%\n",
      "total_backward_count 1742620 real_backward_count 32107   1.842%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.634797/  1.771213, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9548%\n",
      "layer   2  Sparsity: 74.8155%\n",
      "layer   3  Sparsity: 76.1361%\n",
      "total_backward_count 1752410 real_backward_count 32144   1.834%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.633578/  1.770128, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9414%\n",
      "layer   2  Sparsity: 74.9237%\n",
      "layer   3  Sparsity: 76.1721%\n",
      "total_backward_count 1762200 real_backward_count 32181   1.826%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.631544/  1.770219, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9429%\n",
      "layer   2  Sparsity: 74.9211%\n",
      "layer   3  Sparsity: 76.1387%\n",
      "total_backward_count 1771990 real_backward_count 32211   1.818%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.632747/  1.769600, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9508%\n",
      "layer   2  Sparsity: 74.8527%\n",
      "layer   3  Sparsity: 76.1417%\n",
      "total_backward_count 1781780 real_backward_count 32244   1.810%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.633136/  1.769516, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9393%\n",
      "layer   2  Sparsity: 74.8548%\n",
      "layer   3  Sparsity: 76.1195%\n",
      "total_backward_count 1791570 real_backward_count 32275   1.801%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.634190/  1.767443, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9519%\n",
      "layer   2  Sparsity: 74.8623%\n",
      "layer   3  Sparsity: 76.1084%\n",
      "total_backward_count 1801360 real_backward_count 32307   1.793%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.633384/  1.766327, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9523%\n",
      "layer   2  Sparsity: 74.8746%\n",
      "layer   3  Sparsity: 76.0688%\n",
      "total_backward_count 1811150 real_backward_count 32340   1.786%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.632809/  1.765465, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9423%\n",
      "layer   2  Sparsity: 74.9629%\n",
      "layer   3  Sparsity: 76.0296%\n",
      "total_backward_count 1820940 real_backward_count 32376   1.778%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.632668/  1.767385, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9404%\n",
      "layer   2  Sparsity: 74.9529%\n",
      "layer   3  Sparsity: 76.0539%\n",
      "total_backward_count 1830730 real_backward_count 32416   1.771%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.633128/  1.767477, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9609%\n",
      "layer   2  Sparsity: 74.9084%\n",
      "layer   3  Sparsity: 76.1044%\n",
      "total_backward_count 1840520 real_backward_count 32448   1.763%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.632717/  1.765901, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9590%\n",
      "layer   2  Sparsity: 74.8048%\n",
      "layer   3  Sparsity: 75.9539%\n",
      "total_backward_count 1850310 real_backward_count 32484   1.756%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.632645/  1.762463, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9372%\n",
      "layer   2  Sparsity: 74.8786%\n",
      "layer   3  Sparsity: 75.7753%\n",
      "total_backward_count 1860100 real_backward_count 32524   1.749%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.632789/  1.763546, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9730%\n",
      "layer   2  Sparsity: 74.8875%\n",
      "layer   3  Sparsity: 75.7763%\n",
      "total_backward_count 1869890 real_backward_count 32554   1.741%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.631582/  1.763789, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9502%\n",
      "layer   2  Sparsity: 74.8942%\n",
      "layer   3  Sparsity: 75.7290%\n",
      "total_backward_count 1879680 real_backward_count 32587   1.734%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.631767/  1.765354, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9388%\n",
      "layer   2  Sparsity: 74.9248%\n",
      "layer   3  Sparsity: 75.8662%\n",
      "total_backward_count 1889470 real_backward_count 32618   1.726%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.632488/  1.763221, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9328%\n",
      "layer   2  Sparsity: 74.9546%\n",
      "layer   3  Sparsity: 75.8989%\n",
      "total_backward_count 1899260 real_backward_count 32653   1.719%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.630491/  1.764323, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9527%\n",
      "layer   2  Sparsity: 74.9613%\n",
      "layer   3  Sparsity: 75.8211%\n",
      "total_backward_count 1909050 real_backward_count 32685   1.712%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.632906/  1.768141, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9572%\n",
      "layer   2  Sparsity: 74.9877%\n",
      "layer   3  Sparsity: 75.9583%\n",
      "total_backward_count 1918840 real_backward_count 32718   1.705%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.634259/  1.768141, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9508%\n",
      "layer   2  Sparsity: 74.9816%\n",
      "layer   3  Sparsity: 76.0595%\n",
      "total_backward_count 1928630 real_backward_count 32747   1.698%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.634116/  1.768141, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9415%\n",
      "layer   2  Sparsity: 74.9890%\n",
      "layer   3  Sparsity: 76.0695%\n",
      "total_backward_count 1938420 real_backward_count 32776   1.691%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.633444/  1.768141, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9576%\n",
      "layer   2  Sparsity: 74.9888%\n",
      "layer   3  Sparsity: 76.0610%\n",
      "total_backward_count 1948210 real_backward_count 32805   1.684%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.633511/  1.768141, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.9579%\n",
      "layer   2  Sparsity: 74.9715%\n",
      "layer   3  Sparsity: 76.0660%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839b73a142c949f9b785f25ba4e9b07b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.63351</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.85</td></tr><tr><td>val_loss</td><td>1.76814</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feasible-sweep-302</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a9fiz5b7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a9fiz5b7</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251127_054415-a9fiz5b7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tw5juqjo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251127_095915-tw5juqjo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tw5juqjo' target=\"_blank\">sweet-sweep-311</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tw5juqjo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tw5juqjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251127_095924_547', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 15, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = df820968b21bc2412e6634ebdd407347\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=15, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=15, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 67\n",
      "fc layer 1 self.abs_max_out: 1131.0\n",
      "lif layer 1 self.abs_max_v: 1131.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1703.0\n",
      "lif layer 2 self.abs_max_v: 1703.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 967.0\n",
      "fc layer 1 self.abs_max_out: 1207.0\n",
      "lif layer 1 self.abs_max_v: 1690.5\n",
      "fc layer 2 self.abs_max_out: 1760.0\n",
      "lif layer 2 self.abs_max_v: 2534.5\n",
      "lif layer 2 self.abs_max_v: 2555.0\n",
      "fc layer 1 self.abs_max_out: 1407.0\n",
      "lif layer 1 self.abs_max_v: 1851.5\n",
      "fc layer 2 self.abs_max_out: 2040.0\n",
      "lif layer 2 self.abs_max_v: 3317.5\n",
      "fc layer 1 self.abs_max_out: 1434.0\n",
      "smallest_now_T updated: 60\n",
      "fc layer 1 self.abs_max_out: 2080.0\n",
      "lif layer 1 self.abs_max_v: 2080.0\n",
      "lif layer 1 self.abs_max_v: 2259.0\n",
      "lif layer 1 self.abs_max_v: 2382.5\n",
      "smallest_now_T updated: 45\n",
      "fc layer 2 self.abs_max_out: 2101.0\n",
      "fc layer 2 self.abs_max_out: 2401.0\n",
      "lif layer 2 self.abs_max_v: 3721.5\n",
      "fc layer 2 self.abs_max_out: 2463.0\n",
      "lif layer 1 self.abs_max_v: 2504.0\n",
      "fc layer 1 self.abs_max_out: 2418.0\n",
      "fc layer 1 self.abs_max_out: 3066.0\n",
      "lif layer 1 self.abs_max_v: 3066.0\n",
      "lif layer 1 self.abs_max_v: 4177.0\n",
      "fc layer 1 self.abs_max_out: 3237.0\n",
      "lif layer 1 self.abs_max_v: 5325.5\n",
      "lif layer 1 self.abs_max_v: 5423.0\n",
      "fc layer 1 self.abs_max_out: 3535.0\n",
      "lif layer 1 self.abs_max_v: 5971.5\n",
      "fc layer 1 self.abs_max_out: 3770.0\n",
      "fc layer 1 self.abs_max_out: 4311.0\n",
      "fc layer 2 self.abs_max_out: 2538.0\n",
      "fc layer 3 self.abs_max_out: 1161.0\n",
      "fc layer 2 self.abs_max_out: 2670.0\n",
      "lif layer 2 self.abs_max_v: 4473.0\n",
      "lif layer 1 self.abs_max_v: 6044.0\n",
      "lif layer 1 self.abs_max_v: 6374.0\n",
      "smallest_now_T updated: 37\n",
      "fc layer 1 self.abs_max_out: 4694.0\n",
      "smallest_now_T updated: 34\n",
      "fc layer 2 self.abs_max_out: 2674.0\n",
      "fc layer 2 self.abs_max_out: 2735.0\n",
      "lif layer 1 self.abs_max_v: 6436.5\n",
      "fc layer 2 self.abs_max_out: 2778.0\n",
      "smallest_now_T updated: 30\n",
      "fc layer 2 self.abs_max_out: 2846.0\n",
      "lif layer 1 self.abs_max_v: 6554.0\n",
      "lif layer 1 self.abs_max_v: 6695.5\n",
      "fc layer 1 self.abs_max_out: 4724.0\n",
      "fc layer 1 self.abs_max_out: 4887.0\n",
      "fc layer 1 self.abs_max_out: 5137.0\n",
      "lif layer 1 self.abs_max_v: 6708.5\n",
      "fc layer 1 self.abs_max_out: 5167.0\n",
      "smallest_now_T updated: 26\n",
      "smallest_now_T updated: 25\n",
      "lif layer 1 self.abs_max_v: 6940.5\n",
      "lif layer 1 self.abs_max_v: 7474.5\n",
      "lif layer 1 self.abs_max_v: 7738.0\n",
      "fc layer 3 self.abs_max_out: 1293.0\n",
      "fc layer 3 self.abs_max_out: 1369.0\n",
      "fc layer 1 self.abs_max_out: 5174.0\n",
      "lif layer 1 self.abs_max_v: 7825.0\n",
      "fc layer 1 self.abs_max_out: 5269.0\n",
      "lif layer 1 self.abs_max_v: 8571.5\n",
      "lif layer 1 self.abs_max_v: 8679.0\n",
      "lif layer 2 self.abs_max_v: 4620.0\n",
      "lif layer 2 self.abs_max_v: 4754.5\n",
      "lif layer 2 self.abs_max_v: 4826.5\n",
      "fc layer 1 self.abs_max_out: 5749.0\n",
      "lif layer 2 self.abs_max_v: 4891.0\n",
      "lif layer 2 self.abs_max_v: 5111.5\n",
      "lif layer 2 self.abs_max_v: 5394.0\n",
      "fc layer 3 self.abs_max_out: 1465.0\n",
      "fc layer 3 self.abs_max_out: 1513.0\n",
      "fc layer 3 self.abs_max_out: 1560.0\n",
      "lif layer 1 self.abs_max_v: 8995.0\n",
      "fc layer 2 self.abs_max_out: 2853.0\n",
      "fc layer 2 self.abs_max_out: 2863.0\n",
      "lif layer 1 self.abs_max_v: 9900.5\n",
      "lif layer 1 self.abs_max_v: 9994.0\n",
      "fc layer 2 self.abs_max_out: 2885.0\n",
      "lif layer 1 self.abs_max_v: 10282.5\n",
      "fc layer 1 self.abs_max_out: 5978.0\n",
      "fc layer 1 self.abs_max_out: 5984.0\n",
      "lif layer 1 self.abs_max_v: 10751.0\n",
      "fc layer 2 self.abs_max_out: 2908.0\n",
      "fc layer 1 self.abs_max_out: 6389.0\n",
      "smallest_now_T_val updated: 62\n",
      "smallest_now_T_val updated: 51\n",
      "smallest_now_T_val updated: 50\n",
      "smallest_now_T_val updated: 49\n",
      "smallest_now_T_val updated: 40\n",
      "smallest_now_T_val updated: 25\n",
      "lif layer 1 self.abs_max_v: 10797.5\n",
      "fc layer 2 self.abs_max_out: 2998.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.930238/  2.016572, val:  37.92%, val_best:  37.92%, tr:  85.29%, tr_best:  85.29%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 84.7432%\n",
      "layer   2  Sparsity: 71.8576%\n",
      "layer   3  Sparsity: 74.0082%\n",
      "total_backward_count 4895 real_backward_count 1426  29.132%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 6846.0\n",
      "lif layer 1 self.abs_max_v: 11806.0\n",
      "fc layer 3 self.abs_max_out: 1621.0\n",
      "fc layer 3 self.abs_max_out: 1629.0\n",
      "fc layer 1 self.abs_max_out: 6882.0\n",
      "lif layer 1 self.abs_max_v: 12033.5\n",
      "lif layer 1 self.abs_max_v: 12143.0\n",
      "fc layer 1 self.abs_max_out: 7485.0\n",
      "lif layer 1 self.abs_max_v: 12596.5\n",
      "lif layer 1 self.abs_max_v: 12746.5\n",
      "lif layer 1 self.abs_max_v: 12757.0\n",
      "lif layer 1 self.abs_max_v: 13066.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.829973/  1.962917, val:  39.17%, val_best:  39.17%, tr:  92.03%, tr_best:  92.03%, epoch time: 39.64 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7595%\n",
      "layer   2  Sparsity: 76.0261%\n",
      "layer   3  Sparsity: 75.8488%\n",
      "total_backward_count 9790 real_backward_count 2523  25.771%\n",
      "lif layer 1 self.abs_max_v: 13072.5\n",
      "lif layer 1 self.abs_max_v: 13223.5\n",
      "fc layer 1 self.abs_max_out: 7840.0\n",
      "lif layer 1 self.abs_max_v: 14100.5\n",
      "fc layer 3 self.abs_max_out: 1678.0\n",
      "fc layer 1 self.abs_max_out: 7959.0\n",
      "fc layer 1 self.abs_max_out: 8278.0\n",
      "lif layer 1 self.abs_max_v: 14391.0\n",
      "lif layer 1 self.abs_max_v: 15096.5\n",
      "fc layer 3 self.abs_max_out: 1793.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.815906/  1.943611, val:  47.92%, val_best:  47.92%, tr:  92.95%, tr_best:  92.95%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7555%\n",
      "layer   2  Sparsity: 77.5321%\n",
      "layer   3  Sparsity: 76.5644%\n",
      "total_backward_count 14685 real_backward_count 3585  24.413%\n",
      "fc layer 3 self.abs_max_out: 1806.0\n",
      "fc layer 3 self.abs_max_out: 1915.0\n",
      "lif layer 1 self.abs_max_v: 15181.5\n",
      "fc layer 1 self.abs_max_out: 8448.0\n",
      "lif layer 1 self.abs_max_v: 15256.5\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.824124/  1.978072, val:  45.83%, val_best:  47.92%, tr:  92.75%, tr_best:  92.95%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7316%\n",
      "layer   2  Sparsity: 78.2890%\n",
      "layer   3  Sparsity: 77.6683%\n",
      "total_backward_count 19580 real_backward_count 4614  23.565%\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.787315/  1.897985, val:  58.33%, val_best:  58.33%, tr:  94.79%, tr_best:  94.79%, epoch time: 40.39 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7731%\n",
      "layer   2  Sparsity: 79.3006%\n",
      "layer   3  Sparsity: 77.7158%\n",
      "total_backward_count 24475 real_backward_count 5545  22.656%\n",
      "fc layer 2 self.abs_max_out: 3031.0\n",
      "fc layer 1 self.abs_max_out: 8474.0\n",
      "fc layer 1 self.abs_max_out: 8900.0\n",
      "lif layer 1 self.abs_max_v: 15337.0\n",
      "fc layer 2 self.abs_max_out: 3158.0\n",
      "lif layer 2 self.abs_max_v: 5503.5\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.782295/  1.941022, val:  52.50%, val_best:  58.33%, tr:  94.08%, tr_best:  94.79%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7679%\n",
      "layer   2  Sparsity: 79.4525%\n",
      "layer   3  Sparsity: 78.2607%\n",
      "total_backward_count 29370 real_backward_count 6503  22.142%\n",
      "lif layer 2 self.abs_max_v: 5577.0\n",
      "lif layer 2 self.abs_max_v: 5761.5\n",
      "lif layer 1 self.abs_max_v: 15692.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.804017/  1.939831, val:  55.42%, val_best:  58.33%, tr:  95.71%, tr_best:  95.71%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7841%\n",
      "layer   2  Sparsity: 78.6427%\n",
      "layer   3  Sparsity: 79.0992%\n",
      "total_backward_count 34265 real_backward_count 7406  21.614%\n",
      "fc layer 1 self.abs_max_out: 11181.0\n",
      "lif layer 1 self.abs_max_v: 16185.5\n",
      "lif layer 1 self.abs_max_v: 17183.0\n",
      "lif layer 1 self.abs_max_v: 18133.5\n",
      "lif layer 1 self.abs_max_v: 18326.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.798321/  1.876903, val:  57.92%, val_best:  58.33%, tr:  93.67%, tr_best:  95.71%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7393%\n",
      "layer   2  Sparsity: 79.2937%\n",
      "layer   3  Sparsity: 80.2482%\n",
      "total_backward_count 39160 real_backward_count 8357  21.341%\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.788148/  1.901816, val:  61.25%, val_best:  61.25%, tr:  94.48%, tr_best:  95.71%, epoch time: 40.98 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 84.7436%\n",
      "layer   2  Sparsity: 79.3857%\n",
      "layer   3  Sparsity: 80.9999%\n",
      "total_backward_count 44055 real_backward_count 9266  21.033%\n",
      "lif layer 1 self.abs_max_v: 18326.5\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.774815/  1.900349, val:  55.83%, val_best:  61.25%, tr:  94.79%, tr_best:  95.71%, epoch time: 39.87 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7654%\n",
      "layer   2  Sparsity: 80.0669%\n",
      "layer   3  Sparsity: 80.4318%\n",
      "total_backward_count 48950 real_backward_count 10147  20.729%\n",
      "lif layer 1 self.abs_max_v: 18575.5\n",
      "lif layer 1 self.abs_max_v: 18952.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.779421/  1.903797, val:  50.83%, val_best:  61.25%, tr:  94.89%, tr_best:  95.71%, epoch time: 40.39 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7524%\n",
      "layer   2  Sparsity: 80.4518%\n",
      "layer   3  Sparsity: 80.7891%\n",
      "total_backward_count 53845 real_backward_count 11003  20.435%\n",
      "lif layer 1 self.abs_max_v: 19090.5\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.771129/  1.869509, val:  64.17%, val_best:  64.17%, tr:  95.40%, tr_best:  95.71%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7786%\n",
      "layer   2  Sparsity: 81.7673%\n",
      "layer   3  Sparsity: 79.3332%\n",
      "total_backward_count 58740 real_backward_count 11777  20.049%\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.718810/  1.874949, val:  57.92%, val_best:  64.17%, tr:  96.02%, tr_best:  96.02%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7619%\n",
      "layer   2  Sparsity: 82.0043%\n",
      "layer   3  Sparsity: 78.3385%\n",
      "total_backward_count 63635 real_backward_count 12545  19.714%\n",
      "lif layer 1 self.abs_max_v: 19488.0\n",
      "lif layer 1 self.abs_max_v: 19495.0\n",
      "fc layer 1 self.abs_max_out: 11241.0\n",
      "lif layer 1 self.abs_max_v: 20271.5\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.736015/  1.848549, val:  52.92%, val_best:  64.17%, tr:  95.40%, tr_best:  96.02%, epoch time: 38.15 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 84.7406%\n",
      "layer   2  Sparsity: 82.0746%\n",
      "layer   3  Sparsity: 79.0631%\n",
      "total_backward_count 68530 real_backward_count 13354  19.486%\n",
      "fc layer 1 self.abs_max_out: 11266.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.718804/  1.824473, val:  61.25%, val_best:  64.17%, tr:  95.51%, tr_best:  96.02%, epoch time: 39.50 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7829%\n",
      "layer   2  Sparsity: 81.9870%\n",
      "layer   3  Sparsity: 80.2919%\n",
      "total_backward_count 73425 real_backward_count 14081  19.177%\n",
      "fc layer 1 self.abs_max_out: 11285.0\n",
      "fc layer 2 self.abs_max_out: 3167.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.694050/  1.817844, val:  61.67%, val_best:  64.17%, tr:  96.22%, tr_best:  96.22%, epoch time: 39.81 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7698%\n",
      "layer   2  Sparsity: 81.9598%\n",
      "layer   3  Sparsity: 78.6568%\n",
      "total_backward_count 78320 real_backward_count 14822  18.925%\n",
      "fc layer 1 self.abs_max_out: 11399.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.686726/  1.812435, val:  67.50%, val_best:  67.50%, tr:  96.02%, tr_best:  96.22%, epoch time: 40.85 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 84.7335%\n",
      "layer   2  Sparsity: 82.0852%\n",
      "layer   3  Sparsity: 79.5934%\n",
      "total_backward_count 83215 real_backward_count 15544  18.679%\n",
      "fc layer 1 self.abs_max_out: 11437.0\n",
      "fc layer 1 self.abs_max_out: 11447.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.706376/  1.838304, val:  58.33%, val_best:  67.50%, tr:  96.22%, tr_best:  96.22%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7410%\n",
      "layer   2  Sparsity: 82.3175%\n",
      "layer   3  Sparsity: 80.2301%\n",
      "total_backward_count 88110 real_backward_count 16290  18.488%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.697526/  1.827919, val:  51.25%, val_best:  67.50%, tr:  96.83%, tr_best:  96.83%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.8247%\n",
      "layer   2  Sparsity: 82.3527%\n",
      "layer   3  Sparsity: 78.6006%\n",
      "total_backward_count 93005 real_backward_count 17034  18.315%\n",
      "fc layer 1 self.abs_max_out: 11479.0\n",
      "fc layer 1 self.abs_max_out: 11682.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.690915/  1.859816, val:  59.58%, val_best:  67.50%, tr:  95.40%, tr_best:  96.83%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7654%\n",
      "layer   2  Sparsity: 82.0176%\n",
      "layer   3  Sparsity: 78.7705%\n",
      "total_backward_count 97900 real_backward_count 17725  18.105%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.698990/  1.849323, val:  60.83%, val_best:  67.50%, tr:  97.14%, tr_best:  97.14%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 84.7534%\n",
      "layer   2  Sparsity: 81.2603%\n",
      "layer   3  Sparsity: 78.5627%\n",
      "total_backward_count 102795 real_backward_count 18388  17.888%\n",
      "lif layer 1 self.abs_max_v: 20626.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.701903/  1.836009, val:  62.50%, val_best:  67.50%, tr:  96.42%, tr_best:  97.14%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7493%\n",
      "layer   2  Sparsity: 81.6463%\n",
      "layer   3  Sparsity: 78.1185%\n",
      "total_backward_count 107690 real_backward_count 19119  17.754%\n",
      "fc layer 2 self.abs_max_out: 3217.0\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.689708/  1.799336, val:  58.33%, val_best:  67.50%, tr:  97.45%, tr_best:  97.45%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7290%\n",
      "layer   2  Sparsity: 81.7028%\n",
      "layer   3  Sparsity: 79.4267%\n",
      "total_backward_count 112585 real_backward_count 19794  17.581%\n",
      "fc layer 1 self.abs_max_out: 11710.0\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.664130/  1.829721, val:  57.50%, val_best:  67.50%, tr:  95.91%, tr_best:  97.45%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7601%\n",
      "layer   2  Sparsity: 82.7481%\n",
      "layer   3  Sparsity: 79.9966%\n",
      "total_backward_count 117480 real_backward_count 20500  17.450%\n",
      "fc layer 1 self.abs_max_out: 12013.0\n",
      "lif layer 1 self.abs_max_v: 20680.0\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.648830/  1.774392, val:  65.42%, val_best:  67.50%, tr:  96.22%, tr_best:  97.45%, epoch time: 40.16 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7701%\n",
      "layer   2  Sparsity: 82.6970%\n",
      "layer   3  Sparsity: 79.5585%\n",
      "total_backward_count 122375 real_backward_count 21213  17.334%\n",
      "fc layer 2 self.abs_max_out: 3242.0\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.669341/  1.838882, val:  68.33%, val_best:  68.33%, tr:  97.24%, tr_best:  97.45%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7370%\n",
      "layer   2  Sparsity: 82.6847%\n",
      "layer   3  Sparsity: 80.3844%\n",
      "total_backward_count 127270 real_backward_count 21839  17.160%\n",
      "fc layer 1 self.abs_max_out: 12765.0\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.677748/  1.802364, val:  60.42%, val_best:  68.33%, tr:  97.14%, tr_best:  97.45%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7972%\n",
      "layer   2  Sparsity: 82.4059%\n",
      "layer   3  Sparsity: 79.8534%\n",
      "total_backward_count 132165 real_backward_count 22437  16.977%\n",
      "fc layer 2 self.abs_max_out: 3277.0\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.659639/  1.801388, val:  68.33%, val_best:  68.33%, tr:  95.91%, tr_best:  97.45%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7677%\n",
      "layer   2  Sparsity: 81.7514%\n",
      "layer   3  Sparsity: 79.9999%\n",
      "total_backward_count 137060 real_backward_count 23089  16.846%\n",
      "fc layer 2 self.abs_max_out: 3293.0\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.640966/  1.777941, val:  70.42%, val_best:  70.42%, tr:  95.71%, tr_best:  97.45%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7443%\n",
      "layer   2  Sparsity: 82.2575%\n",
      "layer   3  Sparsity: 79.2899%\n",
      "total_backward_count 141955 real_backward_count 23719  16.709%\n",
      "lif layer 2 self.abs_max_v: 5763.5\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.634336/  1.794849, val:  58.33%, val_best:  70.42%, tr:  96.94%, tr_best:  97.45%, epoch time: 40.94 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 84.7459%\n",
      "layer   2  Sparsity: 82.3382%\n",
      "layer   3  Sparsity: 79.1932%\n",
      "total_backward_count 146850 real_backward_count 24332  16.569%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.625341/  1.784219, val:  70.00%, val_best:  70.42%, tr:  97.65%, tr_best:  97.65%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7598%\n",
      "layer   2  Sparsity: 82.5658%\n",
      "layer   3  Sparsity: 78.5223%\n",
      "total_backward_count 151745 real_backward_count 24933  16.431%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.608575/  1.747082, val:  65.42%, val_best:  70.42%, tr:  98.16%, tr_best:  98.16%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 84.7412%\n",
      "layer   2  Sparsity: 82.1633%\n",
      "layer   3  Sparsity: 78.2975%\n",
      "total_backward_count 156640 real_backward_count 25532  16.300%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.631047/  1.777318, val:  69.17%, val_best:  70.42%, tr:  97.14%, tr_best:  98.16%, epoch time: 40.92 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 84.7757%\n",
      "layer   2  Sparsity: 81.9235%\n",
      "layer   3  Sparsity: 79.4221%\n",
      "total_backward_count 161535 real_backward_count 26100  16.157%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.584747/  1.738206, val:  64.58%, val_best:  70.42%, tr:  97.75%, tr_best:  98.16%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7685%\n",
      "layer   2  Sparsity: 82.1519%\n",
      "layer   3  Sparsity: 77.4667%\n",
      "total_backward_count 166430 real_backward_count 26681  16.031%\n",
      "fc layer 3 self.abs_max_out: 1920.0\n",
      "lif layer 2 self.abs_max_v: 5914.5\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.621430/  1.767045, val:  72.08%, val_best:  72.08%, tr:  96.32%, tr_best:  98.16%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7661%\n",
      "layer   2  Sparsity: 82.1577%\n",
      "layer   3  Sparsity: 79.3893%\n",
      "total_backward_count 171325 real_backward_count 27259  15.911%\n",
      "lif layer 2 self.abs_max_v: 6085.5\n",
      "fc layer 2 self.abs_max_out: 3304.0\n",
      "lif layer 2 self.abs_max_v: 6347.0\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.591322/  1.761642, val:  73.75%, val_best:  73.75%, tr:  96.83%, tr_best:  98.16%, epoch time: 39.82 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7201%\n",
      "layer   2  Sparsity: 81.8780%\n",
      "layer   3  Sparsity: 79.6184%\n",
      "total_backward_count 176220 real_backward_count 27822  15.788%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.593972/  1.732472, val:  70.83%, val_best:  73.75%, tr:  98.16%, tr_best:  98.16%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.8183%\n",
      "layer   2  Sparsity: 81.2486%\n",
      "layer   3  Sparsity: 80.2235%\n",
      "total_backward_count 181115 real_backward_count 28376  15.667%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.535970/  1.743264, val:  67.08%, val_best:  73.75%, tr:  98.37%, tr_best:  98.37%, epoch time: 39.79 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7219%\n",
      "layer   2  Sparsity: 81.4806%\n",
      "layer   3  Sparsity: 79.1747%\n",
      "total_backward_count 186010 real_backward_count 28890  15.531%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.557644/  1.728130, val:  66.25%, val_best:  73.75%, tr:  97.75%, tr_best:  98.37%, epoch time: 39.82 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7406%\n",
      "layer   2  Sparsity: 81.5365%\n",
      "layer   3  Sparsity: 78.5747%\n",
      "total_backward_count 190905 real_backward_count 29479  15.442%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.535087/  1.717937, val:  65.00%, val_best:  73.75%, tr:  97.45%, tr_best:  98.37%, epoch time: 39.46 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7436%\n",
      "layer   2  Sparsity: 81.7167%\n",
      "layer   3  Sparsity: 77.8336%\n",
      "total_backward_count 195800 real_backward_count 30029  15.337%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.541845/  1.702286, val:  70.83%, val_best:  73.75%, tr:  97.75%, tr_best:  98.37%, epoch time: 39.91 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7771%\n",
      "layer   2  Sparsity: 81.7094%\n",
      "layer   3  Sparsity: 79.6558%\n",
      "total_backward_count 200695 real_backward_count 30562  15.228%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.537787/  1.711739, val:  68.33%, val_best:  73.75%, tr:  97.65%, tr_best:  98.37%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7510%\n",
      "layer   2  Sparsity: 81.5148%\n",
      "layer   3  Sparsity: 79.2655%\n",
      "total_backward_count 205590 real_backward_count 31091  15.123%\n",
      "fc layer 2 self.abs_max_out: 3428.0\n",
      "lif layer 1 self.abs_max_v: 21585.5\n",
      "fc layer 1 self.abs_max_out: 13114.0\n",
      "fc layer 1 self.abs_max_out: 14083.0\n",
      "lif layer 1 self.abs_max_v: 21875.0\n",
      "fc layer 3 self.abs_max_out: 1961.0\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.514257/  1.671248, val:  70.00%, val_best:  73.75%, tr:  98.06%, tr_best:  98.37%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7924%\n",
      "layer   2  Sparsity: 81.8260%\n",
      "layer   3  Sparsity: 79.2246%\n",
      "total_backward_count 210485 real_backward_count 31607  15.016%\n",
      "fc layer 3 self.abs_max_out: 1964.0\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.533647/  1.692820, val:  72.92%, val_best:  73.75%, tr:  98.16%, tr_best:  98.37%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7559%\n",
      "layer   2  Sparsity: 82.0793%\n",
      "layer   3  Sparsity: 80.5633%\n",
      "total_backward_count 215380 real_backward_count 32091  14.900%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.533860/  1.647471, val:  75.42%, val_best:  75.42%, tr:  98.37%, tr_best:  98.37%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7522%\n",
      "layer   2  Sparsity: 82.1600%\n",
      "layer   3  Sparsity: 80.4206%\n",
      "total_backward_count 220275 real_backward_count 32577  14.789%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.510647/  1.661330, val:  71.67%, val_best:  75.42%, tr:  98.57%, tr_best:  98.57%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7915%\n",
      "layer   2  Sparsity: 82.4687%\n",
      "layer   3  Sparsity: 79.7065%\n",
      "total_backward_count 225170 real_backward_count 33102  14.701%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.524518/  1.689035, val:  67.92%, val_best:  75.42%, tr:  97.24%, tr_best:  98.57%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7693%\n",
      "layer   2  Sparsity: 82.4917%\n",
      "layer   3  Sparsity: 79.8543%\n",
      "total_backward_count 230065 real_backward_count 33611  14.609%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.530098/  1.674960, val:  72.08%, val_best:  75.42%, tr:  97.65%, tr_best:  98.57%, epoch time: 39.36 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7444%\n",
      "layer   2  Sparsity: 82.2153%\n",
      "layer   3  Sparsity: 80.2263%\n",
      "total_backward_count 234960 real_backward_count 34099  14.513%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.503541/  1.667036, val:  72.92%, val_best:  75.42%, tr:  98.47%, tr_best:  98.57%, epoch time: 39.90 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7747%\n",
      "layer   2  Sparsity: 82.4614%\n",
      "layer   3  Sparsity: 79.7440%\n",
      "total_backward_count 239855 real_backward_count 34556  14.407%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.496552/  1.663511, val:  68.33%, val_best:  75.42%, tr:  98.37%, tr_best:  98.57%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7493%\n",
      "layer   2  Sparsity: 82.3892%\n",
      "layer   3  Sparsity: 78.6675%\n",
      "total_backward_count 244750 real_backward_count 35038  14.316%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.509979/  1.664351, val:  59.17%, val_best:  75.42%, tr:  98.57%, tr_best:  98.57%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7612%\n",
      "layer   2  Sparsity: 81.7486%\n",
      "layer   3  Sparsity: 78.3258%\n",
      "total_backward_count 249645 real_backward_count 35531  14.233%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.492788/  1.682845, val:  72.50%, val_best:  75.42%, tr:  97.85%, tr_best:  98.57%, epoch time: 39.55 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7519%\n",
      "layer   2  Sparsity: 81.5808%\n",
      "layer   3  Sparsity: 77.9844%\n",
      "total_backward_count 254540 real_backward_count 36016  14.149%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.506756/  1.689300, val:  67.92%, val_best:  75.42%, tr:  97.65%, tr_best:  98.57%, epoch time: 40.13 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7902%\n",
      "layer   2  Sparsity: 81.6084%\n",
      "layer   3  Sparsity: 78.5237%\n",
      "total_backward_count 259435 real_backward_count 36554  14.090%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.506393/  1.682829, val:  67.50%, val_best:  75.42%, tr:  97.85%, tr_best:  98.57%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7578%\n",
      "layer   2  Sparsity: 81.7916%\n",
      "layer   3  Sparsity: 79.1060%\n",
      "total_backward_count 264330 real_backward_count 37024  14.007%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.503195/  1.674100, val:  70.42%, val_best:  75.42%, tr:  98.37%, tr_best:  98.57%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7485%\n",
      "layer   2  Sparsity: 81.9564%\n",
      "layer   3  Sparsity: 79.1318%\n",
      "total_backward_count 269225 real_backward_count 37503  13.930%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.494242/  1.649363, val:  75.42%, val_best:  75.42%, tr:  97.55%, tr_best:  98.57%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7491%\n",
      "layer   2  Sparsity: 81.9141%\n",
      "layer   3  Sparsity: 78.4947%\n",
      "total_backward_count 274120 real_backward_count 37991  13.859%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.512637/  1.717049, val:  62.08%, val_best:  75.42%, tr:  97.85%, tr_best:  98.57%, epoch time: 39.50 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7490%\n",
      "layer   2  Sparsity: 82.2656%\n",
      "layer   3  Sparsity: 80.5210%\n",
      "total_backward_count 279015 real_backward_count 38478  13.791%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.486356/  1.655339, val:  68.75%, val_best:  75.42%, tr:  97.75%, tr_best:  98.57%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7869%\n",
      "layer   2  Sparsity: 81.7463%\n",
      "layer   3  Sparsity: 79.2918%\n",
      "total_backward_count 283910 real_backward_count 38937  13.715%\n",
      "fc layer 3 self.abs_max_out: 2104.0\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.462299/  1.654454, val:  58.33%, val_best:  75.42%, tr:  97.85%, tr_best:  98.57%, epoch time: 39.75 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7639%\n",
      "layer   2  Sparsity: 81.1475%\n",
      "layer   3  Sparsity: 77.9432%\n",
      "total_backward_count 288805 real_backward_count 39404  13.644%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.468343/  1.642482, val:  60.42%, val_best:  75.42%, tr:  98.16%, tr_best:  98.57%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7414%\n",
      "layer   2  Sparsity: 81.3711%\n",
      "layer   3  Sparsity: 78.5719%\n",
      "total_backward_count 293700 real_backward_count 39841  13.565%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.465361/  1.642024, val:  68.75%, val_best:  75.42%, tr:  98.16%, tr_best:  98.57%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.8082%\n",
      "layer   2  Sparsity: 81.5828%\n",
      "layer   3  Sparsity: 78.4842%\n",
      "total_backward_count 298595 real_backward_count 40313  13.501%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.458001/  1.664508, val:  75.00%, val_best:  75.42%, tr:  97.14%, tr_best:  98.57%, epoch time: 39.84 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7393%\n",
      "layer   2  Sparsity: 81.8267%\n",
      "layer   3  Sparsity: 78.6759%\n",
      "total_backward_count 303490 real_backward_count 40780  13.437%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.460936/  1.647081, val:  71.67%, val_best:  75.42%, tr:  97.96%, tr_best:  98.57%, epoch time: 39.70 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7743%\n",
      "layer   2  Sparsity: 81.9178%\n",
      "layer   3  Sparsity: 78.8897%\n",
      "total_backward_count 308385 real_backward_count 41227  13.369%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.478615/  1.696400, val:  60.83%, val_best:  75.42%, tr:  98.57%, tr_best:  98.57%, epoch time: 39.61 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7436%\n",
      "layer   2  Sparsity: 82.2463%\n",
      "layer   3  Sparsity: 79.9422%\n",
      "total_backward_count 313280 real_backward_count 41661  13.298%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.465400/  1.644445, val:  72.92%, val_best:  75.42%, tr:  97.75%, tr_best:  98.57%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7753%\n",
      "layer   2  Sparsity: 81.8848%\n",
      "layer   3  Sparsity: 79.8848%\n",
      "total_backward_count 318175 real_backward_count 42051  13.216%\n",
      "fc layer 1 self.abs_max_out: 14517.0\n",
      "lif layer 1 self.abs_max_v: 22297.0\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.452302/  1.652409, val:  70.00%, val_best:  75.42%, tr:  98.37%, tr_best:  98.57%, epoch time: 39.89 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7499%\n",
      "layer   2  Sparsity: 81.6112%\n",
      "layer   3  Sparsity: 79.1305%\n",
      "total_backward_count 323070 real_backward_count 42491  13.152%\n",
      "lif layer 1 self.abs_max_v: 23120.5\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.435517/  1.621751, val:  73.33%, val_best:  75.42%, tr:  98.67%, tr_best:  98.67%, epoch time: 39.89 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7615%\n",
      "layer   2  Sparsity: 82.1679%\n",
      "layer   3  Sparsity: 78.5582%\n",
      "total_backward_count 327965 real_backward_count 42918  13.086%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.422494/  1.631241, val:  71.67%, val_best:  75.42%, tr:  98.67%, tr_best:  98.67%, epoch time: 36.36 seconds, 0.61 minutes\n",
      "layer   1  Sparsity: 84.8017%\n",
      "layer   2  Sparsity: 82.2497%\n",
      "layer   3  Sparsity: 78.5436%\n",
      "total_backward_count 332860 real_backward_count 43343  13.021%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.434531/  1.623740, val:  70.00%, val_best:  75.42%, tr:  99.08%, tr_best:  99.08%, epoch time: 38.84 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.7491%\n",
      "layer   2  Sparsity: 82.1358%\n",
      "layer   3  Sparsity: 80.6434%\n",
      "total_backward_count 337755 real_backward_count 43712  12.942%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.433680/  1.619720, val:  72.08%, val_best:  75.42%, tr:  98.77%, tr_best:  99.08%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7491%\n",
      "layer   2  Sparsity: 82.3531%\n",
      "layer   3  Sparsity: 80.3566%\n",
      "total_backward_count 342650 real_backward_count 44098  12.870%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.464504/  1.643780, val:  70.42%, val_best:  75.42%, tr:  98.57%, tr_best:  99.08%, epoch time: 39.35 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7729%\n",
      "layer   2  Sparsity: 82.4759%\n",
      "layer   3  Sparsity: 81.1108%\n",
      "total_backward_count 347545 real_backward_count 44517  12.809%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.460477/  1.650940, val:  68.75%, val_best:  75.42%, tr:  98.77%, tr_best:  99.08%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7388%\n",
      "layer   2  Sparsity: 83.0655%\n",
      "layer   3  Sparsity: 80.8056%\n",
      "total_backward_count 352440 real_backward_count 44930  12.748%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.456922/  1.644886, val:  71.67%, val_best:  75.42%, tr:  98.57%, tr_best:  99.08%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7728%\n",
      "layer   2  Sparsity: 83.0592%\n",
      "layer   3  Sparsity: 81.2814%\n",
      "total_backward_count 357335 real_backward_count 45313  12.681%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.471615/  1.662854, val:  67.08%, val_best:  75.42%, tr:  98.57%, tr_best:  99.08%, epoch time: 39.64 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7343%\n",
      "layer   2  Sparsity: 82.4935%\n",
      "layer   3  Sparsity: 80.8657%\n",
      "total_backward_count 362230 real_backward_count 45718  12.621%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.441660/  1.637997, val:  73.75%, val_best:  75.42%, tr:  99.18%, tr_best:  99.18%, epoch time: 39.26 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.7619%\n",
      "layer   2  Sparsity: 82.5563%\n",
      "layer   3  Sparsity: 78.9816%\n",
      "total_backward_count 367125 real_backward_count 46143  12.569%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.442685/  1.645182, val:  69.58%, val_best:  75.42%, tr:  98.37%, tr_best:  99.18%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 84.7493%\n",
      "layer   2  Sparsity: 82.1716%\n",
      "layer   3  Sparsity: 79.1471%\n",
      "total_backward_count 372020 real_backward_count 46537  12.509%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.459301/  1.669060, val:  62.08%, val_best:  75.42%, tr:  98.26%, tr_best:  99.18%, epoch time: 39.63 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7359%\n",
      "layer   2  Sparsity: 82.1290%\n",
      "layer   3  Sparsity: 78.9755%\n",
      "total_backward_count 376915 real_backward_count 46989  12.467%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.459481/  1.688421, val:  68.75%, val_best:  75.42%, tr:  99.18%, tr_best:  99.18%, epoch time: 41.05 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 84.7285%\n",
      "layer   2  Sparsity: 82.8676%\n",
      "layer   3  Sparsity: 79.8642%\n",
      "total_backward_count 381810 real_backward_count 47398  12.414%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.468589/  1.674084, val:  75.83%, val_best:  75.83%, tr:  98.47%, tr_best:  99.18%, epoch time: 39.17 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.8009%\n",
      "layer   2  Sparsity: 82.8445%\n",
      "layer   3  Sparsity: 80.7487%\n",
      "total_backward_count 386705 real_backward_count 47777  12.355%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.457193/  1.656679, val:  77.50%, val_best:  77.50%, tr:  98.37%, tr_best:  99.18%, epoch time: 40.86 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 84.7741%\n",
      "layer   2  Sparsity: 83.1111%\n",
      "layer   3  Sparsity: 80.1105%\n",
      "total_backward_count 391600 real_backward_count 48170  12.301%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.438386/  1.642804, val:  76.67%, val_best:  77.50%, tr:  99.18%, tr_best:  99.18%, epoch time: 39.33 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7615%\n",
      "layer   2  Sparsity: 83.4360%\n",
      "layer   3  Sparsity: 79.9297%\n",
      "total_backward_count 396495 real_backward_count 48545  12.244%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.459699/  1.648028, val:  66.25%, val_best:  77.50%, tr:  98.67%, tr_best:  99.18%, epoch time: 40.16 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7735%\n",
      "layer   2  Sparsity: 83.0480%\n",
      "layer   3  Sparsity: 80.2960%\n",
      "total_backward_count 401390 real_backward_count 48931  12.190%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.447336/  1.683888, val:  70.00%, val_best:  77.50%, tr:  98.77%, tr_best:  99.18%, epoch time: 39.50 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7947%\n",
      "layer   2  Sparsity: 83.0770%\n",
      "layer   3  Sparsity: 80.6372%\n",
      "total_backward_count 406285 real_backward_count 49288  12.131%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.462174/  1.665259, val:  64.58%, val_best:  77.50%, tr:  98.57%, tr_best:  99.18%, epoch time: 39.84 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7785%\n",
      "layer   2  Sparsity: 82.9340%\n",
      "layer   3  Sparsity: 81.0031%\n",
      "total_backward_count 411180 real_backward_count 49693  12.085%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.455508/  1.631771, val:  72.08%, val_best:  77.50%, tr:  98.57%, tr_best:  99.18%, epoch time: 39.72 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7397%\n",
      "layer   2  Sparsity: 83.2030%\n",
      "layer   3  Sparsity: 80.5566%\n",
      "total_backward_count 416075 real_backward_count 50070  12.034%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.430371/  1.646521, val:  71.67%, val_best:  77.50%, tr:  98.67%, tr_best:  99.18%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.8020%\n",
      "layer   2  Sparsity: 83.3937%\n",
      "layer   3  Sparsity: 80.5236%\n",
      "total_backward_count 420970 real_backward_count 50462  11.987%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.444649/  1.637349, val:  71.67%, val_best:  77.50%, tr:  99.08%, tr_best:  99.18%, epoch time: 39.36 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7881%\n",
      "layer   2  Sparsity: 83.3967%\n",
      "layer   3  Sparsity: 80.8481%\n",
      "total_backward_count 425865 real_backward_count 50860  11.943%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.433509/  1.630133, val:  69.58%, val_best:  77.50%, tr:  99.08%, tr_best:  99.18%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7467%\n",
      "layer   2  Sparsity: 83.6022%\n",
      "layer   3  Sparsity: 80.3683%\n",
      "total_backward_count 430760 real_backward_count 51250  11.898%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.441446/  1.644608, val:  69.58%, val_best:  77.50%, tr:  98.98%, tr_best:  99.18%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7286%\n",
      "layer   2  Sparsity: 83.4409%\n",
      "layer   3  Sparsity: 81.2864%\n",
      "total_backward_count 435655 real_backward_count 51629  11.851%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.446281/  1.631263, val:  73.33%, val_best:  77.50%, tr:  99.18%, tr_best:  99.18%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7766%\n",
      "layer   2  Sparsity: 83.3651%\n",
      "layer   3  Sparsity: 81.0720%\n",
      "total_backward_count 440550 real_backward_count 52000  11.803%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.427026/  1.615798, val:  71.67%, val_best:  77.50%, tr:  98.88%, tr_best:  99.18%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7794%\n",
      "layer   2  Sparsity: 82.6270%\n",
      "layer   3  Sparsity: 80.1842%\n",
      "total_backward_count 445445 real_backward_count 52404  11.764%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.435287/  1.633542, val:  79.17%, val_best:  79.17%, tr:  98.88%, tr_best:  99.18%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7374%\n",
      "layer   2  Sparsity: 82.9984%\n",
      "layer   3  Sparsity: 79.8737%\n",
      "total_backward_count 450340 real_backward_count 52770  11.718%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.454156/  1.667394, val:  76.25%, val_best:  79.17%, tr:  98.67%, tr_best:  99.18%, epoch time: 40.26 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7625%\n",
      "layer   2  Sparsity: 82.9017%\n",
      "layer   3  Sparsity: 80.3480%\n",
      "total_backward_count 455235 real_backward_count 53138  11.673%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.443408/  1.626590, val:  74.17%, val_best:  79.17%, tr:  98.67%, tr_best:  99.18%, epoch time: 39.12 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.7339%\n",
      "layer   2  Sparsity: 83.1040%\n",
      "layer   3  Sparsity: 80.1655%\n",
      "total_backward_count 460130 real_backward_count 53531  11.634%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.452474/  1.706248, val:  72.50%, val_best:  79.17%, tr:  98.77%, tr_best:  99.18%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.8010%\n",
      "layer   2  Sparsity: 82.9144%\n",
      "layer   3  Sparsity: 80.6853%\n",
      "total_backward_count 465025 real_backward_count 53923  11.596%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.493392/  1.689737, val:  68.33%, val_best:  79.17%, tr:  98.98%, tr_best:  99.18%, epoch time: 39.49 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7125%\n",
      "layer   2  Sparsity: 82.5718%\n",
      "layer   3  Sparsity: 82.0601%\n",
      "total_backward_count 469920 real_backward_count 54325  11.560%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.469363/  1.678867, val:  65.00%, val_best:  79.17%, tr:  98.67%, tr_best:  99.18%, epoch time: 39.79 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7667%\n",
      "layer   2  Sparsity: 82.9924%\n",
      "layer   3  Sparsity: 80.6357%\n",
      "total_backward_count 474815 real_backward_count 54651  11.510%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.441456/  1.656721, val:  77.50%, val_best:  79.17%, tr:  98.88%, tr_best:  99.18%, epoch time: 39.40 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7829%\n",
      "layer   2  Sparsity: 82.7180%\n",
      "layer   3  Sparsity: 80.3742%\n",
      "total_backward_count 479710 real_backward_count 54991  11.463%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.454744/  1.660405, val:  73.33%, val_best:  79.17%, tr:  98.16%, tr_best:  99.18%, epoch time: 39.83 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7695%\n",
      "layer   2  Sparsity: 82.7764%\n",
      "layer   3  Sparsity: 80.4445%\n",
      "total_backward_count 484605 real_backward_count 55404  11.433%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.450269/  1.635999, val:  70.00%, val_best:  79.17%, tr:  99.08%, tr_best:  99.18%, epoch time: 39.15 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.7403%\n",
      "layer   2  Sparsity: 83.1586%\n",
      "layer   3  Sparsity: 80.5016%\n",
      "total_backward_count 489500 real_backward_count 55772  11.394%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.431677/  1.659811, val:  77.92%, val_best:  79.17%, tr:  98.88%, tr_best:  99.18%, epoch time: 39.90 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7352%\n",
      "layer   2  Sparsity: 83.2546%\n",
      "layer   3  Sparsity: 79.7798%\n",
      "total_backward_count 494395 real_backward_count 56123  11.352%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.450294/  1.615371, val:  73.33%, val_best:  79.17%, tr:  98.47%, tr_best:  99.18%, epoch time: 38.73 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.7815%\n",
      "layer   2  Sparsity: 82.7345%\n",
      "layer   3  Sparsity: 79.7450%\n",
      "total_backward_count 499290 real_backward_count 56535  11.323%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.440842/  1.613355, val:  72.50%, val_best:  79.17%, tr:  98.88%, tr_best:  99.18%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7098%\n",
      "layer   2  Sparsity: 83.0434%\n",
      "layer   3  Sparsity: 80.7337%\n",
      "total_backward_count 504185 real_backward_count 56902  11.286%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.416603/  1.611547, val:  79.58%, val_best:  79.58%, tr:  98.77%, tr_best:  99.18%, epoch time: 39.37 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7202%\n",
      "layer   2  Sparsity: 82.9107%\n",
      "layer   3  Sparsity: 79.9814%\n",
      "total_backward_count 509080 real_backward_count 57255  11.247%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.416906/  1.626214, val:  67.50%, val_best:  79.58%, tr:  99.18%, tr_best:  99.18%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7576%\n",
      "layer   2  Sparsity: 83.1486%\n",
      "layer   3  Sparsity: 80.7066%\n",
      "total_backward_count 513975 real_backward_count 57616  11.210%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.427607/  1.612274, val:  70.00%, val_best:  79.58%, tr:  98.98%, tr_best:  99.18%, epoch time: 39.75 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7590%\n",
      "layer   2  Sparsity: 82.8748%\n",
      "layer   3  Sparsity: 80.1084%\n",
      "total_backward_count 518870 real_backward_count 57945  11.168%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.426286/  1.606852, val:  80.42%, val_best:  80.42%, tr:  98.77%, tr_best:  99.18%, epoch time: 39.91 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7800%\n",
      "layer   2  Sparsity: 82.9651%\n",
      "layer   3  Sparsity: 80.3045%\n",
      "total_backward_count 523765 real_backward_count 58302  11.131%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.419509/  1.604159, val:  77.50%, val_best:  80.42%, tr:  99.28%, tr_best:  99.28%, epoch time: 39.58 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7436%\n",
      "layer   2  Sparsity: 82.9016%\n",
      "layer   3  Sparsity: 80.5566%\n",
      "total_backward_count 528660 real_backward_count 58592  11.083%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.389216/  1.599164, val:  74.58%, val_best:  80.42%, tr:  99.28%, tr_best:  99.28%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7685%\n",
      "layer   2  Sparsity: 82.8757%\n",
      "layer   3  Sparsity: 80.3885%\n",
      "total_backward_count 533555 real_backward_count 58891  11.037%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.403182/  1.592370, val:  79.58%, val_best:  80.42%, tr:  99.18%, tr_best:  99.28%, epoch time: 39.25 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.7759%\n",
      "layer   2  Sparsity: 82.7045%\n",
      "layer   3  Sparsity: 80.4648%\n",
      "total_backward_count 538450 real_backward_count 59217  10.998%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.411301/  1.601238, val:  70.83%, val_best:  80.42%, tr:  99.18%, tr_best:  99.28%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7943%\n",
      "layer   2  Sparsity: 82.8112%\n",
      "layer   3  Sparsity: 79.7984%\n",
      "total_backward_count 543345 real_backward_count 59511  10.953%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.399781/  1.582831, val:  69.58%, val_best:  80.42%, tr:  99.59%, tr_best:  99.59%, epoch time: 39.54 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7567%\n",
      "layer   2  Sparsity: 82.8648%\n",
      "layer   3  Sparsity: 79.6875%\n",
      "total_backward_count 548240 real_backward_count 59782  10.904%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.405041/  1.613082, val:  77.50%, val_best:  80.42%, tr:  99.49%, tr_best:  99.59%, epoch time: 39.49 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7731%\n",
      "layer   2  Sparsity: 82.8296%\n",
      "layer   3  Sparsity: 79.7884%\n",
      "total_backward_count 553135 real_backward_count 60087  10.863%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.403303/  1.612755, val:  74.17%, val_best:  80.42%, tr:  98.98%, tr_best:  99.59%, epoch time: 39.58 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7245%\n",
      "layer   2  Sparsity: 82.8992%\n",
      "layer   3  Sparsity: 80.4738%\n",
      "total_backward_count 558030 real_backward_count 60374  10.819%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.388177/  1.613375, val:  71.25%, val_best:  80.42%, tr:  99.18%, tr_best:  99.59%, epoch time: 39.97 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7824%\n",
      "layer   2  Sparsity: 82.9028%\n",
      "layer   3  Sparsity: 80.2954%\n",
      "total_backward_count 562925 real_backward_count 60692  10.782%\n",
      "lif layer 1 self.abs_max_v: 23172.0\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.388223/  1.599663, val:  75.83%, val_best:  80.42%, tr:  99.28%, tr_best:  99.59%, epoch time: 39.90 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7867%\n",
      "layer   2  Sparsity: 83.1545%\n",
      "layer   3  Sparsity: 80.5948%\n",
      "total_backward_count 567820 real_backward_count 60996  10.742%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.396774/  1.614931, val:  69.58%, val_best:  80.42%, tr:  99.49%, tr_best:  99.59%, epoch time: 39.19 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.7641%\n",
      "layer   2  Sparsity: 83.2568%\n",
      "layer   3  Sparsity: 80.6632%\n",
      "total_backward_count 572715 real_backward_count 61289  10.701%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.400992/  1.624855, val:  79.58%, val_best:  80.42%, tr:  99.08%, tr_best:  99.59%, epoch time: 40.10 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7475%\n",
      "layer   2  Sparsity: 83.4365%\n",
      "layer   3  Sparsity: 80.9138%\n",
      "total_backward_count 577610 real_backward_count 61578  10.661%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.396159/  1.604736, val:  74.58%, val_best:  80.42%, tr:  98.98%, tr_best:  99.59%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7408%\n",
      "layer   2  Sparsity: 83.5731%\n",
      "layer   3  Sparsity: 80.1247%\n",
      "total_backward_count 582505 real_backward_count 61902  10.627%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.387955/  1.583756, val:  77.08%, val_best:  80.42%, tr:  99.28%, tr_best:  99.59%, epoch time: 39.65 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7230%\n",
      "layer   2  Sparsity: 83.6783%\n",
      "layer   3  Sparsity: 79.6145%\n",
      "total_backward_count 587400 real_backward_count 62222  10.593%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.379105/  1.580484, val:  73.75%, val_best:  80.42%, tr:  99.28%, tr_best:  99.59%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7445%\n",
      "layer   2  Sparsity: 83.5286%\n",
      "layer   3  Sparsity: 80.1299%\n",
      "total_backward_count 592295 real_backward_count 62520  10.556%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.395438/  1.623010, val:  79.58%, val_best:  80.42%, tr:  99.59%, tr_best:  99.59%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7905%\n",
      "layer   2  Sparsity: 83.3293%\n",
      "layer   3  Sparsity: 80.4622%\n",
      "total_backward_count 597190 real_backward_count 62830  10.521%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.416791/  1.617642, val:  72.92%, val_best:  80.42%, tr:  99.28%, tr_best:  99.59%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.8089%\n",
      "layer   2  Sparsity: 83.5991%\n",
      "layer   3  Sparsity: 81.0279%\n",
      "total_backward_count 602085 real_backward_count 63126  10.485%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.413395/  1.615419, val:  75.83%, val_best:  80.42%, tr:  99.49%, tr_best:  99.59%, epoch time: 39.55 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7676%\n",
      "layer   2  Sparsity: 83.5721%\n",
      "layer   3  Sparsity: 80.5244%\n",
      "total_backward_count 606980 real_backward_count 63399  10.445%\n",
      "lif layer 1 self.abs_max_v: 23200.0\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.399399/  1.630887, val:  74.17%, val_best:  80.42%, tr:  99.28%, tr_best:  99.59%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7856%\n",
      "layer   2  Sparsity: 83.3430%\n",
      "layer   3  Sparsity: 80.2688%\n",
      "total_backward_count 611875 real_backward_count 63694  10.410%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.391107/  1.606285, val:  77.08%, val_best:  80.42%, tr:  99.49%, tr_best:  99.59%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7102%\n",
      "layer   2  Sparsity: 83.5953%\n",
      "layer   3  Sparsity: 80.4664%\n",
      "total_backward_count 616770 real_backward_count 63980  10.373%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.401889/  1.597346, val:  71.67%, val_best:  80.42%, tr:  99.08%, tr_best:  99.59%, epoch time: 39.72 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7528%\n",
      "layer   2  Sparsity: 83.6348%\n",
      "layer   3  Sparsity: 81.1463%\n",
      "total_backward_count 621665 real_backward_count 64258  10.336%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.404941/  1.637585, val:  74.17%, val_best:  80.42%, tr:  98.67%, tr_best:  99.59%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7380%\n",
      "layer   2  Sparsity: 83.6821%\n",
      "layer   3  Sparsity: 80.6297%\n",
      "total_backward_count 626560 real_backward_count 64566  10.305%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.416369/  1.608231, val:  73.33%, val_best:  80.42%, tr:  99.59%, tr_best:  99.59%, epoch time: 39.44 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.8012%\n",
      "layer   2  Sparsity: 83.7679%\n",
      "layer   3  Sparsity: 80.2026%\n",
      "total_backward_count 631455 real_backward_count 64878  10.274%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.402110/  1.586293, val:  81.25%, val_best:  81.25%, tr:  99.49%, tr_best:  99.59%, epoch time: 38.97 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.7549%\n",
      "layer   2  Sparsity: 83.2863%\n",
      "layer   3  Sparsity: 79.3794%\n",
      "total_backward_count 636350 real_backward_count 65177  10.242%\n",
      "lif layer 1 self.abs_max_v: 23228.0\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.398845/  1.643466, val:  70.00%, val_best:  81.25%, tr:  99.69%, tr_best:  99.69%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7810%\n",
      "layer   2  Sparsity: 83.4400%\n",
      "layer   3  Sparsity: 79.8821%\n",
      "total_backward_count 641245 real_backward_count 65506  10.215%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.393312/  1.642827, val:  68.75%, val_best:  81.25%, tr:  99.08%, tr_best:  99.69%, epoch time: 39.48 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.8102%\n",
      "layer   2  Sparsity: 83.7596%\n",
      "layer   3  Sparsity: 80.7575%\n",
      "total_backward_count 646140 real_backward_count 65790  10.182%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.395159/  1.573403, val:  79.58%, val_best:  81.25%, tr:  98.57%, tr_best:  99.69%, epoch time: 39.42 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7528%\n",
      "layer   2  Sparsity: 83.7952%\n",
      "layer   3  Sparsity: 79.8683%\n",
      "total_backward_count 651035 real_backward_count 66102  10.153%\n",
      "fc layer 1 self.abs_max_out: 14801.0\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.400435/  1.622210, val:  70.83%, val_best:  81.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7594%\n",
      "layer   2  Sparsity: 83.7357%\n",
      "layer   3  Sparsity: 80.6897%\n",
      "total_backward_count 655930 real_backward_count 66383  10.120%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.394524/  1.607378, val:  70.42%, val_best:  81.25%, tr:  99.69%, tr_best:  99.69%, epoch time: 39.49 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7768%\n",
      "layer   2  Sparsity: 84.0551%\n",
      "layer   3  Sparsity: 80.3827%\n",
      "total_backward_count 660825 real_backward_count 66669  10.089%\n",
      "fc layer 3 self.abs_max_out: 2133.0\n",
      "fc layer 1 self.abs_max_out: 15237.0\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.393324/  1.580897, val:  77.08%, val_best:  81.25%, tr:  99.49%, tr_best:  99.69%, epoch time: 39.80 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7646%\n",
      "layer   2  Sparsity: 84.0237%\n",
      "layer   3  Sparsity: 79.7181%\n",
      "total_backward_count 665720 real_backward_count 67044  10.071%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.391106/  1.607333, val:  72.92%, val_best:  81.25%, tr:  99.39%, tr_best:  99.69%, epoch time: 39.31 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7589%\n",
      "layer   2  Sparsity: 84.0073%\n",
      "layer   3  Sparsity: 80.5051%\n",
      "total_backward_count 670615 real_backward_count 67368  10.046%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.407683/  1.609570, val:  79.58%, val_best:  81.25%, tr:  99.39%, tr_best:  99.69%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7661%\n",
      "layer   2  Sparsity: 84.0639%\n",
      "layer   3  Sparsity: 80.0144%\n",
      "total_backward_count 675510 real_backward_count 67704  10.023%\n",
      "fc layer 3 self.abs_max_out: 2157.0\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.431569/  1.617784, val:  79.58%, val_best:  81.25%, tr:  98.47%, tr_best:  99.69%, epoch time: 39.09 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.7440%\n",
      "layer   2  Sparsity: 83.9038%\n",
      "layer   3  Sparsity: 81.4920%\n",
      "total_backward_count 680405 real_backward_count 68050  10.001%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.404109/  1.580899, val:  74.58%, val_best:  81.25%, tr:  99.49%, tr_best:  99.69%, epoch time: 39.82 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7730%\n",
      "layer   2  Sparsity: 84.3141%\n",
      "layer   3  Sparsity: 80.2610%\n",
      "total_backward_count 685300 real_backward_count 68343   9.973%\n",
      "lif layer 1 self.abs_max_v: 23288.5\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.383202/  1.616098, val:  78.33%, val_best:  81.25%, tr:  99.28%, tr_best:  99.69%, epoch time: 39.70 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7787%\n",
      "layer   2  Sparsity: 84.5729%\n",
      "layer   3  Sparsity: 81.2448%\n",
      "total_backward_count 690195 real_backward_count 68618   9.942%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.377111/  1.598420, val:  75.42%, val_best:  81.25%, tr:  99.49%, tr_best:  99.69%, epoch time: 39.83 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7849%\n",
      "layer   2  Sparsity: 84.6932%\n",
      "layer   3  Sparsity: 81.3048%\n",
      "total_backward_count 695090 real_backward_count 68892   9.911%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.397385/  1.608101, val:  69.17%, val_best:  81.25%, tr:  98.98%, tr_best:  99.69%, epoch time: 39.11 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.8111%\n",
      "layer   2  Sparsity: 84.6078%\n",
      "layer   3  Sparsity: 81.0884%\n",
      "total_backward_count 699985 real_backward_count 69208   9.887%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.408801/  1.626631, val:  74.17%, val_best:  81.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 39.23 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.7513%\n",
      "layer   2  Sparsity: 84.2636%\n",
      "layer   3  Sparsity: 81.8762%\n",
      "total_backward_count 704880 real_backward_count 69498   9.860%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.391348/  1.581954, val:  72.08%, val_best:  81.25%, tr:  99.49%, tr_best:  99.69%, epoch time: 39.39 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7580%\n",
      "layer   2  Sparsity: 84.2161%\n",
      "layer   3  Sparsity: 81.4267%\n",
      "total_backward_count 709775 real_backward_count 69771   9.830%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.380756/  1.606118, val:  68.33%, val_best:  81.25%, tr:  99.18%, tr_best:  99.69%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7606%\n",
      "layer   2  Sparsity: 84.2536%\n",
      "layer   3  Sparsity: 80.8660%\n",
      "total_backward_count 714670 real_backward_count 70038   9.800%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.401580/  1.606430, val:  76.25%, val_best:  81.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7647%\n",
      "layer   2  Sparsity: 84.4824%\n",
      "layer   3  Sparsity: 81.8139%\n",
      "total_backward_count 719565 real_backward_count 70342   9.776%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.377647/  1.610435, val:  72.92%, val_best:  81.25%, tr:  99.28%, tr_best:  99.69%, epoch time: 39.38 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7793%\n",
      "layer   2  Sparsity: 84.5477%\n",
      "layer   3  Sparsity: 81.7233%\n",
      "total_backward_count 724460 real_backward_count 70596   9.745%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.382819/  1.623185, val:  69.17%, val_best:  81.25%, tr:  98.98%, tr_best:  99.69%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7616%\n",
      "layer   2  Sparsity: 84.6527%\n",
      "layer   3  Sparsity: 81.1444%\n",
      "total_backward_count 729355 real_backward_count 70882   9.718%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.381429/  1.631834, val:  75.00%, val_best:  81.25%, tr:  99.39%, tr_best:  99.69%, epoch time: 39.30 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.7443%\n",
      "layer   2  Sparsity: 84.6739%\n",
      "layer   3  Sparsity: 81.1475%\n",
      "total_backward_count 734250 real_backward_count 71183   9.695%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.403045/  1.639780, val:  75.00%, val_best:  81.25%, tr:  99.28%, tr_best:  99.69%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7757%\n",
      "layer   2  Sparsity: 84.6687%\n",
      "layer   3  Sparsity: 81.3433%\n",
      "total_backward_count 739145 real_backward_count 71482   9.671%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.383066/  1.627532, val:  70.83%, val_best:  81.25%, tr:  99.18%, tr_best:  99.69%, epoch time: 39.46 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7968%\n",
      "layer   2  Sparsity: 84.4392%\n",
      "layer   3  Sparsity: 81.2587%\n",
      "total_backward_count 744040 real_backward_count 71744   9.642%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.386300/  1.623295, val:  68.75%, val_best:  81.25%, tr:  98.88%, tr_best:  99.69%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7640%\n",
      "layer   2  Sparsity: 84.2019%\n",
      "layer   3  Sparsity: 80.8310%\n",
      "total_backward_count 748935 real_backward_count 72009   9.615%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.388738/  1.640711, val:  72.92%, val_best:  81.25%, tr:  99.49%, tr_best:  99.69%, epoch time: 39.34 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7905%\n",
      "layer   2  Sparsity: 84.1889%\n",
      "layer   3  Sparsity: 81.0694%\n",
      "total_backward_count 753830 real_backward_count 72293   9.590%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.380657/  1.622196, val:  76.67%, val_best:  81.25%, tr:  99.39%, tr_best:  99.69%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7540%\n",
      "layer   2  Sparsity: 84.1025%\n",
      "layer   3  Sparsity: 80.4654%\n",
      "total_backward_count 758725 real_backward_count 72580   9.566%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.391337/  1.628708, val:  73.75%, val_best:  81.25%, tr:  98.88%, tr_best:  99.69%, epoch time: 39.70 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7632%\n",
      "layer   2  Sparsity: 84.1267%\n",
      "layer   3  Sparsity: 80.7610%\n",
      "total_backward_count 763620 real_backward_count 72861   9.542%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.392421/  1.624791, val:  73.33%, val_best:  81.25%, tr:  98.77%, tr_best:  99.69%, epoch time: 39.61 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7903%\n",
      "layer   2  Sparsity: 84.2389%\n",
      "layer   3  Sparsity: 80.6615%\n",
      "total_backward_count 768515 real_backward_count 73142   9.517%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.387852/  1.610235, val:  69.58%, val_best:  81.25%, tr:  99.49%, tr_best:  99.69%, epoch time: 39.26 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.7731%\n",
      "layer   2  Sparsity: 84.1569%\n",
      "layer   3  Sparsity: 80.3372%\n",
      "total_backward_count 773410 real_backward_count 73384   9.488%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.365186/  1.617975, val:  75.83%, val_best:  81.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7667%\n",
      "layer   2  Sparsity: 84.1879%\n",
      "layer   3  Sparsity: 80.5833%\n",
      "total_backward_count 778305 real_backward_count 73624   9.460%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.379097/  1.618473, val:  73.33%, val_best:  81.25%, tr:  99.28%, tr_best:  99.69%, epoch time: 39.79 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7785%\n",
      "layer   2  Sparsity: 84.2063%\n",
      "layer   3  Sparsity: 80.2426%\n",
      "total_backward_count 783200 real_backward_count 73920   9.438%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.379488/  1.586608, val:  77.92%, val_best:  81.25%, tr:  99.49%, tr_best:  99.69%, epoch time: 41.01 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 84.7430%\n",
      "layer   2  Sparsity: 84.1396%\n",
      "layer   3  Sparsity: 81.1940%\n",
      "total_backward_count 788095 real_backward_count 74188   9.414%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.375274/  1.605229, val:  75.83%, val_best:  81.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 39.46 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7510%\n",
      "layer   2  Sparsity: 84.1856%\n",
      "layer   3  Sparsity: 80.8051%\n",
      "total_backward_count 792990 real_backward_count 74423   9.385%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.372892/  1.615402, val:  75.83%, val_best:  81.25%, tr:  99.69%, tr_best:  99.69%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7083%\n",
      "layer   2  Sparsity: 84.0989%\n",
      "layer   3  Sparsity: 81.2152%\n",
      "total_backward_count 797885 real_backward_count 74647   9.356%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.391078/  1.626578, val:  78.33%, val_best:  81.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7675%\n",
      "layer   2  Sparsity: 84.3130%\n",
      "layer   3  Sparsity: 80.9665%\n",
      "total_backward_count 802780 real_backward_count 74908   9.331%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.402408/  1.627153, val:  72.50%, val_best:  81.25%, tr:  98.88%, tr_best:  99.69%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7692%\n",
      "layer   2  Sparsity: 84.5984%\n",
      "layer   3  Sparsity: 80.2661%\n",
      "total_backward_count 807675 real_backward_count 75216   9.313%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.368161/  1.595737, val:  70.42%, val_best:  81.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 39.72 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7532%\n",
      "layer   2  Sparsity: 84.9505%\n",
      "layer   3  Sparsity: 80.5793%\n",
      "total_backward_count 812570 real_backward_count 75472   9.288%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.360843/  1.568690, val:  70.42%, val_best:  81.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 39.12 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.7762%\n",
      "layer   2  Sparsity: 84.7340%\n",
      "layer   3  Sparsity: 80.3062%\n",
      "total_backward_count 817465 real_backward_count 75722   9.263%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.371047/  1.611763, val:  72.92%, val_best:  81.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7381%\n",
      "layer   2  Sparsity: 84.6411%\n",
      "layer   3  Sparsity: 81.4633%\n",
      "total_backward_count 822360 real_backward_count 75950   9.236%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.381304/  1.630981, val:  73.33%, val_best:  81.25%, tr:  99.28%, tr_best:  99.90%, epoch time: 39.61 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7312%\n",
      "layer   2  Sparsity: 84.6123%\n",
      "layer   3  Sparsity: 80.8663%\n",
      "total_backward_count 827255 real_backward_count 76233   9.215%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.381265/  1.597119, val:  80.00%, val_best:  81.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 39.33 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7585%\n",
      "layer   2  Sparsity: 84.2626%\n",
      "layer   3  Sparsity: 81.3608%\n",
      "total_backward_count 832150 real_backward_count 76487   9.191%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.384462/  1.579394, val:  79.17%, val_best:  81.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 39.81 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7574%\n",
      "layer   2  Sparsity: 84.5255%\n",
      "layer   3  Sparsity: 81.1550%\n",
      "total_backward_count 837045 real_backward_count 76760   9.170%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.359853/  1.552289, val:  77.50%, val_best:  81.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 39.75 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7752%\n",
      "layer   2  Sparsity: 84.3685%\n",
      "layer   3  Sparsity: 80.8559%\n",
      "total_backward_count 841940 real_backward_count 76994   9.145%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.319353/  1.561267, val:  74.58%, val_best:  81.25%, tr:  99.18%, tr_best:  99.90%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7352%\n",
      "layer   2  Sparsity: 84.3947%\n",
      "layer   3  Sparsity: 80.2589%\n",
      "total_backward_count 846835 real_backward_count 77235   9.120%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.343022/  1.602383, val:  77.92%, val_best:  81.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7229%\n",
      "layer   2  Sparsity: 84.5598%\n",
      "layer   3  Sparsity: 80.4065%\n",
      "total_backward_count 851730 real_backward_count 77464   9.095%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.350375/  1.560148, val:  79.58%, val_best:  81.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 39.58 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7595%\n",
      "layer   2  Sparsity: 84.5161%\n",
      "layer   3  Sparsity: 80.1237%\n",
      "total_backward_count 856625 real_backward_count 77755   9.077%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.341879/  1.589435, val:  76.67%, val_best:  81.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 39.37 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7717%\n",
      "layer   2  Sparsity: 84.3870%\n",
      "layer   3  Sparsity: 80.7775%\n",
      "total_backward_count 861520 real_backward_count 77968   9.050%\n",
      "fc layer 2 self.abs_max_out: 3541.0\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.353060/  1.552388, val:  73.75%, val_best:  81.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7656%\n",
      "layer   2  Sparsity: 84.4411%\n",
      "layer   3  Sparsity: 80.2145%\n",
      "total_backward_count 866415 real_backward_count 78203   9.026%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.344594/  1.603400, val:  72.92%, val_best:  81.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 38.93 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.7514%\n",
      "layer   2  Sparsity: 84.4093%\n",
      "layer   3  Sparsity: 80.7860%\n",
      "total_backward_count 871310 real_backward_count 78459   9.005%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.350811/  1.561251, val:  74.58%, val_best:  81.25%, tr:  99.08%, tr_best:  99.90%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7616%\n",
      "layer   2  Sparsity: 84.5948%\n",
      "layer   3  Sparsity: 80.6465%\n",
      "total_backward_count 876205 real_backward_count 78713   8.983%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.339152/  1.573762, val:  75.00%, val_best:  81.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 39.84 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7604%\n",
      "layer   2  Sparsity: 84.6437%\n",
      "layer   3  Sparsity: 80.3792%\n",
      "total_backward_count 881100 real_backward_count 78965   8.962%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.347011/  1.572510, val:  75.00%, val_best:  81.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 39.83 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7232%\n",
      "layer   2  Sparsity: 84.6258%\n",
      "layer   3  Sparsity: 81.3328%\n",
      "total_backward_count 885995 real_backward_count 79196   8.939%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.362520/  1.587638, val:  75.00%, val_best:  81.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 39.61 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7688%\n",
      "layer   2  Sparsity: 84.5808%\n",
      "layer   3  Sparsity: 81.5737%\n",
      "total_backward_count 890890 real_backward_count 79432   8.916%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.363747/  1.564452, val:  76.67%, val_best:  81.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 39.85 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7405%\n",
      "layer   2  Sparsity: 84.6101%\n",
      "layer   3  Sparsity: 81.5024%\n",
      "total_backward_count 895785 real_backward_count 79627   8.889%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.342815/  1.576830, val:  67.92%, val_best:  81.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 39.14 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.7716%\n",
      "layer   2  Sparsity: 84.6664%\n",
      "layer   3  Sparsity: 80.4523%\n",
      "total_backward_count 900680 real_backward_count 79844   8.865%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.341456/  1.574958, val:  73.33%, val_best:  81.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 39.78 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7559%\n",
      "layer   2  Sparsity: 84.5113%\n",
      "layer   3  Sparsity: 81.4063%\n",
      "total_backward_count 905575 real_backward_count 80063   8.841%\n",
      "fc layer 3 self.abs_max_out: 2167.0\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.349693/  1.562296, val:  75.00%, val_best:  81.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 39.58 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7659%\n",
      "layer   2  Sparsity: 84.6628%\n",
      "layer   3  Sparsity: 81.2251%\n",
      "total_backward_count 910470 real_backward_count 80278   8.817%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.355365/  1.555391, val:  69.58%, val_best:  81.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 39.65 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7229%\n",
      "layer   2  Sparsity: 84.5683%\n",
      "layer   3  Sparsity: 81.4576%\n",
      "total_backward_count 915365 real_backward_count 80523   8.797%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.347293/  1.558819, val:  80.42%, val_best:  81.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 39.97 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.8102%\n",
      "layer   2  Sparsity: 84.5779%\n",
      "layer   3  Sparsity: 81.2889%\n",
      "total_backward_count 920260 real_backward_count 80783   8.778%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.346122/  1.563914, val:  79.58%, val_best:  81.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 39.58 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7675%\n",
      "layer   2  Sparsity: 84.6932%\n",
      "layer   3  Sparsity: 81.1778%\n",
      "total_backward_count 925155 real_backward_count 81021   8.758%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.362010/  1.545325, val:  81.25%, val_best:  81.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 39.70 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7574%\n",
      "layer   2  Sparsity: 84.1815%\n",
      "layer   3  Sparsity: 81.3204%\n",
      "total_backward_count 930050 real_backward_count 81272   8.738%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.341949/  1.564493, val:  77.08%, val_best:  81.25%, tr:  99.28%, tr_best:  99.90%, epoch time: 39.27 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 84.7622%\n",
      "layer   2  Sparsity: 84.2094%\n",
      "layer   3  Sparsity: 81.3018%\n",
      "total_backward_count 934945 real_backward_count 81483   8.715%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.337488/  1.551479, val:  76.25%, val_best:  81.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7611%\n",
      "layer   2  Sparsity: 84.2965%\n",
      "layer   3  Sparsity: 81.1978%\n",
      "total_backward_count 939840 real_backward_count 81699   8.693%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.345202/  1.590931, val:  81.25%, val_best:  81.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7650%\n",
      "layer   2  Sparsity: 84.3559%\n",
      "layer   3  Sparsity: 81.5989%\n",
      "total_backward_count 944735 real_backward_count 81895   8.669%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.368104/  1.590084, val:  76.25%, val_best:  81.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7656%\n",
      "layer   2  Sparsity: 84.3313%\n",
      "layer   3  Sparsity: 81.1570%\n",
      "total_backward_count 949630 real_backward_count 82119   8.647%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.354590/  1.580382, val:  69.17%, val_best:  81.25%, tr:  99.28%, tr_best:  99.90%, epoch time: 39.98 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7683%\n",
      "layer   2  Sparsity: 84.3607%\n",
      "layer   3  Sparsity: 81.0739%\n",
      "total_backward_count 954525 real_backward_count 82340   8.626%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.332071/  1.552878, val:  70.83%, val_best:  81.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7623%\n",
      "layer   2  Sparsity: 84.4661%\n",
      "layer   3  Sparsity: 80.1801%\n",
      "total_backward_count 959420 real_backward_count 82569   8.606%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.343482/  1.586314, val:  74.17%, val_best:  81.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 39.60 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7659%\n",
      "layer   2  Sparsity: 84.5617%\n",
      "layer   3  Sparsity: 79.8560%\n",
      "total_backward_count 964315 real_backward_count 82817   8.588%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.353375/  1.584807, val:  72.92%, val_best:  81.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 84.7848%\n",
      "layer   2  Sparsity: 84.8290%\n",
      "layer   3  Sparsity: 80.5300%\n",
      "total_backward_count 969210 real_backward_count 83086   8.573%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.364068/  1.574311, val:  72.50%, val_best:  81.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 39.81 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7680%\n",
      "layer   2  Sparsity: 84.8400%\n",
      "layer   3  Sparsity: 80.9648%\n",
      "total_backward_count 974105 real_backward_count 83291   8.551%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.349869/  1.560302, val:  73.75%, val_best:  81.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 39.40 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 84.7603%\n",
      "layer   2  Sparsity: 84.6193%\n",
      "layer   3  Sparsity: 80.6932%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a3ccdfb024434a86a684134f9fb60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñÜ‚ñà‚ñà‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñà‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñÜ‚ñà‚ñà‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñà‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>1.34987</td></tr><tr><td>val_acc_best</td><td>0.8125</td></tr><tr><td>val_acc_now</td><td>0.7375</td></tr><tr><td>val_loss</td><td>1.5603</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sweet-sweep-311</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tw5juqjo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tw5juqjo</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251127_095915-tw5juqjo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zgaznwz9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 50000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251127_121254-zgaznwz9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zgaznwz9' target=\"_blank\">graceful-sweep-317</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zgaznwz9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zgaznwz9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251127_121303_229', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 15, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 50000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = dac77cc348b2d880ae59906e26f08f17\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=15, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=15, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 139\n",
      "fc layer 1 self.abs_max_out: 807.0\n",
      "lif layer 1 self.abs_max_v: 807.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1797.0\n",
      "lif layer 2 self.abs_max_v: 1797.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 405.0\n",
      "fc layer 1 self.abs_max_out: 901.0\n",
      "lif layer 1 self.abs_max_v: 1095.5\n",
      "lif layer 2 self.abs_max_v: 1939.5\n",
      "fc layer 3 self.abs_max_out: 502.0\n",
      "lif layer 1 self.abs_max_v: 1371.0\n",
      "fc layer 2 self.abs_max_out: 1842.0\n",
      "lif layer 2 self.abs_max_v: 2812.0\n",
      "fc layer 3 self.abs_max_out: 631.0\n",
      "fc layer 1 self.abs_max_out: 1034.0\n",
      "lif layer 1 self.abs_max_v: 1414.5\n",
      "fc layer 3 self.abs_max_out: 780.0\n",
      "fc layer 1 self.abs_max_out: 1632.0\n",
      "lif layer 1 self.abs_max_v: 1632.0\n",
      "smallest_now_T updated: 125\n",
      "fc layer 3 self.abs_max_out: 813.0\n",
      "fc layer 1 self.abs_max_out: 1785.0\n",
      "lif layer 1 self.abs_max_v: 1785.0\n",
      "fc layer 3 self.abs_max_out: 931.0\n",
      "fc layer 2 self.abs_max_out: 2482.0\n",
      "lif layer 2 self.abs_max_v: 3175.0\n",
      "fc layer 3 self.abs_max_out: 1083.0\n",
      "smallest_now_T updated: 94\n",
      "fc layer 1 self.abs_max_out: 2038.0\n",
      "lif layer 1 self.abs_max_v: 2038.0\n",
      "lif layer 1 self.abs_max_v: 2601.5\n",
      "fc layer 2 self.abs_max_out: 2594.0\n",
      "lif layer 2 self.abs_max_v: 3280.5\n",
      "fc layer 1 self.abs_max_out: 2335.0\n",
      "lif layer 1 self.abs_max_v: 2730.5\n",
      "fc layer 1 self.abs_max_out: 2405.0\n",
      "fc layer 3 self.abs_max_out: 1099.0\n",
      "fc layer 3 self.abs_max_out: 1112.0\n",
      "fc layer 1 self.abs_max_out: 2528.0\n",
      "smallest_now_T updated: 79\n",
      "fc layer 3 self.abs_max_out: 1385.0\n",
      "fc layer 1 self.abs_max_out: 3159.0\n",
      "lif layer 1 self.abs_max_v: 3159.0\n",
      "lif layer 1 self.abs_max_v: 3678.5\n",
      "lif layer 1 self.abs_max_v: 3744.5\n",
      "lif layer 1 self.abs_max_v: 3971.5\n",
      "fc layer 1 self.abs_max_out: 4257.0\n",
      "lif layer 1 self.abs_max_v: 4257.0\n",
      "lif layer 1 self.abs_max_v: 4492.5\n",
      "lif layer 1 self.abs_max_v: 5297.0\n",
      "lif layer 2 self.abs_max_v: 3320.5\n",
      "lif layer 2 self.abs_max_v: 3724.0\n",
      "lif layer 2 self.abs_max_v: 3790.0\n",
      "smallest_now_T updated: 73\n",
      "fc layer 2 self.abs_max_out: 2611.0\n",
      "lif layer 1 self.abs_max_v: 5460.0\n",
      "lif layer 2 self.abs_max_v: 3812.0\n",
      "fc layer 2 self.abs_max_out: 2662.0\n",
      "lif layer 2 self.abs_max_v: 4027.0\n",
      "fc layer 2 self.abs_max_out: 2733.0\n",
      "smallest_now_T updated: 65\n",
      "lif layer 2 self.abs_max_v: 4050.5\n",
      "fc layer 2 self.abs_max_out: 2775.0\n",
      "lif layer 2 self.abs_max_v: 4496.5\n",
      "lif layer 2 self.abs_max_v: 4635.5\n",
      "fc layer 3 self.abs_max_out: 1394.0\n",
      "lif layer 1 self.abs_max_v: 5478.0\n",
      "fc layer 2 self.abs_max_out: 2933.0\n",
      "lif layer 2 self.abs_max_v: 4891.0\n",
      "lif layer 2 self.abs_max_v: 5307.5\n",
      "fc layer 3 self.abs_max_out: 1444.0\n",
      "fc layer 2 self.abs_max_out: 2950.0\n",
      "fc layer 3 self.abs_max_out: 1491.0\n",
      "fc layer 3 self.abs_max_out: 1558.0\n",
      "fc layer 2 self.abs_max_out: 2953.0\n",
      "fc layer 2 self.abs_max_out: 3077.0\n",
      "fc layer 1 self.abs_max_out: 4451.0\n",
      "smallest_now_T updated: 56\n",
      "smallest_now_T updated: 43\n",
      "fc layer 3 self.abs_max_out: 1695.0\n",
      "fc layer 1 self.abs_max_out: 5012.0\n",
      "fc layer 3 self.abs_max_out: 2162.0\n",
      "lif layer 1 self.abs_max_v: 5550.0\n",
      "fc layer 1 self.abs_max_out: 5326.0\n",
      "lif layer 1 self.abs_max_v: 5720.0\n",
      "lif layer 1 self.abs_max_v: 6165.0\n",
      "lif layer 1 self.abs_max_v: 7164.5\n",
      "lif layer 1 self.abs_max_v: 7408.5\n",
      "lif layer 1 self.abs_max_v: 7494.5\n",
      "fc layer 2 self.abs_max_out: 3119.0\n",
      "lif layer 2 self.abs_max_v: 5427.5\n",
      "fc layer 2 self.abs_max_out: 3142.0\n",
      "fc layer 2 self.abs_max_out: 3230.0\n",
      "lif layer 2 self.abs_max_v: 5610.5\n",
      "fc layer 1 self.abs_max_out: 5506.0\n",
      "lif layer 1 self.abs_max_v: 8556.0\n",
      "fc layer 1 self.abs_max_out: 5538.0\n",
      "fc layer 1 self.abs_max_out: 5830.0\n",
      "lif layer 1 self.abs_max_v: 9476.5\n",
      "fc layer 2 self.abs_max_out: 3258.0\n",
      "fc layer 2 self.abs_max_out: 3417.0\n",
      "fc layer 1 self.abs_max_out: 5914.0\n",
      "fc layer 1 self.abs_max_out: 7000.0\n",
      "smallest_now_T_val updated: 129\n",
      "fc layer 1 self.abs_max_out: 7041.0\n",
      "smallest_now_T_val updated: 106\n",
      "smallest_now_T_val updated: 104\n",
      "smallest_now_T_val updated: 102\n",
      "fc layer 2 self.abs_max_out: 3420.0\n",
      "smallest_now_T_val updated: 85\n",
      "smallest_now_T_val updated: 29\n",
      "fc layer 2 self.abs_max_out: 3706.0\n",
      "fc layer 1 self.abs_max_out: 7164.0\n",
      "fc layer 1 self.abs_max_out: 7641.0\n",
      "lif layer 1 self.abs_max_v: 10197.5\n",
      "lif layer 1 self.abs_max_v: 10850.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.692590/  1.997128, val:  22.08%, val_best:  22.08%, tr:  91.73%, tr_best:  91.73%, epoch time: 41.03 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6880%\n",
      "layer   2  Sparsity: 74.3390%\n",
      "layer   3  Sparsity: 74.0619%\n",
      "total_backward_count 4895 real_backward_count 1243  25.393%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 2188.0\n",
      "fc layer 2 self.abs_max_out: 3763.0\n",
      "fc layer 2 self.abs_max_out: 3879.0\n",
      "lif layer 2 self.abs_max_v: 5972.0\n",
      "lif layer 2 self.abs_max_v: 6206.0\n",
      "lif layer 1 self.abs_max_v: 10865.5\n",
      "lif layer 1 self.abs_max_v: 11003.5\n",
      "lif layer 1 self.abs_max_v: 11476.0\n",
      "fc layer 1 self.abs_max_out: 7938.0\n",
      "fc layer 1 self.abs_max_out: 8131.0\n",
      "lif layer 1 self.abs_max_v: 12100.0\n",
      "lif layer 1 self.abs_max_v: 13954.0\n",
      "lif layer 1 self.abs_max_v: 15020.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.584323/  1.893673, val:  38.75%, val_best:  38.75%, tr:  92.44%, tr_best:  92.44%, epoch time: 39.72 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6822%\n",
      "layer   2  Sparsity: 76.4154%\n",
      "layer   3  Sparsity: 75.5532%\n",
      "total_backward_count 9790 real_backward_count 2271  23.197%\n",
      "fc layer 2 self.abs_max_out: 4060.0\n",
      "lif layer 2 self.abs_max_v: 6628.0\n",
      "lif layer 2 self.abs_max_v: 6911.0\n",
      "fc layer 3 self.abs_max_out: 2223.0\n",
      "fc layer 3 self.abs_max_out: 2233.0\n",
      "fc layer 1 self.abs_max_out: 8197.0\n",
      "fc layer 1 self.abs_max_out: 8718.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.575196/  1.847656, val:  50.83%, val_best:  50.83%, tr:  93.56%, tr_best:  93.56%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7310%\n",
      "layer   2  Sparsity: 77.4916%\n",
      "layer   3  Sparsity: 76.6398%\n",
      "total_backward_count 14685 real_backward_count 3278  22.322%\n",
      "fc layer 3 self.abs_max_out: 2311.0\n",
      "fc layer 2 self.abs_max_out: 4116.0\n",
      "fc layer 3 self.abs_max_out: 2343.0\n",
      "fc layer 3 self.abs_max_out: 2442.0\n",
      "fc layer 3 self.abs_max_out: 2464.0\n",
      "fc layer 3 self.abs_max_out: 2596.0\n",
      "fc layer 3 self.abs_max_out: 2668.0\n",
      "fc layer 1 self.abs_max_out: 9423.0\n",
      "lif layer 1 self.abs_max_v: 15740.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.540440/  1.814499, val:  38.33%, val_best:  50.83%, tr:  94.89%, tr_best:  94.89%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7149%\n",
      "layer   2  Sparsity: 78.6368%\n",
      "layer   3  Sparsity: 77.1874%\n",
      "total_backward_count 19580 real_backward_count 4288  21.900%\n",
      "fc layer 1 self.abs_max_out: 10519.0\n",
      "fc layer 1 self.abs_max_out: 11014.0\n",
      "lif layer 1 self.abs_max_v: 16273.5\n",
      "lif layer 1 self.abs_max_v: 18415.0\n",
      "lif layer 1 self.abs_max_v: 19215.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.569413/  1.851538, val:  43.33%, val_best:  50.83%, tr:  93.87%, tr_best:  94.89%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6562%\n",
      "layer   2  Sparsity: 79.4058%\n",
      "layer   3  Sparsity: 79.9463%\n",
      "total_backward_count 24475 real_backward_count 5303  21.667%\n",
      "fc layer 2 self.abs_max_out: 4270.0\n",
      "fc layer 1 self.abs_max_out: 11041.0\n",
      "lif layer 1 self.abs_max_v: 19424.0\n",
      "lif layer 1 self.abs_max_v: 19682.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.607143/  1.797082, val:  42.92%, val_best:  50.83%, tr:  93.26%, tr_best:  94.89%, epoch time: 39.40 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7218%\n",
      "layer   2  Sparsity: 79.6609%\n",
      "layer   3  Sparsity: 80.5972%\n",
      "total_backward_count 29370 real_backward_count 6341  21.590%\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.553953/  1.805677, val:  43.33%, val_best:  50.83%, tr:  93.87%, tr_best:  94.89%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6972%\n",
      "layer   2  Sparsity: 79.0098%\n",
      "layer   3  Sparsity: 80.6993%\n",
      "total_backward_count 34265 real_backward_count 7306  21.322%\n",
      "fc layer 1 self.abs_max_out: 11115.0\n",
      "lif layer 1 self.abs_max_v: 19797.0\n",
      "lif layer 1 self.abs_max_v: 20020.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.580116/  1.802690, val:  54.17%, val_best:  54.17%, tr:  95.61%, tr_best:  95.61%, epoch time: 39.60 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7120%\n",
      "layer   2  Sparsity: 78.5253%\n",
      "layer   3  Sparsity: 81.6977%\n",
      "total_backward_count 39160 real_backward_count 8237  21.034%\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.630764/  1.798908, val:  50.83%, val_best:  54.17%, tr:  94.69%, tr_best:  95.61%, epoch time: 40.07 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6878%\n",
      "layer   2  Sparsity: 79.1464%\n",
      "layer   3  Sparsity: 83.4995%\n",
      "total_backward_count 44055 real_backward_count 9218  20.924%\n",
      "fc layer 1 self.abs_max_out: 11259.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.614611/  1.825881, val:  51.67%, val_best:  54.17%, tr:  94.79%, tr_best:  95.61%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7217%\n",
      "layer   2  Sparsity: 79.3841%\n",
      "layer   3  Sparsity: 83.8049%\n",
      "total_backward_count 48950 real_backward_count 10168  20.772%\n",
      "fc layer 1 self.abs_max_out: 11356.0\n",
      "fc layer 1 self.abs_max_out: 12174.0\n",
      "lif layer 1 self.abs_max_v: 21544.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.601331/  1.867828, val:  43.33%, val_best:  54.17%, tr:  96.32%, tr_best:  96.32%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6601%\n",
      "layer   2  Sparsity: 79.8839%\n",
      "layer   3  Sparsity: 83.5198%\n",
      "total_backward_count 53845 real_backward_count 11068  20.555%\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.606899/  1.817828, val:  53.75%, val_best:  54.17%, tr:  95.40%, tr_best:  96.32%, epoch time: 39.42 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6747%\n",
      "layer   2  Sparsity: 79.9761%\n",
      "layer   3  Sparsity: 83.2973%\n",
      "total_backward_count 58740 real_backward_count 11958  20.358%\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.635691/  1.856358, val:  58.75%, val_best:  58.75%, tr:  94.69%, tr_best:  96.32%, epoch time: 39.58 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6668%\n",
      "layer   2  Sparsity: 79.9290%\n",
      "layer   3  Sparsity: 85.1797%\n",
      "total_backward_count 63635 real_backward_count 12894  20.262%\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.647787/  1.856874, val:  50.42%, val_best:  58.75%, tr:  94.38%, tr_best:  96.32%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6796%\n",
      "layer   2  Sparsity: 80.3160%\n",
      "layer   3  Sparsity: 85.1387%\n",
      "total_backward_count 68530 real_backward_count 13790  20.123%\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.647614/  1.828286, val:  53.33%, val_best:  58.75%, tr:  94.28%, tr_best:  96.32%, epoch time: 39.87 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7314%\n",
      "layer   2  Sparsity: 79.5996%\n",
      "layer   3  Sparsity: 85.3415%\n",
      "total_backward_count 73425 real_backward_count 14683  19.997%\n",
      "fc layer 1 self.abs_max_out: 12233.0\n",
      "lif layer 1 self.abs_max_v: 21616.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.636756/  1.796705, val:  52.50%, val_best:  58.75%, tr:  96.02%, tr_best:  96.32%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6899%\n",
      "layer   2  Sparsity: 79.1951%\n",
      "layer   3  Sparsity: 85.3801%\n",
      "total_backward_count 78320 real_backward_count 15587  19.902%\n",
      "fc layer 2 self.abs_max_out: 4375.0\n",
      "fc layer 2 self.abs_max_out: 4560.0\n",
      "lif layer 2 self.abs_max_v: 7022.0\n",
      "lif layer 2 self.abs_max_v: 7047.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.620580/  1.814464, val:  65.83%, val_best:  65.83%, tr:  95.51%, tr_best:  96.32%, epoch time: 39.64 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6674%\n",
      "layer   2  Sparsity: 80.1156%\n",
      "layer   3  Sparsity: 84.9426%\n",
      "total_backward_count 83215 real_backward_count 16436  19.751%\n",
      "fc layer 1 self.abs_max_out: 12947.0\n",
      "fc layer 1 self.abs_max_out: 13660.0\n",
      "lif layer 1 self.abs_max_v: 23014.0\n",
      "fc layer 1 self.abs_max_out: 13982.0\n",
      "lif layer 1 self.abs_max_v: 25489.0\n",
      "lif layer 1 self.abs_max_v: 25825.5\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.628145/  1.850757, val:  57.50%, val_best:  65.83%, tr:  95.61%, tr_best:  96.32%, epoch time: 39.43 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6796%\n",
      "layer   2  Sparsity: 80.5331%\n",
      "layer   3  Sparsity: 85.0614%\n",
      "total_backward_count 88110 real_backward_count 17327  19.665%\n",
      "lif layer 2 self.abs_max_v: 7224.0\n",
      "lif layer 2 self.abs_max_v: 7335.0\n",
      "fc layer 1 self.abs_max_out: 14442.0\n",
      "fc layer 1 self.abs_max_out: 14682.0\n",
      "lif layer 1 self.abs_max_v: 26846.5\n",
      "lif layer 1 self.abs_max_v: 27308.5\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.622874/  1.813632, val:  53.75%, val_best:  65.83%, tr:  95.40%, tr_best:  96.32%, epoch time: 39.90 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7023%\n",
      "layer   2  Sparsity: 80.6560%\n",
      "layer   3  Sparsity: 84.7006%\n",
      "total_backward_count 93005 real_backward_count 18255  19.628%\n",
      "fc layer 2 self.abs_max_out: 4577.0\n",
      "fc layer 1 self.abs_max_out: 14812.0\n",
      "fc layer 1 self.abs_max_out: 15048.0\n",
      "lif layer 1 self.abs_max_v: 27528.0\n",
      "lif layer 1 self.abs_max_v: 28061.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.597608/  1.805510, val:  48.75%, val_best:  65.83%, tr:  96.22%, tr_best:  96.32%, epoch time: 39.60 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7102%\n",
      "layer   2  Sparsity: 80.6419%\n",
      "layer   3  Sparsity: 84.3849%\n",
      "total_backward_count 97900 real_backward_count 19104  19.514%\n",
      "fc layer 1 self.abs_max_out: 15072.0\n",
      "fc layer 1 self.abs_max_out: 15238.0\n",
      "lif layer 1 self.abs_max_v: 28443.5\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.570691/  1.736773, val:  60.00%, val_best:  65.83%, tr:  96.53%, tr_best:  96.53%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6775%\n",
      "layer   2  Sparsity: 80.3952%\n",
      "layer   3  Sparsity: 83.4597%\n",
      "total_backward_count 102795 real_backward_count 19898  19.357%\n",
      "fc layer 1 self.abs_max_out: 15395.0\n",
      "lif layer 1 self.abs_max_v: 28773.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.551453/  1.794680, val:  53.75%, val_best:  65.83%, tr:  96.73%, tr_best:  96.73%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7039%\n",
      "layer   2  Sparsity: 80.9046%\n",
      "layer   3  Sparsity: 83.3221%\n",
      "total_backward_count 107690 real_backward_count 20766  19.283%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.558184/  1.714131, val:  57.50%, val_best:  65.83%, tr:  96.42%, tr_best:  96.73%, epoch time: 39.53 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6829%\n",
      "layer   2  Sparsity: 80.8485%\n",
      "layer   3  Sparsity: 82.8598%\n",
      "total_backward_count 112585 real_backward_count 21632  19.214%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.543959/  1.718731, val:  63.75%, val_best:  65.83%, tr:  96.42%, tr_best:  96.73%, epoch time: 39.29 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.6641%\n",
      "layer   2  Sparsity: 80.4865%\n",
      "layer   3  Sparsity: 82.8857%\n",
      "total_backward_count 117480 real_backward_count 22433  19.095%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.525238/  1.781950, val:  52.92%, val_best:  65.83%, tr:  96.53%, tr_best:  96.73%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6752%\n",
      "layer   2  Sparsity: 80.6929%\n",
      "layer   3  Sparsity: 81.5122%\n",
      "total_backward_count 122375 real_backward_count 23267  19.013%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.561133/  1.764709, val:  70.42%, val_best:  70.42%, tr:  96.63%, tr_best:  96.73%, epoch time: 39.43 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7055%\n",
      "layer   2  Sparsity: 80.9190%\n",
      "layer   3  Sparsity: 83.4817%\n",
      "total_backward_count 127270 real_backward_count 24164  18.986%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.575523/  1.784779, val:  59.17%, val_best:  70.42%, tr:  96.53%, tr_best:  96.73%, epoch time: 39.81 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7079%\n",
      "layer   2  Sparsity: 81.0513%\n",
      "layer   3  Sparsity: 84.6164%\n",
      "total_backward_count 132165 real_backward_count 25001  18.917%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.580770/  1.754526, val:  59.17%, val_best:  70.42%, tr:  96.94%, tr_best:  96.94%, epoch time: 39.78 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6996%\n",
      "layer   2  Sparsity: 81.0879%\n",
      "layer   3  Sparsity: 84.4966%\n",
      "total_backward_count 137060 real_backward_count 25818  18.837%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.557643/  1.786786, val:  55.00%, val_best:  70.42%, tr:  96.63%, tr_best:  96.94%, epoch time: 39.60 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7068%\n",
      "layer   2  Sparsity: 81.0516%\n",
      "layer   3  Sparsity: 84.4570%\n",
      "total_backward_count 141955 real_backward_count 26626  18.757%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.587047/  1.747658, val:  61.67%, val_best:  70.42%, tr:  95.61%, tr_best:  96.94%, epoch time: 39.03 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.7082%\n",
      "layer   2  Sparsity: 81.2185%\n",
      "layer   3  Sparsity: 85.8887%\n",
      "total_backward_count 146850 real_backward_count 27451  18.693%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.575038/  1.768345, val:  63.75%, val_best:  70.42%, tr:  97.14%, tr_best:  97.14%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6671%\n",
      "layer   2  Sparsity: 81.3655%\n",
      "layer   3  Sparsity: 85.2044%\n",
      "total_backward_count 151745 real_backward_count 28243  18.612%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.596603/  1.764704, val:  56.67%, val_best:  70.42%, tr:  95.81%, tr_best:  97.14%, epoch time: 39.03 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.7276%\n",
      "layer   2  Sparsity: 80.9587%\n",
      "layer   3  Sparsity: 84.8495%\n",
      "total_backward_count 156640 real_backward_count 29068  18.557%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.578332/  1.793319, val:  56.67%, val_best:  70.42%, tr:  96.53%, tr_best:  97.14%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6771%\n",
      "layer   2  Sparsity: 81.1670%\n",
      "layer   3  Sparsity: 84.7132%\n",
      "total_backward_count 161535 real_backward_count 29866  18.489%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.552297/  1.716123, val:  64.58%, val_best:  70.42%, tr:  97.65%, tr_best:  97.65%, epoch time: 39.98 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7001%\n",
      "layer   2  Sparsity: 81.8501%\n",
      "layer   3  Sparsity: 84.6234%\n",
      "total_backward_count 166430 real_backward_count 30652  18.417%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.534851/  1.716860, val:  65.00%, val_best:  70.42%, tr:  96.63%, tr_best:  97.65%, epoch time: 39.80 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7388%\n",
      "layer   2  Sparsity: 81.5969%\n",
      "layer   3  Sparsity: 83.9793%\n",
      "total_backward_count 171325 real_backward_count 31433  18.347%\n",
      "fc layer 2 self.abs_max_out: 4663.0\n",
      "lif layer 2 self.abs_max_v: 7703.0\n",
      "lif layer 2 self.abs_max_v: 7822.5\n",
      "lif layer 2 self.abs_max_v: 8251.5\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.523116/  1.724005, val:  72.50%, val_best:  72.50%, tr:  96.94%, tr_best:  97.65%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6794%\n",
      "layer   2  Sparsity: 81.2781%\n",
      "layer   3  Sparsity: 83.4496%\n",
      "total_backward_count 176220 real_backward_count 32180  18.261%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.514192/  1.716649, val:  70.00%, val_best:  72.50%, tr:  97.24%, tr_best:  97.65%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7464%\n",
      "layer   2  Sparsity: 80.8961%\n",
      "layer   3  Sparsity: 83.8349%\n",
      "total_backward_count 181115 real_backward_count 32888  18.159%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.498154/  1.734728, val:  63.75%, val_best:  72.50%, tr:  98.37%, tr_best:  98.37%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7172%\n",
      "layer   2  Sparsity: 81.0831%\n",
      "layer   3  Sparsity: 83.9429%\n",
      "total_backward_count 186010 real_backward_count 33568  18.046%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.520198/  1.715877, val:  73.33%, val_best:  73.33%, tr:  96.73%, tr_best:  98.37%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6940%\n",
      "layer   2  Sparsity: 81.3808%\n",
      "layer   3  Sparsity: 83.4450%\n",
      "total_backward_count 190905 real_backward_count 34382  18.010%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.506434/  1.720654, val:  63.75%, val_best:  73.33%, tr:  97.04%, tr_best:  98.37%, epoch time: 39.98 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7063%\n",
      "layer   2  Sparsity: 81.7074%\n",
      "layer   3  Sparsity: 83.5496%\n",
      "total_backward_count 195800 real_backward_count 35131  17.942%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.512418/  1.707318, val:  64.17%, val_best:  73.33%, tr:  96.53%, tr_best:  98.37%, epoch time: 39.73 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7089%\n",
      "layer   2  Sparsity: 81.8893%\n",
      "layer   3  Sparsity: 83.3099%\n",
      "total_backward_count 200695 real_backward_count 35884  17.880%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.486668/  1.760235, val:  60.00%, val_best:  73.33%, tr:  97.65%, tr_best:  98.37%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7072%\n",
      "layer   2  Sparsity: 81.9088%\n",
      "layer   3  Sparsity: 83.6939%\n",
      "total_backward_count 205590 real_backward_count 36622  17.813%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.492588/  1.723380, val:  70.42%, val_best:  73.33%, tr:  97.85%, tr_best:  98.37%, epoch time: 39.34 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7026%\n",
      "layer   2  Sparsity: 81.7944%\n",
      "layer   3  Sparsity: 83.5033%\n",
      "total_backward_count 210485 real_backward_count 37309  17.725%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.462722/  1.657029, val:  69.17%, val_best:  73.33%, tr:  98.16%, tr_best:  98.37%, epoch time: 39.80 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6801%\n",
      "layer   2  Sparsity: 81.7960%\n",
      "layer   3  Sparsity: 82.6954%\n",
      "total_backward_count 215380 real_backward_count 38007  17.646%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.471769/  1.682453, val:  64.58%, val_best:  73.33%, tr:  96.94%, tr_best:  98.37%, epoch time: 39.63 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7398%\n",
      "layer   2  Sparsity: 81.8282%\n",
      "layer   3  Sparsity: 82.9016%\n",
      "total_backward_count 220275 real_backward_count 38747  17.590%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.465819/  1.678322, val:  72.08%, val_best:  73.33%, tr:  97.04%, tr_best:  98.37%, epoch time: 40.16 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6870%\n",
      "layer   2  Sparsity: 81.5705%\n",
      "layer   3  Sparsity: 82.9223%\n",
      "total_backward_count 225170 real_backward_count 39510  17.547%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.470345/  1.710219, val:  63.33%, val_best:  73.33%, tr:  97.75%, tr_best:  98.37%, epoch time: 38.98 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.6902%\n",
      "layer   2  Sparsity: 81.4339%\n",
      "layer   3  Sparsity: 83.6842%\n",
      "total_backward_count 230065 real_backward_count 40262  17.500%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.466417/  1.706114, val:  61.67%, val_best:  73.33%, tr:  97.04%, tr_best:  98.37%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6937%\n",
      "layer   2  Sparsity: 82.0080%\n",
      "layer   3  Sparsity: 83.6990%\n",
      "total_backward_count 234960 real_backward_count 40969  17.437%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.483700/  1.692397, val:  72.08%, val_best:  73.33%, tr:  97.34%, tr_best:  98.37%, epoch time: 39.97 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7021%\n",
      "layer   2  Sparsity: 81.8105%\n",
      "layer   3  Sparsity: 84.2131%\n",
      "total_backward_count 239855 real_backward_count 41680  17.377%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.499324/  1.719695, val:  72.50%, val_best:  73.33%, tr:  96.63%, tr_best:  98.37%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6828%\n",
      "layer   2  Sparsity: 82.0016%\n",
      "layer   3  Sparsity: 83.8801%\n",
      "total_backward_count 244750 real_backward_count 42351  17.304%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.474376/  1.689159, val:  58.75%, val_best:  73.33%, tr:  97.65%, tr_best:  98.37%, epoch time: 39.65 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7324%\n",
      "layer   2  Sparsity: 82.1139%\n",
      "layer   3  Sparsity: 82.4117%\n",
      "total_backward_count 249645 real_backward_count 43056  17.247%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.450467/  1.657258, val:  72.92%, val_best:  73.33%, tr:  98.67%, tr_best:  98.67%, epoch time: 39.54 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7105%\n",
      "layer   2  Sparsity: 82.5778%\n",
      "layer   3  Sparsity: 82.6616%\n",
      "total_backward_count 254540 real_backward_count 43759  17.191%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.436985/  1.674587, val:  68.75%, val_best:  73.33%, tr:  97.75%, tr_best:  98.67%, epoch time: 39.54 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6905%\n",
      "layer   2  Sparsity: 83.1820%\n",
      "layer   3  Sparsity: 83.0024%\n",
      "total_backward_count 259435 real_backward_count 44463  17.138%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.435496/  1.566662, val:  84.17%, val_best:  84.17%, tr:  97.34%, tr_best:  98.67%, epoch time: 39.43 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6896%\n",
      "layer   2  Sparsity: 82.7825%\n",
      "layer   3  Sparsity: 82.5512%\n",
      "total_backward_count 264330 real_backward_count 45118  17.069%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.438919/  1.654166, val:  76.67%, val_best:  84.17%, tr:  97.55%, tr_best:  98.67%, epoch time: 39.89 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7095%\n",
      "layer   2  Sparsity: 83.4571%\n",
      "layer   3  Sparsity: 82.8071%\n",
      "total_backward_count 269225 real_backward_count 45782  17.005%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.473261/  1.714741, val:  71.25%, val_best:  84.17%, tr:  98.26%, tr_best:  98.67%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6694%\n",
      "layer   2  Sparsity: 83.4534%\n",
      "layer   3  Sparsity: 84.6624%\n",
      "total_backward_count 274120 real_backward_count 46465  16.951%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.507158/  1.729443, val:  57.50%, val_best:  84.17%, tr:  97.55%, tr_best:  98.67%, epoch time: 40.13 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7038%\n",
      "layer   2  Sparsity: 82.8822%\n",
      "layer   3  Sparsity: 84.5784%\n",
      "total_backward_count 279015 real_backward_count 47122  16.889%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.472685/  1.638842, val:  78.75%, val_best:  84.17%, tr:  98.26%, tr_best:  98.67%, epoch time: 39.46 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6597%\n",
      "layer   2  Sparsity: 82.5153%\n",
      "layer   3  Sparsity: 83.5148%\n",
      "total_backward_count 283910 real_backward_count 47770  16.826%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.419336/  1.612906, val:  77.92%, val_best:  84.17%, tr:  98.06%, tr_best:  98.67%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7127%\n",
      "layer   2  Sparsity: 82.9208%\n",
      "layer   3  Sparsity: 83.0061%\n",
      "total_backward_count 288805 real_backward_count 48372  16.749%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.440225/  1.697945, val:  66.25%, val_best:  84.17%, tr:  97.04%, tr_best:  98.67%, epoch time: 39.26 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.6816%\n",
      "layer   2  Sparsity: 82.5945%\n",
      "layer   3  Sparsity: 84.3823%\n",
      "total_backward_count 293700 real_backward_count 48995  16.682%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.471813/  1.705137, val:  60.00%, val_best:  84.17%, tr:  97.45%, tr_best:  98.67%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6935%\n",
      "layer   2  Sparsity: 82.4888%\n",
      "layer   3  Sparsity: 84.4841%\n",
      "total_backward_count 298595 real_backward_count 49644  16.626%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.465760/  1.629910, val:  71.67%, val_best:  84.17%, tr:  97.75%, tr_best:  98.67%, epoch time: 39.49 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6991%\n",
      "layer   2  Sparsity: 82.3263%\n",
      "layer   3  Sparsity: 84.3937%\n",
      "total_backward_count 303490 real_backward_count 50311  16.577%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.448653/  1.712287, val:  69.58%, val_best:  84.17%, tr:  97.55%, tr_best:  98.67%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6890%\n",
      "layer   2  Sparsity: 82.3942%\n",
      "layer   3  Sparsity: 84.4701%\n",
      "total_backward_count 308385 real_backward_count 50958  16.524%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.495159/  1.733496, val:  67.08%, val_best:  84.17%, tr:  98.26%, tr_best:  98.67%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6414%\n",
      "layer   2  Sparsity: 82.6951%\n",
      "layer   3  Sparsity: 84.7912%\n",
      "total_backward_count 313280 real_backward_count 51561  16.458%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.491746/  1.688019, val:  66.67%, val_best:  84.17%, tr:  98.16%, tr_best:  98.67%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6844%\n",
      "layer   2  Sparsity: 82.6730%\n",
      "layer   3  Sparsity: 84.3017%\n",
      "total_backward_count 318175 real_backward_count 52215  16.411%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.449807/  1.659190, val:  72.08%, val_best:  84.17%, tr:  97.96%, tr_best:  98.67%, epoch time: 39.22 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.7062%\n",
      "layer   2  Sparsity: 83.4662%\n",
      "layer   3  Sparsity: 84.0291%\n",
      "total_backward_count 323070 real_backward_count 52868  16.364%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.490653/  1.688403, val:  70.42%, val_best:  84.17%, tr:  98.06%, tr_best:  98.67%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7528%\n",
      "layer   2  Sparsity: 83.7420%\n",
      "layer   3  Sparsity: 84.9098%\n",
      "total_backward_count 327965 real_backward_count 53548  16.327%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.522374/  1.690438, val:  70.83%, val_best:  84.17%, tr:  96.94%, tr_best:  98.67%, epoch time: 39.46 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7145%\n",
      "layer   2  Sparsity: 82.6415%\n",
      "layer   3  Sparsity: 85.2490%\n",
      "total_backward_count 332860 real_backward_count 54278  16.307%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.481023/  1.660450, val:  75.42%, val_best:  84.17%, tr:  98.77%, tr_best:  98.77%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6955%\n",
      "layer   2  Sparsity: 83.0966%\n",
      "layer   3  Sparsity: 84.4690%\n",
      "total_backward_count 337755 real_backward_count 54923  16.261%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.431828/  1.692750, val:  53.33%, val_best:  84.17%, tr:  97.85%, tr_best:  98.77%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6702%\n",
      "layer   2  Sparsity: 83.2838%\n",
      "layer   3  Sparsity: 83.4309%\n",
      "total_backward_count 342650 real_backward_count 55541  16.209%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.462287/  1.681816, val:  72.50%, val_best:  84.17%, tr:  97.65%, tr_best:  98.77%, epoch time: 39.82 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6991%\n",
      "layer   2  Sparsity: 83.6856%\n",
      "layer   3  Sparsity: 84.6000%\n",
      "total_backward_count 347545 real_backward_count 56159  16.159%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.436428/  1.695148, val:  62.08%, val_best:  84.17%, tr:  98.67%, tr_best:  98.77%, epoch time: 39.22 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.7000%\n",
      "layer   2  Sparsity: 83.6121%\n",
      "layer   3  Sparsity: 84.0553%\n",
      "total_backward_count 352440 real_backward_count 56741  16.099%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.438210/  1.667620, val:  77.08%, val_best:  84.17%, tr:  97.85%, tr_best:  98.77%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7109%\n",
      "layer   2  Sparsity: 83.1222%\n",
      "layer   3  Sparsity: 83.6025%\n",
      "total_backward_count 357335 real_backward_count 57396  16.062%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.438197/  1.649248, val:  67.08%, val_best:  84.17%, tr:  98.77%, tr_best:  98.77%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6826%\n",
      "layer   2  Sparsity: 83.0205%\n",
      "layer   3  Sparsity: 83.6402%\n",
      "total_backward_count 362230 real_backward_count 58017  16.017%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.440907/  1.660617, val:  65.00%, val_best:  84.17%, tr:  98.47%, tr_best:  98.77%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7234%\n",
      "layer   2  Sparsity: 83.0942%\n",
      "layer   3  Sparsity: 83.6272%\n",
      "total_backward_count 367125 real_backward_count 58654  15.977%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.420657/  1.647354, val:  72.92%, val_best:  84.17%, tr:  97.85%, tr_best:  98.77%, epoch time: 39.64 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6638%\n",
      "layer   2  Sparsity: 83.6121%\n",
      "layer   3  Sparsity: 83.1938%\n",
      "total_backward_count 372020 real_backward_count 59261  15.930%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.422453/  1.657434, val:  63.75%, val_best:  84.17%, tr:  97.96%, tr_best:  98.77%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7301%\n",
      "layer   2  Sparsity: 83.8641%\n",
      "layer   3  Sparsity: 82.6126%\n",
      "total_backward_count 376915 real_backward_count 59900  15.892%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.431134/  1.717831, val:  70.42%, val_best:  84.17%, tr:  97.75%, tr_best:  98.77%, epoch time: 39.49 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7235%\n",
      "layer   2  Sparsity: 83.9235%\n",
      "layer   3  Sparsity: 83.0912%\n",
      "total_backward_count 381810 real_backward_count 60504  15.847%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.451785/  1.690994, val:  70.00%, val_best:  84.17%, tr:  98.06%, tr_best:  98.77%, epoch time: 39.42 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6960%\n",
      "layer   2  Sparsity: 84.2286%\n",
      "layer   3  Sparsity: 83.5009%\n",
      "total_backward_count 386705 real_backward_count 61136  15.809%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.418703/  1.630501, val:  79.58%, val_best:  84.17%, tr:  98.37%, tr_best:  98.77%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7137%\n",
      "layer   2  Sparsity: 84.3978%\n",
      "layer   3  Sparsity: 83.8636%\n",
      "total_backward_count 391600 real_backward_count 61736  15.765%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.416957/  1.674994, val:  71.25%, val_best:  84.17%, tr:  97.96%, tr_best:  98.77%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7087%\n",
      "layer   2  Sparsity: 84.6248%\n",
      "layer   3  Sparsity: 84.8051%\n",
      "total_backward_count 396495 real_backward_count 62359  15.728%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.446552/  1.698282, val:  62.08%, val_best:  84.17%, tr:  97.65%, tr_best:  98.77%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7193%\n",
      "layer   2  Sparsity: 84.1364%\n",
      "layer   3  Sparsity: 84.5769%\n",
      "total_backward_count 401390 real_backward_count 62997  15.695%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.439215/  1.624477, val:  73.75%, val_best:  84.17%, tr:  98.37%, tr_best:  98.77%, epoch time: 39.73 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6993%\n",
      "layer   2  Sparsity: 83.6578%\n",
      "layer   3  Sparsity: 84.1769%\n",
      "total_backward_count 406285 real_backward_count 63593  15.652%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.426930/  1.691402, val:  58.33%, val_best:  84.17%, tr:  98.37%, tr_best:  98.77%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6948%\n",
      "layer   2  Sparsity: 83.9938%\n",
      "layer   3  Sparsity: 84.2487%\n",
      "total_backward_count 411180 real_backward_count 64194  15.612%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.454978/  1.633340, val:  71.67%, val_best:  84.17%, tr:  97.14%, tr_best:  98.77%, epoch time: 39.24 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.6724%\n",
      "layer   2  Sparsity: 84.0586%\n",
      "layer   3  Sparsity: 84.7197%\n",
      "total_backward_count 416075 real_backward_count 64819  15.579%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.424339/  1.654930, val:  72.92%, val_best:  84.17%, tr:  97.65%, tr_best:  98.77%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7241%\n",
      "layer   2  Sparsity: 84.8125%\n",
      "layer   3  Sparsity: 83.6017%\n",
      "total_backward_count 420970 real_backward_count 65472  15.553%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.434533/  1.597235, val:  61.25%, val_best:  84.17%, tr:  97.55%, tr_best:  98.77%, epoch time: 39.23 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.7226%\n",
      "layer   2  Sparsity: 84.5281%\n",
      "layer   3  Sparsity: 83.7039%\n",
      "total_backward_count 425865 real_backward_count 66078  15.516%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.407105/  1.608898, val:  80.42%, val_best:  84.17%, tr:  97.85%, tr_best:  98.77%, epoch time: 39.72 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7395%\n",
      "layer   2  Sparsity: 84.3267%\n",
      "layer   3  Sparsity: 83.4178%\n",
      "total_backward_count 430760 real_backward_count 66747  15.495%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.455179/  1.610057, val:  76.67%, val_best:  84.17%, tr:  97.34%, tr_best:  98.77%, epoch time: 39.44 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6898%\n",
      "layer   2  Sparsity: 84.6688%\n",
      "layer   3  Sparsity: 84.0308%\n",
      "total_backward_count 435655 real_backward_count 67397  15.470%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.446374/  1.636048, val:  77.50%, val_best:  84.17%, tr:  98.98%, tr_best:  98.98%, epoch time: 39.27 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.6876%\n",
      "layer   2  Sparsity: 84.4643%\n",
      "layer   3  Sparsity: 84.5918%\n",
      "total_backward_count 440550 real_backward_count 68007  15.437%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.450793/  1.680204, val:  75.83%, val_best:  84.17%, tr:  97.04%, tr_best:  98.98%, epoch time: 39.46 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7156%\n",
      "layer   2  Sparsity: 84.6737%\n",
      "layer   3  Sparsity: 84.7450%\n",
      "total_backward_count 445445 real_backward_count 68674  15.417%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.457285/  1.659620, val:  77.92%, val_best:  84.17%, tr:  97.85%, tr_best:  98.98%, epoch time: 39.26 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.7014%\n",
      "layer   2  Sparsity: 84.3892%\n",
      "layer   3  Sparsity: 84.7594%\n",
      "total_backward_count 450340 real_backward_count 69314  15.391%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.439087/  1.627691, val:  77.08%, val_best:  84.17%, tr:  97.34%, tr_best:  98.98%, epoch time: 39.48 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6680%\n",
      "layer   2  Sparsity: 84.2053%\n",
      "layer   3  Sparsity: 84.4558%\n",
      "total_backward_count 455235 real_backward_count 69918  15.359%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.440159/  1.624264, val:  81.25%, val_best:  84.17%, tr:  97.85%, tr_best:  98.98%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7071%\n",
      "layer   2  Sparsity: 84.5672%\n",
      "layer   3  Sparsity: 84.7196%\n",
      "total_backward_count 460130 real_backward_count 70526  15.327%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.464331/  1.656951, val:  68.75%, val_best:  84.17%, tr:  97.65%, tr_best:  98.98%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6692%\n",
      "layer   2  Sparsity: 84.0361%\n",
      "layer   3  Sparsity: 84.4486%\n",
      "total_backward_count 465025 real_backward_count 71157  15.302%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.466552/  1.654003, val:  76.25%, val_best:  84.17%, tr:  98.67%, tr_best:  98.98%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7113%\n",
      "layer   2  Sparsity: 83.9596%\n",
      "layer   3  Sparsity: 84.8101%\n",
      "total_backward_count 469920 real_backward_count 71799  15.279%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.468589/  1.694920, val:  74.58%, val_best:  84.17%, tr:  97.24%, tr_best:  98.98%, epoch time: 39.44 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7080%\n",
      "layer   2  Sparsity: 84.4503%\n",
      "layer   3  Sparsity: 85.5285%\n",
      "total_backward_count 474815 real_backward_count 72410  15.250%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.460241/  1.628273, val:  67.92%, val_best:  84.17%, tr:  96.83%, tr_best:  98.98%, epoch time: 39.81 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7222%\n",
      "layer   2  Sparsity: 84.1152%\n",
      "layer   3  Sparsity: 83.9053%\n",
      "total_backward_count 479710 real_backward_count 73071  15.232%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.431620/  1.670860, val:  74.17%, val_best:  84.17%, tr:  97.75%, tr_best:  98.98%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7013%\n",
      "layer   2  Sparsity: 84.7473%\n",
      "layer   3  Sparsity: 83.2269%\n",
      "total_backward_count 484605 real_backward_count 73677  15.204%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.407411/  1.646631, val:  68.33%, val_best:  84.17%, tr:  97.75%, tr_best:  98.98%, epoch time: 39.79 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7222%\n",
      "layer   2  Sparsity: 84.5904%\n",
      "layer   3  Sparsity: 83.5285%\n",
      "total_backward_count 489500 real_backward_count 74300  15.179%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.415565/  1.656769, val:  77.08%, val_best:  84.17%, tr:  98.26%, tr_best:  98.98%, epoch time: 39.49 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6846%\n",
      "layer   2  Sparsity: 85.0377%\n",
      "layer   3  Sparsity: 84.2066%\n",
      "total_backward_count 494395 real_backward_count 74901  15.150%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.454270/  1.626766, val:  74.17%, val_best:  84.17%, tr:  97.04%, tr_best:  98.98%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6993%\n",
      "layer   2  Sparsity: 85.6004%\n",
      "layer   3  Sparsity: 85.4403%\n",
      "total_backward_count 499290 real_backward_count 75546  15.131%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.458881/  1.640745, val:  79.58%, val_best:  84.17%, tr:  97.45%, tr_best:  98.98%, epoch time: 39.34 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6832%\n",
      "layer   2  Sparsity: 85.6401%\n",
      "layer   3  Sparsity: 84.2887%\n",
      "total_backward_count 504185 real_backward_count 76169  15.107%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.475617/  1.674233, val:  66.67%, val_best:  84.17%, tr:  98.37%, tr_best:  98.98%, epoch time: 39.89 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6951%\n",
      "layer   2  Sparsity: 85.3694%\n",
      "layer   3  Sparsity: 84.4206%\n",
      "total_backward_count 509080 real_backward_count 76793  15.085%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.467409/  1.644666, val:  76.67%, val_best:  84.17%, tr:  97.96%, tr_best:  98.98%, epoch time: 39.15 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.6942%\n",
      "layer   2  Sparsity: 84.7695%\n",
      "layer   3  Sparsity: 84.8231%\n",
      "total_backward_count 513975 real_backward_count 77428  15.065%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.479207/  1.681137, val:  60.42%, val_best:  84.17%, tr:  97.14%, tr_best:  98.98%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6855%\n",
      "layer   2  Sparsity: 85.0199%\n",
      "layer   3  Sparsity: 85.4560%\n",
      "total_backward_count 518870 real_backward_count 78077  15.048%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.441983/  1.621468, val:  78.33%, val_best:  84.17%, tr:  97.65%, tr_best:  98.98%, epoch time: 39.29 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.6911%\n",
      "layer   2  Sparsity: 84.7895%\n",
      "layer   3  Sparsity: 84.7518%\n",
      "total_backward_count 523765 real_backward_count 78753  15.036%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.460412/  1.658409, val:  61.25%, val_best:  84.17%, tr:  97.24%, tr_best:  98.98%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7220%\n",
      "layer   2  Sparsity: 84.4780%\n",
      "layer   3  Sparsity: 84.9017%\n",
      "total_backward_count 528660 real_backward_count 79430  15.025%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.432337/  1.626329, val:  67.92%, val_best:  84.17%, tr:  97.34%, tr_best:  98.98%, epoch time: 39.28 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.6794%\n",
      "layer   2  Sparsity: 85.2395%\n",
      "layer   3  Sparsity: 83.4481%\n",
      "total_backward_count 533555 real_backward_count 80020  14.998%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.397857/  1.607481, val:  72.50%, val_best:  84.17%, tr:  98.57%, tr_best:  98.98%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7033%\n",
      "layer   2  Sparsity: 85.7070%\n",
      "layer   3  Sparsity: 83.6533%\n",
      "total_backward_count 538450 real_backward_count 80608  14.970%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.403015/  1.627039, val:  77.08%, val_best:  84.17%, tr:  97.75%, tr_best:  98.98%, epoch time: 39.58 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7131%\n",
      "layer   2  Sparsity: 85.5965%\n",
      "layer   3  Sparsity: 84.1769%\n",
      "total_backward_count 543345 real_backward_count 81216  14.947%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.429218/  1.624893, val:  75.42%, val_best:  84.17%, tr:  98.26%, tr_best:  98.98%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6963%\n",
      "layer   2  Sparsity: 85.0109%\n",
      "layer   3  Sparsity: 84.8845%\n",
      "total_backward_count 548240 real_backward_count 81804  14.921%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.428396/  1.626389, val:  75.42%, val_best:  84.17%, tr:  98.16%, tr_best:  98.98%, epoch time: 39.18 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.6979%\n",
      "layer   2  Sparsity: 85.2980%\n",
      "layer   3  Sparsity: 84.3901%\n",
      "total_backward_count 553135 real_backward_count 82394  14.896%\n",
      "fc layer 3 self.abs_max_out: 2721.0\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.439026/  1.678213, val:  75.83%, val_best:  84.17%, tr:  97.45%, tr_best:  98.98%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7144%\n",
      "layer   2  Sparsity: 85.6606%\n",
      "layer   3  Sparsity: 84.3242%\n",
      "total_backward_count 558030 real_backward_count 83031  14.879%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.409364/  1.615778, val:  75.00%, val_best:  84.17%, tr:  97.55%, tr_best:  98.98%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6944%\n",
      "layer   2  Sparsity: 85.1586%\n",
      "layer   3  Sparsity: 83.6628%\n",
      "total_backward_count 562925 real_backward_count 83621  14.855%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.407430/  1.658655, val:  71.25%, val_best:  84.17%, tr:  97.85%, tr_best:  98.98%, epoch time: 39.90 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6977%\n",
      "layer   2  Sparsity: 85.1578%\n",
      "layer   3  Sparsity: 83.4441%\n",
      "total_backward_count 567820 real_backward_count 84211  14.831%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.398021/  1.610201, val:  67.08%, val_best:  84.17%, tr:  98.77%, tr_best:  98.98%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7314%\n",
      "layer   2  Sparsity: 85.1781%\n",
      "layer   3  Sparsity: 83.2281%\n",
      "total_backward_count 572715 real_backward_count 84766  14.801%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.401186/  1.592365, val:  78.75%, val_best:  84.17%, tr:  98.37%, tr_best:  98.98%, epoch time: 39.58 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7170%\n",
      "layer   2  Sparsity: 85.7983%\n",
      "layer   3  Sparsity: 84.0802%\n",
      "total_backward_count 577610 real_backward_count 85375  14.781%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.387069/  1.601941, val:  76.67%, val_best:  84.17%, tr:  97.96%, tr_best:  98.98%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6916%\n",
      "layer   2  Sparsity: 85.6365%\n",
      "layer   3  Sparsity: 83.7637%\n",
      "total_backward_count 582505 real_backward_count 85943  14.754%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.395062/  1.613126, val:  75.42%, val_best:  84.17%, tr:  98.37%, tr_best:  98.98%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7129%\n",
      "layer   2  Sparsity: 85.2957%\n",
      "layer   3  Sparsity: 83.7545%\n",
      "total_backward_count 587400 real_backward_count 86558  14.736%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.386064/  1.598411, val:  73.75%, val_best:  84.17%, tr:  97.75%, tr_best:  98.98%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6875%\n",
      "layer   2  Sparsity: 85.0578%\n",
      "layer   3  Sparsity: 83.0051%\n",
      "total_backward_count 592295 real_backward_count 87171  14.717%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.405649/  1.568990, val:  78.33%, val_best:  84.17%, tr:  97.14%, tr_best:  98.98%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6814%\n",
      "layer   2  Sparsity: 85.3693%\n",
      "layer   3  Sparsity: 83.6864%\n",
      "total_backward_count 597190 real_backward_count 87837  14.708%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.380831/  1.651160, val:  77.08%, val_best:  84.17%, tr:  97.45%, tr_best:  98.98%, epoch time: 39.64 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6999%\n",
      "layer   2  Sparsity: 85.0483%\n",
      "layer   3  Sparsity: 84.2804%\n",
      "total_backward_count 602085 real_backward_count 88443  14.689%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.439947/  1.603009, val:  82.92%, val_best:  84.17%, tr:  97.75%, tr_best:  98.98%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7025%\n",
      "layer   2  Sparsity: 85.1792%\n",
      "layer   3  Sparsity: 84.2938%\n",
      "total_backward_count 606980 real_backward_count 89027  14.667%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.416539/  1.568094, val:  85.42%, val_best:  85.42%, tr:  97.55%, tr_best:  98.98%, epoch time: 39.73 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6484%\n",
      "layer   2  Sparsity: 85.1997%\n",
      "layer   3  Sparsity: 84.5148%\n",
      "total_backward_count 611875 real_backward_count 89594  14.643%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.420054/  1.661106, val:  76.25%, val_best:  85.42%, tr:  98.98%, tr_best:  98.98%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6666%\n",
      "layer   2  Sparsity: 85.6560%\n",
      "layer   3  Sparsity: 85.1226%\n",
      "total_backward_count 616770 real_backward_count 90124  14.612%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.452012/  1.678504, val:  65.42%, val_best:  85.42%, tr:  97.85%, tr_best:  98.98%, epoch time: 39.60 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6877%\n",
      "layer   2  Sparsity: 86.1193%\n",
      "layer   3  Sparsity: 85.0620%\n",
      "total_backward_count 621665 real_backward_count 90713  14.592%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.423359/  1.646779, val:  75.83%, val_best:  85.42%, tr:  97.85%, tr_best:  98.98%, epoch time: 39.59 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7022%\n",
      "layer   2  Sparsity: 86.2626%\n",
      "layer   3  Sparsity: 84.6509%\n",
      "total_backward_count 626560 real_backward_count 91279  14.568%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.438807/  1.630816, val:  78.33%, val_best:  85.42%, tr:  97.34%, tr_best:  98.98%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6985%\n",
      "layer   2  Sparsity: 85.9314%\n",
      "layer   3  Sparsity: 84.5651%\n",
      "total_backward_count 631455 real_backward_count 91873  14.549%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.439114/  1.633939, val:  81.25%, val_best:  85.42%, tr:  97.75%, tr_best:  98.98%, epoch time: 39.33 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7164%\n",
      "layer   2  Sparsity: 85.7021%\n",
      "layer   3  Sparsity: 84.6139%\n",
      "total_backward_count 636350 real_backward_count 92493  14.535%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.505875/  1.643627, val:  70.00%, val_best:  85.42%, tr:  96.32%, tr_best:  98.98%, epoch time: 39.31 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7094%\n",
      "layer   2  Sparsity: 85.7165%\n",
      "layer   3  Sparsity: 84.8340%\n",
      "total_backward_count 641245 real_backward_count 93185  14.532%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.468816/  1.661754, val:  70.83%, val_best:  85.42%, tr:  97.75%, tr_best:  98.98%, epoch time: 39.21 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.7271%\n",
      "layer   2  Sparsity: 85.5803%\n",
      "layer   3  Sparsity: 84.4729%\n",
      "total_backward_count 646140 real_backward_count 93832  14.522%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.471998/  1.703166, val:  75.83%, val_best:  85.42%, tr:  97.75%, tr_best:  98.98%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6922%\n",
      "layer   2  Sparsity: 85.2651%\n",
      "layer   3  Sparsity: 85.3581%\n",
      "total_backward_count 651035 real_backward_count 94453  14.508%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.542255/  1.719034, val:  75.00%, val_best:  85.42%, tr:  97.14%, tr_best:  98.98%, epoch time: 39.49 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7202%\n",
      "layer   2  Sparsity: 85.4075%\n",
      "layer   3  Sparsity: 86.8802%\n",
      "total_backward_count 655930 real_backward_count 95054  14.491%\n",
      "fc layer 1 self.abs_max_out: 15744.0\n",
      "fc layer 2 self.abs_max_out: 4798.0\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.529837/  1.720792, val:  70.83%, val_best:  85.42%, tr:  97.45%, tr_best:  98.98%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6922%\n",
      "layer   2  Sparsity: 85.1916%\n",
      "layer   3  Sparsity: 85.6118%\n",
      "total_backward_count 660825 real_backward_count 95712  14.484%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.463659/  1.635576, val:  73.33%, val_best:  85.42%, tr:  98.26%, tr_best:  98.98%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6942%\n",
      "layer   2  Sparsity: 85.5794%\n",
      "layer   3  Sparsity: 84.8955%\n",
      "total_backward_count 665720 real_backward_count 96331  14.470%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.466583/  1.613539, val:  76.25%, val_best:  85.42%, tr:  98.26%, tr_best:  98.98%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7036%\n",
      "layer   2  Sparsity: 85.8426%\n",
      "layer   3  Sparsity: 85.1582%\n",
      "total_backward_count 670615 real_backward_count 96931  14.454%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.428665/  1.617030, val:  73.33%, val_best:  85.42%, tr:  97.45%, tr_best:  98.98%, epoch time: 39.39 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6754%\n",
      "layer   2  Sparsity: 86.1062%\n",
      "layer   3  Sparsity: 85.1150%\n",
      "total_backward_count 675510 real_backward_count 97467  14.429%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.432092/  1.575562, val:  83.75%, val_best:  85.42%, tr:  98.26%, tr_best:  98.98%, epoch time: 39.14 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.7007%\n",
      "layer   2  Sparsity: 85.7972%\n",
      "layer   3  Sparsity: 84.6450%\n",
      "total_backward_count 680405 real_backward_count 98012  14.405%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.405260/  1.617168, val:  72.50%, val_best:  85.42%, tr:  98.16%, tr_best:  98.98%, epoch time: 39.29 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.7208%\n",
      "layer   2  Sparsity: 86.1751%\n",
      "layer   3  Sparsity: 83.5996%\n",
      "total_backward_count 685300 real_backward_count 98584  14.386%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.443336/  1.594445, val:  79.58%, val_best:  85.42%, tr:  97.45%, tr_best:  98.98%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7085%\n",
      "layer   2  Sparsity: 86.1094%\n",
      "layer   3  Sparsity: 83.7487%\n",
      "total_backward_count 690195 real_backward_count 99167  14.368%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.442525/  1.610176, val:  80.42%, val_best:  85.42%, tr:  97.45%, tr_best:  98.98%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6919%\n",
      "layer   2  Sparsity: 86.0565%\n",
      "layer   3  Sparsity: 83.6574%\n",
      "total_backward_count 695090 real_backward_count 99806  14.359%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.371438/  1.592035, val:  80.83%, val_best:  85.42%, tr:  97.96%, tr_best:  98.98%, epoch time: 39.43 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7066%\n",
      "layer   2  Sparsity: 86.4340%\n",
      "layer   3  Sparsity: 82.7059%\n",
      "total_backward_count 699985 real_backward_count 100385  14.341%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.370440/  1.569932, val:  77.92%, val_best:  85.42%, tr:  98.16%, tr_best:  98.98%, epoch time: 39.60 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7116%\n",
      "layer   2  Sparsity: 86.8410%\n",
      "layer   3  Sparsity: 82.5478%\n",
      "total_backward_count 704880 real_backward_count 100975  14.325%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.404640/  1.575768, val:  75.83%, val_best:  85.42%, tr:  97.34%, tr_best:  98.98%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7479%\n",
      "layer   2  Sparsity: 86.3526%\n",
      "layer   3  Sparsity: 82.8301%\n",
      "total_backward_count 709775 real_backward_count 101648  14.321%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.377579/  1.634529, val:  65.42%, val_best:  85.42%, tr:  98.37%, tr_best:  98.98%, epoch time: 39.35 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7212%\n",
      "layer   2  Sparsity: 85.8632%\n",
      "layer   3  Sparsity: 82.6116%\n",
      "total_backward_count 714670 real_backward_count 102204  14.301%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.378572/  1.599029, val:  75.00%, val_best:  85.42%, tr:  98.06%, tr_best:  98.98%, epoch time: 39.53 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6727%\n",
      "layer   2  Sparsity: 86.1993%\n",
      "layer   3  Sparsity: 82.9969%\n",
      "total_backward_count 719565 real_backward_count 102793  14.285%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.373976/  1.612238, val:  71.25%, val_best:  85.42%, tr:  98.47%, tr_best:  98.98%, epoch time: 39.50 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6845%\n",
      "layer   2  Sparsity: 86.1998%\n",
      "layer   3  Sparsity: 82.6655%\n",
      "total_backward_count 724460 real_backward_count 103368  14.268%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.387211/  1.645312, val:  74.17%, val_best:  85.42%, tr:  97.96%, tr_best:  98.98%, epoch time: 39.78 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6622%\n",
      "layer   2  Sparsity: 86.2055%\n",
      "layer   3  Sparsity: 83.2806%\n",
      "total_backward_count 729355 real_backward_count 103972  14.255%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.381212/  1.608331, val:  65.42%, val_best:  85.42%, tr:  97.34%, tr_best:  98.98%, epoch time: 39.51 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6906%\n",
      "layer   2  Sparsity: 85.8155%\n",
      "layer   3  Sparsity: 82.7878%\n",
      "total_backward_count 734250 real_backward_count 104579  14.243%\n",
      "fc layer 3 self.abs_max_out: 2985.0\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.383126/  1.662985, val:  67.50%, val_best:  85.42%, tr:  97.45%, tr_best:  98.98%, epoch time: 39.14 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.6938%\n",
      "layer   2  Sparsity: 85.7955%\n",
      "layer   3  Sparsity: 82.7614%\n",
      "total_backward_count 739145 real_backward_count 105161  14.227%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.364126/  1.589038, val:  65.83%, val_best:  85.42%, tr:  98.67%, tr_best:  98.98%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6939%\n",
      "layer   2  Sparsity: 85.8493%\n",
      "layer   3  Sparsity: 82.0605%\n",
      "total_backward_count 744040 real_backward_count 105753  14.213%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.338012/  1.567983, val:  71.67%, val_best:  85.42%, tr:  98.16%, tr_best:  98.98%, epoch time: 39.26 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.7004%\n",
      "layer   2  Sparsity: 85.6173%\n",
      "layer   3  Sparsity: 82.6274%\n",
      "total_backward_count 748935 real_backward_count 106318  14.196%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.328147/  1.506812, val:  75.00%, val_best:  85.42%, tr:  98.06%, tr_best:  98.98%, epoch time: 39.94 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7209%\n",
      "layer   2  Sparsity: 85.9804%\n",
      "layer   3  Sparsity: 82.3144%\n",
      "total_backward_count 753830 real_backward_count 106928  14.185%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.348123/  1.606940, val:  79.58%, val_best:  85.42%, tr:  97.75%, tr_best:  98.98%, epoch time: 39.01 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.6642%\n",
      "layer   2  Sparsity: 86.3904%\n",
      "layer   3  Sparsity: 83.4694%\n",
      "total_backward_count 758725 real_backward_count 107512  14.170%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.397833/  1.690181, val:  63.75%, val_best:  85.42%, tr:  97.65%, tr_best:  98.98%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7196%\n",
      "layer   2  Sparsity: 86.6198%\n",
      "layer   3  Sparsity: 83.9356%\n",
      "total_backward_count 763620 real_backward_count 108108  14.157%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.419280/  1.597158, val:  77.50%, val_best:  85.42%, tr:  96.53%, tr_best:  98.98%, epoch time: 39.43 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7001%\n",
      "layer   2  Sparsity: 86.7994%\n",
      "layer   3  Sparsity: 84.4811%\n",
      "total_backward_count 768515 real_backward_count 108751  14.151%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.368528/  1.541684, val:  80.83%, val_best:  85.42%, tr:  98.06%, tr_best:  98.98%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6960%\n",
      "layer   2  Sparsity: 86.8496%\n",
      "layer   3  Sparsity: 82.5878%\n",
      "total_backward_count 773410 real_backward_count 109339  14.137%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.384341/  1.627371, val:  69.58%, val_best:  85.42%, tr:  97.45%, tr_best:  98.98%, epoch time: 39.65 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6946%\n",
      "layer   2  Sparsity: 87.1468%\n",
      "layer   3  Sparsity: 82.6009%\n",
      "total_backward_count 778305 real_backward_count 109933  14.125%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.394085/  1.634541, val:  69.58%, val_best:  85.42%, tr:  98.77%, tr_best:  98.98%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6919%\n",
      "layer   2  Sparsity: 86.7684%\n",
      "layer   3  Sparsity: 82.7522%\n",
      "total_backward_count 783200 real_backward_count 110522  14.112%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.413871/  1.658625, val:  75.42%, val_best:  85.42%, tr:  98.06%, tr_best:  98.98%, epoch time: 39.34 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7590%\n",
      "layer   2  Sparsity: 86.4288%\n",
      "layer   3  Sparsity: 83.3936%\n",
      "total_backward_count 788095 real_backward_count 111145  14.103%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.428290/  1.620438, val:  79.58%, val_best:  85.42%, tr:  97.96%, tr_best:  98.98%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6747%\n",
      "layer   2  Sparsity: 86.6825%\n",
      "layer   3  Sparsity: 83.4336%\n",
      "total_backward_count 792990 real_backward_count 111742  14.091%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.418846/  1.665797, val:  70.83%, val_best:  85.42%, tr:  97.75%, tr_best:  98.98%, epoch time: 39.20 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.6980%\n",
      "layer   2  Sparsity: 86.9021%\n",
      "layer   3  Sparsity: 84.8484%\n",
      "total_backward_count 797885 real_backward_count 112360  14.082%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.450609/  1.665604, val:  68.75%, val_best:  85.42%, tr:  98.16%, tr_best:  98.98%, epoch time: 39.44 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7346%\n",
      "layer   2  Sparsity: 86.7372%\n",
      "layer   3  Sparsity: 84.8809%\n",
      "total_backward_count 802780 real_backward_count 112923  14.066%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.436724/  1.638864, val:  73.75%, val_best:  85.42%, tr:  98.47%, tr_best:  98.98%, epoch time: 39.24 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.6776%\n",
      "layer   2  Sparsity: 85.8751%\n",
      "layer   3  Sparsity: 83.9227%\n",
      "total_backward_count 807675 real_backward_count 113545  14.058%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.466623/  1.636155, val:  66.67%, val_best:  85.42%, tr:  97.04%, tr_best:  98.98%, epoch time: 39.64 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6860%\n",
      "layer   2  Sparsity: 86.1776%\n",
      "layer   3  Sparsity: 84.9756%\n",
      "total_backward_count 812570 real_backward_count 114158  14.049%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.446597/  1.723443, val:  64.58%, val_best:  85.42%, tr:  97.45%, tr_best:  98.98%, epoch time: 39.21 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.7324%\n",
      "layer   2  Sparsity: 86.4268%\n",
      "layer   3  Sparsity: 84.7694%\n",
      "total_backward_count 817465 real_backward_count 114746  14.037%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.417361/  1.573295, val:  71.25%, val_best:  85.42%, tr:  98.47%, tr_best:  98.98%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7239%\n",
      "layer   2  Sparsity: 86.0136%\n",
      "layer   3  Sparsity: 83.7252%\n",
      "total_backward_count 822360 real_backward_count 115330  14.024%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.402410/  1.571761, val:  77.08%, val_best:  85.42%, tr:  98.26%, tr_best:  98.98%, epoch time: 39.82 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6682%\n",
      "layer   2  Sparsity: 86.1838%\n",
      "layer   3  Sparsity: 83.9355%\n",
      "total_backward_count 827255 real_backward_count 115911  14.012%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.338507/  1.550110, val:  78.33%, val_best:  85.42%, tr:  99.28%, tr_best:  99.28%, epoch time: 39.16 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.7134%\n",
      "layer   2  Sparsity: 86.2187%\n",
      "layer   3  Sparsity: 82.3804%\n",
      "total_backward_count 832150 real_backward_count 116425  13.991%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.347412/  1.584992, val:  77.08%, val_best:  85.42%, tr:  98.37%, tr_best:  99.28%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6368%\n",
      "layer   2  Sparsity: 86.1073%\n",
      "layer   3  Sparsity: 83.7217%\n",
      "total_backward_count 837045 real_backward_count 116927  13.969%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.432166/  1.673053, val:  66.25%, val_best:  85.42%, tr:  97.75%, tr_best:  99.28%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6997%\n",
      "layer   2  Sparsity: 86.1824%\n",
      "layer   3  Sparsity: 84.6328%\n",
      "total_backward_count 841940 real_backward_count 117509  13.957%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.412459/  1.570390, val:  79.58%, val_best:  85.42%, tr:  98.37%, tr_best:  99.28%, epoch time: 39.53 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6942%\n",
      "layer   2  Sparsity: 86.5304%\n",
      "layer   3  Sparsity: 83.8857%\n",
      "total_backward_count 846835 real_backward_count 118072  13.943%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.401173/  1.619284, val:  73.75%, val_best:  85.42%, tr:  97.55%, tr_best:  99.28%, epoch time: 38.98 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.7077%\n",
      "layer   2  Sparsity: 86.9358%\n",
      "layer   3  Sparsity: 83.4920%\n",
      "total_backward_count 851730 real_backward_count 118637  13.929%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.347470/  1.568735, val:  75.42%, val_best:  85.42%, tr:  97.85%, tr_best:  99.28%, epoch time: 39.65 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7219%\n",
      "layer   2  Sparsity: 86.3354%\n",
      "layer   3  Sparsity: 82.4651%\n",
      "total_backward_count 856625 real_backward_count 119183  13.913%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.354128/  1.567081, val:  77.92%, val_best:  85.42%, tr:  97.85%, tr_best:  99.28%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7152%\n",
      "layer   2  Sparsity: 86.4962%\n",
      "layer   3  Sparsity: 82.1893%\n",
      "total_backward_count 861520 real_backward_count 119767  13.902%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.369663/  1.518187, val:  76.25%, val_best:  85.42%, tr:  97.96%, tr_best:  99.28%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7360%\n",
      "layer   2  Sparsity: 86.6157%\n",
      "layer   3  Sparsity: 83.1448%\n",
      "total_backward_count 866415 real_backward_count 120332  13.888%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.310315/  1.528459, val:  79.17%, val_best:  85.42%, tr:  97.65%, tr_best:  99.28%, epoch time: 39.44 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6875%\n",
      "layer   2  Sparsity: 86.3829%\n",
      "layer   3  Sparsity: 82.6420%\n",
      "total_backward_count 871310 real_backward_count 120872  13.872%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.367507/  1.635910, val:  67.92%, val_best:  85.42%, tr:  99.08%, tr_best:  99.28%, epoch time: 39.75 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6822%\n",
      "layer   2  Sparsity: 86.4925%\n",
      "layer   3  Sparsity: 82.9734%\n",
      "total_backward_count 876205 real_backward_count 121403  13.856%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.374946/  1.606234, val:  73.75%, val_best:  85.42%, tr:  98.26%, tr_best:  99.28%, epoch time: 39.36 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7194%\n",
      "layer   2  Sparsity: 86.6430%\n",
      "layer   3  Sparsity: 83.6158%\n",
      "total_backward_count 881100 real_backward_count 121998  13.846%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.367668/  1.553125, val:  80.83%, val_best:  85.42%, tr:  97.85%, tr_best:  99.28%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6916%\n",
      "layer   2  Sparsity: 86.7232%\n",
      "layer   3  Sparsity: 83.7264%\n",
      "total_backward_count 885995 real_backward_count 122588  13.836%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.356957/  1.552132, val:  74.17%, val_best:  85.42%, tr:  98.16%, tr_best:  99.28%, epoch time: 39.46 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6602%\n",
      "layer   2  Sparsity: 86.9734%\n",
      "layer   3  Sparsity: 82.9272%\n",
      "total_backward_count 890890 real_backward_count 123159  13.824%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.362339/  1.606942, val:  77.92%, val_best:  85.42%, tr:  98.47%, tr_best:  99.28%, epoch time: 39.42 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6648%\n",
      "layer   2  Sparsity: 87.2724%\n",
      "layer   3  Sparsity: 84.2285%\n",
      "total_backward_count 895785 real_backward_count 123707  13.810%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.377231/  1.584200, val:  66.25%, val_best:  85.42%, tr:  98.16%, tr_best:  99.28%, epoch time: 39.06 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.7269%\n",
      "layer   2  Sparsity: 86.8036%\n",
      "layer   3  Sparsity: 83.7257%\n",
      "total_backward_count 900680 real_backward_count 124256  13.796%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.360010/  1.574304, val:  75.42%, val_best:  85.42%, tr:  97.45%, tr_best:  99.28%, epoch time: 39.85 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7057%\n",
      "layer   2  Sparsity: 86.8431%\n",
      "layer   3  Sparsity: 83.6158%\n",
      "total_backward_count 905575 real_backward_count 124846  13.786%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.377677/  1.665873, val:  70.42%, val_best:  85.42%, tr:  98.26%, tr_best:  99.28%, epoch time: 39.63 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6562%\n",
      "layer   2  Sparsity: 86.6309%\n",
      "layer   3  Sparsity: 83.6571%\n",
      "total_backward_count 910470 real_backward_count 125412  13.774%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.393973/  1.616586, val:  73.33%, val_best:  85.42%, tr:  97.96%, tr_best:  99.28%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7245%\n",
      "layer   2  Sparsity: 86.6574%\n",
      "layer   3  Sparsity: 84.1557%\n",
      "total_backward_count 915365 real_backward_count 125976  13.762%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.418854/  1.676789, val:  68.75%, val_best:  85.42%, tr:  97.45%, tr_best:  99.28%, epoch time: 39.10 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.7054%\n",
      "layer   2  Sparsity: 86.5209%\n",
      "layer   3  Sparsity: 83.8758%\n",
      "total_backward_count 920260 real_backward_count 126597  13.757%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.390711/  1.600747, val:  80.42%, val_best:  85.42%, tr:  98.47%, tr_best:  99.28%, epoch time: 39.81 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6967%\n",
      "layer   2  Sparsity: 86.7498%\n",
      "layer   3  Sparsity: 84.4952%\n",
      "total_backward_count 925155 real_backward_count 127202  13.749%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.381406/  1.555909, val:  77.50%, val_best:  85.42%, tr:  97.75%, tr_best:  99.28%, epoch time: 39.37 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6802%\n",
      "layer   2  Sparsity: 86.6965%\n",
      "layer   3  Sparsity: 84.1478%\n",
      "total_backward_count 930050 real_backward_count 127752  13.736%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.356559/  1.575248, val:  77.92%, val_best:  85.42%, tr:  98.06%, tr_best:  99.28%, epoch time: 39.31 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6948%\n",
      "layer   2  Sparsity: 86.8361%\n",
      "layer   3  Sparsity: 83.1352%\n",
      "total_backward_count 934945 real_backward_count 128323  13.725%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.382814/  1.625463, val:  65.42%, val_best:  85.42%, tr:  97.96%, tr_best:  99.28%, epoch time: 39.49 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6797%\n",
      "layer   2  Sparsity: 86.4806%\n",
      "layer   3  Sparsity: 83.1977%\n",
      "total_backward_count 939840 real_backward_count 128841  13.709%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.422658/  1.641564, val:  81.25%, val_best:  85.42%, tr:  97.85%, tr_best:  99.28%, epoch time: 39.39 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6939%\n",
      "layer   2  Sparsity: 86.2610%\n",
      "layer   3  Sparsity: 84.5955%\n",
      "total_backward_count 944735 real_backward_count 129397  13.697%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.407429/  1.624578, val:  67.50%, val_best:  85.42%, tr:  97.96%, tr_best:  99.28%, epoch time: 36.83 seconds, 0.61 minutes\n",
      "layer   1  Sparsity: 91.6874%\n",
      "layer   2  Sparsity: 86.6183%\n",
      "layer   3  Sparsity: 83.9797%\n",
      "total_backward_count 949630 real_backward_count 129911  13.680%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.373040/  1.544024, val:  77.92%, val_best:  85.42%, tr:  97.75%, tr_best:  99.28%, epoch time: 39.31 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7019%\n",
      "layer   2  Sparsity: 86.6154%\n",
      "layer   3  Sparsity: 82.9379%\n",
      "total_backward_count 954525 real_backward_count 130457  13.667%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.355307/  1.576251, val:  71.67%, val_best:  85.42%, tr:  98.06%, tr_best:  99.28%, epoch time: 39.29 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 91.6770%\n",
      "layer   2  Sparsity: 87.0812%\n",
      "layer   3  Sparsity: 83.7039%\n",
      "total_backward_count 959420 real_backward_count 131000  13.654%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.370958/  1.559371, val:  70.42%, val_best:  85.42%, tr:  97.55%, tr_best:  99.28%, epoch time: 39.54 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7167%\n",
      "layer   2  Sparsity: 87.0843%\n",
      "layer   3  Sparsity: 82.8860%\n",
      "total_backward_count 964315 real_backward_count 131593  13.646%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.362881/  1.565930, val:  75.00%, val_best:  85.42%, tr:  97.65%, tr_best:  99.28%, epoch time: 39.54 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7323%\n",
      "layer   2  Sparsity: 86.8221%\n",
      "layer   3  Sparsity: 83.3878%\n",
      "total_backward_count 969210 real_backward_count 132141  13.634%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.340116/  1.579541, val:  65.42%, val_best:  85.42%, tr:  97.45%, tr_best:  99.28%, epoch time: 39.32 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6874%\n",
      "layer   2  Sparsity: 86.9095%\n",
      "layer   3  Sparsity: 83.3431%\n",
      "total_backward_count 974105 real_backward_count 132653  13.618%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.362960/  1.598857, val:  66.67%, val_best:  85.42%, tr:  98.37%, tr_best:  99.28%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6972%\n",
      "layer   2  Sparsity: 87.2746%\n",
      "layer   3  Sparsity: 83.4684%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35d6640a2d54cf986711d178445cb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñà‚ñÜ‚ñá‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñà‚ñÜ‚ñá‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98366</td></tr><tr><td>tr_epoch_loss</td><td>1.36296</td></tr><tr><td>val_acc_best</td><td>0.85417</td></tr><tr><td>val_acc_now</td><td>0.66667</td></tr><tr><td>val_loss</td><td>1.59886</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-sweep-317</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zgaznwz9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zgaznwz9</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251127_121254-zgaznwz9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9id08g8c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 50000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251127_142559-9id08g8c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9id08g8c' target=\"_blank\">daily-sweep-320</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9id08g8c' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9id08g8c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251127_142607_518', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 50000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = dac77cc348b2d880ae59906e26f08f17\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 139\n",
      "fc layer 1 self.abs_max_out: 457.0\n",
      "lif layer 1 self.abs_max_v: 457.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 978.0\n",
      "lif layer 2 self.abs_max_v: 978.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 456.0\n",
      "fc layer 1 self.abs_max_out: 520.0\n",
      "lif layer 1 self.abs_max_v: 543.5\n",
      "fc layer 1 self.abs_max_out: 554.0\n",
      "lif layer 1 self.abs_max_v: 700.0\n",
      "fc layer 2 self.abs_max_out: 1051.0\n",
      "lif layer 2 self.abs_max_v: 1367.5\n",
      "fc layer 1 self.abs_max_out: 762.0\n",
      "lif layer 1 self.abs_max_v: 765.5\n",
      "fc layer 2 self.abs_max_out: 1122.0\n",
      "lif layer 2 self.abs_max_v: 1426.5\n",
      "smallest_now_T updated: 125\n",
      "lif layer 1 self.abs_max_v: 831.5\n",
      "lif layer 1 self.abs_max_v: 852.5\n",
      "lif layer 1 self.abs_max_v: 1103.0\n",
      "smallest_now_T updated: 94\n",
      "fc layer 1 self.abs_max_out: 854.0\n",
      "fc layer 1 self.abs_max_out: 884.0\n",
      "lif layer 1 self.abs_max_v: 1141.0\n",
      "fc layer 2 self.abs_max_out: 1134.0\n",
      "lif layer 2 self.abs_max_v: 1774.0\n",
      "fc layer 2 self.abs_max_out: 1168.0\n",
      "lif layer 2 self.abs_max_v: 1779.0\n",
      "fc layer 1 self.abs_max_out: 905.0\n",
      "fc layer 1 self.abs_max_out: 969.0\n",
      "lif layer 1 self.abs_max_v: 1341.0\n",
      "lif layer 1 self.abs_max_v: 1376.5\n",
      "fc layer 2 self.abs_max_out: 1212.0\n",
      "lif layer 1 self.abs_max_v: 1476.0\n",
      "fc layer 1 self.abs_max_out: 986.0\n",
      "fc layer 2 self.abs_max_out: 1405.0\n",
      "lif layer 2 self.abs_max_v: 1789.0\n",
      "fc layer 1 self.abs_max_out: 1113.0\n",
      "lif layer 1 self.abs_max_v: 1483.5\n",
      "lif layer 1 self.abs_max_v: 1501.5\n",
      "lif layer 2 self.abs_max_v: 1850.0\n",
      "smallest_now_T updated: 79\n",
      "fc layer 3 self.abs_max_out: 509.0\n",
      "fc layer 1 self.abs_max_out: 1140.0\n",
      "lif layer 1 self.abs_max_v: 1514.5\n",
      "fc layer 1 self.abs_max_out: 1187.0\n",
      "lif layer 1 self.abs_max_v: 1548.5\n",
      "fc layer 1 self.abs_max_out: 1378.0\n",
      "lif layer 1 self.abs_max_v: 1841.5\n",
      "lif layer 1 self.abs_max_v: 1943.0\n",
      "fc layer 1 self.abs_max_out: 1441.0\n",
      "lif layer 2 self.abs_max_v: 1884.0\n",
      "fc layer 1 self.abs_max_out: 1466.0\n",
      "fc layer 1 self.abs_max_out: 1475.0\n",
      "lif layer 1 self.abs_max_v: 2194.0\n",
      "lif layer 2 self.abs_max_v: 2278.0\n",
      "smallest_now_T updated: 73\n",
      "lif layer 2 self.abs_max_v: 2299.0\n",
      "fc layer 2 self.abs_max_out: 1430.0\n",
      "lif layer 2 self.abs_max_v: 2336.0\n",
      "fc layer 1 self.abs_max_out: 1536.0\n",
      "fc layer 1 self.abs_max_out: 1541.0\n",
      "lif layer 1 self.abs_max_v: 2621.5\n",
      "lif layer 2 self.abs_max_v: 2441.5\n",
      "smallest_now_T updated: 65\n",
      "fc layer 1 self.abs_max_out: 1546.0\n",
      "fc layer 1 self.abs_max_out: 1605.0\n",
      "fc layer 3 self.abs_max_out: 531.0\n",
      "lif layer 1 self.abs_max_v: 2678.5\n",
      "fc layer 2 self.abs_max_out: 1548.0\n",
      "fc layer 1 self.abs_max_out: 1636.0\n",
      "fc layer 2 self.abs_max_out: 1793.0\n",
      "fc layer 1 self.abs_max_out: 1744.0\n",
      "fc layer 3 self.abs_max_out: 546.0\n",
      "smallest_now_T updated: 56\n",
      "smallest_now_T updated: 43\n",
      "lif layer 1 self.abs_max_v: 2746.0\n",
      "lif layer 1 self.abs_max_v: 2881.0\n",
      "fc layer 1 self.abs_max_out: 1835.0\n",
      "fc layer 3 self.abs_max_out: 561.0\n",
      "fc layer 3 self.abs_max_out: 566.0\n",
      "fc layer 3 self.abs_max_out: 650.0\n",
      "fc layer 1 self.abs_max_out: 1999.0\n",
      "lif layer 2 self.abs_max_v: 2484.5\n",
      "lif layer 2 self.abs_max_v: 2491.0\n",
      "lif layer 1 self.abs_max_v: 2924.0\n",
      "lif layer 1 self.abs_max_v: 3101.0\n",
      "lif layer 1 self.abs_max_v: 3377.5\n",
      "lif layer 2 self.abs_max_v: 2527.5\n",
      "fc layer 2 self.abs_max_out: 1797.0\n",
      "lif layer 2 self.abs_max_v: 2635.5\n",
      "lif layer 2 self.abs_max_v: 2722.0\n",
      "fc layer 1 self.abs_max_out: 2044.0\n",
      "lif layer 1 self.abs_max_v: 3482.5\n",
      "fc layer 2 self.abs_max_out: 1933.0\n",
      "fc layer 1 self.abs_max_out: 2107.0\n",
      "lif layer 2 self.abs_max_v: 2970.0\n",
      "fc layer 1 self.abs_max_out: 2108.0\n",
      "fc layer 1 self.abs_max_out: 2123.0\n",
      "fc layer 1 self.abs_max_out: 2143.0\n",
      "lif layer 1 self.abs_max_v: 3499.5\n",
      "fc layer 1 self.abs_max_out: 2250.0\n",
      "lif layer 1 self.abs_max_v: 3911.5\n",
      "lif layer 1 self.abs_max_v: 4008.0\n",
      "lif layer 2 self.abs_max_v: 3042.5\n",
      "fc layer 2 self.abs_max_out: 1936.0\n",
      "lif layer 2 self.abs_max_v: 3372.0\n",
      "fc layer 1 self.abs_max_out: 2361.0\n",
      "fc layer 1 self.abs_max_out: 2558.0\n",
      "lif layer 1 self.abs_max_v: 4220.5\n",
      "smallest_now_T_val updated: 129\n",
      "smallest_now_T_val updated: 106\n",
      "smallest_now_T_val updated: 104\n",
      "smallest_now_T_val updated: 102\n",
      "smallest_now_T_val updated: 85\n",
      "smallest_now_T_val updated: 29\n",
      "fc layer 1 self.abs_max_out: 2626.0\n",
      "lif layer 1 self.abs_max_v: 4370.5\n",
      "lif layer 1 self.abs_max_v: 4477.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  1.919839/  1.977096, val:  38.75%, val_best:  38.75%, tr:  74.36%, tr_best:  74.36%, epoch time: 39.73 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8491%\n",
      "layer   2  Sparsity: 68.2479%\n",
      "layer   3  Sparsity: 65.7930%\n",
      "total_backward_count 4895 real_backward_count 1943  39.694%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 2697.0\n",
      "lif layer 1 self.abs_max_v: 4584.5\n",
      "fc layer 3 self.abs_max_out: 661.0\n",
      "fc layer 3 self.abs_max_out: 675.0\n",
      "fc layer 3 self.abs_max_out: 689.0\n",
      "fc layer 3 self.abs_max_out: 726.0\n",
      "fc layer 2 self.abs_max_out: 1971.0\n",
      "lif layer 2 self.abs_max_v: 3401.5\n",
      "lif layer 2 self.abs_max_v: 3445.0\n",
      "fc layer 1 self.abs_max_out: 2876.0\n",
      "lif layer 1 self.abs_max_v: 4709.0\n",
      "fc layer 2 self.abs_max_out: 1990.0\n",
      "fc layer 2 self.abs_max_out: 2011.0\n",
      "fc layer 2 self.abs_max_out: 2035.0\n",
      "fc layer 1 self.abs_max_out: 3095.0\n",
      "lif layer 1 self.abs_max_v: 4872.5\n",
      "fc layer 2 self.abs_max_out: 2062.0\n",
      "lif layer 2 self.abs_max_v: 3500.0\n",
      "lif layer 2 self.abs_max_v: 3642.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.841420/  1.982265, val:  49.58%, val_best:  49.58%, tr:  84.88%, tr_best:  84.88%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8379%\n",
      "layer   2  Sparsity: 69.5305%\n",
      "layer   3  Sparsity: 64.4062%\n",
      "total_backward_count 9790 real_backward_count 3352  34.239%\n",
      "fc layer 2 self.abs_max_out: 2182.0\n",
      "lif layer 1 self.abs_max_v: 4912.0\n",
      "fc layer 3 self.abs_max_out: 733.0\n",
      "fc layer 1 self.abs_max_out: 3174.0\n",
      "lif layer 1 self.abs_max_v: 4928.0\n",
      "lif layer 1 self.abs_max_v: 4969.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.813213/  1.934968, val:  48.33%, val_best:  49.58%, tr:  88.76%, tr_best:  88.76%, epoch time: 39.80 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9140%\n",
      "layer   2  Sparsity: 70.7570%\n",
      "layer   3  Sparsity: 63.2832%\n",
      "total_backward_count 14685 real_backward_count 4585  31.222%\n",
      "fc layer 2 self.abs_max_out: 2191.0\n",
      "lif layer 1 self.abs_max_v: 5146.5\n",
      "fc layer 2 self.abs_max_out: 2241.0\n",
      "fc layer 1 self.abs_max_out: 3528.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  1.806346/  1.941148, val:  42.92%, val_best:  49.58%, tr:  90.40%, tr_best:  90.40%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8813%\n",
      "layer   2  Sparsity: 71.9453%\n",
      "layer   3  Sparsity: 63.2985%\n",
      "total_backward_count 19580 real_backward_count 5685  29.035%\n",
      "lif layer 1 self.abs_max_v: 5227.5\n",
      "fc layer 2 self.abs_max_out: 2248.0\n",
      "fc layer 2 self.abs_max_out: 2429.0\n",
      "lif layer 1 self.abs_max_v: 5399.0\n",
      "fc layer 2 self.abs_max_out: 2463.0\n",
      "lif layer 1 self.abs_max_v: 5658.0\n",
      "fc layer 1 self.abs_max_out: 3896.0\n",
      "lif layer 1 self.abs_max_v: 6139.5\n",
      "fc layer 2 self.abs_max_out: 2464.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  1.811250/  1.951097, val:  51.25%, val_best:  51.25%, tr:  91.52%, tr_best:  91.52%, epoch time: 39.10 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8118%\n",
      "layer   2  Sparsity: 71.3518%\n",
      "layer   3  Sparsity: 63.8727%\n",
      "total_backward_count 24475 real_backward_count 6702  27.383%\n",
      "fc layer 2 self.abs_max_out: 2586.0\n",
      "fc layer 1 self.abs_max_out: 4113.0\n",
      "lif layer 1 self.abs_max_v: 6494.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  1.817247/  1.922109, val:  62.50%, val_best:  62.50%, tr:  92.54%, tr_best:  92.54%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8913%\n",
      "layer   2  Sparsity: 72.0678%\n",
      "layer   3  Sparsity: 64.8757%\n",
      "total_backward_count 29370 real_backward_count 7694  26.197%\n",
      "fc layer 3 self.abs_max_out: 756.0\n",
      "fc layer 1 self.abs_max_out: 4204.0\n",
      "lif layer 1 self.abs_max_v: 6494.5\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  1.805050/  1.941117, val:  54.17%, val_best:  62.50%, tr:  93.87%, tr_best:  93.87%, epoch time: 39.54 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8583%\n",
      "layer   2  Sparsity: 72.6439%\n",
      "layer   3  Sparsity: 65.3947%\n",
      "total_backward_count 34265 real_backward_count 8670  25.303%\n",
      "fc layer 2 self.abs_max_out: 2694.0\n",
      "lif layer 2 self.abs_max_v: 3686.5\n",
      "lif layer 1 self.abs_max_v: 6786.0\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  1.794105/  1.900848, val:  59.58%, val_best:  62.50%, tr:  93.97%, tr_best:  93.97%, epoch time: 39.97 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8775%\n",
      "layer   2  Sparsity: 72.0696%\n",
      "layer   3  Sparsity: 65.7936%\n",
      "total_backward_count 39160 real_backward_count 9600  24.515%\n",
      "lif layer 2 self.abs_max_v: 3782.0\n",
      "lif layer 2 self.abs_max_v: 3797.0\n",
      "fc layer 1 self.abs_max_out: 4326.0\n",
      "lif layer 1 self.abs_max_v: 7063.0\n",
      "lif layer 1 self.abs_max_v: 7191.5\n",
      "fc layer 1 self.abs_max_out: 4422.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  1.794934/  1.903442, val:  60.00%, val_best:  62.50%, tr:  93.26%, tr_best:  93.97%, epoch time: 38.95 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8470%\n",
      "layer   2  Sparsity: 72.2967%\n",
      "layer   3  Sparsity: 66.5210%\n",
      "total_backward_count 44055 real_backward_count 10512  23.861%\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  1.777429/  1.898397, val:  55.42%, val_best:  62.50%, tr:  94.28%, tr_best:  94.28%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8945%\n",
      "layer   2  Sparsity: 72.3774%\n",
      "layer   3  Sparsity: 67.2260%\n",
      "total_backward_count 48950 real_backward_count 11415  23.320%\n",
      "fc layer 3 self.abs_max_out: 772.0\n",
      "fc layer 1 self.abs_max_out: 4424.0\n",
      "lif layer 1 self.abs_max_v: 7242.0\n",
      "lif layer 1 self.abs_max_v: 7279.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  1.768715/  1.916060, val:  50.83%, val_best:  62.50%, tr:  95.40%, tr_best:  95.40%, epoch time: 39.22 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8241%\n",
      "layer   2  Sparsity: 72.4220%\n",
      "layer   3  Sparsity: 68.0564%\n",
      "total_backward_count 53845 real_backward_count 12272  22.791%\n",
      "fc layer 1 self.abs_max_out: 4621.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  1.781265/  1.894286, val:  60.42%, val_best:  62.50%, tr:  96.22%, tr_best:  96.22%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8326%\n",
      "layer   2  Sparsity: 72.1006%\n",
      "layer   3  Sparsity: 68.1570%\n",
      "total_backward_count 58740 real_backward_count 13121  22.337%\n",
      "lif layer 1 self.abs_max_v: 7320.5\n",
      "lif layer 1 self.abs_max_v: 7418.5\n",
      "lif layer 2 self.abs_max_v: 3914.5\n",
      "lif layer 2 self.abs_max_v: 3990.5\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  1.777001/  1.908972, val:  62.08%, val_best:  62.50%, tr:  97.24%, tr_best:  97.24%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8258%\n",
      "layer   2  Sparsity: 72.3390%\n",
      "layer   3  Sparsity: 68.3857%\n",
      "total_backward_count 63635 real_backward_count 13889  21.826%\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  1.772113/  1.910397, val:  70.00%, val_best:  70.00%, tr:  96.73%, tr_best:  97.24%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8427%\n",
      "layer   2  Sparsity: 71.2022%\n",
      "layer   3  Sparsity: 67.8873%\n",
      "total_backward_count 68530 real_backward_count 14638  21.360%\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  1.780946/  1.882421, val:  71.67%, val_best:  71.67%, tr:  97.04%, tr_best:  97.24%, epoch time: 39.36 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8979%\n",
      "layer   2  Sparsity: 71.9217%\n",
      "layer   3  Sparsity: 68.8802%\n",
      "total_backward_count 73425 real_backward_count 15395  20.967%\n",
      "fc layer 1 self.abs_max_out: 4723.0\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  1.782151/  1.902004, val:  57.50%, val_best:  71.67%, tr:  95.91%, tr_best:  97.24%, epoch time: 39.53 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8514%\n",
      "layer   2  Sparsity: 72.8106%\n",
      "layer   3  Sparsity: 70.4155%\n",
      "total_backward_count 78320 real_backward_count 16158  20.631%\n",
      "lif layer 2 self.abs_max_v: 4021.0\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  1.773112/  1.890669, val:  69.58%, val_best:  71.67%, tr:  96.42%, tr_best:  97.24%, epoch time: 39.08 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8359%\n",
      "layer   2  Sparsity: 73.1385%\n",
      "layer   3  Sparsity: 70.7815%\n",
      "total_backward_count 83215 real_backward_count 16852  20.251%\n",
      "fc layer 1 self.abs_max_out: 4806.0\n",
      "lif layer 1 self.abs_max_v: 7496.5\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  1.784572/  1.867464, val:  77.08%, val_best:  77.08%, tr:  96.83%, tr_best:  97.24%, epoch time: 39.80 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8467%\n",
      "layer   2  Sparsity: 73.0921%\n",
      "layer   3  Sparsity: 71.2007%\n",
      "total_backward_count 88110 real_backward_count 17551  19.919%\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  1.773400/  1.880093, val:  71.25%, val_best:  77.08%, tr:  97.14%, tr_best:  97.24%, epoch time: 39.68 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8874%\n",
      "layer   2  Sparsity: 73.1148%\n",
      "layer   3  Sparsity: 70.9182%\n",
      "total_backward_count 93005 real_backward_count 18213  19.583%\n",
      "lif layer 1 self.abs_max_v: 7584.5\n",
      "lif layer 1 self.abs_max_v: 7833.5\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  1.767054/  1.893813, val:  53.75%, val_best:  77.08%, tr:  98.16%, tr_best:  98.16%, epoch time: 39.97 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8833%\n",
      "layer   2  Sparsity: 72.4839%\n",
      "layer   3  Sparsity: 70.7206%\n",
      "total_backward_count 97900 real_backward_count 18832  19.236%\n",
      "lif layer 2 self.abs_max_v: 4105.5\n",
      "fc layer 1 self.abs_max_out: 4808.0\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  1.780525/  1.904909, val:  72.50%, val_best:  77.08%, tr:  98.16%, tr_best:  98.16%, epoch time: 39.37 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8434%\n",
      "layer   2  Sparsity: 72.0548%\n",
      "layer   3  Sparsity: 70.6936%\n",
      "total_backward_count 102795 real_backward_count 19441  18.912%\n",
      "fc layer 1 self.abs_max_out: 5142.0\n",
      "lif layer 1 self.abs_max_v: 7903.5\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  1.780661/  1.887305, val:  74.17%, val_best:  77.08%, tr:  97.96%, tr_best:  98.16%, epoch time: 39.24 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8668%\n",
      "layer   2  Sparsity: 72.7660%\n",
      "layer   3  Sparsity: 71.1894%\n",
      "total_backward_count 107690 real_backward_count 20082  18.648%\n",
      "fc layer 2 self.abs_max_out: 2700.0\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  1.786327/  1.879038, val:  67.92%, val_best:  77.08%, tr:  97.85%, tr_best:  98.16%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8557%\n",
      "layer   2  Sparsity: 72.1317%\n",
      "layer   3  Sparsity: 71.2036%\n",
      "total_backward_count 112585 real_backward_count 20707  18.392%\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  1.770663/  1.890090, val:  74.17%, val_best:  77.08%, tr:  98.47%, tr_best:  98.47%, epoch time: 39.35 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8237%\n",
      "layer   2  Sparsity: 72.2549%\n",
      "layer   3  Sparsity: 71.3701%\n",
      "total_backward_count 117480 real_backward_count 21280  18.114%\n",
      "lif layer 2 self.abs_max_v: 4292.0\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  1.782778/  1.892742, val:  80.83%, val_best:  80.83%, tr:  97.85%, tr_best:  98.47%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8199%\n",
      "layer   2  Sparsity: 71.3250%\n",
      "layer   3  Sparsity: 71.1653%\n",
      "total_backward_count 122375 real_backward_count 21851  17.856%\n",
      "lif layer 1 self.abs_max_v: 8192.0\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  1.781695/  1.875841, val:  81.67%, val_best:  81.67%, tr:  97.04%, tr_best:  98.47%, epoch time: 39.30 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8771%\n",
      "layer   2  Sparsity: 71.4780%\n",
      "layer   3  Sparsity: 70.8663%\n",
      "total_backward_count 127270 real_backward_count 22485  17.667%\n",
      "fc layer 2 self.abs_max_out: 2817.0\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  1.777099/  1.890078, val:  70.42%, val_best:  81.67%, tr:  98.37%, tr_best:  98.47%, epoch time: 39.34 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8770%\n",
      "layer   2  Sparsity: 71.9833%\n",
      "layer   3  Sparsity: 70.9527%\n",
      "total_backward_count 132165 real_backward_count 23042  17.434%\n",
      "fc layer 1 self.abs_max_out: 5144.0\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  1.773453/  1.879996, val:  77.92%, val_best:  81.67%, tr:  98.57%, tr_best:  98.57%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8544%\n",
      "layer   2  Sparsity: 71.7082%\n",
      "layer   3  Sparsity: 71.0287%\n",
      "total_backward_count 137060 real_backward_count 23545  17.179%\n",
      "fc layer 1 self.abs_max_out: 5204.0\n",
      "fc layer 1 self.abs_max_out: 5227.0\n",
      "lif layer 1 self.abs_max_v: 8272.0\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  1.766399/  1.906093, val:  72.50%, val_best:  81.67%, tr:  98.26%, tr_best:  98.57%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8781%\n",
      "layer   2  Sparsity: 72.0952%\n",
      "layer   3  Sparsity: 71.5977%\n",
      "total_backward_count 141955 real_backward_count 24049  16.941%\n",
      "lif layer 2 self.abs_max_v: 4425.0\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  1.773938/  1.862748, val:  74.17%, val_best:  81.67%, tr:  98.88%, tr_best:  98.88%, epoch time: 39.27 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8778%\n",
      "layer   2  Sparsity: 71.2817%\n",
      "layer   3  Sparsity: 71.1760%\n",
      "total_backward_count 146850 real_backward_count 24546  16.715%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  1.759727/  1.856357, val:  77.92%, val_best:  81.67%, tr:  98.98%, tr_best:  98.98%, epoch time: 39.29 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8190%\n",
      "layer   2  Sparsity: 71.0707%\n",
      "layer   3  Sparsity: 70.5324%\n",
      "total_backward_count 151745 real_backward_count 25013  16.484%\n",
      "fc layer 1 self.abs_max_out: 5234.0\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  1.761158/  1.863617, val:  74.58%, val_best:  81.67%, tr:  98.57%, tr_best:  98.98%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8897%\n",
      "layer   2  Sparsity: 70.5074%\n",
      "layer   3  Sparsity: 70.6262%\n",
      "total_backward_count 156640 real_backward_count 25533  16.300%\n",
      "fc layer 1 self.abs_max_out: 5242.0\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  1.752040/  1.874114, val:  76.25%, val_best:  81.67%, tr:  98.88%, tr_best:  98.98%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8318%\n",
      "layer   2  Sparsity: 70.2976%\n",
      "layer   3  Sparsity: 71.1627%\n",
      "total_backward_count 161535 real_backward_count 26041  16.121%\n",
      "lif layer 1 self.abs_max_v: 8340.0\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  1.749539/  1.847416, val:  77.50%, val_best:  81.67%, tr:  98.98%, tr_best:  98.98%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8477%\n",
      "layer   2  Sparsity: 70.5976%\n",
      "layer   3  Sparsity: 71.5838%\n",
      "total_backward_count 166430 real_backward_count 26495  15.920%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  1.736854/  1.848635, val:  76.25%, val_best:  81.67%, tr:  98.77%, tr_best:  98.98%, epoch time: 39.51 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9184%\n",
      "layer   2  Sparsity: 70.4870%\n",
      "layer   3  Sparsity: 71.7473%\n",
      "total_backward_count 171325 real_backward_count 26945  15.727%\n",
      "fc layer 2 self.abs_max_out: 2852.0\n",
      "fc layer 1 self.abs_max_out: 5308.0\n",
      "fc layer 1 self.abs_max_out: 5441.0\n",
      "lif layer 1 self.abs_max_v: 8567.5\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  1.734875/  1.847350, val:  84.58%, val_best:  84.58%, tr:  98.77%, tr_best:  98.98%, epoch time: 39.39 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8496%\n",
      "layer   2  Sparsity: 70.7297%\n",
      "layer   3  Sparsity: 71.8358%\n",
      "total_backward_count 176220 real_backward_count 27406  15.552%\n",
      "fc layer 2 self.abs_max_out: 2943.0\n",
      "lif layer 2 self.abs_max_v: 4438.0\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  1.741110/  1.848992, val:  76.67%, val_best:  84.58%, tr:  99.28%, tr_best:  99.28%, epoch time: 39.29 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.9181%\n",
      "layer   2  Sparsity: 70.2860%\n",
      "layer   3  Sparsity: 71.6452%\n",
      "total_backward_count 181115 real_backward_count 27845  15.374%\n",
      "lif layer 1 self.abs_max_v: 8648.0\n",
      "lif layer 1 self.abs_max_v: 9114.0\n",
      "fc layer 1 self.abs_max_out: 5519.0\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  1.739321/  1.877147, val:  71.25%, val_best:  84.58%, tr:  99.08%, tr_best:  99.28%, epoch time: 39.81 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8904%\n",
      "layer   2  Sparsity: 70.6226%\n",
      "layer   3  Sparsity: 71.9016%\n",
      "total_backward_count 186010 real_backward_count 28249  15.187%\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  1.744884/  1.874873, val:  77.50%, val_best:  84.58%, tr:  99.39%, tr_best:  99.39%, epoch time: 39.13 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8601%\n",
      "layer   2  Sparsity: 70.2793%\n",
      "layer   3  Sparsity: 71.9882%\n",
      "total_backward_count 190905 real_backward_count 28678  15.022%\n",
      "fc layer 2 self.abs_max_out: 2967.0\n",
      "fc layer 1 self.abs_max_out: 5586.0\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  1.745708/  1.855800, val:  73.75%, val_best:  84.58%, tr:  99.28%, tr_best:  99.39%, epoch time: 39.20 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8740%\n",
      "layer   2  Sparsity: 70.4808%\n",
      "layer   3  Sparsity: 71.9623%\n",
      "total_backward_count 195800 real_backward_count 29054  14.839%\n",
      "fc layer 1 self.abs_max_out: 5646.0\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  1.734168/  1.841341, val:  84.17%, val_best:  84.58%, tr:  99.28%, tr_best:  99.39%, epoch time: 38.98 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8879%\n",
      "layer   2  Sparsity: 70.9988%\n",
      "layer   3  Sparsity: 71.7916%\n",
      "total_backward_count 200695 real_backward_count 29454  14.676%\n",
      "lif layer 2 self.abs_max_v: 4474.0\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  1.733354/  1.869930, val:  77.50%, val_best:  84.58%, tr:  99.08%, tr_best:  99.39%, epoch time: 39.58 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8772%\n",
      "layer   2  Sparsity: 70.5520%\n",
      "layer   3  Sparsity: 71.9377%\n",
      "total_backward_count 205590 real_backward_count 29832  14.510%\n",
      "lif layer 2 self.abs_max_v: 4536.5\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  1.721113/  1.831684, val:  82.08%, val_best:  84.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 39.54 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8626%\n",
      "layer   2  Sparsity: 70.7760%\n",
      "layer   3  Sparsity: 72.1024%\n",
      "total_backward_count 210485 real_backward_count 30231  14.363%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  1.719150/  1.841284, val:  82.92%, val_best:  84.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 39.70 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8516%\n",
      "layer   2  Sparsity: 70.4950%\n",
      "layer   3  Sparsity: 72.3058%\n",
      "total_backward_count 215380 real_backward_count 30643  14.227%\n",
      "fc layer 1 self.abs_max_out: 5651.0\n",
      "fc layer 1 self.abs_max_out: 5808.0\n",
      "lif layer 1 self.abs_max_v: 9114.5\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  1.726710/  1.840917, val:  77.50%, val_best:  84.58%, tr:  99.39%, tr_best:  99.59%, epoch time: 39.16 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.9153%\n",
      "layer   2  Sparsity: 70.7203%\n",
      "layer   3  Sparsity: 72.3702%\n",
      "total_backward_count 220275 real_backward_count 30999  14.073%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  1.719311/  1.816500, val:  85.42%, val_best:  85.42%, tr:  99.59%, tr_best:  99.59%, epoch time: 39.73 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8325%\n",
      "layer   2  Sparsity: 70.4131%\n",
      "layer   3  Sparsity: 72.5703%\n",
      "total_backward_count 225170 real_backward_count 31376  13.934%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  1.719151/  1.828935, val:  74.17%, val_best:  85.42%, tr:  99.39%, tr_best:  99.59%, epoch time: 39.30 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8490%\n",
      "layer   2  Sparsity: 70.2884%\n",
      "layer   3  Sparsity: 72.4383%\n",
      "total_backward_count 230065 real_backward_count 31746  13.799%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  1.719947/  1.840892, val:  81.67%, val_best:  85.42%, tr:  99.49%, tr_best:  99.59%, epoch time: 39.21 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8545%\n",
      "layer   2  Sparsity: 70.1345%\n",
      "layer   3  Sparsity: 72.3387%\n",
      "total_backward_count 234960 real_backward_count 32090  13.658%\n",
      "fc layer 1 self.abs_max_out: 5950.0\n",
      "lif layer 1 self.abs_max_v: 9306.0\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  1.720982/  1.831540, val:  81.67%, val_best:  85.42%, tr:  99.49%, tr_best:  99.59%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8681%\n",
      "layer   2  Sparsity: 70.3290%\n",
      "layer   3  Sparsity: 72.8441%\n",
      "total_backward_count 239855 real_backward_count 32431  13.521%\n",
      "fc layer 2 self.abs_max_out: 3028.0\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  1.717689/  1.829659, val:  80.42%, val_best:  85.42%, tr:  99.59%, tr_best:  99.59%, epoch time: 39.02 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8446%\n",
      "layer   2  Sparsity: 70.1313%\n",
      "layer   3  Sparsity: 73.1678%\n",
      "total_backward_count 244750 real_backward_count 32778  13.392%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  1.710366/  1.821321, val:  78.75%, val_best:  85.42%, tr:  99.08%, tr_best:  99.59%, epoch time: 38.72 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.9149%\n",
      "layer   2  Sparsity: 69.8925%\n",
      "layer   3  Sparsity: 73.1481%\n",
      "total_backward_count 249645 real_backward_count 33160  13.283%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  1.703404/  1.811135, val:  82.08%, val_best:  85.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 39.25 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8718%\n",
      "layer   2  Sparsity: 69.8704%\n",
      "layer   3  Sparsity: 73.0515%\n",
      "total_backward_count 254540 real_backward_count 33501  13.161%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  1.715497/  1.836223, val:  80.42%, val_best:  85.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 39.72 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8335%\n",
      "layer   2  Sparsity: 69.7814%\n",
      "layer   3  Sparsity: 73.1759%\n",
      "total_backward_count 259435 real_backward_count 33825  13.038%\n",
      "fc layer 2 self.abs_max_out: 3103.0\n",
      "fc layer 1 self.abs_max_out: 6036.0\n",
      "lif layer 1 self.abs_max_v: 9451.5\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  1.714534/  1.819114, val:  81.25%, val_best:  85.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 39.59 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8604%\n",
      "layer   2  Sparsity: 69.8021%\n",
      "layer   3  Sparsity: 72.9190%\n",
      "total_backward_count 264330 real_backward_count 34144  12.917%\n",
      "fc layer 1 self.abs_max_out: 6152.0\n",
      "lif layer 1 self.abs_max_v: 9662.5\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  1.701535/  1.807981, val:  81.25%, val_best:  85.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 39.34 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8794%\n",
      "layer   2  Sparsity: 70.1142%\n",
      "layer   3  Sparsity: 73.1557%\n",
      "total_backward_count 269225 real_backward_count 34454  12.797%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.693165/  1.802652, val:  81.25%, val_best:  85.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 39.36 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8278%\n",
      "layer   2  Sparsity: 70.1465%\n",
      "layer   3  Sparsity: 73.2104%\n",
      "total_backward_count 274120 real_backward_count 34766  12.683%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.700178/  1.807622, val:  77.50%, val_best:  85.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 39.58 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8651%\n",
      "layer   2  Sparsity: 70.1079%\n",
      "layer   3  Sparsity: 73.3023%\n",
      "total_backward_count 279015 real_backward_count 35061  12.566%\n",
      "fc layer 2 self.abs_max_out: 3164.0\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.699609/  1.814528, val:  82.08%, val_best:  85.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 39.16 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8060%\n",
      "layer   2  Sparsity: 69.9180%\n",
      "layer   3  Sparsity: 73.0093%\n",
      "total_backward_count 283910 real_backward_count 35344  12.449%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.704460/  1.828788, val:  82.92%, val_best:  85.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8818%\n",
      "layer   2  Sparsity: 69.8418%\n",
      "layer   3  Sparsity: 72.9728%\n",
      "total_backward_count 288805 real_backward_count 35603  12.328%\n",
      "lif layer 2 self.abs_max_v: 4542.5\n",
      "fc layer 1 self.abs_max_out: 6168.0\n",
      "lif layer 1 self.abs_max_v: 9677.0\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.704631/  1.819831, val:  79.17%, val_best:  85.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8368%\n",
      "layer   2  Sparsity: 69.9001%\n",
      "layer   3  Sparsity: 72.6267%\n",
      "total_backward_count 293700 real_backward_count 35871  12.213%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.698479/  1.810328, val:  80.42%, val_best:  85.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 39.47 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8464%\n",
      "layer   2  Sparsity: 69.6085%\n",
      "layer   3  Sparsity: 72.1784%\n",
      "total_backward_count 298595 real_backward_count 36187  12.119%\n",
      "lif layer 2 self.abs_max_v: 4553.5\n",
      "fc layer 1 self.abs_max_out: 6211.0\n",
      "lif layer 1 self.abs_max_v: 9762.5\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.695685/  1.798529, val:  84.17%, val_best:  85.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 38.23 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 88.8696%\n",
      "layer   2  Sparsity: 69.8788%\n",
      "layer   3  Sparsity: 72.2416%\n",
      "total_backward_count 303490 real_backward_count 36444  12.008%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.685088/  1.806409, val:  86.25%, val_best:  86.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 37.62 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 88.8453%\n",
      "layer   2  Sparsity: 70.4267%\n",
      "layer   3  Sparsity: 72.3859%\n",
      "total_backward_count 308385 real_backward_count 36697  11.900%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.690960/  1.811444, val:  78.75%, val_best:  86.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 39.42 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.7822%\n",
      "layer   2  Sparsity: 70.2236%\n",
      "layer   3  Sparsity: 72.5345%\n",
      "total_backward_count 313280 real_backward_count 36940  11.791%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.689792/  1.814791, val:  84.58%, val_best:  86.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 39.42 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8521%\n",
      "layer   2  Sparsity: 70.0158%\n",
      "layer   3  Sparsity: 73.2167%\n",
      "total_backward_count 318175 real_backward_count 37211  11.695%\n",
      "lif layer 2 self.abs_max_v: 4704.0\n",
      "lif layer 2 self.abs_max_v: 4795.0\n",
      "lif layer 2 self.abs_max_v: 4868.0\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.685777/  1.804475, val:  84.17%, val_best:  86.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8733%\n",
      "layer   2  Sparsity: 69.3851%\n",
      "layer   3  Sparsity: 73.0710%\n",
      "total_backward_count 323070 real_backward_count 37500  11.607%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.683712/  1.818227, val:  81.67%, val_best:  86.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 39.49 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9290%\n",
      "layer   2  Sparsity: 69.5568%\n",
      "layer   3  Sparsity: 73.0998%\n",
      "total_backward_count 327965 real_backward_count 37742  11.508%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.684764/  1.802160, val:  83.33%, val_best:  86.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 39.73 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8769%\n",
      "layer   2  Sparsity: 69.1235%\n",
      "layer   3  Sparsity: 72.5568%\n",
      "total_backward_count 332860 real_backward_count 37982  11.411%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.677969/  1.793276, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.22 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8589%\n",
      "layer   2  Sparsity: 69.4527%\n",
      "layer   3  Sparsity: 72.6071%\n",
      "total_backward_count 337755 real_backward_count 38246  11.324%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.676768/  1.805177, val:  82.08%, val_best:  86.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8243%\n",
      "layer   2  Sparsity: 69.5106%\n",
      "layer   3  Sparsity: 72.8794%\n",
      "total_backward_count 342650 real_backward_count 38499  11.236%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.678914/  1.797782, val:  83.75%, val_best:  86.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 39.48 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8597%\n",
      "layer   2  Sparsity: 69.9381%\n",
      "layer   3  Sparsity: 72.7436%\n",
      "total_backward_count 347545 real_backward_count 38754  11.151%\n",
      "lif layer 1 self.abs_max_v: 9871.5\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.677650/  1.795833, val:  82.08%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.42 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8668%\n",
      "layer   2  Sparsity: 69.8390%\n",
      "layer   3  Sparsity: 72.7596%\n",
      "total_backward_count 352440 real_backward_count 39003  11.067%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.676551/  1.786571, val:  86.25%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.48 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8844%\n",
      "layer   2  Sparsity: 69.9443%\n",
      "layer   3  Sparsity: 72.9325%\n",
      "total_backward_count 357335 real_backward_count 39226  10.977%\n",
      "lif layer 2 self.abs_max_v: 5033.0\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.671736/  1.793941, val:  80.83%, val_best:  86.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 39.07 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8231%\n",
      "layer   2  Sparsity: 69.8600%\n",
      "layer   3  Sparsity: 72.9651%\n",
      "total_backward_count 362230 real_backward_count 39456  10.893%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.671756/  1.784731, val:  86.25%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.40 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8892%\n",
      "layer   2  Sparsity: 69.6531%\n",
      "layer   3  Sparsity: 72.4852%\n",
      "total_backward_count 367125 real_backward_count 39691  10.811%\n",
      "fc layer 3 self.abs_max_out: 774.0\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.668357/  1.786538, val:  86.67%, val_best:  86.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.29 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8237%\n",
      "layer   2  Sparsity: 69.6802%\n",
      "layer   3  Sparsity: 72.9492%\n",
      "total_backward_count 372020 real_backward_count 39914  10.729%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.665996/  1.778749, val:  84.58%, val_best:  86.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8917%\n",
      "layer   2  Sparsity: 69.9473%\n",
      "layer   3  Sparsity: 73.2541%\n",
      "total_backward_count 376915 real_backward_count 40119  10.644%\n",
      "lif layer 2 self.abs_max_v: 5177.5\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.654809/  1.775732, val:  82.92%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.27 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.9032%\n",
      "layer   2  Sparsity: 69.8151%\n",
      "layer   3  Sparsity: 73.2306%\n",
      "total_backward_count 381810 real_backward_count 40324  10.561%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.656894/  1.773741, val:  87.08%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.17 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8504%\n",
      "layer   2  Sparsity: 70.1962%\n",
      "layer   3  Sparsity: 73.2178%\n",
      "total_backward_count 386705 real_backward_count 40522  10.479%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.647235/  1.769310, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 38.93 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8860%\n",
      "layer   2  Sparsity: 70.3090%\n",
      "layer   3  Sparsity: 73.4089%\n",
      "total_backward_count 391600 real_backward_count 40714  10.397%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.653595/  1.774868, val:  83.75%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.38 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8665%\n",
      "layer   2  Sparsity: 70.2516%\n",
      "layer   3  Sparsity: 73.2896%\n",
      "total_backward_count 396495 real_backward_count 40949  10.328%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.654350/  1.787310, val:  83.33%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 39.29 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8784%\n",
      "layer   2  Sparsity: 70.3655%\n",
      "layer   3  Sparsity: 73.3521%\n",
      "total_backward_count 401390 real_backward_count 41146  10.251%\n",
      "fc layer 3 self.abs_max_out: 832.0\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.666254/  1.790951, val:  85.83%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.19 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8501%\n",
      "layer   2  Sparsity: 70.1343%\n",
      "layer   3  Sparsity: 73.4289%\n",
      "total_backward_count 406285 real_backward_count 41345  10.176%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.663959/  1.774095, val:  83.75%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8629%\n",
      "layer   2  Sparsity: 69.9120%\n",
      "layer   3  Sparsity: 73.3716%\n",
      "total_backward_count 411180 real_backward_count 41551  10.105%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.655878/  1.768652, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.30 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8267%\n",
      "layer   2  Sparsity: 69.6221%\n",
      "layer   3  Sparsity: 73.5543%\n",
      "total_backward_count 416075 real_backward_count 41734  10.030%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.646094/  1.769069, val:  87.50%, val_best:  88.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.73 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8899%\n",
      "layer   2  Sparsity: 69.4450%\n",
      "layer   3  Sparsity: 73.7732%\n",
      "total_backward_count 420970 real_backward_count 41934   9.961%\n",
      "fc layer 1 self.abs_max_out: 6264.0\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.647285/  1.770291, val:  86.25%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.02 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.9045%\n",
      "layer   2  Sparsity: 69.5931%\n",
      "layer   3  Sparsity: 73.6219%\n",
      "total_backward_count 425865 real_backward_count 42129   9.893%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.652851/  1.771809, val:  87.50%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9119%\n",
      "layer   2  Sparsity: 69.4365%\n",
      "layer   3  Sparsity: 73.8175%\n",
      "total_backward_count 430760 real_backward_count 42320   9.824%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.653649/  1.770083, val:  81.25%, val_best:  88.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 39.35 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8433%\n",
      "layer   2  Sparsity: 69.2512%\n",
      "layer   3  Sparsity: 73.8999%\n",
      "total_backward_count 435655 real_backward_count 42523   9.761%\n",
      "fc layer 1 self.abs_max_out: 6303.0\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.652231/  1.759616, val:  85.00%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.83 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8404%\n",
      "layer   2  Sparsity: 69.3270%\n",
      "layer   3  Sparsity: 74.0025%\n",
      "total_backward_count 440550 real_backward_count 42734   9.700%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.637247/  1.754547, val:  84.58%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.05 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8924%\n",
      "layer   2  Sparsity: 69.5323%\n",
      "layer   3  Sparsity: 74.0933%\n",
      "total_backward_count 445445 real_backward_count 42934   9.638%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.635268/  1.754654, val:  86.25%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.33 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8641%\n",
      "layer   2  Sparsity: 69.2436%\n",
      "layer   3  Sparsity: 73.7472%\n",
      "total_backward_count 450340 real_backward_count 43116   9.574%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.637823/  1.746918, val:  87.92%, val_best:  88.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.14 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8146%\n",
      "layer   2  Sparsity: 69.0462%\n",
      "layer   3  Sparsity: 73.6171%\n",
      "total_backward_count 455235 real_backward_count 43280   9.507%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.632475/  1.755051, val:  87.50%, val_best:  88.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8677%\n",
      "layer   2  Sparsity: 69.1199%\n",
      "layer   3  Sparsity: 73.5103%\n",
      "total_backward_count 460130 real_backward_count 43480   9.450%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.634864/  1.757430, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 38.86 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8234%\n",
      "layer   2  Sparsity: 69.5837%\n",
      "layer   3  Sparsity: 73.8595%\n",
      "total_backward_count 465025 real_backward_count 43638   9.384%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.635234/  1.773085, val:  86.67%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8831%\n",
      "layer   2  Sparsity: 69.7734%\n",
      "layer   3  Sparsity: 73.9174%\n",
      "total_backward_count 469920 real_backward_count 43820   9.325%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.639017/  1.774138, val:  82.08%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.39 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8758%\n",
      "layer   2  Sparsity: 70.3226%\n",
      "layer   3  Sparsity: 73.9052%\n",
      "total_backward_count 474815 real_backward_count 43959   9.258%\n",
      "fc layer 1 self.abs_max_out: 6334.0\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.645558/  1.771746, val:  85.83%, val_best:  88.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9146%\n",
      "layer   2  Sparsity: 69.9345%\n",
      "layer   3  Sparsity: 74.1375%\n",
      "total_backward_count 479710 real_backward_count 44165   9.207%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.641847/  1.765269, val:  85.83%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.04 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8788%\n",
      "layer   2  Sparsity: 69.5824%\n",
      "layer   3  Sparsity: 74.2445%\n",
      "total_backward_count 484605 real_backward_count 44323   9.146%\n",
      "lif layer 1 self.abs_max_v: 9952.0\n",
      "fc layer 1 self.abs_max_out: 6425.0\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.641048/  1.776497, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8845%\n",
      "layer   2  Sparsity: 69.2578%\n",
      "layer   3  Sparsity: 74.1135%\n",
      "total_backward_count 489500 real_backward_count 44495   9.090%\n",
      "fc layer 1 self.abs_max_out: 6457.0\n",
      "lif layer 1 self.abs_max_v: 9984.0\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.639537/  1.764827, val:  85.42%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.01 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8460%\n",
      "layer   2  Sparsity: 69.3077%\n",
      "layer   3  Sparsity: 74.3441%\n",
      "total_backward_count 494395 real_backward_count 44670   9.035%\n",
      "fc layer 2 self.abs_max_out: 3202.0\n",
      "fc layer 1 self.abs_max_out: 6494.0\n",
      "lif layer 1 self.abs_max_v: 10039.5\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.641275/  1.754237, val:  87.08%, val_best:  88.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 40.13 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8549%\n",
      "layer   2  Sparsity: 69.5503%\n",
      "layer   3  Sparsity: 74.0931%\n",
      "total_backward_count 499290 real_backward_count 44848   8.982%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.637387/  1.754826, val:  85.83%, val_best:  88.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 39.32 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8554%\n",
      "layer   2  Sparsity: 69.6430%\n",
      "layer   3  Sparsity: 74.0183%\n",
      "total_backward_count 504185 real_backward_count 45028   8.931%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.628996/  1.748427, val:  85.00%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8394%\n",
      "layer   2  Sparsity: 69.8834%\n",
      "layer   3  Sparsity: 73.8076%\n",
      "total_backward_count 509080 real_backward_count 45218   8.882%\n",
      "fc layer 2 self.abs_max_out: 3282.0\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.627195/  1.750860, val:  86.67%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 38.92 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8572%\n",
      "layer   2  Sparsity: 69.4442%\n",
      "layer   3  Sparsity: 73.5826%\n",
      "total_backward_count 513975 real_backward_count 45377   8.829%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.636434/  1.747314, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 38.80 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8398%\n",
      "layer   2  Sparsity: 69.2755%\n",
      "layer   3  Sparsity: 73.5814%\n",
      "total_backward_count 518870 real_backward_count 45533   8.775%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.636125/  1.748630, val:  85.00%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.53 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8595%\n",
      "layer   2  Sparsity: 69.3402%\n",
      "layer   3  Sparsity: 73.7566%\n",
      "total_backward_count 523765 real_backward_count 45702   8.726%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.625691/  1.742490, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.55 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9072%\n",
      "layer   2  Sparsity: 69.4114%\n",
      "layer   3  Sparsity: 73.9741%\n",
      "total_backward_count 528660 real_backward_count 45818   8.667%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.623665/  1.751450, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.60 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8416%\n",
      "layer   2  Sparsity: 69.4916%\n",
      "layer   3  Sparsity: 73.9148%\n",
      "total_backward_count 533555 real_backward_count 45978   8.617%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.620351/  1.744723, val:  85.42%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8736%\n",
      "layer   2  Sparsity: 69.2775%\n",
      "layer   3  Sparsity: 74.1586%\n",
      "total_backward_count 538450 real_backward_count 46123   8.566%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.610105/  1.737271, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.48 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8588%\n",
      "layer   2  Sparsity: 69.4863%\n",
      "layer   3  Sparsity: 74.2593%\n",
      "total_backward_count 543345 real_backward_count 46259   8.514%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.613528/  1.738860, val:  84.17%, val_best:  88.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.79 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8693%\n",
      "layer   2  Sparsity: 69.5831%\n",
      "layer   3  Sparsity: 74.5390%\n",
      "total_backward_count 548240 real_backward_count 46422   8.467%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.603674/  1.734221, val:  86.25%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.06 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8683%\n",
      "layer   2  Sparsity: 69.6706%\n",
      "layer   3  Sparsity: 74.6208%\n",
      "total_backward_count 553135 real_backward_count 46573   8.420%\n",
      "lif layer 1 self.abs_max_v: 10045.0\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.603324/  1.733100, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8899%\n",
      "layer   2  Sparsity: 69.6143%\n",
      "layer   3  Sparsity: 74.3928%\n",
      "total_backward_count 558030 real_backward_count 46713   8.371%\n",
      "fc layer 3 self.abs_max_out: 842.0\n",
      "fc layer 1 self.abs_max_out: 6571.0\n",
      "lif layer 1 self.abs_max_v: 10227.0\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.596928/  1.725125, val:  88.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.26 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8554%\n",
      "layer   2  Sparsity: 69.4857%\n",
      "layer   3  Sparsity: 74.2626%\n",
      "total_backward_count 562925 real_backward_count 46840   8.321%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.592625/  1.723433, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.34 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8525%\n",
      "layer   2  Sparsity: 69.2829%\n",
      "layer   3  Sparsity: 73.9116%\n",
      "total_backward_count 567820 real_backward_count 46995   8.276%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.590709/  1.723215, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.19 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.9155%\n",
      "layer   2  Sparsity: 69.4319%\n",
      "layer   3  Sparsity: 73.6830%\n",
      "total_backward_count 572715 real_backward_count 47126   8.229%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.592154/  1.719317, val:  85.42%, val_best:  88.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8725%\n",
      "layer   2  Sparsity: 69.4263%\n",
      "layer   3  Sparsity: 73.6509%\n",
      "total_backward_count 577610 real_backward_count 47260   8.182%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.586460/  1.718722, val:  87.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8551%\n",
      "layer   2  Sparsity: 69.4116%\n",
      "layer   3  Sparsity: 73.7007%\n",
      "total_backward_count 582505 real_backward_count 47393   8.136%\n",
      "fc layer 3 self.abs_max_out: 844.0\n",
      "lif layer 2 self.abs_max_v: 5247.5\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.585411/  1.724897, val:  87.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 38.99 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8921%\n",
      "layer   2  Sparsity: 69.4224%\n",
      "layer   3  Sparsity: 74.0130%\n",
      "total_backward_count 587400 real_backward_count 47536   8.093%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.590954/  1.723052, val:  87.08%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8458%\n",
      "layer   2  Sparsity: 69.5948%\n",
      "layer   3  Sparsity: 74.1240%\n",
      "total_backward_count 592295 real_backward_count 47664   8.047%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.588162/  1.730297, val:  85.42%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8343%\n",
      "layer   2  Sparsity: 69.5067%\n",
      "layer   3  Sparsity: 74.4213%\n",
      "total_backward_count 597190 real_backward_count 47794   8.003%\n",
      "fc layer 1 self.abs_max_out: 6677.0\n",
      "lif layer 1 self.abs_max_v: 10321.5\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.597396/  1.747928, val:  85.42%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.50 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8755%\n",
      "layer   2  Sparsity: 69.3187%\n",
      "layer   3  Sparsity: 74.3550%\n",
      "total_backward_count 602085 real_backward_count 47929   7.961%\n",
      "fc layer 1 self.abs_max_out: 6703.0\n",
      "lif layer 1 self.abs_max_v: 10382.0\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.600726/  1.737034, val:  84.58%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.37 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8819%\n",
      "layer   2  Sparsity: 69.4578%\n",
      "layer   3  Sparsity: 74.2739%\n",
      "total_backward_count 606980 real_backward_count 48048   7.916%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.600576/  1.735387, val:  83.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.7995%\n",
      "layer   2  Sparsity: 69.3978%\n",
      "layer   3  Sparsity: 74.5477%\n",
      "total_backward_count 611875 real_backward_count 48194   7.876%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.606680/  1.734740, val:  86.67%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.42 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8186%\n",
      "layer   2  Sparsity: 69.2231%\n",
      "layer   3  Sparsity: 74.7223%\n",
      "total_backward_count 616770 real_backward_count 48318   7.834%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.595627/  1.737331, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8489%\n",
      "layer   2  Sparsity: 69.2944%\n",
      "layer   3  Sparsity: 74.7009%\n",
      "total_backward_count 621665 real_backward_count 48447   7.793%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.603580/  1.741419, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.14 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8653%\n",
      "layer   2  Sparsity: 69.1885%\n",
      "layer   3  Sparsity: 74.4958%\n",
      "total_backward_count 626560 real_backward_count 48573   7.752%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.593241/  1.729543, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8650%\n",
      "layer   2  Sparsity: 68.9080%\n",
      "layer   3  Sparsity: 74.1215%\n",
      "total_backward_count 631455 real_backward_count 48682   7.709%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.595636/  1.720716, val:  85.42%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 38.81 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8589%\n",
      "layer   2  Sparsity: 69.0879%\n",
      "layer   3  Sparsity: 73.9390%\n",
      "total_backward_count 636350 real_backward_count 48799   7.669%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.585011/  1.719127, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8669%\n",
      "layer   2  Sparsity: 69.4230%\n",
      "layer   3  Sparsity: 74.0000%\n",
      "total_backward_count 641245 real_backward_count 48930   7.630%\n",
      "fc layer 1 self.abs_max_out: 6874.0\n",
      "lif layer 1 self.abs_max_v: 10663.0\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.593937/  1.729754, val:  88.33%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 38.76 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8929%\n",
      "layer   2  Sparsity: 69.4627%\n",
      "layer   3  Sparsity: 74.0865%\n",
      "total_backward_count 646140 real_backward_count 49059   7.593%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.589703/  1.719231, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8528%\n",
      "layer   2  Sparsity: 69.4870%\n",
      "layer   3  Sparsity: 74.2386%\n",
      "total_backward_count 651035 real_backward_count 49182   7.554%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.585348/  1.713504, val:  88.33%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.02 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.9000%\n",
      "layer   2  Sparsity: 69.3594%\n",
      "layer   3  Sparsity: 74.1150%\n",
      "total_backward_count 655930 real_backward_count 49295   7.515%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.592481/  1.728061, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.70 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8680%\n",
      "layer   2  Sparsity: 68.7347%\n",
      "layer   3  Sparsity: 74.1283%\n",
      "total_backward_count 660825 real_backward_count 49412   7.477%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.594005/  1.724378, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.50 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8526%\n",
      "layer   2  Sparsity: 68.9792%\n",
      "layer   3  Sparsity: 74.1007%\n",
      "total_backward_count 665720 real_backward_count 49520   7.439%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.589850/  1.714042, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8654%\n",
      "layer   2  Sparsity: 68.8136%\n",
      "layer   3  Sparsity: 74.1933%\n",
      "total_backward_count 670615 real_backward_count 49630   7.401%\n",
      "fc layer 3 self.abs_max_out: 865.0\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.584588/  1.707516, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 38.90 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8402%\n",
      "layer   2  Sparsity: 69.0550%\n",
      "layer   3  Sparsity: 74.4107%\n",
      "total_backward_count 675510 real_backward_count 49746   7.364%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.572670/  1.721506, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8594%\n",
      "layer   2  Sparsity: 69.2719%\n",
      "layer   3  Sparsity: 74.5753%\n",
      "total_backward_count 680405 real_backward_count 49866   7.329%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.580656/  1.719245, val:  85.00%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.26 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8733%\n",
      "layer   2  Sparsity: 69.0368%\n",
      "layer   3  Sparsity: 74.6684%\n",
      "total_backward_count 685300 real_backward_count 49966   7.291%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.576016/  1.708597, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.83 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8719%\n",
      "layer   2  Sparsity: 69.3239%\n",
      "layer   3  Sparsity: 74.7114%\n",
      "total_backward_count 690195 real_backward_count 50078   7.256%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.583318/  1.712882, val:  87.08%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8507%\n",
      "layer   2  Sparsity: 69.1469%\n",
      "layer   3  Sparsity: 74.5075%\n",
      "total_backward_count 695090 real_backward_count 50181   7.219%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.582043/  1.718411, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.16 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8505%\n",
      "layer   2  Sparsity: 69.4333%\n",
      "layer   3  Sparsity: 74.6537%\n",
      "total_backward_count 699985 real_backward_count 50297   7.185%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.579280/  1.727873, val:  87.92%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.58 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8651%\n",
      "layer   2  Sparsity: 69.3969%\n",
      "layer   3  Sparsity: 74.7273%\n",
      "total_backward_count 704880 real_backward_count 50387   7.148%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.588502/  1.714929, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9177%\n",
      "layer   2  Sparsity: 69.0406%\n",
      "layer   3  Sparsity: 74.6304%\n",
      "total_backward_count 709775 real_backward_count 50513   7.117%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.585845/  1.722175, val:  85.83%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.48 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8903%\n",
      "layer   2  Sparsity: 69.4176%\n",
      "layer   3  Sparsity: 74.7876%\n",
      "total_backward_count 714670 real_backward_count 50624   7.084%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.594444/  1.721107, val:  85.83%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.37 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8330%\n",
      "layer   2  Sparsity: 69.2966%\n",
      "layer   3  Sparsity: 74.8308%\n",
      "total_backward_count 719565 real_backward_count 50735   7.051%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.583673/  1.712099, val:  87.92%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8384%\n",
      "layer   2  Sparsity: 69.3559%\n",
      "layer   3  Sparsity: 74.7865%\n",
      "total_backward_count 724460 real_backward_count 50833   7.017%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.585021/  1.717644, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.17 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8154%\n",
      "layer   2  Sparsity: 69.0167%\n",
      "layer   3  Sparsity: 74.9774%\n",
      "total_backward_count 729355 real_backward_count 50928   6.983%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.582744/  1.711037, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8546%\n",
      "layer   2  Sparsity: 69.0938%\n",
      "layer   3  Sparsity: 75.0021%\n",
      "total_backward_count 734250 real_backward_count 51016   6.948%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.573846/  1.706458, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.27 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8559%\n",
      "layer   2  Sparsity: 69.1707%\n",
      "layer   3  Sparsity: 74.9410%\n",
      "total_backward_count 739145 real_backward_count 51099   6.913%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.576019/  1.715837, val:  86.67%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.53 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8694%\n",
      "layer   2  Sparsity: 69.1989%\n",
      "layer   3  Sparsity: 74.9623%\n",
      "total_backward_count 744040 real_backward_count 51188   6.880%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.580590/  1.709947, val:  85.00%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 38.82 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8722%\n",
      "layer   2  Sparsity: 68.9601%\n",
      "layer   3  Sparsity: 75.0701%\n",
      "total_backward_count 748935 real_backward_count 51288   6.848%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.576516/  1.704197, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.91 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8937%\n",
      "layer   2  Sparsity: 69.1344%\n",
      "layer   3  Sparsity: 75.1181%\n",
      "total_backward_count 753830 real_backward_count 51375   6.815%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.573681/  1.705269, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.13 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8184%\n",
      "layer   2  Sparsity: 69.3693%\n",
      "layer   3  Sparsity: 75.2255%\n",
      "total_backward_count 758725 real_backward_count 51457   6.782%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.564788/  1.707713, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8856%\n",
      "layer   2  Sparsity: 69.4171%\n",
      "layer   3  Sparsity: 75.1922%\n",
      "total_backward_count 763620 real_backward_count 51547   6.750%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.556207/  1.691606, val:  86.67%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.18 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8573%\n",
      "layer   2  Sparsity: 69.4411%\n",
      "layer   3  Sparsity: 75.4659%\n",
      "total_backward_count 768515 real_backward_count 51628   6.718%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.558826/  1.702248, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8612%\n",
      "layer   2  Sparsity: 69.3893%\n",
      "layer   3  Sparsity: 75.4975%\n",
      "total_backward_count 773410 real_backward_count 51722   6.688%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.567095/  1.704732, val:  85.42%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8592%\n",
      "layer   2  Sparsity: 69.1293%\n",
      "layer   3  Sparsity: 75.6048%\n",
      "total_backward_count 778305 real_backward_count 51798   6.655%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.565332/  1.707685, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.51 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8435%\n",
      "layer   2  Sparsity: 69.0677%\n",
      "layer   3  Sparsity: 75.4278%\n",
      "total_backward_count 783200 real_backward_count 51877   6.624%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.564235/  1.701882, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9394%\n",
      "layer   2  Sparsity: 69.1762%\n",
      "layer   3  Sparsity: 75.5157%\n",
      "total_backward_count 788095 real_backward_count 51960   6.593%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.567328/  1.715216, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.37 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8288%\n",
      "layer   2  Sparsity: 69.2153%\n",
      "layer   3  Sparsity: 75.5610%\n",
      "total_backward_count 792990 real_backward_count 52033   6.562%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.565398/  1.703508, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.32 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8600%\n",
      "layer   2  Sparsity: 69.3309%\n",
      "layer   3  Sparsity: 75.6114%\n",
      "total_backward_count 797885 real_backward_count 52100   6.530%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.564089/  1.697098, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.54 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9106%\n",
      "layer   2  Sparsity: 69.2046%\n",
      "layer   3  Sparsity: 75.6090%\n",
      "total_backward_count 802780 real_backward_count 52188   6.501%\n",
      "fc layer 2 self.abs_max_out: 3326.0\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.566914/  1.707510, val:  86.67%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.43 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8253%\n",
      "layer   2  Sparsity: 69.2593%\n",
      "layer   3  Sparsity: 75.4196%\n",
      "total_backward_count 807675 real_backward_count 52308   6.476%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.568845/  1.710604, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8552%\n",
      "layer   2  Sparsity: 69.1949%\n",
      "layer   3  Sparsity: 75.4025%\n",
      "total_backward_count 812570 real_backward_count 52390   6.447%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.571887/  1.711753, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.31 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9076%\n",
      "layer   2  Sparsity: 69.4005%\n",
      "layer   3  Sparsity: 75.3015%\n",
      "total_backward_count 817465 real_backward_count 52485   6.420%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.574136/  1.725323, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.20 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8907%\n",
      "layer   2  Sparsity: 68.9930%\n",
      "layer   3  Sparsity: 75.2521%\n",
      "total_backward_count 822360 real_backward_count 52585   6.394%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.584592/  1.721406, val:  85.42%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.33 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8238%\n",
      "layer   2  Sparsity: 68.6888%\n",
      "layer   3  Sparsity: 75.1927%\n",
      "total_backward_count 827255 real_backward_count 52671   6.367%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.579331/  1.718142, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.31 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8839%\n",
      "layer   2  Sparsity: 68.7747%\n",
      "layer   3  Sparsity: 75.1973%\n",
      "total_backward_count 832150 real_backward_count 52746   6.339%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.584565/  1.727580, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.53 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.7795%\n",
      "layer   2  Sparsity: 68.7751%\n",
      "layer   3  Sparsity: 75.1281%\n",
      "total_backward_count 837045 real_backward_count 52827   6.311%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.581975/  1.711828, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.29 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8609%\n",
      "layer   2  Sparsity: 68.9687%\n",
      "layer   3  Sparsity: 75.0295%\n",
      "total_backward_count 841940 real_backward_count 52908   6.284%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.570189/  1.709890, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.35 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8533%\n",
      "layer   2  Sparsity: 69.1051%\n",
      "layer   3  Sparsity: 75.2491%\n",
      "total_backward_count 846835 real_backward_count 52998   6.258%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.572959/  1.710989, val:  86.67%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.22 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8653%\n",
      "layer   2  Sparsity: 69.0199%\n",
      "layer   3  Sparsity: 75.2362%\n",
      "total_backward_count 851730 real_backward_count 53096   6.234%\n",
      "fc layer 2 self.abs_max_out: 3409.0\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.569268/  1.705372, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8844%\n",
      "layer   2  Sparsity: 69.1025%\n",
      "layer   3  Sparsity: 75.2435%\n",
      "total_backward_count 856625 real_backward_count 53190   6.209%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.569960/  1.695324, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.42 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8893%\n",
      "layer   2  Sparsity: 68.8696%\n",
      "layer   3  Sparsity: 75.2749%\n",
      "total_backward_count 861520 real_backward_count 53283   6.185%\n",
      "lif layer 2 self.abs_max_v: 5271.5\n",
      "lif layer 2 self.abs_max_v: 5427.0\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.560868/  1.699465, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.51 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9205%\n",
      "layer   2  Sparsity: 68.9144%\n",
      "layer   3  Sparsity: 75.4478%\n",
      "total_backward_count 866415 real_backward_count 53362   6.159%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.559277/  1.697877, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8411%\n",
      "layer   2  Sparsity: 68.9338%\n",
      "layer   3  Sparsity: 75.1417%\n",
      "total_backward_count 871310 real_backward_count 53435   6.133%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.562132/  1.695526, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.68 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8369%\n",
      "layer   2  Sparsity: 69.2293%\n",
      "layer   3  Sparsity: 75.0936%\n",
      "total_backward_count 876205 real_backward_count 53496   6.105%\n",
      "lif layer 2 self.abs_max_v: 5453.0\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.558695/  1.695396, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8910%\n",
      "layer   2  Sparsity: 69.1151%\n",
      "layer   3  Sparsity: 74.9415%\n",
      "total_backward_count 881100 real_backward_count 53565   6.079%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.555113/  1.709469, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.10 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8480%\n",
      "layer   2  Sparsity: 69.4495%\n",
      "layer   3  Sparsity: 74.9067%\n",
      "total_backward_count 885995 real_backward_count 53636   6.054%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.564438/  1.711026, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.40 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8035%\n",
      "layer   2  Sparsity: 69.2935%\n",
      "layer   3  Sparsity: 75.0603%\n",
      "total_backward_count 890890 real_backward_count 53721   6.030%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.559907/  1.690743, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8373%\n",
      "layer   2  Sparsity: 69.0344%\n",
      "layer   3  Sparsity: 75.1836%\n",
      "total_backward_count 895785 real_backward_count 53792   6.005%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.552260/  1.696145, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8959%\n",
      "layer   2  Sparsity: 69.3510%\n",
      "layer   3  Sparsity: 75.0733%\n",
      "total_backward_count 900680 real_backward_count 53877   5.982%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.553398/  1.700650, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8702%\n",
      "layer   2  Sparsity: 69.3911%\n",
      "layer   3  Sparsity: 75.3084%\n",
      "total_backward_count 905575 real_backward_count 53947   5.957%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.560356/  1.698739, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.82 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8088%\n",
      "layer   2  Sparsity: 69.2893%\n",
      "layer   3  Sparsity: 75.3098%\n",
      "total_backward_count 910470 real_backward_count 54016   5.933%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.556497/  1.694934, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.84 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8874%\n",
      "layer   2  Sparsity: 69.2317%\n",
      "layer   3  Sparsity: 75.3537%\n",
      "total_backward_count 915365 real_backward_count 54066   5.906%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.553477/  1.694650, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8775%\n",
      "layer   2  Sparsity: 69.3761%\n",
      "layer   3  Sparsity: 75.4071%\n",
      "total_backward_count 920260 real_backward_count 54125   5.881%\n",
      "fc layer 1 self.abs_max_out: 6897.0\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.561875/  1.700075, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.83 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8535%\n",
      "layer   2  Sparsity: 69.4784%\n",
      "layer   3  Sparsity: 75.3674%\n",
      "total_backward_count 925155 real_backward_count 54212   5.860%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.561327/  1.705447, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8437%\n",
      "layer   2  Sparsity: 69.4400%\n",
      "layer   3  Sparsity: 75.4152%\n",
      "total_backward_count 930050 real_backward_count 54271   5.835%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.558239/  1.697381, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.89 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8464%\n",
      "layer   2  Sparsity: 69.4432%\n",
      "layer   3  Sparsity: 75.4044%\n",
      "total_backward_count 934945 real_backward_count 54327   5.811%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.555943/  1.702087, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8463%\n",
      "layer   2  Sparsity: 69.0202%\n",
      "layer   3  Sparsity: 75.3296%\n",
      "total_backward_count 939840 real_backward_count 54404   5.789%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.554649/  1.685461, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8605%\n",
      "layer   2  Sparsity: 69.0511%\n",
      "layer   3  Sparsity: 75.2518%\n",
      "total_backward_count 944735 real_backward_count 54475   5.766%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.548032/  1.686636, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.42 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8641%\n",
      "layer   2  Sparsity: 69.3014%\n",
      "layer   3  Sparsity: 75.4150%\n",
      "total_backward_count 949630 real_backward_count 54523   5.741%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.547333/  1.690408, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8705%\n",
      "layer   2  Sparsity: 69.3973%\n",
      "layer   3  Sparsity: 75.3273%\n",
      "total_backward_count 954525 real_backward_count 54574   5.717%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.547967/  1.686614, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.54 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8267%\n",
      "layer   2  Sparsity: 69.2559%\n",
      "layer   3  Sparsity: 75.4717%\n",
      "total_backward_count 959420 real_backward_count 54651   5.696%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.546608/  1.680624, val:  87.50%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8760%\n",
      "layer   2  Sparsity: 69.0573%\n",
      "layer   3  Sparsity: 75.4532%\n",
      "total_backward_count 964315 real_backward_count 54713   5.674%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.548877/  1.689533, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8955%\n",
      "layer   2  Sparsity: 68.9120%\n",
      "layer   3  Sparsity: 75.6330%\n",
      "total_backward_count 969210 real_backward_count 54783   5.652%\n",
      "fc layer 1 self.abs_max_out: 6948.0\n",
      "lif layer 1 self.abs_max_v: 10753.5\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.548005/  1.682806, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.65 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8483%\n",
      "layer   2  Sparsity: 69.1436%\n",
      "layer   3  Sparsity: 75.2309%\n",
      "total_backward_count 974105 real_backward_count 54859   5.632%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.541558/  1.670825, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8678%\n",
      "layer   2  Sparsity: 69.2436%\n",
      "layer   3  Sparsity: 75.2173%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be74e409a432464c85f64d529b0710bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.54156</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>1.67083</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">daily-sweep-320</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9id08g8c' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9id08g8c</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251127_142559-9id08g8c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lrzrir77 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 50000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251127_163825-lrzrir77</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lrzrir77' target=\"_blank\">distinctive-sweep-325</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lrzrir77' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lrzrir77</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251127_163834_272', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 50000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = dac77cc348b2d880ae59906e26f08f17\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 139\n",
      "fc layer 1 self.abs_max_out: 797.0\n",
      "lif layer 1 self.abs_max_v: 797.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 2018.0\n",
      "lif layer 2 self.abs_max_v: 2018.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 630.0\n",
      "fc layer 1 self.abs_max_out: 969.0\n",
      "lif layer 1 self.abs_max_v: 1030.5\n",
      "lif layer 2 self.abs_max_v: 2366.0\n",
      "fc layer 1 self.abs_max_out: 1074.0\n",
      "lif layer 1 self.abs_max_v: 1312.0\n",
      "lif layer 2 self.abs_max_v: 3087.0\n",
      "lif layer 1 self.abs_max_v: 1357.0\n",
      "fc layer 3 self.abs_max_out: 713.0\n",
      "fc layer 1 self.abs_max_out: 1258.0\n",
      "lif layer 1 self.abs_max_v: 1402.5\n",
      "fc layer 2 self.abs_max_out: 2050.0\n",
      "smallest_now_T updated: 125\n",
      "fc layer 1 self.abs_max_out: 1427.0\n",
      "lif layer 1 self.abs_max_v: 1427.0\n",
      "fc layer 2 self.abs_max_out: 2117.0\n",
      "lif layer 1 self.abs_max_v: 1511.0\n",
      "fc layer 2 self.abs_max_out: 2131.0\n",
      "fc layer 1 self.abs_max_out: 1549.0\n",
      "lif layer 1 self.abs_max_v: 1651.5\n",
      "fc layer 2 self.abs_max_out: 2201.0\n",
      "lif layer 2 self.abs_max_v: 3131.5\n",
      "lif layer 1 self.abs_max_v: 2029.0\n",
      "fc layer 2 self.abs_max_out: 2343.0\n",
      "smallest_now_T updated: 94\n",
      "fc layer 1 self.abs_max_out: 1577.0\n",
      "fc layer 3 self.abs_max_out: 924.0\n",
      "fc layer 3 self.abs_max_out: 1010.0\n",
      "fc layer 1 self.abs_max_out: 1690.0\n",
      "lif layer 2 self.abs_max_v: 3316.0\n",
      "fc layer 2 self.abs_max_out: 2361.0\n",
      "lif layer 2 self.abs_max_v: 3669.5\n",
      "fc layer 1 self.abs_max_out: 1855.0\n",
      "lif layer 1 self.abs_max_v: 2219.5\n",
      "lif layer 1 self.abs_max_v: 2575.5\n",
      "fc layer 1 self.abs_max_out: 1946.0\n",
      "lif layer 1 self.abs_max_v: 2803.5\n",
      "fc layer 2 self.abs_max_out: 2702.0\n",
      "lif layer 2 self.abs_max_v: 3822.5\n",
      "fc layer 1 self.abs_max_out: 2273.0\n",
      "lif layer 2 self.abs_max_v: 4028.0\n",
      "fc layer 2 self.abs_max_out: 2719.0\n",
      "lif layer 2 self.abs_max_v: 4627.5\n",
      "smallest_now_T updated: 79\n",
      "fc layer 1 self.abs_max_out: 2336.0\n",
      "lif layer 1 self.abs_max_v: 2869.0\n",
      "fc layer 2 self.abs_max_out: 2795.0\n",
      "fc layer 1 self.abs_max_out: 2390.0\n",
      "fc layer 1 self.abs_max_out: 2572.0\n",
      "lif layer 1 self.abs_max_v: 3390.5\n",
      "lif layer 1 self.abs_max_v: 3929.5\n",
      "fc layer 1 self.abs_max_out: 2838.0\n",
      "lif layer 1 self.abs_max_v: 4035.0\n",
      "fc layer 1 self.abs_max_out: 2841.0\n",
      "lif layer 1 self.abs_max_v: 4329.5\n",
      "fc layer 2 self.abs_max_out: 2816.0\n",
      "fc layer 2 self.abs_max_out: 2839.0\n",
      "smallest_now_T updated: 73\n",
      "fc layer 1 self.abs_max_out: 3362.0\n",
      "fc layer 2 self.abs_max_out: 2976.0\n",
      "lif layer 2 self.abs_max_v: 4996.5\n",
      "lif layer 1 self.abs_max_v: 4959.0\n",
      "fc layer 2 self.abs_max_out: 2994.0\n",
      "smallest_now_T updated: 65\n",
      "fc layer 2 self.abs_max_out: 2997.0\n",
      "lif layer 2 self.abs_max_v: 5023.0\n",
      "fc layer 2 self.abs_max_out: 3037.0\n",
      "lif layer 2 self.abs_max_v: 5101.5\n",
      "fc layer 1 self.abs_max_out: 4847.0\n",
      "lif layer 1 self.abs_max_v: 5390.5\n",
      "lif layer 1 self.abs_max_v: 5907.5\n",
      "fc layer 2 self.abs_max_out: 3164.0\n",
      "lif layer 2 self.abs_max_v: 5213.5\n",
      "lif layer 1 self.abs_max_v: 7475.0\n",
      "lif layer 2 self.abs_max_v: 5304.0\n",
      "lif layer 2 self.abs_max_v: 5502.5\n",
      "lif layer 2 self.abs_max_v: 5647.0\n",
      "fc layer 3 self.abs_max_out: 1024.0\n",
      "fc layer 2 self.abs_max_out: 3200.0\n",
      "fc layer 2 self.abs_max_out: 3202.0\n",
      "fc layer 2 self.abs_max_out: 3449.0\n",
      "lif layer 2 self.abs_max_v: 5863.0\n",
      "lif layer 2 self.abs_max_v: 6110.5\n",
      "smallest_now_T updated: 56\n",
      "smallest_now_T updated: 43\n",
      "fc layer 3 self.abs_max_out: 1033.0\n",
      "fc layer 3 self.abs_max_out: 1130.0\n",
      "fc layer 3 self.abs_max_out: 1159.0\n",
      "fc layer 2 self.abs_max_out: 3552.0\n",
      "fc layer 3 self.abs_max_out: 1227.0\n",
      "lif layer 2 self.abs_max_v: 6192.5\n",
      "fc layer 2 self.abs_max_out: 3574.0\n",
      "fc layer 2 self.abs_max_out: 3766.0\n",
      "lif layer 2 self.abs_max_v: 6213.0\n",
      "fc layer 1 self.abs_max_out: 5103.0\n",
      "lif layer 1 self.abs_max_v: 8287.0\n",
      "lif layer 1 self.abs_max_v: 8455.5\n",
      "lif layer 2 self.abs_max_v: 6422.0\n",
      "lif layer 1 self.abs_max_v: 8565.5\n",
      "lif layer 2 self.abs_max_v: 6768.0\n",
      "fc layer 3 self.abs_max_out: 1424.0\n",
      "fc layer 1 self.abs_max_out: 5259.0\n",
      "lif layer 2 self.abs_max_v: 6792.5\n",
      "fc layer 2 self.abs_max_out: 3779.0\n",
      "lif layer 2 self.abs_max_v: 7175.5\n",
      "fc layer 2 self.abs_max_out: 3807.0\n",
      "fc layer 1 self.abs_max_out: 5293.0\n",
      "fc layer 1 self.abs_max_out: 5484.0\n",
      "fc layer 1 self.abs_max_out: 5694.0\n",
      "lif layer 1 self.abs_max_v: 9354.0\n",
      "fc layer 2 self.abs_max_out: 3817.0\n",
      "fc layer 1 self.abs_max_out: 5703.0\n",
      "lif layer 1 self.abs_max_v: 9994.0\n",
      "smallest_now_T_val updated: 129\n",
      "smallest_now_T_val updated: 106\n",
      "smallest_now_T_val updated: 104\n",
      "smallest_now_T_val updated: 102\n",
      "smallest_now_T_val updated: 85\n",
      "smallest_now_T_val updated: 29\n",
      "fc layer 1 self.abs_max_out: 6348.0\n",
      "fc layer 2 self.abs_max_out: 3931.0\n",
      "fc layer 1 self.abs_max_out: 6602.0\n",
      "lif layer 1 self.abs_max_v: 10054.5\n",
      "lif layer 1 self.abs_max_v: 10190.0\n",
      "lif layer 1 self.abs_max_v: 10884.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.930702/  2.024813, val:  34.17%, val_best:  34.17%, tr:  81.82%, tr_best:  81.82%, epoch time: 39.79 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5051%\n",
      "layer   2  Sparsity: 62.5508%\n",
      "layer   3  Sparsity: 62.1066%\n",
      "total_backward_count 4895 real_backward_count 1610  32.891%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 4086.0\n",
      "lif layer 2 self.abs_max_v: 7274.5\n",
      "fc layer 2 self.abs_max_out: 4113.0\n",
      "fc layer 2 self.abs_max_out: 4224.0\n",
      "fc layer 2 self.abs_max_out: 4567.0\n",
      "fc layer 2 self.abs_max_out: 4643.0\n",
      "fc layer 2 self.abs_max_out: 4689.0\n",
      "fc layer 2 self.abs_max_out: 4787.0\n",
      "fc layer 2 self.abs_max_out: 4811.0\n",
      "lif layer 2 self.abs_max_v: 7388.5\n",
      "fc layer 2 self.abs_max_out: 4896.0\n",
      "fc layer 2 self.abs_max_out: 5054.0\n",
      "lif layer 2 self.abs_max_v: 7441.0\n",
      "fc layer 1 self.abs_max_out: 6806.0\n",
      "fc layer 2 self.abs_max_out: 5057.0\n",
      "fc layer 2 self.abs_max_out: 5058.0\n",
      "fc layer 1 self.abs_max_out: 7918.0\n",
      "lif layer 1 self.abs_max_v: 11144.0\n",
      "lif layer 1 self.abs_max_v: 12334.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.856085/  1.987993, val:  47.92%, val_best:  47.92%, tr:  86.52%, tr_best:  86.52%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.4966%\n",
      "layer   2  Sparsity: 66.3848%\n",
      "layer   3  Sparsity: 62.3657%\n",
      "total_backward_count 9790 real_backward_count 2898  29.602%\n",
      "lif layer 2 self.abs_max_v: 7802.5\n",
      "fc layer 2 self.abs_max_out: 5095.0\n",
      "fc layer 2 self.abs_max_out: 5384.0\n",
      "fc layer 1 self.abs_max_out: 8548.0\n",
      "lif layer 1 self.abs_max_v: 12520.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.831758/  1.963623, val:  50.42%, val_best:  50.42%, tr:  90.19%, tr_best:  90.19%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5602%\n",
      "layer   2  Sparsity: 68.6427%\n",
      "layer   3  Sparsity: 61.6350%\n",
      "total_backward_count 14685 real_backward_count 4039  27.504%\n",
      "lif layer 1 self.abs_max_v: 12608.0\n",
      "lif layer 1 self.abs_max_v: 13037.0\n",
      "fc layer 2 self.abs_max_out: 5462.0\n",
      "fc layer 1 self.abs_max_out: 9345.0\n",
      "lif layer 1 self.abs_max_v: 13259.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.820360/  1.955849, val:  42.08%, val_best:  50.42%, tr:  89.99%, tr_best:  90.19%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5372%\n",
      "layer   2  Sparsity: 69.9025%\n",
      "layer   3  Sparsity: 62.2649%\n",
      "total_backward_count 19580 real_backward_count 5134  26.221%\n",
      "lif layer 1 self.abs_max_v: 13619.5\n",
      "fc layer 1 self.abs_max_out: 10205.0\n",
      "lif layer 1 self.abs_max_v: 14260.5\n",
      "lif layer 1 self.abs_max_v: 15788.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.812556/  1.958583, val:  53.75%, val_best:  53.75%, tr:  93.16%, tr_best:  93.16%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4704%\n",
      "layer   2  Sparsity: 70.3076%\n",
      "layer   3  Sparsity: 61.6898%\n",
      "total_backward_count 24475 real_backward_count 6107  24.952%\n",
      "lif layer 2 self.abs_max_v: 7804.5\n",
      "lif layer 2 self.abs_max_v: 7963.5\n",
      "fc layer 2 self.abs_max_out: 5650.0\n",
      "fc layer 1 self.abs_max_out: 10370.0\n",
      "lif layer 1 self.abs_max_v: 16233.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.815715/  1.942958, val:  55.00%, val_best:  55.00%, tr:  92.95%, tr_best:  93.16%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5431%\n",
      "layer   2  Sparsity: 70.7259%\n",
      "layer   3  Sparsity: 61.7332%\n",
      "total_backward_count 29370 real_backward_count 7047  23.994%\n",
      "lif layer 2 self.abs_max_v: 8271.0\n",
      "fc layer 1 self.abs_max_out: 10710.0\n",
      "lif layer 1 self.abs_max_v: 16673.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.795266/  1.950624, val:  50.83%, val_best:  55.00%, tr:  91.32%, tr_best:  93.16%, epoch time: 39.82 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5193%\n",
      "layer   2  Sparsity: 71.3925%\n",
      "layer   3  Sparsity: 63.5528%\n",
      "total_backward_count 34265 real_backward_count 8029  23.432%\n",
      "lif layer 2 self.abs_max_v: 8333.5\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.803556/  1.946520, val:  57.08%, val_best:  57.08%, tr:  92.24%, tr_best:  93.16%, epoch time: 39.79 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5279%\n",
      "layer   2  Sparsity: 70.0244%\n",
      "layer   3  Sparsity: 64.8722%\n",
      "total_backward_count 39160 real_backward_count 8992  22.962%\n",
      "fc layer 1 self.abs_max_out: 10917.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.823472/  1.951279, val:  51.67%, val_best:  57.08%, tr:  93.36%, tr_best:  93.36%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5069%\n",
      "layer   2  Sparsity: 70.0656%\n",
      "layer   3  Sparsity: 65.5337%\n",
      "total_backward_count 44055 real_backward_count 9931  22.542%\n",
      "lif layer 2 self.abs_max_v: 8636.5\n",
      "fc layer 1 self.abs_max_out: 11457.0\n",
      "lif layer 1 self.abs_max_v: 17351.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.809732/  1.917190, val:  62.08%, val_best:  62.08%, tr:  94.28%, tr_best:  94.28%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5480%\n",
      "layer   2  Sparsity: 70.4689%\n",
      "layer   3  Sparsity: 66.5080%\n",
      "total_backward_count 48950 real_backward_count 10872  22.210%\n",
      "fc layer 1 self.abs_max_out: 11588.0\n",
      "lif layer 1 self.abs_max_v: 17790.5\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.791921/  1.924154, val:  40.42%, val_best:  62.08%, tr:  94.79%, tr_best:  94.79%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4839%\n",
      "layer   2  Sparsity: 71.5008%\n",
      "layer   3  Sparsity: 67.0567%\n",
      "total_backward_count 53845 real_backward_count 11727  21.779%\n",
      "lif layer 2 self.abs_max_v: 8900.5\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.796528/  1.931360, val:  67.50%, val_best:  67.50%, tr:  94.69%, tr_best:  94.79%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4922%\n",
      "layer   2  Sparsity: 71.0450%\n",
      "layer   3  Sparsity: 66.1206%\n",
      "total_backward_count 58740 real_backward_count 12597  21.445%\n",
      "fc layer 1 self.abs_max_out: 11923.0\n",
      "lif layer 1 self.abs_max_v: 18387.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.782494/  1.934765, val:  62.92%, val_best:  67.50%, tr:  95.81%, tr_best:  95.81%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.4843%\n",
      "layer   2  Sparsity: 70.7225%\n",
      "layer   3  Sparsity: 66.1293%\n",
      "total_backward_count 63635 real_backward_count 13440  21.120%\n",
      "lif layer 2 self.abs_max_v: 9125.5\n",
      "fc layer 1 self.abs_max_out: 12074.0\n",
      "lif layer 1 self.abs_max_v: 18665.5\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.787790/  1.918807, val:  62.50%, val_best:  67.50%, tr:  95.51%, tr_best:  95.81%, epoch time: 39.42 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4934%\n",
      "layer   2  Sparsity: 71.1697%\n",
      "layer   3  Sparsity: 66.1858%\n",
      "total_backward_count 68530 real_backward_count 14238  20.776%\n",
      "lif layer 2 self.abs_max_v: 9137.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.779362/  1.916898, val:  61.67%, val_best:  67.50%, tr:  96.94%, tr_best:  96.94%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5534%\n",
      "layer   2  Sparsity: 70.6595%\n",
      "layer   3  Sparsity: 66.4083%\n",
      "total_backward_count 73425 real_backward_count 15000  20.429%\n",
      "fc layer 1 self.abs_max_out: 12145.0\n",
      "lif layer 1 self.abs_max_v: 19140.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.793282/  1.903948, val:  65.42%, val_best:  67.50%, tr:  95.81%, tr_best:  96.94%, epoch time: 39.25 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 90.5112%\n",
      "layer   2  Sparsity: 70.5178%\n",
      "layer   3  Sparsity: 67.2497%\n",
      "total_backward_count 78320 real_backward_count 15729  20.083%\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.784261/  1.905766, val:  70.42%, val_best:  70.42%, tr:  96.02%, tr_best:  96.94%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.4921%\n",
      "layer   2  Sparsity: 70.7687%\n",
      "layer   3  Sparsity: 68.5457%\n",
      "total_backward_count 83215 real_backward_count 16477  19.801%\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.790258/  1.893824, val:  71.25%, val_best:  71.25%, tr:  97.24%, tr_best:  97.24%, epoch time: 39.43 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5009%\n",
      "layer   2  Sparsity: 70.4924%\n",
      "layer   3  Sparsity: 69.0085%\n",
      "total_backward_count 88110 real_backward_count 17160  19.476%\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.801822/  1.909353, val:  72.92%, val_best:  72.92%, tr:  97.14%, tr_best:  97.24%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5338%\n",
      "layer   2  Sparsity: 70.0229%\n",
      "layer   3  Sparsity: 69.9097%\n",
      "total_backward_count 93005 real_backward_count 17843  19.185%\n",
      "fc layer 3 self.abs_max_out: 1451.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.779655/  1.917426, val:  68.75%, val_best:  72.92%, tr:  96.83%, tr_best:  97.24%, epoch time: 39.83 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5326%\n",
      "layer   2  Sparsity: 69.2701%\n",
      "layer   3  Sparsity: 69.0326%\n",
      "total_backward_count 97900 real_backward_count 18470  18.866%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.792630/  1.906832, val:  66.67%, val_best:  72.92%, tr:  98.06%, tr_best:  98.06%, epoch time: 40.13 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.4994%\n",
      "layer   2  Sparsity: 69.6975%\n",
      "layer   3  Sparsity: 69.4118%\n",
      "total_backward_count 102795 real_backward_count 19088  18.569%\n",
      "lif layer 2 self.abs_max_v: 9206.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.773970/  1.895549, val:  73.33%, val_best:  73.33%, tr:  97.65%, tr_best:  98.06%, epoch time: 39.33 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5236%\n",
      "layer   2  Sparsity: 70.1167%\n",
      "layer   3  Sparsity: 69.4444%\n",
      "total_backward_count 107690 real_backward_count 19692  18.286%\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.804459/  1.902992, val:  75.83%, val_best:  75.83%, tr:  97.45%, tr_best:  98.06%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5021%\n",
      "layer   2  Sparsity: 69.2152%\n",
      "layer   3  Sparsity: 68.8282%\n",
      "total_backward_count 112585 real_backward_count 20348  18.073%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.781042/  1.905640, val:  72.08%, val_best:  75.83%, tr:  97.55%, tr_best:  98.06%, epoch time: 39.70 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4738%\n",
      "layer   2  Sparsity: 69.3320%\n",
      "layer   3  Sparsity: 69.2293%\n",
      "total_backward_count 117480 real_backward_count 20960  17.841%\n",
      "lif layer 2 self.abs_max_v: 9280.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.786546/  1.901356, val:  77.08%, val_best:  77.08%, tr:  97.96%, tr_best:  98.06%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.4884%\n",
      "layer   2  Sparsity: 69.1107%\n",
      "layer   3  Sparsity: 70.1274%\n",
      "total_backward_count 122375 real_backward_count 21566  17.623%\n",
      "lif layer 2 self.abs_max_v: 9338.0\n",
      "lif layer 1 self.abs_max_v: 19526.0\n",
      "fc layer 1 self.abs_max_out: 12302.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.786648/  1.898037, val:  78.75%, val_best:  78.75%, tr:  97.45%, tr_best:  98.06%, epoch time: 39.11 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 90.5334%\n",
      "layer   2  Sparsity: 69.4596%\n",
      "layer   3  Sparsity: 69.9725%\n",
      "total_backward_count 127270 real_backward_count 22181  17.428%\n",
      "lif layer 2 self.abs_max_v: 9543.5\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.776708/  1.901427, val:  79.58%, val_best:  79.58%, tr:  98.16%, tr_best:  98.16%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5309%\n",
      "layer   2  Sparsity: 69.6280%\n",
      "layer   3  Sparsity: 70.7275%\n",
      "total_backward_count 132165 real_backward_count 22746  17.210%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.792365/  1.906163, val:  78.75%, val_best:  79.58%, tr:  98.06%, tr_best:  98.16%, epoch time: 39.79 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5128%\n",
      "layer   2  Sparsity: 69.3322%\n",
      "layer   3  Sparsity: 71.8488%\n",
      "total_backward_count 137060 real_backward_count 23302  17.001%\n",
      "lif layer 2 self.abs_max_v: 9731.5\n",
      "lif layer 2 self.abs_max_v: 9760.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.799415/  1.901558, val:  77.92%, val_best:  79.58%, tr:  98.47%, tr_best:  98.47%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5293%\n",
      "layer   2  Sparsity: 69.0207%\n",
      "layer   3  Sparsity: 72.1103%\n",
      "total_backward_count 141955 real_backward_count 23847  16.799%\n",
      "lif layer 2 self.abs_max_v: 9767.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.807496/  1.911124, val:  73.75%, val_best:  79.58%, tr:  98.67%, tr_best:  98.67%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5321%\n",
      "layer   2  Sparsity: 69.0803%\n",
      "layer   3  Sparsity: 71.5414%\n",
      "total_backward_count 146850 real_backward_count 24380  16.602%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.793782/  1.887069, val:  73.33%, val_best:  79.58%, tr:  98.57%, tr_best:  98.67%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4828%\n",
      "layer   2  Sparsity: 69.2822%\n",
      "layer   3  Sparsity: 71.3969%\n",
      "total_backward_count 151745 real_backward_count 24917  16.420%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.780588/  1.901538, val:  77.08%, val_best:  79.58%, tr:  97.75%, tr_best:  98.67%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5444%\n",
      "layer   2  Sparsity: 68.6011%\n",
      "layer   3  Sparsity: 70.8808%\n",
      "total_backward_count 156640 real_backward_count 25462  16.255%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.788489/  1.890875, val:  77.08%, val_best:  79.58%, tr:  98.88%, tr_best:  98.88%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4939%\n",
      "layer   2  Sparsity: 69.0368%\n",
      "layer   3  Sparsity: 71.5203%\n",
      "total_backward_count 161535 real_backward_count 25955  16.068%\n",
      "lif layer 2 self.abs_max_v: 9791.5\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.789209/  1.907849, val:  72.08%, val_best:  79.58%, tr:  98.88%, tr_best:  98.88%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5076%\n",
      "layer   2  Sparsity: 68.7874%\n",
      "layer   3  Sparsity: 71.5837%\n",
      "total_backward_count 166430 real_backward_count 26436  15.884%\n",
      "lif layer 2 self.abs_max_v: 9799.5\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.787619/  1.893624, val:  72.92%, val_best:  79.58%, tr:  98.47%, tr_best:  98.88%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5609%\n",
      "layer   2  Sparsity: 68.6299%\n",
      "layer   3  Sparsity: 71.6801%\n",
      "total_backward_count 171325 real_backward_count 26962  15.737%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.784849/  1.895953, val:  83.33%, val_best:  83.33%, tr:  99.08%, tr_best:  99.08%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5024%\n",
      "layer   2  Sparsity: 69.1895%\n",
      "layer   3  Sparsity: 71.9505%\n",
      "total_backward_count 176220 real_backward_count 27419  15.560%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.780622/  1.892164, val:  77.50%, val_best:  83.33%, tr:  98.37%, tr_best:  99.08%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5714%\n",
      "layer   2  Sparsity: 69.7200%\n",
      "layer   3  Sparsity: 71.2756%\n",
      "total_backward_count 181115 real_backward_count 27862  15.384%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.774596/  1.911570, val:  72.08%, val_best:  83.33%, tr:  98.47%, tr_best:  99.08%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5434%\n",
      "layer   2  Sparsity: 69.0295%\n",
      "layer   3  Sparsity: 70.8331%\n",
      "total_backward_count 186010 real_backward_count 28315  15.222%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.790694/  1.910128, val:  82.08%, val_best:  83.33%, tr:  98.47%, tr_best:  99.08%, epoch time: 39.87 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5184%\n",
      "layer   2  Sparsity: 68.8981%\n",
      "layer   3  Sparsity: 70.8002%\n",
      "total_backward_count 190905 real_backward_count 28804  15.088%\n",
      "fc layer 2 self.abs_max_out: 5701.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.783661/  1.891379, val:  74.17%, val_best:  83.33%, tr:  98.98%, tr_best:  99.08%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5274%\n",
      "layer   2  Sparsity: 68.6785%\n",
      "layer   3  Sparsity: 71.6030%\n",
      "total_backward_count 195800 real_backward_count 29223  14.925%\n",
      "lif layer 2 self.abs_max_v: 10003.5\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.775115/  1.893629, val:  82.50%, val_best:  83.33%, tr:  98.98%, tr_best:  99.08%, epoch time: 39.78 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5365%\n",
      "layer   2  Sparsity: 68.7116%\n",
      "layer   3  Sparsity: 71.0293%\n",
      "total_backward_count 200695 real_backward_count 29666  14.782%\n",
      "fc layer 2 self.abs_max_out: 5799.0\n",
      "lif layer 2 self.abs_max_v: 10343.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.784990/  1.931188, val:  71.67%, val_best:  83.33%, tr:  98.57%, tr_best:  99.08%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5306%\n",
      "layer   2  Sparsity: 69.0605%\n",
      "layer   3  Sparsity: 71.4979%\n",
      "total_backward_count 205590 real_backward_count 30069  14.626%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.778174/  1.876756, val:  81.67%, val_best:  83.33%, tr:  99.08%, tr_best:  99.08%, epoch time: 39.49 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5201%\n",
      "layer   2  Sparsity: 68.7101%\n",
      "layer   3  Sparsity: 71.7495%\n",
      "total_backward_count 210485 real_backward_count 30499  14.490%\n",
      "lif layer 2 self.abs_max_v: 10466.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.773241/  1.885322, val:  80.42%, val_best:  83.33%, tr:  99.18%, tr_best:  99.18%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 90.5028%\n",
      "layer   2  Sparsity: 68.7963%\n",
      "layer   3  Sparsity: 72.4224%\n",
      "total_backward_count 215380 real_backward_count 30926  14.359%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.773466/  1.879175, val:  81.67%, val_best:  83.33%, tr:  99.08%, tr_best:  99.18%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5656%\n",
      "layer   2  Sparsity: 68.7057%\n",
      "layer   3  Sparsity: 72.2814%\n",
      "total_backward_count 220275 real_backward_count 31372  14.242%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.780570/  1.876896, val:  81.25%, val_best:  83.33%, tr:  98.98%, tr_best:  99.18%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4962%\n",
      "layer   2  Sparsity: 68.9206%\n",
      "layer   3  Sparsity: 73.0480%\n",
      "total_backward_count 225170 real_backward_count 31781  14.114%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.776456/  1.911529, val:  73.33%, val_best:  83.33%, tr:  98.98%, tr_best:  99.18%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5071%\n",
      "layer   2  Sparsity: 68.7327%\n",
      "layer   3  Sparsity: 72.2655%\n",
      "total_backward_count 230065 real_backward_count 32194  13.993%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.782441/  1.927064, val:  67.50%, val_best:  83.33%, tr:  98.67%, tr_best:  99.18%, epoch time: 39.89 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5161%\n",
      "layer   2  Sparsity: 68.5515%\n",
      "layer   3  Sparsity: 72.1993%\n",
      "total_backward_count 234960 real_backward_count 32617  13.882%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.792330/  1.902887, val:  82.50%, val_best:  83.33%, tr:  99.08%, tr_best:  99.18%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5226%\n",
      "layer   2  Sparsity: 68.4225%\n",
      "layer   3  Sparsity: 72.7887%\n",
      "total_backward_count 239855 real_backward_count 32986  13.752%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.796760/  1.917904, val:  76.25%, val_best:  83.33%, tr:  99.08%, tr_best:  99.18%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5036%\n",
      "layer   2  Sparsity: 68.4897%\n",
      "layer   3  Sparsity: 72.8524%\n",
      "total_backward_count 244750 real_backward_count 33391  13.643%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.799476/  1.904335, val:  80.83%, val_best:  83.33%, tr:  98.37%, tr_best:  99.18%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5583%\n",
      "layer   2  Sparsity: 68.3745%\n",
      "layer   3  Sparsity: 73.2476%\n",
      "total_backward_count 249645 real_backward_count 33804  13.541%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.792637/  1.900131, val:  79.58%, val_best:  83.33%, tr:  99.28%, tr_best:  99.28%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5304%\n",
      "layer   2  Sparsity: 68.6115%\n",
      "layer   3  Sparsity: 72.8320%\n",
      "total_backward_count 254540 real_backward_count 34189  13.432%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.786242/  1.880871, val:  82.50%, val_best:  83.33%, tr:  98.77%, tr_best:  99.28%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5032%\n",
      "layer   2  Sparsity: 68.5020%\n",
      "layer   3  Sparsity: 72.9108%\n",
      "total_backward_count 259435 real_backward_count 34591  13.333%\n",
      "fc layer 2 self.abs_max_out: 5923.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.776877/  1.886711, val:  81.25%, val_best:  83.33%, tr:  99.39%, tr_best:  99.39%, epoch time: 39.42 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5095%\n",
      "layer   2  Sparsity: 68.3844%\n",
      "layer   3  Sparsity: 72.8597%\n",
      "total_backward_count 264330 real_backward_count 34956  13.224%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.762823/  1.872564, val:  81.67%, val_best:  83.33%, tr:  98.77%, tr_best:  99.39%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5381%\n",
      "layer   2  Sparsity: 68.4411%\n",
      "layer   3  Sparsity: 72.2260%\n",
      "total_backward_count 269225 real_backward_count 35328  13.122%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.760012/  1.869284, val:  84.17%, val_best:  84.17%, tr:  98.57%, tr_best:  99.39%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4835%\n",
      "layer   2  Sparsity: 68.8008%\n",
      "layer   3  Sparsity: 71.9217%\n",
      "total_backward_count 274120 real_backward_count 35736  13.037%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.761726/  1.871400, val:  74.58%, val_best:  84.17%, tr:  98.77%, tr_best:  99.39%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 90.5215%\n",
      "layer   2  Sparsity: 68.4848%\n",
      "layer   3  Sparsity: 71.9495%\n",
      "total_backward_count 279015 real_backward_count 36124  12.947%\n",
      "fc layer 3 self.abs_max_out: 1472.0\n",
      "fc layer 3 self.abs_max_out: 1497.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.746612/  1.873340, val:  80.00%, val_best:  84.17%, tr:  99.28%, tr_best:  99.39%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4707%\n",
      "layer   2  Sparsity: 68.6719%\n",
      "layer   3  Sparsity: 72.6668%\n",
      "total_backward_count 283910 real_backward_count 36483  12.850%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.759229/  1.869781, val:  81.25%, val_best:  84.17%, tr:  99.49%, tr_best:  99.49%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5304%\n",
      "layer   2  Sparsity: 68.6282%\n",
      "layer   3  Sparsity: 73.3957%\n",
      "total_backward_count 288805 real_backward_count 36838  12.755%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.741941/  1.862691, val:  77.50%, val_best:  84.17%, tr:  99.39%, tr_best:  99.49%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4993%\n",
      "layer   2  Sparsity: 68.5858%\n",
      "layer   3  Sparsity: 72.2053%\n",
      "total_backward_count 293700 real_backward_count 37156  12.651%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.748856/  1.871138, val:  78.75%, val_best:  84.17%, tr:  98.57%, tr_best:  99.49%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5051%\n",
      "layer   2  Sparsity: 68.4317%\n",
      "layer   3  Sparsity: 72.3789%\n",
      "total_backward_count 298595 real_backward_count 37501  12.559%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.733944/  1.834718, val:  78.33%, val_best:  84.17%, tr:  99.39%, tr_best:  99.49%, epoch time: 39.75 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5242%\n",
      "layer   2  Sparsity: 68.4746%\n",
      "layer   3  Sparsity: 72.4326%\n",
      "total_backward_count 303490 real_backward_count 37851  12.472%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.736800/  1.853146, val:  82.50%, val_best:  84.17%, tr:  99.08%, tr_best:  99.49%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5057%\n",
      "layer   2  Sparsity: 68.3496%\n",
      "layer   3  Sparsity: 72.5477%\n",
      "total_backward_count 308385 real_backward_count 38186  12.383%\n",
      "fc layer 2 self.abs_max_out: 5924.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.750712/  1.869132, val:  78.75%, val_best:  84.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4471%\n",
      "layer   2  Sparsity: 68.7189%\n",
      "layer   3  Sparsity: 73.3111%\n",
      "total_backward_count 313280 real_backward_count 38535  12.300%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.754047/  1.881703, val:  77.08%, val_best:  84.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5063%\n",
      "layer   2  Sparsity: 68.4532%\n",
      "layer   3  Sparsity: 74.6411%\n",
      "total_backward_count 318175 real_backward_count 38884  12.221%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.763646/  1.873264, val:  80.00%, val_best:  84.17%, tr:  99.39%, tr_best:  99.59%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5238%\n",
      "layer   2  Sparsity: 68.0304%\n",
      "layer   3  Sparsity: 74.8176%\n",
      "total_backward_count 323070 real_backward_count 39210  12.137%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.752616/  1.863091, val:  80.00%, val_best:  84.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 39.79 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5825%\n",
      "layer   2  Sparsity: 68.3606%\n",
      "layer   3  Sparsity: 74.4193%\n",
      "total_backward_count 327965 real_backward_count 39535  12.055%\n",
      "fc layer 2 self.abs_max_out: 6049.0\n",
      "lif layer 2 self.abs_max_v: 10505.5\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.755361/  1.871145, val:  81.67%, val_best:  84.17%, tr:  99.39%, tr_best:  99.59%, epoch time: 39.83 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5346%\n",
      "layer   2  Sparsity: 68.2287%\n",
      "layer   3  Sparsity: 73.6033%\n",
      "total_backward_count 332860 real_backward_count 39911  11.990%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.757398/  1.857571, val:  82.08%, val_best:  84.17%, tr:  99.39%, tr_best:  99.59%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5201%\n",
      "layer   2  Sparsity: 68.6195%\n",
      "layer   3  Sparsity: 73.5888%\n",
      "total_backward_count 337755 real_backward_count 40256  11.919%\n",
      "lif layer 2 self.abs_max_v: 10742.5\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.747054/  1.867291, val:  85.83%, val_best:  85.83%, tr:  99.18%, tr_best:  99.59%, epoch time: 39.91 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.4866%\n",
      "layer   2  Sparsity: 68.5860%\n",
      "layer   3  Sparsity: 73.8296%\n",
      "total_backward_count 342650 real_backward_count 40576  11.842%\n",
      "lif layer 1 self.abs_max_v: 19565.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.753503/  1.868711, val:  77.92%, val_best:  85.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5207%\n",
      "layer   2  Sparsity: 68.2326%\n",
      "layer   3  Sparsity: 74.1537%\n",
      "total_backward_count 347545 real_backward_count 40931  11.777%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.740297/  1.850069, val:  81.67%, val_best:  85.83%, tr:  99.39%, tr_best:  99.59%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5237%\n",
      "layer   2  Sparsity: 67.9782%\n",
      "layer   3  Sparsity: 73.8417%\n",
      "total_backward_count 352440 real_backward_count 41227  11.698%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.732381/  1.848471, val:  81.67%, val_best:  85.83%, tr:  99.49%, tr_best:  99.59%, epoch time: 39.87 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5350%\n",
      "layer   2  Sparsity: 67.4501%\n",
      "layer   3  Sparsity: 73.1543%\n",
      "total_backward_count 357335 real_backward_count 41529  11.622%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.728995/  1.852429, val:  77.50%, val_best:  85.83%, tr:  99.28%, tr_best:  99.59%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4854%\n",
      "layer   2  Sparsity: 67.8303%\n",
      "layer   3  Sparsity: 73.7334%\n",
      "total_backward_count 362230 real_backward_count 41837  11.550%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.752907/  1.873847, val:  78.75%, val_best:  85.83%, tr:  99.39%, tr_best:  99.59%, epoch time: 39.81 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5438%\n",
      "layer   2  Sparsity: 67.4486%\n",
      "layer   3  Sparsity: 74.2556%\n",
      "total_backward_count 367125 real_backward_count 42154  11.482%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.740564/  1.863791, val:  82.08%, val_best:  85.83%, tr:  99.39%, tr_best:  99.59%, epoch time: 39.85 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4827%\n",
      "layer   2  Sparsity: 67.9683%\n",
      "layer   3  Sparsity: 73.5943%\n",
      "total_backward_count 372020 real_backward_count 42469  11.416%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.745347/  1.864969, val:  80.83%, val_best:  85.83%, tr:  99.28%, tr_best:  99.59%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5497%\n",
      "layer   2  Sparsity: 68.1655%\n",
      "layer   3  Sparsity: 73.8020%\n",
      "total_backward_count 376915 real_backward_count 42789  11.352%\n",
      "fc layer 2 self.abs_max_out: 6169.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.748669/  1.874172, val:  77.08%, val_best:  85.83%, tr:  98.98%, tr_best:  99.59%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5506%\n",
      "layer   2  Sparsity: 67.6905%\n",
      "layer   3  Sparsity: 72.9305%\n",
      "total_backward_count 381810 real_backward_count 43093  11.287%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.762324/  1.869504, val:  83.75%, val_best:  85.83%, tr:  99.18%, tr_best:  99.59%, epoch time: 39.55 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5157%\n",
      "layer   2  Sparsity: 67.8801%\n",
      "layer   3  Sparsity: 73.1466%\n",
      "total_backward_count 386705 real_backward_count 43385  11.219%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.746495/  1.846997, val:  80.00%, val_best:  85.83%, tr:  99.69%, tr_best:  99.69%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5362%\n",
      "layer   2  Sparsity: 68.3116%\n",
      "layer   3  Sparsity: 73.8871%\n",
      "total_backward_count 391600 real_backward_count 43650  11.147%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.741343/  1.856134, val:  78.33%, val_best:  85.83%, tr:  99.69%, tr_best:  99.69%, epoch time: 39.60 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5268%\n",
      "layer   2  Sparsity: 68.1696%\n",
      "layer   3  Sparsity: 73.6365%\n",
      "total_backward_count 396495 real_backward_count 43953  11.085%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.741484/  1.867552, val:  80.42%, val_best:  85.83%, tr:  99.49%, tr_best:  99.69%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5343%\n",
      "layer   2  Sparsity: 68.2675%\n",
      "layer   3  Sparsity: 73.3819%\n",
      "total_backward_count 401390 real_backward_count 44254  11.025%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.747164/  1.862559, val:  80.83%, val_best:  85.83%, tr:  99.28%, tr_best:  99.69%, epoch time: 39.53 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5145%\n",
      "layer   2  Sparsity: 68.2025%\n",
      "layer   3  Sparsity: 73.3095%\n",
      "total_backward_count 406285 real_backward_count 44562  10.968%\n",
      "lif layer 1 self.abs_max_v: 19752.5\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.745191/  1.862513, val:  73.75%, val_best:  85.83%, tr:  99.39%, tr_best:  99.69%, epoch time: 39.81 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5098%\n",
      "layer   2  Sparsity: 68.1943%\n",
      "layer   3  Sparsity: 73.9687%\n",
      "total_backward_count 411180 real_backward_count 44855  10.909%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.729043/  1.846777, val:  80.00%, val_best:  85.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 39.91 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.4885%\n",
      "layer   2  Sparsity: 68.0088%\n",
      "layer   3  Sparsity: 73.3587%\n",
      "total_backward_count 416075 real_backward_count 45157  10.853%\n",
      "lif layer 1 self.abs_max_v: 20007.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.736655/  1.861760, val:  82.08%, val_best:  85.83%, tr:  99.08%, tr_best:  99.80%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5426%\n",
      "layer   2  Sparsity: 67.8732%\n",
      "layer   3  Sparsity: 74.5954%\n",
      "total_backward_count 420970 real_backward_count 45475  10.802%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.745991/  1.864968, val:  77.92%, val_best:  85.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5537%\n",
      "layer   2  Sparsity: 67.5043%\n",
      "layer   3  Sparsity: 74.0093%\n",
      "total_backward_count 425865 real_backward_count 45762  10.746%\n",
      "lif layer 2 self.abs_max_v: 10918.0\n",
      "lif layer 2 self.abs_max_v: 10937.0\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.746124/  1.882113, val:  77.92%, val_best:  85.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 40.24 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5647%\n",
      "layer   2  Sparsity: 67.5610%\n",
      "layer   3  Sparsity: 73.4798%\n",
      "total_backward_count 430760 real_backward_count 46092  10.700%\n",
      "fc layer 2 self.abs_max_out: 6266.0\n",
      "lif layer 2 self.abs_max_v: 11059.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.760115/  1.872298, val:  83.33%, val_best:  85.83%, tr:  98.88%, tr_best:  99.80%, epoch time: 39.39 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5034%\n",
      "layer   2  Sparsity: 67.3994%\n",
      "layer   3  Sparsity: 73.8995%\n",
      "total_backward_count 435655 real_backward_count 46406  10.652%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.745527/  1.850771, val:  82.50%, val_best:  85.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5002%\n",
      "layer   2  Sparsity: 67.5587%\n",
      "layer   3  Sparsity: 74.3797%\n",
      "total_backward_count 440550 real_backward_count 46709  10.602%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.724485/  1.860184, val:  79.58%, val_best:  85.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 39.51 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5396%\n",
      "layer   2  Sparsity: 67.9433%\n",
      "layer   3  Sparsity: 74.5370%\n",
      "total_backward_count 445445 real_backward_count 46997  10.551%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.722253/  1.851721, val:  81.67%, val_best:  85.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5255%\n",
      "layer   2  Sparsity: 68.0737%\n",
      "layer   3  Sparsity: 74.4472%\n",
      "total_backward_count 450340 real_backward_count 47277  10.498%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.728747/  1.838984, val:  84.17%, val_best:  85.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4769%\n",
      "layer   2  Sparsity: 67.8605%\n",
      "layer   3  Sparsity: 74.7688%\n",
      "total_backward_count 455235 real_backward_count 47544  10.444%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.725521/  1.836465, val:  87.08%, val_best:  87.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 39.78 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5224%\n",
      "layer   2  Sparsity: 67.9543%\n",
      "layer   3  Sparsity: 74.7492%\n",
      "total_backward_count 460130 real_backward_count 47808  10.390%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.719494/  1.843062, val:  85.00%, val_best:  87.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 39.19 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 90.4846%\n",
      "layer   2  Sparsity: 68.0308%\n",
      "layer   3  Sparsity: 74.4765%\n",
      "total_backward_count 465025 real_backward_count 48048  10.332%\n",
      "fc layer 2 self.abs_max_out: 6302.0\n",
      "lif layer 1 self.abs_max_v: 20125.5\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.735856/  1.851589, val:  85.00%, val_best:  87.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5345%\n",
      "layer   2  Sparsity: 67.6900%\n",
      "layer   3  Sparsity: 74.2861%\n",
      "total_backward_count 469920 real_backward_count 48341  10.287%\n",
      "lif layer 1 self.abs_max_v: 21280.0\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.725882/  1.843015, val:  81.25%, val_best:  87.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 39.63 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5274%\n",
      "layer   2  Sparsity: 67.5901%\n",
      "layer   3  Sparsity: 74.0615%\n",
      "total_backward_count 474815 real_backward_count 48629  10.242%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.725413/  1.824258, val:  83.33%, val_best:  87.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5526%\n",
      "layer   2  Sparsity: 67.5192%\n",
      "layer   3  Sparsity: 73.0177%\n",
      "total_backward_count 479710 real_backward_count 48927  10.199%\n",
      "fc layer 1 self.abs_max_out: 12360.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.723055/  1.847969, val:  79.58%, val_best:  87.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5274%\n",
      "layer   2  Sparsity: 67.4127%\n",
      "layer   3  Sparsity: 73.2481%\n",
      "total_backward_count 484605 real_backward_count 49215  10.156%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.735629/  1.869702, val:  82.08%, val_best:  87.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5404%\n",
      "layer   2  Sparsity: 67.3421%\n",
      "layer   3  Sparsity: 73.6566%\n",
      "total_backward_count 489500 real_backward_count 49512  10.115%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.734197/  1.842376, val:  84.17%, val_best:  87.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 39.72 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5000%\n",
      "layer   2  Sparsity: 67.4125%\n",
      "layer   3  Sparsity: 74.4419%\n",
      "total_backward_count 494395 real_backward_count 49762  10.065%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.722420/  1.826262, val:  83.75%, val_best:  87.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 39.89 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5164%\n",
      "layer   2  Sparsity: 67.4728%\n",
      "layer   3  Sparsity: 74.8030%\n",
      "total_backward_count 499290 real_backward_count 50021  10.018%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.720269/  1.832932, val:  86.25%, val_best:  87.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 39.85 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5037%\n",
      "layer   2  Sparsity: 67.7549%\n",
      "layer   3  Sparsity: 75.3539%\n",
      "total_backward_count 504185 real_backward_count 50270   9.971%\n",
      "lif layer 1 self.abs_max_v: 21384.5\n",
      "fc layer 1 self.abs_max_out: 12641.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.730799/  1.870463, val:  75.83%, val_best:  87.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 39.61 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5066%\n",
      "layer   2  Sparsity: 67.3152%\n",
      "layer   3  Sparsity: 74.4340%\n",
      "total_backward_count 509080 real_backward_count 50536   9.927%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.747842/  1.868118, val:  81.67%, val_best:  87.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 39.54 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5144%\n",
      "layer   2  Sparsity: 67.7718%\n",
      "layer   3  Sparsity: 74.9622%\n",
      "total_backward_count 513975 real_backward_count 50769   9.878%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.736421/  1.868883, val:  77.50%, val_best:  87.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 39.59 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4929%\n",
      "layer   2  Sparsity: 68.1073%\n",
      "layer   3  Sparsity: 74.6752%\n",
      "total_backward_count 518870 real_backward_count 50993   9.828%\n",
      "fc layer 2 self.abs_max_out: 6363.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.727186/  1.832090, val:  80.83%, val_best:  87.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 39.84 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5110%\n",
      "layer   2  Sparsity: 67.4955%\n",
      "layer   3  Sparsity: 74.1894%\n",
      "total_backward_count 523765 real_backward_count 51249   9.785%\n",
      "fc layer 3 self.abs_max_out: 1515.0\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.719974/  1.831570, val:  77.92%, val_best:  87.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5546%\n",
      "layer   2  Sparsity: 67.3983%\n",
      "layer   3  Sparsity: 73.9113%\n",
      "total_backward_count 528660 real_backward_count 51495   9.741%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.721742/  1.826747, val:  82.50%, val_best:  87.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 39.70 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4955%\n",
      "layer   2  Sparsity: 67.2850%\n",
      "layer   3  Sparsity: 74.2402%\n",
      "total_backward_count 533555 real_backward_count 51731   9.696%\n",
      "lif layer 1 self.abs_max_v: 21648.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.710618/  1.820068, val:  82.50%, val_best:  87.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 39.75 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5267%\n",
      "layer   2  Sparsity: 67.7491%\n",
      "layer   3  Sparsity: 74.2509%\n",
      "total_backward_count 538450 real_backward_count 51991   9.656%\n",
      "fc layer 3 self.abs_max_out: 1547.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.693309/  1.816737, val:  83.33%, val_best:  87.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5240%\n",
      "layer   2  Sparsity: 68.0716%\n",
      "layer   3  Sparsity: 73.5833%\n",
      "total_backward_count 543345 real_backward_count 52239   9.614%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.695087/  1.809027, val:  82.50%, val_best:  87.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 39.59 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5218%\n",
      "layer   2  Sparsity: 67.6992%\n",
      "layer   3  Sparsity: 73.4386%\n",
      "total_backward_count 548240 real_backward_count 52493   9.575%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.689482/  1.796289, val:  82.92%, val_best:  87.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 39.78 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5209%\n",
      "layer   2  Sparsity: 67.5969%\n",
      "layer   3  Sparsity: 73.6623%\n",
      "total_backward_count 553135 real_backward_count 52714   9.530%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.692747/  1.815956, val:  82.50%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 39.73 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5369%\n",
      "layer   2  Sparsity: 67.5945%\n",
      "layer   3  Sparsity: 73.9399%\n",
      "total_backward_count 558030 real_backward_count 52930   9.485%\n",
      "fc layer 1 self.abs_max_out: 12745.0\n",
      "fc layer 2 self.abs_max_out: 6497.0\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.690808/  1.814703, val:  82.08%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.10 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5116%\n",
      "layer   2  Sparsity: 67.3099%\n",
      "layer   3  Sparsity: 73.1895%\n",
      "total_backward_count 562925 real_backward_count 53163   9.444%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.692159/  1.828169, val:  80.83%, val_best:  87.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 39.27 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 90.5155%\n",
      "layer   2  Sparsity: 67.1482%\n",
      "layer   3  Sparsity: 74.6354%\n",
      "total_backward_count 567820 real_backward_count 53378   9.401%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.697094/  1.827324, val:  79.17%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 39.97 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5571%\n",
      "layer   2  Sparsity: 67.4222%\n",
      "layer   3  Sparsity: 75.0645%\n",
      "total_backward_count 572715 real_backward_count 53590   9.357%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.703088/  1.829579, val:  79.58%, val_best:  87.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 39.37 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5375%\n",
      "layer   2  Sparsity: 67.6563%\n",
      "layer   3  Sparsity: 75.3211%\n",
      "total_backward_count 577610 real_backward_count 53819   9.318%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.699641/  1.819498, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.90 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5089%\n",
      "layer   2  Sparsity: 67.6643%\n",
      "layer   3  Sparsity: 74.5830%\n",
      "total_backward_count 582505 real_backward_count 54003   9.271%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.700235/  1.836954, val:  80.83%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.59 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5376%\n",
      "layer   2  Sparsity: 67.4731%\n",
      "layer   3  Sparsity: 74.4566%\n",
      "total_backward_count 587400 real_backward_count 54233   9.233%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.705788/  1.822521, val:  79.17%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5089%\n",
      "layer   2  Sparsity: 67.0602%\n",
      "layer   3  Sparsity: 74.5199%\n",
      "total_backward_count 592295 real_backward_count 54471   9.197%\n",
      "lif layer 1 self.abs_max_v: 21712.5\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.700987/  1.810456, val:  85.83%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4972%\n",
      "layer   2  Sparsity: 67.6685%\n",
      "layer   3  Sparsity: 75.1259%\n",
      "total_backward_count 597190 real_backward_count 54710   9.161%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.696867/  1.822011, val:  85.00%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.90 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5245%\n",
      "layer   2  Sparsity: 67.4692%\n",
      "layer   3  Sparsity: 74.2048%\n",
      "total_backward_count 602085 real_backward_count 54947   9.126%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.685934/  1.812165, val:  82.50%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5282%\n",
      "layer   2  Sparsity: 67.2633%\n",
      "layer   3  Sparsity: 74.4733%\n",
      "total_backward_count 606980 real_backward_count 55167   9.089%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.691579/  1.808823, val:  80.42%, val_best:  87.08%, tr:  99.28%, tr_best: 100.00%, epoch time: 39.91 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.4654%\n",
      "layer   2  Sparsity: 67.1262%\n",
      "layer   3  Sparsity: 75.0801%\n",
      "total_backward_count 611875 real_backward_count 55401   9.054%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.696065/  1.815663, val:  80.83%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.4845%\n",
      "layer   2  Sparsity: 67.4182%\n",
      "layer   3  Sparsity: 74.2030%\n",
      "total_backward_count 616770 real_backward_count 55635   9.020%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.691861/  1.837318, val:  81.25%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 39.89 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5059%\n",
      "layer   2  Sparsity: 67.6427%\n",
      "layer   3  Sparsity: 74.2644%\n",
      "total_backward_count 621665 real_backward_count 55861   8.986%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.695629/  1.814513, val:  85.83%, val_best:  87.08%, tr:  99.18%, tr_best: 100.00%, epoch time: 39.68 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5181%\n",
      "layer   2  Sparsity: 67.7589%\n",
      "layer   3  Sparsity: 74.5683%\n",
      "total_backward_count 626560 real_backward_count 56105   8.954%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.690153/  1.825773, val:  81.67%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 39.90 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5222%\n",
      "layer   2  Sparsity: 67.6148%\n",
      "layer   3  Sparsity: 75.2537%\n",
      "total_backward_count 631455 real_backward_count 56345   8.923%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.687347/  1.812438, val:  80.00%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5287%\n",
      "layer   2  Sparsity: 67.4323%\n",
      "layer   3  Sparsity: 74.8944%\n",
      "total_backward_count 636350 real_backward_count 56559   8.888%\n",
      "fc layer 3 self.abs_max_out: 1621.0\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.687597/  1.819507, val:  80.42%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.84 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5265%\n",
      "layer   2  Sparsity: 67.7833%\n",
      "layer   3  Sparsity: 75.1065%\n",
      "total_backward_count 641245 real_backward_count 56801   8.858%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.695922/  1.824670, val:  82.92%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5462%\n",
      "layer   2  Sparsity: 67.6468%\n",
      "layer   3  Sparsity: 75.2618%\n",
      "total_backward_count 646140 real_backward_count 57029   8.826%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.715761/  1.847336, val:  79.17%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.64 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5095%\n",
      "layer   2  Sparsity: 67.7710%\n",
      "layer   3  Sparsity: 76.1867%\n",
      "total_backward_count 651035 real_backward_count 57257   8.795%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.714705/  1.843422, val:  79.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.16 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5466%\n",
      "layer   2  Sparsity: 67.6646%\n",
      "layer   3  Sparsity: 76.5190%\n",
      "total_backward_count 655930 real_backward_count 57464   8.761%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.706908/  1.821888, val:  82.92%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 39.34 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5166%\n",
      "layer   2  Sparsity: 67.5744%\n",
      "layer   3  Sparsity: 76.6212%\n",
      "total_backward_count 660825 real_backward_count 57709   8.733%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.696993/  1.808886, val:  84.17%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5100%\n",
      "layer   2  Sparsity: 67.6307%\n",
      "layer   3  Sparsity: 76.4652%\n",
      "total_backward_count 665720 real_backward_count 57908   8.699%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.687804/  1.809343, val:  85.42%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.38 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5250%\n",
      "layer   2  Sparsity: 67.3185%\n",
      "layer   3  Sparsity: 75.9085%\n",
      "total_backward_count 670615 real_backward_count 58116   8.666%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.693115/  1.821045, val:  83.33%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.4923%\n",
      "layer   2  Sparsity: 67.3073%\n",
      "layer   3  Sparsity: 75.5727%\n",
      "total_backward_count 675510 real_backward_count 58305   8.631%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.690601/  1.832485, val:  82.08%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.39 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5171%\n",
      "layer   2  Sparsity: 67.3304%\n",
      "layer   3  Sparsity: 74.8472%\n",
      "total_backward_count 680405 real_backward_count 58482   8.595%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.705587/  1.833494, val:  81.25%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5369%\n",
      "layer   2  Sparsity: 67.1713%\n",
      "layer   3  Sparsity: 74.8150%\n",
      "total_backward_count 685300 real_backward_count 58694   8.565%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.696576/  1.824671, val:  82.50%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.43 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5294%\n",
      "layer   2  Sparsity: 67.5354%\n",
      "layer   3  Sparsity: 75.4059%\n",
      "total_backward_count 690195 real_backward_count 58904   8.534%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.701110/  1.814573, val:  81.25%, val_best:  87.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 39.97 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5054%\n",
      "layer   2  Sparsity: 67.0801%\n",
      "layer   3  Sparsity: 75.0272%\n",
      "total_backward_count 695090 real_backward_count 59120   8.505%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.683019/  1.806769, val:  80.83%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 39.70 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5210%\n",
      "layer   2  Sparsity: 67.3701%\n",
      "layer   3  Sparsity: 74.6147%\n",
      "total_backward_count 699985 real_backward_count 59334   8.476%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.681690/  1.805049, val:  79.58%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 40.67 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 90.5247%\n",
      "layer   2  Sparsity: 67.6214%\n",
      "layer   3  Sparsity: 75.1140%\n",
      "total_backward_count 704880 real_backward_count 59523   8.444%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.684126/  1.806814, val:  81.67%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.20 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 90.5633%\n",
      "layer   2  Sparsity: 67.6891%\n",
      "layer   3  Sparsity: 75.6509%\n",
      "total_backward_count 709775 real_backward_count 59726   8.415%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.684032/  1.812848, val:  82.08%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5430%\n",
      "layer   2  Sparsity: 67.6039%\n",
      "layer   3  Sparsity: 76.0093%\n",
      "total_backward_count 714670 real_backward_count 59915   8.384%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.688059/  1.821597, val:  83.75%, val_best:  87.08%, tr:  99.28%, tr_best: 100.00%, epoch time: 39.64 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4878%\n",
      "layer   2  Sparsity: 67.1041%\n",
      "layer   3  Sparsity: 76.2521%\n",
      "total_backward_count 719565 real_backward_count 60138   8.358%\n",
      "fc layer 3 self.abs_max_out: 1647.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.689870/  1.799786, val:  82.92%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5032%\n",
      "layer   2  Sparsity: 67.1648%\n",
      "layer   3  Sparsity: 76.4139%\n",
      "total_backward_count 724460 real_backward_count 60363   8.332%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.682643/  1.801803, val:  78.75%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 39.47 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4735%\n",
      "layer   2  Sparsity: 67.2107%\n",
      "layer   3  Sparsity: 75.6783%\n",
      "total_backward_count 729355 real_backward_count 60577   8.306%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.664417/  1.799830, val:  80.42%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.83 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5069%\n",
      "layer   2  Sparsity: 67.4238%\n",
      "layer   3  Sparsity: 75.1774%\n",
      "total_backward_count 734250 real_backward_count 60769   8.276%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.669452/  1.808121, val:  80.83%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5129%\n",
      "layer   2  Sparsity: 67.4002%\n",
      "layer   3  Sparsity: 75.0357%\n",
      "total_backward_count 739145 real_backward_count 60970   8.249%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.668804/  1.780790, val:  85.00%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5177%\n",
      "layer   2  Sparsity: 67.5189%\n",
      "layer   3  Sparsity: 75.6840%\n",
      "total_backward_count 744040 real_backward_count 61163   8.220%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.677137/  1.810998, val:  83.75%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 39.35 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5237%\n",
      "layer   2  Sparsity: 67.0792%\n",
      "layer   3  Sparsity: 76.2224%\n",
      "total_backward_count 748935 real_backward_count 61378   8.195%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.672548/  1.794387, val:  85.00%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.64 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5457%\n",
      "layer   2  Sparsity: 66.7391%\n",
      "layer   3  Sparsity: 75.3820%\n",
      "total_backward_count 753830 real_backward_count 61556   8.166%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.670521/  1.814867, val:  80.83%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.50 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4794%\n",
      "layer   2  Sparsity: 66.7667%\n",
      "layer   3  Sparsity: 75.3158%\n",
      "total_backward_count 758725 real_backward_count 61725   8.135%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.681423/  1.816910, val:  85.00%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5367%\n",
      "layer   2  Sparsity: 67.1708%\n",
      "layer   3  Sparsity: 75.0759%\n",
      "total_backward_count 763620 real_backward_count 61907   8.107%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.684523/  1.818720, val:  82.92%, val_best:  87.08%, tr:  98.98%, tr_best: 100.00%, epoch time: 39.65 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5120%\n",
      "layer   2  Sparsity: 67.0610%\n",
      "layer   3  Sparsity: 75.2893%\n",
      "total_backward_count 768515 real_backward_count 62110   8.082%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.683927/  1.810836, val:  82.92%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.87 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5163%\n",
      "layer   2  Sparsity: 66.8199%\n",
      "layer   3  Sparsity: 75.1560%\n",
      "total_backward_count 773410 real_backward_count 62328   8.059%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.673837/  1.807626, val:  81.25%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5157%\n",
      "layer   2  Sparsity: 66.8824%\n",
      "layer   3  Sparsity: 75.3317%\n",
      "total_backward_count 778305 real_backward_count 62506   8.031%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.663510/  1.795003, val:  76.25%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 39.73 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5046%\n",
      "layer   2  Sparsity: 66.8134%\n",
      "layer   3  Sparsity: 74.4635%\n",
      "total_backward_count 783200 real_backward_count 62699   8.005%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.657645/  1.800514, val:  81.67%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5850%\n",
      "layer   2  Sparsity: 66.8462%\n",
      "layer   3  Sparsity: 74.1667%\n",
      "total_backward_count 788095 real_backward_count 62887   7.980%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.666864/  1.789596, val:  81.67%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4895%\n",
      "layer   2  Sparsity: 66.9779%\n",
      "layer   3  Sparsity: 74.4413%\n",
      "total_backward_count 792990 real_backward_count 63084   7.955%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.654315/  1.786413, val:  82.92%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5159%\n",
      "layer   2  Sparsity: 67.1172%\n",
      "layer   3  Sparsity: 74.5691%\n",
      "total_backward_count 797885 real_backward_count 63263   7.929%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.652742/  1.790291, val:  79.17%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5619%\n",
      "layer   2  Sparsity: 67.0824%\n",
      "layer   3  Sparsity: 74.6905%\n",
      "total_backward_count 802780 real_backward_count 63451   7.904%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.668991/  1.801228, val:  80.42%, val_best:  87.08%, tr:  98.98%, tr_best: 100.00%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4898%\n",
      "layer   2  Sparsity: 66.9527%\n",
      "layer   3  Sparsity: 74.0740%\n",
      "total_backward_count 807675 real_backward_count 63709   7.888%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.658821/  1.793293, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.48 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5074%\n",
      "layer   2  Sparsity: 67.1080%\n",
      "layer   3  Sparsity: 74.9476%\n",
      "total_backward_count 812570 real_backward_count 63905   7.865%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.664827/  1.817313, val:  76.67%, val_best:  87.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5591%\n",
      "layer   2  Sparsity: 66.8838%\n",
      "layer   3  Sparsity: 75.4277%\n",
      "total_backward_count 817465 real_backward_count 64105   7.842%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.670352/  1.811904, val:  72.92%, val_best:  87.08%, tr:  99.28%, tr_best: 100.00%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5448%\n",
      "layer   2  Sparsity: 66.7884%\n",
      "layer   3  Sparsity: 74.8493%\n",
      "total_backward_count 822360 real_backward_count 64319   7.821%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.661877/  1.794740, val:  82.08%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 39.94 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.4846%\n",
      "layer   2  Sparsity: 66.6741%\n",
      "layer   3  Sparsity: 74.7930%\n",
      "total_backward_count 827255 real_backward_count 64533   7.801%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.661930/  1.788701, val:  81.25%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.34 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5383%\n",
      "layer   2  Sparsity: 66.0614%\n",
      "layer   3  Sparsity: 75.0916%\n",
      "total_backward_count 832150 real_backward_count 64713   7.777%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.650602/  1.794825, val:  82.92%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.4452%\n",
      "layer   2  Sparsity: 66.3644%\n",
      "layer   3  Sparsity: 75.3168%\n",
      "total_backward_count 837045 real_backward_count 64867   7.750%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.656690/  1.769523, val:  85.42%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.79 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5235%\n",
      "layer   2  Sparsity: 66.3952%\n",
      "layer   3  Sparsity: 75.3950%\n",
      "total_backward_count 841940 real_backward_count 65067   7.728%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.655376/  1.789898, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 37.66 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 90.5110%\n",
      "layer   2  Sparsity: 66.5299%\n",
      "layer   3  Sparsity: 75.5153%\n",
      "total_backward_count 846835 real_backward_count 65244   7.704%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.660249/  1.804698, val:  83.75%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5247%\n",
      "layer   2  Sparsity: 66.6605%\n",
      "layer   3  Sparsity: 75.2698%\n",
      "total_backward_count 851730 real_backward_count 65386   7.677%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.657442/  1.792359, val:  82.92%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5473%\n",
      "layer   2  Sparsity: 66.7694%\n",
      "layer   3  Sparsity: 74.4333%\n",
      "total_backward_count 856625 real_backward_count 65570   7.654%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.659174/  1.783997, val:  83.33%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.73 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5402%\n",
      "layer   2  Sparsity: 66.4116%\n",
      "layer   3  Sparsity: 74.4678%\n",
      "total_backward_count 861520 real_backward_count 65753   7.632%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.656722/  1.787330, val:  85.00%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.85 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5646%\n",
      "layer   2  Sparsity: 66.5482%\n",
      "layer   3  Sparsity: 74.5666%\n",
      "total_backward_count 866415 real_backward_count 65930   7.610%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.666403/  1.815162, val:  78.33%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 39.17 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 90.5044%\n",
      "layer   2  Sparsity: 66.8512%\n",
      "layer   3  Sparsity: 74.7919%\n",
      "total_backward_count 871310 real_backward_count 66131   7.590%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.671251/  1.810910, val:  78.75%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.4966%\n",
      "layer   2  Sparsity: 66.8478%\n",
      "layer   3  Sparsity: 74.9074%\n",
      "total_backward_count 876205 real_backward_count 66334   7.571%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.665999/  1.793013, val:  83.75%, val_best:  87.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 39.59 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5449%\n",
      "layer   2  Sparsity: 67.0325%\n",
      "layer   3  Sparsity: 75.1080%\n",
      "total_backward_count 881100 real_backward_count 66497   7.547%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.656322/  1.787560, val:  82.08%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5052%\n",
      "layer   2  Sparsity: 66.8792%\n",
      "layer   3  Sparsity: 75.2121%\n",
      "total_backward_count 885995 real_backward_count 66669   7.525%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.652845/  1.802285, val:  76.25%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4675%\n",
      "layer   2  Sparsity: 67.0053%\n",
      "layer   3  Sparsity: 74.7308%\n",
      "total_backward_count 890890 real_backward_count 66836   7.502%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.655328/  1.790691, val:  81.67%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.4897%\n",
      "layer   2  Sparsity: 67.0189%\n",
      "layer   3  Sparsity: 74.6908%\n",
      "total_backward_count 895785 real_backward_count 66999   7.479%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.658718/  1.792688, val:  81.25%, val_best:  87.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 39.60 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5491%\n",
      "layer   2  Sparsity: 66.7852%\n",
      "layer   3  Sparsity: 75.0386%\n",
      "total_backward_count 900680 real_backward_count 67193   7.460%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.648732/  1.796254, val:  80.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.72 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5245%\n",
      "layer   2  Sparsity: 66.8590%\n",
      "layer   3  Sparsity: 74.3679%\n",
      "total_backward_count 905575 real_backward_count 67362   7.439%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.661806/  1.802445, val:  82.08%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.44 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4714%\n",
      "layer   2  Sparsity: 66.7485%\n",
      "layer   3  Sparsity: 75.1462%\n",
      "total_backward_count 910470 real_backward_count 67543   7.418%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.671518/  1.818947, val:  80.42%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.42 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5415%\n",
      "layer   2  Sparsity: 66.6832%\n",
      "layer   3  Sparsity: 74.5586%\n",
      "total_backward_count 915365 real_backward_count 67773   7.404%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.663468/  1.806413, val:  82.92%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.41 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5284%\n",
      "layer   2  Sparsity: 66.8337%\n",
      "layer   3  Sparsity: 74.9959%\n",
      "total_backward_count 920260 real_backward_count 67944   7.383%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.668955/  1.809925, val:  83.75%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.38 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5123%\n",
      "layer   2  Sparsity: 66.7175%\n",
      "layer   3  Sparsity: 75.3742%\n",
      "total_backward_count 925155 real_backward_count 68109   7.362%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.664255/  1.794677, val:  78.75%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.54 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4984%\n",
      "layer   2  Sparsity: 66.7212%\n",
      "layer   3  Sparsity: 75.3829%\n",
      "total_backward_count 930050 real_backward_count 68256   7.339%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.653112/  1.789398, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5089%\n",
      "layer   2  Sparsity: 66.5236%\n",
      "layer   3  Sparsity: 75.6692%\n",
      "total_backward_count 934945 real_backward_count 68399   7.316%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.645160/  1.778277, val:  79.17%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.78 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5044%\n",
      "layer   2  Sparsity: 66.3023%\n",
      "layer   3  Sparsity: 75.5038%\n",
      "total_backward_count 939840 real_backward_count 68564   7.295%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.649582/  1.805349, val:  79.58%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.53 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5106%\n",
      "layer   2  Sparsity: 65.9751%\n",
      "layer   3  Sparsity: 75.2290%\n",
      "total_backward_count 944735 real_backward_count 68767   7.279%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.639460/  1.768270, val:  83.75%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 90.5093%\n",
      "layer   2  Sparsity: 66.2532%\n",
      "layer   3  Sparsity: 75.0987%\n",
      "total_backward_count 949630 real_backward_count 68940   7.260%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.638626/  1.774337, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.78 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5241%\n",
      "layer   2  Sparsity: 66.5249%\n",
      "layer   3  Sparsity: 75.6828%\n",
      "total_backward_count 954525 real_backward_count 69097   7.239%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.638162/  1.792897, val:  77.92%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.54 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.4872%\n",
      "layer   2  Sparsity: 66.8066%\n",
      "layer   3  Sparsity: 75.3805%\n",
      "total_backward_count 959420 real_backward_count 69279   7.221%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.640580/  1.779579, val:  77.92%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.80 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5367%\n",
      "layer   2  Sparsity: 66.8295%\n",
      "layer   3  Sparsity: 75.2283%\n",
      "total_backward_count 964315 real_backward_count 69456   7.203%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.631690/  1.765560, val:  85.00%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 90.5495%\n",
      "layer   2  Sparsity: 66.6796%\n",
      "layer   3  Sparsity: 73.9692%\n",
      "total_backward_count 969210 real_backward_count 69627   7.184%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.622459/  1.745576, val:  82.50%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 38.29 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 90.5019%\n",
      "layer   2  Sparsity: 66.9390%\n",
      "layer   3  Sparsity: 74.4205%\n",
      "total_backward_count 974105 real_backward_count 69790   7.165%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.627326/  1.758739, val:  83.75%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 35.79 seconds, 0.60 minutes\n",
      "layer   1  Sparsity: 90.5187%\n",
      "layer   2  Sparsity: 66.6643%\n",
      "layer   3  Sparsity: 74.8199%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebaeb3604484a1580b17d1e18775fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99796</td></tr><tr><td>tr_epoch_loss</td><td>1.62733</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.8375</td></tr><tr><td>val_loss</td><td>1.75874</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">distinctive-sweep-325</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lrzrir77' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lrzrir77</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251127_163825-lrzrir77/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e2ln1yek with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 12000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251127_185158-e2ln1yek</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e2ln1yek' target=\"_blank\">autumn-sweep-332</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e2ln1yek' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e2ln1yek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251127_185205_808', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 12000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c247f45ff938aa370993ba27bace6d15\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 592\n",
      "fc layer 1 self.abs_max_out: 171.0\n",
      "lif layer 1 self.abs_max_v: 171.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 682.0\n",
      "lif layer 2 self.abs_max_v: 682.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 514.0\n",
      "fc layer 1 self.abs_max_out: 241.0\n",
      "lif layer 1 self.abs_max_v: 268.5\n",
      "lif layer 2 self.abs_max_v: 848.0\n",
      "fc layer 2 self.abs_max_out: 749.0\n",
      "lif layer 2 self.abs_max_v: 1030.5\n",
      "lif layer 1 self.abs_max_v: 280.5\n",
      "fc layer 2 self.abs_max_out: 791.0\n",
      "lif layer 2 self.abs_max_v: 1096.0\n",
      "fc layer 2 self.abs_max_out: 810.0\n",
      "lif layer 2 self.abs_max_v: 1298.5\n",
      "lif layer 1 self.abs_max_v: 281.0\n",
      "lif layer 1 self.abs_max_v: 314.0\n",
      "smallest_now_T updated: 534\n",
      "fc layer 2 self.abs_max_out: 871.0\n",
      "fc layer 3 self.abs_max_out: 573.0\n",
      "lif layer 1 self.abs_max_v: 321.0\n",
      "fc layer 1 self.abs_max_out: 265.0\n",
      "lif layer 1 self.abs_max_v: 424.5\n",
      "fc layer 2 self.abs_max_out: 932.0\n",
      "lif layer 2 self.abs_max_v: 1352.0\n",
      "fc layer 1 self.abs_max_out: 308.0\n",
      "lif layer 1 self.abs_max_v: 471.5\n",
      "fc layer 2 self.abs_max_out: 1002.0\n",
      "lif layer 2 self.abs_max_v: 1678.0\n",
      "fc layer 1 self.abs_max_out: 340.0\n",
      "lif layer 1 self.abs_max_v: 559.5\n",
      "fc layer 3 self.abs_max_out: 644.0\n",
      "fc layer 1 self.abs_max_out: 369.0\n",
      "fc layer 2 self.abs_max_out: 1116.0\n",
      "smallest_now_T updated: 407\n",
      "fc layer 3 self.abs_max_out: 762.0\n",
      "fc layer 1 self.abs_max_out: 374.0\n",
      "fc layer 1 self.abs_max_out: 375.0\n",
      "fc layer 1 self.abs_max_out: 406.0\n",
      "lif layer 1 self.abs_max_v: 560.0\n",
      "fc layer 2 self.abs_max_out: 1212.0\n",
      "fc layer 1 self.abs_max_out: 448.0\n",
      "lif layer 1 self.abs_max_v: 571.0\n",
      "fc layer 1 self.abs_max_out: 469.0\n",
      "lif layer 1 self.abs_max_v: 726.0\n",
      "fc layer 1 self.abs_max_out: 483.0\n",
      "fc layer 1 self.abs_max_out: 545.0\n",
      "fc layer 1 self.abs_max_out: 679.0\n",
      "lif layer 1 self.abs_max_v: 739.5\n",
      "lif layer 1 self.abs_max_v: 745.5\n",
      "lif layer 2 self.abs_max_v: 1817.5\n",
      "fc layer 2 self.abs_max_out: 1299.0\n",
      "lif layer 1 self.abs_max_v: 786.5\n",
      "fc layer 1 self.abs_max_out: 688.0\n",
      "lif layer 1 self.abs_max_v: 969.0\n",
      "lif layer 1 self.abs_max_v: 1019.5\n",
      "fc layer 1 self.abs_max_out: 690.0\n",
      "lif layer 1 self.abs_max_v: 1131.0\n",
      "fc layer 3 self.abs_max_out: 775.0\n",
      "lif layer 1 self.abs_max_v: 1201.5\n",
      "fc layer 1 self.abs_max_out: 696.0\n",
      "smallest_now_T updated: 345\n",
      "lif layer 2 self.abs_max_v: 1848.5\n",
      "fc layer 1 self.abs_max_out: 773.0\n",
      "lif layer 2 self.abs_max_v: 1975.0\n",
      "fc layer 1 self.abs_max_out: 828.0\n",
      "fc layer 2 self.abs_max_out: 1348.0\n",
      "fc layer 1 self.abs_max_out: 838.0\n",
      "lif layer 2 self.abs_max_v: 2031.0\n",
      "fc layer 2 self.abs_max_out: 1417.0\n",
      "fc layer 1 self.abs_max_out: 942.0\n",
      "lif layer 2 self.abs_max_v: 2090.5\n",
      "lif layer 2 self.abs_max_v: 2130.0\n",
      "fc layer 2 self.abs_max_out: 1521.0\n",
      "lif layer 2 self.abs_max_v: 2586.0\n",
      "fc layer 2 self.abs_max_out: 1558.0\n",
      "smallest_now_T updated: 317\n",
      "fc layer 3 self.abs_max_out: 825.0\n",
      "fc layer 3 self.abs_max_out: 851.0\n",
      "fc layer 3 self.abs_max_out: 895.0\n",
      "fc layer 1 self.abs_max_out: 1021.0\n",
      "fc layer 3 self.abs_max_out: 902.0\n",
      "fc layer 1 self.abs_max_out: 1037.0\n",
      "fc layer 2 self.abs_max_out: 1608.0\n",
      "fc layer 2 self.abs_max_out: 1631.0\n",
      "smallest_now_T updated: 286\n",
      "fc layer 2 self.abs_max_out: 1677.0\n",
      "fc layer 1 self.abs_max_out: 1041.0\n",
      "fc layer 3 self.abs_max_out: 995.0\n",
      "fc layer 3 self.abs_max_out: 1042.0\n",
      "lif layer 1 self.abs_max_v: 1350.5\n",
      "lif layer 1 self.abs_max_v: 1397.5\n",
      "lif layer 2 self.abs_max_v: 2778.5\n",
      "fc layer 1 self.abs_max_out: 1099.0\n",
      "fc layer 1 self.abs_max_out: 1157.0\n",
      "fc layer 1 self.abs_max_out: 1463.0\n",
      "lif layer 1 self.abs_max_v: 1463.0\n",
      "fc layer 1 self.abs_max_out: 1539.0\n",
      "lif layer 1 self.abs_max_v: 1539.0\n",
      "fc layer 3 self.abs_max_out: 1161.0\n",
      "fc layer 2 self.abs_max_out: 1964.0\n",
      "smallest_now_T updated: 247\n",
      "fc layer 2 self.abs_max_out: 2007.0\n",
      "smallest_now_T updated: 192\n",
      "fc layer 2 self.abs_max_out: 2032.0\n",
      "lif layer 2 self.abs_max_v: 2870.5\n",
      "fc layer 3 self.abs_max_out: 1205.0\n",
      "fc layer 3 self.abs_max_out: 1324.0\n",
      "fc layer 3 self.abs_max_out: 1328.0\n",
      "fc layer 3 self.abs_max_out: 1377.0\n",
      "lif layer 1 self.abs_max_v: 1598.5\n",
      "lif layer 1 self.abs_max_v: 1677.5\n",
      "fc layer 2 self.abs_max_out: 2126.0\n",
      "lif layer 2 self.abs_max_v: 2982.5\n",
      "lif layer 2 self.abs_max_v: 3116.5\n",
      "lif layer 1 self.abs_max_v: 1681.0\n",
      "lif layer 2 self.abs_max_v: 3173.0\n",
      "lif layer 1 self.abs_max_v: 1810.5\n",
      "fc layer 1 self.abs_max_out: 1659.0\n",
      "fc layer 2 self.abs_max_out: 2163.0\n",
      "lif layer 2 self.abs_max_v: 3177.0\n",
      "lif layer 2 self.abs_max_v: 3351.5\n",
      "lif layer 2 self.abs_max_v: 3451.0\n",
      "fc layer 2 self.abs_max_out: 2166.0\n",
      "lif layer 1 self.abs_max_v: 1959.5\n",
      "lif layer 1 self.abs_max_v: 2101.5\n",
      "lif layer 1 self.abs_max_v: 2435.0\n",
      "fc layer 1 self.abs_max_out: 1708.0\n",
      "fc layer 2 self.abs_max_out: 2289.0\n",
      "fc layer 2 self.abs_max_out: 2399.0\n",
      "fc layer 3 self.abs_max_out: 1378.0\n",
      "fc layer 3 self.abs_max_out: 1382.0\n",
      "lif layer 2 self.abs_max_v: 3605.0\n",
      "fc layer 2 self.abs_max_out: 2452.0\n",
      "fc layer 2 self.abs_max_out: 2513.0\n",
      "fc layer 3 self.abs_max_out: 1412.0\n",
      "fc layer 3 self.abs_max_out: 1562.0\n",
      "lif layer 2 self.abs_max_v: 3616.0\n",
      "lif layer 2 self.abs_max_v: 3653.0\n",
      "lif layer 2 self.abs_max_v: 3836.0\n",
      "fc layer 1 self.abs_max_out: 1828.0\n",
      "lif layer 2 self.abs_max_v: 3837.0\n",
      "fc layer 1 self.abs_max_out: 1948.0\n",
      "fc layer 1 self.abs_max_out: 1965.0\n",
      "lif layer 2 self.abs_max_v: 3880.0\n",
      "lif layer 2 self.abs_max_v: 4066.0\n",
      "lif layer 1 self.abs_max_v: 2751.0\n",
      "smallest_now_T_val updated: 552\n",
      "smallest_now_T_val updated: 456\n",
      "smallest_now_T_val updated: 448\n",
      "smallest_now_T_val updated: 440\n",
      "fc layer 2 self.abs_max_out: 2557.0\n",
      "lif layer 2 self.abs_max_v: 4111.0\n",
      "lif layer 2 self.abs_max_v: 4247.5\n",
      "smallest_now_T_val updated: 368\n",
      "smallest_now_T_val updated: 137\n",
      "fc layer 1 self.abs_max_out: 2068.0\n",
      "fc layer 2 self.abs_max_out: 2617.0\n",
      "lif layer 1 self.abs_max_v: 2789.0\n",
      "lif layer 1 self.abs_max_v: 2809.0\n",
      "lif layer 1 self.abs_max_v: 2946.0\n",
      "lif layer 1 self.abs_max_v: 3299.0\n",
      "lif layer 2 self.abs_max_v: 4388.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.432871/  2.015073, val:  27.92%, val_best:  27.92%, tr:  98.16%, tr_best:  98.16%, epoch time: 68.81 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 97.2922%\n",
      "layer   2  Sparsity: 74.9305%\n",
      "layer   3  Sparsity: 63.1847%\n",
      "total_backward_count 9790 real_backward_count 1839  18.784%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 2741.0\n",
      "fc layer 2 self.abs_max_out: 2842.0\n",
      "fc layer 2 self.abs_max_out: 2997.0\n",
      "fc layer 1 self.abs_max_out: 2094.0\n",
      "lif layer 2 self.abs_max_v: 4412.5\n",
      "fc layer 3 self.abs_max_out: 1571.0\n",
      "fc layer 3 self.abs_max_out: 1576.0\n",
      "lif layer 2 self.abs_max_v: 4517.0\n",
      "fc layer 1 self.abs_max_out: 2360.0\n",
      "lif layer 2 self.abs_max_v: 4608.5\n",
      "lif layer 2 self.abs_max_v: 4640.5\n",
      "lif layer 1 self.abs_max_v: 3320.5\n",
      "fc layer 2 self.abs_max_out: 3074.0\n",
      "lif layer 2 self.abs_max_v: 4705.0\n",
      "lif layer 2 self.abs_max_v: 4746.5\n",
      "fc layer 3 self.abs_max_out: 1579.0\n",
      "fc layer 3 self.abs_max_out: 1600.0\n",
      "lif layer 2 self.abs_max_v: 4863.0\n",
      "lif layer 2 self.abs_max_v: 5065.5\n",
      "lif layer 2 self.abs_max_v: 5113.0\n",
      "lif layer 2 self.abs_max_v: 5133.5\n",
      "fc layer 1 self.abs_max_out: 2424.0\n",
      "lif layer 1 self.abs_max_v: 3569.5\n",
      "fc layer 1 self.abs_max_out: 2508.0\n",
      "lif layer 1 self.abs_max_v: 3656.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.301300/  1.897115, val:  30.42%, val_best:  30.42%, tr:  98.67%, tr_best:  98.67%, epoch time: 68.59 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 97.2864%\n",
      "layer   2  Sparsity: 73.5905%\n",
      "layer   3  Sparsity: 63.4591%\n",
      "total_backward_count 19580 real_backward_count 3552  18.141%\n",
      "fc layer 3 self.abs_max_out: 1611.0\n",
      "fc layer 2 self.abs_max_out: 3083.0\n",
      "lif layer 2 self.abs_max_v: 5154.0\n",
      "fc layer 2 self.abs_max_out: 3103.0\n",
      "lif layer 2 self.abs_max_v: 5168.5\n",
      "fc layer 2 self.abs_max_out: 3105.0\n",
      "lif layer 2 self.abs_max_v: 5235.5\n",
      "fc layer 2 self.abs_max_out: 3239.0\n",
      "fc layer 2 self.abs_max_out: 3364.0\n",
      "fc layer 3 self.abs_max_out: 1704.0\n",
      "fc layer 2 self.abs_max_out: 3459.0\n",
      "lif layer 2 self.abs_max_v: 5631.5\n",
      "fc layer 2 self.abs_max_out: 3532.0\n",
      "fc layer 3 self.abs_max_out: 1874.0\n",
      "fc layer 1 self.abs_max_out: 2526.0\n",
      "fc layer 3 self.abs_max_out: 1919.0\n",
      "fc layer 3 self.abs_max_out: 1989.0\n",
      "fc layer 3 self.abs_max_out: 2046.0\n",
      "fc layer 1 self.abs_max_out: 2733.0\n",
      "lif layer 1 self.abs_max_v: 4092.5\n",
      "lif layer 2 self.abs_max_v: 5685.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.254776/  1.855692, val:  25.00%, val_best:  30.42%, tr:  99.08%, tr_best:  99.08%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 97.2847%\n",
      "layer   2  Sparsity: 72.6851%\n",
      "layer   3  Sparsity: 62.6952%\n",
      "total_backward_count 29370 real_backward_count 5271  17.947%\n",
      "lif layer 2 self.abs_max_v: 5736.0\n",
      "fc layer 3 self.abs_max_out: 2117.0\n",
      "fc layer 1 self.abs_max_out: 2780.0\n",
      "lif layer 2 self.abs_max_v: 5921.0\n",
      "lif layer 2 self.abs_max_v: 6126.0\n",
      "lif layer 2 self.abs_max_v: 6244.0\n",
      "fc layer 2 self.abs_max_out: 3561.0\n",
      "fc layer 3 self.abs_max_out: 2155.0\n",
      "fc layer 1 self.abs_max_out: 2803.0\n",
      "fc layer 1 self.abs_max_out: 3137.0\n",
      "lif layer 1 self.abs_max_v: 4309.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.203942/  1.823336, val:  29.58%, val_best:  30.42%, tr:  99.18%, tr_best:  99.18%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 97.2839%\n",
      "layer   2  Sparsity: 73.3577%\n",
      "layer   3  Sparsity: 61.9126%\n",
      "total_backward_count 39160 real_backward_count 6918  17.666%\n",
      "fc layer 2 self.abs_max_out: 3759.0\n",
      "lif layer 2 self.abs_max_v: 6289.0\n",
      "fc layer 1 self.abs_max_out: 3184.0\n",
      "lif layer 2 self.abs_max_v: 6297.5\n",
      "fc layer 2 self.abs_max_out: 3930.0\n",
      "fc layer 2 self.abs_max_out: 3979.0\n",
      "fc layer 2 self.abs_max_out: 3984.0\n",
      "lif layer 1 self.abs_max_v: 4310.0\n",
      "lif layer 1 self.abs_max_v: 4781.0\n",
      "lif layer 1 self.abs_max_v: 5373.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.173407/  1.833084, val:  27.08%, val_best:  30.42%, tr:  99.39%, tr_best:  99.39%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2805%\n",
      "layer   2  Sparsity: 73.3320%\n",
      "layer   3  Sparsity: 60.7035%\n",
      "total_backward_count 48950 real_backward_count 8494  17.352%\n",
      "fc layer 2 self.abs_max_out: 4239.0\n",
      "lif layer 2 self.abs_max_v: 6298.5\n",
      "lif layer 2 self.abs_max_v: 6372.5\n",
      "lif layer 2 self.abs_max_v: 6411.0\n",
      "lif layer 2 self.abs_max_v: 6473.5\n",
      "lif layer 2 self.abs_max_v: 6634.5\n",
      "lif layer 2 self.abs_max_v: 6638.5\n",
      "lif layer 2 self.abs_max_v: 6770.5\n",
      "fc layer 1 self.abs_max_out: 3335.0\n",
      "lif layer 1 self.abs_max_v: 5865.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.140974/  1.929494, val:  25.83%, val_best:  30.42%, tr:  98.57%, tr_best:  99.39%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.3120%\n",
      "layer   2  Sparsity: 72.9164%\n",
      "layer   3  Sparsity: 60.3728%\n",
      "total_backward_count 58740 real_backward_count 10100  17.194%\n",
      "fc layer 1 self.abs_max_out: 3571.0\n",
      "lif layer 1 self.abs_max_v: 6200.5\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.142343/  1.897605, val:  29.17%, val_best:  30.42%, tr:  99.28%, tr_best:  99.39%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2798%\n",
      "layer   2  Sparsity: 73.3090%\n",
      "layer   3  Sparsity: 59.4881%\n",
      "total_backward_count 68530 real_backward_count 11688  17.055%\n",
      "fc layer 3 self.abs_max_out: 2230.0\n",
      "lif layer 2 self.abs_max_v: 7032.5\n",
      "lif layer 2 self.abs_max_v: 7326.5\n",
      "fc layer 2 self.abs_max_out: 4356.0\n",
      "fc layer 3 self.abs_max_out: 2299.0\n",
      "fc layer 3 self.abs_max_out: 2392.0\n",
      "fc layer 3 self.abs_max_out: 2398.0\n",
      "fc layer 3 self.abs_max_out: 2402.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.132267/  1.767219, val:  34.58%, val_best:  34.58%, tr:  99.08%, tr_best:  99.39%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 97.2779%\n",
      "layer   2  Sparsity: 72.8581%\n",
      "layer   3  Sparsity: 60.2287%\n",
      "total_backward_count 78320 real_backward_count 13232  16.895%\n",
      "fc layer 1 self.abs_max_out: 3957.0\n",
      "lif layer 1 self.abs_max_v: 6533.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.116363/  1.751363, val:  34.58%, val_best:  34.58%, tr:  98.88%, tr_best:  99.39%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2834%\n",
      "layer   2  Sparsity: 72.2073%\n",
      "layer   3  Sparsity: 60.6977%\n",
      "total_backward_count 88110 real_backward_count 14876  16.883%\n",
      "lif layer 1 self.abs_max_v: 6726.0\n",
      "fc layer 1 self.abs_max_out: 3993.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.118248/  1.765377, val:  40.42%, val_best:  40.42%, tr:  99.39%, tr_best:  99.39%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2656%\n",
      "layer   2  Sparsity: 72.5794%\n",
      "layer   3  Sparsity: 61.0003%\n",
      "total_backward_count 97900 real_backward_count 16424  16.776%\n",
      "lif layer 2 self.abs_max_v: 7451.5\n",
      "lif layer 2 self.abs_max_v: 7521.0\n",
      "lif layer 2 self.abs_max_v: 7675.5\n",
      "lif layer 2 self.abs_max_v: 7713.0\n",
      "fc layer 1 self.abs_max_out: 4237.0\n",
      "lif layer 1 self.abs_max_v: 6932.0\n",
      "fc layer 2 self.abs_max_out: 4426.0\n",
      "fc layer 2 self.abs_max_out: 4450.0\n",
      "lif layer 2 self.abs_max_v: 7818.5\n",
      "lif layer 2 self.abs_max_v: 7861.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.094638/  1.723998, val:  39.58%, val_best:  40.42%, tr:  99.28%, tr_best:  99.39%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2800%\n",
      "layer   2  Sparsity: 72.8363%\n",
      "layer   3  Sparsity: 62.2885%\n",
      "total_backward_count 107690 real_backward_count 18027  16.740%\n",
      "lif layer 2 self.abs_max_v: 7901.0\n",
      "lif layer 2 self.abs_max_v: 8025.0\n",
      "fc layer 2 self.abs_max_out: 4458.0\n",
      "lif layer 2 self.abs_max_v: 8215.0\n",
      "fc layer 1 self.abs_max_out: 4647.0\n",
      "lif layer 1 self.abs_max_v: 7386.5\n",
      "lif layer 1 self.abs_max_v: 7641.5\n",
      "fc layer 2 self.abs_max_out: 4488.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.130878/  1.803759, val:  32.08%, val_best:  40.42%, tr:  99.59%, tr_best:  99.59%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 97.2886%\n",
      "layer   2  Sparsity: 72.7822%\n",
      "layer   3  Sparsity: 62.6402%\n",
      "total_backward_count 117480 real_backward_count 19605  16.688%\n",
      "fc layer 2 self.abs_max_out: 4526.0\n",
      "fc layer 2 self.abs_max_out: 4557.0\n",
      "fc layer 2 self.abs_max_out: 4590.0\n",
      "fc layer 2 self.abs_max_out: 4959.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.128897/  1.683958, val:  35.00%, val_best:  40.42%, tr:  98.98%, tr_best:  99.59%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 97.2843%\n",
      "layer   2  Sparsity: 72.6557%\n",
      "layer   3  Sparsity: 63.9116%\n",
      "total_backward_count 127270 real_backward_count 21166  16.631%\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.138536/  1.950894, val:  17.50%, val_best:  40.42%, tr:  99.18%, tr_best:  99.59%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2719%\n",
      "layer   2  Sparsity: 72.5425%\n",
      "layer   3  Sparsity: 65.1623%\n",
      "total_backward_count 137060 real_backward_count 22759  16.605%\n",
      "lif layer 2 self.abs_max_v: 8270.0\n",
      "lif layer 2 self.abs_max_v: 8313.5\n",
      "lif layer 2 self.abs_max_v: 8372.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.148252/  1.658870, val:  41.25%, val_best:  41.25%, tr:  99.18%, tr_best:  99.59%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.3003%\n",
      "layer   2  Sparsity: 72.7413%\n",
      "layer   3  Sparsity: 65.4475%\n",
      "total_backward_count 146850 real_backward_count 24276  16.531%\n",
      "fc layer 2 self.abs_max_out: 4979.0\n",
      "lif layer 2 self.abs_max_v: 8378.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.146999/  1.769424, val:  31.25%, val_best:  41.25%, tr:  99.18%, tr_best:  99.59%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 97.2860%\n",
      "layer   2  Sparsity: 73.1032%\n",
      "layer   3  Sparsity: 66.0877%\n",
      "total_backward_count 156640 real_backward_count 25840  16.496%\n",
      "lif layer 2 self.abs_max_v: 8443.5\n",
      "lif layer 1 self.abs_max_v: 7757.0\n",
      "lif layer 2 self.abs_max_v: 8461.5\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.128443/  1.673481, val:  48.33%, val_best:  48.33%, tr:  99.18%, tr_best:  99.59%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 97.2693%\n",
      "layer   2  Sparsity: 72.8438%\n",
      "layer   3  Sparsity: 66.1443%\n",
      "total_backward_count 166430 real_backward_count 27391  16.458%\n",
      "lif layer 2 self.abs_max_v: 8844.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.161888/  1.653990, val:  45.83%, val_best:  48.33%, tr:  99.28%, tr_best:  99.59%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2764%\n",
      "layer   2  Sparsity: 72.4665%\n",
      "layer   3  Sparsity: 66.5139%\n",
      "total_backward_count 176220 real_backward_count 28959  16.433%\n",
      "fc layer 2 self.abs_max_out: 5007.0\n",
      "lif layer 2 self.abs_max_v: 8983.5\n",
      "lif layer 1 self.abs_max_v: 7842.5\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.164240/  1.694029, val:  40.00%, val_best:  48.33%, tr:  99.28%, tr_best:  99.59%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 97.2994%\n",
      "layer   2  Sparsity: 72.3572%\n",
      "layer   3  Sparsity: 66.8287%\n",
      "total_backward_count 186010 real_backward_count 30523  16.409%\n",
      "fc layer 2 self.abs_max_out: 5161.0\n",
      "fc layer 2 self.abs_max_out: 5398.0\n",
      "lif layer 1 self.abs_max_v: 8133.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.152102/  1.803138, val:  29.17%, val_best:  48.33%, tr:  99.49%, tr_best:  99.59%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2643%\n",
      "layer   2  Sparsity: 72.3726%\n",
      "layer   3  Sparsity: 67.3227%\n",
      "total_backward_count 195800 real_backward_count 32042  16.365%\n",
      "fc layer 1 self.abs_max_out: 4780.0\n",
      "lif layer 1 self.abs_max_v: 8275.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.146913/  1.780139, val:  33.33%, val_best:  48.33%, tr:  98.47%, tr_best:  99.59%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2812%\n",
      "layer   2  Sparsity: 72.3301%\n",
      "layer   3  Sparsity: 67.0186%\n",
      "total_backward_count 205590 real_backward_count 33586  16.336%\n",
      "fc layer 2 self.abs_max_out: 5651.0\n",
      "fc layer 1 self.abs_max_out: 5216.0\n",
      "lif layer 1 self.abs_max_v: 9009.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.152625/  1.705122, val:  36.25%, val_best:  48.33%, tr:  99.39%, tr_best:  99.59%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 97.2766%\n",
      "layer   2  Sparsity: 72.3686%\n",
      "layer   3  Sparsity: 67.4667%\n",
      "total_backward_count 215380 real_backward_count 35196  16.341%\n",
      "fc layer 1 self.abs_max_out: 5389.0\n",
      "lif layer 1 self.abs_max_v: 9289.5\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.144667/  1.575223, val:  54.17%, val_best:  54.17%, tr:  99.18%, tr_best:  99.59%, epoch time: 75.21 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 97.2703%\n",
      "layer   2  Sparsity: 72.6644%\n",
      "layer   3  Sparsity: 67.6692%\n",
      "total_backward_count 225170 real_backward_count 36774  16.332%\n",
      "fc layer 3 self.abs_max_out: 2474.0\n",
      "lif layer 2 self.abs_max_v: 9118.5\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.125924/  1.680011, val:  42.50%, val_best:  54.17%, tr:  99.28%, tr_best:  99.59%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 97.2961%\n",
      "layer   2  Sparsity: 72.3947%\n",
      "layer   3  Sparsity: 67.8311%\n",
      "total_backward_count 234960 real_backward_count 38376  16.333%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.136204/  1.751098, val:  40.83%, val_best:  54.17%, tr:  98.98%, tr_best:  99.59%, epoch time: 83.74 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 97.2967%\n",
      "layer   2  Sparsity: 72.2388%\n",
      "layer   3  Sparsity: 67.2975%\n",
      "total_backward_count 244750 real_backward_count 39959  16.326%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.168164/  1.607107, val:  53.33%, val_best:  54.17%, tr:  98.88%, tr_best:  99.59%, epoch time: 83.91 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 97.2786%\n",
      "layer   2  Sparsity: 72.0659%\n",
      "layer   3  Sparsity: 67.8215%\n",
      "total_backward_count 254540 real_backward_count 41651  16.363%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.146329/  1.684957, val:  43.33%, val_best:  54.17%, tr:  99.80%, tr_best:  99.80%, epoch time: 84.52 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 97.2821%\n",
      "layer   2  Sparsity: 72.2594%\n",
      "layer   3  Sparsity: 67.1620%\n",
      "total_backward_count 264330 real_backward_count 43181  16.336%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.138359/  1.634304, val:  50.42%, val_best:  54.17%, tr:  98.98%, tr_best:  99.80%, epoch time: 84.00 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 97.2835%\n",
      "layer   2  Sparsity: 72.0627%\n",
      "layer   3  Sparsity: 66.5502%\n",
      "total_backward_count 274120 real_backward_count 44775  16.334%\n",
      "lif layer 2 self.abs_max_v: 9163.5\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.130889/  1.676830, val:  46.67%, val_best:  54.17%, tr:  99.08%, tr_best:  99.80%, epoch time: 83.42 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2801%\n",
      "layer   2  Sparsity: 72.0332%\n",
      "layer   3  Sparsity: 66.9178%\n",
      "total_backward_count 283910 real_backward_count 46311  16.312%\n",
      "fc layer 1 self.abs_max_out: 5396.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.154413/  1.843010, val:  18.33%, val_best:  54.17%, tr:  99.39%, tr_best:  99.80%, epoch time: 80.90 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 97.2777%\n",
      "layer   2  Sparsity: 72.3352%\n",
      "layer   3  Sparsity: 67.6759%\n",
      "total_backward_count 293700 real_backward_count 47877  16.301%\n",
      "fc layer 1 self.abs_max_out: 5411.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.139668/  1.573134, val:  48.75%, val_best:  54.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 83.61 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2753%\n",
      "layer   2  Sparsity: 72.4032%\n",
      "layer   3  Sparsity: 68.1766%\n",
      "total_backward_count 303490 real_backward_count 49419  16.284%\n",
      "lif layer 2 self.abs_max_v: 9360.5\n",
      "lif layer 2 self.abs_max_v: 9892.5\n",
      "fc layer 2 self.abs_max_out: 5697.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.169560/  1.732634, val:  30.42%, val_best:  54.17%, tr:  99.39%, tr_best:  99.80%, epoch time: 83.95 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 97.2641%\n",
      "layer   2  Sparsity: 71.9491%\n",
      "layer   3  Sparsity: 68.8050%\n",
      "total_backward_count 313280 real_backward_count 50946  16.262%\n",
      "fc layer 2 self.abs_max_out: 5780.0\n",
      "fc layer 1 self.abs_max_out: 5415.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.164011/  1.702086, val:  35.42%, val_best:  54.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 83.61 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2793%\n",
      "layer   2  Sparsity: 72.0699%\n",
      "layer   3  Sparsity: 68.4686%\n",
      "total_backward_count 323070 real_backward_count 52434  16.230%\n",
      "fc layer 3 self.abs_max_out: 2493.0\n",
      "fc layer 1 self.abs_max_out: 5887.0\n",
      "lif layer 1 self.abs_max_v: 10110.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.166894/  1.698579, val:  43.75%, val_best:  54.17%, tr:  99.39%, tr_best:  99.80%, epoch time: 83.90 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 97.2709%\n",
      "layer   2  Sparsity: 72.0431%\n",
      "layer   3  Sparsity: 68.4343%\n",
      "total_backward_count 332860 real_backward_count 54037  16.234%\n",
      "lif layer 2 self.abs_max_v: 10052.0\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.147126/  1.734743, val:  30.42%, val_best:  54.17%, tr:  99.49%, tr_best:  99.80%, epoch time: 84.30 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 97.2774%\n",
      "layer   2  Sparsity: 71.7370%\n",
      "layer   3  Sparsity: 68.0694%\n",
      "total_backward_count 342650 real_backward_count 55542  16.210%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.142154/  1.586898, val:  48.75%, val_best:  54.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 83.84 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 97.2827%\n",
      "layer   2  Sparsity: 71.9874%\n",
      "layer   3  Sparsity: 68.6559%\n",
      "total_backward_count 352440 real_backward_count 57062  16.191%\n",
      "fc layer 1 self.abs_max_out: 6278.0\n",
      "lif layer 1 self.abs_max_v: 10669.0\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.148643/  1.582317, val:  63.75%, val_best:  63.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 84.02 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 97.2930%\n",
      "layer   2  Sparsity: 71.8338%\n",
      "layer   3  Sparsity: 69.0007%\n",
      "total_backward_count 362230 real_backward_count 58574  16.170%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.119914/  1.995163, val:  20.42%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.53 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2938%\n",
      "layer   2  Sparsity: 71.9172%\n",
      "layer   3  Sparsity: 68.9576%\n",
      "total_backward_count 372020 real_backward_count 60026  16.135%\n",
      "fc layer 3 self.abs_max_out: 2658.0\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.174188/  1.610179, val:  49.17%, val_best:  63.75%, tr:  99.08%, tr_best: 100.00%, epoch time: 83.86 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 97.2903%\n",
      "layer   2  Sparsity: 71.7289%\n",
      "layer   3  Sparsity: 68.7813%\n",
      "total_backward_count 381810 real_backward_count 61493  16.106%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.160467/  1.608031, val:  50.83%, val_best:  63.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 84.08 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 97.2856%\n",
      "layer   2  Sparsity: 71.9519%\n",
      "layer   3  Sparsity: 69.7573%\n",
      "total_backward_count 391600 real_backward_count 63011  16.091%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.161596/  1.630382, val:  41.67%, val_best:  63.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 83.45 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2761%\n",
      "layer   2  Sparsity: 71.4183%\n",
      "layer   3  Sparsity: 69.5306%\n",
      "total_backward_count 401390 real_backward_count 64569  16.086%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.141773/  1.582857, val:  50.42%, val_best:  63.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 83.64 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2864%\n",
      "layer   2  Sparsity: 71.6167%\n",
      "layer   3  Sparsity: 69.2284%\n",
      "total_backward_count 411180 real_backward_count 66071  16.069%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.149468/  1.756648, val:  35.00%, val_best:  63.75%, tr:  98.88%, tr_best: 100.00%, epoch time: 84.11 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 97.2780%\n",
      "layer   2  Sparsity: 72.0214%\n",
      "layer   3  Sparsity: 69.3689%\n",
      "total_backward_count 420970 real_backward_count 67598  16.058%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.105602/  1.625199, val:  54.58%, val_best:  63.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 83.87 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 97.2711%\n",
      "layer   2  Sparsity: 71.6920%\n",
      "layer   3  Sparsity: 69.2647%\n",
      "total_backward_count 430760 real_backward_count 69116  16.045%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.139909/  1.578704, val:  57.50%, val_best:  63.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 84.38 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 97.2828%\n",
      "layer   2  Sparsity: 71.8129%\n",
      "layer   3  Sparsity: 69.2869%\n",
      "total_backward_count 440550 real_backward_count 70629  16.032%\n",
      "fc layer 1 self.abs_max_out: 6375.0\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.135304/  1.638507, val:  51.67%, val_best:  63.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 84.41 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 97.2789%\n",
      "layer   2  Sparsity: 71.7041%\n",
      "layer   3  Sparsity: 69.6163%\n",
      "total_backward_count 450340 real_backward_count 72115  16.013%\n",
      "fc layer 1 self.abs_max_out: 6626.0\n",
      "lif layer 1 self.abs_max_v: 11236.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.149195/  1.581282, val:  49.17%, val_best:  63.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 83.86 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 97.2839%\n",
      "layer   2  Sparsity: 71.6799%\n",
      "layer   3  Sparsity: 69.5771%\n",
      "total_backward_count 460130 real_backward_count 73664  16.009%\n",
      "fc layer 1 self.abs_max_out: 6703.0\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.100878/  1.608107, val:  43.75%, val_best:  63.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 84.23 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 97.2825%\n",
      "layer   2  Sparsity: 71.5193%\n",
      "layer   3  Sparsity: 68.9509%\n",
      "total_backward_count 469920 real_backward_count 75194  16.001%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.098872/  1.683155, val:  42.92%, val_best:  63.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 83.20 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.3058%\n",
      "layer   2  Sparsity: 71.6041%\n",
      "layer   3  Sparsity: 68.6908%\n",
      "total_backward_count 479710 real_backward_count 76722  15.993%\n",
      "fc layer 1 self.abs_max_out: 6722.0\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.104932/  1.546276, val:  41.25%, val_best:  63.75%, tr:  98.98%, tr_best: 100.00%, epoch time: 82.95 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2793%\n",
      "layer   2  Sparsity: 71.7923%\n",
      "layer   3  Sparsity: 68.7584%\n",
      "total_backward_count 489500 real_backward_count 78239  15.983%\n",
      "fc layer 1 self.abs_max_out: 6777.0\n",
      "lif layer 1 self.abs_max_v: 11316.0\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.101806/  1.591818, val:  40.42%, val_best:  63.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 81.05 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 97.2964%\n",
      "layer   2  Sparsity: 71.8253%\n",
      "layer   3  Sparsity: 68.8133%\n",
      "total_backward_count 499290 real_backward_count 79749  15.972%\n",
      "fc layer 1 self.abs_max_out: 6965.0\n",
      "lif layer 1 self.abs_max_v: 11725.0\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.043915/  1.542575, val:  58.75%, val_best:  63.75%, tr:  99.08%, tr_best: 100.00%, epoch time: 82.97 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2703%\n",
      "layer   2  Sparsity: 71.5573%\n",
      "layer   3  Sparsity: 68.3169%\n",
      "total_backward_count 509080 real_backward_count 81235  15.957%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.069705/  1.559355, val:  53.33%, val_best:  63.75%, tr:  99.18%, tr_best: 100.00%, epoch time: 83.45 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2804%\n",
      "layer   2  Sparsity: 71.8115%\n",
      "layer   3  Sparsity: 67.9613%\n",
      "total_backward_count 518870 real_backward_count 82782  15.954%\n",
      "fc layer 2 self.abs_max_out: 5837.0\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.083168/  1.506442, val:  60.00%, val_best:  63.75%, tr:  99.18%, tr_best: 100.00%, epoch time: 83.28 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2957%\n",
      "layer   2  Sparsity: 71.8266%\n",
      "layer   3  Sparsity: 68.6375%\n",
      "total_backward_count 528660 real_backward_count 84263  15.939%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.067129/  1.620692, val:  45.83%, val_best:  63.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 81.69 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 97.2665%\n",
      "layer   2  Sparsity: 71.6033%\n",
      "layer   3  Sparsity: 68.4500%\n",
      "total_backward_count 538450 real_backward_count 85757  15.927%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.070949/  1.612702, val:  38.33%, val_best:  63.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 83.46 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2682%\n",
      "layer   2  Sparsity: 71.7244%\n",
      "layer   3  Sparsity: 68.9352%\n",
      "total_backward_count 548240 real_backward_count 87283  15.921%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.089291/  1.575736, val:  56.25%, val_best:  63.75%, tr:  99.08%, tr_best: 100.00%, epoch time: 82.05 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2815%\n",
      "layer   2  Sparsity: 71.5406%\n",
      "layer   3  Sparsity: 69.1763%\n",
      "total_backward_count 558030 real_backward_count 88785  15.910%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.108868/  1.538780, val:  65.00%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.03 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2979%\n",
      "layer   2  Sparsity: 71.5191%\n",
      "layer   3  Sparsity: 70.2888%\n",
      "total_backward_count 567820 real_backward_count 90227  15.890%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.091123/  1.547901, val:  55.00%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.11 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2840%\n",
      "layer   2  Sparsity: 71.4954%\n",
      "layer   3  Sparsity: 70.4247%\n",
      "total_backward_count 577610 real_backward_count 91642  15.866%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.086149/  1.620584, val:  42.92%, val_best:  65.00%, tr:  99.28%, tr_best: 100.00%, epoch time: 82.45 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2728%\n",
      "layer   2  Sparsity: 71.5321%\n",
      "layer   3  Sparsity: 70.2551%\n",
      "total_backward_count 587400 real_backward_count 93099  15.849%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.088850/  1.591094, val:  43.75%, val_best:  65.00%, tr:  99.08%, tr_best: 100.00%, epoch time: 81.96 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2865%\n",
      "layer   2  Sparsity: 71.4966%\n",
      "layer   3  Sparsity: 70.2802%\n",
      "total_backward_count 597190 real_backward_count 94557  15.834%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.088761/  1.566212, val:  45.83%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.18 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2807%\n",
      "layer   2  Sparsity: 71.9018%\n",
      "layer   3  Sparsity: 70.5588%\n",
      "total_backward_count 606980 real_backward_count 96071  15.828%\n",
      "fc layer 2 self.abs_max_out: 6085.0\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.074914/  1.594720, val:  47.50%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.43 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2908%\n",
      "layer   2  Sparsity: 71.6322%\n",
      "layer   3  Sparsity: 70.4576%\n",
      "total_backward_count 616770 real_backward_count 97512  15.810%\n",
      "fc layer 1 self.abs_max_out: 7007.0\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.104093/  1.626886, val:  45.42%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 81.96 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2767%\n",
      "layer   2  Sparsity: 71.7550%\n",
      "layer   3  Sparsity: 70.5676%\n",
      "total_backward_count 626560 real_backward_count 98962  15.794%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.138752/  1.565820, val:  53.75%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 81.77 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 97.2865%\n",
      "layer   2  Sparsity: 71.7910%\n",
      "layer   3  Sparsity: 70.6739%\n",
      "total_backward_count 636350 real_backward_count 100478  15.790%\n",
      "lif layer 2 self.abs_max_v: 10108.5\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.128377/  1.586943, val:  48.75%, val_best:  65.00%, tr:  99.39%, tr_best: 100.00%, epoch time: 81.65 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 97.2859%\n",
      "layer   2  Sparsity: 71.7724%\n",
      "layer   3  Sparsity: 70.4719%\n",
      "total_backward_count 646140 real_backward_count 101874  15.767%\n",
      "fc layer 2 self.abs_max_out: 6086.0\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.099924/  1.629080, val:  47.08%, val_best:  65.00%, tr:  99.39%, tr_best: 100.00%, epoch time: 81.95 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2796%\n",
      "layer   2  Sparsity: 71.6758%\n",
      "layer   3  Sparsity: 70.0441%\n",
      "total_backward_count 655930 real_backward_count 103325  15.752%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.085761/  1.548509, val:  54.58%, val_best:  65.00%, tr:  99.39%, tr_best: 100.00%, epoch time: 82.26 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2778%\n",
      "layer   2  Sparsity: 71.8640%\n",
      "layer   3  Sparsity: 69.6568%\n",
      "total_backward_count 665720 real_backward_count 104714  15.729%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.120191/  1.539160, val:  62.50%, val_best:  65.00%, tr:  98.98%, tr_best: 100.00%, epoch time: 82.24 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2787%\n",
      "layer   2  Sparsity: 71.8690%\n",
      "layer   3  Sparsity: 69.6884%\n",
      "total_backward_count 675510 real_backward_count 106140  15.713%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.107720/  1.593239, val:  50.00%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.58 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2673%\n",
      "layer   2  Sparsity: 71.7714%\n",
      "layer   3  Sparsity: 69.9068%\n",
      "total_backward_count 685300 real_backward_count 107554  15.694%\n",
      "fc layer 1 self.abs_max_out: 7044.0\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.096005/  1.504670, val:  64.17%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.14 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2938%\n",
      "layer   2  Sparsity: 71.9781%\n",
      "layer   3  Sparsity: 69.8862%\n",
      "total_backward_count 695090 real_backward_count 109023  15.685%\n",
      "fc layer 1 self.abs_max_out: 7092.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.089743/  1.575260, val:  45.42%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.39 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2710%\n",
      "layer   2  Sparsity: 72.0112%\n",
      "layer   3  Sparsity: 70.1055%\n",
      "total_backward_count 704880 real_backward_count 110462  15.671%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.120139/  1.522913, val:  60.83%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.49 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.3025%\n",
      "layer   2  Sparsity: 71.8516%\n",
      "layer   3  Sparsity: 70.1019%\n",
      "total_backward_count 714670 real_backward_count 111910  15.659%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.091743/  1.626359, val:  48.75%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 83.54 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.3059%\n",
      "layer   2  Sparsity: 71.5726%\n",
      "layer   3  Sparsity: 69.7473%\n",
      "total_backward_count 724460 real_backward_count 113295  15.639%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.092606/  1.547048, val:  55.83%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.47 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2819%\n",
      "layer   2  Sparsity: 71.9194%\n",
      "layer   3  Sparsity: 70.1737%\n",
      "total_backward_count 734250 real_backward_count 114706  15.622%\n",
      "lif layer 2 self.abs_max_v: 10311.5\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.092667/  1.525038, val:  53.75%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.52 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2761%\n",
      "layer   2  Sparsity: 71.8434%\n",
      "layer   3  Sparsity: 70.4983%\n",
      "total_backward_count 744040 real_backward_count 116141  15.610%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.127623/  1.574565, val:  50.00%, val_best:  65.00%, tr:  99.28%, tr_best: 100.00%, epoch time: 82.58 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2648%\n",
      "layer   2  Sparsity: 71.7535%\n",
      "layer   3  Sparsity: 70.7033%\n",
      "total_backward_count 753830 real_backward_count 117650  15.607%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.101756/  1.679097, val:  32.08%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 81.90 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 97.2900%\n",
      "layer   2  Sparsity: 71.6895%\n",
      "layer   3  Sparsity: 70.1110%\n",
      "total_backward_count 763620 real_backward_count 119027  15.587%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.077843/  1.571488, val:  48.75%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.87 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2666%\n",
      "layer   2  Sparsity: 71.3651%\n",
      "layer   3  Sparsity: 69.6884%\n",
      "total_backward_count 773410 real_backward_count 120424  15.571%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.062645/  1.614392, val:  41.25%, val_best:  65.00%, tr:  99.18%, tr_best: 100.00%, epoch time: 82.13 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2795%\n",
      "layer   2  Sparsity: 71.6946%\n",
      "layer   3  Sparsity: 69.0314%\n",
      "total_backward_count 783200 real_backward_count 121852  15.558%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.069995/  1.541898, val:  57.92%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 82.64 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2835%\n",
      "layer   2  Sparsity: 71.7582%\n",
      "layer   3  Sparsity: 69.3774%\n",
      "total_backward_count 792990 real_backward_count 123327  15.552%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.062410/  1.700260, val:  37.92%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.46 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2694%\n",
      "layer   2  Sparsity: 71.6217%\n",
      "layer   3  Sparsity: 68.9924%\n",
      "total_backward_count 802780 real_backward_count 124755  15.540%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.051154/  1.526719, val:  52.92%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.58 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2659%\n",
      "layer   2  Sparsity: 71.7010%\n",
      "layer   3  Sparsity: 68.1330%\n",
      "total_backward_count 812570 real_backward_count 126207  15.532%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.042168/  1.605739, val:  53.75%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.31 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2673%\n",
      "layer   2  Sparsity: 72.0681%\n",
      "layer   3  Sparsity: 68.1696%\n",
      "total_backward_count 822360 real_backward_count 127656  15.523%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.057256/  1.536799, val:  40.83%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.76 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2795%\n",
      "layer   2  Sparsity: 71.7222%\n",
      "layer   3  Sparsity: 68.8472%\n",
      "total_backward_count 832150 real_backward_count 129089  15.513%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.046111/  1.699903, val:  34.17%, val_best:  65.00%, tr:  99.18%, tr_best: 100.00%, epoch time: 83.35 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2762%\n",
      "layer   2  Sparsity: 71.6686%\n",
      "layer   3  Sparsity: 69.0114%\n",
      "total_backward_count 841940 real_backward_count 130525  15.503%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.062388/  1.529660, val:  57.92%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.35 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2997%\n",
      "layer   2  Sparsity: 71.6908%\n",
      "layer   3  Sparsity: 68.6653%\n",
      "total_backward_count 851730 real_backward_count 131996  15.497%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.073052/  1.513520, val:  62.08%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.58 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2813%\n",
      "layer   2  Sparsity: 71.8304%\n",
      "layer   3  Sparsity: 68.0850%\n",
      "total_backward_count 861520 real_backward_count 133415  15.486%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.037184/  1.577994, val:  54.17%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.67 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2726%\n",
      "layer   2  Sparsity: 71.8743%\n",
      "layer   3  Sparsity: 68.3881%\n",
      "total_backward_count 871310 real_backward_count 134823  15.474%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.084706/  1.558939, val:  50.00%, val_best:  65.00%, tr:  99.18%, tr_best: 100.00%, epoch time: 82.34 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2737%\n",
      "layer   2  Sparsity: 71.9160%\n",
      "layer   3  Sparsity: 69.3141%\n",
      "total_backward_count 881100 real_backward_count 136246  15.463%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.075195/  1.572194, val:  45.42%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 82.48 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2928%\n",
      "layer   2  Sparsity: 71.9428%\n",
      "layer   3  Sparsity: 69.9297%\n",
      "total_backward_count 890890 real_backward_count 137677  15.454%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.095394/  1.483527, val:  62.92%, val_best:  65.00%, tr:  99.39%, tr_best: 100.00%, epoch time: 82.21 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2730%\n",
      "layer   2  Sparsity: 71.6941%\n",
      "layer   3  Sparsity: 70.1568%\n",
      "total_backward_count 900680 real_backward_count 139120  15.446%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.067808/  1.576589, val:  55.83%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.29 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2921%\n",
      "layer   2  Sparsity: 71.5333%\n",
      "layer   3  Sparsity: 69.4164%\n",
      "total_backward_count 910470 real_backward_count 140585  15.441%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.073987/  1.568390, val:  61.67%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.50 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2805%\n",
      "layer   2  Sparsity: 71.2856%\n",
      "layer   3  Sparsity: 69.2297%\n",
      "total_backward_count 920260 real_backward_count 141998  15.430%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.066222/  1.485860, val:  53.33%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.42 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2739%\n",
      "layer   2  Sparsity: 71.2454%\n",
      "layer   3  Sparsity: 69.0918%\n",
      "total_backward_count 930050 real_backward_count 143403  15.419%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.039434/  1.629500, val:  41.67%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 81.79 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 97.2810%\n",
      "layer   2  Sparsity: 71.5702%\n",
      "layer   3  Sparsity: 68.2072%\n",
      "total_backward_count 939840 real_backward_count 144794  15.406%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.040554/  1.587827, val:  40.42%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.56 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2599%\n",
      "layer   2  Sparsity: 71.7200%\n",
      "layer   3  Sparsity: 68.3528%\n",
      "total_backward_count 949630 real_backward_count 146150  15.390%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.052182/  1.468757, val:  57.92%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.35 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2932%\n",
      "layer   2  Sparsity: 71.8154%\n",
      "layer   3  Sparsity: 69.7476%\n",
      "total_backward_count 959420 real_backward_count 147554  15.380%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.037374/  1.538137, val:  57.50%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 83.21 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2880%\n",
      "layer   2  Sparsity: 71.6735%\n",
      "layer   3  Sparsity: 69.6205%\n",
      "total_backward_count 969210 real_backward_count 148968  15.370%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.051963/  1.468951, val:  65.83%, val_best:  65.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.72 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2927%\n",
      "layer   2  Sparsity: 71.5645%\n",
      "layer   3  Sparsity: 69.4447%\n",
      "total_backward_count 979000 real_backward_count 150324  15.355%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.057593/  1.476942, val:  57.08%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 82.18 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2814%\n",
      "layer   2  Sparsity: 71.6947%\n",
      "layer   3  Sparsity: 69.4761%\n",
      "total_backward_count 988790 real_backward_count 151712  15.343%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.065729/  1.507268, val:  54.58%, val_best:  65.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.39 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2785%\n",
      "layer   2  Sparsity: 71.5351%\n",
      "layer   3  Sparsity: 69.7204%\n",
      "total_backward_count 998580 real_backward_count 153099  15.332%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.041585/  1.472819, val:  57.08%, val_best:  65.83%, tr:  99.39%, tr_best: 100.00%, epoch time: 82.93 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2774%\n",
      "layer   2  Sparsity: 71.6049%\n",
      "layer   3  Sparsity: 69.2741%\n",
      "total_backward_count 1008370 real_backward_count 154483  15.320%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.044842/  1.539948, val:  47.08%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 82.59 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2701%\n",
      "layer   2  Sparsity: 71.9629%\n",
      "layer   3  Sparsity: 69.5238%\n",
      "total_backward_count 1018160 real_backward_count 155878  15.310%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.035528/  1.470377, val:  69.17%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.74 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2645%\n",
      "layer   2  Sparsity: 71.9434%\n",
      "layer   3  Sparsity: 68.8904%\n",
      "total_backward_count 1027950 real_backward_count 157319  15.304%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.049257/  1.671464, val:  33.75%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.25 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2906%\n",
      "layer   2  Sparsity: 72.0912%\n",
      "layer   3  Sparsity: 68.6222%\n",
      "total_backward_count 1037740 real_backward_count 158740  15.297%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.034018/  1.657498, val:  38.75%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.27 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2945%\n",
      "layer   2  Sparsity: 72.1987%\n",
      "layer   3  Sparsity: 69.0054%\n",
      "total_backward_count 1047530 real_backward_count 160133  15.287%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.035376/  1.536686, val:  53.75%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 82.13 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2439%\n",
      "layer   2  Sparsity: 72.1968%\n",
      "layer   3  Sparsity: 69.5349%\n",
      "total_backward_count 1057320 real_backward_count 161547  15.279%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.056524/  1.516020, val:  60.83%, val_best:  69.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 82.46 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2793%\n",
      "layer   2  Sparsity: 72.1078%\n",
      "layer   3  Sparsity: 69.3324%\n",
      "total_backward_count 1067110 real_backward_count 162970  15.272%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.028758/  1.559743, val:  46.25%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 82.90 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2813%\n",
      "layer   2  Sparsity: 72.1717%\n",
      "layer   3  Sparsity: 69.1159%\n",
      "total_backward_count 1076900 real_backward_count 164355  15.262%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.054069/  1.480319, val:  65.00%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 81.77 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 97.2822%\n",
      "layer   2  Sparsity: 72.1306%\n",
      "layer   3  Sparsity: 69.6911%\n",
      "total_backward_count 1086690 real_backward_count 165722  15.250%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.028242/  1.610491, val:  39.17%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.68 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2798%\n",
      "layer   2  Sparsity: 72.3178%\n",
      "layer   3  Sparsity: 69.2947%\n",
      "total_backward_count 1096480 real_backward_count 167109  15.240%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.027535/  1.546506, val:  52.08%, val_best:  69.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 82.77 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2927%\n",
      "layer   2  Sparsity: 72.1067%\n",
      "layer   3  Sparsity: 68.4408%\n",
      "total_backward_count 1106270 real_backward_count 168545  15.235%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.023656/  1.527635, val:  52.92%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.67 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2964%\n",
      "layer   2  Sparsity: 71.8071%\n",
      "layer   3  Sparsity: 68.1543%\n",
      "total_backward_count 1116060 real_backward_count 169924  15.225%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.013090/  1.545792, val:  47.08%, val_best:  69.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 82.18 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2744%\n",
      "layer   2  Sparsity: 72.0360%\n",
      "layer   3  Sparsity: 68.6061%\n",
      "total_backward_count 1125850 real_backward_count 171300  15.215%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.028341/  1.504009, val:  60.42%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 83.06 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2783%\n",
      "layer   2  Sparsity: 72.0622%\n",
      "layer   3  Sparsity: 68.4883%\n",
      "total_backward_count 1135640 real_backward_count 172687  15.206%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.050492/  1.597996, val:  36.67%, val_best:  69.17%, tr:  99.08%, tr_best: 100.00%, epoch time: 82.67 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2936%\n",
      "layer   2  Sparsity: 72.2925%\n",
      "layer   3  Sparsity: 68.6230%\n",
      "total_backward_count 1145430 real_backward_count 174121  15.201%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.033406/  1.550520, val:  57.50%, val_best:  69.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 83.20 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2814%\n",
      "layer   2  Sparsity: 72.1573%\n",
      "layer   3  Sparsity: 68.5588%\n",
      "total_backward_count 1155220 real_backward_count 175525  15.194%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.057136/  1.570155, val:  49.17%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.33 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2985%\n",
      "layer   2  Sparsity: 72.2257%\n",
      "layer   3  Sparsity: 69.1700%\n",
      "total_backward_count 1165010 real_backward_count 176961  15.190%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.074469/  1.507566, val:  53.33%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 83.35 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2573%\n",
      "layer   2  Sparsity: 72.3485%\n",
      "layer   3  Sparsity: 69.2547%\n",
      "total_backward_count 1174800 real_backward_count 178352  15.181%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.015887/  1.510062, val:  57.08%, val_best:  69.17%, tr:  98.98%, tr_best: 100.00%, epoch time: 82.88 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2857%\n",
      "layer   2  Sparsity: 72.3009%\n",
      "layer   3  Sparsity: 68.9240%\n",
      "total_backward_count 1184590 real_backward_count 179726  15.172%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.046899/  1.518324, val:  55.42%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 82.83 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2984%\n",
      "layer   2  Sparsity: 72.1357%\n",
      "layer   3  Sparsity: 68.7881%\n",
      "total_backward_count 1194380 real_backward_count 181125  15.165%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.053972/  1.541385, val:  57.08%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.63 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2702%\n",
      "layer   2  Sparsity: 71.8374%\n",
      "layer   3  Sparsity: 69.0596%\n",
      "total_backward_count 1204170 real_backward_count 182555  15.160%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.047887/  1.455201, val:  67.50%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.70 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2750%\n",
      "layer   2  Sparsity: 71.7167%\n",
      "layer   3  Sparsity: 69.8693%\n",
      "total_backward_count 1213960 real_backward_count 183943  15.152%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.024363/  1.454854, val:  65.00%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 84.22 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 97.2705%\n",
      "layer   2  Sparsity: 72.0572%\n",
      "layer   3  Sparsity: 69.8321%\n",
      "total_backward_count 1223750 real_backward_count 185333  15.145%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.033369/  1.584155, val:  54.17%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.66 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2775%\n",
      "layer   2  Sparsity: 71.9609%\n",
      "layer   3  Sparsity: 69.9160%\n",
      "total_backward_count 1233540 real_backward_count 186718  15.137%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.058672/  1.619553, val:  33.33%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 83.34 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2974%\n",
      "layer   2  Sparsity: 71.9622%\n",
      "layer   3  Sparsity: 69.7940%\n",
      "total_backward_count 1243330 real_backward_count 188100  15.129%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.016227/  1.601005, val:  40.83%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 83.17 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2829%\n",
      "layer   2  Sparsity: 71.7291%\n",
      "layer   3  Sparsity: 68.5071%\n",
      "total_backward_count 1253120 real_backward_count 189416  15.116%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.036191/  1.560235, val:  52.50%, val_best:  69.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 83.39 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2872%\n",
      "layer   2  Sparsity: 71.8663%\n",
      "layer   3  Sparsity: 68.0634%\n",
      "total_backward_count 1262910 real_backward_count 190759  15.105%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.046703/  1.485099, val:  59.58%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 83.44 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2892%\n",
      "layer   2  Sparsity: 71.8968%\n",
      "layer   3  Sparsity: 68.6009%\n",
      "total_backward_count 1272700 real_backward_count 192118  15.095%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.020985/  1.657265, val:  34.58%, val_best:  69.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.94 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2592%\n",
      "layer   2  Sparsity: 72.2740%\n",
      "layer   3  Sparsity: 68.1019%\n",
      "total_backward_count 1282490 real_backward_count 193472  15.086%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.014482/  1.509217, val:  59.17%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 83.26 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2728%\n",
      "layer   2  Sparsity: 71.7791%\n",
      "layer   3  Sparsity: 68.5970%\n",
      "total_backward_count 1292280 real_backward_count 194874  15.080%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.015970/  1.445471, val:  55.00%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 83.11 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2968%\n",
      "layer   2  Sparsity: 71.8808%\n",
      "layer   3  Sparsity: 67.8981%\n",
      "total_backward_count 1302070 real_backward_count 196296  15.076%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.010090/  1.500247, val:  60.00%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 82.61 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2728%\n",
      "layer   2  Sparsity: 71.9708%\n",
      "layer   3  Sparsity: 67.8705%\n",
      "total_backward_count 1311860 real_backward_count 197686  15.069%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.039273/  1.515924, val:  58.33%, val_best:  69.17%, tr:  99.28%, tr_best: 100.00%, epoch time: 82.76 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2743%\n",
      "layer   2  Sparsity: 72.0472%\n",
      "layer   3  Sparsity: 69.1023%\n",
      "total_backward_count 1321650 real_backward_count 199016  15.058%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.020159/  1.514694, val:  48.33%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.13 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.3011%\n",
      "layer   2  Sparsity: 71.9619%\n",
      "layer   3  Sparsity: 68.7815%\n",
      "total_backward_count 1331440 real_backward_count 200418  15.053%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.999395/  1.426395, val:  57.08%, val_best:  69.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 82.74 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2892%\n",
      "layer   2  Sparsity: 71.6660%\n",
      "layer   3  Sparsity: 68.3402%\n",
      "total_backward_count 1341230 real_backward_count 201776  15.044%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.018759/  1.461178, val:  62.50%, val_best:  69.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 83.00 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2647%\n",
      "layer   2  Sparsity: 71.7519%\n",
      "layer   3  Sparsity: 68.7091%\n",
      "total_backward_count 1351020 real_backward_count 203118  15.034%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.995563/  1.497184, val:  60.83%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 83.22 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2635%\n",
      "layer   2  Sparsity: 71.6648%\n",
      "layer   3  Sparsity: 68.2979%\n",
      "total_backward_count 1360810 real_backward_count 204488  15.027%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.025814/  1.575204, val:  42.92%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.91 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2780%\n",
      "layer   2  Sparsity: 72.0881%\n",
      "layer   3  Sparsity: 68.6737%\n",
      "total_backward_count 1370600 real_backward_count 205870  15.020%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.060920/  1.591274, val:  42.08%, val_best:  69.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 82.71 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2881%\n",
      "layer   2  Sparsity: 71.7776%\n",
      "layer   3  Sparsity: 69.3380%\n",
      "total_backward_count 1380390 real_backward_count 207259  15.015%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.037198/  1.501766, val:  59.58%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.68 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2638%\n",
      "layer   2  Sparsity: 71.7805%\n",
      "layer   3  Sparsity: 69.0547%\n",
      "total_backward_count 1390180 real_backward_count 208613  15.006%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.040618/  1.563991, val:  48.33%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.12 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2896%\n",
      "layer   2  Sparsity: 71.7920%\n",
      "layer   3  Sparsity: 69.2385%\n",
      "total_backward_count 1399970 real_backward_count 210005  15.001%\n",
      "lif layer 1 self.abs_max_v: 11746.0\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.035504/  1.561429, val:  40.83%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 83.06 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2640%\n",
      "layer   2  Sparsity: 71.9214%\n",
      "layer   3  Sparsity: 68.9231%\n",
      "total_backward_count 1409760 real_backward_count 211399  14.995%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.050808/  1.465617, val:  67.50%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.54 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2808%\n",
      "layer   2  Sparsity: 71.7839%\n",
      "layer   3  Sparsity: 68.8466%\n",
      "total_backward_count 1419550 real_backward_count 212771  14.989%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.034094/  1.471059, val:  50.83%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 82.76 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2859%\n",
      "layer   2  Sparsity: 71.7726%\n",
      "layer   3  Sparsity: 68.4565%\n",
      "total_backward_count 1429340 real_backward_count 214068  14.977%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.051698/  1.547892, val:  57.92%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.81 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2818%\n",
      "layer   2  Sparsity: 71.5630%\n",
      "layer   3  Sparsity: 68.7076%\n",
      "total_backward_count 1439130 real_backward_count 215433  14.970%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.019237/  1.775422, val:  32.92%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 82.87 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2786%\n",
      "layer   2  Sparsity: 71.8488%\n",
      "layer   3  Sparsity: 69.1823%\n",
      "total_backward_count 1448920 real_backward_count 216777  14.961%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.016652/  1.432974, val:  66.25%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 82.62 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.3014%\n",
      "layer   2  Sparsity: 71.8600%\n",
      "layer   3  Sparsity: 69.4557%\n",
      "total_backward_count 1458710 real_backward_count 218164  14.956%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.029357/  1.513135, val:  53.33%, val_best:  69.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 83.34 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2806%\n",
      "layer   2  Sparsity: 72.2692%\n",
      "layer   3  Sparsity: 68.0748%\n",
      "total_backward_count 1468500 real_backward_count 219538  14.950%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.006415/  1.521225, val:  54.58%, val_best:  69.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 83.42 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2540%\n",
      "layer   2  Sparsity: 71.8494%\n",
      "layer   3  Sparsity: 68.5548%\n",
      "total_backward_count 1478290 real_backward_count 220919  14.944%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.020357/  1.485226, val:  60.00%, val_best:  69.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 84.01 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 97.3031%\n",
      "layer   2  Sparsity: 71.7302%\n",
      "layer   3  Sparsity: 68.5016%\n",
      "total_backward_count 1488080 real_backward_count 222259  14.936%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.000714/  1.470175, val:  54.58%, val_best:  69.17%, tr:  99.18%, tr_best: 100.00%, epoch time: 82.82 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2831%\n",
      "layer   2  Sparsity: 71.8774%\n",
      "layer   3  Sparsity: 68.2521%\n",
      "total_backward_count 1497870 real_backward_count 223603  14.928%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.007350/  1.502360, val:  47.92%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 83.05 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2825%\n",
      "layer   2  Sparsity: 71.6592%\n",
      "layer   3  Sparsity: 68.1145%\n",
      "total_backward_count 1507660 real_backward_count 224991  14.923%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.998034/  1.434875, val:  61.25%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.71 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2814%\n",
      "layer   2  Sparsity: 71.9626%\n",
      "layer   3  Sparsity: 68.3747%\n",
      "total_backward_count 1517450 real_backward_count 226361  14.917%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.027839/  1.523899, val:  55.00%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.67 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2869%\n",
      "layer   2  Sparsity: 71.9678%\n",
      "layer   3  Sparsity: 68.7742%\n",
      "total_backward_count 1527240 real_backward_count 227728  14.911%\n",
      "fc layer 2 self.abs_max_out: 6095.0\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.035440/  1.491997, val:  55.42%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 83.58 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2890%\n",
      "layer   2  Sparsity: 71.9100%\n",
      "layer   3  Sparsity: 67.8154%\n",
      "total_backward_count 1537030 real_backward_count 229113  14.906%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.009337/  1.397053, val:  70.83%, val_best:  70.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.63 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2684%\n",
      "layer   2  Sparsity: 72.0349%\n",
      "layer   3  Sparsity: 68.1033%\n",
      "total_backward_count 1546820 real_backward_count 230463  14.899%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.008104/  1.439019, val:  61.67%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.48 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2754%\n",
      "layer   2  Sparsity: 71.8346%\n",
      "layer   3  Sparsity: 68.1195%\n",
      "total_backward_count 1556610 real_backward_count 231767  14.889%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.016342/  1.510178, val:  47.92%, val_best:  70.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.62 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2855%\n",
      "layer   2  Sparsity: 71.8800%\n",
      "layer   3  Sparsity: 68.3377%\n",
      "total_backward_count 1566400 real_backward_count 233095  14.881%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.027621/  1.432933, val:  67.50%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 82.73 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2680%\n",
      "layer   2  Sparsity: 72.0279%\n",
      "layer   3  Sparsity: 68.4884%\n",
      "total_backward_count 1576190 real_backward_count 234442  14.874%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.015041/  1.525516, val:  56.67%, val_best:  70.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.75 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2783%\n",
      "layer   2  Sparsity: 72.0843%\n",
      "layer   3  Sparsity: 68.6927%\n",
      "total_backward_count 1585980 real_backward_count 235794  14.867%\n",
      "fc layer 2 self.abs_max_out: 6152.0\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.998040/  1.519682, val:  47.08%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 82.92 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2728%\n",
      "layer   2  Sparsity: 71.9797%\n",
      "layer   3  Sparsity: 68.0797%\n",
      "total_backward_count 1595770 real_backward_count 237073  14.856%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.976683/  1.502782, val:  56.25%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 82.80 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2785%\n",
      "layer   2  Sparsity: 71.9880%\n",
      "layer   3  Sparsity: 67.5921%\n",
      "total_backward_count 1605560 real_backward_count 238404  14.849%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.006159/  1.442481, val:  62.50%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.68 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2955%\n",
      "layer   2  Sparsity: 71.8409%\n",
      "layer   3  Sparsity: 67.2066%\n",
      "total_backward_count 1615350 real_backward_count 239740  14.841%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.991042/  1.569429, val:  43.75%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 83.67 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2861%\n",
      "layer   2  Sparsity: 71.8277%\n",
      "layer   3  Sparsity: 67.3366%\n",
      "total_backward_count 1625140 real_backward_count 241112  14.836%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.002581/  1.611087, val:  45.83%, val_best:  70.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.88 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2712%\n",
      "layer   2  Sparsity: 71.6194%\n",
      "layer   3  Sparsity: 66.9147%\n",
      "total_backward_count 1634930 real_backward_count 242440  14.829%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.990889/  1.549739, val:  47.50%, val_best:  70.83%, tr:  99.39%, tr_best: 100.00%, epoch time: 83.58 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2819%\n",
      "layer   2  Sparsity: 71.5075%\n",
      "layer   3  Sparsity: 66.6735%\n",
      "total_backward_count 1644720 real_backward_count 243835  14.825%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.995602/  1.448829, val:  56.67%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 82.97 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2754%\n",
      "layer   2  Sparsity: 71.8159%\n",
      "layer   3  Sparsity: 67.0581%\n",
      "total_backward_count 1654510 real_backward_count 245167  14.818%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.982353/  1.439633, val:  51.25%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 83.15 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2677%\n",
      "layer   2  Sparsity: 71.6681%\n",
      "layer   3  Sparsity: 67.3662%\n",
      "total_backward_count 1664300 real_backward_count 246516  14.812%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.969148/  1.464508, val:  54.17%, val_best:  70.83%, tr:  99.18%, tr_best: 100.00%, epoch time: 82.95 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2886%\n",
      "layer   2  Sparsity: 71.8340%\n",
      "layer   3  Sparsity: 67.7334%\n",
      "total_backward_count 1674090 real_backward_count 247878  14.807%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.985500/  1.517022, val:  50.00%, val_best:  70.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.44 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2768%\n",
      "layer   2  Sparsity: 71.7241%\n",
      "layer   3  Sparsity: 66.8775%\n",
      "total_backward_count 1683880 real_backward_count 249288  14.804%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.003260/  1.462610, val:  60.42%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.66 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2711%\n",
      "layer   2  Sparsity: 72.0987%\n",
      "layer   3  Sparsity: 67.3900%\n",
      "total_backward_count 1693670 real_backward_count 250695  14.802%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.999734/  1.580981, val:  41.67%, val_best:  70.83%, tr:  98.77%, tr_best: 100.00%, epoch time: 82.63 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.3063%\n",
      "layer   2  Sparsity: 71.5420%\n",
      "layer   3  Sparsity: 67.5171%\n",
      "total_backward_count 1703460 real_backward_count 252039  14.796%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.008582/  1.461434, val:  52.50%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 82.75 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2630%\n",
      "layer   2  Sparsity: 71.6014%\n",
      "layer   3  Sparsity: 68.1139%\n",
      "total_backward_count 1713250 real_backward_count 253421  14.792%\n",
      "fc layer 2 self.abs_max_out: 6164.0\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.971516/  1.405504, val:  62.50%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.31 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2727%\n",
      "layer   2  Sparsity: 71.8290%\n",
      "layer   3  Sparsity: 67.8023%\n",
      "total_backward_count 1723040 real_backward_count 254701  14.782%\n",
      "fc layer 2 self.abs_max_out: 6269.0\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.958117/  1.450695, val:  59.58%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 83.52 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2784%\n",
      "layer   2  Sparsity: 71.7342%\n",
      "layer   3  Sparsity: 67.4114%\n",
      "total_backward_count 1732830 real_backward_count 255983  14.773%\n",
      "lif layer 1 self.abs_max_v: 11832.5\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.994663/  1.432692, val:  60.83%, val_best:  70.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.88 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2649%\n",
      "layer   2  Sparsity: 72.0418%\n",
      "layer   3  Sparsity: 67.7323%\n",
      "total_backward_count 1742620 real_backward_count 257312  14.766%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.972758/  1.574802, val:  52.92%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 83.21 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 97.2651%\n",
      "layer   2  Sparsity: 72.0978%\n",
      "layer   3  Sparsity: 68.3297%\n",
      "total_backward_count 1752410 real_backward_count 258560  14.755%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.977682/  1.440543, val:  63.75%, val_best:  70.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 83.01 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2844%\n",
      "layer   2  Sparsity: 72.0767%\n",
      "layer   3  Sparsity: 67.8543%\n",
      "total_backward_count 1762200 real_backward_count 259841  14.745%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.983863/  1.435273, val:  65.00%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 82.58 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2520%\n",
      "layer   2  Sparsity: 72.0416%\n",
      "layer   3  Sparsity: 67.3249%\n",
      "total_backward_count 1771990 real_backward_count 261181  14.739%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.959335/  1.460782, val:  47.50%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 83.06 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2766%\n",
      "layer   2  Sparsity: 71.9895%\n",
      "layer   3  Sparsity: 67.0856%\n",
      "total_backward_count 1781780 real_backward_count 262479  14.731%\n",
      "lif layer 1 self.abs_max_v: 11975.5\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.956678/  1.444668, val:  64.17%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 82.39 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2811%\n",
      "layer   2  Sparsity: 71.5858%\n",
      "layer   3  Sparsity: 67.6622%\n",
      "total_backward_count 1791570 real_backward_count 263867  14.728%\n",
      "lif layer 1 self.abs_max_v: 12080.0\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.983462/  1.727338, val:  35.83%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 82.67 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2935%\n",
      "layer   2  Sparsity: 72.0144%\n",
      "layer   3  Sparsity: 67.4937%\n",
      "total_backward_count 1801360 real_backward_count 265250  14.725%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.981546/  1.487517, val:  51.67%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 81.98 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2971%\n",
      "layer   2  Sparsity: 72.0491%\n",
      "layer   3  Sparsity: 67.4232%\n",
      "total_backward_count 1811150 real_backward_count 266581  14.719%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.992815/  1.454097, val:  62.08%, val_best:  70.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.60 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2954%\n",
      "layer   2  Sparsity: 71.6263%\n",
      "layer   3  Sparsity: 66.8557%\n",
      "total_backward_count 1820940 real_backward_count 267896  14.712%\n",
      "fc layer 3 self.abs_max_out: 2694.0\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.978716/  1.587084, val:  45.42%, val_best:  70.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.58 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2865%\n",
      "layer   2  Sparsity: 71.7427%\n",
      "layer   3  Sparsity: 66.4728%\n",
      "total_backward_count 1830730 real_backward_count 269266  14.708%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.984811/  1.540305, val:  43.75%, val_best:  70.83%, tr:  99.18%, tr_best: 100.00%, epoch time: 82.59 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2940%\n",
      "layer   2  Sparsity: 72.0188%\n",
      "layer   3  Sparsity: 66.5167%\n",
      "total_backward_count 1840520 real_backward_count 270614  14.703%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.974232/  1.553923, val:  42.50%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 82.18 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2799%\n",
      "layer   2  Sparsity: 72.1648%\n",
      "layer   3  Sparsity: 66.9007%\n",
      "total_backward_count 1850310 real_backward_count 271925  14.696%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.961741/  1.434458, val:  60.83%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.52 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2761%\n",
      "layer   2  Sparsity: 71.8561%\n",
      "layer   3  Sparsity: 66.1055%\n",
      "total_backward_count 1860100 real_backward_count 273257  14.690%\n",
      "lif layer 1 self.abs_max_v: 12142.5\n",
      "lif layer 1 self.abs_max_v: 12217.5\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.973457/  1.418731, val:  67.92%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.17 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2752%\n",
      "layer   2  Sparsity: 71.8872%\n",
      "layer   3  Sparsity: 66.1523%\n",
      "total_backward_count 1869890 real_backward_count 274526  14.681%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.000622/  1.601948, val:  42.50%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 82.84 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2826%\n",
      "layer   2  Sparsity: 71.8712%\n",
      "layer   3  Sparsity: 67.2227%\n",
      "total_backward_count 1879680 real_backward_count 275838  14.675%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.003613/  1.492665, val:  55.42%, val_best:  70.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.62 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2839%\n",
      "layer   2  Sparsity: 72.0089%\n",
      "layer   3  Sparsity: 67.8970%\n",
      "total_backward_count 1889470 real_backward_count 277192  14.670%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.979415/  1.557982, val:  37.92%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 81.93 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2816%\n",
      "layer   2  Sparsity: 71.8112%\n",
      "layer   3  Sparsity: 68.6841%\n",
      "total_backward_count 1899260 real_backward_count 278463  14.662%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.001062/  1.561044, val:  38.75%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 82.59 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2828%\n",
      "layer   2  Sparsity: 72.0444%\n",
      "layer   3  Sparsity: 68.0785%\n",
      "total_backward_count 1909050 real_backward_count 279806  14.657%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.990538/  1.456022, val:  58.75%, val_best:  70.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 81.94 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 97.2854%\n",
      "layer   2  Sparsity: 72.3412%\n",
      "layer   3  Sparsity: 67.5151%\n",
      "total_backward_count 1918840 real_backward_count 281204  14.655%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.017398/  1.446138, val:  57.50%, val_best:  70.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 82.77 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2859%\n",
      "layer   2  Sparsity: 72.2974%\n",
      "layer   3  Sparsity: 67.6128%\n",
      "total_backward_count 1928630 real_backward_count 282582  14.652%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.016132/  1.460574, val:  52.92%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 83.10 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2955%\n",
      "layer   2  Sparsity: 71.9653%\n",
      "layer   3  Sparsity: 67.3113%\n",
      "total_backward_count 1938420 real_backward_count 283894  14.646%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.991222/  1.493642, val:  51.25%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 83.08 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2802%\n",
      "layer   2  Sparsity: 72.0494%\n",
      "layer   3  Sparsity: 68.1992%\n",
      "total_backward_count 1948210 real_backward_count 285231  14.641%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.980826/  1.447378, val:  57.50%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 82.62 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 97.2968%\n",
      "layer   2  Sparsity: 71.9556%\n",
      "layer   3  Sparsity: 67.8535%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7655e955524e3ea04baf66c0000f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÇ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ‚ñÜ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÖ‚ñÇ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÇ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99796</td></tr><tr><td>tr_epoch_loss</td><td>0.98083</td></tr><tr><td>val_acc_best</td><td>0.70833</td></tr><tr><td>val_acc_now</td><td>0.575</td></tr><tr><td>val_loss</td><td>1.44738</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">autumn-sweep-332</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e2ln1yek' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e2ln1yek</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251127_185158-e2ln1yek/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e8kd01bw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251127_232545-e8kd01bw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e8kd01bw' target=\"_blank\">breezy-sweep-338</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e8kd01bw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e8kd01bw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251127_232555_302', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = df820968b21bc2412e6634ebdd407347\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 67\n",
      "fc layer 1 self.abs_max_out: 549.0\n",
      "lif layer 1 self.abs_max_v: 549.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 645.0\n",
      "lif layer 2 self.abs_max_v: 645.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 118.0\n",
      "fc layer 1 self.abs_max_out: 615.0\n",
      "lif layer 1 self.abs_max_v: 889.5\n",
      "fc layer 2 self.abs_max_out: 745.0\n",
      "lif layer 2 self.abs_max_v: 842.5\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "fc layer 1 self.abs_max_out: 716.0\n",
      "fc layer 2 self.abs_max_out: 829.0\n",
      "lif layer 2 self.abs_max_v: 1135.0\n",
      "fc layer 1 self.abs_max_out: 864.0\n",
      "lif layer 1 self.abs_max_v: 955.5\n",
      "lif layer 2 self.abs_max_v: 1180.5\n",
      "smallest_now_T updated: 60\n",
      "fc layer 1 self.abs_max_out: 938.0\n",
      "fc layer 2 self.abs_max_out: 1053.0\n",
      "lif layer 2 self.abs_max_v: 1208.0\n",
      "lif layer 1 self.abs_max_v: 1099.5\n",
      "lif layer 1 self.abs_max_v: 1129.0\n",
      "fc layer 1 self.abs_max_out: 1283.0\n",
      "lif layer 1 self.abs_max_v: 1283.0\n",
      "lif layer 2 self.abs_max_v: 1329.0\n",
      "smallest_now_T updated: 45\n",
      "lif layer 1 self.abs_max_v: 1292.5\n",
      "lif layer 1 self.abs_max_v: 1623.5\n",
      "fc layer 1 self.abs_max_out: 1399.0\n",
      "lif layer 1 self.abs_max_v: 1754.0\n",
      "fc layer 1 self.abs_max_out: 1543.0\n",
      "fc layer 1 self.abs_max_out: 1622.0\n",
      "lif layer 2 self.abs_max_v: 1426.5\n",
      "fc layer 1 self.abs_max_out: 2043.0\n",
      "lif layer 1 self.abs_max_v: 2043.0\n",
      "lif layer 2 self.abs_max_v: 1556.5\n",
      "fc layer 1 self.abs_max_out: 2342.0\n",
      "lif layer 1 self.abs_max_v: 2342.0\n",
      "lif layer 2 self.abs_max_v: 1599.0\n",
      "fc layer 2 self.abs_max_out: 1098.0\n",
      "lif layer 2 self.abs_max_v: 1767.5\n",
      "fc layer 3 self.abs_max_out: 396.0\n",
      "fc layer 2 self.abs_max_out: 1135.0\n",
      "fc layer 2 self.abs_max_out: 1152.0\n",
      "fc layer 2 self.abs_max_out: 1213.0\n",
      "fc layer 2 self.abs_max_out: 1225.0\n",
      "lif layer 2 self.abs_max_v: 1847.5\n",
      "fc layer 2 self.abs_max_out: 1447.0\n",
      "smallest_now_T updated: 37\n",
      "fc layer 3 self.abs_max_out: 483.0\n",
      "lif layer 1 self.abs_max_v: 2486.5\n",
      "fc layer 2 self.abs_max_out: 1448.0\n",
      "lif layer 2 self.abs_max_v: 1870.5\n",
      "lif layer 2 self.abs_max_v: 1976.5\n",
      "smallest_now_T updated: 34\n",
      "fc layer 1 self.abs_max_out: 2349.0\n",
      "fc layer 1 self.abs_max_out: 2513.0\n",
      "lif layer 1 self.abs_max_v: 2513.0\n",
      "lif layer 2 self.abs_max_v: 1977.0\n",
      "lif layer 2 self.abs_max_v: 2114.5\n",
      "lif layer 1 self.abs_max_v: 2796.5\n",
      "lif layer 2 self.abs_max_v: 2315.5\n",
      "lif layer 1 self.abs_max_v: 2910.0\n",
      "smallest_now_T updated: 30\n",
      "fc layer 2 self.abs_max_out: 1501.0\n",
      "lif layer 2 self.abs_max_v: 2318.5\n",
      "fc layer 1 self.abs_max_out: 2789.0\n",
      "fc layer 2 self.abs_max_out: 1529.0\n",
      "fc layer 2 self.abs_max_out: 1695.0\n",
      "fc layer 2 self.abs_max_out: 1747.0\n",
      "lif layer 1 self.abs_max_v: 2989.0\n",
      "lif layer 1 self.abs_max_v: 3019.0\n",
      "lif layer 1 self.abs_max_v: 3228.0\n",
      "lif layer 1 self.abs_max_v: 3349.5\n",
      "fc layer 2 self.abs_max_out: 1787.0\n",
      "fc layer 3 self.abs_max_out: 532.0\n",
      "fc layer 2 self.abs_max_out: 1788.0\n",
      "smallest_now_T updated: 26\n",
      "smallest_now_T updated: 25\n",
      "lif layer 1 self.abs_max_v: 3428.5\n",
      "lif layer 1 self.abs_max_v: 3468.0\n",
      "lif layer 1 self.abs_max_v: 3484.0\n",
      "lif layer 1 self.abs_max_v: 4022.0\n",
      "lif layer 2 self.abs_max_v: 2398.0\n",
      "fc layer 3 self.abs_max_out: 547.0\n",
      "lif layer 1 self.abs_max_v: 4073.0\n",
      "fc layer 1 self.abs_max_out: 3302.0\n",
      "lif layer 1 self.abs_max_v: 4258.5\n",
      "lif layer 1 self.abs_max_v: 4336.0\n",
      "lif layer 1 self.abs_max_v: 4352.0\n",
      "lif layer 1 self.abs_max_v: 4616.0\n",
      "lif layer 1 self.abs_max_v: 4793.0\n",
      "lif layer 2 self.abs_max_v: 2418.5\n",
      "lif layer 2 self.abs_max_v: 2424.5\n",
      "lif layer 1 self.abs_max_v: 5140.0\n",
      "fc layer 2 self.abs_max_out: 1796.0\n",
      "fc layer 2 self.abs_max_out: 1854.0\n",
      "lif layer 2 self.abs_max_v: 2665.0\n",
      "lif layer 2 self.abs_max_v: 2853.5\n",
      "fc layer 3 self.abs_max_out: 552.0\n",
      "fc layer 3 self.abs_max_out: 567.0\n",
      "fc layer 3 self.abs_max_out: 578.0\n",
      "fc layer 3 self.abs_max_out: 591.0\n",
      "fc layer 3 self.abs_max_out: 596.0\n",
      "fc layer 3 self.abs_max_out: 599.0\n",
      "fc layer 3 self.abs_max_out: 614.0\n",
      "smallest_now_T_val updated: 62\n",
      "smallest_now_T_val updated: 51\n",
      "smallest_now_T_val updated: 50\n",
      "smallest_now_T_val updated: 49\n",
      "smallest_now_T_val updated: 40\n",
      "smallest_now_T_val updated: 25\n",
      "lif layer 1 self.abs_max_v: 5182.5\n",
      "lif layer 1 self.abs_max_v: 5192.0\n",
      "lif layer 1 self.abs_max_v: 5344.0\n",
      "fc layer 1 self.abs_max_out: 3316.0\n",
      "lif layer 1 self.abs_max_v: 5747.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.925538/  1.972865, val:  46.25%, val_best:  46.25%, tr:  83.45%, tr_best:  83.45%, epoch time: 46.01 seconds, 0.77 minutes\n",
      "layer   1  Sparsity: 86.4480%\n",
      "layer   2  Sparsity: 73.5638%\n",
      "layer   3  Sparsity: 78.3899%\n",
      "total_backward_count 4895 real_backward_count 1550  31.665%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 2889.0\n",
      "fc layer 2 self.abs_max_out: 1942.0\n",
      "fc layer 2 self.abs_max_out: 2082.0\n",
      "fc layer 2 self.abs_max_out: 2121.0\n",
      "fc layer 2 self.abs_max_out: 2192.0\n",
      "fc layer 2 self.abs_max_out: 2228.0\n",
      "fc layer 1 self.abs_max_out: 3330.0\n",
      "fc layer 3 self.abs_max_out: 660.0\n",
      "fc layer 1 self.abs_max_out: 3403.0\n",
      "lif layer 2 self.abs_max_v: 2946.0\n",
      "fc layer 3 self.abs_max_out: 674.0\n",
      "fc layer 3 self.abs_max_out: 683.0\n",
      "fc layer 1 self.abs_max_out: 3445.0\n",
      "lif layer 1 self.abs_max_v: 5919.5\n",
      "fc layer 3 self.abs_max_out: 757.0\n",
      "fc layer 3 self.abs_max_out: 777.0\n",
      "fc layer 1 self.abs_max_out: 3739.0\n",
      "fc layer 3 self.abs_max_out: 788.0\n",
      "fc layer 3 self.abs_max_out: 794.0\n",
      "fc layer 3 self.abs_max_out: 818.0\n",
      "lif layer 1 self.abs_max_v: 6034.5\n",
      "lif layer 1 self.abs_max_v: 6418.5\n",
      "lif layer 1 self.abs_max_v: 6459.5\n",
      "fc layer 1 self.abs_max_out: 3763.0\n",
      "lif layer 1 self.abs_max_v: 6592.5\n",
      "lif layer 2 self.abs_max_v: 3138.5\n",
      "fc layer 1 self.abs_max_out: 3857.0\n",
      "lif layer 1 self.abs_max_v: 6628.5\n",
      "fc layer 1 self.abs_max_out: 3879.0\n",
      "lif layer 1 self.abs_max_v: 6789.0\n",
      "lif layer 1 self.abs_max_v: 6953.5\n",
      "fc layer 1 self.abs_max_out: 3962.0\n",
      "lif layer 1 self.abs_max_v: 7019.5\n",
      "fc layer 1 self.abs_max_out: 4073.0\n",
      "lif layer 1 self.abs_max_v: 7194.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.830067/  1.954908, val:  45.83%, val_best:  46.25%, tr:  90.91%, tr_best:  90.91%, epoch time: 44.22 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.4562%\n",
      "layer   2  Sparsity: 77.3326%\n",
      "layer   3  Sparsity: 79.3607%\n",
      "total_backward_count 9790 real_backward_count 2677  27.344%\n",
      "fc layer 1 self.abs_max_out: 4124.0\n",
      "fc layer 1 self.abs_max_out: 4145.0\n",
      "fc layer 1 self.abs_max_out: 4538.0\n",
      "fc layer 1 self.abs_max_out: 4581.0\n",
      "lif layer 1 self.abs_max_v: 7440.5\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.814254/  1.921066, val:  48.75%, val_best:  48.75%, tr:  91.62%, tr_best:  91.62%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.4570%\n",
      "layer   2  Sparsity: 78.3058%\n",
      "layer   3  Sparsity: 80.2643%\n",
      "total_backward_count 14685 real_backward_count 3763  25.625%\n",
      "fc layer 1 self.abs_max_out: 4595.0\n",
      "fc layer 3 self.abs_max_out: 836.0\n",
      "lif layer 1 self.abs_max_v: 7568.5\n",
      "lif layer 1 self.abs_max_v: 7629.0\n",
      "fc layer 1 self.abs_max_out: 4598.0\n",
      "lif layer 1 self.abs_max_v: 7792.5\n",
      "lif layer 2 self.abs_max_v: 3337.5\n",
      "lif layer 1 self.abs_max_v: 7874.5\n",
      "lif layer 1 self.abs_max_v: 8169.0\n",
      "lif layer 2 self.abs_max_v: 3523.5\n",
      "fc layer 1 self.abs_max_out: 4622.0\n",
      "fc layer 1 self.abs_max_out: 5033.0\n",
      "fc layer 1 self.abs_max_out: 5245.0\n",
      "lif layer 1 self.abs_max_v: 8482.0\n",
      "lif layer 1 self.abs_max_v: 9281.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.826211/  1.942677, val:  53.75%, val_best:  53.75%, tr:  92.75%, tr_best:  92.75%, epoch time: 43.21 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4359%\n",
      "layer   2  Sparsity: 78.7871%\n",
      "layer   3  Sparsity: 81.4150%\n",
      "total_backward_count 19580 real_backward_count 4788  24.454%\n",
      "lif layer 2 self.abs_max_v: 3560.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.811835/  1.927931, val:  57.50%, val_best:  57.50%, tr:  94.18%, tr_best:  94.18%, epoch time: 42.93 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4672%\n",
      "layer   2  Sparsity: 79.3295%\n",
      "layer   3  Sparsity: 81.6728%\n",
      "total_backward_count 24475 real_backward_count 5731  23.416%\n",
      "fc layer 1 self.abs_max_out: 5479.0\n",
      "fc layer 1 self.abs_max_out: 5577.0\n",
      "lif layer 1 self.abs_max_v: 9445.5\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.813545/  1.939045, val:  51.25%, val_best:  57.50%, tr:  93.77%, tr_best:  94.18%, epoch time: 43.73 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4632%\n",
      "layer   2  Sparsity: 79.8631%\n",
      "layer   3  Sparsity: 82.2189%\n",
      "total_backward_count 29370 real_backward_count 6647  22.632%\n",
      "lif layer 2 self.abs_max_v: 3608.0\n",
      "lif layer 2 self.abs_max_v: 3622.0\n",
      "lif layer 2 self.abs_max_v: 3643.0\n",
      "lif layer 2 self.abs_max_v: 3695.5\n",
      "lif layer 1 self.abs_max_v: 9668.5\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.824499/  1.895853, val:  55.83%, val_best:  57.50%, tr:  94.89%, tr_best:  94.89%, epoch time: 42.60 seconds, 0.71 minutes\n",
      "layer   1  Sparsity: 86.4821%\n",
      "layer   2  Sparsity: 79.7532%\n",
      "layer   3  Sparsity: 82.5680%\n",
      "total_backward_count 34265 real_backward_count 7510  21.917%\n",
      "lif layer 2 self.abs_max_v: 3764.5\n",
      "lif layer 2 self.abs_max_v: 3972.5\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.804431/  1.925949, val:  61.67%, val_best:  61.67%, tr:  96.22%, tr_best:  96.22%, epoch time: 41.18 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 86.4463%\n",
      "layer   2  Sparsity: 79.9104%\n",
      "layer   3  Sparsity: 82.7210%\n",
      "total_backward_count 39160 real_backward_count 8264  21.103%\n",
      "lif layer 2 self.abs_max_v: 4121.0\n",
      "fc layer 1 self.abs_max_out: 5679.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.794513/  1.892800, val:  64.17%, val_best:  64.17%, tr:  96.63%, tr_best:  96.63%, epoch time: 43.28 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4439%\n",
      "layer   2  Sparsity: 80.2259%\n",
      "layer   3  Sparsity: 83.0683%\n",
      "total_backward_count 44055 real_backward_count 9025  20.486%\n",
      "fc layer 2 self.abs_max_out: 2280.0\n",
      "fc layer 1 self.abs_max_out: 5798.0\n",
      "fc layer 2 self.abs_max_out: 2327.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.797737/  1.915851, val:  60.00%, val_best:  64.17%, tr:  96.12%, tr_best:  96.63%, epoch time: 43.39 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4583%\n",
      "layer   2  Sparsity: 79.6235%\n",
      "layer   3  Sparsity: 82.8966%\n",
      "total_backward_count 48950 real_backward_count 9759  19.937%\n",
      "fc layer 2 self.abs_max_out: 2458.0\n",
      "lif layer 2 self.abs_max_v: 4234.5\n",
      "fc layer 1 self.abs_max_out: 6016.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.806647/  1.908324, val:  55.00%, val_best:  64.17%, tr:  97.34%, tr_best:  97.34%, epoch time: 43.44 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4531%\n",
      "layer   2  Sparsity: 78.8594%\n",
      "layer   3  Sparsity: 82.7870%\n",
      "total_backward_count 53845 real_backward_count 10422  19.356%\n",
      "lif layer 1 self.abs_max_v: 10059.0\n",
      "lif layer 1 self.abs_max_v: 10145.0\n",
      "lif layer 1 self.abs_max_v: 10667.5\n",
      "fc layer 1 self.abs_max_out: 6100.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.805826/  1.902294, val:  66.67%, val_best:  66.67%, tr:  96.22%, tr_best:  97.34%, epoch time: 44.12 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.4727%\n",
      "layer   2  Sparsity: 79.6566%\n",
      "layer   3  Sparsity: 82.4715%\n",
      "total_backward_count 58740 real_backward_count 11108  18.910%\n",
      "fc layer 1 self.abs_max_out: 6169.0\n",
      "fc layer 1 self.abs_max_out: 6405.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.789579/  1.880420, val:  70.83%, val_best:  70.83%, tr:  97.96%, tr_best:  97.96%, epoch time: 43.48 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4576%\n",
      "layer   2  Sparsity: 80.1756%\n",
      "layer   3  Sparsity: 83.3103%\n",
      "total_backward_count 63635 real_backward_count 11714  18.408%\n",
      "lif layer 1 self.abs_max_v: 10963.5\n",
      "fc layer 1 self.abs_max_out: 6423.0\n",
      "fc layer 1 self.abs_max_out: 6676.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.788444/  1.876447, val:  67.08%, val_best:  70.83%, tr:  96.94%, tr_best:  97.96%, epoch time: 43.55 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4524%\n",
      "layer   2  Sparsity: 80.5566%\n",
      "layer   3  Sparsity: 83.6442%\n",
      "total_backward_count 68530 real_backward_count 12347  18.017%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.761320/  1.857464, val:  66.25%, val_best:  70.83%, tr:  98.06%, tr_best:  98.06%, epoch time: 42.81 seconds, 0.71 minutes\n",
      "layer   1  Sparsity: 86.4861%\n",
      "layer   2  Sparsity: 80.8951%\n",
      "layer   3  Sparsity: 83.0702%\n",
      "total_backward_count 73425 real_backward_count 12909  17.581%\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.755404/  1.857232, val:  68.75%, val_best:  70.83%, tr:  98.06%, tr_best:  98.06%, epoch time: 43.51 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4689%\n",
      "layer   2  Sparsity: 80.5119%\n",
      "layer   3  Sparsity: 83.4567%\n",
      "total_backward_count 78320 real_backward_count 13488  17.222%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.744045/  1.860340, val:  71.25%, val_best:  71.25%, tr:  98.37%, tr_best:  98.37%, epoch time: 43.48 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4409%\n",
      "layer   2  Sparsity: 80.7080%\n",
      "layer   3  Sparsity: 84.0978%\n",
      "total_backward_count 83215 real_backward_count 14038  16.870%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.743158/  1.831848, val:  72.92%, val_best:  72.92%, tr:  97.65%, tr_best:  98.37%, epoch time: 43.26 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4473%\n",
      "layer   2  Sparsity: 80.7358%\n",
      "layer   3  Sparsity: 83.3167%\n",
      "total_backward_count 88110 real_backward_count 14601  16.571%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.729584/  1.819719, val:  75.42%, val_best:  75.42%, tr:  97.65%, tr_best:  98.37%, epoch time: 43.69 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.5135%\n",
      "layer   2  Sparsity: 80.4928%\n",
      "layer   3  Sparsity: 83.2822%\n",
      "total_backward_count 93005 real_backward_count 15144  16.283%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.717753/  1.829034, val:  68.75%, val_best:  75.42%, tr:  98.06%, tr_best:  98.37%, epoch time: 42.98 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4625%\n",
      "layer   2  Sparsity: 80.3362%\n",
      "layer   3  Sparsity: 83.6246%\n",
      "total_backward_count 97900 real_backward_count 15628  15.963%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.707937/  1.823064, val:  77.08%, val_best:  77.08%, tr:  99.08%, tr_best:  99.08%, epoch time: 43.89 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4592%\n",
      "layer   2  Sparsity: 80.3084%\n",
      "layer   3  Sparsity: 83.3598%\n",
      "total_backward_count 102795 real_backward_count 16071  15.634%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.714962/  1.817761, val:  76.25%, val_best:  77.08%, tr:  98.67%, tr_best:  99.08%, epoch time: 42.98 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4508%\n",
      "layer   2  Sparsity: 79.8727%\n",
      "layer   3  Sparsity: 83.4473%\n",
      "total_backward_count 107690 real_backward_count 16557  15.375%\n",
      "fc layer 1 self.abs_max_out: 6827.0\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.712276/  1.809257, val:  71.67%, val_best:  77.08%, tr:  98.47%, tr_best:  99.08%, epoch time: 43.70 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4362%\n",
      "layer   2  Sparsity: 79.6971%\n",
      "layer   3  Sparsity: 83.4140%\n",
      "total_backward_count 112585 real_backward_count 16988  15.089%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.706868/  1.801691, val:  79.58%, val_best:  79.58%, tr:  98.88%, tr_best:  99.08%, epoch time: 43.90 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4654%\n",
      "layer   2  Sparsity: 79.3243%\n",
      "layer   3  Sparsity: 83.6504%\n",
      "total_backward_count 117480 real_backward_count 17470  14.871%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.689496/  1.810925, val:  80.83%, val_best:  80.83%, tr:  99.28%, tr_best:  99.28%, epoch time: 43.25 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4649%\n",
      "layer   2  Sparsity: 79.3978%\n",
      "layer   3  Sparsity: 83.6598%\n",
      "total_backward_count 122375 real_backward_count 17890  14.619%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.685065/  1.772184, val:  83.75%, val_best:  83.75%, tr:  99.28%, tr_best:  99.28%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.4370%\n",
      "layer   2  Sparsity: 79.3742%\n",
      "layer   3  Sparsity: 83.4661%\n",
      "total_backward_count 127270 real_backward_count 18281  14.364%\n",
      "fc layer 1 self.abs_max_out: 6869.0\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.678699/  1.771574, val:  83.75%, val_best:  83.75%, tr:  99.08%, tr_best:  99.28%, epoch time: 43.37 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4924%\n",
      "layer   2  Sparsity: 79.1101%\n",
      "layer   3  Sparsity: 83.5667%\n",
      "total_backward_count 132165 real_backward_count 18683  14.136%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.664876/  1.773740, val:  80.42%, val_best:  83.75%, tr:  99.49%, tr_best:  99.49%, epoch time: 43.55 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4650%\n",
      "layer   2  Sparsity: 78.9938%\n",
      "layer   3  Sparsity: 83.8007%\n",
      "total_backward_count 137060 real_backward_count 19040  13.892%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.666016/  1.770345, val:  84.58%, val_best:  84.58%, tr:  99.18%, tr_best:  99.49%, epoch time: 43.11 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4438%\n",
      "layer   2  Sparsity: 79.0395%\n",
      "layer   3  Sparsity: 83.4809%\n",
      "total_backward_count 141955 real_backward_count 19419  13.680%\n",
      "lif layer 1 self.abs_max_v: 10991.0\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.650328/  1.748006, val:  77.08%, val_best:  84.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 44.21 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.4497%\n",
      "layer   2  Sparsity: 78.9698%\n",
      "layer   3  Sparsity: 82.7926%\n",
      "total_backward_count 146850 real_backward_count 19750  13.449%\n",
      "lif layer 1 self.abs_max_v: 11083.0\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.637963/  1.769297, val:  83.75%, val_best:  84.58%, tr:  98.88%, tr_best:  99.49%, epoch time: 43.40 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4591%\n",
      "layer   2  Sparsity: 78.6253%\n",
      "layer   3  Sparsity: 82.8218%\n",
      "total_backward_count 151745 real_backward_count 20051  13.214%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.643654/  1.761668, val:  81.25%, val_best:  84.58%, tr:  98.98%, tr_best:  99.49%, epoch time: 44.06 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4421%\n",
      "layer   2  Sparsity: 78.9018%\n",
      "layer   3  Sparsity: 83.6980%\n",
      "total_backward_count 156640 real_backward_count 20379  13.010%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.634075/  1.757581, val:  77.08%, val_best:  84.58%, tr:  98.88%, tr_best:  99.49%, epoch time: 43.21 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4727%\n",
      "layer   2  Sparsity: 79.1550%\n",
      "layer   3  Sparsity: 83.9220%\n",
      "total_backward_count 161535 real_backward_count 20684  12.805%\n",
      "lif layer 1 self.abs_max_v: 11252.5\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.630897/  1.742663, val:  82.08%, val_best:  84.58%, tr:  99.08%, tr_best:  99.49%, epoch time: 43.72 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4679%\n",
      "layer   2  Sparsity: 79.3395%\n",
      "layer   3  Sparsity: 84.3537%\n",
      "total_backward_count 166430 real_backward_count 20955  12.591%\n",
      "fc layer 1 self.abs_max_out: 6894.0\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.634146/  1.761143, val:  76.25%, val_best:  84.58%, tr:  99.18%, tr_best:  99.49%, epoch time: 43.17 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4595%\n",
      "layer   2  Sparsity: 79.3329%\n",
      "layer   3  Sparsity: 84.2678%\n",
      "total_backward_count 171325 real_backward_count 21259  12.409%\n",
      "lif layer 1 self.abs_max_v: 11455.5\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.630166/  1.741586, val:  80.00%, val_best:  84.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 43.72 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4246%\n",
      "layer   2  Sparsity: 79.0195%\n",
      "layer   3  Sparsity: 84.1730%\n",
      "total_backward_count 176220 real_backward_count 21531  12.218%\n",
      "fc layer 1 self.abs_max_out: 6901.0\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.633360/  1.754338, val:  80.00%, val_best:  84.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 43.57 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.5150%\n",
      "layer   2  Sparsity: 78.9474%\n",
      "layer   3  Sparsity: 84.3301%\n",
      "total_backward_count 181115 real_backward_count 21796  12.034%\n",
      "fc layer 3 self.abs_max_out: 860.0\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.617714/  1.740675, val:  82.08%, val_best:  84.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 43.98 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4230%\n",
      "layer   2  Sparsity: 79.0797%\n",
      "layer   3  Sparsity: 83.9879%\n",
      "total_backward_count 186010 real_backward_count 22075  11.868%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.618349/  1.733694, val:  76.25%, val_best:  84.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 43.86 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4405%\n",
      "layer   2  Sparsity: 78.6454%\n",
      "layer   3  Sparsity: 83.8475%\n",
      "total_backward_count 190905 real_backward_count 22370  11.718%\n",
      "fc layer 2 self.abs_max_out: 2575.0\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.608974/  1.726130, val:  81.25%, val_best:  84.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 43.53 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4442%\n",
      "layer   2  Sparsity: 78.8664%\n",
      "layer   3  Sparsity: 83.5955%\n",
      "total_backward_count 195800 real_backward_count 22635  11.560%\n",
      "fc layer 2 self.abs_max_out: 2628.0\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.599900/  1.718782, val:  75.83%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 43.42 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4686%\n",
      "layer   2  Sparsity: 79.0834%\n",
      "layer   3  Sparsity: 83.7541%\n",
      "total_backward_count 200695 real_backward_count 22854  11.387%\n",
      "fc layer 3 self.abs_max_out: 867.0\n",
      "fc layer 3 self.abs_max_out: 885.0\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.581391/  1.694745, val:  82.92%, val_best:  84.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 43.62 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4515%\n",
      "layer   2  Sparsity: 79.3562%\n",
      "layer   3  Sparsity: 82.8639%\n",
      "total_backward_count 205590 real_backward_count 23085  11.229%\n",
      "fc layer 1 self.abs_max_out: 7089.0\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.573901/  1.713490, val:  79.58%, val_best:  84.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 43.79 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4895%\n",
      "layer   2  Sparsity: 79.0587%\n",
      "layer   3  Sparsity: 83.2566%\n",
      "total_backward_count 210485 real_backward_count 23301  11.070%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.588061/  1.713781, val:  82.08%, val_best:  84.58%, tr:  99.28%, tr_best:  99.80%, epoch time: 43.42 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4580%\n",
      "layer   2  Sparsity: 79.0358%\n",
      "layer   3  Sparsity: 83.5819%\n",
      "total_backward_count 215380 real_backward_count 23552  10.935%\n",
      "lif layer 1 self.abs_max_v: 11562.5\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.578631/  1.699563, val:  80.42%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 43.39 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4567%\n",
      "layer   2  Sparsity: 79.4846%\n",
      "layer   3  Sparsity: 83.3863%\n",
      "total_backward_count 220275 real_backward_count 23786  10.798%\n",
      "lif layer 1 self.abs_max_v: 11569.0\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.575984/  1.692078, val:  81.67%, val_best:  84.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 43.84 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4908%\n",
      "layer   2  Sparsity: 79.4558%\n",
      "layer   3  Sparsity: 82.6861%\n",
      "total_backward_count 225170 real_backward_count 24060  10.685%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.560852/  1.692160, val:  77.92%, val_best:  84.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 43.56 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4720%\n",
      "layer   2  Sparsity: 79.6118%\n",
      "layer   3  Sparsity: 82.9724%\n",
      "total_backward_count 230065 real_backward_count 24253  10.542%\n",
      "lif layer 1 self.abs_max_v: 12130.5\n",
      "fc layer 1 self.abs_max_out: 7223.0\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.563533/  1.687890, val:  82.50%, val_best:  84.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 43.36 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4466%\n",
      "layer   2  Sparsity: 79.3794%\n",
      "layer   3  Sparsity: 83.3964%\n",
      "total_backward_count 234960 real_backward_count 24449  10.406%\n",
      "fc layer 1 self.abs_max_out: 7384.0\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.554877/  1.692068, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 43.33 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4739%\n",
      "layer   2  Sparsity: 79.4566%\n",
      "layer   3  Sparsity: 83.5998%\n",
      "total_backward_count 239855 real_backward_count 24637  10.272%\n",
      "fc layer 3 self.abs_max_out: 895.0\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.557938/  1.690317, val:  82.50%, val_best:  84.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 43.82 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4494%\n",
      "layer   2  Sparsity: 79.7615%\n",
      "layer   3  Sparsity: 83.3132%\n",
      "total_backward_count 244750 real_backward_count 24847  10.152%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.551632/  1.682158, val:  83.75%, val_best:  84.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 43.90 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4551%\n",
      "layer   2  Sparsity: 79.6165%\n",
      "layer   3  Sparsity: 83.5005%\n",
      "total_backward_count 249645 real_backward_count 25034  10.028%\n",
      "fc layer 3 self.abs_max_out: 900.0\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.558931/  1.684196, val:  81.67%, val_best:  84.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 43.39 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4522%\n",
      "layer   2  Sparsity: 79.4425%\n",
      "layer   3  Sparsity: 83.5014%\n",
      "total_backward_count 254540 real_backward_count 25219   9.908%\n",
      "fc layer 3 self.abs_max_out: 917.0\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.541827/  1.673048, val:  81.25%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.58 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 86.4893%\n",
      "layer   2  Sparsity: 79.5820%\n",
      "layer   3  Sparsity: 83.2480%\n",
      "total_backward_count 259435 real_backward_count 25458   9.813%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.543299/  1.667349, val:  80.00%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 43.48 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4602%\n",
      "layer   2  Sparsity: 79.4949%\n",
      "layer   3  Sparsity: 84.0867%\n",
      "total_backward_count 264330 real_backward_count 25632   9.697%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.552425/  1.691463, val:  81.25%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 43.84 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4536%\n",
      "layer   2  Sparsity: 79.2657%\n",
      "layer   3  Sparsity: 84.2456%\n",
      "total_backward_count 269225 real_backward_count 25802   9.584%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.565307/  1.669380, val:  84.17%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 43.26 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4542%\n",
      "layer   2  Sparsity: 79.0071%\n",
      "layer   3  Sparsity: 84.3436%\n",
      "total_backward_count 274120 real_backward_count 25961   9.471%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.548598/  1.680011, val:  80.42%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 43.54 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4583%\n",
      "layer   2  Sparsity: 78.9842%\n",
      "layer   3  Sparsity: 84.3057%\n",
      "total_backward_count 279015 real_backward_count 26106   9.356%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.539443/  1.674811, val:  84.17%, val_best:  84.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 43.52 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4807%\n",
      "layer   2  Sparsity: 79.1597%\n",
      "layer   3  Sparsity: 84.2403%\n",
      "total_backward_count 283910 real_backward_count 26249   9.246%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.548199/  1.675104, val:  85.00%, val_best:  85.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 43.41 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4607%\n",
      "layer   2  Sparsity: 79.2642%\n",
      "layer   3  Sparsity: 84.2911%\n",
      "total_backward_count 288805 real_backward_count 26403   9.142%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.548093/  1.667798, val:  82.92%, val_best:  85.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 43.59 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4408%\n",
      "layer   2  Sparsity: 78.9273%\n",
      "layer   3  Sparsity: 84.1966%\n",
      "total_backward_count 293700 real_backward_count 26581   9.050%\n",
      "fc layer 3 self.abs_max_out: 947.0\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.538042/  1.660583, val:  82.08%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 43.58 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4987%\n",
      "layer   2  Sparsity: 79.3815%\n",
      "layer   3  Sparsity: 84.2639%\n",
      "total_backward_count 298595 real_backward_count 26743   8.956%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.536137/  1.651052, val:  84.58%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 43.37 seconds, 0.72 minutes\n",
      "layer   1  Sparsity: 86.4394%\n",
      "layer   2  Sparsity: 79.2413%\n",
      "layer   3  Sparsity: 83.9690%\n",
      "total_backward_count 303490 real_backward_count 26895   8.862%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.535828/  1.667074, val:  84.58%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.4692%\n",
      "layer   2  Sparsity: 79.5203%\n",
      "layer   3  Sparsity: 84.3977%\n",
      "total_backward_count 308385 real_backward_count 27040   8.768%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.536913/  1.660593, val:  83.75%, val_best:  85.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 43.79 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4449%\n",
      "layer   2  Sparsity: 79.6864%\n",
      "layer   3  Sparsity: 84.5724%\n",
      "total_backward_count 313280 real_backward_count 27197   8.681%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.535584/  1.676560, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 43.81 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4697%\n",
      "layer   2  Sparsity: 79.5987%\n",
      "layer   3  Sparsity: 84.9299%\n",
      "total_backward_count 318175 real_backward_count 27313   8.584%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.538206/  1.671443, val:  82.92%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 43.59 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4532%\n",
      "layer   2  Sparsity: 79.6825%\n",
      "layer   3  Sparsity: 85.0531%\n",
      "total_backward_count 323070 real_backward_count 27460   8.500%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.537494/  1.658356, val:  84.58%, val_best:  85.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.25 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.4574%\n",
      "layer   2  Sparsity: 79.5915%\n",
      "layer   3  Sparsity: 84.6936%\n",
      "total_backward_count 327965 real_backward_count 27589   8.412%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.541014/  1.668985, val:  81.25%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 38.03 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 86.5047%\n",
      "layer   2  Sparsity: 79.9640%\n",
      "layer   3  Sparsity: 84.7884%\n",
      "total_backward_count 332860 real_backward_count 27730   8.331%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.523753/  1.655147, val:  83.75%, val_best:  85.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 38.11 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 86.4578%\n",
      "layer   2  Sparsity: 79.7945%\n",
      "layer   3  Sparsity: 84.5865%\n",
      "total_backward_count 337755 real_backward_count 27855   8.247%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.526274/  1.660223, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.18 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4542%\n",
      "layer   2  Sparsity: 79.5015%\n",
      "layer   3  Sparsity: 84.5308%\n",
      "total_backward_count 342650 real_backward_count 27972   8.163%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.525901/  1.657213, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.16 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4689%\n",
      "layer   2  Sparsity: 79.4944%\n",
      "layer   3  Sparsity: 84.6172%\n",
      "total_backward_count 347545 real_backward_count 28103   8.086%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.517133/  1.650991, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.18 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4391%\n",
      "layer   2  Sparsity: 79.4156%\n",
      "layer   3  Sparsity: 84.5689%\n",
      "total_backward_count 352440 real_backward_count 28202   8.002%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.516850/  1.653881, val:  82.08%, val_best:  85.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 38.96 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4705%\n",
      "layer   2  Sparsity: 79.5509%\n",
      "layer   3  Sparsity: 84.5450%\n",
      "total_backward_count 357335 real_backward_count 28307   7.922%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.513215/  1.652397, val:  80.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.18 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4420%\n",
      "layer   2  Sparsity: 79.5693%\n",
      "layer   3  Sparsity: 84.7294%\n",
      "total_backward_count 362230 real_backward_count 28407   7.842%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.515997/  1.648790, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 38.76 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4661%\n",
      "layer   2  Sparsity: 79.4920%\n",
      "layer   3  Sparsity: 84.5402%\n",
      "total_backward_count 367125 real_backward_count 28521   7.769%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.514405/  1.648792, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.29 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4504%\n",
      "layer   2  Sparsity: 79.5943%\n",
      "layer   3  Sparsity: 84.8313%\n",
      "total_backward_count 372020 real_backward_count 28621   7.693%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.520217/  1.663845, val:  80.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4418%\n",
      "layer   2  Sparsity: 79.4312%\n",
      "layer   3  Sparsity: 84.7956%\n",
      "total_backward_count 376915 real_backward_count 28738   7.625%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.518624/  1.655823, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 38.99 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4314%\n",
      "layer   2  Sparsity: 79.6760%\n",
      "layer   3  Sparsity: 84.8085%\n",
      "total_backward_count 381810 real_backward_count 28835   7.552%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.499249/  1.654119, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.67 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 86.4972%\n",
      "layer   2  Sparsity: 79.5650%\n",
      "layer   3  Sparsity: 84.5035%\n",
      "total_backward_count 386705 real_backward_count 28941   7.484%\n",
      "fc layer 3 self.abs_max_out: 960.0\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.493230/  1.645283, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 86.4768%\n",
      "layer   2  Sparsity: 79.6029%\n",
      "layer   3  Sparsity: 84.7462%\n",
      "total_backward_count 391600 real_backward_count 29044   7.417%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.486821/  1.642187, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4587%\n",
      "layer   2  Sparsity: 79.5657%\n",
      "layer   3  Sparsity: 84.5658%\n",
      "total_backward_count 396495 real_backward_count 29148   7.351%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.496748/  1.642516, val:  80.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4732%\n",
      "layer   2  Sparsity: 79.7132%\n",
      "layer   3  Sparsity: 84.5881%\n",
      "total_backward_count 401390 real_backward_count 29228   7.282%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.493001/  1.652227, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4906%\n",
      "layer   2  Sparsity: 79.5980%\n",
      "layer   3  Sparsity: 84.6870%\n",
      "total_backward_count 406285 real_backward_count 29310   7.214%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.497879/  1.655091, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.35 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4744%\n",
      "layer   2  Sparsity: 79.9725%\n",
      "layer   3  Sparsity: 84.5997%\n",
      "total_backward_count 411180 real_backward_count 29390   7.148%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.490854/  1.637546, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4414%\n",
      "layer   2  Sparsity: 80.2045%\n",
      "layer   3  Sparsity: 84.5687%\n",
      "total_backward_count 416075 real_backward_count 29527   7.097%\n",
      "fc layer 3 self.abs_max_out: 962.0\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.469687/  1.624722, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.43 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4938%\n",
      "layer   2  Sparsity: 80.2635%\n",
      "layer   3  Sparsity: 84.3170%\n",
      "total_backward_count 420970 real_backward_count 29611   7.034%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.476308/  1.622523, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.38 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4863%\n",
      "layer   2  Sparsity: 79.9857%\n",
      "layer   3  Sparsity: 84.4587%\n",
      "total_backward_count 425865 real_backward_count 29715   6.978%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.469832/  1.628924, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.26 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4456%\n",
      "layer   2  Sparsity: 80.0462%\n",
      "layer   3  Sparsity: 84.2307%\n",
      "total_backward_count 430760 real_backward_count 29820   6.923%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.466776/  1.621089, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.44 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4283%\n",
      "layer   2  Sparsity: 80.1196%\n",
      "layer   3  Sparsity: 84.5382%\n",
      "total_backward_count 435655 real_backward_count 29900   6.863%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.468532/  1.617316, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.31 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4761%\n",
      "layer   2  Sparsity: 79.9335%\n",
      "layer   3  Sparsity: 84.4610%\n",
      "total_backward_count 440550 real_backward_count 29964   6.801%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.467054/  1.620001, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.50 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4787%\n",
      "layer   2  Sparsity: 80.2377%\n",
      "layer   3  Sparsity: 84.7386%\n",
      "total_backward_count 445445 real_backward_count 30037   6.743%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.461021/  1.621775, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.10 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4350%\n",
      "layer   2  Sparsity: 80.3609%\n",
      "layer   3  Sparsity: 84.9266%\n",
      "total_backward_count 450340 real_backward_count 30101   6.684%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.471088/  1.629945, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.98 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4651%\n",
      "layer   2  Sparsity: 80.4039%\n",
      "layer   3  Sparsity: 85.0614%\n",
      "total_backward_count 455235 real_backward_count 30197   6.633%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.481009/  1.641404, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4363%\n",
      "layer   2  Sparsity: 80.3887%\n",
      "layer   3  Sparsity: 85.1925%\n",
      "total_backward_count 460130 real_backward_count 30303   6.586%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.470711/  1.631824, val:  83.33%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.24 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4975%\n",
      "layer   2  Sparsity: 80.3735%\n",
      "layer   3  Sparsity: 85.2083%\n",
      "total_backward_count 465025 real_backward_count 30391   6.535%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.470991/  1.622589, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.04 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4165%\n",
      "layer   2  Sparsity: 80.3798%\n",
      "layer   3  Sparsity: 84.9431%\n",
      "total_backward_count 469920 real_backward_count 30471   6.484%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.468102/  1.620710, val:  78.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4698%\n",
      "layer   2  Sparsity: 80.3648%\n",
      "layer   3  Sparsity: 84.6856%\n",
      "total_backward_count 474815 real_backward_count 30555   6.435%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.461556/  1.618258, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.20 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4833%\n",
      "layer   2  Sparsity: 80.4202%\n",
      "layer   3  Sparsity: 84.5953%\n",
      "total_backward_count 479710 real_backward_count 30629   6.385%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.464681/  1.617443, val:  83.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4620%\n",
      "layer   2  Sparsity: 80.4076%\n",
      "layer   3  Sparsity: 84.6001%\n",
      "total_backward_count 484605 real_backward_count 30708   6.337%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.459085/  1.616818, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4386%\n",
      "layer   2  Sparsity: 80.4494%\n",
      "layer   3  Sparsity: 84.7307%\n",
      "total_backward_count 489500 real_backward_count 30780   6.288%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.449817/  1.601917, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.51 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4394%\n",
      "layer   2  Sparsity: 80.3806%\n",
      "layer   3  Sparsity: 84.6518%\n",
      "total_backward_count 494395 real_backward_count 30828   6.235%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.448748/  1.606064, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.17 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4824%\n",
      "layer   2  Sparsity: 80.2793%\n",
      "layer   3  Sparsity: 84.3954%\n",
      "total_backward_count 499290 real_backward_count 30919   6.193%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.455259/  1.610337, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.87 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4155%\n",
      "layer   2  Sparsity: 80.3555%\n",
      "layer   3  Sparsity: 84.2283%\n",
      "total_backward_count 504185 real_backward_count 30984   6.145%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.451495/  1.616234, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.42 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4258%\n",
      "layer   2  Sparsity: 80.6030%\n",
      "layer   3  Sparsity: 84.5058%\n",
      "total_backward_count 509080 real_backward_count 31044   6.098%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.448811/  1.614403, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.75 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4533%\n",
      "layer   2  Sparsity: 80.6660%\n",
      "layer   3  Sparsity: 84.6771%\n",
      "total_backward_count 513975 real_backward_count 31096   6.050%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.446252/  1.610735, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4554%\n",
      "layer   2  Sparsity: 80.7879%\n",
      "layer   3  Sparsity: 84.7209%\n",
      "total_backward_count 518870 real_backward_count 31150   6.003%\n",
      "lif layer 1 self.abs_max_v: 12314.0\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.443210/  1.603958, val:  83.33%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.61 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4754%\n",
      "layer   2  Sparsity: 80.8829%\n",
      "layer   3  Sparsity: 84.7372%\n",
      "total_backward_count 523765 real_backward_count 31203   5.957%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.442513/  1.600840, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4532%\n",
      "layer   2  Sparsity: 80.9857%\n",
      "layer   3  Sparsity: 84.7393%\n",
      "total_backward_count 528660 real_backward_count 31260   5.913%\n",
      "fc layer 3 self.abs_max_out: 969.0\n",
      "fc layer 3 self.abs_max_out: 978.0\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.444711/  1.610275, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4675%\n",
      "layer   2  Sparsity: 81.0177%\n",
      "layer   3  Sparsity: 84.9717%\n",
      "total_backward_count 533555 real_backward_count 31303   5.867%\n",
      "fc layer 1 self.abs_max_out: 7455.0\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.440696/  1.608993, val:  85.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4730%\n",
      "layer   2  Sparsity: 80.9081%\n",
      "layer   3  Sparsity: 84.9441%\n",
      "total_backward_count 538450 real_backward_count 31361   5.824%\n",
      "lif layer 1 self.abs_max_v: 12434.0\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.433377/  1.599590, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.41 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4891%\n",
      "layer   2  Sparsity: 80.8700%\n",
      "layer   3  Sparsity: 84.6975%\n",
      "total_backward_count 543345 real_backward_count 31414   5.782%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.433669/  1.602438, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4520%\n",
      "layer   2  Sparsity: 80.7619%\n",
      "layer   3  Sparsity: 84.6485%\n",
      "total_backward_count 548240 real_backward_count 31457   5.738%\n",
      "lif layer 1 self.abs_max_v: 12699.0\n",
      "lif layer 1 self.abs_max_v: 12732.5\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.436662/  1.619905, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.38 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4665%\n",
      "layer   2  Sparsity: 80.7896%\n",
      "layer   3  Sparsity: 84.9395%\n",
      "total_backward_count 553135 real_backward_count 31511   5.697%\n",
      "lif layer 1 self.abs_max_v: 12801.5\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.440116/  1.610728, val:  83.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.60 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4291%\n",
      "layer   2  Sparsity: 80.8811%\n",
      "layer   3  Sparsity: 84.8893%\n",
      "total_backward_count 558030 real_backward_count 31558   5.655%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.440214/  1.606235, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4796%\n",
      "layer   2  Sparsity: 80.5865%\n",
      "layer   3  Sparsity: 84.5914%\n",
      "total_backward_count 562925 real_backward_count 31610   5.615%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.438601/  1.595901, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.65 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4779%\n",
      "layer   2  Sparsity: 80.3321%\n",
      "layer   3  Sparsity: 84.3553%\n",
      "total_backward_count 567820 real_backward_count 31688   5.581%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.431861/  1.604298, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.54 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4632%\n",
      "layer   2  Sparsity: 80.5779%\n",
      "layer   3  Sparsity: 84.5106%\n",
      "total_backward_count 572715 real_backward_count 31737   5.541%\n",
      "lif layer 1 self.abs_max_v: 13050.0\n",
      "lif layer 1 self.abs_max_v: 13166.5\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.429957/  1.597621, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.59 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4544%\n",
      "layer   2  Sparsity: 80.7529%\n",
      "layer   3  Sparsity: 84.6651%\n",
      "total_backward_count 577610 real_backward_count 31797   5.505%\n",
      "fc layer 1 self.abs_max_out: 7583.0\n",
      "fc layer 1 self.abs_max_out: 7626.0\n",
      "lif layer 1 self.abs_max_v: 13212.0\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.424806/  1.612651, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.10 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4447%\n",
      "layer   2  Sparsity: 80.9249%\n",
      "layer   3  Sparsity: 84.7447%\n",
      "total_backward_count 582505 real_backward_count 31855   5.469%\n",
      "fc layer 1 self.abs_max_out: 7685.0\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.431860/  1.616029, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4238%\n",
      "layer   2  Sparsity: 80.7358%\n",
      "layer   3  Sparsity: 84.8167%\n",
      "total_backward_count 587400 real_backward_count 31932   5.436%\n",
      "fc layer 1 self.abs_max_out: 7832.0\n",
      "lif layer 1 self.abs_max_v: 13255.5\n",
      "fc layer 1 self.abs_max_out: 7860.0\n",
      "lif layer 1 self.abs_max_v: 13704.0\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.440363/  1.619199, val:  83.33%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.47 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4485%\n",
      "layer   2  Sparsity: 80.5805%\n",
      "layer   3  Sparsity: 84.9726%\n",
      "total_backward_count 592295 real_backward_count 31981   5.400%\n",
      "lif layer 1 self.abs_max_v: 13819.5\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.448072/  1.618727, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.68 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4893%\n",
      "layer   2  Sparsity: 80.5711%\n",
      "layer   3  Sparsity: 85.0494%\n",
      "total_backward_count 597190 real_backward_count 32044   5.366%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.437134/  1.614718, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.5003%\n",
      "layer   2  Sparsity: 80.6330%\n",
      "layer   3  Sparsity: 84.6902%\n",
      "total_backward_count 602085 real_backward_count 32091   5.330%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.433213/  1.603401, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4710%\n",
      "layer   2  Sparsity: 80.6762%\n",
      "layer   3  Sparsity: 84.7825%\n",
      "total_backward_count 606980 real_backward_count 32134   5.294%\n",
      "fc layer 1 self.abs_max_out: 7895.0\n",
      "fc layer 1 self.abs_max_out: 7919.0\n",
      "lif layer 1 self.abs_max_v: 13829.0\n",
      "lif layer 1 self.abs_max_v: 13986.5\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.428915/  1.614000, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.44 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4825%\n",
      "layer   2  Sparsity: 80.7737%\n",
      "layer   3  Sparsity: 85.0281%\n",
      "total_backward_count 611875 real_backward_count 32168   5.257%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.431146/  1.607732, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.58 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4157%\n",
      "layer   2  Sparsity: 80.8662%\n",
      "layer   3  Sparsity: 85.2140%\n",
      "total_backward_count 616770 real_backward_count 32207   5.222%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.426190/  1.603349, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.50 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4553%\n",
      "layer   2  Sparsity: 80.9184%\n",
      "layer   3  Sparsity: 85.2425%\n",
      "total_backward_count 621665 real_backward_count 32271   5.191%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.422653/  1.607445, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4390%\n",
      "layer   2  Sparsity: 80.9264%\n",
      "layer   3  Sparsity: 85.1103%\n",
      "total_backward_count 626560 real_backward_count 32320   5.158%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.424069/  1.607435, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4972%\n",
      "layer   2  Sparsity: 80.7674%\n",
      "layer   3  Sparsity: 85.2019%\n",
      "total_backward_count 631455 real_backward_count 32364   5.125%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.424255/  1.603661, val:  83.33%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.37 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4565%\n",
      "layer   2  Sparsity: 80.5340%\n",
      "layer   3  Sparsity: 85.3710%\n",
      "total_backward_count 636350 real_backward_count 32415   5.094%\n",
      "fc layer 1 self.abs_max_out: 8012.0\n",
      "lif layer 1 self.abs_max_v: 13998.5\n",
      "lif layer 1 self.abs_max_v: 14243.0\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.428363/  1.612489, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.65 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4820%\n",
      "layer   2  Sparsity: 80.5657%\n",
      "layer   3  Sparsity: 85.2302%\n",
      "total_backward_count 641245 real_backward_count 32451   5.061%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.434804/  1.610765, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.38 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.5022%\n",
      "layer   2  Sparsity: 80.6860%\n",
      "layer   3  Sparsity: 85.1437%\n",
      "total_backward_count 646140 real_backward_count 32474   5.026%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.435708/  1.607794, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.58 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4551%\n",
      "layer   2  Sparsity: 80.7180%\n",
      "layer   3  Sparsity: 84.9276%\n",
      "total_backward_count 651035 real_backward_count 32512   4.994%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.433114/  1.601881, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.38 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4541%\n",
      "layer   2  Sparsity: 80.5575%\n",
      "layer   3  Sparsity: 85.0169%\n",
      "total_backward_count 655930 real_backward_count 32569   4.965%\n",
      "fc layer 3 self.abs_max_out: 989.0\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.425548/  1.603293, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4735%\n",
      "layer   2  Sparsity: 80.6794%\n",
      "layer   3  Sparsity: 84.8913%\n",
      "total_backward_count 660825 real_backward_count 32594   4.932%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.427134/  1.610687, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4582%\n",
      "layer   2  Sparsity: 80.6921%\n",
      "layer   3  Sparsity: 85.0760%\n",
      "total_backward_count 665720 real_backward_count 32623   4.900%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.430312/  1.606300, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4538%\n",
      "layer   2  Sparsity: 80.7744%\n",
      "layer   3  Sparsity: 85.0944%\n",
      "total_backward_count 670615 real_backward_count 32657   4.870%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.425178/  1.604083, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.43 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4664%\n",
      "layer   2  Sparsity: 80.7924%\n",
      "layer   3  Sparsity: 84.9737%\n",
      "total_backward_count 675510 real_backward_count 32688   4.839%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.423529/  1.603542, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.79 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4521%\n",
      "layer   2  Sparsity: 80.6941%\n",
      "layer   3  Sparsity: 84.8712%\n",
      "total_backward_count 680405 real_backward_count 32741   4.812%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.422169/  1.605624, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4702%\n",
      "layer   2  Sparsity: 80.7494%\n",
      "layer   3  Sparsity: 85.0141%\n",
      "total_backward_count 685300 real_backward_count 32781   4.783%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.420813/  1.601011, val:  83.75%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.60 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4747%\n",
      "layer   2  Sparsity: 80.8817%\n",
      "layer   3  Sparsity: 85.1267%\n",
      "total_backward_count 690195 real_backward_count 32808   4.753%\n",
      "fc layer 3 self.abs_max_out: 990.0\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.422032/  1.602398, val:  82.92%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.82 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4918%\n",
      "layer   2  Sparsity: 80.8770%\n",
      "layer   3  Sparsity: 85.1454%\n",
      "total_backward_count 695090 real_backward_count 32850   4.726%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.420801/  1.611576, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.33 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.5035%\n",
      "layer   2  Sparsity: 80.9162%\n",
      "layer   3  Sparsity: 85.1344%\n",
      "total_backward_count 699985 real_backward_count 32874   4.696%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.415165/  1.606957, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.01 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4477%\n",
      "layer   2  Sparsity: 80.9529%\n",
      "layer   3  Sparsity: 85.3323%\n",
      "total_backward_count 704880 real_backward_count 32907   4.668%\n",
      "fc layer 3 self.abs_max_out: 996.0\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.412662/  1.604923, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.63 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4655%\n",
      "layer   2  Sparsity: 80.9975%\n",
      "layer   3  Sparsity: 85.4726%\n",
      "total_backward_count 709775 real_backward_count 32936   4.640%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.420539/  1.601989, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.22 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4578%\n",
      "layer   2  Sparsity: 81.0239%\n",
      "layer   3  Sparsity: 85.4996%\n",
      "total_backward_count 714670 real_backward_count 32962   4.612%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.423153/  1.605132, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.50 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4645%\n",
      "layer   2  Sparsity: 81.0002%\n",
      "layer   3  Sparsity: 85.4286%\n",
      "total_backward_count 719565 real_backward_count 32995   4.585%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.417253/  1.603127, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.06 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4795%\n",
      "layer   2  Sparsity: 80.9192%\n",
      "layer   3  Sparsity: 85.4011%\n",
      "total_backward_count 724460 real_backward_count 33026   4.559%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.419476/  1.597848, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.48 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4584%\n",
      "layer   2  Sparsity: 80.7713%\n",
      "layer   3  Sparsity: 85.2037%\n",
      "total_backward_count 729355 real_backward_count 33054   4.532%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.418253/  1.616125, val:  80.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.54 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4460%\n",
      "layer   2  Sparsity: 80.7447%\n",
      "layer   3  Sparsity: 85.2505%\n",
      "total_backward_count 734250 real_backward_count 33090   4.507%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.409893/  1.584740, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4725%\n",
      "layer   2  Sparsity: 80.7244%\n",
      "layer   3  Sparsity: 85.1938%\n",
      "total_backward_count 739145 real_backward_count 33137   4.483%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.403574/  1.592451, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 38.53 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 86.4901%\n",
      "layer   2  Sparsity: 80.6373%\n",
      "layer   3  Sparsity: 84.8665%\n",
      "total_backward_count 744040 real_backward_count 33164   4.457%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.403952/  1.584501, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 86.4635%\n",
      "layer   2  Sparsity: 80.5231%\n",
      "layer   3  Sparsity: 84.9219%\n",
      "total_backward_count 748935 real_backward_count 33191   4.432%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.406695/  1.590626, val:  84.58%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.39 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4909%\n",
      "layer   2  Sparsity: 80.6637%\n",
      "layer   3  Sparsity: 84.9669%\n",
      "total_backward_count 753830 real_backward_count 33228   4.408%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.401954/  1.587100, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.07 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4544%\n",
      "layer   2  Sparsity: 80.6697%\n",
      "layer   3  Sparsity: 85.0098%\n",
      "total_backward_count 758725 real_backward_count 33263   4.384%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.403889/  1.585997, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.64 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4601%\n",
      "layer   2  Sparsity: 80.5706%\n",
      "layer   3  Sparsity: 85.0073%\n",
      "total_backward_count 763620 real_backward_count 33283   4.359%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.400754/  1.568490, val:  82.08%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.80 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4924%\n",
      "layer   2  Sparsity: 80.4723%\n",
      "layer   3  Sparsity: 84.8253%\n",
      "total_backward_count 768515 real_backward_count 33330   4.337%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.394972/  1.572272, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.15 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4769%\n",
      "layer   2  Sparsity: 80.5132%\n",
      "layer   3  Sparsity: 84.7945%\n",
      "total_backward_count 773410 real_backward_count 33361   4.313%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.395473/  1.571201, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.85 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4674%\n",
      "layer   2  Sparsity: 80.5655%\n",
      "layer   3  Sparsity: 84.9591%\n",
      "total_backward_count 778305 real_backward_count 33409   4.293%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.394532/  1.576773, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.51 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4829%\n",
      "layer   2  Sparsity: 80.7769%\n",
      "layer   3  Sparsity: 84.8893%\n",
      "total_backward_count 783200 real_backward_count 33443   4.270%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.399894/  1.581195, val:  81.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4443%\n",
      "layer   2  Sparsity: 80.7072%\n",
      "layer   3  Sparsity: 85.0817%\n",
      "total_backward_count 788095 real_backward_count 33484   4.249%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.404460/  1.591205, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4518%\n",
      "layer   2  Sparsity: 80.8679%\n",
      "layer   3  Sparsity: 85.1537%\n",
      "total_backward_count 792990 real_backward_count 33514   4.226%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.407257/  1.585917, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4127%\n",
      "layer   2  Sparsity: 80.9305%\n",
      "layer   3  Sparsity: 84.9723%\n",
      "total_backward_count 797885 real_backward_count 33534   4.203%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.407670/  1.591802, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4652%\n",
      "layer   2  Sparsity: 81.0253%\n",
      "layer   3  Sparsity: 85.1934%\n",
      "total_backward_count 802780 real_backward_count 33560   4.180%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.406585/  1.586995, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4716%\n",
      "layer   2  Sparsity: 81.0024%\n",
      "layer   3  Sparsity: 85.3592%\n",
      "total_backward_count 807675 real_backward_count 33589   4.159%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.408747/  1.586339, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.49 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4553%\n",
      "layer   2  Sparsity: 80.9367%\n",
      "layer   3  Sparsity: 85.4408%\n",
      "total_backward_count 812570 real_backward_count 33611   4.136%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.400697/  1.582857, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.53 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4742%\n",
      "layer   2  Sparsity: 80.9022%\n",
      "layer   3  Sparsity: 85.3743%\n",
      "total_backward_count 817465 real_backward_count 33636   4.115%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.398226/  1.583781, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.82 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4422%\n",
      "layer   2  Sparsity: 80.9176%\n",
      "layer   3  Sparsity: 85.3280%\n",
      "total_backward_count 822360 real_backward_count 33661   4.093%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.395438/  1.589923, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.53 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4405%\n",
      "layer   2  Sparsity: 81.0039%\n",
      "layer   3  Sparsity: 85.2573%\n",
      "total_backward_count 827255 real_backward_count 33682   4.072%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.397462/  1.583006, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4650%\n",
      "layer   2  Sparsity: 80.9501%\n",
      "layer   3  Sparsity: 85.1966%\n",
      "total_backward_count 832150 real_backward_count 33698   4.050%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.392603/  1.581654, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.58 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4554%\n",
      "layer   2  Sparsity: 80.8793%\n",
      "layer   3  Sparsity: 85.1953%\n",
      "total_backward_count 837045 real_backward_count 33725   4.029%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.394182/  1.588266, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4777%\n",
      "layer   2  Sparsity: 80.9922%\n",
      "layer   3  Sparsity: 85.1222%\n",
      "total_backward_count 841940 real_backward_count 33748   4.008%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.397124/  1.584435, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.31 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4378%\n",
      "layer   2  Sparsity: 81.0386%\n",
      "layer   3  Sparsity: 85.0258%\n",
      "total_backward_count 846835 real_backward_count 33773   3.988%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.400751/  1.585374, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.61 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4283%\n",
      "layer   2  Sparsity: 80.9491%\n",
      "layer   3  Sparsity: 84.9574%\n",
      "total_backward_count 851730 real_backward_count 33809   3.969%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.402630/  1.590278, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.31 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4559%\n",
      "layer   2  Sparsity: 80.8543%\n",
      "layer   3  Sparsity: 85.0304%\n",
      "total_backward_count 856625 real_backward_count 33846   3.951%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.404535/  1.592238, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4683%\n",
      "layer   2  Sparsity: 80.9377%\n",
      "layer   3  Sparsity: 84.9658%\n",
      "total_backward_count 861520 real_backward_count 33867   3.931%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.402998/  1.595509, val:  81.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4602%\n",
      "layer   2  Sparsity: 81.1838%\n",
      "layer   3  Sparsity: 85.2025%\n",
      "total_backward_count 866415 real_backward_count 33893   3.912%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.399146/  1.585093, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.90 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4500%\n",
      "layer   2  Sparsity: 81.0120%\n",
      "layer   3  Sparsity: 85.1115%\n",
      "total_backward_count 871310 real_backward_count 33926   3.894%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.391702/  1.578572, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.36 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4579%\n",
      "layer   2  Sparsity: 80.8993%\n",
      "layer   3  Sparsity: 85.0883%\n",
      "total_backward_count 876205 real_backward_count 33946   3.874%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.390373/  1.576708, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.07 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4581%\n",
      "layer   2  Sparsity: 80.7965%\n",
      "layer   3  Sparsity: 85.1082%\n",
      "total_backward_count 881100 real_backward_count 33960   3.854%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.387674/  1.576377, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4295%\n",
      "layer   2  Sparsity: 80.8304%\n",
      "layer   3  Sparsity: 85.0333%\n",
      "total_backward_count 885995 real_backward_count 33982   3.835%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.396355/  1.579611, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4671%\n",
      "layer   2  Sparsity: 80.8600%\n",
      "layer   3  Sparsity: 85.0931%\n",
      "total_backward_count 890890 real_backward_count 34011   3.818%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.391189/  1.585113, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4428%\n",
      "layer   2  Sparsity: 80.9220%\n",
      "layer   3  Sparsity: 85.0225%\n",
      "total_backward_count 895785 real_backward_count 34029   3.799%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.384377/  1.585708, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.72 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4659%\n",
      "layer   2  Sparsity: 80.9035%\n",
      "layer   3  Sparsity: 85.0404%\n",
      "total_backward_count 900680 real_backward_count 34057   3.781%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.391912/  1.572662, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4583%\n",
      "layer   2  Sparsity: 80.7715%\n",
      "layer   3  Sparsity: 85.1896%\n",
      "total_backward_count 905575 real_backward_count 34078   3.763%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.384007/  1.572440, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.63 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4625%\n",
      "layer   2  Sparsity: 80.8860%\n",
      "layer   3  Sparsity: 85.1127%\n",
      "total_backward_count 910470 real_backward_count 34099   3.745%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.385465/  1.566337, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4303%\n",
      "layer   2  Sparsity: 80.9769%\n",
      "layer   3  Sparsity: 85.0022%\n",
      "total_backward_count 915365 real_backward_count 34111   3.726%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.380759/  1.564332, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.37 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.5045%\n",
      "layer   2  Sparsity: 80.9761%\n",
      "layer   3  Sparsity: 85.0366%\n",
      "total_backward_count 920260 real_backward_count 34135   3.709%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.378096/  1.568116, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.59 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4653%\n",
      "layer   2  Sparsity: 80.9512%\n",
      "layer   3  Sparsity: 84.9696%\n",
      "total_backward_count 925155 real_backward_count 34150   3.691%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.384142/  1.571691, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.43 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4616%\n",
      "layer   2  Sparsity: 80.9226%\n",
      "layer   3  Sparsity: 84.8437%\n",
      "total_backward_count 930050 real_backward_count 34181   3.675%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.381422/  1.566135, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.07 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4650%\n",
      "layer   2  Sparsity: 80.8197%\n",
      "layer   3  Sparsity: 84.7486%\n",
      "total_backward_count 934945 real_backward_count 34203   3.658%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.379776/  1.567776, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.16 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4578%\n",
      "layer   2  Sparsity: 80.8983%\n",
      "layer   3  Sparsity: 84.8358%\n",
      "total_backward_count 939840 real_backward_count 34220   3.641%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.379867/  1.565861, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4655%\n",
      "layer   2  Sparsity: 80.9338%\n",
      "layer   3  Sparsity: 84.7514%\n",
      "total_backward_count 944735 real_backward_count 34241   3.624%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.382160/  1.558987, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.80 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4671%\n",
      "layer   2  Sparsity: 80.8797%\n",
      "layer   3  Sparsity: 84.7316%\n",
      "total_backward_count 949630 real_backward_count 34261   3.608%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.376687/  1.571419, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4664%\n",
      "layer   2  Sparsity: 80.8107%\n",
      "layer   3  Sparsity: 84.9072%\n",
      "total_backward_count 954525 real_backward_count 34277   3.591%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.369897/  1.563781, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.25 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 86.4644%\n",
      "layer   2  Sparsity: 80.8993%\n",
      "layer   3  Sparsity: 84.9938%\n",
      "total_backward_count 959420 real_backward_count 34299   3.575%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.373140/  1.574427, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 86.4630%\n",
      "layer   2  Sparsity: 80.9032%\n",
      "layer   3  Sparsity: 85.2187%\n",
      "total_backward_count 964315 real_backward_count 34318   3.559%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.377072/  1.571510, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.43 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4804%\n",
      "layer   2  Sparsity: 80.8594%\n",
      "layer   3  Sparsity: 85.1532%\n",
      "total_backward_count 969210 real_backward_count 34339   3.543%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.370950/  1.573453, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.78 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4660%\n",
      "layer   2  Sparsity: 80.8528%\n",
      "layer   3  Sparsity: 85.2460%\n",
      "total_backward_count 974105 real_backward_count 34358   3.527%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.375692/  1.565313, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.34 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 86.4555%\n",
      "layer   2  Sparsity: 80.8342%\n",
      "layer   3  Sparsity: 85.1113%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39de06d83614cb2aa1fa6e9a0ad7012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.37569</td></tr><tr><td>val_acc_best</td><td>0.85833</td></tr><tr><td>val_acc_now</td><td>0.825</td></tr><tr><td>val_loss</td><td>1.56531</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">breezy-sweep-338</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e8kd01bw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e8kd01bw</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251127_232545-e8kd01bw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o1xzzlne with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251128_014258-o1xzzlne</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o1xzzlne' target=\"_blank\">gallant-sweep-341</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o1xzzlne' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o1xzzlne</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251128_014307_676', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 20, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=20, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=20, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 282\n",
      "fc layer 1 self.abs_max_out: 696.0\n",
      "lif layer 1 self.abs_max_v: 696.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1511.0\n",
      "lif layer 2 self.abs_max_v: 1511.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 568.0\n",
      "fc layer 1 self.abs_max_out: 826.0\n",
      "lif layer 1 self.abs_max_v: 898.5\n",
      "fc layer 2 self.abs_max_out: 2055.0\n",
      "lif layer 2 self.abs_max_v: 2707.5\n",
      "fc layer 3 self.abs_max_out: 696.0\n",
      "lif layer 1 self.abs_max_v: 1070.5\n",
      "lif layer 2 self.abs_max_v: 3105.0\n",
      "fc layer 3 self.abs_max_out: 941.0\n",
      "fc layer 1 self.abs_max_out: 998.0\n",
      "lif layer 1 self.abs_max_v: 1118.5\n",
      "lif layer 1 self.abs_max_v: 1247.0\n",
      "lif layer 2 self.abs_max_v: 3352.0\n",
      "fc layer 3 self.abs_max_out: 952.0\n",
      "lif layer 1 self.abs_max_v: 1294.0\n",
      "fc layer 3 self.abs_max_out: 1152.0\n",
      "fc layer 1 self.abs_max_out: 1171.0\n",
      "lif layer 1 self.abs_max_v: 1602.0\n",
      "fc layer 1 self.abs_max_out: 1185.0\n",
      "fc layer 2 self.abs_max_out: 2064.0\n",
      "fc layer 3 self.abs_max_out: 1200.0\n",
      "smallest_now_T updated: 254\n",
      "fc layer 2 self.abs_max_out: 2220.0\n",
      "fc layer 1 self.abs_max_out: 1495.0\n",
      "lif layer 1 self.abs_max_v: 1685.0\n",
      "fc layer 2 self.abs_max_out: 2559.0\n",
      "fc layer 1 self.abs_max_out: 1653.0\n",
      "lif layer 1 self.abs_max_v: 2064.5\n",
      "lif layer 2 self.abs_max_v: 3543.5\n",
      "smallest_now_T updated: 193\n",
      "fc layer 2 self.abs_max_out: 2563.0\n",
      "lif layer 2 self.abs_max_v: 3724.0\n",
      "fc layer 1 self.abs_max_out: 1828.0\n",
      "fc layer 3 self.abs_max_out: 1268.0\n",
      "fc layer 1 self.abs_max_out: 1894.0\n",
      "lif layer 1 self.abs_max_v: 2263.5\n",
      "lif layer 1 self.abs_max_v: 2564.0\n",
      "lif layer 2 self.abs_max_v: 4067.0\n",
      "lif layer 1 self.abs_max_v: 2615.0\n",
      "fc layer 1 self.abs_max_out: 2158.0\n",
      "lif layer 1 self.abs_max_v: 3041.5\n",
      "fc layer 1 self.abs_max_out: 3316.0\n",
      "lif layer 1 self.abs_max_v: 4837.0\n",
      "fc layer 3 self.abs_max_out: 1488.0\n",
      "lif layer 2 self.abs_max_v: 4093.0\n",
      "lif layer 2 self.abs_max_v: 4322.5\n",
      "fc layer 3 self.abs_max_out: 1601.0\n",
      "fc layer 2 self.abs_max_out: 2655.0\n",
      "fc layer 3 self.abs_max_out: 1669.0\n",
      "lif layer 2 self.abs_max_v: 4379.5\n",
      "fc layer 2 self.abs_max_out: 2704.0\n",
      "fc layer 2 self.abs_max_out: 2965.0\n",
      "smallest_now_T updated: 163\n",
      "fc layer 2 self.abs_max_out: 3520.0\n",
      "lif layer 2 self.abs_max_v: 4622.5\n",
      "fc layer 1 self.abs_max_out: 3333.0\n",
      "lif layer 2 self.abs_max_v: 4738.5\n",
      "lif layer 2 self.abs_max_v: 4810.5\n",
      "smallest_now_T updated: 150\n",
      "lif layer 2 self.abs_max_v: 4938.5\n",
      "lif layer 2 self.abs_max_v: 5068.5\n",
      "lif layer 2 self.abs_max_v: 5134.0\n",
      "lif layer 2 self.abs_max_v: 5152.0\n",
      "lif layer 2 self.abs_max_v: 5173.0\n",
      "smallest_now_T updated: 135\n",
      "fc layer 3 self.abs_max_out: 1835.0\n",
      "fc layer 1 self.abs_max_out: 3909.0\n",
      "fc layer 1 self.abs_max_out: 4266.0\n",
      "lif layer 1 self.abs_max_v: 4843.0\n",
      "fc layer 1 self.abs_max_out: 4548.0\n",
      "lif layer 1 self.abs_max_v: 5134.0\n",
      "lif layer 1 self.abs_max_v: 5218.5\n",
      "lif layer 2 self.abs_max_v: 5292.0\n",
      "fc layer 3 self.abs_max_out: 1886.0\n",
      "lif layer 1 self.abs_max_v: 5404.5\n",
      "lif layer 2 self.abs_max_v: 5438.0\n",
      "lif layer 1 self.abs_max_v: 5557.0\n",
      "lif layer 1 self.abs_max_v: 6257.5\n",
      "lif layer 1 self.abs_max_v: 6286.0\n",
      "smallest_now_T updated: 116\n",
      "smallest_now_T updated: 90\n",
      "fc layer 3 self.abs_max_out: 2039.0\n",
      "fc layer 3 self.abs_max_out: 2070.0\n",
      "fc layer 3 self.abs_max_out: 2072.0\n",
      "lif layer 2 self.abs_max_v: 5482.5\n",
      "lif layer 2 self.abs_max_v: 5735.5\n",
      "lif layer 2 self.abs_max_v: 5922.0\n",
      "lif layer 1 self.abs_max_v: 6294.0\n",
      "lif layer 1 self.abs_max_v: 6632.5\n",
      "lif layer 1 self.abs_max_v: 6858.5\n",
      "lif layer 1 self.abs_max_v: 7108.5\n",
      "lif layer 1 self.abs_max_v: 7152.5\n",
      "lif layer 1 self.abs_max_v: 7344.5\n",
      "lif layer 1 self.abs_max_v: 7530.5\n",
      "fc layer 2 self.abs_max_out: 3751.0\n",
      "fc layer 1 self.abs_max_out: 5001.0\n",
      "lif layer 1 self.abs_max_v: 8006.5\n",
      "lif layer 1 self.abs_max_v: 8084.5\n",
      "lif layer 1 self.abs_max_v: 8568.5\n",
      "fc layer 3 self.abs_max_out: 2145.0\n",
      "fc layer 3 self.abs_max_out: 2217.0\n",
      "fc layer 3 self.abs_max_out: 2531.0\n",
      "lif layer 2 self.abs_max_v: 5941.5\n",
      "fc layer 1 self.abs_max_out: 5100.0\n",
      "lif layer 1 self.abs_max_v: 8588.0\n",
      "lif layer 1 self.abs_max_v: 9137.0\n",
      "fc layer 1 self.abs_max_out: 5384.0\n",
      "lif layer 2 self.abs_max_v: 5959.0\n",
      "lif layer 2 self.abs_max_v: 6003.5\n",
      "fc layer 3 self.abs_max_out: 2654.0\n",
      "fc layer 3 self.abs_max_out: 2766.0\n",
      "fc layer 3 self.abs_max_out: 2874.0\n",
      "fc layer 3 self.abs_max_out: 2949.0\n",
      "fc layer 3 self.abs_max_out: 2975.0\n",
      "fc layer 3 self.abs_max_out: 2984.0\n",
      "fc layer 3 self.abs_max_out: 3142.0\n",
      "lif layer 2 self.abs_max_v: 6056.5\n",
      "fc layer 1 self.abs_max_out: 5553.0\n",
      "fc layer 1 self.abs_max_out: 5751.0\n",
      "fc layer 1 self.abs_max_out: 5802.0\n",
      "fc layer 1 self.abs_max_out: 5999.0\n",
      "lif layer 2 self.abs_max_v: 6087.5\n",
      "lif layer 2 self.abs_max_v: 6113.0\n",
      "lif layer 2 self.abs_max_v: 6114.0\n",
      "smallest_now_T_val updated: 262\n",
      "smallest_now_T_val updated: 217\n",
      "smallest_now_T_val updated: 213\n",
      "smallest_now_T_val updated: 209\n",
      "smallest_now_T_val updated: 174\n",
      "smallest_now_T_val updated: 63\n",
      "lif layer 1 self.abs_max_v: 9688.0\n",
      "lif layer 1 self.abs_max_v: 9728.0\n",
      "lif layer 1 self.abs_max_v: 10115.0\n",
      "lif layer 1 self.abs_max_v: 10411.0\n",
      "lif layer 1 self.abs_max_v: 10576.5\n",
      "lif layer 1 self.abs_max_v: 10974.0\n",
      "lif layer 1 self.abs_max_v: 11172.0\n",
      "fc layer 1 self.abs_max_out: 6757.0\n",
      "lif layer 1 self.abs_max_v: 11878.0\n",
      "lif layer 1 self.abs_max_v: 12184.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.447342/  1.920051, val:  34.17%, val_best:  34.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5278%\n",
      "layer   2  Sparsity: 68.6406%\n",
      "layer   3  Sparsity: 62.9932%\n",
      "total_backward_count 9790 real_backward_count 1283  13.105%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 6140.0\n",
      "lif layer 2 self.abs_max_v: 6421.5\n",
      "lif layer 2 self.abs_max_v: 6490.5\n",
      "lif layer 2 self.abs_max_v: 6707.0\n",
      "lif layer 2 self.abs_max_v: 6798.5\n",
      "fc layer 3 self.abs_max_out: 3187.0\n",
      "fc layer 1 self.abs_max_out: 6812.0\n",
      "fc layer 1 self.abs_max_out: 7546.0\n",
      "lif layer 1 self.abs_max_v: 12512.0\n",
      "lif layer 1 self.abs_max_v: 12594.0\n",
      "lif layer 1 self.abs_max_v: 12906.0\n",
      "lif layer 1 self.abs_max_v: 13578.0\n",
      "lif layer 1 self.abs_max_v: 13673.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.404602/  1.843633, val:  28.75%, val_best:  34.17%, tr:  99.80%, tr_best:  99.80%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.5591%\n",
      "layer   2  Sparsity: 73.8151%\n",
      "layer   3  Sparsity: 68.4525%\n",
      "total_backward_count 19580 real_backward_count 2474  12.635%\n",
      "lif layer 2 self.abs_max_v: 6828.5\n",
      "fc layer 1 self.abs_max_out: 8165.0\n",
      "lif layer 1 self.abs_max_v: 13753.0\n",
      "lif layer 1 self.abs_max_v: 14546.5\n",
      "fc layer 1 self.abs_max_out: 8172.0\n",
      "fc layer 1 self.abs_max_out: 9319.0\n",
      "fc layer 1 self.abs_max_out: 9646.0\n",
      "lif layer 1 self.abs_max_v: 16348.5\n",
      "lif layer 1 self.abs_max_v: 17224.5\n",
      "lif layer 1 self.abs_max_v: 17418.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.412070/  1.842648, val:  37.92%, val_best:  37.92%, tr:  99.08%, tr_best:  99.80%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5556%\n",
      "layer   2  Sparsity: 75.6806%\n",
      "layer   3  Sparsity: 72.5925%\n",
      "total_backward_count 29370 real_backward_count 3704  12.612%\n",
      "fc layer 2 self.abs_max_out: 3797.0\n",
      "lif layer 2 self.abs_max_v: 6887.5\n",
      "fc layer 2 self.abs_max_out: 3824.0\n",
      "lif layer 2 self.abs_max_v: 6900.5\n",
      "lif layer 2 self.abs_max_v: 7020.5\n",
      "lif layer 2 self.abs_max_v: 7173.5\n",
      "fc layer 2 self.abs_max_out: 3998.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.431019/  1.846449, val:  40.42%, val_best:  40.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5050%\n",
      "layer   2  Sparsity: 76.8375%\n",
      "layer   3  Sparsity: 74.1726%\n",
      "total_backward_count 39160 real_backward_count 4936  12.605%\n",
      "lif layer 1 self.abs_max_v: 17612.0\n",
      "fc layer 1 self.abs_max_out: 10137.0\n",
      "lif layer 1 self.abs_max_v: 18162.5\n",
      "lif layer 1 self.abs_max_v: 18755.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.424398/  1.808185, val:  42.08%, val_best:  42.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5296%\n",
      "layer   2  Sparsity: 78.2861%\n",
      "layer   3  Sparsity: 74.5305%\n",
      "total_backward_count 48950 real_backward_count 6127  12.517%\n",
      "lif layer 2 self.abs_max_v: 7374.0\n",
      "fc layer 1 self.abs_max_out: 10214.0\n",
      "lif layer 1 self.abs_max_v: 19525.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.390142/  1.757677, val:  50.00%, val_best:  50.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5047%\n",
      "layer   2  Sparsity: 78.7292%\n",
      "layer   3  Sparsity: 73.6845%\n",
      "total_backward_count 58740 real_backward_count 7296  12.421%\n",
      "fc layer 3 self.abs_max_out: 3350.0\n",
      "fc layer 1 self.abs_max_out: 10674.0\n",
      "fc layer 1 self.abs_max_out: 10687.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.359670/  1.753258, val:  41.67%, val_best:  50.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5358%\n",
      "layer   2  Sparsity: 80.1062%\n",
      "layer   3  Sparsity: 73.9388%\n",
      "total_backward_count 68530 real_backward_count 8475  12.367%\n",
      "fc layer 1 self.abs_max_out: 10693.0\n",
      "lif layer 1 self.abs_max_v: 19735.5\n",
      "lif layer 1 self.abs_max_v: 20116.0\n",
      "fc layer 1 self.abs_max_out: 11125.0\n",
      "lif layer 1 self.abs_max_v: 21183.0\n",
      "lif layer 1 self.abs_max_v: 21439.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.364803/  1.765033, val:  43.75%, val_best:  50.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5457%\n",
      "layer   2  Sparsity: 80.4146%\n",
      "layer   3  Sparsity: 74.0202%\n",
      "total_backward_count 78320 real_backward_count 9643  12.312%\n",
      "fc layer 1 self.abs_max_out: 11241.0\n",
      "lif layer 1 self.abs_max_v: 21713.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.399493/  1.813902, val:  33.33%, val_best:  50.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5046%\n",
      "layer   2  Sparsity: 80.5070%\n",
      "layer   3  Sparsity: 75.2207%\n",
      "total_backward_count 88110 real_backward_count 10864  12.330%\n",
      "fc layer 2 self.abs_max_out: 4008.0\n",
      "fc layer 1 self.abs_max_out: 11434.0\n",
      "lif layer 1 self.abs_max_v: 22185.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.374613/  1.770585, val:  38.33%, val_best:  50.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5681%\n",
      "layer   2  Sparsity: 80.7840%\n",
      "layer   3  Sparsity: 75.7348%\n",
      "total_backward_count 97900 real_backward_count 12021  12.279%\n",
      "fc layer 2 self.abs_max_out: 4013.0\n",
      "fc layer 2 self.abs_max_out: 4053.0\n",
      "lif layer 2 self.abs_max_v: 7518.5\n",
      "lif layer 2 self.abs_max_v: 7534.5\n",
      "fc layer 2 self.abs_max_out: 4077.0\n",
      "lif layer 2 self.abs_max_v: 7643.5\n",
      "fc layer 2 self.abs_max_out: 4106.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.337063/  1.757977, val:  38.75%, val_best:  50.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5297%\n",
      "layer   2  Sparsity: 81.0432%\n",
      "layer   3  Sparsity: 74.8372%\n",
      "total_backward_count 107690 real_backward_count 13159  12.219%\n",
      "fc layer 2 self.abs_max_out: 4234.0\n",
      "lif layer 1 self.abs_max_v: 22224.5\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.354787/  1.766536, val:  45.00%, val_best:  50.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5178%\n",
      "layer   2  Sparsity: 81.3655%\n",
      "layer   3  Sparsity: 74.3995%\n",
      "total_backward_count 117480 real_backward_count 14311  12.182%\n",
      "fc layer 1 self.abs_max_out: 11546.0\n",
      "fc layer 1 self.abs_max_out: 12331.0\n",
      "lif layer 1 self.abs_max_v: 23411.5\n",
      "lif layer 1 self.abs_max_v: 23495.0\n",
      "lif layer 1 self.abs_max_v: 23851.5\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.359362/  1.753337, val:  40.00%, val_best:  50.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5560%\n",
      "layer   2  Sparsity: 81.1184%\n",
      "layer   3  Sparsity: 75.6261%\n",
      "total_backward_count 127270 real_backward_count 15396  12.097%\n",
      "fc layer 1 self.abs_max_out: 12339.0\n",
      "lif layer 1 self.abs_max_v: 23867.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.383821/  1.789927, val:  38.33%, val_best:  50.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5677%\n",
      "layer   2  Sparsity: 81.7911%\n",
      "layer   3  Sparsity: 76.0718%\n",
      "total_backward_count 137060 real_backward_count 16519  12.052%\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.357206/  1.719563, val:  46.25%, val_best:  50.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5595%\n",
      "layer   2  Sparsity: 81.7299%\n",
      "layer   3  Sparsity: 75.6523%\n",
      "total_backward_count 146850 real_backward_count 17647  12.017%\n",
      "fc layer 1 self.abs_max_out: 12543.0\n",
      "fc layer 1 self.abs_max_out: 13391.0\n",
      "lif layer 1 self.abs_max_v: 24874.0\n",
      "lif layer 1 self.abs_max_v: 25243.0\n",
      "lif layer 1 self.abs_max_v: 25955.5\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.352148/  1.752265, val:  46.25%, val_best:  50.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5245%\n",
      "layer   2  Sparsity: 82.2823%\n",
      "layer   3  Sparsity: 75.5644%\n",
      "total_backward_count 156640 real_backward_count 18808  12.007%\n",
      "fc layer 2 self.abs_max_out: 4264.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.298515/  1.670982, val:  51.25%, val_best:  51.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5393%\n",
      "layer   2  Sparsity: 81.4484%\n",
      "layer   3  Sparsity: 75.5567%\n",
      "total_backward_count 166430 real_backward_count 19884  11.947%\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.326357/  1.732511, val:  45.42%, val_best:  51.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5226%\n",
      "layer   2  Sparsity: 80.0605%\n",
      "layer   3  Sparsity: 75.1346%\n",
      "total_backward_count 176220 real_backward_count 21018  11.927%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.352880/  1.737712, val:  42.92%, val_best:  51.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5366%\n",
      "layer   2  Sparsity: 79.8038%\n",
      "layer   3  Sparsity: 75.1274%\n",
      "total_backward_count 186010 real_backward_count 22189  11.929%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.336010/  1.784180, val:  32.50%, val_best:  51.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5320%\n",
      "layer   2  Sparsity: 80.8187%\n",
      "layer   3  Sparsity: 75.1970%\n",
      "total_backward_count 195800 real_backward_count 23269  11.884%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.324435/  1.766217, val:  37.92%, val_best:  51.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5424%\n",
      "layer   2  Sparsity: 80.5425%\n",
      "layer   3  Sparsity: 74.9706%\n",
      "total_backward_count 205590 real_backward_count 24353  11.845%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.330507/  1.778805, val:  32.92%, val_best:  51.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5294%\n",
      "layer   2  Sparsity: 81.0470%\n",
      "layer   3  Sparsity: 74.8592%\n",
      "total_backward_count 215380 real_backward_count 25486  11.833%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.338230/  1.782748, val:  47.50%, val_best:  51.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5547%\n",
      "layer   2  Sparsity: 80.2614%\n",
      "layer   3  Sparsity: 75.0998%\n",
      "total_backward_count 225170 real_backward_count 26617  11.821%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.357615/  1.730781, val:  47.08%, val_best:  51.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5085%\n",
      "layer   2  Sparsity: 80.9142%\n",
      "layer   3  Sparsity: 75.4913%\n",
      "total_backward_count 234960 real_backward_count 27703  11.791%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.355354/  1.718765, val:  44.17%, val_best:  51.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5319%\n",
      "layer   2  Sparsity: 80.3418%\n",
      "layer   3  Sparsity: 75.7368%\n",
      "total_backward_count 244750 real_backward_count 28791  11.763%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.362694/  1.709836, val:  49.58%, val_best:  51.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5522%\n",
      "layer   2  Sparsity: 80.7069%\n",
      "layer   3  Sparsity: 75.1267%\n",
      "total_backward_count 254540 real_backward_count 29927  11.757%\n",
      "fc layer 1 self.abs_max_out: 13480.0\n",
      "lif layer 1 self.abs_max_v: 26081.5\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.344590/  1.665873, val:  55.00%, val_best:  55.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5398%\n",
      "layer   2  Sparsity: 81.0377%\n",
      "layer   3  Sparsity: 75.9998%\n",
      "total_backward_count 264330 real_backward_count 31040  11.743%\n",
      "fc layer 2 self.abs_max_out: 4293.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.320562/  1.677943, val:  54.17%, val_best:  55.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5483%\n",
      "layer   2  Sparsity: 81.7095%\n",
      "layer   3  Sparsity: 75.7784%\n",
      "total_backward_count 274120 real_backward_count 32188  11.742%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.300000/  1.730255, val:  42.50%, val_best:  55.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5469%\n",
      "layer   2  Sparsity: 81.6632%\n",
      "layer   3  Sparsity: 75.6418%\n",
      "total_backward_count 283910 real_backward_count 33283  11.723%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.333991/  1.750550, val:  37.50%, val_best:  55.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5455%\n",
      "layer   2  Sparsity: 82.0257%\n",
      "layer   3  Sparsity: 75.9459%\n",
      "total_backward_count 293700 real_backward_count 34385  11.708%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.340065/  1.750702, val:  44.17%, val_best:  55.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.40 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.5758%\n",
      "layer   2  Sparsity: 81.6288%\n",
      "layer   3  Sparsity: 75.8345%\n",
      "total_backward_count 303490 real_backward_count 35487  11.693%\n",
      "fc layer 2 self.abs_max_out: 4378.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.341222/  1.750676, val:  38.75%, val_best:  55.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.5511%\n",
      "layer   2  Sparsity: 80.4224%\n",
      "layer   3  Sparsity: 75.3144%\n",
      "total_backward_count 313280 real_backward_count 36590  11.680%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.318673/  1.727214, val:  53.33%, val_best:  55.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5470%\n",
      "layer   2  Sparsity: 80.2774%\n",
      "layer   3  Sparsity: 76.0610%\n",
      "total_backward_count 323070 real_backward_count 37685  11.665%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.335900/  1.698097, val:  43.75%, val_best:  55.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5317%\n",
      "layer   2  Sparsity: 80.0204%\n",
      "layer   3  Sparsity: 75.6408%\n",
      "total_backward_count 332860 real_backward_count 38794  11.655%\n",
      "lif layer 2 self.abs_max_v: 7652.0\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.328825/  1.634024, val:  53.75%, val_best:  55.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5627%\n",
      "layer   2  Sparsity: 80.0747%\n",
      "layer   3  Sparsity: 75.7057%\n",
      "total_backward_count 342650 real_backward_count 39863  11.634%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.268319/  1.646143, val:  48.33%, val_best:  55.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5630%\n",
      "layer   2  Sparsity: 79.9290%\n",
      "layer   3  Sparsity: 75.6308%\n",
      "total_backward_count 352440 real_backward_count 40941  11.616%\n",
      "fc layer 1 self.abs_max_out: 13547.0\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.323275/  1.666100, val:  53.33%, val_best:  55.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5192%\n",
      "layer   2  Sparsity: 80.1160%\n",
      "layer   3  Sparsity: 76.2507%\n",
      "total_backward_count 362230 real_backward_count 42042  11.606%\n",
      "fc layer 2 self.abs_max_out: 4611.0\n",
      "fc layer 1 self.abs_max_out: 13750.0\n",
      "lif layer 1 self.abs_max_v: 26418.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.374901/  1.730114, val:  52.92%, val_best:  55.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5560%\n",
      "layer   2  Sparsity: 80.5305%\n",
      "layer   3  Sparsity: 77.7007%\n",
      "total_backward_count 372020 real_backward_count 43082  11.581%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.403521/  1.746623, val:  47.92%, val_best:  55.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5541%\n",
      "layer   2  Sparsity: 81.0904%\n",
      "layer   3  Sparsity: 78.6788%\n",
      "total_backward_count 381810 real_backward_count 44197  11.576%\n",
      "lif layer 2 self.abs_max_v: 7730.0\n",
      "lif layer 2 self.abs_max_v: 7869.0\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.384727/  1.752702, val:  45.42%, val_best:  55.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5306%\n",
      "layer   2  Sparsity: 81.3244%\n",
      "layer   3  Sparsity: 77.2652%\n",
      "total_backward_count 391600 real_backward_count 45246  11.554%\n",
      "fc layer 1 self.abs_max_out: 13843.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.366140/  1.705319, val:  48.75%, val_best:  55.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5757%\n",
      "layer   2  Sparsity: 81.6368%\n",
      "layer   3  Sparsity: 77.1272%\n",
      "total_backward_count 401390 real_backward_count 46287  11.532%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.378444/  1.744072, val:  47.08%, val_best:  55.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5353%\n",
      "layer   2  Sparsity: 81.2330%\n",
      "layer   3  Sparsity: 77.9140%\n",
      "total_backward_count 411180 real_backward_count 47339  11.513%\n",
      "fc layer 1 self.abs_max_out: 14056.0\n",
      "lif layer 1 self.abs_max_v: 26625.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.358891/  1.762266, val:  44.58%, val_best:  55.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5237%\n",
      "layer   2  Sparsity: 81.7562%\n",
      "layer   3  Sparsity: 78.1726%\n",
      "total_backward_count 420970 real_backward_count 48381  11.493%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.357412/  1.704296, val:  55.42%, val_best:  55.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5033%\n",
      "layer   2  Sparsity: 81.9283%\n",
      "layer   3  Sparsity: 77.1497%\n",
      "total_backward_count 430760 real_backward_count 49425  11.474%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.394589/  1.750981, val:  43.33%, val_best:  55.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5471%\n",
      "layer   2  Sparsity: 82.8780%\n",
      "layer   3  Sparsity: 78.2445%\n",
      "total_backward_count 440550 real_backward_count 50519  11.467%\n",
      "fc layer 1 self.abs_max_out: 14196.0\n",
      "lif layer 1 self.abs_max_v: 26876.0\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.340410/  1.670677, val:  66.25%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5362%\n",
      "layer   2  Sparsity: 83.2209%\n",
      "layer   3  Sparsity: 78.3491%\n",
      "total_backward_count 450340 real_backward_count 51577  11.453%\n",
      "lif layer 2 self.abs_max_v: 8230.5\n",
      "fc layer 1 self.abs_max_out: 14707.0\n",
      "lif layer 1 self.abs_max_v: 27849.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.333721/  1.700608, val:  45.83%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5597%\n",
      "layer   2  Sparsity: 82.3964%\n",
      "layer   3  Sparsity: 77.3183%\n",
      "total_backward_count 460130 real_backward_count 52641  11.440%\n",
      "fc layer 1 self.abs_max_out: 14897.0\n",
      "lif layer 1 self.abs_max_v: 28108.5\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.291334/  1.710516, val:  39.58%, val_best:  66.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5407%\n",
      "layer   2  Sparsity: 82.4726%\n",
      "layer   3  Sparsity: 76.8191%\n",
      "total_backward_count 469920 real_backward_count 53731  11.434%\n",
      "fc layer 2 self.abs_max_out: 4632.0\n",
      "lif layer 2 self.abs_max_v: 8728.5\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.304808/  1.699867, val:  56.25%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5605%\n",
      "layer   2  Sparsity: 82.3121%\n",
      "layer   3  Sparsity: 76.4614%\n",
      "total_backward_count 479710 real_backward_count 54822  11.428%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.293293/  1.653557, val:  49.17%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4852%\n",
      "layer   2  Sparsity: 82.1437%\n",
      "layer   3  Sparsity: 75.6760%\n",
      "total_backward_count 489500 real_backward_count 55875  11.415%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.284748/  1.687019, val:  49.58%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5446%\n",
      "layer   2  Sparsity: 82.4753%\n",
      "layer   3  Sparsity: 75.6873%\n",
      "total_backward_count 499290 real_backward_count 56948  11.406%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.291463/  1.697466, val:  50.00%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5359%\n",
      "layer   2  Sparsity: 82.8533%\n",
      "layer   3  Sparsity: 75.8957%\n",
      "total_backward_count 509080 real_backward_count 58020  11.397%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.294340/  1.617585, val:  62.50%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5199%\n",
      "layer   2  Sparsity: 82.5249%\n",
      "layer   3  Sparsity: 75.5266%\n",
      "total_backward_count 518870 real_backward_count 59072  11.385%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.256743/  1.623163, val:  54.58%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5095%\n",
      "layer   2  Sparsity: 82.0275%\n",
      "layer   3  Sparsity: 75.9063%\n",
      "total_backward_count 528660 real_backward_count 60223  11.392%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.299881/  1.676079, val:  50.83%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5249%\n",
      "layer   2  Sparsity: 81.9324%\n",
      "layer   3  Sparsity: 76.1222%\n",
      "total_backward_count 538450 real_backward_count 61298  11.384%\n",
      "lif layer 2 self.abs_max_v: 8780.5\n",
      "fc layer 2 self.abs_max_out: 4813.0\n",
      "lif layer 2 self.abs_max_v: 9203.5\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.300352/  1.693008, val:  51.25%, val_best:  66.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5689%\n",
      "layer   2  Sparsity: 82.3137%\n",
      "layer   3  Sparsity: 76.2037%\n",
      "total_backward_count 548240 real_backward_count 62415  11.385%\n",
      "fc layer 2 self.abs_max_out: 4841.0\n",
      "fc layer 2 self.abs_max_out: 4906.0\n",
      "lif layer 2 self.abs_max_v: 9354.0\n",
      "fc layer 2 self.abs_max_out: 5088.0\n",
      "lif layer 2 self.abs_max_v: 9765.0\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.306005/  1.717011, val:  47.08%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4944%\n",
      "layer   2  Sparsity: 81.5928%\n",
      "layer   3  Sparsity: 76.5735%\n",
      "total_backward_count 558030 real_backward_count 63526  11.384%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.329165/  1.721721, val:  45.83%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5008%\n",
      "layer   2  Sparsity: 82.2877%\n",
      "layer   3  Sparsity: 77.5417%\n",
      "total_backward_count 567820 real_backward_count 64557  11.369%\n",
      "lif layer 1 self.abs_max_v: 28327.5\n",
      "fc layer 1 self.abs_max_out: 14936.0\n",
      "lif layer 1 self.abs_max_v: 29100.0\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.350344/  1.729686, val:  44.58%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5268%\n",
      "layer   2  Sparsity: 81.7092%\n",
      "layer   3  Sparsity: 78.3663%\n",
      "total_backward_count 577610 real_backward_count 65651  11.366%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.303034/  1.695802, val:  49.58%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5467%\n",
      "layer   2  Sparsity: 81.5406%\n",
      "layer   3  Sparsity: 76.2207%\n",
      "total_backward_count 587400 real_backward_count 66762  11.366%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.276014/  1.706558, val:  42.50%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5419%\n",
      "layer   2  Sparsity: 81.5175%\n",
      "layer   3  Sparsity: 75.8681%\n",
      "total_backward_count 597190 real_backward_count 67828  11.358%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.325787/  1.620426, val:  58.75%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.4978%\n",
      "layer   2  Sparsity: 80.8188%\n",
      "layer   3  Sparsity: 76.4849%\n",
      "total_backward_count 606980 real_backward_count 68950  11.360%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.284234/  1.699460, val:  41.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5444%\n",
      "layer   2  Sparsity: 81.4822%\n",
      "layer   3  Sparsity: 75.8982%\n",
      "total_backward_count 616770 real_backward_count 70002  11.350%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.291563/  1.720597, val:  43.33%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5118%\n",
      "layer   2  Sparsity: 81.3452%\n",
      "layer   3  Sparsity: 75.6182%\n",
      "total_backward_count 626560 real_backward_count 71045  11.339%\n",
      "fc layer 2 self.abs_max_out: 5092.0\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.297784/  1.739493, val:  42.92%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5525%\n",
      "layer   2  Sparsity: 81.0834%\n",
      "layer   3  Sparsity: 76.4562%\n",
      "total_backward_count 636350 real_backward_count 72112  11.332%\n",
      "fc layer 1 self.abs_max_out: 15115.0\n",
      "fc layer 1 self.abs_max_out: 15268.0\n",
      "lif layer 1 self.abs_max_v: 29120.0\n",
      "lif layer 1 self.abs_max_v: 29818.0\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.286909/  1.704550, val:  44.58%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5172%\n",
      "layer   2  Sparsity: 80.8416%\n",
      "layer   3  Sparsity: 76.3555%\n",
      "total_backward_count 646140 real_backward_count 73206  11.330%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.307693/  1.738953, val:  51.67%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5237%\n",
      "layer   2  Sparsity: 81.2543%\n",
      "layer   3  Sparsity: 77.3681%\n",
      "total_backward_count 655930 real_backward_count 74252  11.320%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.306542/  1.630998, val:  52.92%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5032%\n",
      "layer   2  Sparsity: 81.3697%\n",
      "layer   3  Sparsity: 76.5150%\n",
      "total_backward_count 665720 real_backward_count 75273  11.307%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.278982/  1.642210, val:  50.00%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5415%\n",
      "layer   2  Sparsity: 81.0637%\n",
      "layer   3  Sparsity: 76.7954%\n",
      "total_backward_count 675510 real_backward_count 76344  11.302%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.268618/  1.754586, val:  39.58%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5474%\n",
      "layer   2  Sparsity: 80.5831%\n",
      "layer   3  Sparsity: 76.3456%\n",
      "total_backward_count 685300 real_backward_count 77420  11.297%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.298650/  1.723814, val:  47.08%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5286%\n",
      "layer   2  Sparsity: 80.4594%\n",
      "layer   3  Sparsity: 76.5869%\n",
      "total_backward_count 695090 real_backward_count 78498  11.293%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.273336/  1.701434, val:  45.42%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5529%\n",
      "layer   2  Sparsity: 80.9868%\n",
      "layer   3  Sparsity: 75.2962%\n",
      "total_backward_count 704880 real_backward_count 79574  11.289%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.287016/  1.636452, val:  56.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5551%\n",
      "layer   2  Sparsity: 81.1720%\n",
      "layer   3  Sparsity: 75.6719%\n",
      "total_backward_count 714670 real_backward_count 80657  11.286%\n",
      "fc layer 2 self.abs_max_out: 5161.0\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.266300/  1.674835, val:  56.25%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5388%\n",
      "layer   2  Sparsity: 81.8518%\n",
      "layer   3  Sparsity: 76.2662%\n",
      "total_backward_count 724460 real_backward_count 81727  11.281%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.300940/  1.640638, val:  55.00%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5393%\n",
      "layer   2  Sparsity: 81.8286%\n",
      "layer   3  Sparsity: 77.2362%\n",
      "total_backward_count 734250 real_backward_count 82811  11.278%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.271340/  1.626039, val:  57.50%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5339%\n",
      "layer   2  Sparsity: 82.0694%\n",
      "layer   3  Sparsity: 76.6595%\n",
      "total_backward_count 744040 real_backward_count 83828  11.267%\n",
      "fc layer 2 self.abs_max_out: 5558.0\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.302392/  1.695477, val:  50.83%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5199%\n",
      "layer   2  Sparsity: 81.5208%\n",
      "layer   3  Sparsity: 77.6062%\n",
      "total_backward_count 753830 real_backward_count 84970  11.272%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.299268/  1.711066, val:  40.83%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5502%\n",
      "layer   2  Sparsity: 81.1794%\n",
      "layer   3  Sparsity: 77.6195%\n",
      "total_backward_count 763620 real_backward_count 86072  11.272%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.275144/  1.610311, val:  53.75%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5152%\n",
      "layer   2  Sparsity: 81.5903%\n",
      "layer   3  Sparsity: 75.9957%\n",
      "total_backward_count 773410 real_backward_count 87159  11.269%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.225557/  1.624367, val:  55.42%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5284%\n",
      "layer   2  Sparsity: 81.8865%\n",
      "layer   3  Sparsity: 74.6505%\n",
      "total_backward_count 783200 real_backward_count 88178  11.259%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.261781/  1.649800, val:  48.75%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5631%\n",
      "layer   2  Sparsity: 81.9567%\n",
      "layer   3  Sparsity: 76.4712%\n",
      "total_backward_count 792990 real_backward_count 89252  11.255%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.265136/  1.689137, val:  37.50%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5345%\n",
      "layer   2  Sparsity: 81.6071%\n",
      "layer   3  Sparsity: 76.0740%\n",
      "total_backward_count 802780 real_backward_count 90231  11.240%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.267314/  1.592102, val:  54.17%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5354%\n",
      "layer   2  Sparsity: 82.2910%\n",
      "layer   3  Sparsity: 76.3226%\n",
      "total_backward_count 812570 real_backward_count 91346  11.242%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.248568/  1.656305, val:  53.33%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.4933%\n",
      "layer   2  Sparsity: 81.7630%\n",
      "layer   3  Sparsity: 75.9609%\n",
      "total_backward_count 822360 real_backward_count 92458  11.243%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.277767/  1.641621, val:  48.75%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5525%\n",
      "layer   2  Sparsity: 81.4794%\n",
      "layer   3  Sparsity: 76.3396%\n",
      "total_backward_count 832150 real_backward_count 93491  11.235%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.230614/  1.643456, val:  55.83%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5175%\n",
      "layer   2  Sparsity: 81.6917%\n",
      "layer   3  Sparsity: 75.4685%\n",
      "total_backward_count 841940 real_backward_count 94539  11.229%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.175967/  1.620392, val:  50.83%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5777%\n",
      "layer   2  Sparsity: 81.6036%\n",
      "layer   3  Sparsity: 74.0909%\n",
      "total_backward_count 851730 real_backward_count 95601  11.224%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.212412/  1.545359, val:  60.42%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5687%\n",
      "layer   2  Sparsity: 81.1038%\n",
      "layer   3  Sparsity: 75.3878%\n",
      "total_backward_count 861520 real_backward_count 96666  11.220%\n",
      "lif layer 2 self.abs_max_v: 9951.5\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.218053/  1.596096, val:  52.92%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5250%\n",
      "layer   2  Sparsity: 80.6284%\n",
      "layer   3  Sparsity: 75.6561%\n",
      "total_backward_count 871310 real_backward_count 97698  11.213%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.262590/  1.648917, val:  49.58%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5459%\n",
      "layer   2  Sparsity: 81.4543%\n",
      "layer   3  Sparsity: 76.4492%\n",
      "total_backward_count 881100 real_backward_count 98804  11.214%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.224693/  1.607902, val:  51.25%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5412%\n",
      "layer   2  Sparsity: 81.9103%\n",
      "layer   3  Sparsity: 75.5979%\n",
      "total_backward_count 890890 real_backward_count 99901  11.214%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.213858/  1.551547, val:  60.00%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.41 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.5477%\n",
      "layer   2  Sparsity: 81.8296%\n",
      "layer   3  Sparsity: 75.6598%\n",
      "total_backward_count 900680 real_backward_count 100981  11.212%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.194577/  1.616676, val:  41.67%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5194%\n",
      "layer   2  Sparsity: 82.2337%\n",
      "layer   3  Sparsity: 75.2314%\n",
      "total_backward_count 910470 real_backward_count 102058  11.209%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.205060/  1.605594, val:  50.83%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5466%\n",
      "layer   2  Sparsity: 82.1938%\n",
      "layer   3  Sparsity: 75.8023%\n",
      "total_backward_count 920260 real_backward_count 103131  11.207%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.223441/  1.585519, val:  60.42%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5568%\n",
      "layer   2  Sparsity: 82.7276%\n",
      "layer   3  Sparsity: 75.7573%\n",
      "total_backward_count 930050 real_backward_count 104241  11.208%\n",
      "fc layer 3 self.abs_max_out: 3471.0\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.199724/  1.552876, val:  60.00%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5648%\n",
      "layer   2  Sparsity: 82.6694%\n",
      "layer   3  Sparsity: 75.3040%\n",
      "total_backward_count 939840 real_backward_count 105313  11.205%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.206925/  1.647275, val:  44.58%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5459%\n",
      "layer   2  Sparsity: 82.6279%\n",
      "layer   3  Sparsity: 74.9309%\n",
      "total_backward_count 949630 real_backward_count 106361  11.200%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.207311/  1.576480, val:  56.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5504%\n",
      "layer   2  Sparsity: 82.3189%\n",
      "layer   3  Sparsity: 74.7556%\n",
      "total_backward_count 959420 real_backward_count 107408  11.195%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.167246/  1.616044, val:  51.67%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5286%\n",
      "layer   2  Sparsity: 82.5122%\n",
      "layer   3  Sparsity: 73.4853%\n",
      "total_backward_count 969210 real_backward_count 108435  11.188%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.153674/  1.568310, val:  59.17%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5731%\n",
      "layer   2  Sparsity: 82.6852%\n",
      "layer   3  Sparsity: 72.9451%\n",
      "total_backward_count 979000 real_backward_count 109460  11.181%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.112636/  1.532173, val:  57.08%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5425%\n",
      "layer   2  Sparsity: 82.3830%\n",
      "layer   3  Sparsity: 71.8662%\n",
      "total_backward_count 988790 real_backward_count 110482  11.173%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.147518/  1.544084, val:  56.25%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5350%\n",
      "layer   2  Sparsity: 82.2721%\n",
      "layer   3  Sparsity: 72.9236%\n",
      "total_backward_count 998580 real_backward_count 111570  11.173%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.161258/  1.583299, val:  48.33%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5473%\n",
      "layer   2  Sparsity: 82.8035%\n",
      "layer   3  Sparsity: 74.6104%\n",
      "total_backward_count 1008370 real_backward_count 112616  11.168%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.195280/  1.583340, val:  53.33%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5240%\n",
      "layer   2  Sparsity: 82.8287%\n",
      "layer   3  Sparsity: 75.7317%\n",
      "total_backward_count 1018160 real_backward_count 113672  11.164%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.233786/  1.614813, val:  52.50%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.21 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5197%\n",
      "layer   2  Sparsity: 82.7850%\n",
      "layer   3  Sparsity: 76.0631%\n",
      "total_backward_count 1027950 real_backward_count 114722  11.160%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.200922/  1.595315, val:  55.00%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.98 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.5222%\n",
      "layer   2  Sparsity: 82.9346%\n",
      "layer   3  Sparsity: 75.2783%\n",
      "total_backward_count 1037740 real_backward_count 115806  11.159%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.219354/  1.678342, val:  38.75%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5288%\n",
      "layer   2  Sparsity: 82.7832%\n",
      "layer   3  Sparsity: 75.6320%\n",
      "total_backward_count 1047530 real_backward_count 116865  11.156%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.237390/  1.557555, val:  55.42%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5425%\n",
      "layer   2  Sparsity: 83.1506%\n",
      "layer   3  Sparsity: 75.4132%\n",
      "total_backward_count 1057320 real_backward_count 117947  11.155%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.194793/  1.593889, val:  54.58%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5352%\n",
      "layer   2  Sparsity: 82.4815%\n",
      "layer   3  Sparsity: 74.4473%\n",
      "total_backward_count 1067110 real_backward_count 119055  11.157%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.176154/  1.597379, val:  45.42%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5071%\n",
      "layer   2  Sparsity: 82.3863%\n",
      "layer   3  Sparsity: 74.5902%\n",
      "total_backward_count 1076900 real_backward_count 120146  11.157%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.172334/  1.573664, val:  59.17%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5394%\n",
      "layer   2  Sparsity: 81.9786%\n",
      "layer   3  Sparsity: 74.0794%\n",
      "total_backward_count 1086690 real_backward_count 121221  11.155%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.168836/  1.592061, val:  51.67%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5246%\n",
      "layer   2  Sparsity: 81.2843%\n",
      "layer   3  Sparsity: 73.5207%\n",
      "total_backward_count 1096480 real_backward_count 122232  11.148%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.183228/  1.566279, val:  63.33%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5180%\n",
      "layer   2  Sparsity: 80.9313%\n",
      "layer   3  Sparsity: 74.2162%\n",
      "total_backward_count 1106270 real_backward_count 123286  11.144%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.163854/  1.619663, val:  52.08%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5735%\n",
      "layer   2  Sparsity: 81.2687%\n",
      "layer   3  Sparsity: 73.9063%\n",
      "total_backward_count 1116060 real_backward_count 124338  11.141%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.138798/  1.604837, val:  52.92%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5249%\n",
      "layer   2  Sparsity: 81.2681%\n",
      "layer   3  Sparsity: 73.0341%\n",
      "total_backward_count 1125850 real_backward_count 125343  11.133%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.103468/  1.622901, val:  42.92%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5614%\n",
      "layer   2  Sparsity: 81.0711%\n",
      "layer   3  Sparsity: 71.3516%\n",
      "total_backward_count 1135640 real_backward_count 126379  11.128%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.176399/  1.550389, val:  60.83%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5460%\n",
      "layer   2  Sparsity: 80.4981%\n",
      "layer   3  Sparsity: 73.2061%\n",
      "total_backward_count 1145430 real_backward_count 127428  11.125%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.154252/  1.528393, val:  56.67%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5139%\n",
      "layer   2  Sparsity: 81.0449%\n",
      "layer   3  Sparsity: 73.2776%\n",
      "total_backward_count 1155220 real_backward_count 128504  11.124%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.138999/  1.530872, val:  59.17%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5435%\n",
      "layer   2  Sparsity: 80.8871%\n",
      "layer   3  Sparsity: 73.4815%\n",
      "total_backward_count 1165010 real_backward_count 129557  11.121%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.132223/  1.538008, val:  58.33%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5446%\n",
      "layer   2  Sparsity: 80.8395%\n",
      "layer   3  Sparsity: 73.2874%\n",
      "total_backward_count 1174800 real_backward_count 130600  11.117%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.151405/  1.576085, val:  55.00%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5450%\n",
      "layer   2  Sparsity: 81.0349%\n",
      "layer   3  Sparsity: 73.8485%\n",
      "total_backward_count 1184590 real_backward_count 131602  11.109%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.164653/  1.633932, val:  51.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.53 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.5309%\n",
      "layer   2  Sparsity: 80.8253%\n",
      "layer   3  Sparsity: 73.5303%\n",
      "total_backward_count 1194380 real_backward_count 132653  11.106%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.232066/  1.660856, val:  47.92%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5307%\n",
      "layer   2  Sparsity: 80.7427%\n",
      "layer   3  Sparsity: 74.9628%\n",
      "total_backward_count 1204170 real_backward_count 133671  11.101%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.226093/  1.606107, val:  64.58%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.5469%\n",
      "layer   2  Sparsity: 80.1604%\n",
      "layer   3  Sparsity: 75.4518%\n",
      "total_backward_count 1213960 real_backward_count 134717  11.097%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.236557/  1.577424, val:  55.83%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5685%\n",
      "layer   2  Sparsity: 79.9245%\n",
      "layer   3  Sparsity: 75.2694%\n",
      "total_backward_count 1223750 real_backward_count 135772  11.095%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.222595/  1.665583, val:  50.42%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.4979%\n",
      "layer   2  Sparsity: 79.9857%\n",
      "layer   3  Sparsity: 75.6100%\n",
      "total_backward_count 1233540 real_backward_count 136834  11.093%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.211252/  1.624195, val:  47.50%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5468%\n",
      "layer   2  Sparsity: 80.3310%\n",
      "layer   3  Sparsity: 75.4039%\n",
      "total_backward_count 1243330 real_backward_count 137886  11.090%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.184972/  1.595369, val:  53.75%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5295%\n",
      "layer   2  Sparsity: 80.5333%\n",
      "layer   3  Sparsity: 75.4379%\n",
      "total_backward_count 1253120 real_backward_count 138892  11.084%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.231187/  1.616371, val:  59.17%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5108%\n",
      "layer   2  Sparsity: 80.7922%\n",
      "layer   3  Sparsity: 76.1393%\n",
      "total_backward_count 1262910 real_backward_count 139913  11.079%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.232459/  1.618056, val:  52.50%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5401%\n",
      "layer   2  Sparsity: 81.3308%\n",
      "layer   3  Sparsity: 76.8373%\n",
      "total_backward_count 1272700 real_backward_count 140938  11.074%\n",
      "fc layer 1 self.abs_max_out: 15334.0\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.194917/  1.656587, val:  48.75%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5551%\n",
      "layer   2  Sparsity: 82.2013%\n",
      "layer   3  Sparsity: 76.0862%\n",
      "total_backward_count 1282490 real_backward_count 141989  11.071%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.215918/  1.625555, val:  46.67%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5328%\n",
      "layer   2  Sparsity: 81.0130%\n",
      "layer   3  Sparsity: 76.1982%\n",
      "total_backward_count 1292280 real_backward_count 143103  11.074%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.216236/  1.579890, val:  55.83%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5516%\n",
      "layer   2  Sparsity: 80.7853%\n",
      "layer   3  Sparsity: 76.4305%\n",
      "total_backward_count 1302070 real_backward_count 144134  11.070%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.250279/  1.611130, val:  53.75%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5625%\n",
      "layer   2  Sparsity: 80.6317%\n",
      "layer   3  Sparsity: 77.0706%\n",
      "total_backward_count 1311860 real_backward_count 145206  11.069%\n",
      "fc layer 1 self.abs_max_out: 15574.0\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.251815/  1.648362, val:  51.67%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5539%\n",
      "layer   2  Sparsity: 81.3006%\n",
      "layer   3  Sparsity: 76.6972%\n",
      "total_backward_count 1321650 real_backward_count 146276  11.068%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.263110/  1.620030, val:  49.17%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5405%\n",
      "layer   2  Sparsity: 81.6525%\n",
      "layer   3  Sparsity: 78.4713%\n",
      "total_backward_count 1331440 real_backward_count 147373  11.069%\n",
      "fc layer 1 self.abs_max_out: 15711.0\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.243226/  1.610541, val:  52.08%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5435%\n",
      "layer   2  Sparsity: 81.6726%\n",
      "layer   3  Sparsity: 76.4447%\n",
      "total_backward_count 1341230 real_backward_count 148383  11.063%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.226431/  1.647203, val:  57.92%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5153%\n",
      "layer   2  Sparsity: 81.4310%\n",
      "layer   3  Sparsity: 76.3491%\n",
      "total_backward_count 1351020 real_backward_count 149444  11.062%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.244763/  1.566433, val:  61.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5535%\n",
      "layer   2  Sparsity: 81.2847%\n",
      "layer   3  Sparsity: 76.6781%\n",
      "total_backward_count 1360810 real_backward_count 150513  11.061%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.223710/  1.626479, val:  61.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5286%\n",
      "layer   2  Sparsity: 81.9993%\n",
      "layer   3  Sparsity: 76.7372%\n",
      "total_backward_count 1370600 real_backward_count 151606  11.061%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.251544/  1.604258, val:  54.17%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.5419%\n",
      "layer   2  Sparsity: 82.2790%\n",
      "layer   3  Sparsity: 77.5142%\n",
      "total_backward_count 1380390 real_backward_count 152707  11.063%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.227281/  1.632196, val:  51.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5587%\n",
      "layer   2  Sparsity: 81.7315%\n",
      "layer   3  Sparsity: 76.9896%\n",
      "total_backward_count 1390180 real_backward_count 153795  11.063%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.244628/  1.569131, val:  59.17%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5784%\n",
      "layer   2  Sparsity: 81.8632%\n",
      "layer   3  Sparsity: 76.2955%\n",
      "total_backward_count 1399970 real_backward_count 154883  11.063%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.217964/  1.634017, val:  45.83%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5186%\n",
      "layer   2  Sparsity: 81.5122%\n",
      "layer   3  Sparsity: 76.4232%\n",
      "total_backward_count 1409760 real_backward_count 155971  11.064%\n",
      "fc layer 1 self.abs_max_out: 15962.0\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.225486/  1.643010, val:  47.08%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5080%\n",
      "layer   2  Sparsity: 81.6308%\n",
      "layer   3  Sparsity: 75.7705%\n",
      "total_backward_count 1419550 real_backward_count 157006  11.060%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.194602/  1.577652, val:  52.92%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5331%\n",
      "layer   2  Sparsity: 81.4863%\n",
      "layer   3  Sparsity: 75.3727%\n",
      "total_backward_count 1429340 real_backward_count 158075  11.059%\n",
      "fc layer 1 self.abs_max_out: 16602.0\n",
      "lif layer 1 self.abs_max_v: 30545.0\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.196754/  1.647204, val:  56.25%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5455%\n",
      "layer   2  Sparsity: 81.4549%\n",
      "layer   3  Sparsity: 75.4467%\n",
      "total_backward_count 1439130 real_backward_count 159169  11.060%\n",
      "fc layer 1 self.abs_max_out: 16606.0\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.234782/  1.681666, val:  49.17%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5212%\n",
      "layer   2  Sparsity: 81.6871%\n",
      "layer   3  Sparsity: 76.6473%\n",
      "total_backward_count 1448920 real_backward_count 160235  11.059%\n",
      "fc layer 1 self.abs_max_out: 17386.0\n",
      "lif layer 1 self.abs_max_v: 30705.0\n",
      "lif layer 1 self.abs_max_v: 32103.5\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.232322/  1.544666, val:  62.50%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5476%\n",
      "layer   2  Sparsity: 81.4421%\n",
      "layer   3  Sparsity: 75.7294%\n",
      "total_backward_count 1458710 real_backward_count 161282  11.056%\n",
      "fc layer 1 self.abs_max_out: 17582.0\n",
      "lif layer 1 self.abs_max_v: 32527.5\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.232434/  1.585377, val:  50.00%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5612%\n",
      "layer   2  Sparsity: 80.6538%\n",
      "layer   3  Sparsity: 76.7932%\n",
      "total_backward_count 1468500 real_backward_count 162359  11.056%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.212971/  1.611635, val:  52.50%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5472%\n",
      "layer   2  Sparsity: 80.4361%\n",
      "layer   3  Sparsity: 75.1831%\n",
      "total_backward_count 1478290 real_backward_count 163364  11.051%\n",
      "fc layer 1 self.abs_max_out: 18298.0\n",
      "lif layer 1 self.abs_max_v: 32710.0\n",
      "lif layer 1 self.abs_max_v: 34188.0\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.247589/  1.558058, val:  54.17%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5225%\n",
      "layer   2  Sparsity: 81.3707%\n",
      "layer   3  Sparsity: 76.3867%\n",
      "total_backward_count 1488080 real_backward_count 164437  11.050%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.203440/  1.603060, val:  52.92%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.09 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.5341%\n",
      "layer   2  Sparsity: 81.5503%\n",
      "layer   3  Sparsity: 76.3356%\n",
      "total_backward_count 1497870 real_backward_count 165460  11.046%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.196489/  1.554166, val:  65.00%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5295%\n",
      "layer   2  Sparsity: 81.5713%\n",
      "layer   3  Sparsity: 74.4759%\n",
      "total_backward_count 1507660 real_backward_count 166541  11.046%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.146544/  1.550193, val:  61.67%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5473%\n",
      "layer   2  Sparsity: 81.8769%\n",
      "layer   3  Sparsity: 73.4189%\n",
      "total_backward_count 1517450 real_backward_count 167524  11.040%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.182392/  1.605061, val:  47.50%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5571%\n",
      "layer   2  Sparsity: 81.4077%\n",
      "layer   3  Sparsity: 74.7207%\n",
      "total_backward_count 1527240 real_backward_count 168556  11.037%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.222697/  1.647990, val:  44.17%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5573%\n",
      "layer   2  Sparsity: 81.8368%\n",
      "layer   3  Sparsity: 76.7890%\n",
      "total_backward_count 1537030 real_backward_count 169633  11.036%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.199780/  1.589823, val:  72.50%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5213%\n",
      "layer   2  Sparsity: 81.6730%\n",
      "layer   3  Sparsity: 76.1121%\n",
      "total_backward_count 1546820 real_backward_count 170679  11.034%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.200190/  1.675057, val:  46.25%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5158%\n",
      "layer   2  Sparsity: 82.1165%\n",
      "layer   3  Sparsity: 75.8162%\n",
      "total_backward_count 1556610 real_backward_count 171754  11.034%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.228606/  1.673595, val:  49.17%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5790%\n",
      "layer   2  Sparsity: 81.9354%\n",
      "layer   3  Sparsity: 77.6115%\n",
      "total_backward_count 1566400 real_backward_count 172807  11.032%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.289673/  1.617689, val:  65.83%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5471%\n",
      "layer   2  Sparsity: 82.0236%\n",
      "layer   3  Sparsity: 78.8723%\n",
      "total_backward_count 1576190 real_backward_count 173870  11.031%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.258054/  1.594575, val:  55.42%, val_best:  72.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5328%\n",
      "layer   2  Sparsity: 81.8051%\n",
      "layer   3  Sparsity: 76.3311%\n",
      "total_backward_count 1585980 real_backward_count 174941  11.030%\n",
      "lif layer 2 self.abs_max_v: 9978.0\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.214645/  1.611079, val:  46.25%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5384%\n",
      "layer   2  Sparsity: 81.8053%\n",
      "layer   3  Sparsity: 76.3188%\n",
      "total_backward_count 1595770 real_backward_count 175961  11.027%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.219080/  1.600278, val:  54.58%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5291%\n",
      "layer   2  Sparsity: 81.3562%\n",
      "layer   3  Sparsity: 77.3854%\n",
      "total_backward_count 1605560 real_backward_count 177030  11.026%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.259732/  1.623083, val:  48.75%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5473%\n",
      "layer   2  Sparsity: 81.3803%\n",
      "layer   3  Sparsity: 77.4261%\n",
      "total_backward_count 1615350 real_backward_count 178106  11.026%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.242550/  1.609937, val:  59.17%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5242%\n",
      "layer   2  Sparsity: 81.7164%\n",
      "layer   3  Sparsity: 76.6266%\n",
      "total_backward_count 1625140 real_backward_count 179187  11.026%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.238912/  1.612095, val:  43.75%, val_best:  72.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5283%\n",
      "layer   2  Sparsity: 81.3696%\n",
      "layer   3  Sparsity: 76.6593%\n",
      "total_backward_count 1634930 real_backward_count 180249  11.025%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.182330/  1.625034, val:  41.25%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5444%\n",
      "layer   2  Sparsity: 80.9845%\n",
      "layer   3  Sparsity: 74.5135%\n",
      "total_backward_count 1644720 real_backward_count 181313  11.024%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.182902/  1.649488, val:  52.08%, val_best:  72.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5448%\n",
      "layer   2  Sparsity: 81.3638%\n",
      "layer   3  Sparsity: 75.3128%\n",
      "total_backward_count 1654510 real_backward_count 182353  11.022%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.171444/  1.549919, val:  57.08%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5208%\n",
      "layer   2  Sparsity: 81.1247%\n",
      "layer   3  Sparsity: 75.8282%\n",
      "total_backward_count 1664300 real_backward_count 183376  11.018%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.204371/  1.617033, val:  47.08%, val_best:  72.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5706%\n",
      "layer   2  Sparsity: 80.6787%\n",
      "layer   3  Sparsity: 76.2582%\n",
      "total_backward_count 1674090 real_backward_count 184442  11.017%\n",
      "fc layer 3 self.abs_max_out: 3530.0\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.142086/  1.606101, val:  50.83%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5030%\n",
      "layer   2  Sparsity: 80.8072%\n",
      "layer   3  Sparsity: 75.2562%\n",
      "total_backward_count 1683880 real_backward_count 185490  11.016%\n",
      "fc layer 3 self.abs_max_out: 3537.0\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.174079/  1.572414, val:  62.08%, val_best:  72.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5659%\n",
      "layer   2  Sparsity: 80.8683%\n",
      "layer   3  Sparsity: 75.8071%\n",
      "total_backward_count 1693670 real_backward_count 186570  11.016%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.219751/  1.666927, val:  49.58%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5331%\n",
      "layer   2  Sparsity: 80.7334%\n",
      "layer   3  Sparsity: 77.4524%\n",
      "total_backward_count 1703460 real_backward_count 187606  11.013%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.264917/  1.617367, val:  61.25%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5680%\n",
      "layer   2  Sparsity: 80.2164%\n",
      "layer   3  Sparsity: 78.5468%\n",
      "total_backward_count 1713250 real_backward_count 188660  11.012%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.238126/  1.559866, val:  61.67%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5168%\n",
      "layer   2  Sparsity: 81.1687%\n",
      "layer   3  Sparsity: 77.3345%\n",
      "total_backward_count 1723040 real_backward_count 189750  11.013%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.281910/  1.617215, val:  51.25%, val_best:  72.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5475%\n",
      "layer   2  Sparsity: 81.4641%\n",
      "layer   3  Sparsity: 78.4759%\n",
      "total_backward_count 1732830 real_backward_count 190802  11.011%\n",
      "fc layer 1 self.abs_max_out: 18468.0\n",
      "lif layer 1 self.abs_max_v: 34632.5\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.195920/  1.607234, val:  54.58%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.4936%\n",
      "layer   2  Sparsity: 81.4169%\n",
      "layer   3  Sparsity: 76.0540%\n",
      "total_backward_count 1742620 real_backward_count 191865  11.010%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.213226/  1.591530, val:  56.25%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5021%\n",
      "layer   2  Sparsity: 81.4643%\n",
      "layer   3  Sparsity: 77.1316%\n",
      "total_backward_count 1752410 real_backward_count 192908  11.008%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.194713/  1.591899, val:  63.75%, val_best:  72.50%, tr:  99.39%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5222%\n",
      "layer   2  Sparsity: 82.0595%\n",
      "layer   3  Sparsity: 76.5029%\n",
      "total_backward_count 1762200 real_backward_count 193988  11.008%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.217746/  1.644535, val:  51.67%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5606%\n",
      "layer   2  Sparsity: 81.8213%\n",
      "layer   3  Sparsity: 76.9287%\n",
      "total_backward_count 1771990 real_backward_count 195036  11.007%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.231252/  1.615543, val:  57.50%, val_best:  72.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5261%\n",
      "layer   2  Sparsity: 81.5876%\n",
      "layer   3  Sparsity: 77.2754%\n",
      "total_backward_count 1781780 real_backward_count 196095  11.006%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.207881/  1.704856, val:  45.42%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5113%\n",
      "layer   2  Sparsity: 81.0838%\n",
      "layer   3  Sparsity: 76.5564%\n",
      "total_backward_count 1791570 real_backward_count 197164  11.005%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.201921/  1.693950, val:  34.17%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5070%\n",
      "layer   2  Sparsity: 81.3489%\n",
      "layer   3  Sparsity: 75.6989%\n",
      "total_backward_count 1801360 real_backward_count 198203  11.003%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.191187/  1.591776, val:  50.00%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5253%\n",
      "layer   2  Sparsity: 81.2449%\n",
      "layer   3  Sparsity: 74.4580%\n",
      "total_backward_count 1811150 real_backward_count 199228  11.000%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.171858/  1.575749, val:  52.92%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5255%\n",
      "layer   2  Sparsity: 81.4185%\n",
      "layer   3  Sparsity: 75.6737%\n",
      "total_backward_count 1820940 real_backward_count 200310  11.000%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.219967/  1.628346, val:  42.50%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5591%\n",
      "layer   2  Sparsity: 81.4905%\n",
      "layer   3  Sparsity: 75.9845%\n",
      "total_backward_count 1830730 real_backward_count 201400  11.001%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.177585/  1.592300, val:  49.17%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5449%\n",
      "layer   2  Sparsity: 81.7143%\n",
      "layer   3  Sparsity: 75.3137%\n",
      "total_backward_count 1840520 real_backward_count 202475  11.001%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.161044/  1.581988, val:  62.92%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5528%\n",
      "layer   2  Sparsity: 81.9872%\n",
      "layer   3  Sparsity: 75.1188%\n",
      "total_backward_count 1850310 real_backward_count 203503  10.998%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.186035/  1.492072, val:  68.33%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5408%\n",
      "layer   2  Sparsity: 81.8275%\n",
      "layer   3  Sparsity: 75.2088%\n",
      "total_backward_count 1860100 real_backward_count 204540  10.996%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.182367/  1.495482, val:  65.83%, val_best:  72.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5508%\n",
      "layer   2  Sparsity: 81.5681%\n",
      "layer   3  Sparsity: 75.5326%\n",
      "total_backward_count 1869890 real_backward_count 205553  10.993%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.203194/  1.592068, val:  52.92%, val_best:  72.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5271%\n",
      "layer   2  Sparsity: 82.2270%\n",
      "layer   3  Sparsity: 77.4778%\n",
      "total_backward_count 1879680 real_backward_count 206623  10.992%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.194069/  1.590949, val:  52.92%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.5218%\n",
      "layer   2  Sparsity: 82.5414%\n",
      "layer   3  Sparsity: 76.6450%\n",
      "total_backward_count 1889470 real_backward_count 207678  10.991%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.222317/  1.656609, val:  46.25%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5072%\n",
      "layer   2  Sparsity: 82.7584%\n",
      "layer   3  Sparsity: 77.5036%\n",
      "total_backward_count 1899260 real_backward_count 208728  10.990%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.221785/  1.597735, val:  57.08%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4972%\n",
      "layer   2  Sparsity: 82.6607%\n",
      "layer   3  Sparsity: 77.5051%\n",
      "total_backward_count 1909050 real_backward_count 209728  10.986%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.234311/  1.583733, val:  52.92%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5314%\n",
      "layer   2  Sparsity: 82.0285%\n",
      "layer   3  Sparsity: 77.3208%\n",
      "total_backward_count 1918840 real_backward_count 210833  10.988%\n",
      "fc layer 3 self.abs_max_out: 3552.0\n",
      "fc layer 3 self.abs_max_out: 3631.0\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.196669/  1.567494, val:  54.58%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5243%\n",
      "layer   2  Sparsity: 82.3881%\n",
      "layer   3  Sparsity: 75.4990%\n",
      "total_backward_count 1928630 real_backward_count 211916  10.988%\n",
      "fc layer 3 self.abs_max_out: 3660.0\n",
      "fc layer 3 self.abs_max_out: 3878.0\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.143809/  1.506895, val:  56.67%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5527%\n",
      "layer   2  Sparsity: 82.5113%\n",
      "layer   3  Sparsity: 74.1900%\n",
      "total_backward_count 1938420 real_backward_count 213027  10.990%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.099367/  1.554723, val:  54.58%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5583%\n",
      "layer   2  Sparsity: 82.6603%\n",
      "layer   3  Sparsity: 74.3858%\n",
      "total_backward_count 1948210 real_backward_count 214078  10.988%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.140617/  1.554780, val:  54.58%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.5129%\n",
      "layer   2  Sparsity: 82.2456%\n",
      "layer   3  Sparsity: 75.7340%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789c4d4f36eb45b2a948108dfc007444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñà‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñá‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñÅ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñà‚ñÜ‚ñá‚ñÜ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñà‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99796</td></tr><tr><td>tr_epoch_loss</td><td>1.14062</td></tr><tr><td>val_acc_best</td><td>0.725</td></tr><tr><td>val_acc_now</td><td>0.54583</td></tr><tr><td>val_loss</td><td>1.55478</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gallant-sweep-341</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o1xzzlne' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o1xzzlne</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251128_014258-o1xzzlne/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f4cp2b6i with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251128_055518-f4cp2b6i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f4cp2b6i' target=\"_blank\">likely-sweep-346</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f4cp2b6i' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f4cp2b6i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251128_055526_701', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 282\n",
      "fc layer 1 self.abs_max_out: 195.0\n",
      "lif layer 1 self.abs_max_v: 195.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 552.0\n",
      "lif layer 2 self.abs_max_v: 552.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "fc layer 1 self.abs_max_out: 257.0\n",
      "lif layer 1 self.abs_max_v: 264.0\n",
      "lif layer 2 self.abs_max_v: 643.5\n",
      "fc layer 3 self.abs_max_out: 210.0\n",
      "lif layer 1 self.abs_max_v: 339.0\n",
      "lif layer 2 self.abs_max_v: 801.0\n",
      "fc layer 1 self.abs_max_out: 292.0\n",
      "lif layer 1 self.abs_max_v: 356.0\n",
      "lif layer 2 self.abs_max_v: 883.5\n",
      "fc layer 1 self.abs_max_out: 426.0\n",
      "lif layer 1 self.abs_max_v: 426.0\n",
      "lif layer 2 self.abs_max_v: 898.0\n",
      "fc layer 3 self.abs_max_out: 290.0\n",
      "fc layer 2 self.abs_max_out: 579.0\n",
      "lif layer 2 self.abs_max_v: 955.5\n",
      "smallest_now_T updated: 254\n",
      "fc layer 1 self.abs_max_out: 473.0\n",
      "lif layer 1 self.abs_max_v: 473.0\n",
      "lif layer 1 self.abs_max_v: 513.5\n",
      "lif layer 1 self.abs_max_v: 550.5\n",
      "lif layer 1 self.abs_max_v: 572.5\n",
      "smallest_now_T updated: 193\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "fc layer 1 self.abs_max_out: 537.0\n",
      "lif layer 1 self.abs_max_v: 708.0\n",
      "lif layer 2 self.abs_max_v: 962.0\n",
      "fc layer 1 self.abs_max_out: 648.0\n",
      "fc layer 2 self.abs_max_out: 655.0\n",
      "lif layer 2 self.abs_max_v: 1109.0\n",
      "lif layer 1 self.abs_max_v: 754.5\n",
      "lif layer 1 self.abs_max_v: 775.5\n",
      "fc layer 2 self.abs_max_out: 706.0\n",
      "lif layer 1 self.abs_max_v: 837.5\n",
      "fc layer 2 self.abs_max_out: 754.0\n",
      "lif layer 2 self.abs_max_v: 1140.0\n",
      "lif layer 2 self.abs_max_v: 1254.0\n",
      "lif layer 1 self.abs_max_v: 838.5\n",
      "lif layer 1 self.abs_max_v: 899.5\n",
      "fc layer 1 self.abs_max_out: 712.0\n",
      "fc layer 1 self.abs_max_out: 781.0\n",
      "lif layer 1 self.abs_max_v: 1053.0\n",
      "fc layer 1 self.abs_max_out: 803.0\n",
      "lif layer 1 self.abs_max_v: 1117.5\n",
      "lif layer 2 self.abs_max_v: 1264.0\n",
      "fc layer 2 self.abs_max_out: 770.0\n",
      "lif layer 2 self.abs_max_v: 1365.0\n",
      "smallest_now_T updated: 163\n",
      "lif layer 1 self.abs_max_v: 1306.5\n",
      "lif layer 1 self.abs_max_v: 1344.0\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "smallest_now_T updated: 150\n",
      "fc layer 2 self.abs_max_out: 776.0\n",
      "lif layer 1 self.abs_max_v: 1370.0\n",
      "fc layer 2 self.abs_max_out: 840.0\n",
      "fc layer 3 self.abs_max_out: 313.0\n",
      "fc layer 3 self.abs_max_out: 348.0\n",
      "smallest_now_T updated: 135\n",
      "fc layer 3 self.abs_max_out: 354.0\n",
      "fc layer 3 self.abs_max_out: 355.0\n",
      "fc layer 1 self.abs_max_out: 960.0\n",
      "lif layer 1 self.abs_max_v: 1441.5\n",
      "lif layer 1 self.abs_max_v: 1447.0\n",
      "lif layer 1 self.abs_max_v: 1475.5\n",
      "fc layer 3 self.abs_max_out: 402.0\n",
      "fc layer 3 self.abs_max_out: 421.0\n",
      "lif layer 1 self.abs_max_v: 1538.0\n",
      "lif layer 1 self.abs_max_v: 1605.5\n",
      "lif layer 1 self.abs_max_v: 1634.0\n",
      "lif layer 1 self.abs_max_v: 1761.0\n",
      "lif layer 1 self.abs_max_v: 1786.5\n",
      "fc layer 1 self.abs_max_out: 963.0\n",
      "fc layer 1 self.abs_max_out: 991.0\n",
      "fc layer 1 self.abs_max_out: 993.0\n",
      "lif layer 1 self.abs_max_v: 1839.0\n",
      "smallest_now_T updated: 116\n",
      "smallest_now_T updated: 90\n",
      "fc layer 2 self.abs_max_out: 846.0\n",
      "lif layer 2 self.abs_max_v: 1403.5\n",
      "fc layer 1 self.abs_max_out: 1001.0\n",
      "fc layer 1 self.abs_max_out: 1004.0\n",
      "fc layer 1 self.abs_max_out: 1011.0\n",
      "lif layer 1 self.abs_max_v: 1841.0\n",
      "fc layer 1 self.abs_max_out: 1014.0\n",
      "fc layer 1 self.abs_max_out: 1026.0\n",
      "lif layer 1 self.abs_max_v: 1853.0\n",
      "fc layer 1 self.abs_max_out: 1052.0\n",
      "lif layer 1 self.abs_max_v: 1966.5\n",
      "lif layer 1 self.abs_max_v: 1970.5\n",
      "fc layer 1 self.abs_max_out: 1154.0\n",
      "fc layer 1 self.abs_max_out: 1180.0\n",
      "lif layer 1 self.abs_max_v: 2068.0\n",
      "fc layer 3 self.abs_max_out: 429.0\n",
      "lif layer 2 self.abs_max_v: 1440.5\n",
      "fc layer 2 self.abs_max_out: 871.0\n",
      "fc layer 1 self.abs_max_out: 1187.0\n",
      "fc layer 1 self.abs_max_out: 1236.0\n",
      "lif layer 1 self.abs_max_v: 2261.0\n",
      "lif layer 1 self.abs_max_v: 2318.5\n",
      "fc layer 1 self.abs_max_out: 1273.0\n",
      "lif layer 1 self.abs_max_v: 2432.5\n",
      "fc layer 2 self.abs_max_out: 882.0\n",
      "fc layer 2 self.abs_max_out: 886.0\n",
      "lif layer 2 self.abs_max_v: 1471.5\n",
      "lif layer 2 self.abs_max_v: 1510.0\n",
      "lif layer 2 self.abs_max_v: 1512.0\n",
      "lif layer 2 self.abs_max_v: 1536.0\n",
      "lif layer 2 self.abs_max_v: 1559.0\n",
      "fc layer 2 self.abs_max_out: 898.0\n",
      "fc layer 2 self.abs_max_out: 915.0\n",
      "fc layer 2 self.abs_max_out: 936.0\n",
      "fc layer 2 self.abs_max_out: 953.0\n",
      "lif layer 2 self.abs_max_v: 1617.5\n",
      "fc layer 3 self.abs_max_out: 433.0\n",
      "fc layer 3 self.abs_max_out: 440.0\n",
      "fc layer 3 self.abs_max_out: 451.0\n",
      "fc layer 1 self.abs_max_out: 1372.0\n",
      "lif layer 1 self.abs_max_v: 2460.5\n",
      "lif layer 1 self.abs_max_v: 2509.0\n",
      "lif layer 2 self.abs_max_v: 1636.5\n",
      "lif layer 2 self.abs_max_v: 1649.5\n",
      "fc layer 1 self.abs_max_out: 1513.0\n",
      "lif layer 1 self.abs_max_v: 2538.0\n",
      "fc layer 3 self.abs_max_out: 452.0\n",
      "lif layer 2 self.abs_max_v: 1701.0\n",
      "lif layer 1 self.abs_max_v: 2594.5\n",
      "fc layer 2 self.abs_max_out: 1027.0\n",
      "lif layer 2 self.abs_max_v: 1718.5\n",
      "lif layer 2 self.abs_max_v: 1721.5\n",
      "lif layer 2 self.abs_max_v: 1727.0\n",
      "lif layer 2 self.abs_max_v: 1796.0\n",
      "fc layer 2 self.abs_max_out: 1037.0\n",
      "smallest_now_T_val updated: 262\n",
      "smallest_now_T_val updated: 217\n",
      "smallest_now_T_val updated: 213\n",
      "smallest_now_T_val updated: 209\n",
      "fc layer 2 self.abs_max_out: 1042.0\n",
      "fc layer 2 self.abs_max_out: 1048.0\n",
      "smallest_now_T_val updated: 174\n",
      "fc layer 2 self.abs_max_out: 1074.0\n",
      "smallest_now_T_val updated: 63\n",
      "fc layer 1 self.abs_max_out: 1569.0\n",
      "lif layer 1 self.abs_max_v: 2720.5\n",
      "lif layer 2 self.abs_max_v: 1818.5\n",
      "lif layer 1 self.abs_max_v: 2823.5\n",
      "fc layer 1 self.abs_max_out: 1668.0\n",
      "lif layer 1 self.abs_max_v: 3080.0\n",
      "lif layer 1 self.abs_max_v: 3133.0\n",
      "fc layer 2 self.abs_max_out: 1109.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.648065/  1.958349, val:  27.08%, val_best:  27.08%, tr:  99.28%, tr_best:  99.28%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4711%\n",
      "layer   2  Sparsity: 67.1836%\n",
      "layer   3  Sparsity: 59.2760%\n",
      "total_backward_count 9790 real_backward_count 1594  16.282%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1125.0\n",
      "fc layer 3 self.abs_max_out: 462.0\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "fc layer 3 self.abs_max_out: 492.0\n",
      "fc layer 2 self.abs_max_out: 1138.0\n",
      "fc layer 3 self.abs_max_out: 514.0\n",
      "fc layer 2 self.abs_max_out: 1149.0\n",
      "fc layer 3 self.abs_max_out: 519.0\n",
      "fc layer 3 self.abs_max_out: 537.0\n",
      "fc layer 3 self.abs_max_out: 547.0\n",
      "fc layer 1 self.abs_max_out: 1758.0\n",
      "fc layer 3 self.abs_max_out: 574.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.537934/  1.839184, val:  39.17%, val_best:  39.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5091%\n",
      "layer   2  Sparsity: 70.1337%\n",
      "layer   3  Sparsity: 59.8864%\n",
      "total_backward_count 19580 real_backward_count 2869  14.653%\n",
      "fc layer 1 self.abs_max_out: 1815.0\n",
      "fc layer 3 self.abs_max_out: 576.0\n",
      "fc layer 1 self.abs_max_out: 1867.0\n",
      "fc layer 3 self.abs_max_out: 609.0\n",
      "fc layer 3 self.abs_max_out: 618.0\n",
      "fc layer 3 self.abs_max_out: 625.0\n",
      "fc layer 3 self.abs_max_out: 637.0\n",
      "fc layer 3 self.abs_max_out: 639.0\n",
      "fc layer 3 self.abs_max_out: 645.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.468387/  1.836543, val:  40.83%, val_best:  40.83%, tr:  98.98%, tr_best:  99.59%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5113%\n",
      "layer   2  Sparsity: 70.8804%\n",
      "layer   3  Sparsity: 59.2005%\n",
      "total_backward_count 29370 real_backward_count 4147  14.120%\n",
      "lif layer 2 self.abs_max_v: 1861.5\n",
      "fc layer 2 self.abs_max_out: 1182.0\n",
      "fc layer 2 self.abs_max_out: 1220.0\n",
      "lif layer 1 self.abs_max_v: 3174.5\n",
      "lif layer 1 self.abs_max_v: 3190.5\n",
      "lif layer 1 self.abs_max_v: 3262.5\n",
      "lif layer 2 self.abs_max_v: 1927.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.467267/  1.816110, val:  36.67%, val_best:  40.83%, tr:  99.49%, tr_best:  99.59%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4443%\n",
      "layer   2  Sparsity: 70.9297%\n",
      "layer   3  Sparsity: 59.6265%\n",
      "total_backward_count 39160 real_backward_count 5380  13.739%\n",
      "lif layer 2 self.abs_max_v: 2085.0\n",
      "lif layer 2 self.abs_max_v: 2135.5\n",
      "fc layer 1 self.abs_max_out: 2069.0\n",
      "lif layer 1 self.abs_max_v: 3546.0\n",
      "fc layer 1 self.abs_max_out: 2126.0\n",
      "lif layer 1 self.abs_max_v: 3899.0\n",
      "fc layer 1 self.abs_max_out: 2161.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.451622/  1.782328, val:  47.08%, val_best:  47.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4714%\n",
      "layer   2  Sparsity: 72.7041%\n",
      "layer   3  Sparsity: 60.6837%\n",
      "total_backward_count 48950 real_backward_count 6528  13.336%\n",
      "fc layer 3 self.abs_max_out: 651.0\n",
      "fc layer 1 self.abs_max_out: 2229.0\n",
      "fc layer 1 self.abs_max_out: 2271.0\n",
      "lif layer 1 self.abs_max_v: 4126.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.421914/  1.736892, val:  56.25%, val_best:  56.25%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4404%\n",
      "layer   2  Sparsity: 73.2263%\n",
      "layer   3  Sparsity: 59.8771%\n",
      "total_backward_count 58740 real_backward_count 7719  13.141%\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.414661/  1.706790, val:  50.00%, val_best:  56.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4898%\n",
      "layer   2  Sparsity: 73.3154%\n",
      "layer   3  Sparsity: 59.6898%\n",
      "total_backward_count 68530 real_backward_count 8823  12.875%\n",
      "fc layer 2 self.abs_max_out: 1287.0\n",
      "lif layer 2 self.abs_max_v: 2291.5\n",
      "lif layer 2 self.abs_max_v: 2304.5\n",
      "lif layer 2 self.abs_max_v: 2320.5\n",
      "lif layer 2 self.abs_max_v: 2344.5\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.388175/  1.658302, val:  54.17%, val_best:  56.25%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4888%\n",
      "layer   2  Sparsity: 73.4018%\n",
      "layer   3  Sparsity: 59.8432%\n",
      "total_backward_count 78320 real_backward_count 9895  12.634%\n",
      "lif layer 2 self.abs_max_v: 2365.0\n",
      "lif layer 2 self.abs_max_v: 2405.5\n",
      "lif layer 2 self.abs_max_v: 2406.0\n",
      "fc layer 2 self.abs_max_out: 1292.0\n",
      "lif layer 2 self.abs_max_v: 2495.0\n",
      "fc layer 3 self.abs_max_out: 670.0\n",
      "fc layer 3 self.abs_max_out: 693.0\n",
      "fc layer 2 self.abs_max_out: 1301.0\n",
      "fc layer 2 self.abs_max_out: 1353.0\n",
      "fc layer 1 self.abs_max_out: 2341.0\n",
      "lif layer 1 self.abs_max_v: 4345.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.387954/  1.666774, val:  57.08%, val_best:  57.08%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4447%\n",
      "layer   2  Sparsity: 73.7114%\n",
      "layer   3  Sparsity: 60.4250%\n",
      "total_backward_count 88110 real_backward_count 10980  12.462%\n",
      "fc layer 1 self.abs_max_out: 2452.0\n",
      "fc layer 1 self.abs_max_out: 2462.0\n",
      "lif layer 1 self.abs_max_v: 4444.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.380209/  1.723816, val:  49.17%, val_best:  57.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5260%\n",
      "layer   2  Sparsity: 73.2453%\n",
      "layer   3  Sparsity: 60.6083%\n",
      "total_backward_count 97900 real_backward_count 12038  12.296%\n",
      "fc layer 1 self.abs_max_out: 2481.0\n",
      "lif layer 1 self.abs_max_v: 4490.0\n",
      "lif layer 1 self.abs_max_v: 4506.5\n",
      "lif layer 1 self.abs_max_v: 4609.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.385620/  1.706583, val:  58.75%, val_best:  58.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4756%\n",
      "layer   2  Sparsity: 72.3873%\n",
      "layer   3  Sparsity: 61.5984%\n",
      "total_backward_count 107690 real_backward_count 13112  12.176%\n",
      "fc layer 2 self.abs_max_out: 1441.0\n",
      "lif layer 2 self.abs_max_v: 2501.5\n",
      "lif layer 2 self.abs_max_v: 2504.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.368538/  1.639497, val:  54.17%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4657%\n",
      "layer   2  Sparsity: 72.5096%\n",
      "layer   3  Sparsity: 61.6009%\n",
      "total_backward_count 117480 real_backward_count 14146  12.041%\n",
      "lif layer 1 self.abs_max_v: 4638.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.336473/  1.651808, val:  55.42%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 88.5163%\n",
      "layer   2  Sparsity: 72.3188%\n",
      "layer   3  Sparsity: 61.5174%\n",
      "total_backward_count 127270 real_backward_count 15151  11.905%\n",
      "fc layer 1 self.abs_max_out: 2534.0\n",
      "fc layer 1 self.abs_max_out: 2537.0\n",
      "fc layer 1 self.abs_max_out: 2563.0\n",
      "lif layer 1 self.abs_max_v: 4660.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.359624/  1.740989, val:  41.25%, val_best:  58.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 88.5185%\n",
      "layer   2  Sparsity: 72.1861%\n",
      "layer   3  Sparsity: 62.9687%\n",
      "total_backward_count 137060 real_backward_count 16179  11.804%\n",
      "lif layer 1 self.abs_max_v: 4703.5\n",
      "lif layer 2 self.abs_max_v: 2510.5\n",
      "fc layer 3 self.abs_max_out: 756.0\n",
      "fc layer 1 self.abs_max_out: 2566.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.352845/  1.635586, val:  56.67%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5236%\n",
      "layer   2  Sparsity: 71.8204%\n",
      "layer   3  Sparsity: 63.3294%\n",
      "total_backward_count 146850 real_backward_count 17172  11.694%\n",
      "lif layer 1 self.abs_max_v: 4807.0\n",
      "fc layer 1 self.abs_max_out: 2694.0\n",
      "fc layer 2 self.abs_max_out: 1463.0\n",
      "fc layer 3 self.abs_max_out: 780.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.346963/  1.689722, val:  43.75%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4741%\n",
      "layer   2  Sparsity: 72.6341%\n",
      "layer   3  Sparsity: 62.6285%\n",
      "total_backward_count 156640 real_backward_count 18217  11.630%\n",
      "fc layer 1 self.abs_max_out: 2732.0\n",
      "lif layer 1 self.abs_max_v: 4917.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.318893/  1.606578, val:  54.17%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4813%\n",
      "layer   2  Sparsity: 73.0040%\n",
      "layer   3  Sparsity: 61.7706%\n",
      "total_backward_count 166430 real_backward_count 19179  11.524%\n",
      "fc layer 3 self.abs_max_out: 788.0\n",
      "fc layer 2 self.abs_max_out: 1530.0\n",
      "fc layer 1 self.abs_max_out: 2735.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.310583/  1.603449, val:  67.92%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4532%\n",
      "layer   2  Sparsity: 72.3047%\n",
      "layer   3  Sparsity: 62.8300%\n",
      "total_backward_count 176220 real_backward_count 20186  11.455%\n",
      "fc layer 3 self.abs_max_out: 824.0\n",
      "fc layer 2 self.abs_max_out: 1541.0\n",
      "lif layer 1 self.abs_max_v: 4930.5\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.323033/  1.602987, val:  48.75%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4872%\n",
      "layer   2  Sparsity: 72.0942%\n",
      "layer   3  Sparsity: 63.3697%\n",
      "total_backward_count 186010 real_backward_count 21207  11.401%\n",
      "fc layer 1 self.abs_max_out: 2818.0\n",
      "lif layer 2 self.abs_max_v: 2632.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.303229/  1.670753, val:  42.50%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4744%\n",
      "layer   2  Sparsity: 72.5591%\n",
      "layer   3  Sparsity: 63.1372%\n",
      "total_backward_count 195800 real_backward_count 22149  11.312%\n",
      "lif layer 1 self.abs_max_v: 5021.5\n",
      "fc layer 2 self.abs_max_out: 1554.0\n",
      "fc layer 1 self.abs_max_out: 2827.0\n",
      "lif layer 1 self.abs_max_v: 5102.0\n",
      "lif layer 1 self.abs_max_v: 5104.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.310425/  1.681569, val:  47.50%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4884%\n",
      "layer   2  Sparsity: 71.9133%\n",
      "layer   3  Sparsity: 63.4338%\n",
      "total_backward_count 205590 real_backward_count 23097  11.234%\n",
      "lif layer 2 self.abs_max_v: 2660.0\n",
      "lif layer 2 self.abs_max_v: 2762.5\n",
      "fc layer 2 self.abs_max_out: 1584.0\n",
      "fc layer 1 self.abs_max_out: 3037.0\n",
      "fc layer 1 self.abs_max_out: 3046.0\n",
      "lif layer 1 self.abs_max_v: 5527.5\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.308666/  1.623211, val:  56.25%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4668%\n",
      "layer   2  Sparsity: 72.1395%\n",
      "layer   3  Sparsity: 63.6076%\n",
      "total_backward_count 215380 real_backward_count 24105  11.192%\n",
      "fc layer 2 self.abs_max_out: 1598.0\n",
      "lif layer 2 self.abs_max_v: 2784.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.288309/  1.571634, val:  60.00%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5032%\n",
      "layer   2  Sparsity: 72.4057%\n",
      "layer   3  Sparsity: 64.6313%\n",
      "total_backward_count 225170 real_backward_count 25083  11.140%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.256115/  1.557917, val:  62.08%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4460%\n",
      "layer   2  Sparsity: 72.4186%\n",
      "layer   3  Sparsity: 64.7447%\n",
      "total_backward_count 234960 real_backward_count 26008  11.069%\n",
      "lif layer 1 self.abs_max_v: 5596.5\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.289737/  1.583501, val:  69.17%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4719%\n",
      "layer   2  Sparsity: 70.7426%\n",
      "layer   3  Sparsity: 63.7665%\n",
      "total_backward_count 244750 real_backward_count 26899  10.990%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.279017/  1.542485, val:  61.67%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5125%\n",
      "layer   2  Sparsity: 70.2228%\n",
      "layer   3  Sparsity: 64.4635%\n",
      "total_backward_count 254540 real_backward_count 27910  10.965%\n",
      "fc layer 2 self.abs_max_out: 1689.0\n",
      "fc layer 1 self.abs_max_out: 3072.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.273780/  1.574954, val:  59.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4971%\n",
      "layer   2  Sparsity: 70.8240%\n",
      "layer   3  Sparsity: 64.2233%\n",
      "total_backward_count 264330 real_backward_count 28809  10.899%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.290361/  1.589508, val:  75.83%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4975%\n",
      "layer   2  Sparsity: 70.3521%\n",
      "layer   3  Sparsity: 64.5897%\n",
      "total_backward_count 274120 real_backward_count 29754  10.854%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.268436/  1.638752, val:  48.33%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4947%\n",
      "layer   2  Sparsity: 69.7508%\n",
      "layer   3  Sparsity: 63.9796%\n",
      "total_backward_count 283910 real_backward_count 30630  10.789%\n",
      "lif layer 2 self.abs_max_v: 2817.5\n",
      "lif layer 2 self.abs_max_v: 2824.0\n",
      "lif layer 1 self.abs_max_v: 5645.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.278938/  1.567394, val:  59.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5033%\n",
      "layer   2  Sparsity: 69.4023%\n",
      "layer   3  Sparsity: 64.6776%\n",
      "total_backward_count 293700 real_backward_count 31554  10.744%\n",
      "fc layer 1 self.abs_max_out: 3111.0\n",
      "lif layer 1 self.abs_max_v: 5726.5\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.268550/  1.553442, val:  59.17%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5295%\n",
      "layer   2  Sparsity: 70.1523%\n",
      "layer   3  Sparsity: 64.5727%\n",
      "total_backward_count 303490 real_backward_count 32431  10.686%\n",
      "lif layer 2 self.abs_max_v: 2926.5\n",
      "lif layer 2 self.abs_max_v: 2958.5\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.254021/  1.569826, val:  56.67%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4950%\n",
      "layer   2  Sparsity: 70.4462%\n",
      "layer   3  Sparsity: 64.6646%\n",
      "total_backward_count 313280 real_backward_count 33300  10.629%\n",
      "fc layer 1 self.abs_max_out: 3175.0\n",
      "lif layer 1 self.abs_max_v: 5727.5\n",
      "lif layer 1 self.abs_max_v: 5940.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.264112/  1.530797, val:  63.33%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4919%\n",
      "layer   2  Sparsity: 69.6663%\n",
      "layer   3  Sparsity: 65.4157%\n",
      "total_backward_count 323070 real_backward_count 34143  10.568%\n",
      "lif layer 1 self.abs_max_v: 5953.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.254512/  1.545352, val:  62.92%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4775%\n",
      "layer   2  Sparsity: 69.5299%\n",
      "layer   3  Sparsity: 66.0419%\n",
      "total_backward_count 332860 real_backward_count 35021  10.521%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.243058/  1.521273, val:  62.08%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5180%\n",
      "layer   2  Sparsity: 69.6253%\n",
      "layer   3  Sparsity: 66.0159%\n",
      "total_backward_count 342650 real_backward_count 35884  10.472%\n",
      "fc layer 2 self.abs_max_out: 1704.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.250388/  1.592960, val:  53.75%, val_best:  75.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5167%\n",
      "layer   2  Sparsity: 69.9696%\n",
      "layer   3  Sparsity: 65.8566%\n",
      "total_backward_count 352440 real_backward_count 36727  10.421%\n",
      "lif layer 2 self.abs_max_v: 2987.5\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.232942/  1.519832, val:  65.00%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4624%\n",
      "layer   2  Sparsity: 69.7089%\n",
      "layer   3  Sparsity: 66.0723%\n",
      "total_backward_count 362230 real_backward_count 37566  10.371%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.231850/  1.538694, val:  57.92%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5245%\n",
      "layer   2  Sparsity: 69.2282%\n",
      "layer   3  Sparsity: 65.9946%\n",
      "total_backward_count 372020 real_backward_count 38341  10.306%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.214473/  1.498968, val:  63.33%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5055%\n",
      "layer   2  Sparsity: 69.8087%\n",
      "layer   3  Sparsity: 66.2059%\n",
      "total_backward_count 381810 real_backward_count 39226  10.274%\n",
      "fc layer 1 self.abs_max_out: 3205.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.193759/  1.539410, val:  64.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4776%\n",
      "layer   2  Sparsity: 69.8445%\n",
      "layer   3  Sparsity: 65.3701%\n",
      "total_backward_count 391600 real_backward_count 40067  10.232%\n",
      "fc layer 1 self.abs_max_out: 3333.0\n",
      "lif layer 1 self.abs_max_v: 6009.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.180950/  1.473166, val:  61.67%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5281%\n",
      "layer   2  Sparsity: 69.6505%\n",
      "layer   3  Sparsity: 65.4138%\n",
      "total_backward_count 401390 real_backward_count 40880  10.185%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.188018/  1.472108, val:  72.08%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 88.4786%\n",
      "layer   2  Sparsity: 69.8457%\n",
      "layer   3  Sparsity: 65.8704%\n",
      "total_backward_count 411180 real_backward_count 41683  10.137%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.170765/  1.501006, val:  59.58%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.52 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 88.4703%\n",
      "layer   2  Sparsity: 70.1635%\n",
      "layer   3  Sparsity: 66.0860%\n",
      "total_backward_count 420970 real_backward_count 42451  10.084%\n",
      "lif layer 2 self.abs_max_v: 3090.5\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.190595/  1.438609, val:  71.25%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 88.4534%\n",
      "layer   2  Sparsity: 70.0719%\n",
      "layer   3  Sparsity: 66.1036%\n",
      "total_backward_count 430760 real_backward_count 43238  10.038%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.164350/  1.443902, val:  73.75%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5009%\n",
      "layer   2  Sparsity: 69.5635%\n",
      "layer   3  Sparsity: 66.2129%\n",
      "total_backward_count 440550 real_backward_count 44009   9.990%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.159151/  1.403947, val:  65.42%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4871%\n",
      "layer   2  Sparsity: 69.0390%\n",
      "layer   3  Sparsity: 65.6720%\n",
      "total_backward_count 450340 real_backward_count 44766   9.940%\n",
      "fc layer 2 self.abs_max_out: 1719.0\n",
      "fc layer 2 self.abs_max_out: 1737.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.162219/  1.436300, val:  75.00%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5136%\n",
      "layer   2  Sparsity: 68.8648%\n",
      "layer   3  Sparsity: 65.6162%\n",
      "total_backward_count 460130 real_backward_count 45560   9.902%\n",
      "fc layer 2 self.abs_max_out: 1783.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.152230/  1.496628, val:  52.50%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4729%\n",
      "layer   2  Sparsity: 69.6352%\n",
      "layer   3  Sparsity: 65.7565%\n",
      "total_backward_count 469920 real_backward_count 46343   9.862%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.147971/  1.465112, val:  61.25%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5214%\n",
      "layer   2  Sparsity: 69.6014%\n",
      "layer   3  Sparsity: 65.7368%\n",
      "total_backward_count 479710 real_backward_count 47089   9.816%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.156947/  1.445807, val:  69.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4113%\n",
      "layer   2  Sparsity: 69.5231%\n",
      "layer   3  Sparsity: 65.6575%\n",
      "total_backward_count 489500 real_backward_count 47829   9.771%\n",
      "lif layer 2 self.abs_max_v: 3091.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.154634/  1.442788, val:  66.67%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4931%\n",
      "layer   2  Sparsity: 69.6621%\n",
      "layer   3  Sparsity: 65.8098%\n",
      "total_backward_count 499290 real_backward_count 48564   9.727%\n",
      "fc layer 2 self.abs_max_out: 1788.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.147446/  1.428202, val:  75.83%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4922%\n",
      "layer   2  Sparsity: 69.4301%\n",
      "layer   3  Sparsity: 65.5909%\n",
      "total_backward_count 509080 real_backward_count 49290   9.682%\n",
      "fc layer 2 self.abs_max_out: 1837.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.133245/  1.419070, val:  70.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4632%\n",
      "layer   2  Sparsity: 69.2700%\n",
      "layer   3  Sparsity: 66.3767%\n",
      "total_backward_count 518870 real_backward_count 50082   9.652%\n",
      "lif layer 2 self.abs_max_v: 3096.5\n",
      "lif layer 2 self.abs_max_v: 3299.5\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.112264/  1.405657, val:  71.67%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4404%\n",
      "layer   2  Sparsity: 69.6029%\n",
      "layer   3  Sparsity: 66.5302%\n",
      "total_backward_count 528660 real_backward_count 50790   9.607%\n",
      "fc layer 2 self.abs_max_out: 1848.0\n",
      "fc layer 2 self.abs_max_out: 1936.0\n",
      "lif layer 2 self.abs_max_v: 3458.5\n",
      "fc layer 2 self.abs_max_out: 1940.0\n",
      "lif layer 2 self.abs_max_v: 3629.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.116397/  1.432795, val:  70.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4724%\n",
      "layer   2  Sparsity: 69.4889%\n",
      "layer   3  Sparsity: 66.1058%\n",
      "total_backward_count 538450 real_backward_count 51530   9.570%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.109752/  1.421392, val:  65.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5197%\n",
      "layer   2  Sparsity: 69.8269%\n",
      "layer   3  Sparsity: 65.8159%\n",
      "total_backward_count 548240 real_backward_count 52285   9.537%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.105783/  1.418208, val:  73.75%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4272%\n",
      "layer   2  Sparsity: 69.8940%\n",
      "layer   3  Sparsity: 65.2496%\n",
      "total_backward_count 558030 real_backward_count 52997   9.497%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.119572/  1.408519, val:  60.42%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4345%\n",
      "layer   2  Sparsity: 70.0040%\n",
      "layer   3  Sparsity: 64.7342%\n",
      "total_backward_count 567820 real_backward_count 53669   9.452%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.097010/  1.368796, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4722%\n",
      "layer   2  Sparsity: 69.9427%\n",
      "layer   3  Sparsity: 64.5568%\n",
      "total_backward_count 577610 real_backward_count 54345   9.409%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.103514/  1.419439, val:  69.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4954%\n",
      "layer   2  Sparsity: 69.7388%\n",
      "layer   3  Sparsity: 64.8911%\n",
      "total_backward_count 587400 real_backward_count 55002   9.364%\n",
      "fc layer 2 self.abs_max_out: 2000.0\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.103907/  1.409922, val:  58.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4893%\n",
      "layer   2  Sparsity: 69.6062%\n",
      "layer   3  Sparsity: 64.8317%\n",
      "total_backward_count 597190 real_backward_count 55674   9.323%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.085141/  1.363320, val:  74.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4316%\n",
      "layer   2  Sparsity: 69.7731%\n",
      "layer   3  Sparsity: 65.1743%\n",
      "total_backward_count 606980 real_backward_count 56376   9.288%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.061210/  1.414272, val:  57.50%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4849%\n",
      "layer   2  Sparsity: 69.7833%\n",
      "layer   3  Sparsity: 64.7242%\n",
      "total_backward_count 616770 real_backward_count 57068   9.253%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.061641/  1.345536, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4486%\n",
      "layer   2  Sparsity: 70.4199%\n",
      "layer   3  Sparsity: 64.5085%\n",
      "total_backward_count 626560 real_backward_count 57720   9.212%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.055342/  1.335893, val:  79.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5012%\n",
      "layer   2  Sparsity: 70.1950%\n",
      "layer   3  Sparsity: 64.7615%\n",
      "total_backward_count 636350 real_backward_count 58387   9.175%\n",
      "fc layer 1 self.abs_max_out: 3363.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.041880/  1.342327, val:  76.25%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4626%\n",
      "layer   2  Sparsity: 69.8160%\n",
      "layer   3  Sparsity: 64.5320%\n",
      "total_backward_count 646140 real_backward_count 59029   9.136%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.062545/  1.354656, val:  76.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4685%\n",
      "layer   2  Sparsity: 70.1092%\n",
      "layer   3  Sparsity: 64.7644%\n",
      "total_backward_count 655930 real_backward_count 59667   9.097%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.067093/  1.345577, val:  67.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4384%\n",
      "layer   2  Sparsity: 69.7404%\n",
      "layer   3  Sparsity: 65.1920%\n",
      "total_backward_count 665720 real_backward_count 60357   9.066%\n",
      "fc layer 1 self.abs_max_out: 3514.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.072056/  1.346996, val:  75.83%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4893%\n",
      "layer   2  Sparsity: 69.5249%\n",
      "layer   3  Sparsity: 65.3052%\n",
      "total_backward_count 675510 real_backward_count 61010   9.032%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.051809/  1.404634, val:  57.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4955%\n",
      "layer   2  Sparsity: 69.3884%\n",
      "layer   3  Sparsity: 65.3432%\n",
      "total_backward_count 685300 real_backward_count 61606   8.990%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.037418/  1.342595, val:  74.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4705%\n",
      "layer   2  Sparsity: 69.2469%\n",
      "layer   3  Sparsity: 65.4423%\n",
      "total_backward_count 695090 real_backward_count 62237   8.954%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.035405/  1.367307, val:  71.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5014%\n",
      "layer   2  Sparsity: 69.9366%\n",
      "layer   3  Sparsity: 65.5612%\n",
      "total_backward_count 704880 real_backward_count 62915   8.926%\n",
      "fc layer 3 self.abs_max_out: 861.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.041243/  1.331953, val:  75.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4974%\n",
      "layer   2  Sparsity: 70.0452%\n",
      "layer   3  Sparsity: 65.0027%\n",
      "total_backward_count 714670 real_backward_count 63565   8.894%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.043197/  1.384593, val:  70.00%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5035%\n",
      "layer   2  Sparsity: 69.7977%\n",
      "layer   3  Sparsity: 64.8876%\n",
      "total_backward_count 724460 real_backward_count 64132   8.852%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.030599/  1.362799, val:  71.67%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4800%\n",
      "layer   2  Sparsity: 69.9083%\n",
      "layer   3  Sparsity: 64.8994%\n",
      "total_backward_count 734250 real_backward_count 64768   8.821%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.035412/  1.295152, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4893%\n",
      "layer   2  Sparsity: 69.9023%\n",
      "layer   3  Sparsity: 65.0161%\n",
      "total_backward_count 744040 real_backward_count 65363   8.785%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.044491/  1.336123, val:  74.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4574%\n",
      "layer   2  Sparsity: 69.4727%\n",
      "layer   3  Sparsity: 65.2590%\n",
      "total_backward_count 753830 real_backward_count 66012   8.757%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.018422/  1.369273, val:  62.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5044%\n",
      "layer   2  Sparsity: 69.1554%\n",
      "layer   3  Sparsity: 66.2171%\n",
      "total_backward_count 763620 real_backward_count 66614   8.723%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.007470/  1.351382, val:  70.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4552%\n",
      "layer   2  Sparsity: 69.3109%\n",
      "layer   3  Sparsity: 65.3265%\n",
      "total_backward_count 773410 real_backward_count 67247   8.695%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.003245/  1.319691, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4723%\n",
      "layer   2  Sparsity: 69.5006%\n",
      "layer   3  Sparsity: 65.4687%\n",
      "total_backward_count 783200 real_backward_count 67790   8.656%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.015652/  1.334071, val:  74.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5134%\n",
      "layer   2  Sparsity: 69.4484%\n",
      "layer   3  Sparsity: 66.1765%\n",
      "total_backward_count 792990 real_backward_count 68349   8.619%\n",
      "fc layer 1 self.abs_max_out: 3531.0\n",
      "lif layer 1 self.abs_max_v: 6342.5\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.006914/  1.342676, val:  66.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4770%\n",
      "layer   2  Sparsity: 69.6616%\n",
      "layer   3  Sparsity: 66.7159%\n",
      "total_backward_count 802780 real_backward_count 68924   8.586%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.014573/  1.314305, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4842%\n",
      "layer   2  Sparsity: 69.4699%\n",
      "layer   3  Sparsity: 67.1033%\n",
      "total_backward_count 812570 real_backward_count 69479   8.551%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.002267/  1.320010, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4288%\n",
      "layer   2  Sparsity: 69.7057%\n",
      "layer   3  Sparsity: 66.6346%\n",
      "total_backward_count 822360 real_backward_count 70061   8.520%\n",
      "fc layer 1 self.abs_max_out: 3538.0\n",
      "fc layer 1 self.abs_max_out: 3581.0\n",
      "lif layer 1 self.abs_max_v: 6437.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.030043/  1.314019, val:  75.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5186%\n",
      "layer   2  Sparsity: 69.8358%\n",
      "layer   3  Sparsity: 66.1907%\n",
      "total_backward_count 832150 real_backward_count 70695   8.495%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.027070/  1.311686, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4559%\n",
      "layer   2  Sparsity: 70.4201%\n",
      "layer   3  Sparsity: 66.0801%\n",
      "total_backward_count 841940 real_backward_count 71254   8.463%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.024817/  1.304813, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5411%\n",
      "layer   2  Sparsity: 69.9333%\n",
      "layer   3  Sparsity: 65.8327%\n",
      "total_backward_count 851730 real_backward_count 71817   8.432%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.022652/  1.321417, val:  76.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5153%\n",
      "layer   2  Sparsity: 69.8410%\n",
      "layer   3  Sparsity: 65.6641%\n",
      "total_backward_count 861520 real_backward_count 72375   8.401%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.007865/  1.301889, val:  76.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4663%\n",
      "layer   2  Sparsity: 69.9167%\n",
      "layer   3  Sparsity: 66.0853%\n",
      "total_backward_count 871310 real_backward_count 72936   8.371%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  0.989862/  1.313108, val:  77.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4957%\n",
      "layer   2  Sparsity: 70.1726%\n",
      "layer   3  Sparsity: 66.3360%\n",
      "total_backward_count 881100 real_backward_count 73476   8.339%\n",
      "fc layer 1 self.abs_max_out: 3753.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  0.987717/  1.361537, val:  70.00%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4901%\n",
      "layer   2  Sparsity: 69.9106%\n",
      "layer   3  Sparsity: 65.8264%\n",
      "total_backward_count 890890 real_backward_count 74030   8.310%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  0.975241/  1.246203, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4972%\n",
      "layer   2  Sparsity: 69.9690%\n",
      "layer   3  Sparsity: 65.3449%\n",
      "total_backward_count 900680 real_backward_count 74585   8.281%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  0.981470/  1.262778, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4601%\n",
      "layer   2  Sparsity: 70.4106%\n",
      "layer   3  Sparsity: 65.5634%\n",
      "total_backward_count 910470 real_backward_count 75129   8.252%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  0.971756/  1.297186, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5016%\n",
      "layer   2  Sparsity: 70.3712%\n",
      "layer   3  Sparsity: 65.4443%\n",
      "total_backward_count 920260 real_backward_count 75686   8.224%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  0.967445/  1.257677, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5048%\n",
      "layer   2  Sparsity: 70.2824%\n",
      "layer   3  Sparsity: 65.5999%\n",
      "total_backward_count 930050 real_backward_count 76232   8.197%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  0.963557/  1.289759, val:  75.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5321%\n",
      "layer   2  Sparsity: 70.0217%\n",
      "layer   3  Sparsity: 65.5445%\n",
      "total_backward_count 939840 real_backward_count 76783   8.170%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  0.964444/  1.348194, val:  65.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4841%\n",
      "layer   2  Sparsity: 70.1908%\n",
      "layer   3  Sparsity: 65.7097%\n",
      "total_backward_count 949630 real_backward_count 77258   8.136%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  0.955009/  1.274711, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5122%\n",
      "layer   2  Sparsity: 70.2991%\n",
      "layer   3  Sparsity: 66.5506%\n",
      "total_backward_count 959420 real_backward_count 77786   8.108%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  0.958959/  1.330624, val:  77.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4804%\n",
      "layer   2  Sparsity: 70.3002%\n",
      "layer   3  Sparsity: 66.9166%\n",
      "total_backward_count 969210 real_backward_count 78262   8.075%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  0.972396/  1.323569, val:  86.67%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5338%\n",
      "layer   2  Sparsity: 69.9168%\n",
      "layer   3  Sparsity: 66.7884%\n",
      "total_backward_count 979000 real_backward_count 78733   8.042%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  0.971231/  1.275948, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4953%\n",
      "layer   2  Sparsity: 69.4492%\n",
      "layer   3  Sparsity: 66.4584%\n",
      "total_backward_count 988790 real_backward_count 79213   8.011%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  0.985352/  1.295120, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4913%\n",
      "layer   2  Sparsity: 69.6077%\n",
      "layer   3  Sparsity: 66.7550%\n",
      "total_backward_count 998580 real_backward_count 79739   7.985%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  0.975776/  1.287326, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5004%\n",
      "layer   2  Sparsity: 69.5589%\n",
      "layer   3  Sparsity: 66.4309%\n",
      "total_backward_count 1008370 real_backward_count 80204   7.954%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  0.983778/  1.301892, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4717%\n",
      "layer   2  Sparsity: 69.6167%\n",
      "layer   3  Sparsity: 66.3732%\n",
      "total_backward_count 1018160 real_backward_count 80666   7.923%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  0.983372/  1.310202, val:  74.58%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4743%\n",
      "layer   2  Sparsity: 70.0206%\n",
      "layer   3  Sparsity: 66.9397%\n",
      "total_backward_count 1027950 real_backward_count 81172   7.896%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  0.999583/  1.289710, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4741%\n",
      "layer   2  Sparsity: 70.2894%\n",
      "layer   3  Sparsity: 66.7100%\n",
      "total_backward_count 1037740 real_backward_count 81692   7.872%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  0.981496/  1.325765, val:  66.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4840%\n",
      "layer   2  Sparsity: 70.0121%\n",
      "layer   3  Sparsity: 66.7224%\n",
      "total_backward_count 1047530 real_backward_count 82122   7.840%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  0.986218/  1.287920, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4920%\n",
      "layer   2  Sparsity: 69.8720%\n",
      "layer   3  Sparsity: 66.8378%\n",
      "total_backward_count 1057320 real_backward_count 82615   7.814%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  0.992926/  1.313401, val:  80.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4842%\n",
      "layer   2  Sparsity: 70.2463%\n",
      "layer   3  Sparsity: 66.8387%\n",
      "total_backward_count 1067110 real_backward_count 83101   7.787%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  0.992802/  1.277225, val:  80.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4417%\n",
      "layer   2  Sparsity: 70.3381%\n",
      "layer   3  Sparsity: 67.0979%\n",
      "total_backward_count 1076900 real_backward_count 83584   7.762%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  0.978884/  1.296247, val:  78.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4916%\n",
      "layer   2  Sparsity: 70.4628%\n",
      "layer   3  Sparsity: 67.2693%\n",
      "total_backward_count 1086690 real_backward_count 84077   7.737%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  0.979463/  1.287662, val:  76.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.23 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 88.4657%\n",
      "layer   2  Sparsity: 70.1180%\n",
      "layer   3  Sparsity: 67.7307%\n",
      "total_backward_count 1096480 real_backward_count 84581   7.714%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  0.972302/  1.277251, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4571%\n",
      "layer   2  Sparsity: 69.9965%\n",
      "layer   3  Sparsity: 67.6363%\n",
      "total_backward_count 1106270 real_backward_count 85033   7.686%\n",
      "lif layer 1 self.abs_max_v: 6524.5\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  0.944378/  1.255433, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5206%\n",
      "layer   2  Sparsity: 70.0721%\n",
      "layer   3  Sparsity: 67.6557%\n",
      "total_backward_count 1116060 real_backward_count 85492   7.660%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  0.947127/  1.284867, val:  77.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4624%\n",
      "layer   2  Sparsity: 70.5508%\n",
      "layer   3  Sparsity: 67.9084%\n",
      "total_backward_count 1125850 real_backward_count 85913   7.631%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  0.933295/  1.235685, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5110%\n",
      "layer   2  Sparsity: 70.7674%\n",
      "layer   3  Sparsity: 67.8655%\n",
      "total_backward_count 1135640 real_backward_count 86360   7.605%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  0.940629/  1.254902, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4946%\n",
      "layer   2  Sparsity: 70.6838%\n",
      "layer   3  Sparsity: 67.5583%\n",
      "total_backward_count 1145430 real_backward_count 86806   7.578%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  0.924591/  1.246969, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4515%\n",
      "layer   2  Sparsity: 70.5932%\n",
      "layer   3  Sparsity: 67.2144%\n",
      "total_backward_count 1155220 real_backward_count 87224   7.550%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  0.927370/  1.269331, val:  66.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4983%\n",
      "layer   2  Sparsity: 70.3713%\n",
      "layer   3  Sparsity: 67.0480%\n",
      "total_backward_count 1165010 real_backward_count 87614   7.520%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  0.928391/  1.259549, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4999%\n",
      "layer   2  Sparsity: 70.4621%\n",
      "layer   3  Sparsity: 66.5032%\n",
      "total_backward_count 1174800 real_backward_count 88043   7.494%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  0.929162/  1.265761, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4953%\n",
      "layer   2  Sparsity: 70.2934%\n",
      "layer   3  Sparsity: 67.2295%\n",
      "total_backward_count 1184590 real_backward_count 88461   7.468%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  0.942175/  1.281948, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4792%\n",
      "layer   2  Sparsity: 70.5921%\n",
      "layer   3  Sparsity: 66.7707%\n",
      "total_backward_count 1194380 real_backward_count 88931   7.446%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  0.943026/  1.254817, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4674%\n",
      "layer   2  Sparsity: 70.5053%\n",
      "layer   3  Sparsity: 67.2672%\n",
      "total_backward_count 1204170 real_backward_count 89378   7.422%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  0.939138/  1.239886, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4998%\n",
      "layer   2  Sparsity: 70.4084%\n",
      "layer   3  Sparsity: 67.2559%\n",
      "total_backward_count 1213960 real_backward_count 89769   7.395%\n",
      "lif layer 1 self.abs_max_v: 6576.5\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  0.935525/  1.217963, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5323%\n",
      "layer   2  Sparsity: 70.1643%\n",
      "layer   3  Sparsity: 66.9973%\n",
      "total_backward_count 1223750 real_backward_count 90211   7.372%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  0.934977/  1.237411, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4242%\n",
      "layer   2  Sparsity: 69.8671%\n",
      "layer   3  Sparsity: 66.9557%\n",
      "total_backward_count 1233540 real_backward_count 90650   7.349%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  0.937333/  1.259713, val:  75.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4978%\n",
      "layer   2  Sparsity: 70.0338%\n",
      "layer   3  Sparsity: 66.9498%\n",
      "total_backward_count 1243330 real_backward_count 91097   7.327%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  0.937430/  1.272848, val:  75.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4501%\n",
      "layer   2  Sparsity: 69.9829%\n",
      "layer   3  Sparsity: 67.7582%\n",
      "total_backward_count 1253120 real_backward_count 91490   7.301%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  0.931423/  1.234923, val:  81.25%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4546%\n",
      "layer   2  Sparsity: 69.8169%\n",
      "layer   3  Sparsity: 67.7256%\n",
      "total_backward_count 1262910 real_backward_count 91892   7.276%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  0.923485/  1.205921, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4792%\n",
      "layer   2  Sparsity: 70.0725%\n",
      "layer   3  Sparsity: 67.6019%\n",
      "total_backward_count 1272700 real_backward_count 92295   7.252%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  0.903129/  1.212833, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.80 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4899%\n",
      "layer   2  Sparsity: 70.1840%\n",
      "layer   3  Sparsity: 67.0175%\n",
      "total_backward_count 1282490 real_backward_count 92731   7.231%\n",
      "fc layer 3 self.abs_max_out: 873.0\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  0.913824/  1.213031, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4777%\n",
      "layer   2  Sparsity: 69.8568%\n",
      "layer   3  Sparsity: 67.3424%\n",
      "total_backward_count 1292280 real_backward_count 93169   7.210%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  0.900614/  1.238660, val:  82.50%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5048%\n",
      "layer   2  Sparsity: 69.9420%\n",
      "layer   3  Sparsity: 67.3194%\n",
      "total_backward_count 1302070 real_backward_count 93525   7.183%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  0.921486/  1.270037, val:  77.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5222%\n",
      "layer   2  Sparsity: 70.2100%\n",
      "layer   3  Sparsity: 67.3224%\n",
      "total_backward_count 1311860 real_backward_count 93923   7.160%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  0.909335/  1.219431, val:  75.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5017%\n",
      "layer   2  Sparsity: 70.2714%\n",
      "layer   3  Sparsity: 67.3686%\n",
      "total_backward_count 1321650 real_backward_count 94339   7.138%\n",
      "fc layer 1 self.abs_max_out: 3822.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  0.903620/  1.216171, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4983%\n",
      "layer   2  Sparsity: 70.1365%\n",
      "layer   3  Sparsity: 67.4072%\n",
      "total_backward_count 1331440 real_backward_count 94731   7.115%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  0.891072/  1.204689, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4891%\n",
      "layer   2  Sparsity: 70.1467%\n",
      "layer   3  Sparsity: 67.7941%\n",
      "total_backward_count 1341230 real_backward_count 95093   7.090%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  0.907406/  1.243698, val:  77.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4523%\n",
      "layer   2  Sparsity: 69.6218%\n",
      "layer   3  Sparsity: 67.7879%\n",
      "total_backward_count 1351020 real_backward_count 95497   7.069%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  0.905809/  1.217703, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4960%\n",
      "layer   2  Sparsity: 69.5641%\n",
      "layer   3  Sparsity: 66.9650%\n",
      "total_backward_count 1360810 real_backward_count 95894   7.047%\n",
      "lif layer 1 self.abs_max_v: 6581.0\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  0.902769/  1.205220, val:  77.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4771%\n",
      "layer   2  Sparsity: 69.8541%\n",
      "layer   3  Sparsity: 67.2181%\n",
      "total_backward_count 1370600 real_backward_count 96272   7.024%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  0.881728/  1.211647, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5018%\n",
      "layer   2  Sparsity: 69.5989%\n",
      "layer   3  Sparsity: 67.3286%\n",
      "total_backward_count 1380390 real_backward_count 96629   7.000%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  0.885266/  1.208111, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5141%\n",
      "layer   2  Sparsity: 69.9775%\n",
      "layer   3  Sparsity: 66.9410%\n",
      "total_backward_count 1390180 real_backward_count 96978   6.976%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  0.890701/  1.221194, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5185%\n",
      "layer   2  Sparsity: 70.0479%\n",
      "layer   3  Sparsity: 66.8453%\n",
      "total_backward_count 1399970 real_backward_count 97328   6.952%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  0.891154/  1.211930, val:  80.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4674%\n",
      "layer   2  Sparsity: 70.2299%\n",
      "layer   3  Sparsity: 66.7674%\n",
      "total_backward_count 1409760 real_backward_count 97640   6.926%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  0.874507/  1.206888, val:  80.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4416%\n",
      "layer   2  Sparsity: 70.1781%\n",
      "layer   3  Sparsity: 67.4703%\n",
      "total_backward_count 1419550 real_backward_count 97982   6.902%\n",
      "fc layer 1 self.abs_max_out: 4161.0\n",
      "lif layer 1 self.abs_max_v: 6825.5\n",
      "fc layer 3 self.abs_max_out: 877.0\n",
      "fc layer 3 self.abs_max_out: 880.0\n",
      "fc layer 3 self.abs_max_out: 919.0\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  0.869604/  1.205356, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4759%\n",
      "layer   2  Sparsity: 70.1219%\n",
      "layer   3  Sparsity: 67.4766%\n",
      "total_backward_count 1429340 real_backward_count 98321   6.879%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  0.882529/  1.234006, val:  77.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4916%\n",
      "layer   2  Sparsity: 69.5761%\n",
      "layer   3  Sparsity: 67.4238%\n",
      "total_backward_count 1439130 real_backward_count 98702   6.858%\n",
      "lif layer 1 self.abs_max_v: 6846.5\n",
      "lif layer 1 self.abs_max_v: 7186.5\n",
      "fc layer 1 self.abs_max_out: 4228.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  0.876427/  1.208390, val:  80.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4679%\n",
      "layer   2  Sparsity: 69.5717%\n",
      "layer   3  Sparsity: 67.5424%\n",
      "total_backward_count 1448920 real_backward_count 99036   6.835%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  0.867876/  1.207667, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 88.5059%\n",
      "layer   2  Sparsity: 69.9976%\n",
      "layer   3  Sparsity: 67.2431%\n",
      "total_backward_count 1458710 real_backward_count 99394   6.814%\n",
      "fc layer 1 self.abs_max_out: 4288.0\n",
      "lif layer 1 self.abs_max_v: 7427.0\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  0.871208/  1.192425, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.21 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5203%\n",
      "layer   2  Sparsity: 70.0655%\n",
      "layer   3  Sparsity: 67.2181%\n",
      "total_backward_count 1468500 real_backward_count 99719   6.791%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  0.862240/  1.193069, val:  79.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5097%\n",
      "layer   2  Sparsity: 70.0270%\n",
      "layer   3  Sparsity: 67.3893%\n",
      "total_backward_count 1478290 real_backward_count 100051   6.768%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  0.853815/  1.155931, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4547%\n",
      "layer   2  Sparsity: 70.1223%\n",
      "layer   3  Sparsity: 67.2628%\n",
      "total_backward_count 1488080 real_backward_count 100389   6.746%\n",
      "lif layer 1 self.abs_max_v: 7428.0\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  0.847679/  1.180755, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4904%\n",
      "layer   2  Sparsity: 69.9910%\n",
      "layer   3  Sparsity: 68.0979%\n",
      "total_backward_count 1497870 real_backward_count 100703   6.723%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  0.862384/  1.179048, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4610%\n",
      "layer   2  Sparsity: 70.3814%\n",
      "layer   3  Sparsity: 68.4545%\n",
      "total_backward_count 1507660 real_backward_count 101065   6.703%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  0.866780/  1.190622, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5003%\n",
      "layer   2  Sparsity: 70.4296%\n",
      "layer   3  Sparsity: 67.9109%\n",
      "total_backward_count 1517450 real_backward_count 101405   6.683%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  0.870893/  1.203809, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5094%\n",
      "layer   2  Sparsity: 70.3409%\n",
      "layer   3  Sparsity: 67.6506%\n",
      "total_backward_count 1527240 real_backward_count 101710   6.660%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  0.872280/  1.211864, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5092%\n",
      "layer   2  Sparsity: 70.5340%\n",
      "layer   3  Sparsity: 67.8164%\n",
      "total_backward_count 1537030 real_backward_count 102069   6.641%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  0.861375/  1.173056, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4693%\n",
      "layer   2  Sparsity: 70.4894%\n",
      "layer   3  Sparsity: 68.1297%\n",
      "total_backward_count 1546820 real_backward_count 102390   6.619%\n",
      "fc layer 1 self.abs_max_out: 4386.0\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  0.873529/  1.214648, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4645%\n",
      "layer   2  Sparsity: 70.4701%\n",
      "layer   3  Sparsity: 67.9352%\n",
      "total_backward_count 1556610 real_backward_count 102709   6.598%\n",
      "lif layer 1 self.abs_max_v: 7522.5\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  0.882001/  1.220103, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5354%\n",
      "layer   2  Sparsity: 70.5031%\n",
      "layer   3  Sparsity: 67.6250%\n",
      "total_backward_count 1566400 real_backward_count 103046   6.579%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  0.865567/  1.214278, val:  79.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4963%\n",
      "layer   2  Sparsity: 70.5507%\n",
      "layer   3  Sparsity: 67.6496%\n",
      "total_backward_count 1576190 real_backward_count 103360   6.558%\n",
      "fc layer 3 self.abs_max_out: 929.0\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  0.865422/  1.183921, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4812%\n",
      "layer   2  Sparsity: 70.2895%\n",
      "layer   3  Sparsity: 67.7661%\n",
      "total_backward_count 1585980 real_backward_count 103715   6.539%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  0.850426/  1.209353, val:  78.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4833%\n",
      "layer   2  Sparsity: 70.3465%\n",
      "layer   3  Sparsity: 67.9840%\n",
      "total_backward_count 1595770 real_backward_count 104000   6.517%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  0.857566/  1.204610, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4647%\n",
      "layer   2  Sparsity: 70.4543%\n",
      "layer   3  Sparsity: 67.8048%\n",
      "total_backward_count 1605560 real_backward_count 104336   6.498%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  0.859589/  1.216436, val:  76.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4975%\n",
      "layer   2  Sparsity: 70.4403%\n",
      "layer   3  Sparsity: 67.2699%\n",
      "total_backward_count 1615350 real_backward_count 104635   6.478%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  0.853272/  1.198301, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4671%\n",
      "layer   2  Sparsity: 70.1150%\n",
      "layer   3  Sparsity: 67.4994%\n",
      "total_backward_count 1625140 real_backward_count 104957   6.458%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  0.858066/  1.192796, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4698%\n",
      "layer   2  Sparsity: 69.8835%\n",
      "layer   3  Sparsity: 67.4151%\n",
      "total_backward_count 1634930 real_backward_count 105275   6.439%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  0.856279/  1.204397, val:  73.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4894%\n",
      "layer   2  Sparsity: 69.8709%\n",
      "layer   3  Sparsity: 67.5513%\n",
      "total_backward_count 1644720 real_backward_count 105552   6.418%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  0.845925/  1.168398, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4844%\n",
      "layer   2  Sparsity: 70.0321%\n",
      "layer   3  Sparsity: 67.5018%\n",
      "total_backward_count 1654510 real_backward_count 105868   6.399%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  0.840979/  1.172868, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4661%\n",
      "layer   2  Sparsity: 70.1890%\n",
      "layer   3  Sparsity: 68.0713%\n",
      "total_backward_count 1664300 real_backward_count 106159   6.379%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  0.840359/  1.161649, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5333%\n",
      "layer   2  Sparsity: 69.9904%\n",
      "layer   3  Sparsity: 68.0819%\n",
      "total_backward_count 1674090 real_backward_count 106406   6.356%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  0.847276/  1.174446, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4402%\n",
      "layer   2  Sparsity: 70.2276%\n",
      "layer   3  Sparsity: 67.6884%\n",
      "total_backward_count 1683880 real_backward_count 106706   6.337%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  0.840502/  1.165779, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5049%\n",
      "layer   2  Sparsity: 70.3226%\n",
      "layer   3  Sparsity: 68.0372%\n",
      "total_backward_count 1693670 real_backward_count 107004   6.318%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  0.835408/  1.176001, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4847%\n",
      "layer   2  Sparsity: 70.2889%\n",
      "layer   3  Sparsity: 68.1941%\n",
      "total_backward_count 1703460 real_backward_count 107273   6.297%\n",
      "fc layer 3 self.abs_max_out: 937.0\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  0.841380/  1.195957, val:  76.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5232%\n",
      "layer   2  Sparsity: 69.9899%\n",
      "layer   3  Sparsity: 68.1744%\n",
      "total_backward_count 1713250 real_backward_count 107539   6.277%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  0.842152/  1.145046, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4544%\n",
      "layer   2  Sparsity: 69.9003%\n",
      "layer   3  Sparsity: 68.2346%\n",
      "total_backward_count 1723040 real_backward_count 107806   6.257%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  0.828463/  1.166268, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4891%\n",
      "layer   2  Sparsity: 69.5957%\n",
      "layer   3  Sparsity: 68.3465%\n",
      "total_backward_count 1732830 real_backward_count 108079   6.237%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  0.829717/  1.157202, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4287%\n",
      "layer   2  Sparsity: 69.9167%\n",
      "layer   3  Sparsity: 68.3745%\n",
      "total_backward_count 1742620 real_backward_count 108339   6.217%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  0.839770/  1.177127, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4344%\n",
      "layer   2  Sparsity: 69.8059%\n",
      "layer   3  Sparsity: 68.4901%\n",
      "total_backward_count 1752410 real_backward_count 108604   6.197%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  0.843909/  1.151702, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4565%\n",
      "layer   2  Sparsity: 70.1554%\n",
      "layer   3  Sparsity: 68.3653%\n",
      "total_backward_count 1762200 real_backward_count 108879   6.179%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  0.829929/  1.157339, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5148%\n",
      "layer   2  Sparsity: 70.3753%\n",
      "layer   3  Sparsity: 67.8460%\n",
      "total_backward_count 1771990 real_backward_count 109122   6.158%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  0.827644/  1.150639, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4641%\n",
      "layer   2  Sparsity: 70.1960%\n",
      "layer   3  Sparsity: 68.0822%\n",
      "total_backward_count 1781780 real_backward_count 109411   6.141%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  0.818123/  1.181815, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4501%\n",
      "layer   2  Sparsity: 69.8548%\n",
      "layer   3  Sparsity: 67.9866%\n",
      "total_backward_count 1791570 real_backward_count 109660   6.121%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  0.821017/  1.166468, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4418%\n",
      "layer   2  Sparsity: 69.7758%\n",
      "layer   3  Sparsity: 68.0119%\n",
      "total_backward_count 1801360 real_backward_count 109927   6.102%\n",
      "fc layer 3 self.abs_max_out: 944.0\n",
      "fc layer 3 self.abs_max_out: 947.0\n",
      "fc layer 3 self.abs_max_out: 953.0\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  0.828767/  1.151832, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4811%\n",
      "layer   2  Sparsity: 69.9143%\n",
      "layer   3  Sparsity: 67.7270%\n",
      "total_backward_count 1811150 real_backward_count 110194   6.084%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  0.813013/  1.120876, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4638%\n",
      "layer   2  Sparsity: 70.0497%\n",
      "layer   3  Sparsity: 67.5743%\n",
      "total_backward_count 1820940 real_backward_count 110464   6.066%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  0.821664/  1.150095, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5084%\n",
      "layer   2  Sparsity: 70.1250%\n",
      "layer   3  Sparsity: 67.2681%\n",
      "total_backward_count 1830730 real_backward_count 110733   6.049%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  0.817356/  1.145487, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4894%\n",
      "layer   2  Sparsity: 69.9651%\n",
      "layer   3  Sparsity: 67.0952%\n",
      "total_backward_count 1840520 real_backward_count 110967   6.029%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  0.821449/  1.146302, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5178%\n",
      "layer   2  Sparsity: 70.2144%\n",
      "layer   3  Sparsity: 67.1498%\n",
      "total_backward_count 1850310 real_backward_count 111193   6.009%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  0.813684/  1.154103, val:  81.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4987%\n",
      "layer   2  Sparsity: 70.0750%\n",
      "layer   3  Sparsity: 67.3532%\n",
      "total_backward_count 1860100 real_backward_count 111426   5.990%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  0.802006/  1.135839, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4984%\n",
      "layer   2  Sparsity: 70.2760%\n",
      "layer   3  Sparsity: 67.3090%\n",
      "total_backward_count 1869890 real_backward_count 111673   5.972%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  0.801845/  1.139706, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4826%\n",
      "layer   2  Sparsity: 70.1228%\n",
      "layer   3  Sparsity: 67.7119%\n",
      "total_backward_count 1879680 real_backward_count 111891   5.953%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  0.808185/  1.135541, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4872%\n",
      "layer   2  Sparsity: 70.1264%\n",
      "layer   3  Sparsity: 67.6154%\n",
      "total_backward_count 1889470 real_backward_count 112117   5.934%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  0.805027/  1.131813, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4368%\n",
      "layer   2  Sparsity: 70.3894%\n",
      "layer   3  Sparsity: 66.9372%\n",
      "total_backward_count 1899260 real_backward_count 112395   5.918%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  0.797044/  1.146092, val:  79.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4315%\n",
      "layer   2  Sparsity: 70.2126%\n",
      "layer   3  Sparsity: 66.6823%\n",
      "total_backward_count 1909050 real_backward_count 112657   5.901%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  0.795346/  1.120126, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4749%\n",
      "layer   2  Sparsity: 70.1101%\n",
      "layer   3  Sparsity: 67.1086%\n",
      "total_backward_count 1918840 real_backward_count 112896   5.884%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  0.782150/  1.127406, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4653%\n",
      "layer   2  Sparsity: 70.4051%\n",
      "layer   3  Sparsity: 67.2044%\n",
      "total_backward_count 1928630 real_backward_count 113166   5.868%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  0.790711/  1.148930, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5042%\n",
      "layer   2  Sparsity: 70.3538%\n",
      "layer   3  Sparsity: 66.9865%\n",
      "total_backward_count 1938420 real_backward_count 113436   5.852%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  0.793517/  1.130294, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5052%\n",
      "layer   2  Sparsity: 70.4744%\n",
      "layer   3  Sparsity: 67.4936%\n",
      "total_backward_count 1948210 real_backward_count 113662   5.834%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  0.773244/  1.140640, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4473%\n",
      "layer   2  Sparsity: 70.2598%\n",
      "layer   3  Sparsity: 67.7342%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d6c762f2884fcabb9bf8c9d65d08b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñà‚ñÉ‚ñà‚ñÉ‚ñà‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.77324</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.83333</td></tr><tr><td>val_loss</td><td>1.14064</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">likely-sweep-346</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f4cp2b6i' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f4cp2b6i</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251128_055518-f4cp2b6i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hyaiofsy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251128_100808-hyaiofsy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hyaiofsy' target=\"_blank\">glad-sweep-351</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hyaiofsy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hyaiofsy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251128_100817_841', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 20, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 43471ed3c7f31ff42498182012c432a5\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=20, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=20, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 282\n",
      "fc layer 1 self.abs_max_out: 943.0\n",
      "lif layer 1 self.abs_max_v: 943.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 808.0\n",
      "lif layer 2 self.abs_max_v: 808.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "lif layer 1 self.abs_max_v: 984.5\n",
      "lif layer 2 self.abs_max_v: 895.0\n",
      "fc layer 1 self.abs_max_out: 1240.0\n",
      "lif layer 1 self.abs_max_v: 1441.5\n",
      "fc layer 2 self.abs_max_out: 977.0\n",
      "lif layer 2 self.abs_max_v: 1285.0\n",
      "fc layer 3 self.abs_max_out: 446.0\n",
      "smallest_now_T updated: 254\n",
      "lif layer 1 self.abs_max_v: 1546.5\n",
      "fc layer 2 self.abs_max_out: 1162.0\n",
      "lif layer 2 self.abs_max_v: 1294.5\n",
      "fc layer 1 self.abs_max_out: 1378.0\n",
      "fc layer 2 self.abs_max_out: 1309.0\n",
      "lif layer 2 self.abs_max_v: 1563.0\n",
      "fc layer 1 self.abs_max_out: 1667.0\n",
      "lif layer 1 self.abs_max_v: 1779.0\n",
      "fc layer 2 self.abs_max_out: 1441.0\n",
      "lif layer 2 self.abs_max_v: 2132.5\n",
      "fc layer 1 self.abs_max_out: 1843.0\n",
      "lif layer 1 self.abs_max_v: 2210.5\n",
      "fc layer 2 self.abs_max_out: 1768.0\n",
      "lif layer 2 self.abs_max_v: 2175.5\n",
      "smallest_now_T updated: 193\n",
      "fc layer 1 self.abs_max_out: 2249.0\n",
      "lif layer 1 self.abs_max_v: 2249.0\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "fc layer 3 self.abs_max_out: 619.0\n",
      "lif layer 2 self.abs_max_v: 2448.5\n",
      "fc layer 3 self.abs_max_out: 626.0\n",
      "fc layer 1 self.abs_max_out: 2277.0\n",
      "lif layer 1 self.abs_max_v: 2277.0\n",
      "fc layer 3 self.abs_max_out: 693.0\n",
      "lif layer 1 self.abs_max_v: 2771.5\n",
      "fc layer 2 self.abs_max_out: 1871.0\n",
      "lif layer 2 self.abs_max_v: 2525.0\n",
      "fc layer 2 self.abs_max_out: 2077.0\n",
      "fc layer 1 self.abs_max_out: 2602.0\n",
      "lif layer 2 self.abs_max_v: 2697.0\n",
      "lif layer 1 self.abs_max_v: 2812.5\n",
      "fc layer 3 self.abs_max_out: 753.0\n",
      "smallest_now_T updated: 163\n",
      "fc layer 1 self.abs_max_out: 3136.0\n",
      "lif layer 1 self.abs_max_v: 3136.0\n",
      "fc layer 1 self.abs_max_out: 3445.0\n",
      "lif layer 1 self.abs_max_v: 3445.0\n",
      "lif layer 1 self.abs_max_v: 3795.5\n",
      "lif layer 2 self.abs_max_v: 3124.5\n",
      "lif layer 1 self.abs_max_v: 3991.0\n",
      "fc layer 1 self.abs_max_out: 3534.0\n",
      "fc layer 3 self.abs_max_out: 804.0\n",
      "fc layer 2 self.abs_max_out: 2108.0\n",
      "fc layer 3 self.abs_max_out: 985.0\n",
      "fc layer 2 self.abs_max_out: 2609.0\n",
      "smallest_now_T updated: 150\n",
      "fc layer 1 self.abs_max_out: 3672.0\n",
      "fc layer 1 self.abs_max_out: 4234.0\n",
      "lif layer 1 self.abs_max_v: 4234.0\n",
      "lif layer 1 self.abs_max_v: 4556.0\n",
      "fc layer 2 self.abs_max_out: 2654.0\n",
      "lif layer 2 self.abs_max_v: 3139.5\n",
      "lif layer 2 self.abs_max_v: 3238.0\n",
      "smallest_now_T updated: 135\n",
      "lif layer 2 self.abs_max_v: 3407.0\n",
      "lif layer 2 self.abs_max_v: 3446.5\n",
      "lif layer 2 self.abs_max_v: 3501.5\n",
      "lif layer 1 self.abs_max_v: 4580.5\n",
      "lif layer 1 self.abs_max_v: 4601.5\n",
      "lif layer 2 self.abs_max_v: 3566.5\n",
      "lif layer 2 self.abs_max_v: 4268.5\n",
      "fc layer 3 self.abs_max_out: 1025.0\n",
      "fc layer 3 self.abs_max_out: 1118.0\n",
      "smallest_now_T updated: 116\n",
      "smallest_now_T updated: 90\n",
      "fc layer 2 self.abs_max_out: 2677.0\n",
      "fc layer 1 self.abs_max_out: 4255.0\n",
      "fc layer 2 self.abs_max_out: 2694.0\n",
      "lif layer 1 self.abs_max_v: 4711.5\n",
      "lif layer 1 self.abs_max_v: 5206.0\n",
      "fc layer 2 self.abs_max_out: 2703.0\n",
      "fc layer 2 self.abs_max_out: 2736.0\n",
      "fc layer 2 self.abs_max_out: 2753.0\n",
      "fc layer 2 self.abs_max_out: 2777.0\n",
      "fc layer 1 self.abs_max_out: 4683.0\n",
      "fc layer 1 self.abs_max_out: 4753.0\n",
      "fc layer 1 self.abs_max_out: 4876.0\n",
      "fc layer 2 self.abs_max_out: 2860.0\n",
      "lif layer 1 self.abs_max_v: 5231.0\n",
      "lif layer 1 self.abs_max_v: 5926.5\n",
      "lif layer 1 self.abs_max_v: 6062.5\n",
      "lif layer 2 self.abs_max_v: 4418.0\n",
      "fc layer 2 self.abs_max_out: 2975.0\n",
      "fc layer 1 self.abs_max_out: 5103.0\n",
      "lif layer 1 self.abs_max_v: 6627.5\n",
      "fc layer 3 self.abs_max_out: 1137.0\n",
      "fc layer 3 self.abs_max_out: 1149.0\n",
      "fc layer 3 self.abs_max_out: 1150.0\n",
      "fc layer 3 self.abs_max_out: 1237.0\n",
      "fc layer 1 self.abs_max_out: 5167.0\n",
      "fc layer 1 self.abs_max_out: 5303.0\n",
      "lif layer 1 self.abs_max_v: 7227.0\n",
      "lif layer 1 self.abs_max_v: 7288.5\n",
      "fc layer 1 self.abs_max_out: 5822.0\n",
      "fc layer 1 self.abs_max_out: 5880.0\n",
      "lif layer 2 self.abs_max_v: 4444.0\n",
      "smallest_now_T_val updated: 262\n",
      "smallest_now_T_val updated: 217\n",
      "smallest_now_T_val updated: 213\n",
      "smallest_now_T_val updated: 209\n",
      "smallest_now_T_val updated: 174\n",
      "smallest_now_T_val updated: 63\n",
      "fc layer 1 self.abs_max_out: 5985.0\n",
      "fc layer 1 self.abs_max_out: 6099.0\n",
      "lif layer 1 self.abs_max_v: 7348.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.941936/  2.059081, val:  38.75%, val_best:  38.75%, tr:  79.88%, tr_best:  79.88%, epoch time: 42.13 seconds, 0.70 minutes\n",
      "layer   1  Sparsity: 88.4448%\n",
      "layer   2  Sparsity: 79.0795%\n",
      "layer   3  Sparsity: 77.8194%\n",
      "total_backward_count 4895 real_backward_count 1860  37.998%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 1407.0\n",
      "fc layer 1 self.abs_max_out: 6222.0\n",
      "fc layer 1 self.abs_max_out: 6786.0\n",
      "fc layer 2 self.abs_max_out: 3028.0\n",
      "fc layer 2 self.abs_max_out: 3260.0\n",
      "lif layer 2 self.abs_max_v: 4448.5\n",
      "lif layer 1 self.abs_max_v: 7741.5\n",
      "lif layer 1 self.abs_max_v: 7883.5\n",
      "lif layer 1 self.abs_max_v: 8000.0\n",
      "lif layer 1 self.abs_max_v: 9683.5\n",
      "lif layer 1 self.abs_max_v: 9786.0\n",
      "fc layer 1 self.abs_max_out: 6871.0\n",
      "fc layer 1 self.abs_max_out: 6927.0\n",
      "fc layer 2 self.abs_max_out: 3415.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.887035/  2.043676, val:  43.75%, val_best:  43.75%, tr:  87.03%, tr_best:  87.03%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4171%\n",
      "layer   2  Sparsity: 78.9888%\n",
      "layer   3  Sparsity: 78.4579%\n",
      "total_backward_count 9790 real_backward_count 3344  34.157%\n",
      "fc layer 1 self.abs_max_out: 6960.0\n",
      "fc layer 3 self.abs_max_out: 1466.0\n",
      "lif layer 2 self.abs_max_v: 4524.0\n",
      "fc layer 3 self.abs_max_out: 1484.0\n",
      "fc layer 1 self.abs_max_out: 7094.0\n",
      "fc layer 1 self.abs_max_out: 7123.0\n",
      "lif layer 2 self.abs_max_v: 4543.5\n",
      "fc layer 1 self.abs_max_out: 7320.0\n",
      "fc layer 1 self.abs_max_out: 7330.0\n",
      "lif layer 2 self.abs_max_v: 4604.0\n",
      "lif layer 1 self.abs_max_v: 10000.5\n",
      "lif layer 1 self.abs_max_v: 10186.5\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.875134/  2.007449, val:  40.83%, val_best:  43.75%, tr:  90.50%, tr_best:  90.50%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.4063%\n",
      "layer   2  Sparsity: 78.5774%\n",
      "layer   3  Sparsity: 78.5837%\n",
      "total_backward_count 14685 real_backward_count 4678  31.856%\n",
      "fc layer 1 self.abs_max_out: 7334.0\n",
      "fc layer 1 self.abs_max_out: 7517.0\n",
      "lif layer 1 self.abs_max_v: 10212.5\n",
      "lif layer 1 self.abs_max_v: 10476.5\n",
      "lif layer 1 self.abs_max_v: 10604.5\n",
      "fc layer 2 self.abs_max_out: 3473.0\n",
      "lif layer 1 self.abs_max_v: 11626.5\n",
      "lif layer 1 self.abs_max_v: 11780.5\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.864133/  2.038837, val:  37.08%, val_best:  43.75%, tr:  90.81%, tr_best:  90.81%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.4355%\n",
      "layer   2  Sparsity: 79.1683%\n",
      "layer   3  Sparsity: 78.7521%\n",
      "total_backward_count 19580 real_backward_count 5993  30.608%\n",
      "fc layer 2 self.abs_max_out: 3498.0\n",
      "lif layer 2 self.abs_max_v: 4759.5\n",
      "fc layer 3 self.abs_max_out: 1496.0\n",
      "fc layer 1 self.abs_max_out: 7571.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.872059/  2.016274, val:  41.67%, val_best:  43.75%, tr:  89.58%, tr_best:  90.81%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3903%\n",
      "layer   2  Sparsity: 80.1216%\n",
      "layer   3  Sparsity: 79.7956%\n",
      "total_backward_count 24475 real_backward_count 7287  29.773%\n",
      "fc layer 3 self.abs_max_out: 1511.0\n",
      "fc layer 3 self.abs_max_out: 1534.0\n",
      "fc layer 1 self.abs_max_out: 7697.0\n",
      "lif layer 1 self.abs_max_v: 12741.0\n",
      "fc layer 1 self.abs_max_out: 8471.0\n",
      "lif layer 1 self.abs_max_v: 13829.5\n",
      "lif layer 1 self.abs_max_v: 13967.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.867392/  2.013546, val:  41.25%, val_best:  43.75%, tr:  90.19%, tr_best:  90.81%, epoch time: 39.72 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.4182%\n",
      "layer   2  Sparsity: 79.4893%\n",
      "layer   3  Sparsity: 80.3389%\n",
      "total_backward_count 29370 real_backward_count 8579  29.210%\n",
      "fc layer 3 self.abs_max_out: 1550.0\n",
      "fc layer 2 self.abs_max_out: 3547.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.872005/  2.006937, val:  48.33%, val_best:  48.33%, tr:  88.97%, tr_best:  90.81%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3971%\n",
      "layer   2  Sparsity: 80.0712%\n",
      "layer   3  Sparsity: 81.1643%\n",
      "total_backward_count 34265 real_backward_count 9879  28.831%\n",
      "fc layer 3 self.abs_max_out: 1558.0\n",
      "fc layer 2 self.abs_max_out: 3645.0\n",
      "lif layer 1 self.abs_max_v: 14028.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.884344/  2.006175, val:  45.00%, val_best:  48.33%, tr:  91.32%, tr_best:  91.32%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3735%\n",
      "layer   2  Sparsity: 79.6280%\n",
      "layer   3  Sparsity: 80.8081%\n",
      "total_backward_count 39160 real_backward_count 11139  28.445%\n",
      "fc layer 1 self.abs_max_out: 8749.0\n",
      "lif layer 1 self.abs_max_v: 14237.5\n",
      "lif layer 1 self.abs_max_v: 15441.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.881825/  2.012443, val:  44.58%, val_best:  48.33%, tr:  90.91%, tr_best:  91.32%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3965%\n",
      "layer   2  Sparsity: 79.4828%\n",
      "layer   3  Sparsity: 80.8589%\n",
      "total_backward_count 44055 real_backward_count 12402  28.151%\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.876934/  2.040988, val:  39.58%, val_best:  48.33%, tr:  92.34%, tr_best:  92.34%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.3662%\n",
      "layer   2  Sparsity: 79.8816%\n",
      "layer   3  Sparsity: 81.7947%\n",
      "total_backward_count 48950 real_backward_count 13580  27.743%\n",
      "lif layer 2 self.abs_max_v: 4846.5\n",
      "fc layer 1 self.abs_max_out: 8857.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.910005/  2.057275, val:  39.58%, val_best:  48.33%, tr:  92.13%, tr_best:  92.34%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.3736%\n",
      "layer   2  Sparsity: 80.3930%\n",
      "layer   3  Sparsity: 83.5499%\n",
      "total_backward_count 53845 real_backward_count 14754  27.401%\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.919666/  2.035606, val:  48.75%, val_best:  48.75%, tr:  89.79%, tr_best:  92.34%, epoch time: 39.78 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.3603%\n",
      "layer   2  Sparsity: 80.4668%\n",
      "layer   3  Sparsity: 83.3479%\n",
      "total_backward_count 58740 real_backward_count 15960  27.171%\n",
      "lif layer 2 self.abs_max_v: 4940.0\n",
      "lif layer 2 self.abs_max_v: 4993.0\n",
      "fc layer 1 self.abs_max_out: 9224.0\n",
      "lif layer 1 self.abs_max_v: 15657.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.914667/  2.034220, val:  47.50%, val_best:  48.75%, tr:  92.54%, tr_best:  92.54%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.4264%\n",
      "layer   2  Sparsity: 80.1660%\n",
      "layer   3  Sparsity: 83.8276%\n",
      "total_backward_count 63635 real_backward_count 17122  26.907%\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.944917/  2.046415, val:  37.92%, val_best:  48.75%, tr:  91.83%, tr_best:  92.54%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.4080%\n",
      "layer   2  Sparsity: 80.2812%\n",
      "layer   3  Sparsity: 84.5852%\n",
      "total_backward_count 68530 real_backward_count 18292  26.692%\n",
      "fc layer 1 self.abs_max_out: 9365.0\n",
      "fc layer 1 self.abs_max_out: 9917.0\n",
      "lif layer 1 self.abs_max_v: 16443.0\n",
      "lif layer 1 self.abs_max_v: 16520.5\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.933815/  2.026919, val:  42.92%, val_best:  48.75%, tr:  92.44%, tr_best:  92.54%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4458%\n",
      "layer   2  Sparsity: 79.7304%\n",
      "layer   3  Sparsity: 84.1219%\n",
      "total_backward_count 73425 real_backward_count 19406  26.430%\n",
      "lif layer 2 self.abs_max_v: 5113.5\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.927809/  2.036827, val:  57.08%, val_best:  57.08%, tr:  91.73%, tr_best:  92.54%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3686%\n",
      "layer   2  Sparsity: 79.4676%\n",
      "layer   3  Sparsity: 83.9794%\n",
      "total_backward_count 78320 real_backward_count 20546  26.233%\n",
      "lif layer 1 self.abs_max_v: 17292.5\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.921862/  2.040707, val:  43.33%, val_best:  57.08%, tr:  93.16%, tr_best:  93.16%, epoch time: 40.26 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3812%\n",
      "layer   2  Sparsity: 79.9850%\n",
      "layer   3  Sparsity: 84.0767%\n",
      "total_backward_count 83215 real_backward_count 21660  26.029%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.922330/  2.032527, val:  53.75%, val_best:  57.08%, tr:  91.93%, tr_best:  93.16%, epoch time: 39.83 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.3948%\n",
      "layer   2  Sparsity: 80.1192%\n",
      "layer   3  Sparsity: 83.9035%\n",
      "total_backward_count 88110 real_backward_count 22795  25.871%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.925691/  2.047781, val:  50.42%, val_best:  57.08%, tr:  91.11%, tr_best:  93.16%, epoch time: 39.97 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3681%\n",
      "layer   2  Sparsity: 79.9846%\n",
      "layer   3  Sparsity: 84.9733%\n",
      "total_backward_count 93005 real_backward_count 24021  25.828%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.946828/  2.069057, val:  33.75%, val_best:  57.08%, tr:  91.83%, tr_best:  93.16%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3865%\n",
      "layer   2  Sparsity: 80.7226%\n",
      "layer   3  Sparsity: 85.1377%\n",
      "total_backward_count 97900 real_backward_count 25158  25.698%\n",
      "lif layer 2 self.abs_max_v: 5117.0\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.944790/  2.083092, val:  41.67%, val_best:  57.08%, tr:  91.62%, tr_best:  93.16%, epoch time: 39.81 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.3875%\n",
      "layer   2  Sparsity: 80.8521%\n",
      "layer   3  Sparsity: 85.6117%\n",
      "total_backward_count 102795 real_backward_count 26282  25.567%\n",
      "lif layer 2 self.abs_max_v: 5150.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.954451/  2.049230, val:  36.67%, val_best:  57.08%, tr:  91.32%, tr_best:  93.16%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.4043%\n",
      "layer   2  Sparsity: 81.1457%\n",
      "layer   3  Sparsity: 85.0386%\n",
      "total_backward_count 107690 real_backward_count 27486  25.523%\n",
      "fc layer 1 self.abs_max_out: 10551.0\n",
      "fc layer 1 self.abs_max_out: 11167.0\n",
      "lif layer 1 self.abs_max_v: 18555.0\n",
      "lif layer 1 self.abs_max_v: 18895.5\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.947760/  2.029940, val:  54.17%, val_best:  57.08%, tr:  91.83%, tr_best:  93.16%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.3406%\n",
      "layer   2  Sparsity: 81.2523%\n",
      "layer   3  Sparsity: 84.5688%\n",
      "total_backward_count 112585 real_backward_count 28670  25.465%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.939027/  2.030621, val:  56.67%, val_best:  57.08%, tr:  89.99%, tr_best:  93.16%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.4393%\n",
      "layer   2  Sparsity: 81.0672%\n",
      "layer   3  Sparsity: 84.9293%\n",
      "total_backward_count 117480 real_backward_count 29848  25.407%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.945341/  2.037448, val:  48.75%, val_best:  57.08%, tr:  91.11%, tr_best:  93.16%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.3929%\n",
      "layer   2  Sparsity: 80.7628%\n",
      "layer   3  Sparsity: 85.2893%\n",
      "total_backward_count 122375 real_backward_count 31005  25.336%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.956016/  2.044077, val:  48.75%, val_best:  57.08%, tr:  90.09%, tr_best:  93.16%, epoch time: 40.79 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3435%\n",
      "layer   2  Sparsity: 81.1212%\n",
      "layer   3  Sparsity: 84.8364%\n",
      "total_backward_count 127270 real_backward_count 32205  25.304%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.940959/  2.036560, val:  58.75%, val_best:  58.75%, tr:  92.34%, tr_best:  93.16%, epoch time: 41.30 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3972%\n",
      "layer   2  Sparsity: 81.6792%\n",
      "layer   3  Sparsity: 85.0154%\n",
      "total_backward_count 132165 real_backward_count 33344  25.229%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.931228/  2.043282, val:  53.33%, val_best:  58.75%, tr:  92.75%, tr_best:  93.16%, epoch time: 41.49 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3802%\n",
      "layer   2  Sparsity: 81.9935%\n",
      "layer   3  Sparsity: 84.6035%\n",
      "total_backward_count 137060 real_backward_count 34489  25.163%\n",
      "lif layer 2 self.abs_max_v: 5208.5\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.933399/  2.064640, val:  42.08%, val_best:  58.75%, tr:  91.42%, tr_best:  93.16%, epoch time: 41.20 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3589%\n",
      "layer   2  Sparsity: 82.1337%\n",
      "layer   3  Sparsity: 85.7750%\n",
      "total_backward_count 141955 real_backward_count 35586  25.069%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.956406/  2.041094, val:  48.33%, val_best:  58.75%, tr:  91.73%, tr_best:  93.16%, epoch time: 41.60 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3940%\n",
      "layer   2  Sparsity: 82.3897%\n",
      "layer   3  Sparsity: 85.9461%\n",
      "total_backward_count 146850 real_backward_count 36725  25.009%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.962869/  2.057760, val:  44.17%, val_best:  58.75%, tr:  90.09%, tr_best:  93.16%, epoch time: 41.01 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3535%\n",
      "layer   2  Sparsity: 83.1906%\n",
      "layer   3  Sparsity: 85.2063%\n",
      "total_backward_count 151745 real_backward_count 37873  24.958%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.950223/  2.047323, val:  48.75%, val_best:  58.75%, tr:  91.22%, tr_best:  93.16%, epoch time: 41.19 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3932%\n",
      "layer   2  Sparsity: 82.8629%\n",
      "layer   3  Sparsity: 84.6290%\n",
      "total_backward_count 156640 real_backward_count 39025  24.914%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.957815/  2.071908, val:  47.50%, val_best:  58.75%, tr:  91.32%, tr_best:  93.16%, epoch time: 40.98 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4391%\n",
      "layer   2  Sparsity: 81.7358%\n",
      "layer   3  Sparsity: 86.0413%\n",
      "total_backward_count 161535 real_backward_count 40151  24.856%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.981591/  2.061404, val:  51.25%, val_best:  58.75%, tr:  91.93%, tr_best:  93.16%, epoch time: 41.38 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3805%\n",
      "layer   2  Sparsity: 81.9554%\n",
      "layer   3  Sparsity: 86.2764%\n",
      "total_backward_count 166430 real_backward_count 41277  24.801%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.954829/  2.046400, val:  50.42%, val_best:  58.75%, tr:  91.73%, tr_best:  93.16%, epoch time: 41.38 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3443%\n",
      "layer   2  Sparsity: 82.1582%\n",
      "layer   3  Sparsity: 85.3687%\n",
      "total_backward_count 171325 real_backward_count 42414  24.756%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.966755/  2.066016, val:  52.92%, val_best:  58.75%, tr:  91.32%, tr_best:  93.16%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3637%\n",
      "layer   2  Sparsity: 83.0334%\n",
      "layer   3  Sparsity: 85.9546%\n",
      "total_backward_count 176220 real_backward_count 43547  24.712%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.959309/  2.034374, val:  59.58%, val_best:  59.58%, tr:  92.54%, tr_best:  93.16%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3909%\n",
      "layer   2  Sparsity: 83.1547%\n",
      "layer   3  Sparsity: 85.4387%\n",
      "total_backward_count 181115 real_backward_count 44615  24.634%\n",
      "lif layer 1 self.abs_max_v: 19177.5\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.932710/  2.048829, val:  61.25%, val_best:  61.25%, tr:  92.34%, tr_best:  93.16%, epoch time: 41.14 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3411%\n",
      "layer   2  Sparsity: 83.2874%\n",
      "layer   3  Sparsity: 85.1090%\n",
      "total_backward_count 186010 real_backward_count 45691  24.564%\n",
      "lif layer 1 self.abs_max_v: 19411.0\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.958856/  2.063128, val:  50.42%, val_best:  61.25%, tr:  92.13%, tr_best:  93.16%, epoch time: 41.27 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3756%\n",
      "layer   2  Sparsity: 83.1880%\n",
      "layer   3  Sparsity: 85.4159%\n",
      "total_backward_count 190905 real_backward_count 46842  24.537%\n",
      "lif layer 1 self.abs_max_v: 19743.5\n",
      "lif layer 1 self.abs_max_v: 19934.0\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.944849/  2.058759, val:  48.33%, val_best:  61.25%, tr:  90.91%, tr_best:  93.16%, epoch time: 41.36 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3909%\n",
      "layer   2  Sparsity: 83.0101%\n",
      "layer   3  Sparsity: 85.4326%\n",
      "total_backward_count 195800 real_backward_count 47952  24.490%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.940511/  2.039868, val:  53.33%, val_best:  61.25%, tr:  92.44%, tr_best:  93.16%, epoch time: 40.96 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3957%\n",
      "layer   2  Sparsity: 82.9702%\n",
      "layer   3  Sparsity: 85.5231%\n",
      "total_backward_count 200695 real_backward_count 49056  24.443%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.954885/  2.084878, val:  51.67%, val_best:  61.25%, tr:  92.65%, tr_best:  93.16%, epoch time: 40.80 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3774%\n",
      "layer   2  Sparsity: 83.2233%\n",
      "layer   3  Sparsity: 85.7602%\n",
      "total_backward_count 205590 real_backward_count 50177  24.406%\n",
      "fc layer 1 self.abs_max_out: 11488.0\n",
      "lif layer 1 self.abs_max_v: 20001.0\n",
      "fc layer 1 self.abs_max_out: 11848.0\n",
      "lif layer 1 self.abs_max_v: 20212.0\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.969705/  2.079883, val:  55.83%, val_best:  61.25%, tr:  91.62%, tr_best:  93.16%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4264%\n",
      "layer   2  Sparsity: 83.5769%\n",
      "layer   3  Sparsity: 85.4915%\n",
      "total_backward_count 210485 real_backward_count 51325  24.384%\n",
      "fc layer 1 self.abs_max_out: 12568.0\n",
      "lif layer 1 self.abs_max_v: 21285.5\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.970092/  2.058923, val:  54.58%, val_best:  61.25%, tr:  91.83%, tr_best:  93.16%, epoch time: 40.84 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3268%\n",
      "layer   2  Sparsity: 83.5851%\n",
      "layer   3  Sparsity: 85.3873%\n",
      "total_backward_count 215380 real_backward_count 52460  24.357%\n",
      "lif layer 1 self.abs_max_v: 21332.0\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.974145/  2.081034, val:  39.17%, val_best:  61.25%, tr:  91.93%, tr_best:  93.16%, epoch time: 41.05 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3860%\n",
      "layer   2  Sparsity: 83.0938%\n",
      "layer   3  Sparsity: 85.3550%\n",
      "total_backward_count 220275 real_backward_count 53582  24.325%\n",
      "lif layer 1 self.abs_max_v: 22135.5\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.961315/  2.041146, val:  51.67%, val_best:  61.25%, tr:  92.03%, tr_best:  93.16%, epoch time: 41.17 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3771%\n",
      "layer   2  Sparsity: 83.0518%\n",
      "layer   3  Sparsity: 85.1518%\n",
      "total_backward_count 225170 real_backward_count 54689  24.288%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.944445/  2.042284, val:  56.25%, val_best:  61.25%, tr:  93.16%, tr_best:  93.16%, epoch time: 41.26 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.4276%\n",
      "layer   2  Sparsity: 82.3097%\n",
      "layer   3  Sparsity: 84.5811%\n",
      "total_backward_count 230065 real_backward_count 55760  24.237%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.924960/  2.050794, val:  47.92%, val_best:  61.25%, tr:  92.65%, tr_best:  93.16%, epoch time: 41.54 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3924%\n",
      "layer   2  Sparsity: 81.8760%\n",
      "layer   3  Sparsity: 84.3566%\n",
      "total_backward_count 234960 real_backward_count 56873  24.205%\n",
      "lif layer 1 self.abs_max_v: 23409.0\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.906965/  2.011237, val:  53.33%, val_best:  61.25%, tr:  93.56%, tr_best:  93.56%, epoch time: 41.35 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3729%\n",
      "layer   2  Sparsity: 81.6800%\n",
      "layer   3  Sparsity: 83.4023%\n",
      "total_backward_count 239855 real_backward_count 57911  24.144%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.908610/  2.017182, val:  58.75%, val_best:  61.25%, tr:  92.54%, tr_best:  93.56%, epoch time: 40.98 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3055%\n",
      "layer   2  Sparsity: 82.0675%\n",
      "layer   3  Sparsity: 83.2798%\n",
      "total_backward_count 244750 real_backward_count 59027  24.117%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.904636/  1.987288, val:  57.08%, val_best:  61.25%, tr:  93.05%, tr_best:  93.56%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4128%\n",
      "layer   2  Sparsity: 82.6428%\n",
      "layer   3  Sparsity: 83.2878%\n",
      "total_backward_count 249645 real_backward_count 60056  24.057%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.911788/  2.059797, val:  49.58%, val_best:  61.25%, tr:  91.83%, tr_best:  93.56%, epoch time: 41.20 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3670%\n",
      "layer   2  Sparsity: 83.0620%\n",
      "layer   3  Sparsity: 84.0908%\n",
      "total_backward_count 254540 real_backward_count 61076  23.995%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.927402/  2.024234, val:  56.67%, val_best:  61.25%, tr:  92.75%, tr_best:  93.56%, epoch time: 40.96 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4106%\n",
      "layer   2  Sparsity: 82.8794%\n",
      "layer   3  Sparsity: 84.8319%\n",
      "total_backward_count 259435 real_backward_count 62187  23.970%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.928026/  2.018694, val:  56.67%, val_best:  61.25%, tr:  92.54%, tr_best:  93.56%, epoch time: 40.86 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3405%\n",
      "layer   2  Sparsity: 83.3222%\n",
      "layer   3  Sparsity: 84.9985%\n",
      "total_backward_count 264330 real_backward_count 63246  23.927%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.936919/  2.062225, val:  57.92%, val_best:  61.25%, tr:  92.34%, tr_best:  93.56%, epoch time: 41.07 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3858%\n",
      "layer   2  Sparsity: 83.2935%\n",
      "layer   3  Sparsity: 84.5714%\n",
      "total_backward_count 269225 real_backward_count 64302  23.884%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.933433/  2.030452, val:  60.42%, val_best:  61.25%, tr:  92.03%, tr_best:  93.56%, epoch time: 41.30 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3707%\n",
      "layer   2  Sparsity: 82.8436%\n",
      "layer   3  Sparsity: 84.8288%\n",
      "total_backward_count 274120 real_backward_count 65365  23.845%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.932937/  2.038699, val:  54.17%, val_best:  61.25%, tr:  91.52%, tr_best:  93.56%, epoch time: 40.85 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4487%\n",
      "layer   2  Sparsity: 83.0278%\n",
      "layer   3  Sparsity: 85.1221%\n",
      "total_backward_count 279015 real_backward_count 66413  23.803%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.937045/  2.028071, val:  56.25%, val_best:  61.25%, tr:  93.77%, tr_best:  93.77%, epoch time: 41.43 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3788%\n",
      "layer   2  Sparsity: 83.1788%\n",
      "layer   3  Sparsity: 84.4206%\n",
      "total_backward_count 283910 real_backward_count 67421  23.747%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.924378/  2.007985, val:  58.75%, val_best:  61.25%, tr:  93.26%, tr_best:  93.77%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3792%\n",
      "layer   2  Sparsity: 83.4408%\n",
      "layer   3  Sparsity: 83.4728%\n",
      "total_backward_count 288805 real_backward_count 68408  23.687%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.906890/  2.035114, val:  46.25%, val_best:  61.25%, tr:  93.77%, tr_best:  93.77%, epoch time: 41.44 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3148%\n",
      "layer   2  Sparsity: 83.7078%\n",
      "layer   3  Sparsity: 83.4884%\n",
      "total_backward_count 293700 real_backward_count 69411  23.633%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.910824/  2.028002, val:  39.58%, val_best:  61.25%, tr:  92.44%, tr_best:  93.77%, epoch time: 41.23 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3977%\n",
      "layer   2  Sparsity: 83.5175%\n",
      "layer   3  Sparsity: 83.7815%\n",
      "total_backward_count 298595 real_backward_count 70460  23.597%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.884477/  1.960799, val:  56.67%, val_best:  61.25%, tr:  92.65%, tr_best:  93.77%, epoch time: 41.13 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3821%\n",
      "layer   2  Sparsity: 82.9146%\n",
      "layer   3  Sparsity: 83.2801%\n",
      "total_backward_count 303490 real_backward_count 71545  23.574%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.863974/  1.989961, val:  50.42%, val_best:  61.25%, tr:  92.24%, tr_best:  93.77%, epoch time: 41.41 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.4235%\n",
      "layer   2  Sparsity: 83.1600%\n",
      "layer   3  Sparsity: 83.4799%\n",
      "total_backward_count 308385 real_backward_count 72588  23.538%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.861384/  1.973359, val:  53.33%, val_best:  61.25%, tr:  93.87%, tr_best:  93.87%, epoch time: 41.33 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3941%\n",
      "layer   2  Sparsity: 82.8477%\n",
      "layer   3  Sparsity: 82.7000%\n",
      "total_backward_count 313280 real_backward_count 73623  23.501%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.861619/  1.984940, val:  51.25%, val_best:  61.25%, tr:  94.59%, tr_best:  94.59%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4206%\n",
      "layer   2  Sparsity: 83.0090%\n",
      "layer   3  Sparsity: 82.4389%\n",
      "total_backward_count 318175 real_backward_count 74693  23.475%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.865171/  1.970007, val:  64.17%, val_best:  64.17%, tr:  93.87%, tr_best:  94.59%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4495%\n",
      "layer   2  Sparsity: 83.0032%\n",
      "layer   3  Sparsity: 83.0846%\n",
      "total_backward_count 323070 real_backward_count 75700  23.431%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.884710/  2.003321, val:  52.50%, val_best:  64.17%, tr:  91.62%, tr_best:  94.59%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4125%\n",
      "layer   2  Sparsity: 82.9818%\n",
      "layer   3  Sparsity: 83.4247%\n",
      "total_backward_count 327965 real_backward_count 76801  23.417%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.874316/  1.972168, val:  57.50%, val_best:  64.17%, tr:  93.16%, tr_best:  94.59%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3826%\n",
      "layer   2  Sparsity: 82.9700%\n",
      "layer   3  Sparsity: 82.4404%\n",
      "total_backward_count 332860 real_backward_count 77818  23.379%\n",
      "fc layer 2 self.abs_max_out: 3726.0\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.857908/  1.952945, val:  59.17%, val_best:  64.17%, tr:  92.95%, tr_best:  94.59%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3896%\n",
      "layer   2  Sparsity: 82.7257%\n",
      "layer   3  Sparsity: 81.8078%\n",
      "total_backward_count 337755 real_backward_count 78843  23.343%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.862954/  2.001851, val:  54.58%, val_best:  64.17%, tr:  94.18%, tr_best:  94.59%, epoch time: 40.95 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4266%\n",
      "layer   2  Sparsity: 82.3630%\n",
      "layer   3  Sparsity: 82.8707%\n",
      "total_backward_count 342650 real_backward_count 79879  23.312%\n",
      "lif layer 2 self.abs_max_v: 5299.0\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.868907/  1.996798, val:  53.33%, val_best:  64.17%, tr:  93.26%, tr_best:  94.59%, epoch time: 41.28 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.4066%\n",
      "layer   2  Sparsity: 82.0678%\n",
      "layer   3  Sparsity: 82.0728%\n",
      "total_backward_count 347545 real_backward_count 80893  23.276%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.868847/  1.976256, val:  46.67%, val_best:  64.17%, tr:  92.34%, tr_best:  94.59%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3817%\n",
      "layer   2  Sparsity: 82.5061%\n",
      "layer   3  Sparsity: 81.8103%\n",
      "total_backward_count 352440 real_backward_count 81923  23.245%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.860624/  1.964844, val:  67.08%, val_best:  67.08%, tr:  93.36%, tr_best:  94.59%, epoch time: 41.17 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.4001%\n",
      "layer   2  Sparsity: 82.7367%\n",
      "layer   3  Sparsity: 81.8559%\n",
      "total_backward_count 357335 real_backward_count 82943  23.212%\n",
      "fc layer 2 self.abs_max_out: 3756.0\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.865399/  1.994560, val:  60.83%, val_best:  67.08%, tr:  93.56%, tr_best:  94.59%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3590%\n",
      "layer   2  Sparsity: 83.0180%\n",
      "layer   3  Sparsity: 82.1463%\n",
      "total_backward_count 362230 real_backward_count 83952  23.176%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.855286/  1.997302, val:  55.00%, val_best:  67.08%, tr:  93.05%, tr_best:  94.59%, epoch time: 41.02 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3223%\n",
      "layer   2  Sparsity: 82.9155%\n",
      "layer   3  Sparsity: 81.6008%\n",
      "total_backward_count 367125 real_backward_count 84995  23.152%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.877910/  1.990581, val:  60.83%, val_best:  67.08%, tr:  93.97%, tr_best:  94.59%, epoch time: 40.84 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4029%\n",
      "layer   2  Sparsity: 82.6158%\n",
      "layer   3  Sparsity: 82.5537%\n",
      "total_backward_count 372020 real_backward_count 85992  23.115%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.864397/  1.960357, val:  55.00%, val_best:  67.08%, tr:  93.05%, tr_best:  94.59%, epoch time: 41.33 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.4020%\n",
      "layer   2  Sparsity: 82.6145%\n",
      "layer   3  Sparsity: 81.7666%\n",
      "total_backward_count 376915 real_backward_count 87041  23.093%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.847579/  1.999205, val:  40.83%, val_best:  67.08%, tr:  93.87%, tr_best:  94.59%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4196%\n",
      "layer   2  Sparsity: 82.5624%\n",
      "layer   3  Sparsity: 80.9426%\n",
      "total_backward_count 381810 real_backward_count 88009  23.050%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.856239/  1.968976, val:  58.33%, val_best:  67.08%, tr:  93.56%, tr_best:  94.59%, epoch time: 40.86 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3726%\n",
      "layer   2  Sparsity: 82.1406%\n",
      "layer   3  Sparsity: 81.8146%\n",
      "total_backward_count 386705 real_backward_count 88974  23.008%\n",
      "fc layer 2 self.abs_max_out: 3788.0\n",
      "fc layer 2 self.abs_max_out: 3866.0\n",
      "fc layer 2 self.abs_max_out: 3898.0\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.862624/  1.975769, val:  58.33%, val_best:  67.08%, tr:  92.85%, tr_best:  94.59%, epoch time: 41.07 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4012%\n",
      "layer   2  Sparsity: 82.3929%\n",
      "layer   3  Sparsity: 82.3852%\n",
      "total_backward_count 391600 real_backward_count 89982  22.978%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.847126/  1.955642, val:  60.83%, val_best:  67.08%, tr:  93.67%, tr_best:  94.59%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3073%\n",
      "layer   2  Sparsity: 82.2641%\n",
      "layer   3  Sparsity: 81.6745%\n",
      "total_backward_count 396495 real_backward_count 90987  22.948%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.837328/  1.983009, val:  47.92%, val_best:  67.08%, tr:  93.67%, tr_best:  94.59%, epoch time: 40.82 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4337%\n",
      "layer   2  Sparsity: 82.8858%\n",
      "layer   3  Sparsity: 81.9545%\n",
      "total_backward_count 401390 real_backward_count 91935  22.904%\n",
      "fc layer 1 self.abs_max_out: 12752.0\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.841986/  1.961681, val:  52.50%, val_best:  67.08%, tr:  93.16%, tr_best:  94.59%, epoch time: 40.98 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4254%\n",
      "layer   2  Sparsity: 83.6737%\n",
      "layer   3  Sparsity: 81.2406%\n",
      "total_backward_count 406285 real_backward_count 92956  22.880%\n",
      "fc layer 1 self.abs_max_out: 12777.0\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.828812/  1.971707, val:  55.83%, val_best:  67.08%, tr:  93.97%, tr_best:  94.59%, epoch time: 41.25 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.4117%\n",
      "layer   2  Sparsity: 83.7106%\n",
      "layer   3  Sparsity: 81.5651%\n",
      "total_backward_count 411180 real_backward_count 93979  22.856%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.833381/  1.949776, val:  65.42%, val_best:  67.08%, tr:  94.28%, tr_best:  94.59%, epoch time: 40.60 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4182%\n",
      "layer   2  Sparsity: 83.7505%\n",
      "layer   3  Sparsity: 81.4892%\n",
      "total_backward_count 416075 real_backward_count 94993  22.831%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.823975/  1.980606, val:  54.58%, val_best:  67.08%, tr:  93.46%, tr_best:  94.59%, epoch time: 40.85 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3884%\n",
      "layer   2  Sparsity: 82.4601%\n",
      "layer   3  Sparsity: 80.6719%\n",
      "total_backward_count 420970 real_backward_count 96000  22.804%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.847634/  1.979957, val:  54.17%, val_best:  67.08%, tr:  93.67%, tr_best:  94.59%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4391%\n",
      "layer   2  Sparsity: 82.3387%\n",
      "layer   3  Sparsity: 81.8099%\n",
      "total_backward_count 425865 real_backward_count 96967  22.769%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.844922/  1.963079, val:  59.58%, val_best:  67.08%, tr:  93.97%, tr_best:  94.59%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3442%\n",
      "layer   2  Sparsity: 82.3656%\n",
      "layer   3  Sparsity: 82.2700%\n",
      "total_backward_count 430760 real_backward_count 97995  22.749%\n",
      "lif layer 2 self.abs_max_v: 5303.5\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.844565/  1.966801, val:  55.00%, val_best:  67.08%, tr:  92.44%, tr_best:  94.59%, epoch time: 41.13 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3995%\n",
      "layer   2  Sparsity: 82.5211%\n",
      "layer   3  Sparsity: 81.8984%\n",
      "total_backward_count 435655 real_backward_count 99010  22.727%\n",
      "fc layer 1 self.abs_max_out: 12824.0\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.848562/  1.978016, val:  59.58%, val_best:  67.08%, tr:  93.56%, tr_best:  94.59%, epoch time: 40.83 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3098%\n",
      "layer   2  Sparsity: 82.4888%\n",
      "layer   3  Sparsity: 81.9492%\n",
      "total_backward_count 440550 real_backward_count 100027  22.705%\n",
      "fc layer 1 self.abs_max_out: 13011.0\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.858112/  1.985820, val:  53.33%, val_best:  67.08%, tr:  93.16%, tr_best:  94.59%, epoch time: 40.74 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3525%\n",
      "layer   2  Sparsity: 83.2076%\n",
      "layer   3  Sparsity: 82.3285%\n",
      "total_backward_count 445445 real_backward_count 101020  22.678%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.848990/  1.950381, val:  62.92%, val_best:  67.08%, tr:  94.18%, tr_best:  94.59%, epoch time: 41.22 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.4209%\n",
      "layer   2  Sparsity: 83.3911%\n",
      "layer   3  Sparsity: 82.1294%\n",
      "total_backward_count 450340 real_backward_count 101973  22.644%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.833295/  1.956014, val:  53.75%, val_best:  67.08%, tr:  93.97%, tr_best:  94.59%, epoch time: 40.82 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4504%\n",
      "layer   2  Sparsity: 83.9214%\n",
      "layer   3  Sparsity: 82.1177%\n",
      "total_backward_count 455235 real_backward_count 102904  22.605%\n",
      "fc layer 1 self.abs_max_out: 13155.0\n",
      "lif layer 1 self.abs_max_v: 23661.0\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.842283/  1.966769, val:  60.00%, val_best:  67.08%, tr:  94.08%, tr_best:  94.59%, epoch time: 41.63 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3180%\n",
      "layer   2  Sparsity: 83.9235%\n",
      "layer   3  Sparsity: 81.7921%\n",
      "total_backward_count 460130 real_backward_count 103919  22.585%\n",
      "fc layer 1 self.abs_max_out: 13311.0\n",
      "lif layer 1 self.abs_max_v: 24045.5\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.831232/  1.936998, val:  55.42%, val_best:  67.08%, tr:  94.18%, tr_best:  94.59%, epoch time: 41.26 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3985%\n",
      "layer   2  Sparsity: 83.5833%\n",
      "layer   3  Sparsity: 81.2515%\n",
      "total_backward_count 465025 real_backward_count 104926  22.564%\n",
      "fc layer 2 self.abs_max_out: 3927.0\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.825684/  1.950390, val:  55.83%, val_best:  67.08%, tr:  92.95%, tr_best:  94.59%, epoch time: 40.90 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3990%\n",
      "layer   2  Sparsity: 83.1663%\n",
      "layer   3  Sparsity: 81.5974%\n",
      "total_backward_count 469920 real_backward_count 105949  22.546%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.817049/  1.950599, val:  55.42%, val_best:  67.08%, tr:  93.16%, tr_best:  94.59%, epoch time: 41.16 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3626%\n",
      "layer   2  Sparsity: 83.3046%\n",
      "layer   3  Sparsity: 81.7714%\n",
      "total_backward_count 474815 real_backward_count 106917  22.518%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.843388/  1.950286, val:  63.75%, val_best:  67.08%, tr:  94.38%, tr_best:  94.59%, epoch time: 41.02 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3762%\n",
      "layer   2  Sparsity: 83.3993%\n",
      "layer   3  Sparsity: 82.2779%\n",
      "total_backward_count 479710 real_backward_count 107840  22.480%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.833208/  1.968486, val:  60.00%, val_best:  67.08%, tr:  94.08%, tr_best:  94.59%, epoch time: 41.15 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3391%\n",
      "layer   2  Sparsity: 82.9494%\n",
      "layer   3  Sparsity: 81.7390%\n",
      "total_backward_count 484605 real_backward_count 108835  22.458%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.844098/  1.957601, val:  59.17%, val_best:  67.08%, tr:  93.26%, tr_best:  94.59%, epoch time: 40.67 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3384%\n",
      "layer   2  Sparsity: 83.2043%\n",
      "layer   3  Sparsity: 82.7124%\n",
      "total_backward_count 489500 real_backward_count 109826  22.436%\n",
      "fc layer 1 self.abs_max_out: 13397.0\n",
      "lif layer 1 self.abs_max_v: 24220.0\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.857954/  1.997886, val:  54.58%, val_best:  67.08%, tr:  91.83%, tr_best:  94.59%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4602%\n",
      "layer   2  Sparsity: 82.3761%\n",
      "layer   3  Sparsity: 83.3317%\n",
      "total_backward_count 494395 real_backward_count 110816  22.414%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.874155/  1.970452, val:  64.58%, val_best:  67.08%, tr:  92.54%, tr_best:  94.59%, epoch time: 40.80 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3615%\n",
      "layer   2  Sparsity: 82.2637%\n",
      "layer   3  Sparsity: 82.8691%\n",
      "total_backward_count 499290 real_backward_count 111839  22.400%\n",
      "fc layer 1 self.abs_max_out: 13625.0\n",
      "lif layer 1 self.abs_max_v: 24718.5\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.867259/  1.988761, val:  54.58%, val_best:  67.08%, tr:  93.67%, tr_best:  94.59%, epoch time: 40.75 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3919%\n",
      "layer   2  Sparsity: 82.0777%\n",
      "layer   3  Sparsity: 82.7257%\n",
      "total_backward_count 504185 real_backward_count 112821  22.377%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.855507/  1.970727, val:  60.42%, val_best:  67.08%, tr:  93.36%, tr_best:  94.59%, epoch time: 41.11 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3964%\n",
      "layer   2  Sparsity: 81.7039%\n",
      "layer   3  Sparsity: 81.7936%\n",
      "total_backward_count 509080 real_backward_count 113805  22.355%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.835249/  1.949625, val:  61.25%, val_best:  67.08%, tr:  92.65%, tr_best:  94.59%, epoch time: 41.17 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3921%\n",
      "layer   2  Sparsity: 81.8710%\n",
      "layer   3  Sparsity: 80.7662%\n",
      "total_backward_count 513975 real_backward_count 114788  22.333%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.813841/  1.933799, val:  61.67%, val_best:  67.08%, tr:  94.99%, tr_best:  94.99%, epoch time: 41.03 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3873%\n",
      "layer   2  Sparsity: 82.0061%\n",
      "layer   3  Sparsity: 79.9039%\n",
      "total_backward_count 518870 real_backward_count 115766  22.311%\n",
      "fc layer 1 self.abs_max_out: 13911.0\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.810755/  1.959978, val:  47.50%, val_best:  67.08%, tr:  93.56%, tr_best:  94.99%, epoch time: 41.01 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3865%\n",
      "layer   2  Sparsity: 81.3655%\n",
      "layer   3  Sparsity: 80.9409%\n",
      "total_backward_count 523765 real_backward_count 116707  22.282%\n",
      "lif layer 1 self.abs_max_v: 24783.0\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.820791/  1.951336, val:  56.25%, val_best:  67.08%, tr:  92.65%, tr_best:  94.99%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3063%\n",
      "layer   2  Sparsity: 81.4066%\n",
      "layer   3  Sparsity: 80.3066%\n",
      "total_backward_count 528660 real_backward_count 117693  22.263%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.824254/  1.935728, val:  70.83%, val_best:  70.83%, tr:  94.28%, tr_best:  94.99%, epoch time: 40.84 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3416%\n",
      "layer   2  Sparsity: 81.3600%\n",
      "layer   3  Sparsity: 80.6304%\n",
      "total_backward_count 533555 real_backward_count 118650  22.238%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.804611/  1.944507, val:  60.42%, val_best:  70.83%, tr:  94.79%, tr_best:  94.99%, epoch time: 40.89 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3752%\n",
      "layer   2  Sparsity: 81.2500%\n",
      "layer   3  Sparsity: 80.7364%\n",
      "total_backward_count 538450 real_backward_count 119556  22.204%\n",
      "fc layer 1 self.abs_max_out: 13930.0\n",
      "lif layer 2 self.abs_max_v: 5362.5\n",
      "fc layer 1 self.abs_max_out: 14650.0\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.815956/  1.924003, val:  61.25%, val_best:  70.83%, tr:  92.75%, tr_best:  94.99%, epoch time: 41.24 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3330%\n",
      "layer   2  Sparsity: 80.8211%\n",
      "layer   3  Sparsity: 80.8340%\n",
      "total_backward_count 543345 real_backward_count 120556  22.188%\n",
      "lif layer 1 self.abs_max_v: 24825.5\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.809350/  1.936323, val:  54.17%, val_best:  70.83%, tr:  94.89%, tr_best:  94.99%, epoch time: 41.16 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3855%\n",
      "layer   2  Sparsity: 81.3044%\n",
      "layer   3  Sparsity: 80.2265%\n",
      "total_backward_count 548240 real_backward_count 121532  22.168%\n",
      "lif layer 2 self.abs_max_v: 5454.5\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.808090/  1.936666, val:  57.50%, val_best:  70.83%, tr:  94.08%, tr_best:  94.99%, epoch time: 41.27 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3935%\n",
      "layer   2  Sparsity: 81.3202%\n",
      "layer   3  Sparsity: 80.5981%\n",
      "total_backward_count 553135 real_backward_count 122509  22.148%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.793413/  1.925364, val:  63.33%, val_best:  70.83%, tr:  94.48%, tr_best:  94.99%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3700%\n",
      "layer   2  Sparsity: 81.4138%\n",
      "layer   3  Sparsity: 80.9253%\n",
      "total_backward_count 558030 real_backward_count 123391  22.112%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.778333/  1.924008, val:  55.00%, val_best:  70.83%, tr:  93.97%, tr_best:  94.99%, epoch time: 40.88 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4143%\n",
      "layer   2  Sparsity: 81.3321%\n",
      "layer   3  Sparsity: 79.5531%\n",
      "total_backward_count 562925 real_backward_count 124319  22.084%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.794003/  1.927700, val:  61.25%, val_best:  70.83%, tr:  94.69%, tr_best:  94.99%, epoch time: 40.88 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3968%\n",
      "layer   2  Sparsity: 81.9452%\n",
      "layer   3  Sparsity: 79.4458%\n",
      "total_backward_count 567820 real_backward_count 125238  22.056%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.772038/  1.915032, val:  57.50%, val_best:  70.83%, tr:  94.59%, tr_best:  94.99%, epoch time: 40.70 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3714%\n",
      "layer   2  Sparsity: 82.0423%\n",
      "layer   3  Sparsity: 80.0638%\n",
      "total_backward_count 572715 real_backward_count 126125  22.022%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.769228/  1.888434, val:  64.58%, val_best:  70.83%, tr:  94.18%, tr_best:  94.99%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3555%\n",
      "layer   2  Sparsity: 81.1886%\n",
      "layer   3  Sparsity: 79.0253%\n",
      "total_backward_count 577610 real_backward_count 127056  21.997%\n",
      "fc layer 3 self.abs_max_out: 1570.0\n",
      "lif layer 1 self.abs_max_v: 24826.0\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.760107/  1.901222, val:  60.83%, val_best:  70.83%, tr:  94.59%, tr_best:  94.99%, epoch time: 40.92 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3190%\n",
      "layer   2  Sparsity: 80.9226%\n",
      "layer   3  Sparsity: 78.3549%\n",
      "total_backward_count 582505 real_backward_count 128010  21.976%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.745233/  1.876608, val:  58.75%, val_best:  70.83%, tr:  95.71%, tr_best:  95.71%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3179%\n",
      "layer   2  Sparsity: 80.2492%\n",
      "layer   3  Sparsity: 78.7848%\n",
      "total_backward_count 587400 real_backward_count 128946  21.952%\n",
      "lif layer 2 self.abs_max_v: 5793.5\n",
      "fc layer 3 self.abs_max_out: 1627.0\n",
      "fc layer 3 self.abs_max_out: 1629.0\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.715788/  1.872470, val:  63.33%, val_best:  70.83%, tr:  95.51%, tr_best:  95.71%, epoch time: 41.09 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4497%\n",
      "layer   2  Sparsity: 79.5906%\n",
      "layer   3  Sparsity: 78.5102%\n",
      "total_backward_count 592295 real_backward_count 129871  21.927%\n",
      "fc layer 3 self.abs_max_out: 1709.0\n",
      "fc layer 3 self.abs_max_out: 1793.0\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.726789/  1.880122, val:  56.25%, val_best:  70.83%, tr:  93.77%, tr_best:  95.71%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3856%\n",
      "layer   2  Sparsity: 79.3669%\n",
      "layer   3  Sparsity: 78.6362%\n",
      "total_backward_count 597190 real_backward_count 130812  21.905%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.734133/  1.901727, val:  63.75%, val_best:  70.83%, tr:  94.79%, tr_best:  95.71%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3354%\n",
      "layer   2  Sparsity: 79.5235%\n",
      "layer   3  Sparsity: 79.1156%\n",
      "total_backward_count 602085 real_backward_count 131744  21.881%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.750086/  1.905613, val:  55.00%, val_best:  70.83%, tr:  95.51%, tr_best:  95.71%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3682%\n",
      "layer   2  Sparsity: 79.5925%\n",
      "layer   3  Sparsity: 78.9104%\n",
      "total_backward_count 606980 real_backward_count 132677  21.859%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.739183/  1.897593, val:  58.75%, val_best:  70.83%, tr:  94.99%, tr_best:  95.71%, epoch time: 40.48 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3992%\n",
      "layer   2  Sparsity: 79.9726%\n",
      "layer   3  Sparsity: 79.0381%\n",
      "total_backward_count 611875 real_backward_count 133596  21.834%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.760064/  1.890224, val:  61.67%, val_best:  70.83%, tr:  94.28%, tr_best:  95.71%, epoch time: 40.75 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3985%\n",
      "layer   2  Sparsity: 79.7148%\n",
      "layer   3  Sparsity: 80.2296%\n",
      "total_backward_count 616770 real_backward_count 134534  21.813%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.735272/  1.942036, val:  45.83%, val_best:  70.83%, tr:  94.28%, tr_best:  95.71%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3681%\n",
      "layer   2  Sparsity: 79.7318%\n",
      "layer   3  Sparsity: 79.5603%\n",
      "total_backward_count 621665 real_backward_count 135449  21.788%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.735715/  1.902138, val:  52.50%, val_best:  70.83%, tr:  94.28%, tr_best:  95.71%, epoch time: 41.44 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3854%\n",
      "layer   2  Sparsity: 80.2286%\n",
      "layer   3  Sparsity: 79.4910%\n",
      "total_backward_count 626560 real_backward_count 136349  21.762%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.747423/  1.916273, val:  50.00%, val_best:  70.83%, tr:  94.89%, tr_best:  95.71%, epoch time: 40.87 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4230%\n",
      "layer   2  Sparsity: 80.3268%\n",
      "layer   3  Sparsity: 78.9417%\n",
      "total_backward_count 631455 real_backward_count 137262  21.737%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.754800/  1.908928, val:  69.17%, val_best:  70.83%, tr:  95.91%, tr_best:  95.91%, epoch time: 41.18 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3659%\n",
      "layer   2  Sparsity: 80.4210%\n",
      "layer   3  Sparsity: 80.0370%\n",
      "total_backward_count 636350 real_backward_count 138165  21.712%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.747342/  1.918895, val:  54.17%, val_best:  70.83%, tr:  94.99%, tr_best:  95.91%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3586%\n",
      "layer   2  Sparsity: 80.3520%\n",
      "layer   3  Sparsity: 79.4287%\n",
      "total_backward_count 641245 real_backward_count 139100  21.692%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.781532/  1.918730, val:  56.67%, val_best:  70.83%, tr:  93.46%, tr_best:  95.91%, epoch time: 41.12 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.4090%\n",
      "layer   2  Sparsity: 80.0246%\n",
      "layer   3  Sparsity: 79.6519%\n",
      "total_backward_count 646140 real_backward_count 140101  21.683%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.744619/  1.899008, val:  48.33%, val_best:  70.83%, tr:  95.20%, tr_best:  95.91%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.4045%\n",
      "layer   2  Sparsity: 80.2547%\n",
      "layer   3  Sparsity: 78.5100%\n",
      "total_backward_count 651035 real_backward_count 141001  21.658%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.731294/  1.870871, val:  60.42%, val_best:  70.83%, tr:  94.79%, tr_best:  95.91%, epoch time: 41.26 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3740%\n",
      "layer   2  Sparsity: 80.1473%\n",
      "layer   3  Sparsity: 78.5244%\n",
      "total_backward_count 655930 real_backward_count 141908  21.635%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.706871/  1.885212, val:  60.00%, val_best:  70.83%, tr:  95.20%, tr_best:  95.91%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3721%\n",
      "layer   2  Sparsity: 80.3002%\n",
      "layer   3  Sparsity: 77.0628%\n",
      "total_backward_count 660825 real_backward_count 142790  21.608%\n",
      "lif layer 1 self.abs_max_v: 24864.0\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.716624/  1.877667, val:  62.08%, val_best:  70.83%, tr:  96.73%, tr_best:  96.73%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3356%\n",
      "layer   2  Sparsity: 80.7774%\n",
      "layer   3  Sparsity: 78.2954%\n",
      "total_backward_count 665720 real_backward_count 143681  21.583%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.713246/  1.857491, val:  68.33%, val_best:  70.83%, tr:  94.48%, tr_best:  96.73%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3946%\n",
      "layer   2  Sparsity: 80.7851%\n",
      "layer   3  Sparsity: 78.4015%\n",
      "total_backward_count 670615 real_backward_count 144569  21.558%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.700707/  1.881772, val:  59.17%, val_best:  70.83%, tr:  95.81%, tr_best:  96.73%, epoch time: 40.67 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3884%\n",
      "layer   2  Sparsity: 80.3615%\n",
      "layer   3  Sparsity: 78.6852%\n",
      "total_backward_count 675510 real_backward_count 145444  21.531%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.743452/  1.909087, val:  60.83%, val_best:  70.83%, tr:  93.77%, tr_best:  96.73%, epoch time: 40.98 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4112%\n",
      "layer   2  Sparsity: 79.9993%\n",
      "layer   3  Sparsity: 80.3312%\n",
      "total_backward_count 680405 real_backward_count 146400  21.517%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.745554/  1.896255, val:  53.33%, val_best:  70.83%, tr:  94.38%, tr_best:  96.73%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3757%\n",
      "layer   2  Sparsity: 80.2164%\n",
      "layer   3  Sparsity: 79.6454%\n",
      "total_backward_count 685300 real_backward_count 147353  21.502%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.748806/  1.914475, val:  52.92%, val_best:  70.83%, tr:  95.30%, tr_best:  96.73%, epoch time: 41.10 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.4131%\n",
      "layer   2  Sparsity: 80.3677%\n",
      "layer   3  Sparsity: 80.2211%\n",
      "total_backward_count 690195 real_backward_count 148291  21.485%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.742428/  1.899840, val:  51.67%, val_best:  70.83%, tr:  95.40%, tr_best:  96.73%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3438%\n",
      "layer   2  Sparsity: 80.4240%\n",
      "layer   3  Sparsity: 80.7224%\n",
      "total_backward_count 695090 real_backward_count 149168  21.460%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.747314/  1.902312, val:  65.42%, val_best:  70.83%, tr:  93.97%, tr_best:  96.73%, epoch time: 40.95 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4752%\n",
      "layer   2  Sparsity: 81.1787%\n",
      "layer   3  Sparsity: 81.0853%\n",
      "total_backward_count 699985 real_backward_count 150136  21.448%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.768624/  1.915238, val:  55.42%, val_best:  70.83%, tr:  94.69%, tr_best:  96.73%, epoch time: 41.30 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3819%\n",
      "layer   2  Sparsity: 81.7354%\n",
      "layer   3  Sparsity: 81.5627%\n",
      "total_backward_count 704880 real_backward_count 151033  21.427%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.776533/  1.875641, val:  67.50%, val_best:  70.83%, tr:  93.77%, tr_best:  96.73%, epoch time: 41.10 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4179%\n",
      "layer   2  Sparsity: 81.5173%\n",
      "layer   3  Sparsity: 80.4648%\n",
      "total_backward_count 709775 real_backward_count 152012  21.417%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.758260/  1.896739, val:  55.83%, val_best:  70.83%, tr:  93.87%, tr_best:  96.73%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3986%\n",
      "layer   2  Sparsity: 81.1460%\n",
      "layer   3  Sparsity: 79.3296%\n",
      "total_backward_count 714670 real_backward_count 152939  21.400%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.759094/  1.937298, val:  53.75%, val_best:  70.83%, tr:  94.08%, tr_best:  96.73%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3793%\n",
      "layer   2  Sparsity: 81.6335%\n",
      "layer   3  Sparsity: 79.6816%\n",
      "total_backward_count 719565 real_backward_count 153858  21.382%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.749906/  1.914034, val:  57.92%, val_best:  70.83%, tr:  95.40%, tr_best:  96.73%, epoch time: 41.41 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.4178%\n",
      "layer   2  Sparsity: 82.1512%\n",
      "layer   3  Sparsity: 79.9039%\n",
      "total_backward_count 724460 real_backward_count 154770  21.363%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.757386/  1.881540, val:  63.33%, val_best:  70.83%, tr:  95.71%, tr_best:  96.73%, epoch time: 40.93 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3904%\n",
      "layer   2  Sparsity: 82.0928%\n",
      "layer   3  Sparsity: 79.2856%\n",
      "total_backward_count 729355 real_backward_count 155736  21.353%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.736318/  1.916793, val:  54.17%, val_best:  70.83%, tr:  93.87%, tr_best:  96.73%, epoch time: 41.09 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4196%\n",
      "layer   2  Sparsity: 81.9755%\n",
      "layer   3  Sparsity: 79.7514%\n",
      "total_backward_count 734250 real_backward_count 156686  21.340%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.747070/  1.930775, val:  44.17%, val_best:  70.83%, tr:  94.18%, tr_best:  96.73%, epoch time: 41.65 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.4244%\n",
      "layer   2  Sparsity: 81.3856%\n",
      "layer   3  Sparsity: 80.5272%\n",
      "total_backward_count 739145 real_backward_count 157588  21.320%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.762420/  1.908357, val:  47.50%, val_best:  70.83%, tr:  94.18%, tr_best:  96.73%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3411%\n",
      "layer   2  Sparsity: 81.4834%\n",
      "layer   3  Sparsity: 79.9683%\n",
      "total_backward_count 744040 real_backward_count 158531  21.307%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.749616/  1.905478, val:  61.25%, val_best:  70.83%, tr:  95.71%, tr_best:  96.73%, epoch time: 41.35 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3727%\n",
      "layer   2  Sparsity: 81.1926%\n",
      "layer   3  Sparsity: 79.5090%\n",
      "total_backward_count 748935 real_backward_count 159401  21.284%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.778264/  1.886077, val:  56.67%, val_best:  70.83%, tr:  94.18%, tr_best:  96.73%, epoch time: 40.90 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.2972%\n",
      "layer   2  Sparsity: 80.9915%\n",
      "layer   3  Sparsity: 78.9096%\n",
      "total_backward_count 753830 real_backward_count 160332  21.269%\n",
      "lif layer 1 self.abs_max_v: 24966.5\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.752255/  1.897376, val:  66.25%, val_best:  70.83%, tr:  93.97%, tr_best:  96.73%, epoch time: 41.61 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3560%\n",
      "layer   2  Sparsity: 81.3205%\n",
      "layer   3  Sparsity: 79.7073%\n",
      "total_backward_count 758725 real_backward_count 161257  21.254%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.766950/  1.926840, val:  59.17%, val_best:  70.83%, tr:  93.97%, tr_best:  96.73%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3653%\n",
      "layer   2  Sparsity: 81.7904%\n",
      "layer   3  Sparsity: 80.1216%\n",
      "total_backward_count 763620 real_backward_count 162152  21.235%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.743235/  1.921697, val:  58.75%, val_best:  70.83%, tr:  93.97%, tr_best:  96.73%, epoch time: 41.18 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3871%\n",
      "layer   2  Sparsity: 81.4841%\n",
      "layer   3  Sparsity: 79.6053%\n",
      "total_backward_count 768515 real_backward_count 163133  21.227%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.750405/  1.899759, val:  53.75%, val_best:  70.83%, tr:  94.48%, tr_best:  96.73%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.4815%\n",
      "layer   2  Sparsity: 81.7098%\n",
      "layer   3  Sparsity: 80.1086%\n",
      "total_backward_count 773410 real_backward_count 164040  21.210%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.744723/  1.897698, val:  65.83%, val_best:  70.83%, tr:  95.81%, tr_best:  96.73%, epoch time: 41.07 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3681%\n",
      "layer   2  Sparsity: 81.5797%\n",
      "layer   3  Sparsity: 80.0010%\n",
      "total_backward_count 778305 real_backward_count 164916  21.189%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.752228/  1.911620, val:  60.83%, val_best:  70.83%, tr:  93.77%, tr_best:  96.73%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4375%\n",
      "layer   2  Sparsity: 82.1979%\n",
      "layer   3  Sparsity: 80.1816%\n",
      "total_backward_count 783200 real_backward_count 165879  21.180%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.778603/  1.941625, val:  56.67%, val_best:  70.83%, tr:  95.61%, tr_best:  96.73%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3473%\n",
      "layer   2  Sparsity: 81.5622%\n",
      "layer   3  Sparsity: 79.2431%\n",
      "total_backward_count 788095 real_backward_count 166789  21.164%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.763882/  1.896931, val:  63.33%, val_best:  70.83%, tr:  94.38%, tr_best:  96.73%, epoch time: 40.94 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4056%\n",
      "layer   2  Sparsity: 81.5047%\n",
      "layer   3  Sparsity: 79.4649%\n",
      "total_backward_count 792990 real_backward_count 167679  21.145%\n",
      "lif layer 1 self.abs_max_v: 25010.5\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.748470/  1.918911, val:  56.25%, val_best:  70.83%, tr:  95.51%, tr_best:  96.73%, epoch time: 40.82 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3514%\n",
      "layer   2  Sparsity: 81.2009%\n",
      "layer   3  Sparsity: 79.1653%\n",
      "total_backward_count 797885 real_backward_count 168589  21.129%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.732973/  1.917408, val:  51.67%, val_best:  70.83%, tr:  94.79%, tr_best:  96.73%, epoch time: 41.15 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3586%\n",
      "layer   2  Sparsity: 81.4925%\n",
      "layer   3  Sparsity: 79.2098%\n",
      "total_backward_count 802780 real_backward_count 169468  21.110%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.734951/  1.891098, val:  60.00%, val_best:  70.83%, tr:  94.79%, tr_best:  96.73%, epoch time: 40.74 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4316%\n",
      "layer   2  Sparsity: 81.8103%\n",
      "layer   3  Sparsity: 79.5625%\n",
      "total_backward_count 807675 real_backward_count 170357  21.092%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.729337/  1.874488, val:  59.17%, val_best:  70.83%, tr:  95.61%, tr_best:  96.73%, epoch time: 41.41 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3870%\n",
      "layer   2  Sparsity: 81.2760%\n",
      "layer   3  Sparsity: 79.2984%\n",
      "total_backward_count 812570 real_backward_count 171223  21.072%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.723903/  1.898232, val:  57.92%, val_best:  70.83%, tr:  94.59%, tr_best:  96.73%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3607%\n",
      "layer   2  Sparsity: 80.4740%\n",
      "layer   3  Sparsity: 79.8993%\n",
      "total_backward_count 817465 real_backward_count 172161  21.060%\n",
      "lif layer 1 self.abs_max_v: 25052.0\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.746949/  1.905854, val:  51.67%, val_best:  70.83%, tr:  94.89%, tr_best:  96.73%, epoch time: 40.87 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4116%\n",
      "layer   2  Sparsity: 80.5791%\n",
      "layer   3  Sparsity: 80.5704%\n",
      "total_backward_count 822360 real_backward_count 173058  21.044%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.744045/  1.901642, val:  65.42%, val_best:  70.83%, tr:  94.48%, tr_best:  96.73%, epoch time: 41.20 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3792%\n",
      "layer   2  Sparsity: 80.2380%\n",
      "layer   3  Sparsity: 78.8445%\n",
      "total_backward_count 827255 real_backward_count 173976  21.031%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.731109/  1.866532, val:  58.75%, val_best:  70.83%, tr:  95.51%, tr_best:  96.73%, epoch time: 41.13 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3674%\n",
      "layer   2  Sparsity: 79.8741%\n",
      "layer   3  Sparsity: 78.4317%\n",
      "total_backward_count 832150 real_backward_count 174821  21.008%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.732814/  1.899745, val:  51.67%, val_best:  70.83%, tr:  94.28%, tr_best:  96.73%, epoch time: 40.81 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4218%\n",
      "layer   2  Sparsity: 79.6636%\n",
      "layer   3  Sparsity: 78.1900%\n",
      "total_backward_count 837045 real_backward_count 175694  20.990%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.718521/  1.927375, val:  45.42%, val_best:  70.83%, tr:  94.28%, tr_best:  96.73%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4196%\n",
      "layer   2  Sparsity: 79.9314%\n",
      "layer   3  Sparsity: 77.8243%\n",
      "total_backward_count 841940 real_backward_count 176569  20.972%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.717667/  1.893605, val:  51.67%, val_best:  70.83%, tr:  95.71%, tr_best:  96.73%, epoch time: 41.00 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3849%\n",
      "layer   2  Sparsity: 80.1249%\n",
      "layer   3  Sparsity: 78.2993%\n",
      "total_backward_count 846835 real_backward_count 177435  20.953%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.716449/  1.871996, val:  62.08%, val_best:  70.83%, tr:  95.51%, tr_best:  96.73%, epoch time: 41.14 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3426%\n",
      "layer   2  Sparsity: 80.2104%\n",
      "layer   3  Sparsity: 77.7262%\n",
      "total_backward_count 851730 real_backward_count 178301  20.934%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.705855/  1.854016, val:  62.50%, val_best:  70.83%, tr:  95.81%, tr_best:  96.73%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3791%\n",
      "layer   2  Sparsity: 80.2562%\n",
      "layer   3  Sparsity: 78.0310%\n",
      "total_backward_count 856625 real_backward_count 179224  20.922%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.710123/  1.861124, val:  50.83%, val_best:  70.83%, tr:  95.40%, tr_best:  96.73%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3849%\n",
      "layer   2  Sparsity: 80.5671%\n",
      "layer   3  Sparsity: 78.5973%\n",
      "total_backward_count 861520 real_backward_count 180099  20.905%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.707311/  1.869836, val:  50.83%, val_best:  70.83%, tr:  94.69%, tr_best:  96.73%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4468%\n",
      "layer   2  Sparsity: 80.6173%\n",
      "layer   3  Sparsity: 78.7693%\n",
      "total_backward_count 866415 real_backward_count 180953  20.885%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.701203/  1.881614, val:  57.08%, val_best:  70.83%, tr:  95.71%, tr_best:  96.73%, epoch time: 41.13 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3566%\n",
      "layer   2  Sparsity: 80.9575%\n",
      "layer   3  Sparsity: 79.2951%\n",
      "total_backward_count 871310 real_backward_count 181817  20.867%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.723640/  1.901321, val:  55.00%, val_best:  70.83%, tr:  95.61%, tr_best:  96.73%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3816%\n",
      "layer   2  Sparsity: 80.8713%\n",
      "layer   3  Sparsity: 79.2513%\n",
      "total_backward_count 876205 real_backward_count 182699  20.851%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.729949/  1.883712, val:  71.67%, val_best:  71.67%, tr:  95.20%, tr_best:  96.73%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3537%\n",
      "layer   2  Sparsity: 80.3687%\n",
      "layer   3  Sparsity: 78.9295%\n",
      "total_backward_count 881100 real_backward_count 183587  20.836%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.728020/  1.877368, val:  65.42%, val_best:  71.67%, tr:  95.71%, tr_best:  96.73%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.4262%\n",
      "layer   2  Sparsity: 80.7340%\n",
      "layer   3  Sparsity: 78.4901%\n",
      "total_backward_count 885995 real_backward_count 184456  20.819%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.713426/  1.880907, val:  57.50%, val_best:  71.67%, tr:  96.22%, tr_best:  96.73%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3714%\n",
      "layer   2  Sparsity: 80.2428%\n",
      "layer   3  Sparsity: 78.6428%\n",
      "total_backward_count 890890 real_backward_count 185254  20.794%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.710380/  1.907321, val:  65.83%, val_best:  71.67%, tr:  94.99%, tr_best:  96.73%, epoch time: 40.71 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4109%\n",
      "layer   2  Sparsity: 80.8079%\n",
      "layer   3  Sparsity: 78.1268%\n",
      "total_backward_count 895785 real_backward_count 186104  20.776%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.697701/  1.891973, val:  48.75%, val_best:  71.67%, tr:  95.71%, tr_best:  96.73%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3119%\n",
      "layer   2  Sparsity: 81.0910%\n",
      "layer   3  Sparsity: 78.0782%\n",
      "total_backward_count 900680 real_backward_count 186982  20.760%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.706985/  1.871226, val:  57.92%, val_best:  71.67%, tr:  95.71%, tr_best:  96.73%, epoch time: 40.85 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3619%\n",
      "layer   2  Sparsity: 80.6873%\n",
      "layer   3  Sparsity: 78.8990%\n",
      "total_backward_count 905575 real_backward_count 187880  20.747%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.704991/  1.896516, val:  61.67%, val_best:  71.67%, tr:  94.99%, tr_best:  96.73%, epoch time: 41.20 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3684%\n",
      "layer   2  Sparsity: 80.7418%\n",
      "layer   3  Sparsity: 78.8702%\n",
      "total_backward_count 910470 real_backward_count 188823  20.739%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.719433/  1.886784, val:  52.92%, val_best:  71.67%, tr:  94.38%, tr_best:  96.73%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3513%\n",
      "layer   2  Sparsity: 80.9485%\n",
      "layer   3  Sparsity: 80.0354%\n",
      "total_backward_count 915365 real_backward_count 189725  20.727%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.716910/  1.896869, val:  52.08%, val_best:  71.67%, tr:  95.10%, tr_best:  96.73%, epoch time: 41.52 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3792%\n",
      "layer   2  Sparsity: 81.1525%\n",
      "layer   3  Sparsity: 79.7409%\n",
      "total_backward_count 920260 real_backward_count 190676  20.720%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.730568/  1.900451, val:  59.17%, val_best:  71.67%, tr:  94.89%, tr_best:  96.73%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3760%\n",
      "layer   2  Sparsity: 81.2470%\n",
      "layer   3  Sparsity: 80.0683%\n",
      "total_backward_count 925155 real_backward_count 191592  20.709%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.722787/  1.864501, val:  61.67%, val_best:  71.67%, tr:  95.61%, tr_best:  96.73%, epoch time: 41.20 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.4364%\n",
      "layer   2  Sparsity: 81.0016%\n",
      "layer   3  Sparsity: 79.8413%\n",
      "total_backward_count 930050 real_backward_count 192452  20.693%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.721783/  1.882412, val:  56.25%, val_best:  71.67%, tr:  96.42%, tr_best:  96.73%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4548%\n",
      "layer   2  Sparsity: 81.0227%\n",
      "layer   3  Sparsity: 79.5757%\n",
      "total_backward_count 934945 real_backward_count 193277  20.673%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.732539/  1.912024, val:  54.17%, val_best:  71.67%, tr:  95.51%, tr_best:  96.73%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.4260%\n",
      "layer   2  Sparsity: 81.5820%\n",
      "layer   3  Sparsity: 80.2725%\n",
      "total_backward_count 939840 real_backward_count 194141  20.657%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.718394/  1.882587, val:  60.00%, val_best:  71.67%, tr:  96.02%, tr_best:  96.73%, epoch time: 40.96 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3436%\n",
      "layer   2  Sparsity: 81.7092%\n",
      "layer   3  Sparsity: 80.2183%\n",
      "total_backward_count 944735 real_backward_count 195021  20.643%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.747426/  1.937034, val:  52.50%, val_best:  71.67%, tr:  94.69%, tr_best:  96.73%, epoch time: 40.81 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3792%\n",
      "layer   2  Sparsity: 81.9687%\n",
      "layer   3  Sparsity: 80.9048%\n",
      "total_backward_count 949630 real_backward_count 195942  20.634%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.757649/  1.877691, val:  57.08%, val_best:  71.67%, tr:  94.79%, tr_best:  96.73%, epoch time: 41.15 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3812%\n",
      "layer   2  Sparsity: 81.8874%\n",
      "layer   3  Sparsity: 80.6858%\n",
      "total_backward_count 954525 real_backward_count 196881  20.626%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.730266/  1.886942, val:  53.33%, val_best:  71.67%, tr:  94.59%, tr_best:  96.73%, epoch time: 41.09 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3108%\n",
      "layer   2  Sparsity: 82.1894%\n",
      "layer   3  Sparsity: 80.2449%\n",
      "total_backward_count 959420 real_backward_count 197758  20.612%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.722248/  1.891346, val:  59.17%, val_best:  71.67%, tr:  94.69%, tr_best:  96.73%, epoch time: 41.29 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.3701%\n",
      "layer   2  Sparsity: 82.1489%\n",
      "layer   3  Sparsity: 78.6445%\n",
      "total_backward_count 964315 real_backward_count 198688  20.604%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.701465/  1.869940, val:  61.25%, val_best:  71.67%, tr:  95.10%, tr_best:  96.73%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3795%\n",
      "layer   2  Sparsity: 81.3487%\n",
      "layer   3  Sparsity: 79.0464%\n",
      "total_backward_count 969210 real_backward_count 199543  20.588%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.722417/  1.863819, val:  62.08%, val_best:  71.67%, tr:  94.99%, tr_best:  96.73%, epoch time: 40.96 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.3754%\n",
      "layer   2  Sparsity: 81.3737%\n",
      "layer   3  Sparsity: 78.9253%\n",
      "total_backward_count 974105 real_backward_count 200404  20.573%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.712112/  1.868471, val:  64.58%, val_best:  71.67%, tr:  96.42%, tr_best:  96.73%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.3734%\n",
      "layer   2  Sparsity: 82.1838%\n",
      "layer   3  Sparsity: 78.7104%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e3c94f6e7048f891c484988356a6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÜ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñà</td></tr><tr><td>val_loss</td><td>‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.96425</td></tr><tr><td>tr_epoch_loss</td><td>1.71211</td></tr><tr><td>val_acc_best</td><td>0.71667</td></tr><tr><td>val_acc_now</td><td>0.64583</td></tr><tr><td>val_loss</td><td>1.86847</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glad-sweep-351</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hyaiofsy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hyaiofsy</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251128_100808-hyaiofsy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lmu9n24y with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 75000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251128_122513-lmu9n24y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lmu9n24y' target=\"_blank\">eager-sweep-355</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lmu9n24y' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lmu9n24y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251128_122522_325', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 20, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 75000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 4761a05d236841af8daa3414213005d1\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=20, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=20, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 91\n",
      "fc layer 1 self.abs_max_out: 964.0\n",
      "lif layer 1 self.abs_max_v: 964.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 667.0\n",
      "lif layer 2 self.abs_max_v: 667.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 119.0\n",
      "lif layer 1 self.abs_max_v: 1399.0\n",
      "lif layer 2 self.abs_max_v: 910.0\n",
      "fc layer 3 self.abs_max_out: 198.0\n",
      "lif layer 1 self.abs_max_v: 1470.5\n",
      "fc layer 2 self.abs_max_out: 1169.0\n",
      "lif layer 2 self.abs_max_v: 1402.5\n",
      "fc layer 3 self.abs_max_out: 457.0\n",
      "lif layer 1 self.abs_max_v: 1482.0\n",
      "fc layer 2 self.abs_max_out: 1178.0\n",
      "smallest_now_T updated: 82\n",
      "fc layer 1 self.abs_max_out: 1137.0\n",
      "fc layer 2 self.abs_max_out: 1392.0\n",
      "fc layer 1 self.abs_max_out: 1345.0\n",
      "lif layer 1 self.abs_max_v: 1725.0\n",
      "fc layer 3 self.abs_max_out: 491.0\n",
      "fc layer 1 self.abs_max_out: 1709.0\n",
      "lif layer 1 self.abs_max_v: 2103.0\n",
      "lif layer 2 self.abs_max_v: 1413.5\n",
      "lif layer 1 self.abs_max_v: 2133.5\n",
      "fc layer 2 self.abs_max_out: 1538.0\n",
      "lif layer 2 self.abs_max_v: 2007.5\n",
      "lif layer 1 self.abs_max_v: 2325.0\n",
      "smallest_now_T updated: 61\n",
      "fc layer 2 self.abs_max_out: 1787.0\n",
      "lif layer 2 self.abs_max_v: 2222.5\n",
      "fc layer 3 self.abs_max_out: 504.0\n",
      "lif layer 1 self.abs_max_v: 2686.0\n",
      "lif layer 1 self.abs_max_v: 2981.0\n",
      "lif layer 2 self.abs_max_v: 2476.0\n",
      "fc layer 1 self.abs_max_out: 1839.0\n",
      "lif layer 1 self.abs_max_v: 3071.0\n",
      "fc layer 1 self.abs_max_out: 2017.0\n",
      "lif layer 1 self.abs_max_v: 3552.5\n",
      "fc layer 2 self.abs_max_out: 1835.0\n",
      "fc layer 2 self.abs_max_out: 2228.0\n",
      "lif layer 2 self.abs_max_v: 2569.0\n",
      "fc layer 3 self.abs_max_out: 511.0\n",
      "lif layer 2 self.abs_max_v: 2621.0\n",
      "fc layer 1 self.abs_max_out: 2210.0\n",
      "lif layer 2 self.abs_max_v: 2714.5\n",
      "fc layer 3 self.abs_max_out: 557.0\n",
      "fc layer 3 self.abs_max_out: 724.0\n",
      "smallest_now_T updated: 51\n",
      "fc layer 3 self.abs_max_out: 777.0\n",
      "fc layer 1 self.abs_max_out: 2468.0\n",
      "lif layer 2 self.abs_max_v: 2725.0\n",
      "lif layer 2 self.abs_max_v: 2784.0\n",
      "smallest_now_T updated: 47\n",
      "fc layer 1 self.abs_max_out: 2706.0\n",
      "lif layer 2 self.abs_max_v: 2807.5\n",
      "fc layer 2 self.abs_max_out: 2346.0\n",
      "lif layer 2 self.abs_max_v: 2850.5\n",
      "lif layer 2 self.abs_max_v: 3041.5\n",
      "fc layer 1 self.abs_max_out: 2844.0\n",
      "smallest_now_T updated: 42\n",
      "lif layer 2 self.abs_max_v: 3045.0\n",
      "lif layer 2 self.abs_max_v: 3321.5\n",
      "fc layer 1 self.abs_max_out: 2931.0\n",
      "fc layer 1 self.abs_max_out: 3007.0\n",
      "fc layer 1 self.abs_max_out: 3284.0\n",
      "lif layer 1 self.abs_max_v: 3560.5\n",
      "smallest_now_T updated: 36\n",
      "fc layer 3 self.abs_max_out: 950.0\n",
      "smallest_now_T updated: 27\n",
      "lif layer 2 self.abs_max_v: 3387.0\n",
      "lif layer 2 self.abs_max_v: 3521.5\n",
      "lif layer 1 self.abs_max_v: 3708.5\n",
      "lif layer 2 self.abs_max_v: 3608.5\n",
      "lif layer 1 self.abs_max_v: 3772.5\n",
      "lif layer 2 self.abs_max_v: 3882.5\n",
      "lif layer 2 self.abs_max_v: 4058.5\n",
      "fc layer 1 self.abs_max_out: 3473.0\n",
      "fc layer 2 self.abs_max_out: 2442.0\n",
      "fc layer 2 self.abs_max_out: 2531.0\n",
      "fc layer 2 self.abs_max_out: 2617.0\n",
      "fc layer 1 self.abs_max_out: 3528.0\n",
      "fc layer 2 self.abs_max_out: 2991.0\n",
      "lif layer 1 self.abs_max_v: 3918.0\n",
      "fc layer 3 self.abs_max_out: 1017.0\n",
      "fc layer 1 self.abs_max_out: 3639.0\n",
      "fc layer 1 self.abs_max_out: 3828.0\n",
      "fc layer 1 self.abs_max_out: 3858.0\n",
      "lif layer 1 self.abs_max_v: 4317.0\n",
      "fc layer 1 self.abs_max_out: 4122.0\n",
      "smallest_now_T_val updated: 84\n",
      "smallest_now_T_val updated: 69\n",
      "smallest_now_T_val updated: 68\n",
      "smallest_now_T_val updated: 67\n",
      "smallest_now_T_val updated: 55\n",
      "smallest_now_T_val updated: 25\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.102177/  2.086301, val:  46.67%, val_best:  46.67%, tr:  52.60%, tr_best:  52.60%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8569%\n",
      "layer   2  Sparsity: 81.5354%\n",
      "layer   3  Sparsity: 83.1270%\n",
      "total_backward_count 4895 real_backward_count 2946  60.184%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 4404.5\n",
      "fc layer 1 self.abs_max_out: 4246.0\n",
      "lif layer 1 self.abs_max_v: 4495.5\n",
      "fc layer 1 self.abs_max_out: 4288.0\n",
      "fc layer 1 self.abs_max_out: 4520.0\n",
      "lif layer 1 self.abs_max_v: 4520.0\n",
      "fc layer 1 self.abs_max_out: 5411.0\n",
      "lif layer 1 self.abs_max_v: 5411.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.011090/  2.058901, val:  56.67%, val_best:  56.67%, tr:  74.77%, tr_best:  74.77%, epoch time: 41.10 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8904%\n",
      "layer   2  Sparsity: 80.9283%\n",
      "layer   3  Sparsity: 81.3942%\n",
      "total_backward_count 9790 real_backward_count 4943  50.490%\n",
      "lif layer 1 self.abs_max_v: 5562.5\n",
      "lif layer 1 self.abs_max_v: 5764.0\n",
      "fc layer 3 self.abs_max_out: 1035.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  2.004652/  2.056489, val:  51.67%, val_best:  56.67%, tr:  79.67%, tr_best:  79.67%, epoch time: 40.24 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9222%\n",
      "layer   2  Sparsity: 80.5944%\n",
      "layer   3  Sparsity: 81.3456%\n",
      "total_backward_count 14685 real_backward_count 6689  45.550%\n",
      "fc layer 1 self.abs_max_out: 5849.0\n",
      "lif layer 1 self.abs_max_v: 5849.0\n",
      "fc layer 2 self.abs_max_out: 3027.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  2.007076/  2.065423, val:  54.17%, val_best:  56.67%, tr:  82.43%, tr_best:  82.43%, epoch time: 41.50 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.8999%\n",
      "layer   2  Sparsity: 80.7952%\n",
      "layer   3  Sparsity: 81.0866%\n",
      "total_backward_count 19580 real_backward_count 8223  41.997%\n",
      "fc layer 1 self.abs_max_out: 5914.0\n",
      "lif layer 1 self.abs_max_v: 5914.0\n",
      "lif layer 2 self.abs_max_v: 4059.5\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  2.010610/  2.058947, val:  62.92%, val_best:  62.92%, tr:  86.52%, tr_best:  86.52%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8483%\n",
      "layer   2  Sparsity: 81.2169%\n",
      "layer   3  Sparsity: 80.7928%\n",
      "total_backward_count 24475 real_backward_count 9639  39.383%\n",
      "lif layer 1 self.abs_max_v: 6423.5\n",
      "lif layer 2 self.abs_max_v: 4155.5\n",
      "lif layer 1 self.abs_max_v: 6695.0\n",
      "lif layer 1 self.abs_max_v: 6904.5\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  2.008849/  2.056436, val:  66.67%, val_best:  66.67%, tr:  88.25%, tr_best:  88.25%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8807%\n",
      "layer   2  Sparsity: 81.7751%\n",
      "layer   3  Sparsity: 81.3674%\n",
      "total_backward_count 29370 real_backward_count 10949  37.280%\n",
      "fc layer 2 self.abs_max_out: 3152.0\n",
      "fc layer 1 self.abs_max_out: 6016.0\n",
      "lif layer 1 self.abs_max_v: 7601.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  2.012324/  2.058731, val:  60.00%, val_best:  66.67%, tr:  87.84%, tr_best:  88.25%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8934%\n",
      "layer   2  Sparsity: 81.8653%\n",
      "layer   3  Sparsity: 80.9176%\n",
      "total_backward_count 34265 real_backward_count 12207  35.625%\n",
      "fc layer 1 self.abs_max_out: 6070.0\n",
      "lif layer 1 self.abs_max_v: 7933.0\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  2.016259/  2.061377, val:  61.67%, val_best:  66.67%, tr:  91.22%, tr_best:  91.22%, epoch time: 40.84 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8708%\n",
      "layer   2  Sparsity: 81.7216%\n",
      "layer   3  Sparsity: 81.1674%\n",
      "total_backward_count 39160 real_backward_count 13350  34.091%\n",
      "lif layer 2 self.abs_max_v: 4480.0\n",
      "fc layer 1 self.abs_max_out: 6209.0\n",
      "lif layer 1 self.abs_max_v: 8361.0\n",
      "lif layer 1 self.abs_max_v: 8462.5\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  2.012648/  2.052397, val:  70.42%, val_best:  70.42%, tr:  91.62%, tr_best:  91.62%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9379%\n",
      "layer   2  Sparsity: 81.9463%\n",
      "layer   3  Sparsity: 80.9104%\n",
      "total_backward_count 44055 real_backward_count 14475  32.857%\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  2.007981/  2.061265, val:  61.67%, val_best:  70.42%, tr:  92.95%, tr_best:  92.95%, epoch time: 41.00 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8655%\n",
      "layer   2  Sparsity: 82.0818%\n",
      "layer   3  Sparsity: 80.9295%\n",
      "total_backward_count 48950 real_backward_count 15500  31.665%\n",
      "fc layer 1 self.abs_max_out: 6273.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  2.010173/  2.064487, val:  61.67%, val_best:  70.42%, tr:  94.89%, tr_best:  94.89%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8814%\n",
      "layer   2  Sparsity: 81.9552%\n",
      "layer   3  Sparsity: 80.8933%\n",
      "total_backward_count 53845 real_backward_count 16441  30.534%\n",
      "lif layer 1 self.abs_max_v: 8672.0\n",
      "fc layer 1 self.abs_max_out: 6287.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  2.011079/  2.060743, val:  78.33%, val_best:  78.33%, tr:  93.05%, tr_best:  94.89%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8912%\n",
      "layer   2  Sparsity: 81.2658%\n",
      "layer   3  Sparsity: 80.2984%\n",
      "total_backward_count 58740 real_backward_count 17397  29.617%\n",
      "lif layer 1 self.abs_max_v: 8762.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  2.007132/  2.062332, val:  68.75%, val_best:  78.33%, tr:  94.69%, tr_best:  94.89%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8756%\n",
      "layer   2  Sparsity: 81.4463%\n",
      "layer   3  Sparsity: 80.2148%\n",
      "total_backward_count 63635 real_backward_count 18283  28.731%\n",
      "lif layer 1 self.abs_max_v: 8905.5\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  2.015012/  2.059349, val:  67.92%, val_best:  78.33%, tr:  95.61%, tr_best:  95.61%, epoch time: 39.50 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9097%\n",
      "layer   2  Sparsity: 81.5715%\n",
      "layer   3  Sparsity: 80.6182%\n",
      "total_backward_count 68530 real_backward_count 19139  27.928%\n",
      "fc layer 1 self.abs_max_out: 6333.0\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  2.012526/  2.057429, val:  72.92%, val_best:  78.33%, tr:  96.32%, tr_best:  96.32%, epoch time: 39.53 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8680%\n",
      "layer   2  Sparsity: 81.4111%\n",
      "layer   3  Sparsity: 80.4398%\n",
      "total_backward_count 73425 real_backward_count 19926  27.138%\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  2.017709/  2.075426, val:  70.00%, val_best:  78.33%, tr:  97.24%, tr_best:  97.24%, epoch time: 40.80 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8839%\n",
      "layer   2  Sparsity: 81.4563%\n",
      "layer   3  Sparsity: 80.6596%\n",
      "total_backward_count 78320 real_backward_count 20706  26.438%\n",
      "lif layer 1 self.abs_max_v: 9239.0\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  2.020597/  2.057489, val:  74.17%, val_best:  78.33%, tr:  97.45%, tr_best:  97.45%, epoch time: 40.48 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8820%\n",
      "layer   2  Sparsity: 81.7418%\n",
      "layer   3  Sparsity: 80.8272%\n",
      "total_backward_count 83215 real_backward_count 21456  25.784%\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  2.023893/  2.074450, val:  80.42%, val_best:  80.42%, tr:  96.42%, tr_best:  97.45%, epoch time: 40.58 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8365%\n",
      "layer   2  Sparsity: 81.8553%\n",
      "layer   3  Sparsity: 81.1613%\n",
      "total_backward_count 88110 real_backward_count 22241  25.242%\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  2.024777/  2.066631, val:  75.83%, val_best:  80.42%, tr:  96.73%, tr_best:  97.45%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9005%\n",
      "layer   2  Sparsity: 82.0242%\n",
      "layer   3  Sparsity: 81.3651%\n",
      "total_backward_count 93005 real_backward_count 22972  24.700%\n",
      "fc layer 1 self.abs_max_out: 6492.0\n",
      "lif layer 1 self.abs_max_v: 9290.0\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  2.027601/  2.066615, val:  79.17%, val_best:  80.42%, tr:  97.24%, tr_best:  97.45%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8598%\n",
      "layer   2  Sparsity: 81.9072%\n",
      "layer   3  Sparsity: 81.7092%\n",
      "total_backward_count 97900 real_backward_count 23605  24.111%\n",
      "fc layer 1 self.abs_max_out: 6548.0\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  2.027352/  2.073986, val:  73.75%, val_best:  80.42%, tr:  97.24%, tr_best:  97.45%, epoch time: 40.84 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8772%\n",
      "layer   2  Sparsity: 81.7581%\n",
      "layer   3  Sparsity: 81.4517%\n",
      "total_backward_count 102795 real_backward_count 24289  23.629%\n",
      "fc layer 1 self.abs_max_out: 6608.0\n",
      "lif layer 1 self.abs_max_v: 9297.0\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  2.022470/  2.074225, val:  82.92%, val_best:  82.92%, tr:  97.85%, tr_best:  97.85%, epoch time: 41.24 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.8805%\n",
      "layer   2  Sparsity: 82.3547%\n",
      "layer   3  Sparsity: 81.6303%\n",
      "total_backward_count 107690 real_backward_count 24920  23.140%\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  2.025556/  2.068012, val:  71.67%, val_best:  82.92%, tr:  96.73%, tr_best:  97.85%, epoch time: 40.48 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8925%\n",
      "layer   2  Sparsity: 82.0336%\n",
      "layer   3  Sparsity: 81.2610%\n",
      "total_backward_count 112585 real_backward_count 25583  22.723%\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  2.023824/  2.071570, val:  81.67%, val_best:  82.92%, tr:  97.96%, tr_best:  97.96%, epoch time: 40.26 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8452%\n",
      "layer   2  Sparsity: 81.9206%\n",
      "layer   3  Sparsity: 81.4280%\n",
      "total_backward_count 117480 real_backward_count 26206  22.307%\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  2.028372/  2.077458, val:  71.25%, val_best:  82.92%, tr:  97.45%, tr_best:  97.96%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8882%\n",
      "layer   2  Sparsity: 82.0767%\n",
      "layer   3  Sparsity: 81.6226%\n",
      "total_backward_count 122375 real_backward_count 26840  21.933%\n",
      "lif layer 1 self.abs_max_v: 9328.5\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  2.028437/  2.077053, val:  82.08%, val_best:  82.92%, tr:  97.75%, tr_best:  97.96%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8798%\n",
      "layer   2  Sparsity: 82.2375%\n",
      "layer   3  Sparsity: 81.8897%\n",
      "total_backward_count 127270 real_backward_count 27488  21.598%\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  2.027039/  2.080266, val:  78.75%, val_best:  82.92%, tr:  97.75%, tr_best:  97.96%, epoch time: 40.95 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9264%\n",
      "layer   2  Sparsity: 82.3108%\n",
      "layer   3  Sparsity: 81.7839%\n",
      "total_backward_count 132165 real_backward_count 28074  21.242%\n",
      "lif layer 2 self.abs_max_v: 4704.0\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  2.027085/  2.074251, val:  78.75%, val_best:  82.92%, tr:  98.77%, tr_best:  98.77%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8620%\n",
      "layer   2  Sparsity: 82.2440%\n",
      "layer   3  Sparsity: 81.8332%\n",
      "total_backward_count 137060 real_backward_count 28601  20.868%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  2.025170/  2.078304, val:  73.33%, val_best:  82.92%, tr:  98.37%, tr_best:  98.77%, epoch time: 41.14 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.8968%\n",
      "layer   2  Sparsity: 81.9758%\n",
      "layer   3  Sparsity: 81.7475%\n",
      "total_backward_count 141955 real_backward_count 29131  20.521%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  2.030304/  2.075397, val:  79.17%, val_best:  82.92%, tr:  97.65%, tr_best:  98.77%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8611%\n",
      "layer   2  Sparsity: 81.9910%\n",
      "layer   3  Sparsity: 81.7744%\n",
      "total_backward_count 146850 real_backward_count 29691  20.219%\n",
      "lif layer 1 self.abs_max_v: 9404.5\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  2.033765/  2.081819, val:  82.92%, val_best:  82.92%, tr:  98.98%, tr_best:  98.98%, epoch time: 40.26 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8918%\n",
      "layer   2  Sparsity: 81.8655%\n",
      "layer   3  Sparsity: 81.3886%\n",
      "total_backward_count 151745 real_backward_count 30255  19.938%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  2.029607/  2.081652, val:  74.17%, val_best:  82.92%, tr:  99.08%, tr_best:  99.08%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8686%\n",
      "layer   2  Sparsity: 81.7922%\n",
      "layer   3  Sparsity: 81.4578%\n",
      "total_backward_count 156640 real_backward_count 30781  19.651%\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  2.028242/  2.075017, val:  72.08%, val_best:  82.92%, tr:  98.67%, tr_best:  99.08%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9126%\n",
      "layer   2  Sparsity: 81.7945%\n",
      "layer   3  Sparsity: 81.5442%\n",
      "total_backward_count 161535 real_backward_count 31251  19.346%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  2.031893/  2.080659, val:  81.67%, val_best:  82.92%, tr:  98.77%, tr_best:  99.08%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8883%\n",
      "layer   2  Sparsity: 81.5667%\n",
      "layer   3  Sparsity: 81.4432%\n",
      "total_backward_count 166430 real_backward_count 31723  19.061%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  2.031925/  2.079007, val:  75.00%, val_best:  82.92%, tr:  98.57%, tr_best:  99.08%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8822%\n",
      "layer   2  Sparsity: 81.6655%\n",
      "layer   3  Sparsity: 81.5822%\n",
      "total_backward_count 171325 real_backward_count 32191  18.789%\n",
      "lif layer 1 self.abs_max_v: 9610.5\n",
      "lif layer 1 self.abs_max_v: 10722.0\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  2.032281/  2.075516, val:  83.75%, val_best:  83.75%, tr:  99.39%, tr_best:  99.39%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8964%\n",
      "layer   2  Sparsity: 81.5904%\n",
      "layer   3  Sparsity: 82.2451%\n",
      "total_backward_count 176220 real_backward_count 32609  18.505%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  2.029309/  2.080701, val:  81.67%, val_best:  83.75%, tr:  99.08%, tr_best:  99.39%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8796%\n",
      "layer   2  Sparsity: 81.5309%\n",
      "layer   3  Sparsity: 82.1204%\n",
      "total_backward_count 181115 real_backward_count 33031  18.238%\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  2.029350/  2.077893, val:  80.42%, val_best:  83.75%, tr:  98.67%, tr_best:  99.39%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9242%\n",
      "layer   2  Sparsity: 81.7514%\n",
      "layer   3  Sparsity: 82.4189%\n",
      "total_backward_count 186010 real_backward_count 33440  17.978%\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  2.034838/  2.086772, val:  81.67%, val_best:  83.75%, tr:  99.08%, tr_best:  99.39%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8925%\n",
      "layer   2  Sparsity: 81.6961%\n",
      "layer   3  Sparsity: 82.5760%\n",
      "total_backward_count 190905 real_backward_count 33870  17.742%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  2.035764/  2.081997, val:  81.67%, val_best:  83.75%, tr:  99.28%, tr_best:  99.39%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8651%\n",
      "layer   2  Sparsity: 81.6353%\n",
      "layer   3  Sparsity: 82.7673%\n",
      "total_backward_count 195800 real_backward_count 34316  17.526%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  2.029735/  2.080309, val:  82.92%, val_best:  83.75%, tr:  98.98%, tr_best:  99.39%, epoch time: 40.07 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8611%\n",
      "layer   2  Sparsity: 81.7695%\n",
      "layer   3  Sparsity: 82.4283%\n",
      "total_backward_count 200695 real_backward_count 34742  17.311%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  2.030433/  2.079942, val:  82.50%, val_best:  83.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8839%\n",
      "layer   2  Sparsity: 81.6036%\n",
      "layer   3  Sparsity: 82.4422%\n",
      "total_backward_count 205590 real_backward_count 35072  17.059%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  2.035006/  2.082654, val:  81.67%, val_best:  83.75%, tr:  99.18%, tr_best:  99.69%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9211%\n",
      "layer   2  Sparsity: 81.7130%\n",
      "layer   3  Sparsity: 82.8379%\n",
      "total_backward_count 210485 real_backward_count 35456  16.845%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  2.037663/  2.081728, val:  84.17%, val_best:  84.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9014%\n",
      "layer   2  Sparsity: 81.8194%\n",
      "layer   3  Sparsity: 82.8537%\n",
      "total_backward_count 215380 real_backward_count 35844  16.642%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  2.029754/  2.082554, val:  85.00%, val_best:  85.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8725%\n",
      "layer   2  Sparsity: 81.8938%\n",
      "layer   3  Sparsity: 83.0677%\n",
      "total_backward_count 220275 real_backward_count 36203  16.435%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  2.029235/  2.076478, val:  81.67%, val_best:  85.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 40.93 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8426%\n",
      "layer   2  Sparsity: 81.9481%\n",
      "layer   3  Sparsity: 83.1483%\n",
      "total_backward_count 225170 real_backward_count 36547  16.231%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  2.028935/  2.074214, val:  83.75%, val_best:  85.00%, tr:  99.08%, tr_best:  99.80%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8575%\n",
      "layer   2  Sparsity: 81.8813%\n",
      "layer   3  Sparsity: 82.7824%\n",
      "total_backward_count 230065 real_backward_count 36882  16.031%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  2.021189/  2.068642, val:  82.50%, val_best:  85.00%, tr:  99.49%, tr_best:  99.80%, epoch time: 40.80 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8927%\n",
      "layer   2  Sparsity: 81.8247%\n",
      "layer   3  Sparsity: 82.5869%\n",
      "total_backward_count 234960 real_backward_count 37236  15.848%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  2.024496/  2.074522, val:  83.33%, val_best:  85.00%, tr:  99.28%, tr_best:  99.80%, epoch time: 40.89 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8771%\n",
      "layer   2  Sparsity: 81.8070%\n",
      "layer   3  Sparsity: 82.5877%\n",
      "total_backward_count 239855 real_backward_count 37597  15.675%\n",
      "fc layer 1 self.abs_max_out: 6925.0\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  2.025488/  2.073026, val:  82.50%, val_best:  85.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8863%\n",
      "layer   2  Sparsity: 81.8482%\n",
      "layer   3  Sparsity: 82.7280%\n",
      "total_backward_count 244750 real_backward_count 37941  15.502%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  2.023586/  2.074413, val:  84.58%, val_best:  85.00%, tr:  99.39%, tr_best:  99.80%, epoch time: 40.71 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8712%\n",
      "layer   2  Sparsity: 82.0021%\n",
      "layer   3  Sparsity: 82.6344%\n",
      "total_backward_count 249645 real_backward_count 38242  15.319%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  2.020502/  2.074157, val:  83.75%, val_best:  85.00%, tr:  99.69%, tr_best:  99.80%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8663%\n",
      "layer   2  Sparsity: 81.7457%\n",
      "layer   3  Sparsity: 82.7321%\n",
      "total_backward_count 254540 real_backward_count 38555  15.147%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  2.021041/  2.069960, val:  82.92%, val_best:  85.00%, tr:  99.28%, tr_best:  99.80%, epoch time: 40.58 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9398%\n",
      "layer   2  Sparsity: 81.8573%\n",
      "layer   3  Sparsity: 82.8174%\n",
      "total_backward_count 259435 real_backward_count 38900  14.994%\n",
      "fc layer 1 self.abs_max_out: 6933.0\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  2.023813/  2.077635, val:  82.50%, val_best:  85.00%, tr:  99.49%, tr_best:  99.80%, epoch time: 40.95 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8683%\n",
      "layer   2  Sparsity: 81.6520%\n",
      "layer   3  Sparsity: 82.7383%\n",
      "total_backward_count 264330 real_backward_count 39191  14.827%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  2.024961/  2.081758, val:  81.25%, val_best:  85.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 40.77 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8786%\n",
      "layer   2  Sparsity: 81.8133%\n",
      "layer   3  Sparsity: 83.1676%\n",
      "total_backward_count 269225 real_backward_count 39504  14.673%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  2.027570/  2.081003, val:  81.25%, val_best:  85.00%, tr:  99.49%, tr_best:  99.80%, epoch time: 40.74 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8777%\n",
      "layer   2  Sparsity: 81.9540%\n",
      "layer   3  Sparsity: 83.3801%\n",
      "total_backward_count 274120 real_backward_count 39814  14.524%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  2.027726/  2.074623, val:  81.67%, val_best:  85.00%, tr:  99.39%, tr_best:  99.80%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9210%\n",
      "layer   2  Sparsity: 81.9514%\n",
      "layer   3  Sparsity: 83.1919%\n",
      "total_backward_count 279015 real_backward_count 40107  14.374%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  2.023859/  2.073940, val:  84.58%, val_best:  85.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 40.94 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9170%\n",
      "layer   2  Sparsity: 82.0057%\n",
      "layer   3  Sparsity: 82.9856%\n",
      "total_backward_count 283910 real_backward_count 40379  14.222%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  2.025039/  2.079869, val:  83.33%, val_best:  85.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 41.00 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9105%\n",
      "layer   2  Sparsity: 81.9492%\n",
      "layer   3  Sparsity: 83.0391%\n",
      "total_backward_count 288805 real_backward_count 40666  14.081%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  2.029469/  2.082948, val:  82.92%, val_best:  85.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8795%\n",
      "layer   2  Sparsity: 82.0653%\n",
      "layer   3  Sparsity: 83.4050%\n",
      "total_backward_count 293700 real_backward_count 40948  13.942%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  2.032368/  2.086240, val:  79.58%, val_best:  85.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 40.71 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8702%\n",
      "layer   2  Sparsity: 82.1096%\n",
      "layer   3  Sparsity: 83.6040%\n",
      "total_backward_count 298595 real_backward_count 41223  13.806%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  2.028174/  2.077852, val:  80.42%, val_best:  85.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8758%\n",
      "layer   2  Sparsity: 82.0494%\n",
      "layer   3  Sparsity: 83.1336%\n",
      "total_backward_count 303490 real_backward_count 41471  13.665%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  2.022809/  2.075019, val:  84.58%, val_best:  85.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8753%\n",
      "layer   2  Sparsity: 82.0387%\n",
      "layer   3  Sparsity: 83.1458%\n",
      "total_backward_count 308385 real_backward_count 41716  13.527%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  2.021612/  2.071634, val:  85.42%, val_best:  85.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.86 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8374%\n",
      "layer   2  Sparsity: 81.9890%\n",
      "layer   3  Sparsity: 83.3279%\n",
      "total_backward_count 313280 real_backward_count 41988  13.403%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  2.019791/  2.068634, val:  87.08%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8979%\n",
      "layer   2  Sparsity: 81.9427%\n",
      "layer   3  Sparsity: 83.4251%\n",
      "total_backward_count 318175 real_backward_count 42226  13.271%\n",
      "fc layer 1 self.abs_max_out: 6934.0\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  2.021763/  2.076974, val:  85.42%, val_best:  87.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9084%\n",
      "layer   2  Sparsity: 81.8263%\n",
      "layer   3  Sparsity: 83.5062%\n",
      "total_backward_count 323070 real_backward_count 42484  13.150%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  2.022464/  2.074463, val:  81.25%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8969%\n",
      "layer   2  Sparsity: 81.8566%\n",
      "layer   3  Sparsity: 83.6146%\n",
      "total_backward_count 327965 real_backward_count 42746  13.034%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  2.024520/  2.076438, val:  85.00%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 40.10 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8844%\n",
      "layer   2  Sparsity: 81.8938%\n",
      "layer   3  Sparsity: 83.5725%\n",
      "total_backward_count 332860 real_backward_count 42994  12.917%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  2.020892/  2.071164, val:  85.83%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.74 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9164%\n",
      "layer   2  Sparsity: 82.0989%\n",
      "layer   3  Sparsity: 83.3799%\n",
      "total_backward_count 337755 real_backward_count 43221  12.797%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  2.019280/  2.074682, val:  83.75%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9049%\n",
      "layer   2  Sparsity: 82.2422%\n",
      "layer   3  Sparsity: 83.1673%\n",
      "total_backward_count 342650 real_backward_count 43466  12.685%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  2.018707/  2.074122, val:  85.42%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 40.74 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8943%\n",
      "layer   2  Sparsity: 82.3432%\n",
      "layer   3  Sparsity: 83.5678%\n",
      "total_backward_count 347545 real_backward_count 43719  12.579%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  2.014105/  2.074071, val:  82.92%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9061%\n",
      "layer   2  Sparsity: 82.3106%\n",
      "layer   3  Sparsity: 83.6636%\n",
      "total_backward_count 352440 real_backward_count 43943  12.468%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  2.017612/  2.074315, val:  84.58%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8508%\n",
      "layer   2  Sparsity: 82.0739%\n",
      "layer   3  Sparsity: 83.5922%\n",
      "total_backward_count 357335 real_backward_count 44167  12.360%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  2.016429/  2.070170, val:  80.42%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 41.00 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8218%\n",
      "layer   2  Sparsity: 82.1002%\n",
      "layer   3  Sparsity: 83.5952%\n",
      "total_backward_count 362230 real_backward_count 44363  12.247%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  2.015554/  2.071162, val:  83.75%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9150%\n",
      "layer   2  Sparsity: 82.1427%\n",
      "layer   3  Sparsity: 83.7776%\n",
      "total_backward_count 367125 real_backward_count 44548  12.134%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  2.020631/  2.073252, val:  87.08%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.82 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9128%\n",
      "layer   2  Sparsity: 82.2941%\n",
      "layer   3  Sparsity: 83.7929%\n",
      "total_backward_count 372020 real_backward_count 44774  12.035%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  2.019916/  2.068346, val:  82.08%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 40.85 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8897%\n",
      "layer   2  Sparsity: 82.1314%\n",
      "layer   3  Sparsity: 83.7947%\n",
      "total_backward_count 376915 real_backward_count 44932  11.921%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  2.015966/  2.068103, val:  81.67%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8867%\n",
      "layer   2  Sparsity: 81.9571%\n",
      "layer   3  Sparsity: 83.7635%\n",
      "total_backward_count 381810 real_backward_count 45140  11.823%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  2.014629/  2.074670, val:  80.83%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8956%\n",
      "layer   2  Sparsity: 81.9739%\n",
      "layer   3  Sparsity: 83.7212%\n",
      "total_backward_count 386705 real_backward_count 45339  11.724%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  2.015748/  2.071399, val:  83.75%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9138%\n",
      "layer   2  Sparsity: 82.1623%\n",
      "layer   3  Sparsity: 83.9927%\n",
      "total_backward_count 391600 real_backward_count 45523  11.625%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  2.018944/  2.068208, val:  83.33%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8594%\n",
      "layer   2  Sparsity: 82.0425%\n",
      "layer   3  Sparsity: 84.0203%\n",
      "total_backward_count 396495 real_backward_count 45714  11.530%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  2.016347/  2.069778, val:  82.50%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 41.02 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8655%\n",
      "layer   2  Sparsity: 81.7574%\n",
      "layer   3  Sparsity: 84.0441%\n",
      "total_backward_count 401390 real_backward_count 45893  11.434%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  2.011286/  2.065842, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8960%\n",
      "layer   2  Sparsity: 82.0289%\n",
      "layer   3  Sparsity: 83.9647%\n",
      "total_backward_count 406285 real_backward_count 46077  11.341%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  2.011200/  2.070288, val:  78.75%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8703%\n",
      "layer   2  Sparsity: 82.0461%\n",
      "layer   3  Sparsity: 83.9157%\n",
      "total_backward_count 411180 real_backward_count 46242  11.246%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  2.011140/  2.067632, val:  83.75%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8579%\n",
      "layer   2  Sparsity: 82.0971%\n",
      "layer   3  Sparsity: 84.0153%\n",
      "total_backward_count 416075 real_backward_count 46429  11.159%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  2.012927/  2.064749, val:  85.00%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 41.17 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.8917%\n",
      "layer   2  Sparsity: 82.0351%\n",
      "layer   3  Sparsity: 84.0751%\n",
      "total_backward_count 420970 real_backward_count 46589  11.067%\n",
      "lif layer 2 self.abs_max_v: 4712.5\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  2.007518/  2.067654, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8808%\n",
      "layer   2  Sparsity: 82.0719%\n",
      "layer   3  Sparsity: 84.1616%\n",
      "total_backward_count 425865 real_backward_count 46724  10.972%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  2.006587/  2.062684, val:  84.58%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.67 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9206%\n",
      "layer   2  Sparsity: 82.1066%\n",
      "layer   3  Sparsity: 84.1097%\n",
      "total_backward_count 430760 real_backward_count 46892  10.886%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  2.006151/  2.061732, val:  81.25%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.26 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8588%\n",
      "layer   2  Sparsity: 82.0048%\n",
      "layer   3  Sparsity: 84.1468%\n",
      "total_backward_count 435655 real_backward_count 47059  10.802%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  2.007014/  2.064242, val:  83.33%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.61 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8744%\n",
      "layer   2  Sparsity: 82.0116%\n",
      "layer   3  Sparsity: 84.2474%\n",
      "total_backward_count 440550 real_backward_count 47215  10.717%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  2.007856/  2.059137, val:  85.00%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.26 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8890%\n",
      "layer   2  Sparsity: 82.1302%\n",
      "layer   3  Sparsity: 84.1582%\n",
      "total_backward_count 445445 real_backward_count 47376  10.636%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  2.004974/  2.060632, val:  85.83%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 41.02 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8802%\n",
      "layer   2  Sparsity: 81.9657%\n",
      "layer   3  Sparsity: 84.0838%\n",
      "total_backward_count 450340 real_backward_count 47528  10.554%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  2.006559/  2.061419, val:  86.67%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8949%\n",
      "layer   2  Sparsity: 81.9776%\n",
      "layer   3  Sparsity: 84.1651%\n",
      "total_backward_count 455235 real_backward_count 47677  10.473%\n",
      "fc layer 1 self.abs_max_out: 6962.0\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  2.007283/  2.062227, val:  84.58%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8763%\n",
      "layer   2  Sparsity: 81.9742%\n",
      "layer   3  Sparsity: 83.9892%\n",
      "total_backward_count 460130 real_backward_count 47803  10.389%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  2.005884/  2.061418, val:  82.08%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8737%\n",
      "layer   2  Sparsity: 82.0273%\n",
      "layer   3  Sparsity: 84.1010%\n",
      "total_backward_count 465025 real_backward_count 47935  10.308%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  2.005681/  2.062249, val:  85.83%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8920%\n",
      "layer   2  Sparsity: 82.0724%\n",
      "layer   3  Sparsity: 84.2824%\n",
      "total_backward_count 469920 real_backward_count 48060  10.227%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  2.004837/  2.061723, val:  86.25%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.75 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8834%\n",
      "layer   2  Sparsity: 82.1281%\n",
      "layer   3  Sparsity: 84.3536%\n",
      "total_backward_count 474815 real_backward_count 48191  10.149%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  2.004740/  2.058793, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.07 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9287%\n",
      "layer   2  Sparsity: 82.1083%\n",
      "layer   3  Sparsity: 84.4726%\n",
      "total_backward_count 479710 real_backward_count 48333  10.075%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  2.006261/  2.061691, val:  85.00%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8803%\n",
      "layer   2  Sparsity: 82.0451%\n",
      "layer   3  Sparsity: 84.5809%\n",
      "total_backward_count 484605 real_backward_count 48481  10.004%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  2.008158/  2.065173, val:  87.08%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8846%\n",
      "layer   2  Sparsity: 82.0295%\n",
      "layer   3  Sparsity: 84.7326%\n",
      "total_backward_count 489500 real_backward_count 48623   9.933%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  2.007064/  2.066348, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8909%\n",
      "layer   2  Sparsity: 82.0032%\n",
      "layer   3  Sparsity: 84.7216%\n",
      "total_backward_count 494395 real_backward_count 48746   9.860%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  2.006329/  2.062413, val:  84.58%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9343%\n",
      "layer   2  Sparsity: 82.0115%\n",
      "layer   3  Sparsity: 84.6135%\n",
      "total_backward_count 499290 real_backward_count 48876   9.789%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  2.004272/  2.061917, val:  86.25%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8936%\n",
      "layer   2  Sparsity: 82.1130%\n",
      "layer   3  Sparsity: 84.5603%\n",
      "total_backward_count 504185 real_backward_count 48998   9.718%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  2.000650/  2.061842, val:  86.25%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8903%\n",
      "layer   2  Sparsity: 82.2424%\n",
      "layer   3  Sparsity: 84.4034%\n",
      "total_backward_count 509080 real_backward_count 49101   9.645%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  2.002044/  2.063984, val:  85.42%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.38 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9151%\n",
      "layer   2  Sparsity: 82.2427%\n",
      "layer   3  Sparsity: 84.5309%\n",
      "total_backward_count 513975 real_backward_count 49248   9.582%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  2.004495/  2.063675, val:  86.25%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.10 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8971%\n",
      "layer   2  Sparsity: 82.1564%\n",
      "layer   3  Sparsity: 84.6627%\n",
      "total_backward_count 518870 real_backward_count 49384   9.518%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  2.005888/  2.063194, val:  86.25%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8772%\n",
      "layer   2  Sparsity: 81.8901%\n",
      "layer   3  Sparsity: 84.6695%\n",
      "total_backward_count 523765 real_backward_count 49494   9.450%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.999570/  2.059057, val:  85.42%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8627%\n",
      "layer   2  Sparsity: 82.0558%\n",
      "layer   3  Sparsity: 84.5757%\n",
      "total_backward_count 528660 real_backward_count 49621   9.386%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.999339/  2.063489, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9022%\n",
      "layer   2  Sparsity: 82.1125%\n",
      "layer   3  Sparsity: 84.6278%\n",
      "total_backward_count 533555 real_backward_count 49742   9.323%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  2.003460/  2.065291, val:  83.33%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9002%\n",
      "layer   2  Sparsity: 82.1901%\n",
      "layer   3  Sparsity: 84.5958%\n",
      "total_backward_count 538450 real_backward_count 49862   9.260%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  2.006092/  2.062995, val:  83.75%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.88 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8926%\n",
      "layer   2  Sparsity: 82.1622%\n",
      "layer   3  Sparsity: 84.7796%\n",
      "total_backward_count 543345 real_backward_count 49965   9.196%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  2.003954/  2.059618, val:  82.50%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8837%\n",
      "layer   2  Sparsity: 82.0814%\n",
      "layer   3  Sparsity: 84.5890%\n",
      "total_backward_count 548240 real_backward_count 50074   9.134%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.997205/  2.057639, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 41.15 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.8653%\n",
      "layer   2  Sparsity: 82.2130%\n",
      "layer   3  Sparsity: 84.4622%\n",
      "total_backward_count 553135 real_backward_count 50151   9.067%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.997272/  2.060728, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8611%\n",
      "layer   2  Sparsity: 82.2031%\n",
      "layer   3  Sparsity: 84.5007%\n",
      "total_backward_count 558030 real_backward_count 50256   9.006%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.995831/  2.049869, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8999%\n",
      "layer   2  Sparsity: 82.1308%\n",
      "layer   3  Sparsity: 84.6178%\n",
      "total_backward_count 562925 real_backward_count 50378   8.949%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.993090/  2.055113, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8610%\n",
      "layer   2  Sparsity: 82.1590%\n",
      "layer   3  Sparsity: 84.5514%\n",
      "total_backward_count 567820 real_backward_count 50477   8.890%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.993924/  2.053506, val:  84.58%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8681%\n",
      "layer   2  Sparsity: 82.0244%\n",
      "layer   3  Sparsity: 84.5852%\n",
      "total_backward_count 572715 real_backward_count 50583   8.832%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [5, 10,]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.5, 0.25, 0.125, 0.0625]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [4.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/128, 1/256, 1/512, 1/1024]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [5, 10, 15, 20, 25, 30]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [12_000, 25_000, 50_000, 75_000, 100_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-11,-10,-9]},\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'pyz704uj'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
